Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Inward issue link (Container),Outward issue link (Container),Outward issue link (Container),Outward issue link (Container),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Outward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Required),Inward issue link (Required),Outward issue link (Required),Outward issue link (Required),Inward issue link (Supercedes),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Sprint,Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
ScalaReflection.mirror should be a def,SPARK-8710,12841420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,29/Jun/15 19:21,28/Jul/15 05:31,14/Jul/23 06:26,29/Jun/15 23:26,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"Right now, it is a val (https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L31). This introduces problems when we trigger the creation of ScalaReflection in one thread and use it in another thread.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 19:46:04 UTC 2015,,,,,,,,,,"0|i2gn27:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"29/Jun/15 19:46;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/7094;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exclude hadoop-client's mockito-all dependency to fix test compilation break for Hadoop 2,SPARK-8709,12841409,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,29/Jun/15 18:15,29/Jun/15 21:08,14/Jul/23 06:26,29/Jun/15 21:08,1.5.0,,,,,,1.5.0,,,,,,,,,,0,,,,,,"{{build/sbt -Phadoop-1 -Dhadoop.version=2.0.0-mr1-cdh4.1.1 -Phive -Pkinesis-asl -Phive-thriftserver core/test:compile}} currently fails to compile:

{code}
[error] /Users/joshrosen/Documents/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:117: error: cannot find symbol
[error]     when(shuffleMemoryManager.tryToAcquire(anyLong())).then(returnsFirstArg());
[error]                                                       ^
[error]   symbol:   method then(Answer<Object>)
[error]   location: interface OngoingStubbing<Long>
[error] /Users/joshrosen/Documents/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:408: error: cannot find symbol
[error]       .then(returnsFirstArg()) // Allocate initial sort buffer
[error]       ^
[error]   symbol:   method then(Answer<Object>)
[error]   location: interface OngoingStubbing<Long>
[error] /Users/joshrosen/Documents/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:435: error: cannot find symbol
[error]       .then(returnsFirstArg()) // Allocate initial sort buffer
[error]       ^
[error]   symbol:   method then(Answer<Object>)
[error]   location: interface OngoingStubbing<Long>
[error] 3 errors
[error] (core/test:compile) javac returned nonzero exit code
[error] Total time: 60 s, completed Jun 29, 2015 11:03:13 AM
{code}

This is because {{hadoop-client}} pulls in a dependency on {{mockito-all}}, but I recently changed Spark to depend on {{mockito-core}} instead, which caused Hadoop's earlier Mockito version to take precedence over our newer version. ",,apachespark,joshrosen,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 21:08:19 UTC 2015,,,,,,,,,,"0|i2gmzr:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"29/Jun/15 18:48;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7090;;;","29/Jun/15 21:08;joshrosen;Issue resolved by pull request 7090
[https://github.com/apache/spark/pull/7090];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
RDD#toDebugString fails if any cached RDD has invalid partitions,SPARK-8707,12841377,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,navis,ilikerps,ilikerps,29/Jun/15 16:38,03/Sep/15 05:19,14/Jul/23 06:26,03/Sep/15 05:10,1.4.0,1.4.1,,,,,1.6.0,,,,,,Spark Core,,,,1,starter,,,,,"Repro:

{code}
sc.textFile(""/ThisFileDoesNotExist"").cache()
sc.parallelize(0 until 100).toDebugString
{code}

Output:

{code}
java.io.IOException: Not a file: /ThisFileDoesNotExist
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:215)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
	at org.apache.spark.storage.RDDInfo$.fromRdd(RDDInfo.scala:59)
	at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:1455)
	at org.apache.spark.SparkContext$$anonfun$34.apply(SparkContext.scala:1455)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.MapLike$DefaultValuesIterable.foreach(MapLike.scala:206)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.SparkContext.getRDDStorageInfo(SparkContext.scala:1455)
	at org.apache.spark.rdd.RDD.debugSelf$1(RDD.scala:1573)
	at org.apache.spark.rdd.RDD.firstDebugString$1(RDD.scala:1607)
	at org.apache.spark.rdd.RDD.toDebugString(RDD.scala:1637
{code}

This is because toDebugString gets all the partitions from all RDDs, which fails (via SparkContext#getRDDStorageInfo). This pathway should definitely be resilient to other RDDs being invalid (and getRDDStorageInfo should probably also be).",,@madhu,apachespark,ilikerps,nealmcb,Rosstin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 12:59:04 UTC 2015,,,,,,,,,,"0|i2gmsv:",9223372036854775807,,,,,,,,,,,,,,1.6.0,,,,,,,,,,,,,"29/Jun/15 20:31;@madhu;One possible solution is to have a new version of getRDDStorageInfo which would accept RDD id as an argument and calls RDDInfo.fromRdd only for the concerned RDD. The RDD.toDebugString can pass RDD.id as a argument to this method.;;;","30/Jun/15 12:59;apachespark;User 'navis' has created a pull request for this issue:
https://github.com/apache/spark/pull/7127;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Javascript error in the web console when `totalExecutionTime` of a task is 0,SPARK-8705,12841344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,29/Jun/15 14:01,20/Jul/15 07:44,14/Jul/23 06:26,20/Jul/15 07:44,1.4.0,,,,,,1.5.0,,,,,,Web UI,,,,0,,,,,,"Because System.currentTimeMillis() is not accurate for tasks that only need several milliseconds, sometimes totalExecutionTime in makeTimeline will be 0. If totalExecutionTime is 0, there will the following error in the console.

!https://cloud.githubusercontent.com/assets/1000778/8406776/5cd38e04-1e92-11e5-89f2-0c5134fe4b6b.png!",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 20 07:44:04 UTC 2015,,,,,,,,,,"0|i2gmlj:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"29/Jun/15 14:07;zsxwing;A simple fix is don't add {{rect}} s to {{svg}} when {{totalExecutionTime}} is 0 in https://github.com/apache/spark/blob/04ddcd4db7801abefa9c9effe5d88413b29d713b/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala#L599 

This conflicts with https://github.com/apache/spark/pull/7082 , so I will send a PR after pr #7082 is merged.;;;","29/Jun/15 16:34;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/7088;;;","20/Jul/15 07:44;zsxwing;Resolved by https://github.com/apache/spark/pull/7088;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop Configuration has to disable client cache when writing or reading delegation tokens.,SPARK-8688,12841152,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,carlmartin,carlmartin,carlmartin,28/Jun/15 07:43,03/Feb/16 14:01,14/Jul/23 06:26,02/Jul/15 06:02,1.5.0,,,,,,1.5.0,,,,,,YARN,,,,0,,,,,,"In class *AMDelegationTokenRenewer* and *ExecutorDelegationTokenUpdater*, Spark will write and read the credentials.
But if we don't disable the *fs.hdfs.impl.disable.cache*, Spark will use cached  FileSystem (which will use old token ) to  upload or download file.
Then when the old token is expired, it can't gain the auth to get/put the hdfs.

(I only tested in a very short time with the configuration:
dfs.namenode.delegation.token.renew-interval=3min
dfs.namenode.delegation.token.max-lifetime=10min
I'm not sure whatever it matters.
 )",,apachespark,carlmartin,qwertymaniac,stevel@apache.org,xiaochen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 03 14:01:26 UTC 2016,,,,,,,,,,"0|i2glfb:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"28/Jun/15 10:05;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/7069;;;","03/Feb/16 14:01;stevel@apache.org;Has anyone filed a bug against HDFS for this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on yarn-client mode can't send `spark.yarn.credentials.file` to executor.,SPARK-8687,12841148,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,carlmartin,carlmartin,carlmartin,28/Jun/15 07:05,02/Jul/15 18:35,14/Jul/23 06:26,02/Jul/15 06:14,1.5.0,,,,,,1.4.1,1.5.0,,,,,YARN,,,,0,,,,,,Yarn will set +spark.yarn.credentials.file+ after *DriverEndpoint* initialized. So executor will fetch the old configuration and will cause the problem.,,apachespark,carlmartin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 28 08:31:03 UTC 2015,,,,,,,,,,"0|i2glef:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"28/Jun/15 08:31;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/7066;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Depend on mockito-core instead of mockito-all,SPARK-8683,12841130,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,27/Jun/15 23:17,28/Jun/15 06:28,14/Jul/23 06:26,28/Jun/15 06:28,,,,,,,1.5.0,,,,,,Build,,,,0,,,,,,"Spark's tests currently depend on {{mockito-all}}, which bundles Hamcrest and Objenesis classes. Instead, it should depend on {{mockito-core}}, which declares those libraries as Maven dependencies. This is necessary in order to fix a dependency conflict that leads to a NoSuchMethodError when using certain Hamcrest matchers.

See https://github.com/mockito/mockito/wiki/Declaring-mockito-dependency for more details.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 28 06:28:40 UTC 2015,,,,,,,,,,"0|i2glaf:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"27/Jun/15 23:19;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7061;;;","28/Jun/15 06:28;joshrosen;Issue resolved by pull request 7061
[https://github.com/apache/spark/pull/7061];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
PropagateTypes is very slow when there are lots of columns,SPARK-8680,12841113,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,davies,davies,27/Jun/15 18:05,30/Jun/15 15:18,14/Jul/23 06:26,30/Jun/15 15:18,1.3.1,1.4.0,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"The time for PropagateTypes is O(N*N), N is the number of columns, which is very slow if there many columns (>1000)

There easiest optimization could be put `q.inputSet` outside of  transformExpressions which could have about 4 times improvement for N=3000",,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 15:18:05 UTC 2015,,,,,,,,,,"0|i2gl6n:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"29/Jun/15 15:16;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/7087;;;","30/Jun/15 15:18;davies;Issue resolved by pull request 7087
[https://github.com/apache/spark/pull/7087];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Default values in Pipeline API should be immutable,SPARK-8678,12841103,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,MechCoder,MechCoder,MechCoder,27/Jun/15 16:40,11/Aug/15 17:13,14/Jul/23 06:26,30/Jun/15 17:30,1.4.1,,,,,,1.4.1,1.5.0,,,,,ML,PySpark,,,0,,,,,,"If the default params are mutable, then if the function or method is called again without any value for the default params, then the changed values are used.",,MechCoder,mengxr,yuu.ishikawa@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9828,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 17:30:00 UTC 2015,,,,,,,,,,"0|i2gl4f:",9223372036854775807,,,,,,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"30/Jun/15 17:30;mengxr;Fixed in https://github.com/apache/spark/pull/7058.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executors created by LocalBackend won't get the same classpath as other executor backends ,SPARK-8675,12841038,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coderplay,mzhou,mzhou,27/Jun/15 01:30,10/Jul/15 16:52,14/Jul/23 06:26,10/Jul/15 16:52,1.4.0,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,"AFAIK, some spark application always use LocalBackend to do some local initiatives, spark sql is an example.  Starting a LocalPoint won't add user classpath into executor. 

{noformat}
  override def start() {
    localEndpoint = SparkEnv.get.rpcEnv.setupEndpoint(
      ""LocalBackendEndpoint"", new LocalEndpoint(SparkEnv.get.rpcEnv, scheduler, this, totalCores))
  }
{noformat}

Thus will cause local executor fail with these scenarios,  loading hadoop built-in native libraries,  loading other user defined native libraries, loading user jars,  reading s3 config from a site.xml file, etc
 ",,apachespark,mzhou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 19:36:11 UTC 2015,,,,,,,,,,"0|i2gkpz:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"29/Jun/15 19:22;mzhou;[~lian cheng] I am working on it;;;","29/Jun/15 19:36;apachespark;User 'coderplay' has created a pull request for this issue:
https://github.com/apache/spark/pull/7091;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet 1.7 files that store binary enums crash when inferring schema,SPARK-8669,12840961,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,stevenshe,stevenshe,stevenshe,26/Jun/15 19:28,30/Jun/15 01:50,14/Jul/23 06:26,30/Jun/15 01:50,1.5.0,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Loading a Parquet 1.7 file that contains a binary ENUM field in Spark 1.5-SNAPSHOT crashes with the following exception:

{noformat}
  org.apache.spark.sql.AnalysisException: Illegal Parquet type: BINARY (ENUM);
  at org.apache.spark.sql.parquet.CatalystSchemaConverter.illegalType$1(CatalystSchemaConverter.scala:129)
  at org.apache.spark.sql.parquet.CatalystSchemaConverter.convertPrimitiveField(CatalystSchemaConverter.scala:184)
  at org.apache.spark.sql.parquet.CatalystSchemaConverter.convertField(CatalystSchemaConverter.scala:114)
...
{noformat}",,apachespark,marmbrus,stevenshe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 01:50:27 UTC 2015,,,,,,,,,,"0|i2gk8v:",9223372036854775807,,,,,lian cheng,,,,,,,,,1.5.0,,,,,,,,,,,,,"26/Jun/15 19:33;stevenshe;https://github.com/apache/spark/pull/7048;;;","26/Jun/15 19:33;apachespark;User 'stevencanopy' has created a pull request for this issue:
https://github.com/apache/spark/pull/7048;;;","30/Jun/15 01:50;marmbrus;Issue resolved by pull request 7048
[https://github.com/apache/spark/pull/7048];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
[SparkR] SparkSQL tests fail in R 3.2,SPARK-8662,12840865,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cafreeman,cafreeman,cafreeman,26/Jun/15 14:39,29/Dec/15 09:10,14/Jul/23 06:26,26/Jun/15 17:10,1.4.0,,,,,,1.4.1,1.5.0,,,,,SparkR,,,,0,,,,,,"SparkR tests for equality using `all.equal` on environments fail in R 3.2.

This is due to a change in how equality between environments is handled in the new version of R.

This should most likely not be a huge problem, we'll just have to rewrite some of the tests to be more fine-grained instead of testing equality on entire environments.",,apachespark,cafreeman,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 19:07:54 UTC 2015,,,,,,,,,,"0|i2gjnj:",9223372036854775807,,,,,shivaram,,,,,,,,,,,,,,,,,,,,,,"26/Jun/15 14:51;cafreeman;PR here: https://github.com/apache/spark/pull/7045;;;","26/Jun/15 14:52;apachespark;User 'cafreeman' has created a pull request for this issue:
https://github.com/apache/spark/pull/7045;;;","26/Jun/15 17:08;shivaram;Resolved by https://github.com/apache/spark/pull/7045;;;","26/Jun/15 17:09;shivaram;[~srowen] Could you help add [~cafreeman] to the developers group ? I can't seem to assign this issue;;;","26/Jun/15 18:49;srowen;[~shivaram] you should be able to add to Contributors at https://issues.apache.org/jira/plugins/servlet/project-config/SPARK/roles now. After you do that can you assign?;;;","26/Jun/15 19:07;shivaram;Thanks [~srowen] ! I was able to add contributors and assigned this to Chris. Won't bother you again about this !;;;",,,,,,,,,,,,,,,,,,,,,,,
"AttributeReference equals method only compare name, exprId and dataType",SPARK-8658,12840821,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smilegator,ajnavarro,ajnavarro,26/Jun/15 12:13,17/Nov/15 07:59,14/Jul/23 06:26,16/Nov/15 23:22,1.3.0,1.3.1,1.4.0,,,,1.6.0,,,,,,SQL,,,,2,,,,,,"The AttributeReference ""equals"" method only accept as different objects with different name, expression id or dataType. With this behavior when I tried to do a ""transformExpressionsDown"" and try to transform qualifiers inside ""AttributeReferences"", these objects are not replaced, because the transformer considers them equal.

I propose to add to the ""equals"" method this variables:

name, dataType, nullable, metadata, epxrId, qualifiers",,ajnavarro,apachespark,dkbiswal,HatY,marmbrus,rxin,smilegator,smolav,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 17 07:59:04 UTC 2015,,,,,,,,,,"0|i2gjev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/15 05:25;marmbrus;+1.  Would you like to submit a PR?;;;","09/Oct/15 06:10;smilegator;Hi, Michael and Antonio, 

Trying to understand the problem and fix it if I can. Expression ID is the same but their qualifiers are different???

Could you give a query sample? I am trying to reproduce the problem? Is this problem related to a self join?   

Thanks, 

Xiao Li;;;","15/Oct/15 19:14;marmbrus;There is no query that exposes the problem as its an internal quirk.  The {{equals}} method should check all of the specified fields for equality.  Today it is missing some.;;;","21/Oct/15 20:57;smilegator;Hi, Michael, 

Thank you! It sounds a trivial work. Let me try it. Send a pull request soon. 

Best wishes, 

Xiao Li;;;","22/Oct/15 04:52;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/9216;;;","16/Nov/15 23:22;marmbrus;Issue resolved by pull request 9216
[https://github.com/apache/spark/pull/9216];;;","17/Nov/15 07:59;apachespark;User 'gatorsmile' has created a pull request for this issue:
https://github.com/apache/spark/pull/9761;;;",,,,,,,,,,,,,,,,,,,,,,
Fail to upload conf archive to viewfs,SPARK-8657,12840805,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,litao1990,litao1990,litao1990,26/Jun/15 11:16,10/Dec/15 09:51,14/Jul/23 06:26,08/Jul/15 18:31,1.4.0,,,,,,1.4.1,1.5.0,,,,,YARN,,,,0,distributed_cache,viewfs,,,,"When I run in spark-1.4 yarn-client mode, I throws the following Exception when trying to upload conf archive to viewfs:

15/06/26 17:56:37 INFO yarn.Client: Uploading resource file:/tmp/spark-095ec3d2-5dad-468c-8d46-2c813457404d/__hadoop_conf__8436284925771788661
.zip -> viewfs://nsX/user/ultraman/.sparkStaging/application_1434370929997_191242/__hadoop_conf__8436284925771788661.zip
15/06/26 17:56:38 INFO yarn.Client: Deleting staging directory .sparkStaging/application_1434370929997_191242
15/06/26 17:56:38 ERROR spark.SparkContext: Error initializing SparkContext.
java.lang.IllegalArgumentException: Wrong FS: hdfs://SunshineNameNode2:8020/user/ultraman/.sparkStaging/application_1434370929997_191242/__had
oop_conf__8436284925771788661.zip, expected: viewfs://nsX/
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.getUriPath(ViewFileSystem.java:117)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.getFileStatus(ViewFileSystem.java:346)
        at org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
        at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$5.apply(Client.scala:341)
        at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$5.apply(Client.scala:338)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:338)
        at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:559)
        at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:115)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:58)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:141)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:497)
        at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:1017)
        at $line3.$read$$iwC$$iwC.<init>(<console>:9)
        at $line3.$read$$iwC.<init>(<console>:18)
        at $line3.$read.<init>(<console>:20)
        at $line3.$read$.<init>(<console>:24)
        at $line3.$read$.<clinit>(<console>)
        at $line3.$eval$.<init>(<console>:7)
        at $line3.$eval$.<clinit>(<console>)
        at $line3.$eval.$print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)

The bug is easy to fix, we should pass the correct file system object to addResource. The similar issure is: https://github.com/apache/spark/pull/1483. I will attach my bug fix PR very soon.",spark-1.4.2 & hadoop-2.5.0-cdh5.3.2,apachespark,litao1990,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 09:51:09 UTC 2015,,,,,,,,,,"0|i2gjbb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/15 11:24;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7041;;;","26/Jun/15 11:36;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7042;;;","26/Jun/15 12:41;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7041;;;","27/Jun/15 02:47;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7053;;;","27/Jun/15 10:39;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7055;;;","30/Jun/15 11:21;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7125;;;","03/Jul/15 22:43;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7041;;;","08/Jul/15 19:25;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7297;;;","10/Dec/15 08:13;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7055;;;","10/Dec/15 09:50;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7041;;;","10/Dec/15 09:50;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7042;;;","10/Dec/15 09:51;apachespark;User 'litao-buptsse' has created a pull request for this issue:
https://github.com/apache/spark/pull/7053;;;",,,,,,,,,,,,,,,,,
Spark Standalone master json API's worker number is not match web UI number,SPARK-8656,12840792,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Wisely Chen,Wisely Chen,Wisely Chen,26/Jun/15 10:20,06/Jul/15 23:14,14/Jul/23 06:26,06/Jul/15 23:03,1.4.0,,,,,,1.5.0,,,,,,Web UI,,,,0,,,,,,"Spark standalone master web UI show  ""Alive workers"" worker number, ""Alive Workers"" total core, total used cores and ""Alive workers"" total memory, memory used. 
But the JSON API page ""http://MASTERURL:8088/json"" shows ""all workers"" worker number, core, memory number. 
This webUI data is not sync with the JSON API. 

The proper way is to sync the number with webUI and JSON API. ",,apachespark,Wisely Chen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 10:32:04 UTC 2015,,,,,,,,,,"0|i2gj8f:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"26/Jun/15 10:32;apachespark;User 'thegiive' has created a pull request for this issue:
https://github.com/apache/spark/pull/7038;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Analysis exception when using ""NULL IN (...)"": invalid cast",SPARK-8654,12840766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dkbiswal,smolav,smolav,26/Jun/15 08:17,21/Oct/15 21:29,14/Jul/23 06:26,21/Oct/15 21:29,,,,,,,1.6.0,,,,,,SQL,,,,0,,,,,,"The following query throws an analysis exception:

{code}
SELECT * FROM t WHERE NULL NOT IN (1, 2, 3);
{code}

The exception is:

{code}
org.apache.spark.sql.AnalysisException: invalid cast from int to null;
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:38)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:66)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:52)
{code}

Here is a test that can be added to AnalysisSuite to check the issue:

{code}
  test(""SPARK-XXXX regression test"") {
    val plan = Project(Alias(In(Literal(null), Seq(Literal(1), Literal(2))), ""a"")() :: Nil,
      LocalRelation()
    )
    caseInsensitiveAnalyze(plan)
  }
{code}

Note that this kind of query is a corner case, but it is still valid SQL. An expression such as ""NULL IN (...)"" or ""NULL NOT IN (...)"" always gives NULL as a result, even if the list contains NULL. So it is safe to translate these expressions to Literal(null) during analysis.",,6133d,apachespark,dkbiswal,jliwork,marmbrus,rhillegas,smolav,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Oct 21 21:29:37 UTC 2015,,,,,,,,,,"0|i2gj2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/15 06:42;dkbiswal;I would like to work on this issue.. ;;;","05/Oct/15 18:49;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/8983;;;","08/Oct/15 17:42;marmbrus;Issue resolved by pull request 8983
[https://github.com/apache/spark/pull/8983];;;","08/Oct/15 20:47;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/9034;;;","08/Oct/15 20:48;marmbrus;This broke tests and got reverted.;;;","08/Oct/15 21:31;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/9036;;;","21/Oct/15 21:29;marmbrus;Issue resolved by pull request 9036
[https://github.com/apache/spark/pull/9036];;;",,,,,,,,,,,,,,,,,,,,,,
"PySpark tests sometimes forget to check return status of doctest.testmod(), masking failing tests",SPARK-8652,12840747,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,26/Jun/15 06:21,26/Jun/15 15:13,14/Jul/23 06:26,26/Jun/15 15:13,,,,,,,1.5.0,,,,,,PySpark,Tests,,,0,,,,,,"Several PySpark files call {{doctest.testmod()}} in order to run doctests, but forget to check its return status. As a result, failures will not be automatically detected by our test runner script, creating the potential for bugs to slip through.",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 15:13:40 UTC 2015,,,,,,,,,,"0|i2giyf:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.2,1.5.0,,,,,,,,,,,"26/Jun/15 06:24;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7032;;;","26/Jun/15 15:13;davies;Issue resolved by pull request 7032
[https://github.com/apache/spark/pull/7032];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Use the user-specified app name priority in SparkSQLCLIDriver or HiveThriftServer2,SPARK-8650,12840721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,waterman,waterman,waterman,26/Jun/15 02:18,23/Jul/15 05:29,14/Jul/23 06:26,30/Jun/15 05:35,1.4.0,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,,,apachespark,waterman,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9270,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 05:35:02 UTC 2015,,,,,,,,,,"0|i2gisv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/15 03:51;apachespark;User 'watermen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7030;;;","30/Jun/15 05:35;yhuai;Issue resolved by pull request 7030
[https://github.com/apache/spark/pull/7030];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Mapr repository is not defined properly,SPARK-8649,12840706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,ashok.blend,ashok.blend,ashok.blend,26/Jun/15 00:42,07/Jul/15 09:25,14/Jul/23 06:26,28/Jun/15 08:08,,,,,,,1.5.0,,,,,,Build,,,,0,,,,,,,,apachespark,ashok.blend,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 27 08:22:06 UTC 2015,,,,,,,,,,"0|i2gipj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/15 00:48;ashok.blend;  <repository>
      <id>mapr-repo</id>
      <name>MapR Repository</name>
      <url>http://repository.mapr.com/maven</url>
      <releases>
        <enabled>true</enabled>
      </releases>
      <snapshots>
        <enabled>false</enabled>
      </snapshots>
    </repository>

MapR Repository url is changed to http://repository.mapr.com/maven/;;;","27/Jun/15 01:55;ashok.blend;Updated pom.xml with correct repository url 
i.e
http://repository.mapr.com/maven/;;;","27/Jun/15 08:22;apachespark;User 'tszym' has created a pull request for this issue:
https://github.com/apache/spark/pull/7054;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark does not run on YARN if master not provided in command line,SPARK-8646,12840659,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lianhuiwang,juliet,juliet,25/Jun/15 21:40,31/Jul/15 10:46,14/Jul/23 06:26,17/Jul/15 02:33,1.4.0,,,,,,1.5.0,,,,,,PySpark,YARN,,,0,,,,,,"Running pyspark jobs result in a ""no module named pyspark"" when run in yarn-client mode in spark 1.4.

[I believe this JIRA represents the change that introduced this error.| https://issues.apache.org/jira/browse/SPARK-6869 ]

This does not represent a binary compatible change to spark. Scripts that worked on previous spark versions (ie comands the use spark-submit) should continue to work without modification between minor versions.","SPARK_HOME=local/path/to/spark1.4install/dir

also with
SPARK_HOME=local/path/to/spark1.4install/dir
PYTHONPATH=$SPARK_HOME/python/lib

Spark apps are submitted with the command:
$SPARK_HOME/bin/spark-submit outofstock/data_transform.py hdfs://foe-dev/DEMO_DATA/FACT_POS hdfs:/user/juliet/ex/ yarn-client

data_transform contains a main method, and the rest of the args are parsed in my own code.

",apachespark,davies,juliet,lianhuiwang,vanzin,wumin810711,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6869,,,,,,,,,,,,,"10/Jul/15 21:46;juliet;executor.log;https://issues.apache.org/jira/secure/attachment/12744822/executor.log","26/Jun/15 17:32;juliet;pi-test.log;https://issues.apache.org/jira/secure/attachment/12742163/pi-test.log","25/Jun/15 21:41;juliet;spark1.4-SPARK_HOME-set-PYTHONPATH-set.log;https://issues.apache.org/jira/secure/attachment/12741954/spark1.4-SPARK_HOME-set-PYTHONPATH-set.log","26/Jun/15 17:40;juliet;spark1.4-SPARK_HOME-set-inline-HADOOP_CONF_DIR.log;https://issues.apache.org/jira/secure/attachment/12742165/spark1.4-SPARK_HOME-set-inline-HADOOP_CONF_DIR.log","25/Jun/15 21:41;juliet;spark1.4-SPARK_HOME-set.log;https://issues.apache.org/jira/secure/attachment/12741953/spark1.4-SPARK_HOME-set.log","10/Jul/15 22:26;juliet;spark1.4-verbose.log;https://issues.apache.org/jira/secure/attachment/12744834/spark1.4-verbose.log","10/Jul/15 22:26;juliet;verbose-executor.log;https://issues.apache.org/jira/secure/attachment/12744833/verbose-executor.log",,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 31 10:46:47 UTC 2015,,,,,,,,,,"0|i2gifb:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"25/Jun/15 21:41;juliet;The logs from failures when only SPARK_HOME is set and when PYTHONPATH is also set.;;;","26/Jun/15 08:55;srowen;You're saying it doesn't work at all on YARN? I'd hope there are some unit tests for this but I am not sure if it covers this case. Do we know more about the likely issue here -- something isn't packaging pyspark, or not unpacking it? CC [~lianhuiwang];;;","26/Jun/15 17:18;vanzin;Hi [~j_houg],

Seems there's something weird going on in your setup. I downloaded the 1.4 hadoop 2.6 archive you're using, and ran this command line, without setting any extra env variables:

{code}
HADOOP_CONF_DIR=/etc/hadoop/conf ./bin/spark-submit --master yarn-client examples/src/main/python/pi.py
{code}

And it works. Notably, I see these two lines that seem to be missing from your logs:

{noformat}
15/06/26 10:14:28 INFO yarn.Client: Uploading resource file:/tmp/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip -> hdfs://vanzin-st1-1.vpc.cloudera.com:8020/user/systest/.sparkStaging/application_1435333340717_0002/pyspark.zip
15/06/26 10:14:28 INFO yarn.Client: Uploading resource file:/tmp/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip -> hdfs://vanzin-st1-1.vpc.cloudera.com:8020/user/systest/.sparkStaging/application_1435333340717_0002/py4j-0.8.2.1-src.zip
{noformat}

That's the code added in the change you mention; it's actually what allows pyspark to run with that large assembly (which python cannot read).

Can you double check the command line you're running (or try the simple example above)? Also, make sure your {{$SPARK_HOME/conf}} directory is not pointing at some other Spark configuration, or that you don't have any other env variables that may be affecting Spark configuration.;;;","26/Jun/15 17:32;juliet;Results from pi-test are uploaded in the attachment pi-test.log. Still a module missing error, this time it is pandas.algo.;;;","26/Jun/15 17:40;juliet;I ran the same line that gave me errors last time with HADOOP_CONF_DIR=/etc/hadoop/conf prepended. The command I used was:

HADOOP_CONF_DIR=/etc/hadoop/conf $SPARK_HOME/bin/spark-submit outofstock/data_transform.py hdfs://foe-dev/DEMO_DATA/FACT_POS hdfs:/user/juliet/ex1/ yarn-client 2> spark1.4-SPARK_HOME-set-inline-HADOOP_CONF_DIR.log

I've attached the output from that, it appears to be the same to me.;;;","26/Jun/15 20:16;vanzin;bq. Still a module missing error, this time it is pandas.algo.

Seems like you may have pandas installed on your driver node but not on cluster nodes. Could you check that? The code that uses pandas (in sql/context.py) seems to be careful about only using it when available.;;;","27/Jun/15 04:40;lianhuiwang;from [~juliet] 's logs, i think you miss python 'pandas.algos' module that pyspark does not provide. i think that you need to install it on nodes.;;;","27/Jun/15 06:28;juliet;When I configure spark to use my virtualenv that is on every node of the cluster and includes pandas, the pi job works fine. This makes sense to me because in the job that I have that fails, a spark context can be created without a module import error. the part that doesn't make sense to me is why pandas.algo would be needed at all. Looking at the code for the pi job, it is not part an import that is declared in the file. This is orthogonal to the point of this ticket, but is very very strange to me.

The module import error that is the core of this JIRA occurs when I need to write out results of a computation (ie calling sc.writeTextFile) which require the pyspark module to be available on the worker nodes.;;;","27/Jun/15 16:19;lianhuiwang;now i use spark-1.5.0-SNAPSHOT to run pi.py without install pandas  and it is ok. now i find that sql/dataframe.py must need to import pandas and if you do not use sql/dataframe.py i think it do not need pandas. [~juliet] can you provide executor's logs that can be got more details?;;;","06/Jul/15 07:54;srowen;[~j_houg] is the resolution here just that pandas has to be installed if pandas is used?;;;","06/Jul/15 10:49;juliet;[~sowen] The pandas error came when I tried to run the pi job-- which doesn't import pandas at all. The only imports in $SPARK_1.4_HOME/examples/src/main/python/pi.py are as follows:

    import sys
    from random import random
    from operator import add
    from pyspark import SparkContext

    
 PySpark itself doesn't require pandas (if it does, that should be documented) so having the pi job (doesn't require pandas) fail with a pandas not found error is wrong, because at no point should the pi job or pyspark itself require pandas. The pandas error is very, very weird but not obviously directly related to this ticket. The problem I reported here has to do with pyspark itself not being shipped or perhaps available to the worker nodes when I run a pyspark app from spark 1.4 using YARN.;;;","06/Jul/15 11:11;srowen;Right, none of this uses pandas directly. As [~vanzin] says the code appears to be careful about only calling ""import pandas"" when needed {{toPandas()}} or catching for the error when it's not available. My guess is that {{has_pandas}} is true on the driver but then that causes it to do things that the executors can't honor since they don't have pandas.

It does sound like a docs issue. Some Pyspark operations need pandas and you need a uniform Python installation across driver and executor -- either both have it or both don't. I suppose that's always good practice, but not obvious, that it could manifest like this.

How about adding some docs?

Or [~davies] et al is there a better way to guard this? rather than check once whether pandas can be imported, check at ""runtime"" in the createDataFrame method? kind of like {{toPandas}} does? ;;;","06/Jul/15 16:47;davies;To be clear, PySpark does NOT depends on pandas. In dataframe.py, it works with pandas dataframe only when you have it.

[~juliet] example/pi.py should run fine in YARN (it does not need panda at all). Is it possible that `outofstock/data_transform.py` depends on `pandas.algos` (pandas.algos is used by a closure from driver), and you upload the wrong log file?
;;;","06/Jul/15 22:50;juliet;[~davies] Please look at the logs I have attached. The pandas.algo import error only appears in the pi-test.log file. I ran pi-test as a method to help debug this problem at the request of [~vanzin]. If you look at three other log files (with env diferences in the file names) those are from running my out-of-stock job. That job does have quite a few dependencies but I make sure those are available to the driver and workers. 

The real (first) issue that this ticket is related to is that pyspark isn't available on worker nodes. The same command I can use to run my job on spark 1.3 does not work with spark 1.4.;;;","09/Jul/15 13:04;lianhuiwang;[~j_houg] can you add --verbose to spark-submit command? and look at what is your spark.submit.pyArchives.
because from you logs, i find that it do not upload pyArchive files, like:pyspark.zip and py4j-0.8.2.1-src.zip. and you can check whether in SPARK_HOME/python/lib path it has these two zips.;;;","10/Jul/15 22:40;juliet;[~lianhuiwang] I just uploaded the log files from using the verbose flag. I think I may have important clues as to where the problem lies. Instead of using '--master yarn-client' as part of my spark-submit command, I parse my own cli arg in my main class to get the spark master and initialize a configuration with it. If I add --master yarn-client in addition to my normal master specification, the job runs fine.

The following command works in Spark 1.3 but not in 1.4:
    $SPARK_HOME/bin/spark-submit --verbose outofstock/data_transform.py \
    hdfs://foe-dev/DEMO_DATA/FACT_POS     hdfs:/user/juliet/ex4/ yarn-client

If I add the --master yarn-client parameter to the command it works. Specifically:
    $SPARK_HOME/bin/spark-submit --verbose --master yarn-client outofstock/data_transform.py \
    hdfs://foe-dev/DEMO_DATA/FACT_POS     hdfs:/user/juliet/ex4/ yarn-client;;;","13/Jul/15 11:01;lianhuiwang;[~juliet] from your spark1.4-verbose.log, i find that master= local[*]. so maybe in spark-defaults.conf, you config spark.master=local? other situation is in your data_transform.py, maybe you use sparkConf.set(""spark.master"",""local""). Can you check whether these situations have been happened?;;;","13/Jul/15 18:16;vanzin;[~j_houg] could you also run the command with the SPARK_PRINT_LAUNCH_COMMAND=1 env variable set, and post the command logged to stderr?;;;","13/Jul/15 20:02;juliet;[~lianhuiwang] in $SPARK_HOME/conf I only have the spark-defaults.conf.template file, not a non-template version. I also do not set the spark master to local programmatically.

[~vanzin] The command logged to stderr is:
    
Spark Command: /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.65.x86_64/jre/bin/java -cp /home/juliet/bin/spark-1.4.0-bin-hadoop2.6/conf/:/home/juliet/bin/spark-1.4.0-bin-hadoop2.6/lib/spark-assembly-1.4.0-hadoop2.6.0.jar:/home/juliet/bin/spark-1.4.0-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar:/home/juliet/bin/spark-1.4.0-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar:/home/juliet/bin/spark-1.4.0-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms512m -Xmx512m -XX:
MaxPermSize=128m org.apache.spark.deploy.SparkSubmit --verbose outofstock/data_transform.py
hdfs://foe-dev/DEMO_DATA/FACT_POS hdfs:/user/juliet/ex7/ yarn-client

(sorry for the way the classpath gets chopped up between lines.) yarn-client is getting passed as a argument to my code, but because I am not specifying the master via the cli --master flag or via spark-defaults.conf it does not affect how the job initially starts up.;;;","14/Jul/15 02:45;lianhuiwang;[~juliet] can you provide your spark-submit command? 
i think the correct command in spark 1.4 is $SPARK_HOME/bin/spark-submit --master yarn-client outofstock/data_transform.py hdfs://foe-dev/DEMO_DATA/FACT_POS hdfs:/user/juliet/ex4/
is it the same as your command?;;;","15/Jul/15 21:55;vanzin;Hmmm, the command output looks fine, so it seems this was not a regression caused by the launcher library. But let me try it locally and see what I get.;;;","15/Jul/15 23:25;vanzin;[~j_houg], could you share the exact code you're using to instantiate the context?

Here's a script I wrote:

{code}
import sys
from pyspark import SparkContext
SparkContext(master=sys.argv[1]).stop()
{code}

And invoking spark-submit yields the expected results.

{{./bin/spark-submit /tmp/script.py local}} works. 

{{./bin/spark-submit /tmp/script.py foo}} fails because ""foo"" is not a valid master.

So everything seems to be working as expected, which makes me suspicious of your code.;;;","15/Jul/15 23:42;juliet;The failure happens at the point that I need to write out a file on the cluster and pyspark facilities need to be available to executors, not just the driver program. I can parse args and start a spark context fine, it fails at the point that I call sc.saveAsTextFile. Relevant lines:

{panel}
def analyze(data_io):
    sc = data_io.sc()
    sc.addPyFile(""file:/home/juliet/src/out-of-stock/outofstock/GeometricModel.py"")
    keyed_ts_rdd = to_keyed_ts(sc.textFile(data_io.input_path)).cache()
    keyed_days_btwn_sales = keyed_ts_rdd.mapValues(days_between_sales).cache()
    keyed_outliers = keyed_ts_rdd.mapValues(flag_outliers)
    to_csv_lines(keyed_outliers).saveAsTextFile(data_io.sales_outliers_path) # Point of failure
    <Other Stuff>

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description='Analyze store-item sales history for anomolies.')
    parser.add_argument('input_path')
    parser.add_argument('output_dir')
    parser.add_argument('mode')
    args = parser.parse_args()

    dataIO = DataIO(args.input_path, args.output_dir, mode=args.mode)
    analyze(dataIO)
{panel}

This runs fine on Spark 1.3, and produces reasonable results that get written to files in hdfs. I'm pretty confident that my use of argparse and other logic in my code work fine. (Note eddited because of strange jira formatting);;;","15/Jul/15 23:55;vanzin;I think I know what's going on. Since you're not passing the ""--master"" command line argument to spark-submit, SparkSubmit does not know you'll be running the app in yarn mode, so it does not collect information about the pyspark archives to upload. So pyspark modules are not available in the cluster when you run your app.

If you just add ""--master yarn-client"" to your command line, it should work, even if it is redundant. Nevertheless, it would be nice to fix this in the Spark code too.;;;","16/Jul/15 00:02;juliet;Yea, it works fine if I add that arg. There are two reasons I think this should be fixed in Spark, despite there being a work around. First, I think API compatibility should/does include scripts. Second, if Spark provides the ability to set the master via code, it should be respected and actually work. Otherwise, the option that doesn't work (setting master via code) should not be available at all.;;;","16/Jul/15 00:06;vanzin;Changed the title to be more descriptive of the underlying issue.;;;","16/Jul/15 08:13;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/7438;;;","16/Jul/15 08:15;lianhuiwang;yes,  when i use this command: ./bin/spark-submit ./pi.py yarn-client 10,  yarn' client do not upload pyspark.zip, so that can not be worked. i submit a PR that resolve this problem based on master branch.  there is some problems on spark-1.4.0 branch because it finds pyspark libraries in sparkSubmit, not in Client.  if this must be needed in spark-1.4.0, latter i will take a look at it.
;;;","31/Jul/15 10:46;wumin810711;Hi, I got same issue when I running the pyspark program with yarn-client mode and spark 1.4.1 from Biginsight 4.1(Ambari). Because the assembly jar no longer contains the python scripts of pyspark and py4j, so I set the spark home via SparkContext.setSparkHome() to spark-client location(because this is one Ambari hadoop, so the spark-client contains the python folder, and it includes the py4j and pyspark scripts). The API document shows this will be applied to slave nodes, I assume this can be applied for ""spark on yarn"" also, but it does not work.  The worker nodes always get the PYTHONPATH from cached assembly jar. 
After checked the SparkContext code, seems the sparkHome will be set into SparkConf as ""spark.home"", so I think maybe it should be distributed to all executor and pyspark can use this parameter to locate the PYTHONPATH also.;;;"
SparkException thrown due to Executor exceptions should include caller site in stack trace,SPARK-8644,12840648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,25/Jun/15 21:20,12/Aug/15 15:18,14/Jul/23 06:26,12/Aug/15 15:18,1.4.1,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,"Currently when a job fails due to executor (or other) issues, the exception thrown by Spark has a stack trace which stops at the DAGScheduler EventLoop, which makes it hard to trace back to the user code which submitted the job. It should try to include the user submission stack trace.

Example exception today:

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.RuntimeException: uh-oh!
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34$$anonfun$apply$mcJ$sp$1.apply(DAGSchedulerSuite.scala:851)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34$$anonfun$apply$mcJ$sp$1.apply(DAGSchedulerSuite.scala:851)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1637)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1095)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1095)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1285)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1276)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1275)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1275)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:749)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1486)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
{code}

Here is the part I want to include:

{code}
	at org.apache.spark.rdd.RDD.count(RDD.scala:1095)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34.apply$mcJ$sp(DAGSchedulerSuite.scala:851)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34.apply(DAGSchedulerSuite.scala:851)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34.apply(DAGSchedulerSuite.scala:851)
	at org.scalatest.Assertions$class.intercept(Assertions.scala:997)
	at org.scalatest.FunSuite.intercept(FunSuite.scala:1555)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33.apply$mcV$sp(DAGSchedulerSuite.scala:850)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33.apply(DAGSchedulerSuite.scala:849)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33.apply(DAGSchedulerSuite.scala:849)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
{code}

Observe how much more useful the second one is for knowing what started the job.",,apachespark,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8625,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 01:33:24 UTC 2015,,,,,,,,,,"0|i2gid3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/15 21:32;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/7028;;;","26/Jun/15 07:54;srowen;This seems related to SPARK-8625 which asks to return the whole exception. Would that subsume this?;;;","17/Jul/15 01:33;ilikerps;These are actually complementary. SPARK-8625 makes sure the Executor exception is properly preserved, but that exception may not include user code either (or may not help indicate which driver code caused the job). Now we will properly include all relevant stacks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Packages argument is wrong in sparkR.init,SPARK-8637,12840592,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,shivaram,shivaram,shivaram,25/Jun/15 17:26,26/Jun/15 16:11,14/Jul/23 06:26,25/Jun/15 17:56,1.4.2,,,,,,1.4.1,1.5.0,,,,,SparkR,,,,0,,,,,,This was a bug introduced in https://github.com/apache/spark/pull/6928 and affects branch-1.4 and master branch,,amar.hgce@gmail.com,apachespark,Elie A.,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 16:11:03 UTC 2015,,,,,,,,,,"0|i2gi13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/15 17:33;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/7022;;;","25/Jun/15 17:56;shivaram;Issue resolved by pull request 7022
[https://github.com/apache/spark/pull/7022];;;","26/Jun/15 13:04;amar.hgce@gmail.com;Hi Shivaram,

I was looking for packages option in current release(11 june).

Actaully, I was trying create.df from csv file in Rstudio, but sparkR.init() was not containing packages argument so I can initialize spark context by supplying com.databricks:spark-csv_2.10:1.3.0 package to read csv file

Then I changed code in client.R and sparkR.R to set additional parameter packages and it worked. I thought that I can contribute in this regards in sparkR package. but when I landed to JIRA, it was the first issue that you just resolved. I cross checked my code with current resolution it was almost same except new method for combining all argument that you created

I would like to contribute in sparkR package related to new development as well as into bug resolution. It would be great,if I can contribute anyhow to sparkR package.

Regards,
Amar ;;;","26/Jun/15 16:11;shivaram;Thanks [~amar.hgce@gmail.com] for your interest in contributing to SparkR. You can contribute to the project by picking up issues that are already open in the JIRA. (examples https://issues.apache.org/jira/browse/SPARK-7714 https://issues.apache.org/jira/browse/SPARK-8082)

In general you can check out https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark for more details on how to contribute to the Spark project.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
CaseKeyWhen has incorrect NULL handling,SPARK-8636,12840582,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vinodkc,smolav,smolav,25/Jun/15 16:09,13/Jul/15 19:52,14/Jul/23 06:26,13/Jul/15 19:52,1.4.0,,,,,,1.5.0,,,,,,SQL,,,,0,starter,,,,,"CaseKeyWhen implementation in Spark uses the following equals implementation:

{code}
  private def equalNullSafe(l: Any, r: Any) = {
    if (l == null && r == null) {
      true
    } else if (l == null || r == null) {
      false
    } else {
      l == r
    }
  }
{code}

Which is not correct, since in SQL, NULL is never equal to NULL (actually, it is not unequal either). In this case, a NULL value in a CASE WHEN expression should never match.

For example, you can execute this in MySQL:

{code}
SELECT CASE NULL WHEN NULL THEN ""NULL MATCHES"" ELSE ""NULL DOES NOT MATCH"" END FROM DUAL;
{code}

And the result will be ""NULL DOES NOT MATCH"".",,animeshbaranawal,apachespark,davies,marmbrus,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3813,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 13 19:52:16 UTC 2015,,,,,,,,,,"0|i2ghyv:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.5.0,,,,,,,,,,,,,"26/Jun/15 05:13;animeshbaranawal;So the condition should be :
if (l == null || r == null) false
else l == r ?;;;","26/Jun/15 07:40;smolav;[~animeshbaranawal] Yes, I think so.;;;","26/Jun/15 11:20;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/7040;;;","26/Jun/15 12:01;apachespark;User 'animeshbaranawal' has created a pull request for this issue:
https://github.com/apache/spark/pull/7044;;;","28/Jun/15 15:19;davies;[~animeshbaranawal] What happen if there is null in the grouping key? Does a row with null equal to another row with null?;;;","29/Jun/15 04:49;animeshbaranawal;No a row with null will not be equal to another row with null if we follow the modification. What do you say [~smolav] ?;;;","29/Jun/15 12:48;smolav;[~davies], [~animeshbaranawal] In SQL, NULL is never equal to NULL. Any comparison to NULL is UNKNOWN. Most SQL implementations represent UNKNOWN as NULL, too.;;;","03/Jul/15 22:43;apachespark;User 'animeshbaranawal' has created a pull request for this issue:
https://github.com/apache/spark/pull/7044;;;","04/Jul/15 22:40;davies;[~smolav] I'm just curious that how can we sort of group by a row with NULL in it, If we can not compare NULL with NULL?;;;","06/Jul/15 07:00;smolav;[~davies] NULL values are grouped together when using a GROUP BY clause.

See https://en.wikipedia.org/wiki/Null_%28SQL%29#When_two_nulls_are_equal:_grouping.2C_sorting.2C_and_some_set_operations

{quote}
Because SQL:2003 defines all Null markers as being unequal to one another, a special definition was required in order to group Nulls together when performing certain operations. SQL defines ""any two values that are equal to one another, or any two Nulls"", as ""not distinct"". This definition of not distinct allows SQL to group and sort Nulls when the GROUP BY clause (and other keywords that perform grouping) are used.
{quote};;;","13/Jul/15 19:52;marmbrus;Issue resolved by pull request 7040
[https://github.com/apache/spark/pull/7040];;;",,,,,,,,,,,,,,,,,,
Poor Python UDF performance because of RDD caching,SPARK-8632,12840559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,justin.uang,justin.uang,25/Jun/15 14:46,09/Apr/16 20:37,14/Jul/23 06:26,22/Sep/15 21:23,1.4.0,,,,,,1.5.1,1.6.0,,,,,PySpark,SQL,,,0,,,,,,"{quote}
We have been running into performance problems using Python UDFs with DataFrames at large scale.

From the implementation of BatchPythonEvaluation, it looks like the goal was to reuse the PythonRDD code. It caches the entire child RDD so that it can do two passes over the data. One to give to the PythonRDD, then one to join the python lambda results with the original row (which may have java objects that should be passed through).

In addition, it caches all the columns, even the ones that don't need to be processed by the Python UDF. In the cases I was working with, I had a 500 column table, and i wanted to use a python UDF for one column, and it ended up caching all 500 columns. 
{quote}

http://apache-spark-developers-list.1001551.n3.nabble.com/Python-UDF-performance-at-large-scale-td12843.html",,apachespark,bijay697,davies,joshrosen,kalle,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10685,SPARK-10494,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 09 20:37:00 UTC 2016,,,,,,,,,,"0|i2ghtr:",9223372036854775807,,,,,davies,,,,,,,,,1.5.1,1.6.0,,,,,,,,,,,,"25/Jun/15 14:49;justin.uang;[~davies], my current plan is to switch to a synchronous model so that we can avoid deadlock. From a quick benchmark on my machine of loading pickled data and converting it to a python object, 95% of time is spent on cPickle and 5% on IO. I think the performance drawbacks of a synchronous model are trivial enough that the conceptual simplicity is worth it.;;;","02/Jul/15 07:28;davies;[~justin.uang] Sounds interesting, could you sending out the PR?;;;","02/Jul/15 22:08;justin.uang;Haven't gotten around to it yet. I'll let you know when I can find time to work on it!

Right now, I'm thinking about creating a separate code path for sql udfs, since I realized that the current system with two threads is necessary because the RDD interface is from Iterator -> Iterator. Any type of synchronous batching won't work with RDDs that change the length of the output iterator.;;;","05/Sep/15 08:46;rxin;We don't technically need to cache the RDD at all, do we? Can't we just create batches of rows, pass them to Python, run Python, get the result back, and then add the new column to the batch?

Maybe this would be easier if the entire engine is row-batch based.;;;","07/Sep/15 15:56;justin.uang;Yea, I think that's the best solution for udfs, since the number of input rows and output rows are the same per batch. So do you think we should create a separate code path that uses this row-batch based engine specifically for UDFs? It would also be nice because then we could switch to some language agnostic data format like avro or protobufs, and then allow all language bindings to support UDFs the same way.;;;","07/Sep/15 19:05;justin.uang;I have started working on this. I hope to get a draft out soon.;;;","08/Sep/15 02:53;justin.uang;I have a solution working on my computer. I'm going to clean it up then send it for CR soon.;;;","08/Sep/15 18:53;davies;[~rxin] As [~justin.uang] suggested before, the batch mode will need to flush the rows in every place of the pipeline, or it get deadlock.

I think the goal is to call upstream once and improve the throughput of Python UDF (which is usually the bottleneck). The batch mode is increase the overhead of Python UDF (for each batch), cause worser performance. The problem of older cache is that serialization and memory management (also not purged after used) overhead. With one time (purged after visited) tungsten cache (and spilling), the overhead should be not that high, I think this should be the most performant and stable approach.;;;","08/Sep/15 18:58;justin.uang;Davies, what do you mean by upstream? I didn't quite understand what you
meant.

I have implemented a batch based system that is synchronous instead, so it
doesn't risk deadlock. (There isn't a writer and reader thread anymore)

;;;","08/Sep/15 21:24;davies;The upstream means child of current SparkPlan, could have other Python UDFs. 

We remove the RDD cache in 1.4, then the upstream will be evaluated twice. If you have multiple Python UDFs, for example three, it will end up evaluate the child 8 times (2 x 2 x 2), which will be really slow or cause OOM.

In synchronous batch mode, what's the batch size? if it's small, the overhead of each batch will be high, if it's too large, it's easy to OOM if you have many columns. Also we need to copy the rows (serialization is not need if it's UnsafeRow).

;;;","08/Sep/15 22:32;justin.uang;I set the batch mode to be 100, which is the same as before (things will oom under the same conditions). We can be more clever in the future (add size estimation) or something similar, but I have a feeling that the overhead of batching isn't too great.;;;","09/Sep/15 02:31;apachespark;User 'justinuang' has created a pull request for this issue:
https://github.com/apache/spark/pull/8662;;;","09/Sep/15 02:32;justin.uang;Just pushed, any comments would be much appreciated. I didn't want to move around and refactor too many things, in order to keep this commit from getting massive.;;;","19/Sep/15 05:47;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/8833;;;","20/Sep/15 01:27;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/8835;;;","22/Sep/15 21:23;joshrosen;Issue resolved by pull request 8835
[https://github.com/apache/spark/pull/8835];;;","24/Mar/16 04:43;bijay697;I am still having this issue in Spark 1.6.0 on EMR. The job fails with OOM. I have DataFrame with 250 columns and I am applying UDF on more than 50 of the columns. I am registering the DataFrame as temptable and  applying the UDF in hive_context sql statement. I am applying the UDF after sort merge join of two DataFrame (each of around 4GB) and multiple broadcast joins of 22 Dim table.

Below is how I am applying the UDF.

{code:borderStyle=solid}
data_frame.registerTempTable(""temp_table"")
new_df = hive_context.sql(""select python_udf(column_1),python_udf(column_2), ... , from temp_table"")
{code};;;","08/Apr/16 03:57;davies;[~bijay697] Python UDFs had been improved a lot recently in master, see https://issues.apache.org/jira/browse/SPARK-14267 and https://issues.apache.org/jira/browse/SPARK-14215.

Could you try master ?;;;","09/Apr/16 20:37;bijay697;I'll try and update.;;;",,,,,,,,,,
Race condition in AbstractSparkSQLParser.parse,SPARK-8628,12840505,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vinodkc,smolav,smolav,25/Jun/15 11:12,02/Jul/15 18:48,14/Jul/23 06:26,30/Jun/15 19:25,1.3.0,1.3.1,1.4.0,,,,1.4.1,1.5.0,,,,,SQL,,,,0,regression,,,,,"SPARK-5009 introduced the following code in AbstractSparkSQLParser:

{code}
def parse(input: String): LogicalPlan = {
    // Initialize the Keywords.
    lexical.initialize(reservedWords)
    phrase(start)(new lexical.Scanner(input)) match {
      case Success(plan, _) => plan
      case failureOrError => sys.error(failureOrError.toString)
    }
  }
{code}

The corresponding initialize method in SqlLexical is not thread-safe:

{code}
  /* This is a work around to support the lazy setting */
  def initialize(keywords: Seq[String]): Unit = {
    reserved.clear()
    reserved ++= keywords
  }
{code}

I'm hitting this when parsing multiple SQL queries concurrently. When one query parsing starts, it empties the reserved keyword list, then a race-condition occurs and other queries fail to parse because they recognize keywords as identifiers.",,apachespark,marmbrus,sarutak,smolav,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 02:45:44 UTC 2015,,,,,,,,,,"0|i2ghif:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.3.2,1.4.1,1.5.0,,,,,,,,,,,"25/Jun/15 11:13;smolav;Here is an example of failure with Spark 1.4.0:

{code}
[1.152] failure: ``union'' expected but identifier OR found

SELECT CASE a+1 WHEN b THEN 111 WHEN c THEN 222 WHEN d THEN 333 WHEN e THEN 444 ELSE 555 END, a-b, a FROM t1 WHERE e+d BETWEEN a+b-10 AND c+130 OR a>b OR d>e
                                                                                                                                                       ^
java.lang.RuntimeException: [1.152] failure: ``union'' expected but identifier OR found

SELECT CASE a+1 WHEN b THEN 111 WHEN c THEN 222 WHEN d THEN 333 WHEN e THEN 444 ELSE 555 END, a-b, a FROM t1 WHERE e+d BETWEEN a+b-10 AND c+130 OR a>b OR d>e
                                                                                                                                                       ^
	at scala.sys.package$.error(package.scala:27)
{code};;;","25/Jun/15 12:04;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/7015;;;","30/Jun/15 19:25;marmbrus;Issue resolved by pull request 7015
[https://github.com/apache/spark/pull/7015];;;","01/Jul/15 02:45;vinodkc;Can you please assign this to me;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop RDDs fail to properly serialize configuration,SPARK-8623,12840463,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,bolke,bolke,25/Jun/15 07:50,27/Jun/15 21:33,14/Jul/23 06:26,27/Jun/15 21:33,1.5.0,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,"The following query was executed using ""spark-sql --master yarn-client"" on 1.5.0-SNAPSHOT:

select * from wcs.geolite_city limit 10;

This lead to the following error:

15/06/25 09:38:37 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, lxhnl008.ad.ing.net): java.lang.NullPointerException
	at org.apache.hadoop.conf.Configuration.<init>(Configuration.java:693)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:442)
	at org.apache.hadoop.mapreduce.Job.<init>(Job.java:131)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD.getJob(SqlNewHadoopRDD.scala:83)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD.getConf(SqlNewHadoopRDD.scala:89)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD$$anon$1.<init>(SqlNewHadoopRDD.scala:127)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:124)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)

This does not happen in every case, ie. some queries execute fine, and it is unclear why.

Using just ""spark-sql"" the query executes fine as well and thus the issue seems to rely in the communication with Yarn. Also the query executes fine (with yarn) in spark-shell.","Hadoop 2.6, Kerberos",apachespark,bolke,joshrosen,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 27 21:33:53 UTC 2015,,,,,,,,,,"0|i2gh93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/15 16:31;joshrosen;[~sandyr], do you think that this could possibly be related to our Configuration serialization change?;;;","25/Jun/15 17:31;sandyr;Looking into it;;;","25/Jun/15 18:07;sandyr;I took a look at the line numbers and it seems like this can occur when a null Configuration object is passed in to the constructor of a Hadoop Configuration:
https://github.com/apache/hadoop/blob/branch-2.6/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java#L693

It's puzzling to me how this could occur.  The configuration that's passed in comes from a SerializableConfiguration, but the first line of SerializableConfiguration's readObject method instantiates a Configuration object, so not sure how that could end up null.  I'm pretty sure the broadcasted conf is always non-null as well.;;;","26/Jun/15 06:51;joshrosen;There's another report of this issue at https://github.com/apache/spark/pull/6679#issuecomment-115546773;;;","26/Jun/15 17:23;sandyr;Am able to reproduce this locally.  Looking into the cause.;;;","26/Jun/15 22:14;sandyr;Figured out the issue - my patch omitted registering a custom Kryo serializer for SerializableConfiguration, so it gets serialized and deserialized using Kryo, which means the writeObject and readObject methods are never called, which means the internal Configuration is never instantiated.  Uploading a patch that I've found to fix the issue.;;;","26/Jun/15 22:23;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/7050;;;","27/Jun/15 14:40;bolke;Thanks Sandy, I applied the patch and error is gone for the simple query above. Did not test any further yet;;;","27/Jun/15 21:31;joshrosen;Thanks for confirming that Sandy's patch fixes the issue. I'll merge his fix now.;;;","27/Jun/15 21:33;joshrosen;Issue resolved by pull request 7050
[https://github.com/apache/spark/pull/7050];;;",,,,,,,,,,,,,,,,,,,
Can't find the keytab file when recovering the streaming application.,SPARK-8619,12840423,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,carlmartin,carlmartin,carlmartin,25/Jun/15 03:32,04/Jul/15 07:27,14/Jul/23 06:26,30/Jun/15 18:46,1.4.0,,,,,,1.4.1,1.5.0,,,,,DStreams,,,,0,,,,,,"In a streaming application, I use *--keytab /root/spark.keytab* to get the token.
But when the streaming application failed and I wanted to recover it from checkpoint file, there was an error:
{quote}
java.io.IOException: Login failure for spark/hadoop.hadoop.com@HADOOP.COM from keytab spark.keytab-1fd8f7bb-0d3c-4f65-990a-9ae09055cc8d: javax.security.auth.login.LoginException: Unable to obtain password from user
{quote}

Spark had changed the configuration, so the checkpoint can't find the file:
{code:title=Client.java @ Function: setupCredentials |borderStyle=solid}
      val keytabFileName = f.getName + ""-"" + UUID.randomUUID().toString
      UserGroupInformation.loginUserFromKeytab(args.principal, args.keytab)
      loginFromKeytab = true
      sparkConf.set(""spark.yarn.keytab"", keytabFileName)
{code}

So when recovering the application, we should ignore this configurations.",,apachespark,carlmartin,hshreedharan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 16:43:40 UTC 2015,,,,,,,,,,"0|i2gh0f:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.4.2,,,,,,,,,,,,"25/Jun/15 03:43;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/7008;;;","25/Jun/15 16:43;hshreedharan;Targeting 1.4.1 in case another RC is rolled.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR - Third party jars are not being added to classpath in SparkRBackend,SPARK-8607,12840364,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,cafreeman,cafreeman,cafreeman,24/Jun/15 22:36,29/Dec/15 09:10,14/Jul/23 06:26,27/Jun/15 00:06,1.4.0,,,,,,1.4.1,1.5.0,,,,,SparkR,,,,0,,,,,,"Getting a ClassNotFound exception when using the --jars flag in the SparkR shell, as well as when creating a sparkContext with sparkR.init.

Related to https://issues.apache.org/jira/browse/SPARK-5185",,apachespark,cafreeman,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 27 00:06:34 UTC 2015,,,,,,,,,,"0|i2ggnr:",9223372036854775807,,,,,cafreeman,,,,,,,,,1.4.0,,,,,,,,,,,,,"24/Jun/15 22:47;cafreeman;PR open at https://github.com/apache/spark/pull/7001;;;","24/Jun/15 22:48;apachespark;User 'cafreeman' has created a pull request for this issue:
https://github.com/apache/spark/pull/7001;;;","27/Jun/15 00:06;shivaram;Issue resolved by pull request 7001
[https://github.com/apache/spark/pull/7001];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions in RDD.getPreferredLocations() and getPartitions() should not be able to crash DAGScheduler,SPARK-8606,12840358,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,24/Jun/15 22:16,17/May/20 17:46,14/Jul/23 06:26,27/Jun/15 21:55,1.2.0,1.3.0,1.4.0,1.5.0,,,1.3.2,1.4.1,1.5.0,,,,Scheduler,Spark Core,,,0,,,,,,"RDD.getPreferredLocations() and RDD.getPartitions() may throw exceptions but the DAGScheduler does not guard against this, leaving it vulnerable to crashing and stopping the SparkContext if exceptions occur there.

We should fix this by adding more try blocks around these calls in DAGScheduler.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 27 21:55:00 UTC 2015,,,,,,,,,,"0|i2ggmf:",9223372036854775807,,,,,,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"24/Jun/15 23:40;joshrosen;An example stacktrace exhibiting this bug:

{code}
DAGSchedulerEventProcessLoop: DAGSchedulerEventProcessLoop failed; shutting down SparkContext
org.apache.spark.SparkException: Attempted to use BlockRDD[3021] at createStream at <console>:69 after its blocks have been removed!
	at org.apache.spark.rdd.BlockRDD.assertValid(BlockRDD.scala:83)
	at org.apache.spark.rdd.BlockRDD.getPreferredLocations(BlockRDD.scala:56)
	at org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2.apply(RDD.scala:231)
	at org.apache.spark.rdd.RDD$$anonfun$preferredLocations$2.apply(RDD.scala:231)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.preferredLocations(RDD.scala:230)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(DAGScheduler.scala:1380)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply$mcVI$sp(DAGScheduler.scala:1390)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1389)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1389)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1389)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1387)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(DAGScheduler.scala:1387)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply$mcVI$sp(DAGScheduler.scala:1390)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1389)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2$$anonfun$apply$2.apply(DAGScheduler.scala:1389)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1389)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal$2.apply(DAGScheduler.scala:1387)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal(DAGScheduler.scala:1387)
	at org.apache.spark.scheduler.DAGScheduler.getPreferredLocs(DAGScheduler.scala:1354)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$15.apply(DAGScheduler.scala:892)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$15.apply(DAGScheduler.scala:891)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:891)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:815)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:818)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:817)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:817)
	at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code};;;","25/Jun/15 17:37;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7023;;;","27/Jun/15 21:55;joshrosen;Issue resolved by pull request 7023
[https://github.com/apache/spark/pull/7023];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet data source doesn't write summary file while doing appending,SPARK-8604,12840332,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,24/Jun/15 21:05,25/Jun/15 07:07,14/Jul/23 06:26,25/Jun/15 07:07,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"Currently, Parquet and ORC data sources don't set their output format class, as we override the output committer in Spark SQL. However, SPARK-8678 ignores user defined output committer class while doing appending to avoid potential issues brought by direct output committers (e.g. {{DirectParquetOutputCommitter}}). This makes both of these data sources fallback to the default output committer retrieved from {{TextOutputFormat}}, which is {{FileOutputCommitter}}. For ORC, it's totally fine since ORC itself just uses {{FileOutputCommitter}}. But for Parquet, {{ParquetOutputCommitter}} also writes the summary files while committing the job.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 07:07:51 UTC 2015,,,,,,,,,,"0|i2gggn:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"24/Jun/15 21:33;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6998;;;","25/Jun/15 07:07;lian cheng;Issue resolved by pull request 6998
[https://github.com/apache/spark/pull/6998];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"In Windows,Not able to create a Spark context from R studio ",SPARK-8603,12840331,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,Prakashpc,Prakashpc,24/Jun/15 20:58,12/Dec/22 17:34,14/Jul/23 06:26,27/May/16 03:55,1.4.0,,,,,,2.0.0,,,,,,SparkR,,,,0,,,,,,"In windows ,creation of spark context fails using below code from R studio

Sys.setenv(SPARK_HOME=""C:\\spark\\spark-1.4.0"")
.libPaths(c(file.path(Sys.getenv(""SPARK_HOME""), ""R"", ""lib""), .libPaths()))
library(SparkR)
sc <- sparkR.init(master=""spark://localhost:7077"", appName=""SparkR"")

Error: JVM is not ready after 10 seconds

Reason: Wrong file path computed in client.R. File seperator for windows[""\""] is not respected by ""file.Path"" function by default.
","Windows, R studio",apachespark,dsiegel,Emaasit,felixcheung,Prakashpc,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,30,30,,0%,30,30,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 27 03:55:25 UTC 2016,,,,,,,,,,"0|i2gggf:",9223372036854775807,,,,,shivaram,,,,,,,,,,,,,,,,,,,,,,"24/Jun/15 21:11;shivaram;Could you add some details on how you downloaded / installed Spark 1.4 ?;;;","25/Jun/15 02:27;Prakashpc;1) Downloaded the spark pre-build  version(1.4.0)
2) Set enviroment PATH  variable for R and R tools(3.2)  
3) run mvn install -Phadoop-2.4 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -Pyarn -Psparkr  from spark folder  
4) Start the spark master and worker from spark home directory using below command
    bin\spark-class.cmd org.apache.spark.deploy.worker.Worker spark://localhost:7077
    bin\spark-class.cmd org.apache.spark.deploy.master.Master -h localhost --webui-port 8085
5) Open R studio and run below code
      Sys.setenv(SPARK_HOME=""C:\\sparkspark-1.4.0"")
     .libPaths(c(file.path(Sys.getenv(""SPARK_HOME""), ""R"", ""lib""), .libPaths()))  
     .library(SparkR)
     sc <- sparkR.init(master=""spark://localhost:7077"", appName=""SparkR"");;;","25/Jun/15 07:51;apachespark;User 'prakashpc' has created a pull request for this issue:
https://github.com/apache/spark/pull/7012;;;","25/Jun/15 16:39;shivaram;So a few points about the steps

1. If you download a pre-built version you don't need to recompile Spark i.e. you don't need to run mvn install etc. Also just curious: What tool did you use to unzip the tar.gz file on Windows ?

2. You don't need to start the spark master, spark worker to try out Spark on a single machine -- You can just set the master to ""local"" and that doesn't need separate binaries for Master, Worker.

3. I have seen a similar problem before, but not because of the file separator but because the spark-submit.cmd file was not marked as executable. This happened because unzipping a tar.gz didn't set permissions correctly (This relates to point (1)).;;;","25/Jun/15 17:43;Prakashpc;1) used winrar to unzip the file. Running the process as admin and admin has full control on these executable[in the bin folder].

Setting the master to local works fine.The issue is occurring in standalone cluster mode as described in my previous comment. This works fine after i changed the file separator.;;;","25/Jun/15 18:34;apachespark;User 'prakashpc' has created a pull request for this issue:
https://github.com/apache/spark/pull/7025;;;","28/Jun/15 10:17;srowen;[~Prakashpc] Before opening a JIRA, you need to read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark  You do not set Fix or Target version, for example;;;","18/May/16 08:10;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/13165;;;","27/May/16 03:55;shivaram;Issue resolved by pull request 13165
[https://github.com/apache/spark/pull/13165];;;",,,,,,,,,,,,,,,,,,,,
History Server doesn't show complete application when one attempt inprogress,SPARK-8593,12840237,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rekhajoshm,tgraves,tgraves,24/Jun/15 16:01,17/Jul/15 21:47,14/Jul/23 06:26,17/Jul/15 21:47,1.4.0,,,,,,1.4.2,1.5.0,,,,,YARN,,,,0,,,,,,"The Spark history server doesn't show an application if the first attempt of the application is still inprogress.  

Here are the files in hdfs:
-rwxrwx---   3 tgraves hdfs        234 2015-06-24 15:49 sparkhistory/application_1433751980223_18926_1.inprogress
-rwxrwx---   3 tgraves hdfs    9609450 2015-06-24 15:51 sparkhistory/application_1433751980223_18926_2


The UI shows them if I set the showIncomplete=true.

Removing the inprogress file allows it to show up when showIncomplete is false.

It should be smart enough to atleast show the second successful attempt.",,apachespark,devaraj,rekhajoshm,tgraves,xietingwen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8594,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 21:47:40 UTC 2015,,,,,,,,,,"0|i2gfvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/15 05:44;rekhajoshm;looking into it.thanks;;;","07/Jul/15 05:52;apachespark;User 'rekhajoshm' has created a pull request for this issue:
https://github.com/apache/spark/pull/7253;;;","17/Jul/15 21:47;srowen;Issue resolved by pull request 7253
[https://github.com/apache/spark/pull/7253];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CoarseGrainedExecutorBackend: Cannot register with driver => NPE,SPARK-8592,12840228,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,xuchenCN,sjoerdmulder,sjoerdmulder,24/Jun/15 15:11,11/Feb/16 19:33,14/Jul/23 06:26,30/Jun/15 17:06,1.4.0,,,,,,1.5.0,,,,,,Scheduler,Spark Core,,,0,,,,,,"I cannot reproduce this consistently but when submitting jobs just after another finished it will not come up:

{code}
15/06/24 14:57:24 INFO WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@10.0.7.171:39135/user/Worker
15/06/24 14:57:24 INFO WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@10.0.7.171:39135/user/Worker
15/06/24 14:57:24 ERROR CoarseGrainedExecutorBackend: Cannot register with driver: akka.tcp://sparkDriver@172.17.0.109:47462/user/CoarseGrainedScheduler
java.lang.NullPointerException
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.actorRef$lzycompute(AkkaRpcEnv.scala:273)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.actorRef(AkkaRpcEnv.scala:273)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.toString(AkkaRpcEnv.scala:313)
	at java.lang.String.valueOf(String.java:2982)
	at scala.collection.mutable.StringBuilder.append(StringBuilder.scala:200)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3.apply(CoarseGrainedSchedulerBackend.scala:125)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3.apply(CoarseGrainedSchedulerBackend.scala:125)
	at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.logInfo(CoarseGrainedSchedulerBackend.scala:69)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:125)
	at org.apache.spark.rpc.akka.AkkaRpcEnv.org$apache$spark$rpc$akka$AkkaRpcEnv$$processMessage(AkkaRpcEnv.scala:178)
	at org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1$$anonfun$receiveWithLogging$1$$anonfun$applyOrElse$4.apply$mcV$sp(AkkaRpcEnv.scala:127)
	at org.apache.spark.rpc.akka.AkkaRpcEnv.org$apache$spark$rpc$akka$AkkaRpcEnv$$safelyCall(AkkaRpcEnv.scala:198)
	at org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1$$anonfun$receiveWithLogging$1.applyOrElse(AkkaRpcEnv.scala:126)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:59)
	at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:42)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at org.apache.spark.util.ActorLogReceive$$anon$1.applyOrElse(ActorLogReceive.scala:42)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1.aroundReceive(AkkaRpcEnv.scala:93)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}","Ubuntu 14.04, Scala 2.11, Java 8, ",apachespark,emlyn,joshrosen,nezihyigitbasi,sjoerdmulder,xuchenCN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 11 19:33:02 UTC 2016,,,,,,,,,,"0|i2gftb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/15 03:38;xuchenCN;Seems toString cause NPE ;;;","29/Jun/15 06:06;apachespark;User 'xuchenCN' has created a pull request for this issue:
https://github.com/apache/spark/pull/7077;;;","30/Jun/15 03:33;apachespark;User 'xuchenCN' has created a pull request for this issue:
https://github.com/apache/spark/pull/7110;;;","30/Jun/15 17:06;joshrosen;Issue resolved by pull request 7110
[https://github.com/apache/spark/pull/7110];;;","11/Feb/16 19:32;nezihyigitbasi;We still see this problem with 1.5.2

{code}
16/02/11 07:55:29 ERROR executor.CoarseGrainedExecutorBackend: Cannot register with driver: akka.tcp://sparkDriver@10.148.8.235:55443/user/CoarseGrainedScheduler
java.lang.NullPointerException
        at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.actorRef$lzycompute(AkkaRpcEnv.scala:281)
        at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.actorRef(AkkaRpcEnv.scala:281)
        at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.toString(AkkaRpcEnv.scala:322)
        at java.lang.String.valueOf(String.java:2982)
        at scala.collection.mutable.StringBuilder.append(StringBuilder.scala:197)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3.apply(CoarseGrainedSchedulerBackend.scala:138)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3.apply(CoarseGrainedSchedulerBackend.scala:138)
        at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.logInfo(CoarseGrainedSchedulerBackend.scala:76)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:138)
        at org.apache.spark.rpc.akka.AkkaRpcEnv.org$apache$spark$rpc$akka$AkkaRpcEnv$$processMessage(AkkaRpcEnv.scala:177)
        at org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1$$anonfun$receiveWithLogging$1$$anonfun$applyOrElse$4.apply$mcV$sp(AkkaRpcEnv.scala:126)
        at org.apache.spark.rpc.akka.AkkaRpcEnv.org$apache$spark$rpc$akka$AkkaRpcEnv$$safelyCall(AkkaRpcEnv.scala:197)
        at org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1$$anonfun$receiveWithLogging$1.applyOrElse(AkkaRpcEnv.scala:125)
        at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
        at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
        at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
        at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:59)
        at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:42)
        at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:118)
        at org.apache.spark.util.ActorLogReceive$$anon$1.applyOrElse(ActorLogReceive.scala:42)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:467)
        at org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1.aroundReceive(AkkaRpcEnv.scala:92)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
        at akka.dispatch.Mailbox.run(Mailbox.scala:220)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:397)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code};;;","11/Feb/16 19:33;nezihyigitbasi;Any ideas [~joshrosen]?;;;",,,,,,,,,,,,,,,,,,,,,,,
Could not use concat with UDF in where clause,SPARK-8588,12840057,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,cloud_fan,stanzhai,stanzhai,24/Jun/15 07:25,06/Jul/15 23:54,14/Jul/23 06:26,06/Jul/15 23:54,1.4.0,,,,,,,,,,,,SQL,,,,0,,,,,,"After upgraded the cluster from spark 1.3.1 to 1.4.0(rc4), I encountered the following exception when use concat with UDF in where clause: 

{code}
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'concat(HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFYear(date#1776),年) 
        at org.apache.spark.sql.catalyst.analysis.UnresolvedFunction.dataType(unresolved.scala:82) 
        at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$$anonfun$apply$5$$anonfun$applyOrElse$15.apply(HiveTypeCoercion.scala:299) 
        at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$$anonfun$apply$5$$anonfun$applyOrElse$15.apply(HiveTypeCoercion.scala:299) 
        at scala.collection.LinearSeqOptimized$class.exists(LinearSeqOptimized.scala:80) 
        at scala.collection.immutable.List.exists(List.scala:84) 
        at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$$anonfun$apply$5.applyOrElse(HiveTypeCoercion.scala:299) 
        at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$$anonfun$apply$5.applyOrElse(HiveTypeCoercion.scala:298) 
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222) 
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222) 
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51) 
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221) 
        at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:75) 
        at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:85) 
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
        at scala.collection.Iterator$class.foreach(Iterator.scala:727) 
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) 
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) 
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103) 
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47) 
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273) 
        at scala.collection.AbstractIterator.to(Iterator.scala:1157) 
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265) 
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157) 
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252) 
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) 
        at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDown(QueryPlan.scala:94) 
        at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:64) 
        at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:136) 
        at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformAllExpressions$1.applyOrElse(QueryPlan.scala:135) 
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222) 
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222) 
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51) 
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221) 
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242) 
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
        at scala.collection.Iterator$class.foreach(Iterator.scala:727) 
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) 
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) 
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103) 
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47) 
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273) 
        at scala.collection.AbstractIterator.to(Iterator.scala:1157) 
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265) 
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157) 
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252) 
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) 
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:272) 
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:227) 
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242) 
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) 
        at scala.collection.Iterator$class.foreach(Iterator.scala:727) 
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) 
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) 
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103) 
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47) 
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273) 
        at scala.collection.AbstractIterator.to(Iterator.scala:1157) 
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265) 
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157) 
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252) 
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) 
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:272) 
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:227) 
        at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:212) 
        at org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressions(QueryPlan.scala:135) 
        at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$.apply(HiveTypeCoercion.scala:298) 
        at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$InConversion$.apply(HiveTypeCoercion.scala:297) 
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:61) 
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:59) 
        at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111) 
        at scala.collection.immutable.List.foldLeft(List.scala:84) 
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:59) 
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:51) 
        at scala.collection.immutable.List.foreach(List.scala:318) 
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:51) 
        at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:922) 
        at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:922) 
        at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:920) 
        at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131) 
        at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51) 
        at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:744) 
        at test.service.SparkHiveService.query(SparkHiveService.scala:79) 
        ... 
        at java.lang.Thread.run(Thread.java:745) 
{code}

The SQL is: 
{quote}
select * from test where concat(year(date), '年') in ( '2015年', '2014年' ) limit 10 {quote}

This SQL can be run in spark 1.3.1 but error in spark 1.4. I've tried run some similar sql in spark 1.4.0, found the following sql could be run correctly: 

select * from test where concat(year(date), '年') = '2015年' limit 10 
select * from test where concat(sex, 'T') in ( 'MT' ) limit 10 

In short, when I use 'concat', UDF and 'in' together in sql, I will get the exception:  Invalid call to dataType on unresolved object. ","Centos 7, java 1.7.0_67, scala 2.10.5, run in a spark standalone cluster(or local).",apachespark,cloud_fan,stanzhai,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 23:26:05 UTC 2015,,,,,,,,,,"0|i2gf67:",9223372036854775807,,,,,,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"26/Jun/15 05:01;cloud_fan;cc [~marmbrus] this issue has already been fixed by https://github.com/apache/spark/pull/6145.;;;","29/Jun/15 23:26;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/7103;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Better exception message if invalid checkpoint dir is specified,SPARK-8584,12840013,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,24/Jun/15 01:33,02/Jul/15 17:57,14/Jul/23 06:26,02/Jul/15 17:57,1.0.0,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,"If we're running Spark on a cluster, the checkpoint dir must be a non-local path. Otherwise, the attempt to read from a checkpoint will fail because the checkpoint files are written on the executors, not on the driver.

Currently, the error message that you get looks something like the following, which is not super intuitive:
{code}
Checkpoint RDD 3 (0) has different number of partitions than original RDD 2 (100)
{code}",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 01:36:12 UTC 2015,,,,,,,,,,"0|i2gewn:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"24/Jun/15 01:36;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6968;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize checkpointing to avoid computing an RDD twice,SPARK-8582,12840009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,andrewor14,andrewor14,24/Jun/15 01:23,05/Jan/22 07:58,14/Jul/23 06:26,05/Jan/22 07:58,1.0.0,,,,,,3.3.0,,,,,,Spark Core,,,,16,bulk-closed,,,,,"In Spark, checkpointing allows the user to truncate the lineage of his RDD and save the intermediate contents to HDFS for fault tolerance. However, this is not currently implemented super efficiently:

Every time we checkpoint an RDD, we actually compute it twice: once during the action that triggered the checkpointing in the first place, and once while we checkpoint (we iterate through an RDD's partitions and write them to disk). See this line for more detail: https://github.com/apache/spark/blob/0401cbaa8ee51c71f43604f338b65022a479da0a/core/src/main/scala/org/apache/spark/rdd/RDDCheckpointData.scala#L102.

Instead, we should have a `CheckpointingInterator` that writes checkpoint data to HDFS while we run the action. This will speed up many usages of `RDD#checkpoint` by 2X.

(Alternatively, the user can just cache the RDD before checkpointing it, but this is not always viable for very large input data. It's also not a great API to use in general.)",,ajithshetty,andrewor14,angolon@gmail.com,apachespark,bergun,byungjin.kim,calleo,cloud_fan,copris,dagrawal3409,dmcwhorter,glenn.strycker@gmail.com,godliness,hammer,hongyu.bi,hster,igreenfi,lev,maropu,MBALearnsToCode,michaelmalak,rdub,romi-totango,runzhliu,rxin,shivaram,steve.ash,stevel@apache.org,szhemzhitsky,viirya,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,SPARK-8666,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 05 07:58:59 UTC 2022,,,,,,,,,,"0|i2gevr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/15 01:24;andrewor14;[~tdas] also wants this.;;;","25/Jun/15 16:54;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/7021;;;","26/Jun/15 17:00;glenn.strycker@gmail.com;I didn't see this ticket and made a duplicate, https://issues.apache.org/jira/browse/SPARK-8666

See that ticket for a code example of what is going on.

There is also a stackoverflow question on this topic:  http://stackoverflow.com/questions/31078350/spark-rdd-checkpoint-on-persisted-cached-rdds-are-performing-the-dag-twice
;;;","27/Jun/15 05:29;srowen;[~glenn.strycker@gmail.com] in these cases please close your JIRA then.;;;","10/Sep/15 11:07;byungjin.kim;[~andrewor14]
Because of this bug, we suffer from performance degradation in large stated streaming app. and looking forward to fixing it.
Is there any update or any plan ? ;;;","24/Oct/15 01:04;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/9258;;;","03/Nov/15 01:40;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/9428;;;","13/Nov/15 23:09;andrewor14;Hi everyone, I have bumped this to 1.7.0 because of the potential performance regressions a fix could introduce. If you are affected by this and would like to solve this earlier, then you can workaround this by calling `persist` first before you call `checkpoint`. This ensures that the second time you compute the RDD reads from the cache instead, which is much faster for many workloads.;;;","18/Aug/18 09:45;MBALearnsToCode;hi [~zsxwing], may I check if we're working on resolving this issue? I'm currently experiencing this multiple-DAG-execution issue in Spark 2.3.1, and for huge data sets the repeated materialization takes a long time.;;;","23/Aug/18 19:46;bergun;+1 When is this issue is planned to be resolved?

I am facing it on Spark 2.3.1 when using with Dataset Api. It has been long time and old version since it has been reported? Is this maybe more complex than it seems? Thanks for the help;;;","03/Jun/20 12:02;calleo;Given this comment: [https://github.com/apache/spark/blob/e5b9b862e6011f37dfc0f646d6c3ae8e545e2cd6/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala#L165]

How come this issue has been closed? I've looked at the linked issues/PR's but can't really make out if this has been solved or if there is a work around. [~hyukjin.kwon] [~zsxwing]

Would like some clarification, thanks :)

 ;;;","09/Dec/20 05:41;godliness;[https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/ReliableCheckpointRDD.scala#L164]

 

Looks like we still have this problem right now?;;;","17/Feb/21 07:18;igreenfi;[~zsxwing] Why that Jira closed? I still see that as an issue on the code...;;;","23/Dec/21 20:38;apachespark;User 'agrawaldevesh' has created a pull request for this issue:
https://github.com/apache/spark/pull/35005;;;","23/Dec/21 20:38;dagrawal3409;This issue hasn't been fixed satisfactorily and I am making one more attempt at it: [https://github.com/apache/spark/pull/35005];;;","23/Dec/21 20:39;apachespark;User 'agrawaldevesh' has created a pull request for this issue:
https://github.com/apache/spark/pull/35005;;;","05/Jan/22 07:58;cloud_fan;Issue resolved by pull request 35005
[https://github.com/apache/spark/pull/35005];;;",,,,,,,,,,,,
Simplify and clean up the checkpointing code,SPARK-8581,12840004,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andrewor14,andrewor14,andrewor14,24/Jun/15 00:48,02/Jul/15 17:57,14/Jul/23 06:26,02/Jul/15 17:57,1.0.0,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,It is an old piece of code and a little overly complex at the moment. We can rewrite this to improve the readability and preserve exactly the same semantics.,,andrewor14,apachespark,glenn.strycker@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 01:36:07 UTC 2015,,,,,,,,,,"0|i2geun:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"24/Jun/15 01:36;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6968;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should ignore user defined output committer when appending data,SPARK-8578,12839975,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,lian cheng,lian cheng,23/Jun/15 22:13,25/May/17 10:48,14/Jul/23 06:26,24/Jun/15 16:51,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"When appending data to a file system via Hadoop API, it's safer to ignore user defined output committer classes like {{DirectParquetOutputCommitter}}. Because it's relatively hard to handle task failure in this case.  For example, {{DirectParquetOutputCommitter}} directly writes to the output directory to boost write performance when working with S3. However, there's no general way to determine task output file path of a specific task in Hadoop API, thus we don't know to revert a failed append job. (When doing overwrite, we can just remove the whole output directory.)",,apachespark,lian cheng,stevel@apache.org,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10063,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 25 10:48:21 UTC 2017,,,,,,,,,,"0|i2gep3:",9223372036854775807,,,,,lian cheng,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"24/Jun/15 02:51;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6964;;;","24/Jun/15 02:52;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6966;;;","24/Jun/15 16:51;yhuai;Issue resolved by pull request 6966
[https://github.com/apache/spark/pull/6966];;;","25/May/17 10:48;stevel@apache.org;Given SPARK-10063 has pulled the {{DirectParquetOutputCommitter}} on account of the incompatibility with 'direct' output and 'commit protocol', it may be time to revisit this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
org/apache/spark/unsafe doesn't honor the java source/target versions,SPARK-8574,12839941,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,23/Jun/15 20:25,25/Jun/15 13:29,14/Jul/23 06:26,25/Jun/15 13:29,1.4.0,,,,,,1.4.1,,,,,,Build,,,,0,,,,,,"I built spark using jdk8 and the default source compatibility in the pom is 1.6 so I expected to be able to run Spark with jdk7, but if fails because the unsafe code doesn't seem to be honoring the source/target compatibility options set in the top level pom.

Exception in thread ""main"" java.lang.UnsupportedClassVersionError: org/apache/spark/unsafe/memory/MemoryAllocator : Unsupported major.minor version 52.0
        at java.lang.ClassLoader.defineClass1(Native Method)
        at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
        at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
        at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
        at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
        at org.apache.spark.SparkEnv$.create(SparkEnv.scala:392)
        at org.apache.spark.SparkEnv$.createExecutorEnv(SparkEnv.scala:211)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:180)
        at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:74)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:146)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:245)
        at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)
15/06/23 19:48:24 INFO storage.DiskBlockManager: Shutdown hook called",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 14:21:07 UTC 2015,,,,,,,,,,"0|i2gehj:",9223372036854775807,,,,,,,,,,,,,,1.4.1,,,,,,,,,,,,,"24/Jun/15 14:21;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/6989;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Type coercion for ScalaUDFs,SPARK-8572,12839930,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,cheolsoo,yhuai,yhuai,23/Jun/15 20:13,04/Jul/15 05:14,14/Jul/23 06:26,04/Jul/15 05:14,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Seems we do not do type coercion for ScalaUDFs. The following code will hit a runtime exception.
{code}
import org.apache.spark.sql.functions._
val myUDF = udf((x: Int) => x + 1)
val df = sqlContext.range(1, 10).toDF(""i"").select(myUDF($""i""))
df.explain(true)
df.show
{code}
It is also good to check if we do type coercion for PythonUDFs.",,apachespark,cheolsoo,glenn.strycker@gmail.com,maropu,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 03 01:41:06 UTC 2015,,,,,,,,,,"0|i2gef3:",9223372036854775807,,,,,,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"27/Jun/15 20:41;yhuai;[~cloud_fan] Will you have time to fix this?;;;","03/Jul/15 01:41;apachespark;User 'piaozhexiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/7203;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.sql.hive.HiveSparkSubmitSuite --jars,SPARK-8567,12839895,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,23/Jun/15 17:02,22/Sep/15 07:07,14/Jul/23 06:26,22/Sep/15 07:07,1.4.1,,,,,,1.4.1,1.5.0,1.5.1,1.6.0,,,SQL,Tests,,,0,flaky-test,,,,,Seems tests in HiveSparkSubmitSuite fail with timeout pretty frequently.,,apachespark,mengxr,rekhajoshm,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 21 20:26:05 UTC 2015,,,,,,,,,,"0|i2ge7z:",9223372036854775807,,,,,mengxr,,,,,,,,,1.4.1,1.5.0,1.6.0,,,,,,,,,,,"23/Jun/15 18:30;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6957;;;","24/Jun/15 07:16;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6978;;;","25/Jun/15 05:25;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/7009;;;","25/Jun/15 20:40;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/7027;;;","29/Jun/15 19:42;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/7092;;;","17/Sep/15 22:09;mengxr;Saw more failures recently:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.4-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.3,label=centos/449/testReport/junit/org.apache.spark.sql.hive/HiveSparkSubmitSuite/SPARK_8368__includes_jars_passed_in_through___jars/

{code}
org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-8368: includes jars passed in through --jars

Failing for the past 1 build (Since Aborted#449 )
Took 3 min 1 sec.
Error Message

The code passed to failAfter did not complete within 180 seconds.
Stacktrace

sbt.ForkMain$ForkError: The code passed to failAfter did not complete within 180 seconds.
	at org.scalatest.concurrent.Timeouts$$anonfun$failAfter$1.apply(Timeouts.scala:249)
	at org.scalatest.concurrent.Timeouts$$anonfun$failAfter$1.apply(Timeouts.scala:249)
	at org.scalatest.concurrent.Timeouts$class.timeoutAfter(Timeouts.scala:345)
	at org.scalatest.concurrent.Timeouts$class.failAfter(Timeouts.scala:245)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite.failAfter(HiveSparkSubmitSuite.scala:32)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite.org$apache$spark$sql$hive$HiveSparkSubmitSuite$$runSparkSubmit(HiveSparkSubmitSuite.scala:90)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite$$anonfun$1.apply$mcV$sp(HiveSparkSubmitSuite.scala:57)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite$$anonfun$1.apply(HiveSparkSubmitSuite.scala:44)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite$$anonfun$1.apply(HiveSparkSubmitSuite.scala:44)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(HiveSparkSubmitSuite.scala:32)
	at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite.runTest(HiveSparkSubmitSuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.scalatest.FunSuite.run(FunSuite.scala:1555)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: sbt.ForkMain$ForkError
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:503)
	at java.lang.UNIXProcess.waitFor(UNIXProcess.java:263)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite$$anonfun$4.apply$mcI$sp(HiveSparkSubmitSuite.scala:90)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite$$anonfun$4.apply(HiveSparkSubmitSuite.scala:90)
	at org.apache.spark.sql.hive.HiveSparkSubmitSuite$$anonfun$4.apply(HiveSparkSubmitSuite.scala:90)
	at org.scalatest.concurrent.Timeouts$class.timeoutAfter(Timeouts.scala:326)
	... 46 more
{code};;;","19/Sep/15 22:55;rekhajoshm;Also [~mengxr] there is a worker shutdown issue (SPARK-4300 ?) for HiveSparkSubmitSuite at the SPARK-8368 timeout log.Could that be the cause of the SPARK-8368 failure? [~yhuai] thanks!
{code}
[info] HiveSparkSubmitSuite:Exception in thread ""redirect stderr for command ./bin/spark-submit"" java.io.IOException: Stream closed
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
	at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)
	at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)
	at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)
	at java.io.InputStreamReader.read(InputStreamReader.java:184)
	at java.io.BufferedReader.fill(BufferedReader.java:154)
	at java.io.BufferedReader.readLine(BufferedReader.java:317)
	at java.io.BufferedReader.readLine(BufferedReader.java:382)
	at scala.io.BufferedSource$BufferedLineIterator.hasNext(BufferedSource.scala:67)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at org.apache.spark.util.Utils$$anon$4.run(Utils.scala:1081)
{code};;;","21/Sep/15 20:26;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/8850;;;",,,,,,,,,,,,,,,,,,,,,
Bug that IndexedRowMatrix.computeSVD() yields the U with wrong numCols,SPARK-8563,12839812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lee19,lee19,lee19,23/Jun/15 11:08,02/Jul/15 18:41,14/Jul/23 06:26,30/Jun/15 21:14,1.0.2,1.1.1,1.2.2,1.3.1,1.4.1,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.1,1.5.0,MLlib,,,,0,,,,,,"IndexedRowMatrix.computeSVD() yields a wrong U which *U.numCols() = self.nCols*.

It should have been *U.numCols() = k = svd.U.numCols()*

{code}
self = U * sigma * V.transpose
(m x n) = (m x n) * (k x k) * (k x n)
-->
(m x n) = (m x k) * (k x k) * (k x n)
{code}


Proposed fix: https://github.com/apache/spark/pull/6953
",,apachespark,lee19,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 21:14:01 UTC 2015,,,,,,,,,,"0|i2gdpr:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.1,1.5.0,,,,,,,,"23/Jun/15 11:12;apachespark;User 'lee19' has created a pull request for this issue:
https://github.com/apache/spark/pull/6949;;;","23/Jun/15 13:31;apachespark;User 'lee19' has created a pull request for this issue:
https://github.com/apache/spark/pull/6953;;;","30/Jun/15 21:14;mengxr;Issue resolved by pull request 6953
[https://github.com/apache/spark/pull/6953];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Executors page displays negative active tasks,SPARK-8560,12839792,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,meiyoula,meiyoula,meiyoula,23/Jun/15 08:56,24/Nov/16 06:22,14/Jul/23 06:26,30/Jun/15 20:58,1.0.0,,,,,,1.5.0,,,,,,Spark Core,Web UI,,,0,,,,,,This is caused by resubmitted tasks. See PR for more detail.,,apachespark,cltlfcjin,meiyoula,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10141,,,,,,,,,,,,,"23/Jun/15 09:01;meiyoula;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12741253/screenshot-1.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Nov 24 06:22:48 UTC 2016,,,,,,,,,,"0|i2gdlb:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"23/Jun/15 09:00;apachespark;User 'XuTingjun' has created a pull request for this issue:
https://github.com/apache/spark/pull/6950;;;","23/Jun/15 11:19;srowen;negative what? why would a resubmit be treated differently?
I suspect that the actual cause is a duplicate of several JIRAs already here; please search first.
SPARK-6891 SPARK-6954;;;","28/Jun/15 08:33;meiyoula;Sorry, I think you misunderstand what I described. It's not the dynamic allocation bug, it's about web. you can have a look of the attachment.

Why would a resubmit be treated differently? I have explained in the patch.;;;","30/Jun/15 07:42;srowen;Please fix the title;;;","24/Nov/16 06:22;cltlfcjin;It still displays negative active tasks in UI of Spark-2.0.1;;;",,,,,,,,,,,,,,,,,,,,,,,,
Script /dev/run-tests fails when _JAVA_OPTIONS env var set,SPARK-8558,12839776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,fe2s,fe2s,fe2s,23/Jun/15 08:09,24/Jun/15 22:13,14/Jul/23 06:26,24/Jun/15 22:12,1.5.0,,,,,,1.5.0,,,,,,Build,Tests,,,0,,,,,,"Script /dev/run-tests.py fails when _JAVA_OPTIONS env. var set.

Steps to reproduce in linux:
1. export _JAVA_OPTIONS=""-Xmx2048M
2. ./dev/run-tests

[pivot@fe2s spark]$ ./dev/run-tests
Traceback (most recent call last):
  File ""./dev/run-tests.py"", line 793, in <module>
    main()
  File ""./dev/run-tests.py"", line 722, in main
    java_version = determine_java_version(java_exe)
  File ""./dev/run-tests.py"", line 484, in determine_java_version
    version, update = version_str.split('_')  # eg ['1.8.0', '25']
ValueError: need more than 1 value to unpack

The problem is in 'determine_java_version' function in run-tests.py.
It runs 'java' and extracts version from output. However when _JAVA_OPTIONS set the output of 'java' command is different and it breaks parser. See the first line

[pivot@fe2s spark]$ java -version
Picked up _JAVA_OPTIONS: -Xmx2048M
java version ""1.8.0_31""
Java(TM) SE Runtime Environment (build 1.8.0_31-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode)
",Centos 6,apachespark,fe2s,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 22:12:48 UTC 2015,,,,,,,,,,"0|i2gdhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/15 09:28;srowen;Yeah, although you probably should not set _JAVA_OPTIONS here it seems like it should just ignore this line of output if present. Go for a PR if you can.;;;","23/Jun/15 16:42;apachespark;User 'fe2s' has created a pull request for this issue:
https://github.com/apache/spark/pull/6956;;;","24/Jun/15 22:12;joshrosen;Issue resolved by pull request 6956
[https://github.com/apache/spark/pull/6956];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add the SparkR document files to `.rat-excludes` for `./dev/check-license`,SPARK-8554,12839756,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuu.ishikawa@gmail.com,yuu.ishikawa@gmail.com,yuu.ishikawa@gmail.com,23/Jun/15 06:10,29/Jun/15 16:23,14/Jul/23 06:26,29/Jun/15 16:23,,,,,,,1.5.0,,,,,,SparkR,Tests,,,0,,,,,,"{noformat}
> ./dev/check-license | grep -v boto

Could not find Apache license headers in the following files:
 !????? /Users/01004981/local/src/spark/myspark/R/lib/SparkR/INDEX
 !????? /Users/01004981/local/src/spark/myspark/R/lib/SparkR/help/AnIndex
 !????? /Users/01004981/local/src/spark/myspark/R/lib/SparkR/html/00Index.html
 !????? /Users/01004981/local/src/spark/myspark/R/lib/SparkR/html/R.css
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/DataFrame.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/GroupedData.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/agg.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/arrange.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/cache-methods.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/cacheTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/cancelJobGroup.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/clearCache.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/clearJobGroup.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/collect-methods.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/column.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/columns.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/count.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/createDataFrame.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/createExternalTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/describe.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/distinct.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/dropTempTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/dtypes.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/except.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/explain.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/filter.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/first.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/groupBy.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/hashCode.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/head.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/infer_type.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/insertInto.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/intersect.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/isLocal.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/join.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/jsonFile.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/limit.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/nafunctions.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/parquetFile.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/persist.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/print.jobj.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/print.structField.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/print.structType.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/printSchema.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/read.df.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/registerTempTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/repartition.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sample.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/saveAsParquetFile.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/saveAsTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/schema.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/select.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/selectExpr.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/setJobGroup.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/show.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/showDF.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sparkR.init.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sparkR.stop.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sparkRHive.init.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sparkRSQL.init.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sql.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/structField.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/structType.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/table.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/tableNames.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/tables.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/take.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/uncacheTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/unionAll.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/unpersist-methods.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/withColumn.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/withColumnRenamed.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/write.df.Rd
{noformat}",,apachespark,joshrosen,yuu.ishikawa@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 16:23:28 UTC 2015,,,,,,,,,,"0|i2gddb:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"23/Jun/15 06:23;apachespark;User 'yu-iskw' has created a pull request for this issue:
https://github.com/apache/spark/pull/6947;;;","29/Jun/15 16:23;joshrosen;Issue resolved by pull request 6947
[https://github.com/apache/spark/pull/6947];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark : Can't create DataFrame from Pandas dataframe with no explicit column name,SPARK-8535,12839660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,x1,kriss,kriss,22/Jun/15 20:15,02/Jul/15 19:55,14/Jul/23 06:26,01/Jul/15 03:36,1.4.0,,,,,,1.3.2,1.4.1,1.5.0,,,,PySpark,,,,0,,,,,,"Trying to create a Spark DataFrame from a pandas dataframe with no explicit column name : 

pandasDF = pd.DataFrame([[1, 2], [5, 6]])
sparkDF = sqlContext.createDataFrame(pandasDF)

***********

----> 1 sparkDF = sqlContext.createDataFrame(pandasDF)

/usr/local/Cellar/apache-spark/1.4.0/libexec/python/pyspark/sql/context.pyc in createDataFrame(self, data, schema, samplingRatio)
    344 
    345         jrdd = self._jvm.SerDeUtil.toJavaArray(rdd._to_java_object_rdd())
--> 346         df = self._ssql_ctx.applySchemaToPythonRDD(jrdd.rdd(), schema.json())
    347         return DataFrame(df, self)
    348 

/usr/local/Cellar/apache-spark/1.4.0/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/usr/local/Cellar/apache-spark/1.4.0/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o87.applySchemaToPythonRDD.",,apachespark,davies,kriss,michaelmalak,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 04:49:17 UTC 2015,,,,,,,,,,"0|i2gcs7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/15 11:11;apachespark;User 'x1-' has created a pull request for this issue:
https://github.com/apache/spark/pull/7124;;;","30/Jun/15 11:12;x1;Because implicit name of {{pandas.columns}} are Int, but {{StructField}} json expect {{String}}.
So I think {{pandas.columns}} are should be convert to {{String}}.

I create PR below.
https://github.com/apache/spark/pull/7124;;;","01/Jul/15 03:36;davies;Issue resolved by pull request 7124
[https://github.com/apache/spark/pull/7124];;;","01/Jul/15 04:49;x1;Could you change assignee from no-assignee to me?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"In Python's DataFrameWriter, save/saveAsTable/json/parquet/jdbc always override mode",SPARK-8532,12839601,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,22/Jun/15 16:46,22/Jun/15 20:51,14/Jul/23 06:26,22/Jun/15 20:51,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"Although users can use {{df.write.mode(""overwrite"")}} to specify the mode, when save/saveAsTable/json/parquet/jdbc is called, this mode will be overridden. For example, the implementation of json method is 
{code}
def json(self, path, mode=""error""):
  self._jwrite.mode(mode).json(path)
{code}
If users only call {{json(""path"")}}, the mode will be ""error"" instead of the mode specified in the mode method.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 20:51:48 UTC 2015,,,,,,,,,,"0|i2gcgf:",9223372036854775807,,,,,davies,,,,,,,,,1.4.1,,,,,,,,,,,,,"22/Jun/15 16:46;yhuai;For save/saveAsTable/json/parquet/jdbc, seems we can just remove the mode.;;;","22/Jun/15 17:45;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6937;;;","22/Jun/15 20:51;yhuai;Issue resolved by pull request 6937
[https://github.com/apache/spark/pull/6937];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
StructType's Factory method does not work in java code,SPARK-8527,12839591,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,invkrh,invkrh,22/Jun/15 16:26,07/Oct/16 23:18,14/Jul/23 06:26,07/Oct/16 23:18,1.4.0,,,,,,,,,,,,SQL,,,,0,,,,,,"According to the following line of code below:

https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructType.scala#L209

The casting should be successful, however after type erasure, I encounter {{java.lang.ClassCastException}}:

{code}
ArrayList<StructField> structFields = new ArrayList<>();
// Some add operation
return StructType$.MODULE$.apply(structFields); // run time error

Exception in thread ""main"" java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to [Lorg.apache.spark.sql.types.StructField;
	at org.apache.spark.sql.types.StructType$.apply(StructType.scala:209)
{code}

Am I missing anything ?",,invkrh,smilegator,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 23:17:52 UTC 2016,,,,,,,,,,"0|i2gce7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 23:17;smilegator;This should have been resolved. Could you retry it in the master branch. If you still hit it, please reopen it. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bug in Streaming k-means documentation,SPARK-8525,12839521,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,fe2s,fe2s,fe2s,22/Jun/15 11:55,04/Jul/15 07:29,14/Jul/23 06:26,23/Jun/15 20:12,1.1.1,1.2.2,1.3.1,1.4.1,,,1.1.2,1.2.3,1.3.2,1.4.1,1.5.0,,Documentation,MLlib,,,0,,,,,,"The expected input format is wrong in Streaming K-means documentation.
https://spark.apache.org/docs/latest/mllib-clustering.html#streaming-k-means

It might be a bug in implementation though, not sure.

There shouldn't be any spaces in test data points. I.e. instead of 
(y, [x1, x2, x3]) it should be
(y,[x1,x2,x3])

The exception thrown 
org.apache.spark.SparkException: Cannot parse a double from:  
	at org.apache.spark.mllib.util.NumericParser$.parseDouble(NumericParser.scala:118)
	at org.apache.spark.mllib.util.NumericParser$.parseTuple(NumericParser.scala:103)
	at org.apache.spark.mllib.util.NumericParser$.parse(NumericParser.scala:41)
	at org.apache.spark.mllib.regression.LabeledPoint$.parse(LabeledPoint.scala:49)


Also I would improve documentation saying explicitly that expected data types for both 'x' and 'y' is Double. At the moment it's not obvious especially for 'y'. 

",,apachespark,fe2s,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 20:12:44 UTC 2015,,,,,,,,,,"0|i2gbyv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/15 12:23;srowen;I imagine it should just accept spaces (and ignore them). It would be great if you can create a unit test and propose a fix in a PR.;;;","23/Jun/15 15:14;apachespark;User 'fe2s' has created a pull request for this issue:
https://github.com/apache/spark/pull/6954;;;","23/Jun/15 15:17;fe2s;Makes sense, https://github.com/apache/spark/pull/6954;;;","23/Jun/15 15:59;srowen;(Nit: this isn't really a bug in the docs right? and this can be more specific, about accepting white space in certain k-means input?);;;","23/Jun/15 20:12;mengxr;Issue resolved by pull request 6954
[https://github.com/apache/spark/pull/6954];;;",,,,,,,,,,,,,,,,,,,,,,,,
SparkR does not provide an easy way to depend on Spark Packages when performing init from inside of R,SPARK-8506,12839374,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,holden,holden,holden,20/Jun/15 22:12,10/Nov/15 05:53,14/Jul/23 06:26,24/Jun/15 18:55,1.4.0,,,,,,1.4.1,1.5.0,,,,,SparkR,,,,0,,,,,,"While packages can be specified when using the sparkR or sparkSubmit scripts, the programming guide tells people to create their spark context using the R shell + init. The init does have a parameter for jars but no parameter for packages. Setting the SPARKR_SUBMIT_ARGS overwrites some necessary information. I think a good solution would just be adding another field to the init function to allow people to specify packages in the same way as jars.",,apachespark,girishsml,hlin09,holden,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 18:55:45 UTC 2015,,,,,,,,,,"0|i2gb27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/15 22:42;shivaram;Thats a good point - I just ran into this yesterday and created a PR that improves documentation at https://github.com/apache/spark/pull/6916

Do you think adding a new argument to sparkR.init will be good enough ? i.e. something like sparkR.init(master=""local"", packages=""com.databricks.spark-csv_2.10:1.0.3"");;;","20/Jun/15 23:10;holden;Thats what I thought would be a good solution. I can go ahead and implement if you thinks its a good idea too?;;;","20/Jun/15 23:13;shivaram;Yeah sounds good to me. One minor thing is that I'd suggest calling the argument `sparkPackages` and take in a comma separated list (similar to jars);;;","21/Jun/15 02:38;holden;Sounds like a plan :);;;","21/Jun/15 05:07;hlin09;This is a good solution. I just ran into the same problem when doing the tutorial. ;;;","22/Jun/15 01:07;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6928;;;","24/Jun/15 18:55;shivaram;Issue resolved by pull request 6928
[https://github.com/apache/spark/pull/6928];;;",,,,,,,,,,,,,,,,,,,,,,
ORC data source may give empty schema if an ORC file containing zero rows is picked for schema discovery,SPARK-8501,12839314,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,lian cheng,lian cheng,20/Jun/15 08:04,11/Oct/17 06:22,14/Jul/23 06:26,03/Jul/15 04:33,1.4.0,,,,,,,,,,,,SQL,,,,0,,,,,,"Not sure whether this should be considered as a bug of ORC bundled with Hive 0.13.1: for an ORC file containing zero rows, the schema written in its footer contains zero fields (e.g. {{struct<>}}).

To reproduce this issue, let's first produce an empty ORC file.  Copy data file {{sql/hive/src/test/resources/data/files/kv1.txt}} in Spark code repo to {{/tmp/kv1.txt}} (I just picked a random simple test data file), then run the following lines in Hive 0.13.1 CLI:
{noformat}
$ hive
hive> CREATE TABLE foo(key INT, value STRING);
hive> LOAD DATA LOCAL INPATH '/tmp/kv1.txt' INTO TABLE foo;
hive> CREATE TABLE bar STORED AS ORC AS SELECT * FROM foo WHERE key = -1;
{noformat}
Now inspect the empty ORC file we just wrote:
{noformat}
$ hive --orcfiledump /user/hive/warehouse_hive13/bar/000000_0
Structure for /user/hive/warehouse_hive13/bar/000000_0
15/06/20 00:42:54 INFO orc.ReaderImpl: Reading ORC rows from /user/hive/warehouse_hive13/bar/000000_0 with {include: null, offset: 0, length: 9223372036854775807}
Rows: 0
Compression: ZLIB
Compression size: 262144
Type: struct<>

Stripe Statistics:

File Statistics:
  Column 0: count: 0

Stripes:
{noformat}
Notice the {{struct<>}} part.

This ""feature"" is OK for Hive, which has a central metastore to save table schema.  But for users who read raw data files without Hive metastore with Spark SQL 1.4.0, it causes problem because currently the ORC data source just picks a random part-file whichever comes the first for schema discovery.

Expected behavior can be:

# Try all files one by one until we find a part-file with non-empty schema.
# Throws {{AnalysisException}} if no such part-file can be found.
",Hive 0.13.1,apachespark,lian cheng,yhuai,zzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-15474,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 03 04:33:19 UTC 2015,,,,,,,,,,"0|i2gap3:",9223372036854775807,,,,,yhuai,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"02/Jul/15 22:01;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/7199;;;","02/Jul/15 22:23;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/7200;;;","02/Jul/15 22:48;zzhan;Because in spark, we will not create the orc file if the record is empty. It is only happens with the ORC file created by hive, right? ;;;","02/Jul/15 22:50;lian cheng;Exactly. Please see my PR description here https://github.com/apache/spark/pull/7199;;;","03/Jul/15 04:33;lian cheng;Fixed by https://github.com/apache/spark/pull/7199

Backported to 1.4.1 by https://github.com/apache/spark/pull/7200;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix NullPointerException in error-handling path in UnsafeShuffleWriter,SPARK-8498,12839309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,joshrosen,joshrosen,20/Jun/15 06:41,17/May/20 18:30,14/Jul/23 06:26,12/Aug/15 00:43,1.4.0,,,,,,1.5.0,,,,,,Shuffle,Spark Core,,,0,,,,,,"This bug was reported by [~prudenko] on the dev list.  When the {{tungsten-sort}} shuffle manager was enabled, an executor died with the following exception:

{code}
15/06/19 17:53:35 WARN TaskSetManager: Lost task 38.0 in stage 41.0 (TID 3176, ip-10-50-225-214.ec2.internal): java.lang.NullPointerException
        at org.apache.spark.shuffle.unsafe.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:151)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

I think that this is actually due to an error-handling issue.  In the stack trace, the NPE is being thrown from an error-handling branch of a `finally` block:

{code}
public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException {
    boolean success = false;
    try {
      while (records.hasNext()) {
        insertRecordIntoSorter(records.next());
      }
      closeAndWriteOutput();
      success = true;
    } finally {
      if (!success) {
        sorter.cleanupAfterError();  // <---- this is the line throwing the error
      }
    }
  }
{code}

I suspect that what's happening is that an exception is being thrown from user / upstream code in the initial call to records.next(), but the error-handling block is failing because sorter == null since we haven't initialized it yet.

We should fix this bug with a {{sorter != null}} check and should also add a regression test to ShuffleSuite to ensure that exceptions thrown by user code at this step of the shuffle write path don't get masked by error-handling bugs inside of the shuffle code.",,apachespark,holden,joshrosen,rxin,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-7075,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 11:18:43 UTC 2015,,,,,,,,,,"0|i2ganz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/15 07:20;holden;I could take this :);;;","20/Jun/15 07:58;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6918;;;","23/Jun/15 16:09;joshrosen;Issue resolved by pull request 6918
[https://github.com/apache/spark/pull/6918];;;","23/Jun/15 16:18;joshrosen;Re-opening because I had to revert the branch-1.4 patch due to a Java 6 compilation issue.;;;","26/Jun/15 00:18;x1;I think {{sorter.cleanupAfterError()}} throw {{SparkException}}, - not {{IOException}}.
But method {{write}} declare throw {{IOException}}.
{code}
public void write(scala.collection.Iterator<Product2<K, V>> records) throws IOException
{code}

So, maybe we cannot compile {{UnsafeShuffleWriter}} class.;;;","12/Aug/15 00:43;rxin;Resolving this since we are only fixing it for 1.5, not branch-1.4 since it is an experimental feature in 1.4.
;;;","10/Dec/15 11:18;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6909;;;",,,,,,,,,,,,,,,,,,,,,,
Add regression tests for SPARK-8470,SPARK-8489,12839236,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,19/Jun/15 21:00,01/Sep/18 14:10,14/Jul/23 06:26,20/Jun/15 00:37,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,Tests,,,0,,,,,,"See SPARK-8470 for more detail. Basically the Spark Hive code silently overwrites the context class loader populated in SparkSubmit, resulting in certain classes missing when we do reflection in `SQLContext#createDataFrame`.

That issue is already resolved in https://github.com/apache/spark/pull/6891, but we should add a regression test for the specific manifestation of the bug in SPARK-8470.",,andrewor14,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8470,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 01 14:10:10 UTC 2018,,,,,,,,,,"0|i2ga7z:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"20/Jun/15 00:37;yhuai;Issue resolved by https://github.com/apache/spark/pull/6909 (the pr used a wrong jira number).;;;","10/Mar/16 08:12;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/11630;;;","16/Mar/16 00:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/11744;;;","05/May/16 06:46;apachespark;User 'dilipbiswal' has created a pull request for this issue:
https://github.com/apache/spark/pull/12924;;;","01/Sep/18 14:10;apachespark;User 'sadhen' has created a pull request for this issue:
https://github.com/apache/spark/pull/22308;;;",,,,,,,,,,,,,,,,,,,,,,,,
Remove commons-lang3 depedency from flume-sink,SPARK-8483,12839180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hshreedharan,hshreedharan,hshreedharan,19/Jun/15 17:06,24/Jun/15 19:06,14/Jul/23 06:26,23/Jun/15 06:46,1.4.0,,,,,,1.4.1,1.5.0,,,,,DStreams,,,,0,,,,,,"flume-sink module uses only one method from commons-lang3. Since the build would become complex if we create an assembly and would likely make it more difficult for customers, let's just remove the dependency altogether.",,apachespark,hshreedharan,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 20 01:23:44 UTC 2015,,,,,,,,,,"0|i2g9vb:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"19/Jun/15 22:46;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/6910;;;","20/Jun/15 01:04;tdas;We generally do not the dependency changes between patch releases. Because of potential dependency issues between multiple version of same libraries at runtime, etc. But this sink thing runs only in Flume. So do you think it is okay in this case? For sure?;;;","20/Jun/15 01:23;hshreedharan;Well, we aren't adding a dependency - we are only removing one. So I don't see stuff breaking. We can push it out to 1.5 if this is risky.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Setters inc/decDiskBytesSpilled in TaskMetrics should also be private.,SPARK-8476,12839053,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,ueshin,ueshin,19/Jun/15 04:43,19/Jun/15 17:48,14/Jul/23 06:26,19/Jun/15 17:48,,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,This is a follow-up of SPARK-3288.,,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 04:50:07 UTC 2015,,,,,,,,,,"0|i2g93j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/15 04:50;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6896;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross-validation with RegressionEvaluator prefers higher RMSE,SPARK-8468,12839010,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,chelseaz,chelseaz,19/Jun/15 00:46,20/Jun/15 20:02,14/Jul/23 06:26,20/Jun/15 20:02,1.4.0,,,,,,1.4.1,1.5.0,,,,,ML,,,,0,,,,,,"Please correct me if I'm wrong, but RegressionEvaluator seems to implement the evaluate() method backwards. The interface expects higher return values from evaluate() to indicate better models. RegressionEvaluator returns RMSE by default - a value we should try to minimize.",,apachespark,cfregly,chelseaz,josephkb,yuu.ishikawa@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 20 20:02:34 UTC 2015,,,,,,,,,,"0|i2g8un:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"19/Jun/15 13:58;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6905;;;","19/Jun/15 17:01;josephkb;You're definitely correct---thanks for noticing!;;;","20/Jun/15 20:02;josephkb;Issue resolved by pull request 6905
[https://github.com/apache/spark/pull/6905];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
No suitable driver found for write.jdbc,SPARK-8463,12838966,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,mlety2,mlety2,18/Jun/15 22:42,16/Aug/15 14:50,14/Jul/23 06:26,07/Jul/15 00:17,1.4.0,1.5.0,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"I am getting a java.sql.SQLException: No suitable driver found for jdbc:mysql://dbhost/test when using df.write.jdbc.

I do not get this error when reading from the same database. 

This simple script can repeat the problem.
First one must create a database called test with a table called table1 and insert some rows in it. The user test:secret must have read/write permissions.

*testJDBC.scala:*
import java.util.Properties
import org.apache.spark.sql.Row
import java.sql.Struct
import org.apache.spark.sql.types.\{StructField, StructType, IntegerType, StringType}
import org.apache.spark.\{SparkConf, SparkContext}
import org.apache.spark.sql.SQLContext

val properties = new Properties()
properties.setProperty(""user"", ""test"")
properties.setProperty(""password"", ""secret"")
val readTable = sqlContext.read.jdbc(""jdbc:mysql://dbhost/test"", ""table1"", properties)

print(readTable.show())

val rows = sc.parallelize(List(Row(1, ""write""), Row(2, ""me"")))
val writeTable = sqlContext.createDataFrame(rows, StructType(List(StructField(""id"", IntegerType), StructField(""name"", StringType))))
writeTable.write.jdbc(""jdbc:mysql://dbhost/test"", ""table2"", properties)}}

This is run using:
{{spark-shell --conf spark.executor.extraClassPath=/path/to/mysql-connector-java-5.1.35-bin.jar --driver-class-path /path/to/mysql-connector-java-5.1.35-bin.jar --jars /path/to/mysql-connector-java-5.1.35-bin.jar -i:testJDBC.scala}}

The read works fine and will print the rows in the table. The write fails with {{java.sql.SQLException: No suitable driver found for jdbc:mysql://dbhost/test}}. The new table is successfully created but it is empty.

I have tested this on a Mesos cluster with Spark 1.4.0 and the current master branch as of June 18th.

In the executor logs I do see before the error:
INFO Utils: Fetching http://146.203.54.236:50624/jars/mysql-connector-java-5.1.35-bin.jar
INFO Executor: Adding file:/tmp/mesos/slaves/.../mysql-connector-java-5.1.35-bin.jar to class loader

A workaround is to add the mysql-connector-java-5.1.35-bin.jar to the same location on each executor node as defined in spark.executor.extraClassPath.","Mesos, Ubuntu",apachespark,mlety2,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9985,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 08:48:13 UTC 2015,,,,,,,,,,"0|i2g8kv:",9223372036854775807,,,,,rxin,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"19/Jun/15 09:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6900;;;","02/Jul/15 08:48;rxin;[~mlety2] can you test this the patch created by [~viirya]?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException when code generation is enabled,SPARK-8461,12838964,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,lian cheng,lian cheng,18/Jun/15 22:37,19/Jun/15 18:40,14/Jul/23 06:26,19/Jun/15 18:40,1.5.0,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Build Spark without {{-Phive}} to make sure the isolated classloader for Hive support is irrelevant, then run the following Spark shell snippet:
{code}
sqlContext.range(0, 2).select(lit(""a"") as 'a).coalesce(1).write.mode(""overwrite"").json(""file:///tmp/foo"")
{code}
Exception thrown:
{noformat}
15/06/18 15:36:30 ERROR codegen.GenerateMutableProjection: failed to compile:

      import org.apache.spark.sql.catalyst.InternalRow;

      public SpecificProjection generate(org.apache.spark.sql.catalyst.expressions.Expression[] expr) {
        return new SpecificProjection(expr);
      }

      class SpecificProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {

        private org.apache.spark.sql.catalyst.expressions.Expression[] expressions = null;
        private org.apache.spark.sql.catalyst.expressions.MutableRow mutableRow = null;

        public SpecificProjection(org.apache.spark.sql.catalyst.expressions.Expression[] expr) {
          expressions = expr;
          mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericMutableRow(1);
        }

        public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(org.apache.spark.sql.catalyst.expressions.MutableRow row) {
          mutableRow = row;
          return this;
        }

        /* Provide immutable access to the last projected row. */
        public InternalRow currentValue() {
          return (InternalRow) mutableRow;
        }

        public Object apply(Object _i) {
          InternalRow i = (InternalRow) _i;

      /* expression: a */
      Object obj2 = expressions[0].eval(i);
      boolean isNull0 = obj2 == null;
      org.apache.spark.unsafe.types.UTF8String primitive1 = null;
      if (!isNull0) {
        primitive1 = (org.apache.spark.unsafe.types.UTF8String) obj2;
      }

          if(isNull0)
            mutableRow.setNullAt(0);
          else
            mutableRow.update(0, primitive1);


          return mutableRow;
        }
      }

org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
        at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
        at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
        at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
        at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
        at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835)
        at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063)
        at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:211)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:199)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
        at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
        at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
        at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
        at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
        at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
        at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
        at org.codehaus.janino.ClassBodyEvaluator.<init>(ClassBodyEvaluator.java:72)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:282)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:173)
        at org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:41)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:93)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:92)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.hasNext(DataFrame.scala:1471)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: Object
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:79)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:344)
        at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
        at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
        ... 80 more
Caused by: java.lang.ClassNotFoundException: Object
        at java.lang.ClassLoader.findClass(ClassLoader.java:530)
        at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:74)
        ... 87 more
15/06/18 15:36:30 ERROR executor.Executor: Exception in task 0.0 in stage 4.0 (TID 18)
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
        at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:282)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:173)
        at org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:41)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:93)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:92)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.hasNext(DataFrame.scala:1471)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
        at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
        at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
        at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
        at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
        at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835)
        at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063)
        at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:211)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:199)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
        at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
        at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
        at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
        at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
        at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
        at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
        at org.codehaus.janino.ClassBodyEvaluator.<init>(ClassBodyEvaluator.java:72)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        ... 38 more
Caused by: java.lang.ClassNotFoundException: Object
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:79)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:344)
        at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
        at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
        ... 80 more
Caused by: java.lang.ClassNotFoundException: Object
        at java.lang.ClassLoader.findClass(ClassLoader.java:530)
        at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:74)
        ... 87 more
15/06/18 15:36:30 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 4.0 (TID 18, localhost): java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
        at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:282)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:173)
        at org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:41)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:93)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:92)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.hasNext(DataFrame.scala:1471)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
        at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
        at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
        at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
        at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
        at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835)
        at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063)
        at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:211)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:199)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
        at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
        at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
        at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
        at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
        at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
        at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
        at org.codehaus.janino.ClassBodyEvaluator.<init>(ClassBodyEvaluator.java:72)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        ... 38 more
Caused by: java.lang.ClassNotFoundException: Object
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:79)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:344)
        at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
        at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
        ... 80 more
Caused by: java.lang.ClassNotFoundException: Object
        at java.lang.ClassLoader.findClass(ClassLoader.java:530)
        at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:74)
        ... 87 more

15/06/18 15:36:30 ERROR scheduler.TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
15/06/18 15:36:30 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
15/06/18 15:36:30 INFO scheduler.TaskSchedulerImpl: Cancelling stage 4
15/06/18 15:36:30 INFO scheduler.DAGScheduler: ResultStage 4 (json at <console>:23) failed in 0.054 s
15/06/18 15:36:30 INFO scheduler.DAGScheduler: Job 4 failed: json at <console>:23, took 0.059715 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 18, localhost): java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
        at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:282)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:173)
        at org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:41)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:93)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:92)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.hasNext(DataFrame.scala:1471)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
        at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
        at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
        at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
        at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
        at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835)
        at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063)
        at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:211)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:199)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
        at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
        at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
        at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
        at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
        at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
        at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
        at org.codehaus.janino.ClassBodyEvaluator.<init>(ClassBodyEvaluator.java:72)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        ... 38 more
Caused by: java.lang.ClassNotFoundException: Object
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:79)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:344)
        at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
        at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
        ... 80 more
Caused by: java.lang.ClassNotFoundException: Object
        at java.lang.ClassLoader.findClass(ClassLoader.java:530)
        at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:74)
        ... 87 more

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1285)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1276)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1275)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1275)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:749)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1484)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1445)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{noformat}",,apachespark,davies,hvanhovell,lian cheng,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 18:40:29 UTC 2015,,,,,,,,,,"0|i2g8kf:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"18/Jun/15 23:07;hvanhovell;I have encountered exactly the same problem (I reported it in SPARK-7814).

I think it is a ClassLoader problem related to the REPL, because there are no codegen errors when I use the thriftserver using exactly the same settings (--master local\[\*\] --driver-memory 14G --driver-library-path $HADOOP_NATIVE_LIB --jars custom.jar).;;;","19/Jun/15 07:05;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6898;;;","19/Jun/15 18:40;davies;Issue resolved by pull request 6898
[https://github.com/apache/spark/pull/6898];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ORC data source can only write to the file system defined in Hadoop configuration,SPARK-8458,12838945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,18/Jun/15 21:19,19/Jun/15 05:17,14/Jul/23 06:26,19/Jun/15 05:17,1.4.0,,,,,,,,,,,,SQL,,,,0,,,,,,"To reproduce this issue, we first define {{fs.default.name}} in Hadoop {{core-site.xml}}:
{noformat}
<configuration>
    ...
    <property>
        <name>fs.default.name</name>
        <value>hdfs://localhost:9000</value>
    </property>
    ...
</configuration>
{noformat}
Then execute the following Spark shell snippet:
{code}
sqlContext.range(0, 10).coalesce(1).write.mode(""overwrite"").format(""orc"").save(""file:///tmp/foo"")
{code}
The write job succeeds, but you can only find {{_SUCCESS}} under {{file:///tmp/foo}}. Data files are actually written to HDFS directory {{/tmp/foo/_temporary}} and left uncommitted.

The reason is that, [this line|https://github.com/apache/spark/blob/9b2002722273f98e193ad6cd54c9626292ab27d1/sql/hive/src/main/scala/org/apache/spark/sql/hive/orc/OrcRelation.scala#L113] uses {{Path.toUri.getPath}} rather than {{Path.toString}} to pass the path string to {{OrcRecordWriter}} constructor, and this essentially strips the scheme part of the path.",,apachespark,lian cheng,zzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 05:17:02 UTC 2015,,,,,,,,,,"0|i2g8g7:",9223372036854775807,,,,,yhuai,,,,,,,,,1.4.1,,,,,,,,,,,,,"18/Jun/15 21:19;lian cheng;cc [~zhzhan];;;","18/Jun/15 22:02;zzhan;Thanks [~yhuai]. I didn't cover this test in my local manual test.;;;","19/Jun/15 00:43;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6892;;;","19/Jun/15 05:17;lian cheng;Resolved by https://github.com/apache/spark/pull/6892;;;",,,,,,,,,,,,,,,,,,,,,,,,,
expose jobGroup API in SparkR,SPARK-8452,12838919,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,falaki,falaki,falaki,18/Jun/15 20:04,19/Jun/15 22:53,14/Jul/23 06:26,19/Jun/15 22:52,1.4.0,,,,,,1.4.1,1.5.0,,,,,SparkR,,,,0,,,,,,"Following job management calls are missing in SparkR:
{code}
setJobGroup()
cancelJobGroup()
clearJobGroup()
{code}",,apachespark,falaki,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 22:52:43 UTC 2015,,,,,,,,,,"0|i2g8av:",9223372036854775807,,,,,,,,,,,,,,1.4.1,,,,,,,,,,,,,"18/Jun/15 22:20;apachespark;User 'falaki' has created a pull request for this issue:
https://github.com/apache/spark/pull/6889;;;","19/Jun/15 22:52;shivaram;Issue resolved by pull request 6889
[https://github.com/apache/spark/pull/6889];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSubmitSuite never checks for process exit code,SPARK-8451,12838916,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,18/Jun/15 20:01,19/Jun/15 17:57,14/Jul/23 06:26,19/Jun/15 17:57,1.3.0,,,,,,1.3.2,1.4.1,1.5.0,,,,Spark Submit,Tests,,,0,,,,,,We just never did. If the subprocess throws an exception we just ignore it.,,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 20:12:04 UTC 2015,,,,,,,,,,"0|i2g8a7:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.1,1.5.0,,,,,,,,,,,"18/Jun/15 20:12;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6886;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark write.parquet raises Unsupported datatype DecimalType(),SPARK-8450,12838909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,hoffmann,hoffmann,18/Jun/15 19:34,12/Jul/15 20:36,14/Jul/23 06:26,09/Jul/15 01:23,,,,,,,1.5.0,,,,,,PySpark,SQL,,,0,,,,,,"I'm getting an Exception when I try to save a DataFrame with a DeciamlType as an parquet file

Minimal Example:
{code}
from decimal import Decimal
from pyspark.sql import SQLContext
from pyspark.sql.types import *

sqlContext = SQLContext(sc)
schema = StructType([
    StructField('id', LongType()),
    StructField('value', DecimalType())])
rdd = sc.parallelize([[1, Decimal(""0.5"")],[2, Decimal(""2.9"")]])
df = sqlContext.createDataFrame(rdd, schema)
df.write.parquet(""hdfs://srv:9000/user/ph/decimal.parquet"", 'overwrite')

{code}

Stack Trace
{code}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-19-a77dac8de5f3> in <module>()
----> 1 sr.write.parquet(""hdfs://srv:9000/user/ph/decimal.parquet"", 'overwrite')

/home/spark/spark-1.4.0-bin-hadoop2.6/python/pyspark/sql/readwriter.pyc in parquet(self, path, mode)
    367         :param mode: one of `append`, `overwrite`, `error`, `ignore` (default: error)
    368         """"""
--> 369         return self._jwrite.mode(mode).parquet(path)
    370 
    371     @since(1.4)

/home/spark/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/home/spark/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o361.parquet.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:138)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:114)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:939)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:939)
	at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:332)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:281)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 158 in stage 35.0 failed 4 times, most recent failure: Lost task 158.3 in stage 35.0 (TID 2736, 10.2.160.14): java.lang.RuntimeException: Unsupported datatype DecimalType()
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$fromDataType$2.apply(ParquetTypes.scala:374)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$fromDataType$2.apply(ParquetTypes.scala:318)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetTypes.scala:317)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$4.apply(ParquetTypes.scala:398)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$4.apply(ParquetTypes.scala:397)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.convertFromAttributes(ParquetTypes.scala:396)
	at org.apache.spark.sql.parquet.RowWriteSupport.init(ParquetTableSupport.scala:150)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:278)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:252)
	at org.apache.spark.sql.parquet.ParquetOutputWriter.<init>(newParquet.scala:111)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anon$4.newInstance(newParquet.scala:244)
	at org.apache.spark.sql.sources.DefaultWriterContainer.initWriters(commands.scala:386)
	at org.apache.spark.sql.sources.BaseWriterContainer.executorSideSetup(commands.scala:298)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:142)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:132)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:132)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}

I also tried to set the precision < 18
{code}
schema = StructType([
    StructField('id', LongType()),
    StructField('value', DecimalType(16,2))])
{code}

which raises a different exception
{code}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-23-bba70b7c0805> in <module>()
----> 1 df.write.parquet(""hdfs://srv:9000/user/ph/decimal.parquet"", 'overwrite')

/home/spark/spark-1.4.0-bin-hadoop2.6/python/pyspark/sql/readwriter.pyc in parquet(self, path, mode)
    367         :param mode: one of `append`, `overwrite`, `error`, `ignore` (default: error)
    368         """"""
--> 369         return self._jwrite.mode(mode).parquet(path)
    370 
    371     @since(1.4)

/home/spark/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/home/spark/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o417.parquet.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:138)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:114)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:939)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:939)
	at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:332)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:281)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 159 in stage 41.0 failed 4 times, most recent failure: Lost task 159.3 in stage 41.0 (TID 3211, 10.2.160.14): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:161)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:132)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:132)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal
	at org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:365)
	at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:335)
	at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:321)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.ParquetOutputWriter.write(newParquet.scala:114)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:154)
	... 8 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

{code}
The corresponding Scala Version works

{code}
import org.apache.spark.SparkContext
import org.apache.spark.sql.{ Row, SQLContext }
import org.apache.spark.sql.types.{ DecimalType, IntegerType, StructType, StructField }
 
object ParquetDecimal {
  def main(args: Array[String]) {
    // Connect to Spark
    val sc = new SparkContext()
    val sqlContext = new SQLContext(sc)
 
    val schema = StructType(Seq(StructField(""id"", IntegerType), StructField(""value"", DecimalType(16, 2))))
    val rows = sc.parallelize(Seq(Row(1, BigDecimal(""0.9"")), Row(2, BigDecimal(""2.9""))))
    val df = sqlContext.createDataFrame(rows, schema)
    df.write.parquet(""test.parquet"")
  }
}
{code}",Spark 1.4.0 on Debian,apachespark,davies,hoffmann,sarutak,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 12 20:36:04 UTC 2015,,,,,,,,,,"0|i2g88v:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"30/Jun/15 01:37;apachespark;User 'x1-' has created a pull request for this issue:
https://github.com/apache/spark/pull/7106;;;","30/Jun/15 01:39;x1;When {{createDataFrame}} is called(via *PySpark*), {{CatalystTypeConverters}} convert Decimal to  java.math.BigDecimal.
see: https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystTypeConverters.scala#L71

But, when {{write.parque}} is called, {{MutableRowWriteSupport}} force to cast to Decimal.
So, Exception occured.

I create PR below.
https://github.com/apache/spark/pull/7106;;;","30/Jun/15 19:23;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/7131;;;","09/Jul/15 01:23;davies;Issue resolved by pull request 7131
[https://github.com/apache/spark/pull/7131];;;","12/Jul/15 20:36;hoffmann;I have tried it with todays spark-1.5.0-SNAPSHOT-bin-hadoop2.6 daily build from http://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest/ and was able to save DecimalType(16,2) as parquet in python

Thanks for the quick fix!

;;;",,,,,,,,,,,,,,,,,,,,,,,,
GenerateMutableProjection Exceeds JVM Code Size Limits,SPARK-8443,12838845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,saurfang,saurfang,saurfang,18/Jun/15 16:00,14/Nov/16 19:32,14/Jul/23 06:26,19/Jul/15 04:06,1.4.0,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"GenerateMutableProjection put all expressions columns into a single apply function. When there are a lot of columns, the apply function code size exceeds the 64kb limit, which is a hard limit on jvm that cannot change.

This comes up when we were aggregating about 100 columns using codegen and unsafe feature.

I wrote an unit test that reproduces this issue. 
https://github.com/saurfang/spark/blob/codegen_size_limit/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CodeGenerationSuite.scala

This test currently fails at 2048 expressions. It seems the master is more tolerant than branch-1.4 about this because code is more concise.

While the code on master has changed since branch-1.4, I am able to reproduce the problem in master. For now I hacked my way in branch-1.4 to workaround this problem by wrapping each expression with a separate function then call those functions sequentially in apply. The proper way is probably check the length of the projectCode and break it up as necessary. (This seems to be easier in master actually since we are building code by string rather than quasiquote)

Let me know if anyone has additional thoughts on this, I'm happy to contribute a pull request.

Attaching stack trace produced by unit test
{code}
[info] - code size limit *** FAILED *** (7 seconds, 103 milliseconds)
[info]   com.google.common.util.concurrent.UncheckedExecutionException: org.codehaus.janino.JaninoRuntimeException: Code of method ""(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""SC$SpecificProjection"" grows beyond 64 KB
[info]   at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2263)
[info]   at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
[info]   at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
[info]   at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(CodeGenerationSuite.scala:50)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(CodeGenerationSuite.scala:48)
[info]   at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
[info]   at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
[info]   at scala.collection.immutable.Range.foreach(Range.scala:141)
[info]   at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)
[info]   at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:105)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply$mcV$sp(CodeGenerationSuite.scala:47)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply(CodeGenerationSuite.scala:47)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply(CodeGenerationSuite.scala:47)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuite.run(FunSuite.scala:1555)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info]   at java.lang.Thread.run(Thread.java:745)
[info]   Cause: org.codehaus.janino.JaninoRuntimeException: Code of method ""(Ljava/lang/Object;)Ljava/lang/Object;"" of class ""SC$SpecificProjection"" grows beyond 64 KB
[info]   at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:941)
[info]   at org.codehaus.janino.CodeContext.write(CodeContext.java:874)
[info]   at org.codehaus.janino.CodeContext.writeBranch(CodeContext.java:965)
[info]   at org.codehaus.janino.UnitCompiler.writeBranch(UnitCompiler.java:10261)
[info]   at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:2862)
[info]   at org.codehaus.janino.UnitCompiler.access$4800(UnitCompiler.java:185)
[info]   at org.codehaus.janino.UnitCompiler$8.visitAmbiguousName(UnitCompiler.java:2832)
[info]   at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:3138)
[info]   at org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:2842)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1741)
[info]   at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
[info]   at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
[info]   at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
[info]   at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
[info]   at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
[info]   at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
[info]   at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
[info]   at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
[info]   at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
[info]   at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
[info]   at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
[info]   at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
[info]   at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
[info]   at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
[info]   at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
[info]   at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
[info]   at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
[info]   at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
[info]   at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
[info]   at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
[info]   at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
[info]   at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
[info]   at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
[info]   at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
[info]   at org.codehaus.janino.ClassBodyEvaluator.<init>(ClassBodyEvaluator.java:72)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
[info]   at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
[info]   at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
[info]   at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
[info]   at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
[info]   at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
[info]   at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
[info]   at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(CodeGenerationSuite.scala:50)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(CodeGenerationSuite.scala:48)
[info]   at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
[info]   at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
[info]   at scala.collection.immutable.Range.foreach(Range.scala:141)
[info]   at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)
[info]   at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:105)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply$mcV$sp(CodeGenerationSuite.scala:47)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply(CodeGenerationSuite.scala:47)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply(CodeGenerationSuite.scala:47)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuite.run(FunSuite.scala:1555)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}
",,apachespark,barrybecker4,maropu,saurfang,volkanagun@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9058,,,,,,,SPARK-14138,SPARK-14793,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 14 19:32:45 UTC 2016,,,,,,,,,,"0|i2g7un:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jun/15 01:44;apachespark;User 'saurfang' has created a pull request for this issue:
https://github.com/apache/spark/pull/7076;;;","20/Jul/16 10:55;volkanagun@gmail.com;The same issue for large sql syntax with a lot of unions and iterations over dataframe at spark 1.6.2


Caused by: org.codehaus.janino.JaninoRuntimeException: Code of method ""(Lorg/apache/spark/sql/catalyst/expressions/GeneratedClass$SpecificUnsafeProjection;Lorg/apache/spark/sql/catalyst/InternalRow;)V"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection"" grows beyond 64 KB
	at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:941)
	at org.codehaus.janino.CodeContext.write(CodeContext.java:854)
	at org.codehaus.janino.CodeContext.writeShort(CodeContext.java:959)
	at org.codehaus.janino.UnitCompiler.writeConstantFieldrefInfo(UnitCompiler.java:10279)
	at org.codehaus.janino.UnitCompiler.getfield(UnitCompiler.java:9946)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3322)
	at org.codehaus.janino.UnitCompiler.access$8200(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitFieldAccess(UnitCompiler.java:3282)
	at org.codehaus.janino.Java$FieldAccess.accept(Java.java:3232)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compileContext2(UnitCompiler.java:3190)
	at org.codehaus.janino.UnitCompiler.access$5600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$9.visitFieldAccess(UnitCompiler.java:3152)
	at org.codehaus.janino.Java$FieldAccess.accept(Java.java:3232)
	at org.codehaus.janino.UnitCompiler.compileContext(UnitCompiler.java:3160)
	at org.codehaus.janino.UnitCompiler.compileContext2(UnitCompiler.java:3172)
	at org.codehaus.janino.UnitCompiler.access$5400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$9.visitAmbiguousName(UnitCompiler.java:3150)
	at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:3138)
	at org.codehaus.janino.UnitCompiler.compileContext(UnitCompiler.java:3160)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4367)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:3975)
	at org.codehaus.janino.UnitCompiler.access$6900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitMethodInvocation(UnitCompiler.java:3263)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:2662)
	at org.codehaus.janino.UnitCompiler.access$4400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$7.visitMethodInvocation(UnitCompiler.java:2627)
	at org.codehaus.janino.Java$MethodInvocation.accept(Java.java:3974)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2654)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1643)
	at org.codehaus.janino.UnitCompiler.access$1100(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitExpressionStatement(UnitCompiler.java:936)
	at org.codehaus.janino.Java$ExpressionStatement.accept(Java.java:2097)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
	at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
	at org.codehaus.janino.Java$Block.accept(Java.java:2012)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1742)
	at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
	at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
	at org.codehaus.janino.Java$Block.accept(Java.java:2012)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1745)
	at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
	at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
	at org.codehaus.janino.Java$Block.accept(Java.java:2012)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1745)
	at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:993)
	at org.codehaus.janino.UnitCompiler.access$1000(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitBlock(UnitCompiler.java:935)
	at org.codehaus.janino.Java$Block.accept(Java.java:2012)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1745)
	at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
	at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:597)
	... 46 more;;;","14/Nov/16 19:32;barrybecker4;I see the same error in spark 1.6.3. Is there a workaround? Should this issue be reopened? I am working with a dataset with more than 200 columns. Before 1.6.3 I could not even try this case because of SPARK-16664.
{code}
Caused by: org.codehaus.janino.JaninoRuntimeException: Code of method ""(Lorg/apache/spark/sql/catalyst/InternalRow;Lorg/apache/spark/sql/catalyst/InternalRow;)I"" of class ""org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificOrdering"" grows beyond 64 KB
        at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:941)
        at org.codehaus.janino.CodeContext.write(CodeContext.java:836)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException when select all fields of a parqurt table with nested schema ,SPARK-8441,12838795,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,DoingDone9,DoingDone9,18/Jun/15 13:43,21/Jul/15 06:49,14/Jul/23 06:26,21/Jul/15 06:49,1.4.0,,,,,,,,,,,,SQL,,,,0,,,,,,"Table Info :
{noformat}
create table t1(
key1 int,
key2 String,
key3 int,
key4 array<strcut<a:smallint,b:int,c:int,d:string,e:int,f:string,g:int,h:string,i:string,j:smallint,k:smallint,h:string,i:bigint,j:bigint,k:smallint,l:bigint>>,
key5 array<strcut<a1:smallint,b1:int,c:int,d1:string,e1:int,f1:string,g1:int,h1:string,i1:string,j1:smallint,k1:smallint,h1:string,i1:bigint,j1:bigint,k1:smallint,l1:bigint>>)
partitioned by (hour int,last_num string)
stored as parquet
{noformat}

Question :
{noformat}
When i set spark.sql.hive.convertMetastoreParquet=true, and run sql ""select * from table t1"", then i get a exception :

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, 169.10.35.34): parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file hdfs://169.10.35.33:9000/user/hive/warehouse/cdr_voice_call/hour=10/last_num=0/000017_0
        at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:213)
        at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:204)
        at org.apache.spark.sql.sources.SqlNewHadoopRDD$$anon$1.hasNext(SqlNewHadoopRDD.scala:163)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$3.apply(SparkPlan.scala:143)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$3.apply(SparkPlan.scala:143)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.elementData(ArrayList.java:371)
        at java.util.ArrayList.get(ArrayList.java:384)
        at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
        at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
        at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
        at parquet.io.PrimitiveColumnIO.getLast(PrimitiveColumnIO.java:80)
        at parquet.io.PrimitiveColumnIO.isLast(PrimitiveColumnIO.java:74)
        at parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:290)
        at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:131)
        at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:96)
        at parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:136)
        at parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:96)
        at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:126)
        at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:193)
        ... 28 more

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, 169.10.35.34): parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file hdfs://169.10.35.33:9000/user/hive/warehouse/cdr_voice_call/hour=10/last_num=0/000017_0
        at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:213)
        at parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:204)
        at org.apache.spark.sql.sources.SqlNewHadoopRDD$$anon$1.hasNext(SqlNewHadoopRDD.scala:163)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$3.apply(SparkPlan.scala:143)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$3.apply(SparkPlan.scala:143)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.elementData(ArrayList.java:371)
        at java.util.ArrayList.get(ArrayList.java:384)
        at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
        at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
        at parquet.io.GroupColumnIO.getLast(GroupColumnIO.java:95)
        at parquet.io.PrimitiveColumnIO.getLast(PrimitiveColumnIO.java:80)
        at parquet.io.PrimitiveColumnIO.isLast(PrimitiveColumnIO.java:74)
        at parquet.io.RecordReaderImplementation.<init>(RecordReaderImplementation.java:290)
        at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:131)
        at parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:96)
        at parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:136)
        at parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:96)
        at parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:126)
        at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:193)
        ... 28 more
{noformat}

",,DoingDone9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-18 13:43:58.0,,,,,,,,,,"0|i2g7jr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using directory path without wildcard for filename slow for large number of files with wholeTextFiles and binaryFiles,SPARK-8437,12838740,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,ewanleith,ewanleith,18/Jun/15 09:48,13/Oct/16 16:47,14/Jul/23 06:26,30/Jun/15 17:07,1.3.1,1.4.0,,,,,1.4.1,1.5.0,,,,,Input/Output,,,,0,,,,,,"When calling wholeTextFiles or binaryFiles with a directory path with 10,000s of files in it, Spark hangs for a few minutes before processing the files.

If you add a * to the end of the path, there is no delay.

This happens for me on Spark 1.3.1 and 1.4 on the local filesystem, HDFS, and on S3.

To reproduce, create a directory with 50,000 files in it, then run:


val a = sc.binaryFiles(""file:/path/to/files/"")
a.count()

val b = sc.binaryFiles(""file:/path/to/files/*"")
b.count()

and monitor the different startup times.

For example, in the spark-shell these commands are pasted in together, so the delay at f.count() is from 10:11:08 t- 10:13:29 to output ""Total input paths to process : 49999"", then until 10:15:42 to being processing files:

scala> val f = sc.binaryFiles(""file:/home/ewan/large/"")
15/06/18 10:11:07 INFO MemoryStore: ensureFreeSpace(160616) called with curMem=0, maxMem=278019440
15/06/18 10:11:07 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 156.9 KB, free 265.0 MB)
15/06/18 10:11:08 INFO MemoryStore: ensureFreeSpace(17282) called with curMem=160616, maxMem=278019440
15/06/18 10:11:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.9 KB, free 265.0 MB)
15/06/18 10:11:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:40430 (size: 16.9 KB, free: 265.1 MB)
15/06/18 10:11:08 INFO SparkContext: Created broadcast 0 from binaryFiles at <console>:21
f: org.apache.spark.rdd.RDD[(String, org.apache.spark.input.PortableDataStream)] = file:/home/ewan/large/ BinaryFileRDD[0] at binaryFiles at <console>:21

scala> f.count()
15/06/18 10:13:29 INFO FileInputFormat: Total input paths to process : 49999
15/06/18 10:15:42 INFO FileInputFormat: Total input paths to process : 49999
15/06/18 10:15:42 INFO CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
15/06/18 10:15:42 INFO SparkContext: Starting job: count at <console>:24
15/06/18 10:15:42 INFO DAGScheduler: Got job 0 (count at <console>:24) with 49999 output partitions (allowLocal=false)
15/06/18 10:15:42 INFO DAGScheduler: Final stage: ResultStage 0(count at <console>:24)
15/06/18 10:15:42 INFO DAGScheduler: Parents of final stage: List()

Adding a * to the end of the path removes the delay:


scala> val f = sc.binaryFiles(""file:/home/ewan/large/*"")
15/06/18 10:08:29 INFO MemoryStore: ensureFreeSpace(160616) called with curMem=0, maxMem=278019440
15/06/18 10:08:29 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 156.9 KB, free 265.0 MB)
15/06/18 10:08:29 INFO MemoryStore: ensureFreeSpace(17309) called with curMem=160616, maxMem=278019440
15/06/18 10:08:29 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 16.9 KB, free 265.0 MB)
15/06/18 10:08:29 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:42825 (size: 16.9 KB, free: 265.1 MB)
15/06/18 10:08:29 INFO SparkContext: Created broadcast 0 from binaryFiles at <console>:21
f: org.apache.spark.rdd.RDD[(String, org.apache.spark.input.PortableDataStream)] = file:/home/ewan/large/* BinaryFileRDD[0] at binaryFiles at <console>:21

scala> f.count()
15/06/18 10:08:32 INFO FileInputFormat: Total input paths to process : 49999
15/06/18 10:08:33 INFO FileInputFormat: Total input paths to process : 49999
15/06/18 10:08:35 INFO CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
15/06/18 10:08:35 INFO SparkContext: Starting job: count at <console>:24
15/06/18 10:08:35 INFO DAGScheduler: Got job 0 (count at <console>:24) with 49999 output partitions 

","Ubuntu 15.04 + local filesystem
Amazon EMR + S3 + HDFS",andrewor14,apachespark,dougb,ewanleith,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 13 16:47:35 UTC 2016,,,,,,,,,,"0|i2g77j:",9223372036854775807,,,,,,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"18/Jun/15 10:24;srowen;Yeah that's a weird one, but I'm pretty sure this is a Hadoop API phenomenon rather than Spark-related. I assume that the glob can be pushed down further rather than go manually list and expand directories remotely or something. It might not be something Spark can do anything about, but have a look. At least you could propose a doc change to suggest that the glob expression is more desirable.;;;","18/Jun/15 10:30;ewanleith;Thanks, I wasn't sure if it was Hadoop or Spark specific, initially I thought it was S3 related but it happens all over.

If it is Hadoop, I don't know if it would be feasible for Spark to check if a directory has been given and add a wildcard in the background, that might not give the desired effect, but otherwise there's various doc changes to make.

I've just tried this with textFile(""/path/to/files/"") and got the same issue, so I assume it is a hadoop thing, and documentation changes might be the best option;;;","26/Jun/15 09:11;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7036;;;","30/Jun/15 00:23;andrewor14;The merged PR involves only documentation changes. I don't think there is another Spark fix that can change this, as this issue is fundamental to the underlying file system. I am closing this.;;;","30/Jun/15 01:33;andrewor14;Re-opening because the patch was reverted.;;;","30/Jun/15 11:57;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7126;;;","13/Oct/16 16:47;stevel@apache.org;Just came across by way of comments in the source. This *shouldn't* happen; the glob code ought to recognise when there is no wildcard and exit early —faster than if there was a wildcard. If its taking longer, then the full list process is making a mess of things, or somehow the result generation is playing up. If it was S3 only I'd blame the S3 APIs, but this sounds like S3 just amplifies a problem which may exist already

How big was the directory where this surfaced? Was it deep, wide or deep & wide?;;;",,,,,,,,,,,,,,,,,,,,,,
Fix hashCode and equals() of BinaryType in Row,SPARK-8432,12838709,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,18/Jun/15 07:59,23/Jun/15 18:56,14/Jul/23 06:26,23/Jun/15 18:56,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"The hashCode of BinaryType should be consistent with the bytes in it, and equals() should compare the bytes of BinaryType.",,apachespark,davies,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 18:56:07 UTC 2015,,,,,,,,,,"0|i2g70n:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.5.0,,,,,,,,,,,,,"18/Jun/15 08:03;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6876;;;","23/Jun/15 18:56;marmbrus;Issue resolved by pull request 6876
[https://github.com/apache/spark/pull/6876];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ExternalShuffleBlockResolver should support UnsafeShuffleManager,SPARK-8430,12838699,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lianhuiwang,lianhuiwang,lianhuiwang,18/Jun/15 07:03,17/May/20 18:31,14/Jul/23 06:26,30/Dec/15 21:50,1.4.0,,,,,,1.4.2,1.5.0,,,,,Shuffle,Spark Core,,,0,,,,,,,,apachespark,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 07:37:07 UTC 2015,,,,,,,,,,"0|i2g6yn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/15 07:37;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6873;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimSort Comparison method violates its general contract with CLUSTER BY,SPARK-8428,12838675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sameerag,nemccarthy,nemccarthy,18/Jun/15 04:46,26/May/16 22:51,14/Jul/23 06:26,26/May/16 22:51,1.4.0,,,,,,1.6.2,2.0.0,,,,,SQL,,,,3,,,,,,"Running an SQL query that has a sub query and multiple left joins fails when there is a CLUSTER BY (which implies a sortBy). This gives the following stack trace; 


{code}
Job aborted due to stage failure: Task 118 in stage 4.0 failed 4 times, most recent failure: Lost task 118.3 in stage 4.0 (TID 18392, node142): java.lang.IllegalArgumentException: Comparison method violates its general contract!
	at org.apache.spark.util.collection.TimSort$SortState.mergeHi(TimSort.java:900)
	at org.apache.spark.util.collection.TimSort$SortState.mergeAt(TimSort.java:509)
	at org.apache.spark.util.collection.TimSort$SortState.mergeCollapse(TimSort.java:435)
	at org.apache.spark.util.collection.TimSort$SortState.access$200(TimSort.java:307)
	at org.apache.spark.util.collection.TimSort.sort(TimSort.java:135)
	at org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)
	at org.apache.spark.util.collection.PartitionedPairBuffer.partitionedDestructiveSortedIterator(PartitionedPairBuffer.scala:70)
	at org.apache.spark.util.collection.ExternalSorter.partitionedIterator(ExternalSorter.scala:690)
	at org.apache.spark.util.collection.ExternalSorter.iterator(ExternalSorter.scala:708)
	at org.apache.spark.sql.execution.ExternalSort$$anonfun$doExecute$6$$anonfun$apply$7.apply(basicOperators.scala:222)
	at org.apache.spark.sql.execution.ExternalSort$$anonfun$doExecute$6$$anonfun$apply$7.apply(basicOperators.scala:218)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
{code}

The query looks like;

{code}
 val df = sqlContext.sql(""""""SELECT CID
|,	PW_END_DATE
|,	PROD_NBR_KEY
|,	SUM(CASE WHEN SUBST_IDX = 1 THEN L13W_SALE END) AS SUB1_L13W_SALE
|FROM
|(SELECT	BASE.CID
|,	BASE.PW_END_DATE
|,	BASE.PROD_NBR_KEY
|,	SUBN.SUBST_IDX
|,	CASE WHEN IDX.PW_END_DATE BETWEEN DATE_SUB(BASE.PW_END_DATE, 13*7 - 1) AND BASE.PW_END_DATE THEN IDX.TOT_AMT_INCLD_GST END AS L13W_SALE
|FROM TESTBASE BASE
|LEFT JOIN TABLX SUBN
|ON BASE.PROD_NBR_KEY = SUBN.PROD_NBR_KEY AND SUBN.SUBST_IDX <= 3
|LEFT JOIN TABLEF IDX
|ON BASE.CRN = IDX.CRN
|AND SUBN.CROSS_PROD_NBR = IDX.PROD_NBR_KEY
|) SUBSPREM
| GROUP BY CRN, PW_END_DATE, PROD_NBR_KEY"""""".stripMargin)
{code}
",Oracle Java 7 ,apachespark,dvoskin,jameszhouyi,Joshita,Jvan,lianhuiwang,nemccarthy,paulbarber,rxin,sameerag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6009,,,,SPARK-13850,,,SPARK-6009,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 26 20:49:05 UTC 2016,,,,,,,,,,"0|i2g6tb:",9223372036854775807,,,,,,,,,,,,,,1.6.2,2.0.0,,,,,,,,,,,,"20/Jul/15 00:46;rxin;[~nemccarthy] to be sure - this query doesn't have anything to do with non-deterministic expressions? Are there NaN values in there?;;;","23/Feb/16 17:43;Joshita;Hi, 

I am facing a similar issue where, when I try to do multiple joins I get the same exception as above.

For testing i just added system property to use useLegacyMergeSort and it worked fine.
Is the Timsort issue still there with spark 1.6 ? 

Using spark version 1.6
Java version 1.8.45;;;","06/May/16 02:16;jameszhouyi;We found the similar issue with Spark 1.6.1 in our larger data size test..I posted the details like below. Then we try to increase the spark.sql.shuffle.partitions to resolve it. 

{code}
CREATE TABLE q26_spark_sql_run_query_0_temp (
  cid  BIGINT,
  id1  double,
  id2  double,
  id3  double,
  id4  double,
  id5  double,
  id6  double,
  id7  double,
  id8  double,
  id9  double,
  id10 double,
  id11 double,
  id12 double,
  id13 double,
  id14 double,
  id15 double
)

INSERT INTO TABLE q26_spark_sql_run_query_0_temp
SELECT
  ss.ss_customer_sk AS cid,
  count(CASE WHEN i.i_class_id=1  THEN 1 ELSE NULL END) AS id1,
  count(CASE WHEN i.i_class_id=2  THEN 1 ELSE NULL END) AS id2,
  count(CASE WHEN i.i_class_id=3  THEN 1 ELSE NULL END) AS id3,
  count(CASE WHEN i.i_class_id=4  THEN 1 ELSE NULL END) AS id4,
  count(CASE WHEN i.i_class_id=5  THEN 1 ELSE NULL END) AS id5,
  count(CASE WHEN i.i_class_id=6  THEN 1 ELSE NULL END) AS id6,
  count(CASE WHEN i.i_class_id=7  THEN 1 ELSE NULL END) AS id7,
  count(CASE WHEN i.i_class_id=8  THEN 1 ELSE NULL END) AS id8,
  count(CASE WHEN i.i_class_id=9  THEN 1 ELSE NULL END) AS id9,
  count(CASE WHEN i.i_class_id=10 THEN 1 ELSE NULL END) AS id10,
  count(CASE WHEN i.i_class_id=11 THEN 1 ELSE NULL END) AS id11,
  count(CASE WHEN i.i_class_id=12 THEN 1 ELSE NULL END) AS id12,
  count(CASE WHEN i.i_class_id=13 THEN 1 ELSE NULL END) AS id13,
  count(CASE WHEN i.i_class_id=14 THEN 1 ELSE NULL END) AS id14,
  count(CASE WHEN i.i_class_id=15 THEN 1 ELSE NULL END) AS id15
FROM store_sales ss
INNER JOIN item i
  ON (ss.ss_item_sk = i.i_item_sk
  AND i.i_category IN ('Books')
  AND ss.ss_customer_sk IS NOT NULL
)
GROUP BY ss.ss_customer_sk
HAVING count(ss.ss_item_sk) > 5
ORDER BY cid
{code}

{code}
16/05/05 14:50:03 WARN scheduler.TaskSetManager: Lost task 12.0 in stage 162.0 (TID 15153, node6): java.lang.IllegalArgumentException: Comparison method violates its
general contract!
        at org.apache.spark.util.collection.TimSort$SortState.mergeLo(TimSort.java:794)
        at org.apache.spark.util.collection.TimSort$SortState.mergeAt(TimSort.java:525)
        at org.apache.spark.util.collection.TimSort$SortState.mergeCollapse(TimSort.java:453)
        at org.apache.spark.util.collection.TimSort$SortState.access$200(TimSort.java:325)
        at org.apache.spark.util.collection.TimSort.sort(TimSort.java:153)
        at org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)
        at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.getSortedIterator(UnsafeInMemorySorter.java:228)
        at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:186)
        at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:175)
        at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:249)
        at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:83)
        at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:295)
        at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:330)
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:91)
        at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:168)
        at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:90)
        at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:64)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:728)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:728)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code};;;","26/May/16 20:49;apachespark;User 'sameeragarwal' has created a pull request for this issue:
https://github.com/apache/spark/pull/13336;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent behavior with Dataframe Timestamp between 1.3.1 and 1.4.0,SPARK-8420,12838639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,yipjustin,yipjustin,18/Jun/15 00:55,22/Jun/15 18:36,14/Jul/23 06:26,22/Jun/15 17:45,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"I am trying out 1.4.0 and notice there are some differences in behavior with Timestamp between 1.3.1 and 1.4.0. 

In 1.3.1, I can compare a Timestamp with string.
{code}
scala> val df = sqlContext.createDataFrame(Seq((1, Timestamp.valueOf(""2015-01-01 00:00:00"")), (2, Timestamp.valueOf(""2014-01-01 00:00:00""))))
...
scala> df.filter($""_2"" <= ""2014-06-01"").show
...
_1 _2                  
2  2014-01-01 00:00:...
{code}

However, in 1.4.0, the filter is always false:
{code}
scala> val df = sqlContext.createDataFrame(Seq((1, Timestamp.valueOf(""2015-01-01 00:00:00"")), (2, Timestamp.valueOf(""2014-01-01 00:00:00""))))
df: org.apache.spark.sql.DataFrame = [_1: int, _2: timestamp]

scala> df.filter($""_2"" <= ""2014-06-01"").show
+--+--+
|_1|_2|
+--+--+
+--+--+
{code}


Not sure if that is intended, but I cannot find any doc mentioning these inconsistencies.",,apachespark,dougb,marmbrus,yhuai,yipjustin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 18:36:16 UTC 2015,,,,,,,,,,"0|i2g6lb:",9223372036854775807,,,,,yhuai,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"18/Jun/15 00:55;yipjustin;[~yhuai];;;","18/Jun/15 00:58;marmbrus;I've seen this same problem.  I believe the issue is that we no longer cast the date column to a string.;;;","18/Jun/15 22:02;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/6888;;;","18/Jun/15 22:32;marmbrus;Note that the fix does also have slight change in semantics from 1.4.0, but I think its worth it to get back support for comparing with partial timestamps.  Now when doing equality comparisons you need to use the rounded timestamp (i.e. {{time='1969-12-31 16:00:00'}} instead of {{time='1969-12-31 16:00:00.0'}}).  This does not apply if the users casts manually.;;;","19/Jun/15 20:27;marmbrus;This is no longer true as we now special case equality.;;;","20/Jun/15 00:40;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6914;;;","20/Jun/15 00:51;yhuai;Master has been fixed by https://github.com/apache/spark/pull/6888. ;;;","20/Jun/15 00:51;yhuai;Will resolve this one after 1.4 backport is merged.;;;","22/Jun/15 06:20;apachespark;User 'navis' has created a pull request for this issue:
https://github.com/apache/spark/pull/6931;;;","22/Jun/15 17:45;yhuai;Issue resolved by pull request 6914
[https://github.com/apache/spark/pull/6914];;;","22/Jun/15 17:46;yhuai;1.4 backport (https://github.com/apache/spark/pull/6914) has been merged.;;;","22/Jun/15 17:55;yhuai;[~marmbrus] Do we need to mention it in our release note? If not, I will remove the {{releasenotes}} label.;;;","22/Jun/15 18:36;marmbrus;Not anymore, now that we don't change the equality semantics.  Tag removed.;;;",,,,,,,,,,,,,,,,
Thread dump page should highlight Spark executor threads,SPARK-8416,12838603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,joshrosen,joshrosen,17/Jun/15 22:25,04/Aug/15 01:21,14/Jul/23 06:26,04/Aug/15 01:21,,,,,,,1.5.0,,,,,,Web UI,,,,0,,,,,,"On the Spark thread dump page, it's hard to pick out executor threads from other system threads.  The UI should employ some color coding or highlighting to make this more apparent.",,apachespark,codingcat,joshrosen,pwendell,sb58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 04 01:21:08 UTC 2015,,,,,,,,,,"0|i2g6dj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/15 20:01;pwendell;It would also be nice to put those threads first in the list.;;;","30/Jul/15 22:30;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/7808;;;","04/Aug/15 01:21;joshrosen;Issue resolved by pull request 7808
[https://github.com/apache/spark/pull/7808];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Jenkins compilation spends lots of time re-resolving dependencies and waiting to acquire Ivy cache lock,SPARK-8415,12838572,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,17/Jun/15 19:49,12/Jul/15 23:12,14/Jul/23 06:26,12/Jul/15 23:11,,,,,,,,,,,,,Build,Project Infra,,,0,,,,,,"When watching a pull request build, I noticed that the compilation + packaging + test compilation phases spent huge amounts of time waiting to acquire the Ivy cache lock.  We should see whether we can tell SBT to skip the resolution steps for some of these commands, since this could speed up the compilation process when Jenkins is heavily loaded.",,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 12 23:12:28 UTC 2015,,,,,,,,,,"0|i2g66v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/15 04:51;joshrosen;Here's an example of a build that spent lots of time contended on the lock: https://amplab.cs.berkeley.edu/jenkins/view/Pull%20Request%20Builders/job/SparkPullRequestBuilder/36086/consoleFull;;;","30/Jun/15 06:22;joshrosen;It looks like this problem occurs when a huge number of retests get queued up and triggered all at once.  In these cases, we can end up with 2+ builds proceeding in lockstep on the same Jenkins worker, leading to bad thrashing while trying to acquire the Ivy lock.  When 4+ pull request builds fire off on the same machine within a minute or two of each other, things can get really bad.;;;","12/Jul/15 23:11;joshrosen;I figured out how to configure AMPLab Jenkins to use a separate ivy cache for each pull request builder workspace.  In the Jenkins environment / properties injection, I adeded the following lines

{code}
HOME=/home/sparkivy/${JOB_NAME}_${EXECUTOR_NUMBER}
SBT_OPTS=-Duser.home=/home/sparkivy/${JOB_NAME}_${EXECUTOR_NUMBER} -Dsbt.ivy.home=/home/sparkivy/${JOB_NAME}_${EXECUTOR_NUMBER}/.ivy2
{code}

Here, {{/home/sparkivy}} is a directory that's outside of the build workspace so it won't be deleted by the {{git clean -fdx}} in our Jenkins build.  The substitutions ensure that each build gets its own independent directory.  I'm going to mark this issue as resolved since I'm switching the main SparkPullRequestBuilder to use this configuration change. ;;;","12/Jul/15 23:12;joshrosen;Oh, and I also added a {{mkdir -p ""$HOME""}} to the execute shell command.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Ensure ContextCleaner actually triggers clean ups,SPARK-8414,12838553,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,17/Jun/15 18:39,02/Dec/15 03:37,14/Jul/23 06:26,02/Dec/15 03:37,1.4.0,,,,,,1.6.0,,,,,,Spark Core,,,,0,,,,,,"Right now it cleans up old references only through natural GCs, which may not occur if the driver has infinite RAM. We should do a periodic GC to make sure that we actually do clean things up. Something like once per 30 minutes seems relatively inexpensive.",,andrewor14,apachespark,codingcat,joshrosen,l90006322,marmbrus,rssvihla,sb58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 02 03:37:04 UTC 2015,,,,,,,,,,"0|i2g62v:",9223372036854775807,,,,,,,,,,,,,,1.6.0,,,,,,,,,,,,,"29/Jul/15 07:13;joshrosen;I think the title here should refer to ContextCleaner instead of ClosureCleaner (I always mix them up myself when speaking).

I think we should really aim to get this in for 1.5.0, so I'm going to bump this to critical. I may take a shot at it this week.;;;","29/Jul/15 17:46;andrewor14;Oops, you're right. I always mix them up...;;;","01/Dec/15 05:11;marmbrus;Still planning to do this for 1.6?;;;","01/Dec/15 19:43;andrewor14;I'll submit a patch today.;;;","01/Dec/15 19:43;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/10070;;;","02/Dec/15 03:37;joshrosen;Issue resolved by pull request 10070
[https://github.com/apache/spark/pull/10070];;;",,,,,,,,,,,,,,,,,,,,,,,
Hive VersionsSuite RuntimeException,SPARK-8410,12838471,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,brkyvz,josam,josam,17/Jun/15 13:30,04/Jul/15 07:27,14/Jul/23 06:26,30/Jun/15 00:27,1.3.1,1.4.0,,,,,1.4.1,1.5.0,,,,,SQL,,,,1,,,,,,"While testing Spark Project Hive, there are RuntimeExceptions as follows,

VersionsSuite:
- success sanity check *** FAILED ***
  java.lang.RuntimeException: [download failed: org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle), download failed: org.codehaus.groovy#groovy-all;2.1.6!groovy-all.jar, download failed: asm#asm;3.2!asm.jar]
  at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:978)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$3.apply(IsolatedClientLoader.scala:62)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$3.apply(IsolatedClientLoader.scala:62)
  at org.apache.spark.sql.catalyst.util.package$.quietly(package.scala:38)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$.org$apache$spark$sql$hive$client$IsolatedClientLoader$$downloadVersion(IsolatedClientLoader.scala:61)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$1.apply(IsolatedClientLoader.scala:44)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$1.apply(IsolatedClientLoader.scala:44)
  at scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:189)
  at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:91)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$.forVersion(IsolatedClientLoader.scala:44)
  ...

The tests are executed with the following set of options,

build/mvn --pl sql/hive --fail-never -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 test

Adding the following dependencies in the ""spark/sql/hive/pom.xml""  file solves this issue,

< 	<dependency>
< 		<groupId>org.jboss.netty</groupId>
< 		<artifactId>netty</artifactId>
< 		<version>3.2.2.Final</version>
< 	        <scope>test</scope>
< 	</dependency>
< 	<dependency>
< 		<groupId>org.codehaus.groovy</groupId>
< 		<artifactId>groovy-all</artifactId>
< 		<version>2.1.6</version>
< 		<scope>test</scope>
< 	</dependency>
< 
< 	<dependency>
< 		<groupId>asm</groupId>
< 		<artifactId>asm</artifactId>
< 		<version>3.2</version>
< 	        <scope>test</scope>
< 	</dependency>
< 

The question is, Is this the correct way to fix this runtimeException ?
If yes, Can a pull request fix this issue permanently ?
If not, suggestions please.

Updates:
The above mentioned quick fix is not working with the latest 1.4 because of
this pull commits :
 [SPARK-8095] Resolve dependencies of --packages in local ivy cache #6788 
https://github.com/apache/spark/pull/6788

Due to this above commit, now the lookup directories during testing phase
has changed as follows,
:: problems summary ::
:::: WARNINGS
		[NOT FOUND  ] org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle) (2ms)

	==== local-m2-cache: tried

	  file:/home/joe/sparkibmsoe/spark/sql/hive/dummy/.m2/repository/org/jboss/netty/netty/3.2.2.Final/netty-3.2.2.Final.jar

		[NOT FOUND  ] org.codehaus.groovy#groovy-all;2.1.6!groovy-all.jar (0ms)

	==== local-m2-cache: tried

	  file:/home/joe/sparkibmsoe/spark/sql/hive/dummy/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.jar

		[NOT FOUND  ] asm#asm;3.2!asm.jar (0ms)

	==== local-m2-cache: tried

	  file:/home/joe/sparkibmsoe/spark/sql/hive/dummy/.m2/repository/asm/asm/3.2/asm-3.2.jar

		::::::::::::::::::::::::::::::::::::::::::::::

		::              FAILED DOWNLOADS            ::

		:: ^ see resolution messages for details  ^ ::

		::::::::::::::::::::::::::::::::::::::::::::::

		:: org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle)

		:: org.codehaus.groovy#groovy-all;2.1.6!groovy-all.jar

		:: asm#asm;3.2!asm.jar

		::::::::::::::::::::::::::::::::::::::::::::::
","IBM Power system - P7
running Ubuntu 14.04LE
",apachespark,aroberts,brkyvz,josam,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 09:13:23 UTC 2015,,,,,,,,,,"0|i2g5lb:",9223372036854775807,,,,,,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"17/Jun/15 18:27;srowen;Did you {{install}} the artifacts before this? because you're trying to only test a submodule.
If so, what about if you add the {{hive-thriftserver}} profile to both the build and test commands?;;;","18/Jun/15 00:48;marmbrus;It looks like either a transient error, or you are unable to download jars from maven central.;;;","18/Jun/15 07:15;josam;Thanks for looking into this query.

Yes. I did install the artifacts before proceeding with testing the sql/hive submodule.

I tried with -Phive-thriftserver for both build & test and still no success. The jars are not getting downloaded automatically into the m2 repository.

I did notice that the respective m2 repository directories are getting created w/o the jar file during the install stage as follows,

joe@soe10-vm7:~/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6$ ls -lt
total 28
-rw-rw-r-- 1 joe joe    40 Jun 18 02:38 groovy-all-2.1.6.pom.sha1
-rw-rw-r-- 1 joe joe   156 Jun 18 02:38 _maven.repositories
-rw-rw-r-- 1 joe joe 18038 Jun 18 02:38 groovy-all-2.1.6.pom

[INFO] ------------------------------------------------------------------------
[INFO] Building Spark Project Hive 1.4.0
[INFO] ------------------------------------------------------------------------
Downloading: https://repo1.maven.org/maven2/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.pom
4/18 KB   ^M8/18 KB   ^M12/18 KB   ^M16/18 KB   ^M18/18 KB   ^M           ^M
Downloaded: https://repo1.maven.org/maven2/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.pom (18 KB at 9.8 KB/sec)

   Its the same case for the other 2 dependencies i.e. asm#asm 3.2 & org.jboss.netty#netty 3.2.2.Final

   Do we need to explicitly add these dependencies in the ""spark/sql/hive/pom.xml"" for the respective jars to be downloaded (or)
   The jars will get downloaded by the SparkSubmit.scala(resolveMavenCoordinates)  ?

   Around 29 testcases are impacted due to the missing jars.;;;","18/Jun/15 07:20;josam;Michael, I noticed the same even when testing the RC's of 1.4.1. So it fails always.

Since others files are getting downloaded w/o any issue from the maven central, we can rule out the possibility of such issue.;;;","18/Jun/15 20:05;marmbrus;Does the maven integration work for you in other places?  Like when launching the spark shell?

Details under Advanced Dependency Management:
https://spark.apache.org/docs/latest/submitting-applications.html
;;;","19/Jun/15 14:17;josam;
My findings so far,

1. In most of our machines where it runs successfully, these jars(netty-3.2.2.Final.jar, groovy-all-2.1.6.jar, asm-3.2.jar) are already found in 
the following locations,

~/.ivy2/jars 
~/.ivy2/cache 
~/.m2/repository/

In a fresh installation, these jars are not found and need to be downloaded explicitly from the maven repo. But its not 
triggered when spark is built/package/install/test since these dependencies seems to be missed out in one of the pom.xml(Please correct me if I'm wrong )

2. To confirm it, clear up the jars from the above listed directories, it will consistently
fail to retrieve the jars from the repositories in all the machines.

3. The following code throws the exception while trying to resolve the MavenCoordinates by
the ""SparkSubmitUtils.resolveMavenCoordinates"" call,

>>>
        // resolve dependencies
        val rr: ResolveReport = ivy.resolve(md, resolveOptions)
        if (rr.hasError) {
          throw new RuntimeException(rr.getAllProblemMessages.toString)
        }
<<<


Here is a verbose output of the call,
        

Ivy Default Cache set to: /home/joe/.ivy2/cache
The jars for the packages stored in: /home/joe/.ivy2/jars
http://www.datanucleus.org/downloads/maven2 added as a remote repository with the name: repo-1
org.apache.hive#hive-metastore added as a dependency
org.apache.hive#hive-exec added as a dependency
org.apache.hive#hive-common added as a dependency
org.apache.hive#hive-serde added as a dependency
com.google.guava#guava added as a dependency
org.apache.hadoop#hadoop-client added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found org.apache.hive#hive-metastore;0.13.1 in central
	found org.apache.hive#hive-serde;0.13.1 in central
	found org.apache.hive#hive-common;0.13.1 in central
	found org.apache.hive#hive-shims;0.13.1 in central
	found org.apache.hive.shims#hive-shims-common;0.13.1 in central
...
..
..
.
:: resolution report :: resolve 4149ms :: artifacts dl 100ms
	:: modules in use:
	antlr#antlr;2.7.7 from local-m2-cache in [default]
	aopalliance#aopalliance;1.0 from local-m2-cache in [default]
	asm#asm;3.2 from local-m2-cache in [default]
	com.google.code.findbugs#jsr305;1.3.9 from local-m2-cache in [default]
	com.google.guava#guava;14.0.1 from local-m2-cache in [default]
	com.google.inject#guice;3.0 from local-m2-cache in [default]
...
..
..
.
	:: evicted modules:
	log4j#log4j;1.2.16 by [log4j#log4j;1.2.17] in [default]
	com.google.guava#guava;11.0.2 by [com.google.guava#guava;14.0.1] in [default]
	commons-lang#commons-lang;2.4 by [commons-lang#commons-lang;2.6] in [default]
	commons-collections#commons-collections;3.1 by [commons-collections#commons-collections;3.2.1] in [default]
	commons-httpclient#commons-httpclient;3.0.1 by [commons-httpclient#commons-httpclient;3.1] in [default]
	org.codehaus.jackson#jackson-core-asl;1.8.8 by [org.codehaus.jackson#jackson-core-asl;1.9.2] in [default]
	org.codehaus.jackson#jackson-mapper-asl;1.8.8 by [org.codehaus.jackson#jackson-mapper-asl;1.9.2] in [default]
	org.apache.avro#avro;1.7.4 by [org.apache.avro#avro;1.7.5] in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |  108  |   0   |   0   |   8   ||  100  |   0   |
	---------------------------------------------------------------------

:: problems summary ::
:::: WARNINGS
		[NOT FOUND  ] org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle) (2ms)

	==== local-m2-cache: tried

	  file:/home/joe/.m2/repository/org/jboss/netty/netty/3.2.2.Final/netty-3.2.2.Final.jar

		[NOT FOUND  ] org.codehaus.groovy#groovy-all;2.1.6!groovy-all.jar (0ms)

	==== local-m2-cache: tried

	  file:/home/joe/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.jar

		[NOT FOUND  ] asm#asm;3.2!asm.jar (0ms)

	==== local-m2-cache: tried

	  file:/home/joe/.m2/repository/asm/asm/3.2/asm-3.2.jar

		::::::::::::::::::::::::::::::::::::::::::::::

		::              FAILED DOWNLOADS            ::

		:: ^ see resolution messages for details  ^ ::

		::::::::::::::::::::::::::::::::::::::::::::::

		:: org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle)

		:: org.codehaus.groovy#groovy-all;2.1.6!groovy-all.jar

		:: asm#asm;3.2!asm.jar

		::::::::::::::::::::::::::::::::::::::::::::::

Kindly help me find a solution. 




;;;","24/Jun/15 13:35;josam;It fails with both OpenJDK and IBM JDK. ;;;","26/Jun/15 09:21;srowen;Delete your repos and try again? you may have bad info in your local repo that's causing it to think it has a valid copy downloaded when it doesn't. I suspect this is local to you or else it would fail for everyone, and it doesn't.;;;","26/Jun/15 09:32;josam;I will definitely give one more try after cleaning up the repo and update the JIRA.

Few folks in IBM are facing the same issue specially after a fresh installation(respective jars are not found already in their repo). ;;;","26/Jun/15 14:52;josam;Captured some logs from 2 servers ( server 1 - with the issue, server 2 - w/o the issue )

The below text is collected from file :: ~/.ivy2/cache/org.apache.spark-spark-submit-parent-default.xml

Server 1: ( looks for the respective jars in the ""local-m2-cache"")

>>>> entries from a machine where the problem is <<<<<<<<

                <module organisation=""org.codehaus.groovy"" name=""groovy-all"">
                        <revision name=""2.1.6"" status=""release"" pubdate=""20150618023837"" resolver=""local-m2-cache"" artresolver=""local-m2-cache"" homepage=""http://groovy.codehaus.org/"" downloaded=""false"" searched=""false"" default=""false"" conf=""compile, master(*), runtime, compile(*), runtime(*), master"" position=""52"">
                                <license name=""The Apache Software License, Version 2.0"" url=""http://www.apache.org/licenses/LICENSE-2.0.txt""/>
                                <metadata-artifact status=""no"" details="""" size=""5591"" time=""0"" location=""/home/joe/.ivy2/cache/org.codehaus.groovy/groovy-all/ivy-2.1.6.xml"" searched=""false"" original-local-location=""/home/joe/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.pom"" origin-is-local=""true"" origin-location=""file:/home/joe/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.pom""/>
                                <caller organisation=""org.apache.hive"" name=""hive-exec"" conf=""default, compile, runtime, master"" rev=""2.1.6"" rev-constraint-default=""2.1.6"" rev-constraint-dynamic=""2.1.6"" callerrev=""0.13.1""/>
                                <artifacts>
                                        <artifact name=""groovy-all"" type=""jar"" ext=""jar"" status=""failed"" details=""missing artifact"" size=""0"" time=""0""/>
                                </artifacts>
                        </revision>
                </module>

>>>> entries from a machine where the problem is <<<<<<<<




Server 2:  ( looks for the respective jars in the ""central"")

>>>> entries from a machine where it works <<<<<<<<
                <module organisation=""org.codehaus.groovy"" name=""groovy-all"">
                        <revision name=""2.1.6"" status=""release"" pubdate=""20130709121712"" resolver=""central"" artresolver=""central"" homepage=""http://groovy.codehaus.org/"" downloaded=""false"" searched=""false"" default=""false"" conf=""compile, master(*), runtime, compile(*), runtime(*), master"" position=""52"">
                                <license name=""The Apache Software License, Version 2.0"" url=""http://www.apache.org/licenses/LICENSE-2.0.txt""/>
                                <metadata-artifact status=""no"" details="""" size=""5591"" time=""0"" location=""/home/joe/.ivy2/cache/org.codehaus.groovy/groovy-all/ivy-2.1.6.xml"" searched=""false"" original-local-location=""/home/joe/.ivy2/cache/org.codehaus.groovy/groovy-all/ivy-2.1.6.xml.original"" origin-is-local=""false"" origin-location=""https://repo1.maven.org/maven2/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.pom""/>
                                <caller organisation=""org.apache.hive"" name=""hive-exec"" conf=""default, compile, runtime, master"" rev=""2.1.6"" rev-constraint-default=""2.1.6"" rev-constraint-dynamic=""2.1.6"" callerrev=""0.13.1""/>
                                <artifacts>
                                        <artifact name=""groovy-all"" type=""jar"" ext=""jar"" status=""no"" details="""" size=""6377448"" time=""0"" location=""/home/joe/.ivy2/cache/org.codehaus.groovy/groovy-all/jars/groovy-all-2.1.6.jar"">
                                                <origin-location is-local=""false"" location=""https://repo1.maven.org/maven2/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.jar""/>
                                        </artifact>
                                </artifacts>
                        </revision>
                </module>
>>>> entries from a machine where it works <<<<<<<<



Need some help from the community to identify from where ivy is picking up the settings to populate this file ~/.ivy2/cache/org.apache.spark-spark-submit-parent-default.xml so that I can narrow down the problem.

Thanks,
Joe.;;;","29/Jun/15 17:46;brkyvz;Hi Joe,
Is it possible to delete those files (~/.ivy2/cache/org.apache.spark-spark-submit-parent-default.xml) from the faulty servers? Maybe it would be better to have Spark delete it beforehand. That would however mean that the resolution phase will always take a while, because the whereabouts of the artifacts are never cached.;;;","29/Jun/15 18:31;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/7089;;;","29/Jun/15 18:31;brkyvz;Hi Joe,
Could you please check whether
https://github.com/apache/spark/pull/7089
solves your problem?

Thanks,
Burak;;;","30/Jun/15 09:13;josam;Great Burak. Your fix works.

I have to recreate my m2 and ivy2 repo's by completely  deleting these directories from my home and then applied your commit, kick started the compilation and it works.
For our testing, we were in spark 1.4.0-rc4 and I cherry picked the following commits to get this working,
218be70 [SPARK-8410] [SPARK-8475] remove previous ivy resolution when using spark-submit
06788cc [SPARK-8095] Resolve dependencies of --packages in local ivy cache
01accf0 [SPARK-8126] [BUILD] Use custom temp directory during build.
Now we are moving to spark 1.4.1. 
Thanks,
Joe.;;;",,,,,,,,,,,,,,,
Python OR operator is not considered while creating a column of boolean type,SPARK-8408,12838440,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,davies,felixmaximilian,felixmaximilian,17/Jun/15 11:33,09/Jul/15 00:09,14/Jul/23 06:26,09/Jul/15 00:09,1.4.0,,,,,,1.4.1,,,,,,PySpark,,,,0,,,,,,"h3. Given

{code}
    d = [{'name': 'Alice', 'age': 1},{'name': 'Bob', 'age': 2}]
    person_df = sqlContext.createDataFrame(d)
{code}
h3. When
{code}
    person_df.filter(person_df.age==1 or person_df.age==2).collect()
{code}
h3. Expected

[Row(age=1, name=u'Alice'), Row(age=2, name=u'Bob')]

h3. Actual

[Row(age=1, name=u'Alice')]

h3. While
{code}
    person_df.filter(""age = 1 or age = 2"").collect()
{code}
yields the correct result:

[Row(age=1, name=u'Alice'), Row(age=2, name=u'Bob')]
",OSX Apache Spark 1.4.0,davies,drobertson,felixmaximilian,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/15 11:34;felixmaximilian;bug_report.ipynb.json;https://issues.apache.org/jira/secure/attachment/12740093/bug_report.ipynb.json",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 00:09:15 UTC 2015,,,,,,,,,,"0|i2g5ef:",9223372036854775807,,,,,felixmaximilian,,,,,,,,,,,,,,,,,,,,,,"17/Jun/15 11:34;felixmaximilian;IPython notebook with the code that reflects the bug.;;;","09/Jul/15 00:09;davies;In Python, We cannot override `or` `and` `not`, so we should use `|` `&` `~` for them. We will throw an exception if you have to use `and` with columns. see https://github.com/apache/spark/pull/6961;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition when writing Parquet files,SPARK-8406,12838410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,17/Jun/15 08:20,15/Jul/15 18:25,14/Jul/23 06:26,22/Jun/15 17:04,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,2,,,,,,"To support appending, the Parquet data source tries to find out the max part number of part-files in the destination directory (the <id> in output file name ""part-r-<id>.gz.parquet"") at the beginning of the write job. In 1.3.0, this step happens on driver side before any files are written. However, in 1.4.0, this is moved to task side. Thus, for tasks scheduled later, they may see wrong max part number generated by newly written files by other finished tasks within the same job. This actually causes a race condition. In most cases, this only causes nonconsecutive IDs in output file names. But when the DataFrame contains thousands of RDD partitions, it's likely that two tasks may choose the same part number, thus one of them gets overwritten by the other.

The following Spark shell snippet can reproduce nonconsecutive part numbers:
{code}
sqlContext.range(0, 128).repartition(16).write.mode(""overwrite"").parquet(""foo"")
{code}
""16"" can be replaced with any integer that is greater than the default parallelism on your machine (usually it means core number, on my machine it's 8).
{noformat}
-rw-r--r--   3 lian supergroup          0 2015-06-17 00:06 /user/lian/foo/_SUCCESS
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00001.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00002.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00003.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00004.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00005.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00006.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00007.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00008.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00017.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00018.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00019.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00020.gz.parquet
-rw-r--r--   3 lian supergroup        352 2015-06-17 00:06 /user/lian/foo/part-r-00021.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00022.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00023.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00024.gz.parquet
{noformat}

And here is another Spark shell snippet for reproducing overwriting:
{code}
sqlContext.range(0, 10000).repartition(500).write.mode(""overwrite"").parquet(""foo"")
sqlContext.read.parquet(""foo"").count()
{code}
Expected answer should be {{10000}}, but you may see a number like {{9960}} due to overwriting. The actual number varies for different runs and different nodes.

Notice that the newly added ORC data source is less likely to hit this issue because it uses task ID and {{System.currentTimeMills()}} to generate the output file name. Thus, the ORC data source may hit this issue only when two tasks with the same task ID (which means they are in two concurrent jobs) are writing to the same location within the same millisecond.",,apachespark,ewanleith,joshrosen,Jvan,lian cheng,marmbrus,neelesh77,nemccarthy,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9072,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 17:06:44 UTC 2015,,,,,,,,,,"0|i2g57r:",9223372036854775807,,,,,yhuai,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"17/Jun/15 08:43;nemccarthy;This is hitting us hard. Let me know if there is anything we can do to help on this end with contributing a fix or testing. 

FYI heres details from the mailing list. 

 ---

When trying to save a data frame with 569610608 rows. 

  dfc.write.format(""parquet"").save(“/data/map_parquet_file"")

We get random results between runs. Caching the data frame in memory makes no difference. It looks like the write out misses some of the RDD partitions. We have an RDD with 6750 partitions. When we write out we get less files out than the number of partitions. When reading the data back in and running a count, we get smaller number of rows. 

I’ve tried counting the rows in all different ways. All return the same result, 560214031 rows, missing about 9.4 million rows (0.15%).

  qc.read.parquet(""/data/map_parquet_file"").count
  qc.read.parquet(""/data/map_parquet_file"").rdd.count
  qc.read.parquet(""/data/map_parquet_file"").mapPartitions{itr => var c = 0; itr.foreach(_ => c = c + 1); Seq(c).toIterator }.reduce(_ + _)

Looking on HDFS the files, there are 6643 .parquet files. 107 missing partitions (about 0.15%). 

Then writing out the same cached DF again to a new file gives 6717 files on hdfs (about 33 files missing or 0.5%);

  dfc.write.parquet(“/data/map_parquet_file_2"")

And we get 566670107 rows back (about 3million missing ~0.5%); 

  qc.read.parquet(""/data/map_parquet_file_2"").count

Writing the same df out to json writes the expected number (6750) of parquet files and returns the right number of rows 569610608. 

  dfc.write.format(""json"").save(""/data/map_parquet_file_3"")
  qc.read.format(""json"").load(""/data/map_parquet_file_3"").count

One thing to note is that the parquet part files on HDFS are not the normal sequential part numbers like for the json output and parquet output in Spark 1.3.

part-r-06151.gz.parquet  part-r-118401.gz.parquet  part-r-146249.gz.parquet  part-r-196755.gz.parquet  part-r-35811.gz.parquet   part-r-55628.gz.parquet  part-r-73497.gz.parquet  part-r-97237.gz.parquet
part-r-06161.gz.parquet  part-r-118406.gz.parquet  part-r-146254.gz.parquet  part-r-196763.gz.parquet  part-r-35826.gz.parquet   part-r-55647.gz.parquet  part-r-73500.gz.parquet  _SUCCESS

We are using MapR 4.0.2 for hdfs.;;;","17/Jun/15 10:23;lian cheng;An example task execution order which causes overwriting:

# Writing a DataFrame with 4 RDD partitions to an empty directory.
# Task 1 and task 2 get scheduled, while task 3 and task 4 are queued.  Both task 1 and task 2 find current max part number to be 0 (because destination directory is empty).
# Task 1 finishes, generates {{part-r-00001.gz.parquet}}. Current max part number becomes 1.
# Task 4 gets scheduled, decides to write to {{part-r-00005.gz.parquet}} (5 = current max part number + task ID), but hasn't start writing the file yet.
# Task 2 finishes, generates {{part-r-00002.gz.parquet}}. Current max part number becomes 2.
# Task 3 gets scheduled, also decides to write to {{part-r-00005.gz.parquet}} since task 4 hasn't start writing its output file, and task 3 finds current max part number is still 2.
# Task 4 finishes writing {{part-r-00005.gz.parquet}}
# Task 3 finishes writing {{part-r-00005.gz.parquet}}
# Output of task 4 is overwritten.;;;","17/Jun/15 18:48;marmbrus;It seems to me that ORC is not free of this bug, but instead just more likely to avoid a problem, right?;;;","17/Jun/15 20:16;lian cheng;Yeah, just updated the JIRA description.  ORC may hit this issue only when two tasks with the same task ID (which means they are in two concurrent jobs) are writing to the same location within the same millisecond.;;;","17/Jun/15 22:43;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6864;;;","18/Jun/15 16:55;lian cheng;[~nemccarthy], thanks again for the report.  [Here|https://github.com/apache/spark/pull/6864#issuecomment-113024897] is a summary for better understanding of this issue.;;;","22/Jun/15 08:04;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6932;;;","22/Jun/15 17:04;yhuai;Issue resolved by pull request 6864
[https://github.com/apache/spark/pull/6864];;;","22/Jun/15 17:06;yhuai;[~nemccarthy] I have merged the fix to both master and branch 1.4.;;;",,,,,,,,,,,,,,,,,,,,
Show executor logs on Web UI when Yarn log aggregation is enabled,SPARK-8405,12838404,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,carsonwang,carsonwang,carsonwang,17/Jun/15 07:56,08/Jun/17 16:48,14/Jul/23 06:26,27/Jul/15 13:04,1.4.0,,,,,,1.4.2,,,,,,Web UI,,,,0,,,,,,"When running Spark application in Yarn mode and Yarn log aggregation is enabled, customer is not able to view executor logs on the history server Web UI. The only way for customer to view the logs is through the Yarn command ""yarn logs -applicationId <appId>"".

An screenshot of the error is attached. When you click an executor’s log link on the Spark history server, you’ll see the error if Yarn log aggregation is enabled. The log URL redirects user to the node manager’s UI. This works if the logs are located on that node. But since log aggregation is enabled, the local logs are deleted once log aggregation is completed. 

The logs should be available through the web UIs just like other Hadoop components like MapReduce. For security reasons, end users may not be able to log into the nodes and run the yarn logs -applicationId command. The web UIs can be viewable and exposed through the firewall if necessary.
",,apachespark,carsonwang,hshreedharan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9311,SPARK-21013,,,,,,,,,,,,,,,,,,,,,"17/Jun/15 07:58;carsonwang;SparkLogError.png;https://issues.apache.org/jira/secure/attachment/12740065/SparkLogError.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 08:54:03 UTC 2015,,,,,,,,,,"0|i2g56f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/15 08:24;carsonwang;I had some in progress works and here is the approach I was using.
1. If Yarn log aggreation is enabled, we update each executor's log URL on the history server. The new URL link is a new added log page hosted on the history server. These URLs are passed the same as how other URLs are passed. So we have enough information like the container Id, appOwner, etc.
2. The log page reads the aggregated logs from HDFS by using Yarn APIs. 

This is transparent to the end users. If Yarn log aggreation is not enabled, nothing is changed. If it is eanbled, the end user will be albe to click the executor's log link and view the logs on Web UI. 

Is there any concerns regarding reading the aggregated logs from HDFS? The Map Reduce history server reads the aggregated logs from HDFS as well to show the logs so I suppose it is ok for Spark history server to read it.;;;","26/Jun/15 07:08;apachespark;User 'carsonwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/7033;;;","26/Jun/15 17:32;hshreedharan;If you wait for a  few seconds, that webpage does redirect to the correct logs, in recent versions of YARN.;;;","26/Jun/15 17:35;hshreedharan;Actually I think this is a config issue on your YARN cluster. See: https://groups.google.com/a/cloudera.org/forum/#!topic/cdh-user/HBGzj_NG9_s and http://stackoverflow.com/questions/24076192/yarn-jobhistory-error-failed-redirect-for-container-1400260444475-3309-01-00000;;;","29/Jun/15 05:08;carsonwang;Thank you very much, [~hshreedharan]
After I configured yarn.log.server.url in yarn-site.xml, the log url did redirect to MR job history server's UI and was able to show the logs. But this seems a little strange because we need view Spark's log on MR history server. Would it make more sense that Spark history server reads and shows the aggregated logs itself?;;;","17/Jul/15 08:43;carsonwang;I think we can improve the doc so users will know that the aggregated logs are available on Web UI if MR history server is running and the configuration are correct.;;;","17/Jul/15 08:54;apachespark;User 'carsonwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/7463;;;",,,,,,,,,,,,,,,,,,,,,,
Use thread-safe collections to make KafkaStreamSuite tests more reliable,SPARK-8404,12838402,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,17/Jun/15 07:46,17/Jun/15 22:08,14/Jul/23 06:26,17/Jun/15 22:08,,,,,,,,,,,,,DStreams,Tests,,,0,,,,,,"Fix the non-thread-safe codes in KafkaStreamSuite, DirectKafkaStreamSuite, JavaKafkaStreamSuite and JavaDirectKafkaStreamSuite",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 07:50:09 UTC 2015,,,,,,,,,,"0|i2g55z:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"17/Jun/15 07:50;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6852;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build system scala version selection script fails on Mac OS X,SPARK-8401,12838340,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,michael,michael,michael,17/Jun/15 00:06,24/Jul/15 07:41,14/Jul/23 06:26,21/Jul/15 10:14,1.4.0,,,,,,1.5.0,,,,,,Build,,,,0,,,,,,"The {{dev/change-version-to-*.sh}} selection scripts use syntax for GNU sed which produces incorrect results when run with Mac OS X's built in version of sed. For example:

{noformat}
[msa@Michaels-MacBook-Pro spark-1.4]$ ./dev/change-version-to-2.11.sh 
[msa@Michaels-MacBook-Pro spark-1.4]$ gst
On branch scala-versions
Your branch and 'vamp/scala-versions' have diverged,
and have 7 and 4 different commits each, respectively.
  (use ""git pull"" to merge the remote branch into yours)
Changes not staged for commit:
  (use ""git add <file>..."" to update what will be committed)
  (use ""git checkout -- <file>..."" to discard changes in working directory)

	modified:   assembly/pom.xml
	modified:   bagel/pom.xml
	modified:   core/pom.xml
	modified:   dev/change-scala-version.sh
	modified:   docs/_plugins/copy_api_dirs.rb
	modified:   examples/pom.xml
	modified:   external/flume-sink/pom.xml
	modified:   external/flume/pom.xml
	modified:   external/kafka-assembly/pom.xml
	modified:   external/kafka/pom.xml
	modified:   external/mqtt/pom.xml
	modified:   external/twitter/pom.xml
	modified:   external/zeromq/pom.xml
	modified:   extras/java8-tests/pom.xml
	modified:   extras/kinesis-asl/pom.xml
	modified:   extras/spark-ganglia-lgpl/pom.xml
	modified:   graphx/pom.xml
	modified:   launcher/pom.xml
	modified:   mllib/pom.xml
	modified:   network/common/pom.xml
	modified:   network/shuffle/pom.xml
	modified:   network/yarn/pom.xml
	modified:   pom.xml
	modified:   repl/pom.xml
	modified:   sql/catalyst/pom.xml
	modified:   sql/core/pom.xml
	modified:   sql/hive-thriftserver/pom.xml
	modified:   sql/hive/pom.xml
	modified:   streaming/pom.xml
	modified:   tools/pom.xml
	modified:   unsafe/pom.xml
	modified:   yarn/pom.xml

Untracked files:
  (use ""git add <file>..."" to include in what will be committed)

	assembly/pom.xml-e
	bagel/pom.xml-e
	core/pom.xml-e
	dev/audit-release/blank_maven_build/pom.xml-e
	dev/audit-release/maven_app_core/pom.xml-e
	docs/_plugins/copy_api_dirs.rb-e
	examples/pom.xml-e
	external/flume-sink/pom.xml-e
	external/flume/pom.xml-e
	external/kafka-assembly/pom.xml-e
	external/kafka/pom.xml-e
	external/mqtt/pom.xml-e
	external/twitter/pom.xml-e
	external/zeromq/pom.xml-e
        extras/java8-tests/pom.xml-e
	extras/kinesis-asl/pom.xml-e
	extras/spark-ganglia-lgpl/pom.xml-e
	graphx/pom.xml-e
	launcher/pom.xml-e
	mllib/pom.xml-e
	network/common/pom.xml-e
	network/shuffle/pom.xml-e
	network/yarn/pom.xml-e
	pom.xml-e
	repl/pom.xml-e
	sql/catalyst/pom.xml-e
	sql/core/pom.xml-e
	sql/hive-thriftserver/pom.xml-e
	sql/hive/pom.xml-e
	streaming/pom.xml-e
	tools/pom.xml-e
	unsafe/pom.xml-e
	yarn/pom.xml-e

no changes added to commit (use ""git add"" and/or ""git commit -a"")
{noformat}

Homebrew and MacPorts provide packages for GNU sed which install it as {{gsed}}. Therefore, I suggest that if the default system {{sed}} command is not GNU sed, we look for {{gsed}} and use it if available.",,apachespark,michael,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 07:41:08 UTC 2015,,,,,,,,,,"0|i2g4sf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/15 00:09;apachespark;User 'mallman' has created a pull request for this issue:
https://github.com/apache/spark/pull/6832;;;","21/Jul/15 10:14;srowen;Issue resolved by pull request 6832
[https://github.com/apache/spark/pull/6832];;;","24/Jul/15 07:41;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7639;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ml.ALS doesn't handle -1 block size,SPARK-8400,12838281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bryanc,mengxr,mengxr,16/Jun/15 20:34,25/Aug/15 11:37,14/Jul/23 06:26,25/Aug/15 11:37,1.3.1,,,,,,1.3.2,,,,,,ML,,,,0,,,,,,"Under spark.mllib, if number blocks is set to -1, we set the block size automatically based on the input partition size. However, this behavior is not preserved in the spark.ml API. If user sets -1 in Spark 1.3, it will not work, but no error messages will show.",,apachespark,bryanc,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 11:37:51 UTC 2015,,,,,,,,,,"0|i2g4fz:",9223372036854775807,,,,,mengxr,,,,,,,,,1.3.2,,,,,,,,,,,,,"16/Jun/15 23:54;bryanc;I could do this.  Just to clarify, if the user sets -1 should it print an error and not continue or handle it automatically as in spark.mllib?;;;","18/Jun/15 00:40;josephkb;It should behave as in spark.mllib, handling it automatically (as long as we document it).;;;","18/Jun/15 21:43;bryanc;Ok, sounds good.;;;","07/Jul/15 20:13;mengxr;[~bryanc] Are you still working on this issue?;;;","07/Jul/15 20:44;bryanc;Hi [~mengxr], yes I am.  I'll hopefully have a PR soon for you to check out.;;;","09/Jul/15 22:09;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/7333;;;","13/Jul/15 22:21;bryanc;Hi [~mengxr], just in case you missed my comment in the PR about adding an error message in branch 1.3 for this:

{quote}
{noformat}
using branch-1.3 I gave ALS a -1 block size and got the following exception:

[info] ALSSuite:
[info] - more blocks than ratings *** FAILED *** (1 second, 112 milliseconds)
[info] java.lang.IllegalArgumentException: requirement failed: numBlocks must be positive but found -1.
[info] at scala.Predef$.require(Predef.scala:233)
[info] at org.apache.spark.ml.recommendation.ALS$LocalIndexEncoder.(ALS.scala:1164)

Seems like it's already fixed to me, what do you think?
{noformat}
{quote};;;","21/Aug/15 03:14;mengxr;Sorry for my late reply! We check numBlocks in LocalIndexEncoder. However, I'm not sure whether this happens before any data shuffling. It might be better to check numUserBlocks and numItemBlocks directly.;;;","21/Aug/15 20:23;bryanc;No problem!  It does LocalIndexEncoder once training is started and after some data operations, so you are right that it is probably better to make this check earlier.  I'll submit a PR on 1.3 for this.;;;","21/Aug/15 20:39;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/8363;;;","25/Aug/15 11:37;srowen;Issue resolved by pull request 8363
[https://github.com/apache/spark/pull/8363];;;",,,,,,,,,,,,,,,,,,
Overlap between histograms and axis' name in Spark Streaming UI,SPARK-8399,12838216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,BenFradet,BenFradet,BenFradet,16/Jun/15 16:51,04/Jul/15 07:27,14/Jul/23 06:26,24/Jun/15 19:04,1.4.0,,,,,,1.4.1,1.5.0,,,,,DStreams,Web UI,,,0,,,,,,"If you have an histogram skewed towards the maximum of the displayed values as is the case with the number of messages processed per batchInterval with the Kafka direct API (since it's a constant) for example, the histogram will overlap with the name of the X axis (#batches).

Unfortunately, I don't have any screenshots available.",,apachespark,BenFradet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 17:06:03 UTC 2015,,,,,,,,,,"0|i2g41b:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"16/Jun/15 16:51;BenFradet;I'll submit a patch shortly.;;;","16/Jun/15 17:06;apachespark;User 'BenFradet' has created a pull request for this issue:
https://github.com/apache/spark/pull/6845;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphLoader.edgeListFile does not populate Graph.vertices.,SPARK-8396,12838156,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,bahbarrettmatthew,bahbarrettmatthew,16/Jun/15 13:12,16/Jun/15 13:18,14/Jul/23 06:26,16/Jun/15 13:18,1.4.0,,,,,,,,,,,,GraphX,,,,0,easyfix,newbie,,,,"With input data like this
18090 31237
31237 31225
31225 31285
31285 31200
31200 31197
31197 31195
31195 31346
31346 54013
54013 31256
31256 23121

The code 

val graph : Graph[Int, Int] = GraphLoader.edgeListFile(sc, hdfsNode + ""/data/misc/Sample_DirectedGraphData.ssv"")
graph.vertices.foreach{println}
graph.vertices.foreach{vertex: (VertexId, Int) => println(vertex._1.toString + "" *** "" + vertex._2.toString)}

prints nothing.",Mac OS X.  Spark-1.4.0 pre-compiled binary for Hadoop-2.4.0-bin.,bahbarrettmatthew,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-16 13:12:38.0,,,,,,,,,,"0|i2g3of:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer doesn't read kerberos opts from config,SPARK-8394,12838128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,stevel@apache.org,stevel@apache.org,16/Jun/15 11:13,19/Jun/15 10:41,14/Jul/23 06:26,16/Jun/15 11:15,1.3.1,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"the history server calls {{initSecurity()}} before it reads in the configuration. As a result you can't configure kerberos options in {{spark-defaults.conf}}, but only in {{SPARK_HISTORY_OPTS}}

(this has already been fixed; I'm filing the JIRA as it wasn't there & I'd just hit the same problem in branch-1.3)",,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 11:15:03 UTC 2015,,,,,,,,,,"0|i2g3i7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/15 11:14;stevel@apache.org;Fixed in commit [9042f](https://github.com/apache/spark/commit/9042f8f3784f10f695cba6b80c054695b1c152c5);;;","16/Jun/15 11:15;stevel@apache.org;fixed by Marcelo;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame and JDBC regression,SPARK-8386,12838019,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,huaxingao,phaumer,phaumer,16/Jun/15 02:35,14/Oct/15 19:32,14/Jul/23 06:26,14/Oct/15 19:32,1.4.0,,,,,,1.4.2,1.5.2,1.6.0,,,,SQL,,,,0,,,,,,"I have an ETL app that appends to a JDBC table new results found at each run.  In 1.3.1 I did this:

testResultsDF.insertIntoJDBC(CONNECTION_URL, TABLE_NAME, false);

When I do this now in 1.4 it complains that the ""object"" 'TABLE_NAME' already exists. I get this even if I switch the overwrite to true.  I also tried this now:

testResultsDF.write().mode(SaveMode.Append).jdbc(CONNECTION_URL, TABLE_NAME, connectionProperties);

getting the same error. It works running the first time creating the new table and adding data successfully. But, running it a second time it (the jdbc driver) will tell me that the table already exists. Even SaveMode.Overwrite will give me the same error. 
",RHEL 7.1,apachespark,huaxing,phaumer,rxin,tsuresh,viirya,Vishal B,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 09 01:29:03 UTC 2015,,,,,,,,,,"0|i2g2u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/15 04:05;Vishal B;I just ran into the same issue today. This should be addressed and looks like a sever limitation for JDBC writers.;;;","22/Sep/15 06:34;rxin;[~viirya] do you have time to take a look?;;;","22/Sep/15 09:29;viirya;OK. I will investigate this.;;;","24/Sep/15 07:27;viirya;[~phaumer] I can't reproduce this problem. Can you give me a code snippet that causes this problem? Thanks.;;;","07/Oct/15 22:45;huaxing;I looked the code, it has this
  @deprecated(""Use write.jdbc()"", ""1.4.0"")
  def insertIntoJDBC(url: String, table: String, overwrite: Boolean): Unit = {
    val w = if (overwrite) write.mode(SaveMode.Overwrite) else write
    w.jdbc(url, table, new Properties)
  }
if overwrite is false, it doesn't set and the default is SaveMode.ErrorIfExists.
It seems to me that if the overwrite is false, the mode should be set to be Append

  def insertIntoJDBC(url: String, table: String, overwrite: Boolean): Unit = {
    val w = if (overwrite) write.mode(SaveMode.Overwrite) else write.mode(SaveMode.Append)
    w.jdbc(url, table, new Properties)
  }

;;;","07/Oct/15 22:52;huaxing;If the above fix is correct, can I have a pull request to check in the change?
I am new to spark and this is the very first gira I looked.  I am not so familiar with the process yet. 

I can only recreate the problem for testResultsDF.insertIntoJDBC(CONNECTION_URL, TABLE_NAME, false);
If it sets to true, it works OK for me.
Also, testResultsDF.write().mode(SaveMode.Append).jdbc(CONNECTION_URL, TABLE_NAME, connectionProperties);
works OK for me. 
;;;","07/Oct/15 23:08;huaxing;Actually I can also recreate the problem in the other two cases.  The reason I didn't recreate it earlier is that I already fixed the tableExists method in my code.  In tableExists, it checks if table exists using SELECT 1 FROM $table LIMIT 1. This is not working for all databases.  For the database that doesn't support LIMIT 1, it will return false and the jdbc/insertIntoJDBC will try to create table again and will get table already exists error. I think this is what Visha got. 

I searched Gira, and there is already a problem opened for the LIMIT 1 in tableexists, so I will not fix this problem.  I will only fix the saveMode problem in my first comment. 


 ;;;","07/Oct/15 23:48;phaumer;Huaxin Gao, sorry for not replying earlier. It slipped through the cracks. 

I had stopped using the Spark jdbc framework completely because of this bug and implemented my own as I need to support DB2, Derby, SQL Server, as well as Oracle and ran into this issue with SQL Server and DB2. DB2's Limit is quite different: http://www-01.ibm.com/support/knowledgecenter/SSEPGG_10.5.0/com.ibm.db2.luw.sql.ref.doc/doc/r0059212.html?lang=en. 

However, I ended up using the jdbc meta-data, which in fact does not perform too well on all DBMS, though; and it might be better to provide different queries for each DBMS here:

final ResultSet resultSet = connection.getMetaData().getTables(null,schemaName, tableName, null);
if (resultSet.next()) {
     return true;
};;;","08/Oct/15 00:29;huaxing;The TableExists bug is already resolved in Jira 9078

https://issues.apache.org/jira/browse/SPARK-9078?jql=project%20%3D%20SPARK%20AND%20text%20~%20DB2

I will have a pull request to fix the SaveMode problem ;;;","09/Oct/15 01:29;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/9042;;;",,,,,,,,,,,,,,,,,,,
Improve Analysis Unit test framework,SPARK-8382,12837928,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,marmbrus,marmbrus,15/Jun/15 18:58,07/Aug/15 18:30,14/Jul/23 06:26,07/Aug/15 18:30,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"We have some nice frameworks for doing various unit test {{checkAnswer}}, {{comparePlan}}, {{checkEvaluation}}, etc.  However {{AnalysisSuite}} is kind of sloppy with each test using assertions in different ways.  I'd like a function that looks something like the following:

{code}
def checkAnalysis(
  inputPlan: LogicalPlan,
  expectedPlan: LogicalPlan = null,
  caseInsensitiveOnly: Boolean = false,
  expectedErrors: Seq[String] = Nil)
{code}

This function should construct tests that check the Analyzer works as expected and provides useful error messages when any failures are encountered.  We should then rewrite the existing tests and beef up our coverage here.",,apachespark,ckadner,lianhuiwang,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 07 06:02:04 UTC 2015,,,,,,,,,,"0|i2g2a7:",9223372036854775807,,,,,,,,,,,,,,1.6.0,,,,,,,,,,,,,"07/Aug/15 06:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/8025;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeaseExpiredException when using dynamic partition with speculative execution,SPARK-8379,12837876,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jeanlyn,jeanlyn,jeanlyn,15/Jun/15 15:59,19/Sep/15 13:58,14/Jul/23 06:26,21/Jun/15 07:14,1.3.0,1.3.1,1.4.0,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"when inserting to table using dynamic partitions with *spark.speculation=true*  and there is a skew data of some partitions trigger the speculative tasks ,it will throws the exception like
{code}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): Lease mismatch on /tmp/hive-jeanlyn/hive_2015-06-15_15-20-44_734_8801220787219172413-1/-ext-10000/ds=2015-06-15/type=2/part-00301.lzo owned by DFSClient_attempt_201506031520_0011_m_000189_0_-1513487243_53 but is accessed by DFSClient_attempt_201506031520_0011_m_000042_0_-1275047721_57
{code}",,apachespark,jeanlyn,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6067,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 21 07:14:17 UTC 2015,,,,,,,,,,"0|i2g1zb:",9223372036854775807,,,,,liancheng,,,,,,,,,1.5.0,,,,,,,,,,,,,"16/Jun/15 02:01;apachespark;User 'jeanlyn' has created a pull request for this issue:
https://github.com/apache/spark/pull/6833;;;","21/Jun/15 07:14;lian cheng;Issue resolved by pull request 6833
[https://github.com/apache/spark/pull/6833];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Commons Lang 3 is one of the required JAR of Spark Flume Sink but is missing in the docs,SPARK-8376,12837818,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,15/Jun/15 12:09,18/Jun/15 23:01,14/Jul/23 06:26,18/Jun/15 23:01,,,,,,,1.4.1,1.5.0,,,,,Documentation,,,,0,,,,,,"Commons Lang 3 is added as one of the dependencies of Spark Flume Sink since https://github.com/apache/spark/pull/5703. However, the docs has not yet updated.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 12:12:02 UTC 2015,,,,,,,,,,"0|i2g1mf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/15 12:12;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6829;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When an RDD has no partition, Python sum will throw ""Can not reduce() empty RDD""",SPARK-8373,12837772,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,15/Jun/15 08:31,18/Jun/15 02:29,14/Jul/23 06:26,17/Jun/15 21:00,1.2.0,,,,,,1.4.1,1.5.0,,,,,PySpark,,,,0,,,,,,"The issue is because ""sum"" uses ""reduce"". Replacing it with ""fold"" will fix it.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6878,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 02:29:07 UTC 2015,,,,,,,,,,"0|i2g1c7:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"15/Jun/15 08:34;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6826;;;","15/Jun/15 12:59;srowen;Really the same as SPARK-6878
https://github.com/apache/spark/commit/51b306b930cfe03ad21af72a3a6ef31e6e626235;;;","18/Jun/15 02:29;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6867;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
History server shows incorrect information for application not started,SPARK-8372,12837771,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,carsonwang,carsonwang,15/Jun/15 08:28,02/Jul/15 18:41,14/Jul/23 06:26,30/Jun/15 21:01,1.4.0,,,,,,1.4.1,1.5.0,,,,,Deploy,Web UI,,,0,,,,,,The history server may show an incorrect App ID for an incomplete application like <App ID>.inprogress. This app info will never disappear even after the app is completed. ,,andrewor14,apachespark,carsonwang,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/15 08:31;carsonwang;IncorrectAppInfo.png;https://issues.apache.org/jira/secure/attachment/12739571/IncorrectAppInfo.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 20:29:05 UTC 2015,,,,,,,,,,"0|i2g1bz:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"15/Jun/15 08:41;apachespark;User 'carsonwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6827;;;","25/Jun/15 17:28;vanzin;Just leaving here the comment I left in the PR after this was fixed:

So, one thing that I noticed after I said ""LGTM"" is that this change breaks old logs (those generated by versions of Spark that do not record the app id). Those will never show up anymore (I think that should only be Spark 1.0?).

If we care about that use case, this version should probably be reverted. The proper fix could be something as simple as this:

{code}
appListener.appId.getOrElse(logPath.getName().stripSuffix(EventLoggingListener.IN_PROGRESS)),
{code}
;;;","26/Jun/15 05:05;carsonwang;[~vanzin] The log path name may also end with an attempt id, like application_xxx_xxx_1.inprogress. This happens when running the app in yarn cluster mode. If we still need get the app id from the log path name, the attempt id need to be removed as well if it exists. ;;;","26/Jun/15 16:21;vanzin;bq. The log path name may also end with an attempt id

I'm not saying the single line ""patch"" I posted is the answer, I was just pointing out the current patch in master caused a regression.;;;","29/Jun/15 17:53;andrewor14;OK, per discussion on the #6827 I reverted this.;;;","29/Jun/15 20:29;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/7097;;;",,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException in closure for map ,SPARK-8368,12837737,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,zwChan,zwChan,15/Jun/15 02:47,01/Sep/16 12:24,14/Jul/23 06:26,19/Jun/15 18:13,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"After upgraded the cluster from spark 1.3.0 to 1.4.0(rc4), I encountered the following exception:
======begin exception========
{quote}
Exception in thread ""main"" java.lang.ClassNotFoundException: com.yhd.ycache.magic.Model$$anonfun$9$$anonfun$10
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:278)
	at org.apache.spark.util.InnerClosureFinder$$anon$4.visitMethodInsn(ClosureCleaner.scala:455)
	at com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown Source)
	at com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown Source)
	at org.apache.spark.util.ClosureCleaner$.getInnerClosureClasses(ClosureCleaner.scala:101)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:197)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:1891)
	at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:294)
	at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:293)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
	at org.apache.spark.rdd.RDD.map(RDD.scala:293)
	at org.apache.spark.sql.DataFrame.map(DataFrame.scala:1210)
	at com.yhd.ycache.magic.Model$.main(SSExample.scala:239)
	at com.yhd.ycache.magic.Model.main(SSExample.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{quote}
===============end exception===========

I simplify the code that cause this issue, as following:
==========begin code==================
{noformat}
object Model extends Serializable{
  def main(args: Array[String]) {
    val Array(sql) = args
    val sparkConf = new SparkConf().setAppName(""Mode Example"")
    val sc = new SparkContext(sparkConf)
    val hive = new HiveContext(sc)
    //get data by hive sql
    val rows = hive.sql(sql)

    val data = rows.map(r => { 
      val arr = r.toSeq.toArray
      val label = 1.0
      def fmap = ( input: Any ) => 1.0
      val feature = arr.map(_=>1.0)
      LabeledPoint(label, Vectors.dense(feature))
    })

    data.count()
  }
}
{noformat}
=====end code===========
This code can run pretty well on spark-shell, but error when submit it to spark cluster (standalone or local mode).  I try the same code on spark 1.3.0(local mode), and no exception is encountered.","Centos 6.5, java 1.7.0_67, scala 2.10.4. Build the project on Windows 7 and run in a spark standalone cluster(or local) mode on Centos 6.X. ",andrewor14,apachespark,dondrake,marmbrus,smolav,tone,yhuai,zwChan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8365,SPARK-8470,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 14:49:30 UTC 2015,,,,,,,,,,"0|i2g14f:",9223372036854775807,,,,,andrewor14,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"16/Jun/15 04:35;yhuai;[~zwChan] How was the application submitted?;;;","16/Jun/15 08:37;zwChan;I have tried both local and standalone mode, and both with-dependency and without-dependency jar file. Use the following command:
 
spark-submit --class com.yhd.ycache.magic.Model --jars ./SSExample-0.0.1-SNAPSHOT-jar-with-dependencies.jar  --master local ./SSExample-0.0.1-SNAPSHOT-jar-with-dependencies.jar ""sql statement"" 

BTW, the same code and submit command run on spark 1.3.0 without problem.;;;","17/Jun/15 16:44;yhuai;Can you add {{--verbose}} and post the extra information like {{Main class:}} and {{Classpath elements:}}? 

[~andrewor14] Have we changed any thing related to spark submit in 1.4.0?;;;","17/Jun/15 21:27;yhuai;I have reproduced it. I am investigating it now.;;;","17/Jun/15 22:19;yhuai;Right now, looks like it is a problem caused by spark sql's isolated class loader.;;;","18/Jun/15 02:12;zwChan;Great! 
I am not familiar to the class loader, and hope for good news from you. If there is anything I can help, please tell me without hesitation.;;;","18/Jun/15 02:49;yhuai;[~zwChan] I have found the cause. Will have a fix soon. ;;;","18/Jun/15 03:13;yhuai;The cause of this problem is {{SessionState.setCurrentSessionState(executionHive.state)}} called during the construction of {{HiveContext}} will override the context class loader of the current thread by the class loader of associated with {{executionHive.state}}. We need to correctly set the class loader of {{executionHive.state}}.;;;","19/Jun/15 00:00;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6891;;;","19/Jun/15 03:44;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6895;;;","19/Jun/15 18:13;marmbrus;Issue resolved by pull request 6891
[https://github.com/apache/spark/pull/6891];;;","22/Jun/15 14:42;dondrake;I've verified through a nightly build that this resolves my issue (SPARK-8365).  Thanks!;;;","22/Jun/15 14:49;yhuai;[~dondrake] Great! Thanks for checking it.;;;",,,,,,,,,,,,,,,,
ReliableKafka will loss data when `spark.streaming.blockInterval` was 0,SPARK-8367,12837735,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,carlmartin,carlmartin,carlmartin,15/Jun/15 02:20,16/Jun/15 06:17,14/Jul/23 06:26,16/Jun/15 06:17,1.4.0,,,,,,1.4.1,1.5.0,,,,,DStreams,,,,0,,,,,,"{code:title=BlockGenerator.scala|borderStyle=solid}
  /** Change the buffer to which single records are added to. */
  private def updateCurrentBuffer(time: Long): Unit = synchronized {
    try {
      val newBlockBuffer = currentBuffer
      currentBuffer = new ArrayBuffer[Any]
      if (newBlockBuffer.size > 0) {

       val blockId = StreamBlockId(receiverId, time - blockIntervalMs)

        val newBlock = new Block(blockId, newBlockBuffer)
        listener.onGenerateBlock(blockId)
        blocksForPushing.put(newBlock)  // put is blocking when queue is full
        logDebug(""Last element in "" + blockId + "" is "" + newBlockBuffer.last)
      }
    } catch {
      case ie: InterruptedException =>
        logInfo(""Block updating timer thread was interrupted"")
      case e: Exception =>
        reportError(""Error in block updating thread"", e)
    }
  }
{code}

If *spark.streaming.blockInterval* was 0, the *blockId* in the code will always be the same because of  *time* was 0 and *blockIntervalMs* was 0 too.

{code:title=ReliableKafkaReceiver.scala|borderStyle=solid}
   private def rememberBlockOffsets(blockId: StreamBlockId): Unit = {
    // Get a snapshot of current offset map and store with related block id.
    val offsetSnapshot = topicPartitionOffsetMap.toMap
    blockOffsetMap.put(blockId, offsetSnapshot)
    topicPartitionOffsetMap.clear()
  }
{code}
If the *blockId* was the same,  Streaming will commit the  *offset*  before the really data comsumed(data was waitting to be commit but the offset had updated and commit by previous commit)
So when exception occures, the *offset* had commit but the data will loss since the data was in memory and not comsumed yet.",,apachespark,carlmartin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 02:54:03 UTC 2015,,,,,,,,,,"0|i2g13z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/15 02:54;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/6818;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
maxNumExecutorsNeeded should properly handle failed tasks,SPARK-8366,12837732,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,meiyoula,meiyoula,meiyoula,15/Jun/15 02:00,12/Aug/15 06:19,14/Jul/23 06:26,12/Aug/15 06:19,1.4.0,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,"I use the *dynamic executor allocation* function. 
When an executor is killed, all running tasks on it will be failed. Until reach the maxTaskFailures, this failed task will re-run with a new task id. 
But the *ExecutorAllocationManager* won't concern this new tasks to total and pending tasks, because the total stage task number only set when stage submitted.",,apachespark,meiyoula,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 02:27:04 UTC 2015,,,,,,,,,,"0|i2g13b:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"15/Jun/15 02:27;apachespark;User 'XuTingjun' has created a pull request for this issue:
https://github.com/apache/spark/pull/6817;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame explode with alias and * fails,SPARK-8358,12837652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,14/Jun/15 04:54,14/Jun/15 18:22,14/Jul/23 06:26,14/Jun/15 18:22,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"{code}
scala> Seq((Array(""a""), 1)).toDF(""a"", ""b"").select(explode($""a"").as(""a""), $""*"")
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'a
        at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:60)
        at org.apache.spark.sql.catalyst.expressions.Explode.elementTypes(generators.scala:107)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$AliasedGenerator$.unapply(Analyzer.scala:577)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$$anonfun$apply$16$$anonfun$22.apply(Analyzer.scala:535)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$$anonfun$apply$16$$anonfun$22.apply(Analyzer.scala:534)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
...
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 14 18:22:17 UTC 2015,,,,,,,,,,"0|i2g0lr:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"14/Jun/15 04:57;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/6811;;;","14/Jun/15 18:22;marmbrus;Issue resolved by pull request 6811
[https://github.com/apache/spark/pull/6811];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leakage on unsafe aggregation path with empty input,SPARK-8357,12837648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,navis,navis,navis,14/Jun/15 03:09,21/Jul/15 18:53,14/Jul/23 06:26,21/Jul/15 18:53,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Currently, unsafe-based hash is released on 'next' call but if input is empty, it would not be called ever. ",,apachespark,joshrosen,navis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8850,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 18:53:17 UTC 2015,,,,,,,,,,"0|i2g0kv:",9223372036854775807,,,,,joshrosen,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"14/Jun/15 03:12;apachespark;User 'navis' has created a pull request for this issue:
https://github.com/apache/spark/pull/6810;;;","21/Jul/15 06:38;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7560;;;","21/Jul/15 18:53;joshrosen;Issue resolved by pull request 7560
[https://github.com/apache/spark/pull/7560];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix off-by-factor-of-8 error when allocating scratch space in UnsafeFixedWidthAggregationMap,SPARK-8354,12837639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,14/Jun/15 01:30,14/Jun/15 16:41,14/Jul/23 06:26,14/Jun/15 16:41,,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"UnsafeFixedWidthAggregationMap contains an off-by-factor-of-8 error when allocating row conversion scratch space: we take a size requirement, measured in bytes, then allocate a long array of that size.  This means that we end up allocating 8x too much conversion space.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-7075,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 14 16:41:40 UTC 2015,,,,,,,,,,"0|i2g0iv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jun/15 01:33;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6809;;;","14/Jun/15 16:41;joshrosen;Issue resolved by pull request 6809
[https://github.com/apache/spark/pull/6809];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"R unit tests output should be logged to ""unit-tests.log""",SPARK-8350,12837633,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andrewor14,andrewor14,andrewor14,13/Jun/15 23:37,29/Dec/15 09:10,14/Jul/23 06:26,15/Jun/15 15:18,1.4.0,,,,,,1.5.0,,,,,,SparkR,,,,0,,,,,,"Right now it's logged to ""R-unit-tests.log"". Jenkins currently only archives files named ""unit-tests.log"", and this is what all other modules (e.g. SQL, network, REPL) use.

1. We should be consistent
2. I don't want to reconfigure Jenkins to accept a different file",,andrewor14,apachespark,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 15:18:18 UTC 2015,,,,,,,,,,"0|i2g0hj:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"13/Jun/15 23:39;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6807;;;","15/Jun/15 15:18;shivaram;Issue resolved by pull request 6807
[https://github.com/apache/spark/pull/6807];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Decimal Math beyond ~2^112 is broken",SPARK-8342,12837594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,rtreffer,rtreffer,13/Jun/15 13:41,14/Jun/15 06:58,14/Jul/23 06:26,14/Jun/15 05:42,1.5.0,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Here is a snippet from the spark-shell that should not happen

{code}
scala> val d = Decimal(Long.MaxValue,100,0) * Decimal(Long.MaxValue,100,0)
d: org.apache.spark.sql.types.Decimal = 0
scala> d.toDebugString
res3: String = Decimal(expanded,0,1,0})
{code}

It looks like precision gets reseted on some operations and values are then truncated.",,apachespark,rtreffer,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 14 06:58:47 UTC 2015,,,,,,,,,,"0|i2g08v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/15 15:56;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6797;;;","13/Jun/15 19:10;rtreffer;Oh no, I managed to create a completely unrelated testcase for my problem. Reduction ftw....

Here is what breaks in the >2^112 range
{code}
import org.apache.spark.sql.types.Decimal

val one = Decimal(1)
val two = Decimal(2)

def pow(n : Int) :  Decimal = if (n <= 0) { one } else { 
  val a = pow(n - 1)
  a.changePrecision(n,0)
  two.changePrecision(n,0)
  a * two
}

(109 to 120).foreach(n => println(pow(n).toJavaBigDecimal.unscaledValue.toString))
649037107316853453566312041152512
1298074214633706907132624082305024
2596148429267413814265248164610048
5192296858534827628530496329220096
1038459371706965525706099265844019
2076918743413931051412198531688038
4153837486827862102824397063376076
8307674973655724205648794126752152
1661534994731144841129758825350430
3323069989462289682259517650700860
6646139978924579364519035301401720
1329227995784915872903807060280344
{code}

So precision gets lost. It's not visible on the normal output path, but it is a loss of precision (10^n is always enough to hold 2^n without loss of precision).

Should I open another ticket for this?;;;","14/Jun/15 05:42;rxin;[~rtreffer] if this patch doesn't fix it, can you open another ticket? Thanks.

https://github.com/apache/spark/pull/6797;;;","14/Jun/15 06:58;rtreffer;[~rxin] [~viirya] yes, I've tested with the patch applied (both the original report, which is now fixed, and my original problem, which is not fixed) :-S

I've opened SPARK-8359 for the precision loss.

Sorry for the confusion, this patch still causes a similar problem:
{code}
import org.apache.spark.sql.types.Decimal

val d = Decimal(Long.MaxValue,100,0) * Decimal(Long.MaxValue,100,0)

d.toJavaBigDecimal.unscaledValue.toString

8507059173023461584739690778423250
{code}
But cross-checking with bc says it should be
85070591730234615847396907784232501249 ((2^63 - 1) * (2^63 - 1))
8507059173023461584739690778423250 is truncated.
Calling changePrecision(100,0) after the multiplication results in
85070591730234615847396907784232500000

Anyway, different bug, different ticket, although the problem is also present in this case, it's just hidden behind another bug 0.o;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Itertools islice requires an integer for the stop argument.,SPARK-8339,12837564,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kconor,kconor,kconor,13/Jun/15 01:38,19/Jun/15 10:41,14/Jul/23 06:26,19/Jun/15 07:13,1.4.0,,,,,,1.4.1,1.5.0,,,,,PySpark,,,,0,,,,,,Itertools islice requires an integer for the stop argument.  The bug is in serializers.py and can prevent and rdd from being written to disk.,python 3,apachespark,davies,kconor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 07:13:10 UTC 2015,,,,,,,,,,"0|i2g027:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/15 02:23;apachespark;User 'kconor' has created a pull request for this issue:
https://github.com/apache/spark/pull/6794;;;","19/Jun/15 07:13;davies;Issue resolved by pull request 6794
[https://github.com/apache/spark/pull/6794];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Ganglia fails to start,SPARK-8338,12837559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vvladymyrov,vvladymyrov,vvladymyrov,13/Jun/15 00:46,06/Apr/16 05:33,14/Jul/23 06:26,06/Aug/15 06:45,1.4.0,,,,,,1.5.0,,,,,,EC2,,,,0,,,,,,"Exception
{code}
Starting httpd: httpd: Syntax error on line 154 of /etc/httpd/conf/httpd.conf: Cannot load /etc/httpd/modules/mod_authz_core.so into server: /etc/httpd/modules/mod_authz_core.so: cannot open shared object file: No such file or directory
{code}",,lapolonio,mikeyreilly,vvladymyrov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 06 05:33:51 UTC 2016,,,,,,,,,,"0|i2g013:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/15 00:50;vvladymyrov;Same exception happens for both branch-1.3 (default) and branch-1.4 of the mesos/spark-ec2

Root cause - AMI contains preinstall ganglia-web 3.3 with httpd22 - its modules doesn't match templates/httpd.conf which is written for newer httpd24.
;;;","13/Jun/15 00:52;vvladymyrov;There are several possible solutions 
- downgrade httpd.conf to Feb 2013 commit (https://github.com/mesos/spark-ec2/commit/1a1c65c15f23dc3be5a45a43f7cd1bf1378d6436)
- remove old ganglia and related packages and instal new ones (ganglia-web 3.5) (and reinstall configuration)
- recreate AMI to have newer ganglia 3.5 or 3.6 (ganglia 3.3 dated Apr 2013)
;;;","13/Jun/15 01:43;vvladymyrov;Here are PRs for branches 1.3 and 1.4 that uninstalls old ganglia and installs newer one
https://github.com/mesos/spark-ec2/pull/122
https://github.com/mesos/spark-ec2/pull/121
;;;","05/Aug/15 23:50;vvladymyrov;The following PR resolves the issue
https://github.com/mesos/spark-ec2/pull/133;;;","06/Aug/15 06:45;srowen;OK, considering it fixed then? I'm not sure what to call the Fix Version here as it wasn't a change in a Spark release per se, but will say 1.5.;;;","09/Dec/15 08:35;mikeyreilly;This issue appears to have resurfaced in 1.5.2.

I'm creating a cluster in ec2 with spark-1.5.2-bin-hadoop2.6.

I issue a command like this

 ./spark-ec2 --key-pair=spark-sentiment --identity-file=/home/ec2-user/xxx.pem --spark-version=1.5.2 --region=eu-west-1 --zone=eu-west-1b --user-data=java8.sh launch spark-cluster-test --vpc-id=myvpc --subnet-id=mysubnet

(java8.sh just installs java8 on the cluster nodes)

After about ten minutes it finishes. The last few lines of output are:

Setting up ganglia
RSYNC'ing /etc/ganglia to slaves...
ec2-52-30-193-79.eu-west-1.compute.amazonaws.com
Shutting down GANGLIA gmond:                               [FAILED]
Starting GANGLIA gmond:                                    [  OK  ]
Shutting down GANGLIA gmond:                               [FAILED]
Starting GANGLIA gmond:                                    [  OK  ]
Connection to ec2-52-30-193-79.eu-west-1.compute.amazonaws.com closed.
Shutting down GANGLIA gmetad:                              [FAILED]
Starting GANGLIA gmetad:                                   [  OK  ]
Stopping httpd:                                            [FAILED]
Starting httpd: httpd: Syntax error on line 154 of /etc/httpd/conf/httpd.conf: Cannot load /etc/httpd/modules/mod_authz_core.so into server: /etc/httpd/modules/mod_authz_core.so: cannot open shared object file: No such file or directory
                                                           [FAILED]
[timing] ganglia setup:  00h 00m 02s
Connection to ec2-52-31-xxx-xxx.eu-west-1.compute.amazonaws.com closed.
Spark standalone cluster started at http://ec2-52-31-xxx-xxx.eu-west-1.compute.amazonaws.com:8080
Ganglia started at http://ec2-52-31-xxx-xxx.eu-west-1.compute.amazonaws.com:5080/ganglia
Done!
;;;","06/Apr/16 05:33;lapolonio;I am also seeing the error that [~mikeyreilly] reported in spark-1.6.1-bin-hadoop2.6;;;",,,,,,,,,,,,,,,,,,,,,,
Fix NullPointerException with functions.rand(),SPARK-8336,12837547,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tedyu@apache.org,yuzhihong@gmail.com,yuzhihong@gmail.com,12/Jun/15 23:54,16/Jun/15 00:01,14/Jul/23 06:26,16/Jun/15 00:01,,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"The problem was first reported by Justin Yip in the thread 'NullPointerException with functions.rand()'

Here is how to reproduce the problem:
{code}
sqlContext.createDataFrame(Seq((1,2), (3, 100))).withColumn(""index"", rand(30)).show()
{code}",,apachespark,nemccarthy,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 23:55:03 UTC 2015,,,,,,,,,,"0|i2fzyn:",9223372036854775807,,,,,rxin,,,,,,,,,,,,,,,,,,,,,,"12/Jun/15 23:55;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/6793;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataSource options parser no longer accepts '_',SPARK-8329,12837453,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,12/Jun/15 16:55,13/Jun/15 06:11,14/Jul/23 06:26,13/Jun/15 06:11,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,This is a regression from 1.3.1,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 17:56:03 UTC 2015,,,,,,,,,,"0|i2fze7:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"12/Jun/15 17:56;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/6786;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
EC2 script not fully updated for 1.4.0 release,SPARK-8322,12837312,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,markmsmith,markmsmith,markmsmith,12/Jun/15 04:55,28/Jul/15 13:03,14/Jul/23 06:26,12/Jun/15 15:19,1.4.0,,,,,,1.4.1,1.5.0,,,,,EC2,,,,0,easyfix,,,,,"In the spark_ec2.py script, the ""1.4.0"" spark version hasn't been added to the VALID_SPARK_VERSIONS map or the SPARK_TACHYON_MAP, causing the script to break for the latest release.",,apachespark,markmsmith,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 15 16:46:07 UTC 2015,,,,,,,,,,"0|i2fyjb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jun/15 05:01;srowen;Related to SPARK-8310. You'll probably want a PR for both master and 1.4 here. CC [~shivaram];;;","12/Jun/15 05:11;apachespark;User 'markmsmith' has created a pull request for this issue:
https://github.com/apache/spark/pull/6776;;;","12/Jun/15 05:32;apachespark;User 'markmsmith' has created a pull request for this issue:
https://github.com/apache/spark/pull/6777;;;","12/Jun/15 05:33;markmsmith;This is the backport to branch-1.4;;;","12/Jun/15 15:19;shivaram;Issue resolved by pull request 6776
[https://github.com/apache/spark/pull/6776];;;","12/Jun/15 15:20;shivaram;[~srowen] Could you assign this issue to [~markmsmith] ?;;;","15/Jun/15 16:46;markmsmith;Thanks for making my first PR so painless guys.;;;",,,,,,,,,,,,,,,,,,,,,,
Spark EC2 branch in 1.4 is wrong,SPARK-8310,12837213,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shivaram,shivaram,shivaram,11/Jun/15 18:20,29/Jun/15 10:43,14/Jul/23 06:26,11/Jun/15 20:19,1.4.0,,,,,,1.4.1,1.5.0,,,,,EC2,,,,0,,,,,,"It points to `branch-1.3` of spark-ec2 right now while it should point to `branch-1.4`

cc [~brdwrd] [~pwendell]",,apachespark,darabos,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 10:43:01 UTC 2015,,,,,,,,,,"0|i2fxyf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/15 18:25;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/6764;;;","11/Jun/15 18:27;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/6765;;;","11/Jun/15 20:19;shivaram;Issue resolved by pull request 6764
[https://github.com/apache/spark/pull/6764];;;","29/Jun/15 10:43;darabos;It's an easy mistake to make, and one of the few things that are not covered by the release candidate process. We tested the release candidate on EC2, but we had to specifically override the version, since at that point there was no released 1.4.0. I have no idea how this could be avoided for future releases.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
OpenHashMap doesn't work with more than 12M items,SPARK-8309,12837156,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,slavik.baranov,wildfire,wildfire,11/Jun/15 14:51,17/Jun/15 08:45,14/Jul/23 06:26,17/Jun/15 08:44,1.3.1,1.4.0,,,,,1.3.2,1.4.1,1.5.0,,,,Spark Core,,,,0,,,,,,"The problem might be demonstrated with the following testcase:

{code}
  test(""support for more than 12M items"") {
    val cnt = 12000000 // 12M
    val map = new OpenHashMap[Int, Int](cnt)
    for (i <- 0 until cnt) {
      map(i) = 1
    }
    val numInvalidValues = map.iterator.count(_._2 == 0)
    assertResult(0)(numInvalidValues)
  }

{code}",,apachespark,wildfire,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 08:44:51 UTC 2015,,,,,,,,,,"0|i2fxm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/15 14:55;wildfire;The problem occurs because of incorrect {{POSITION_MASK}} in OpenHashSet.
Its value is {{0xEFFFFFF}}, but it should be {{0x1FFFFFFF}} (2 ^ 29 - 1).

I have a fix for this issue and will submit pull request soon.
;;;","11/Jun/15 15:42;apachespark;User 'SlavikBaranov' has created a pull request for this issue:
https://github.com/apache/spark/pull/6763;;;","17/Jun/15 08:44;srowen;Issue resolved by pull request 6763
[https://github.com/apache/spark/pull/6763];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
add missing save load for python doc example and tune down MatrixFactorization iterations,SPARK-8308,12837102,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,11/Jun/15 10:19,01/Jul/15 18:18,14/Jul/23 06:26,01/Jul/15 18:18,,,,,,,1.5.0,,,,,,MLlib,,,,0,,,,,,"1. add some missing save/load in python examples, LogisticRegression, LinearRegression, NaiveBayes
2. tune down iterations for MatrixFactorization, since current number will trigger StackOverflow.",,apachespark,josephkb,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 18:18:15 UTC 2015,,,,,,,,,,"0|i2fxaf:",9223372036854775807,,,,,josephkb,,,,,,,,,,,,,,,,,,,,,,"11/Jun/15 10:22;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/6760;;;","01/Jul/15 18:18;josephkb;Issue resolved by pull request 6760
[https://github.com/apache/spark/pull/6760];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
AddJar command needs to set the new class loader to the HiveConf inside executionHive.state.,SPARK-8306,12837047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,11/Jun/15 06:33,17/Jun/15 22:17,14/Jul/23 06:26,17/Jun/15 22:17,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"In {{AddJar}} command, we are using {{org.apache.hadoop.hive.ql.metadata.Hive.get().getConf().setClassLoader(newClassLoader)}}. However, the conf returned by {{Hive.get().getConf()}} is not necessary the one set in {{executionHive.state}}. Thus, we may fail to set the correct class loader to {{executionHive}} in some cases.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 19:24:38 UTC 2015,,,,,,,,,,"0|i2fwy7:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"11/Jun/15 06:37;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6758;;;","11/Jun/15 19:24;yhuai;In ClientWrapper, we have the following code
{code}
/** Returns the configuration for the current session. */
def conf: HiveConf = SessionState.get().getConf

// TODO: should be a def?s
private val client = Hive.get(conf)
{code}

So, when we create a ClientWrapper, the conf inside client the the conf of the current SessionState (thread local varibale). Later, if we call HiveContext from another thread, the client's hive will not be the same as the conf in the session state returned by SessionteState.get (it returns the SessionState for the current thread).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve codegen,SPARK-8305,12837027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,11/Jun/15 04:08,11/Jun/15 19:57,14/Jul/23 06:26,11/Jun/15 19:57,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Fix small issues in codegen:

1. Fix Cast Decimal into Boolean
2. Fix Literal(null)
3. refactor",,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-7075,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 04:12:03 UTC 2015,,,,,,,,,,"0|i2fwtr:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"11/Jun/15 04:12;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6755;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler backend is not notified in case node fails in YARN,SPARK-8297,12836941,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mridulm80,mridulm80,mridulm80,10/Jun/15 20:24,30/Jul/15 17:38,14/Jul/23 06:26,30/Jul/15 17:38,1.2.2,1.3.1,1.4.1,1.5.0,,,1.5.0,,,,,,YARN,,,,0,,,,,,"When a node crashes, yarn detects the failure and notifies spark - but this information is not propagated to scheduler backend (unlike in mesos mode, for example).

It results in repeated re-execution of stages (due to FetchFailedException on shuffle side), resulting finally in application failure.",Spark on yarn - both client and cluster mode.,apachespark,jerryshao,jongyoul,lianhuiwang,mridulm80,nemccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 15 21:43:07 UTC 2015,,,,,,,,,,"0|i2fwcf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/15 20:32;mridulm80;Spark on mesos handles this situation by calling removeExecutor() on the scheduler backend - yarn module does not.
I have this fixed locally, but unfortunately, I do not have the bandwidth to shepherd a patch.

The fix is simple - replicate something similar to what is done in CoarseMesosSchedulerBackend.slaveLost().
Essentially :
a) maintain a mapping from container-id to executor-id in YarnAllocator (consistent with and inverse of executorIdToContainer)
b) propagate the scheduler backend to YarnAllocator when YarnClusterScheduler.postCommitHook is called, 
c) In processCompletedContainers, if the container is not in releasedContainers, invoke backend.removeExecutor(executorId, msg) to notify backend that the executor has not exit'ed gracefully/expectedly.
d) Remove mapping from containerIdToExecutorId and executorIdToContainer in processCompletedContainers (The latter also fixes a memory leak in YarnAllocator btw).


In case no one is picking this one up, I can fix it later in 1.5 release cycle.;;;","11/Jun/15 03:15;jerryshao;Hi [~mridulm80], I tried with latest master branch with spark-shell under yarn-client mode. I simulated executor crash by kill it with ""kill -9"". From my observation, the scheduler backend is notified when executor is lost, here is the log:

{noformat}
scala> 15/06/11 11:08:38 ERROR cluster.YarnScheduler: Lost executor 1 on jerryshao-desktop: remote Rpc client disassociated
15/06/11 11:08:38 WARN remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkExecutor@jerryshao-desktop:50766] has failed, address is now gated for [5000] ms. Reason: [Disassociated] 
15/06/11 11:08:38 INFO scheduler.DAGScheduler: Executor lost: 1 (epoch 0)
15/06/11 11:08:38 INFO storage.BlockManagerMasterEndpoint: Trying to remove executor 1 from BlockManagerMaster.
15/06/11 11:08:38 INFO storage.BlockManagerMasterEndpoint: Removing block manager BlockManagerId(1, jerryshao-desktop, 48633)
15/06/11 11:08:38 INFO storage.BlockManagerMaster: Removed 1 successfully in removeExecutor
{noformat}

This is the driver log, also YarnAllocator is correctly remove the metadata in {{processCompletedContainers()}} I cannot fully catch your meaning, would you please describe a little specifically?
;;;","11/Jun/15 09:24;mridulm80;kill -9 is not sufficient - sockets will get closed, which will cause akka to notify master.
To test, pulling the ethernet cable to the node should suffice.

Essentially, when the node goes completely MIA, spark-yarn does not handle it : example exceptions while shuffle fetch would be something like NoRouteToHost, etc.;;;","12/Jun/15 01:31;jerryshao;OK, thanks [~mridulm80], I will take a try, from my understanding, akka will also get notified if connection is abruptly lost, I didn't test it, will take a try.;;;","12/Jun/15 07:54;jerryshao;Hi [~mridulm80], I'm trying to simulate your scenario by pulling the ethernet cable while running the application, still I cannot reproduce like what you mentioned. When I pulled the cable, the Spark application is hung silent without any message, after some time the HeatbeatReceiver detects that one executor is lost, it will send kill request to ApplicationMaster, ApplicationMaster will call YarnAllocator to release the container and related metadata. From what I could observed, in YarnSchedulerBackend and YarnAllocator, the status is expected.

I'm not sure if it can exactly simulate what you mentioned above;;;","12/Jun/15 15:21;mridulm80;
I am not sure why the spark application would hang - if you pull the cable for a worker. What exactly was the behavior you observed ?

Note that we observe yarn detecting the missing node and deallocating all containers for all applications on the node - and notifies the corresponding application master's.
In spark-yarn, we clean up the yarn specific state for that. We just do not propagate that to scheduler backend (which, for example, spark-mesos scheduler does).

To elaborate, the exact scenario where we fairly regularly (about once a month) encounter is like this :

We run the spark application on about 600+ nodes on a much larger cluster, and during the course of the job, one or more nodes will fail [1].
The job typically is a cascade of maps followed by reduces - and so other than initial task, everything else pretty much runs on process local locality level (for maps).
When an executor goes MIA (does not respond to ping, etc [2]), shuffle fetches will fail - causing repeated attempts at reexecution, and eventual application hang [3].



[1] Not all node failures trigger this issue - which makes reproducing this unpredictable - hence needing to rely on logs.

[2] In our specific app, the timeout's for heartbeat is increased due to gc issues we see in spark - when executors are repeatedly killed just cos they were slower in responding to heartbeat.

[3] We have high threshold for task and application failures, since it is a long running job and there are usually frequent transient failures (particularly due to yarn aggresively police'ing the resource limits). 

;;;","06/Jul/15 19:44;apachespark;User 'mridulm' has created a pull request for this issue:
https://github.com/apache/spark/pull/7243;;;","15/Jul/15 21:43;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/7431;;;",,,,,,,,,,,,,,,,,,,,,
spark class command builder need read SPARK_JAVA_OPTS and SPARK_DRIVER_MEMORY properly,SPARK-8290,12836777,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,10/Jun/15 09:20,10/Jun/15 20:30,14/Jul/23 06:26,10/Jun/15 20:30,1.3.0,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,"SPARK_JAVA_OPTS was missed in reconstructing the launcher part, we should add it back so spark-class could read it.

The missing part is here: https://github.com/apache/spark/blob/1c30afdf94b27e1ad65df0735575306e65d148a1/bin/spark-class#L97.",,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 09:25:04 UTC 2015,,,,,,,,,,"0|i2fvbz:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"10/Jun/15 09:25;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/6741;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a specific stack size with all Java implementations to prevent stack overflows with certain tests,SPARK-8289,12836771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aroberts,aroberts,aroberts,10/Jun/15 08:47,11/Jun/15 07:42,14/Jul/23 06:26,11/Jun/15 07:41,1.5.0,,,,,,1.4.1,1.5.0,,,,,Tests,,,,0,,,,,,"Default stack sizes differ per Java implementation - so tests can pass for those with higher stack sizes (OpenJDK) but will fail with Oracle or IBM Java owing to lower default sizes. In particular we can see this happening with the JavaALSSuite - with 15 iterations, we get stackoverflow errors with Oracle and IBM Java. We don't with OpenJDK. This JIRA aims to address such an issue by providing a default specified stack size to be used for all Java distributions: 4096k specified for both SBT test args and for Maven test args (changing project/ScalaBuild.scala and pom.xml respectively). ",Anywhere whereby the Java vendor is not OpenJDK,apachespark,aroberts,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 11 07:41:20 UTC 2015,,,,,,,,,,"0|i2fvan:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/15 08:49;apachespark;User 'a-roberts' has created a pull request for this issue:
https://github.com/apache/spark/pull/6727;;;","11/Jun/15 07:41;srowen;Issue resolved by pull request 6727
[https://github.com/apache/spark/pull/6727];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
CombineSum should be calculated as unlimited decimal first,SPARK-8285,12836732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,navis,navis,navis,10/Jun/15 05:34,11/Jun/15 01:22,14/Jul/23 06:26,11/Jun/15 01:22,,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"{code:title=GeneratedAggregate.scala}
case cs @ CombineSum(expr) =>
        val calcType = expr.dataType
          expr.dataType match {
            case DecimalType.Fixed(_, _) =>
              DecimalType.Unlimited
            case _ =>
              expr.dataType
          }
{code}
calcType is always expr.dataType. credits are all belong to IntelliJ",,apachespark,navis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 05:37:08 UTC 2015,,,,,,,,,,"0|i2fv2f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/15 05:37;apachespark;User 'navis' has created a pull request for this issue:
https://github.com/apache/spark/pull/6736;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR createDataFrame is slow,SPARK-8277,12836595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,shivaram,shivaram,09/Jun/15 17:06,16/Nov/15 08:36,14/Jul/23 06:26,16/Nov/15 08:36,1.4.0,,,,,,1.6.0,,,,,,SparkR,,,,1,,,,,,"For example calling `createDataFrame` on the data from http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv takes a really long time

This is mainly because we try to convert a DataFrame to a List in order to parallelize it by rows and the conversion from DF to list is very slow for large data frames.",,apachespark,dsiegel,Elie A.,Emaasit,felixcheung,shivaram,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 16 08:36:40 UTC 2015,,,,,,,,,,"0|i2fu93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jun/15 04:45;felixcheung;what would be a better approach? Would it work to serialize the R native DataFrame into bytes and then run a version of SQLUtils. bytesToRow?;;;","30/Jun/15 17:17;shivaram;Yeah so the bottleneck is in converting R data frames from columns to a list of rows. It would be interesting to see if we can serialize each column at a time and then somehow add them as columns to the Scala DataFrame (or do a column to row conversion in Scala). [~cafreeman] was looking at some related stuff at some point.;;;","22/Oct/15 22:08;apachespark;User 'saurfang' has created a pull request for this issue:
https://github.com/apache/spark/pull/9234;;;","16/Nov/15 08:36;shivaram;Resolved by https://github.com/apache/spark/pull/9099;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Driver hangs up when yarn shutdown in client mode,SPARK-8273,12836493,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WangTao,WangTaoTheTonic,WangTaoTheTonic,09/Jun/15 09:07,12/Dec/22 18:10,14/Jul/23 06:26,21/Jan/17 14:03,1.3.1,1.4.0,,,,,1.4.2,1.5.0,,,,,Spark Core,YARN,,,0,,,,,,"In client mode, if yarn was shut down with spark application running, the application will hang up after several retries(default: 30) because the exception throwed by YarnClientImpl could not be caught by upper level, we should exit in case that user can not be aware that.",,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7934,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 20 11:34:07 UTC 2017,,,,,,,,,,"0|i2ftmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/15 09:28;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/6717;;;","20/Jan/17 11:34;gurwls223;Oh, [~andrewor14], this one also looks not resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
BigDecimal in parquet not working,SPARK-8272,12836477,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,bipin,bipin,09/Jun/15 07:42,06/Aug/15 23:13,14/Jul/23 06:26,06/Aug/15 23:13,1.3.1,,,,,,1.5.0,,,,,,Spark Core,SQL,,,0,sparksql,,,,,"When trying to save a DDF to parquet file I get the following errror:

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 311, localhost): java.lang.ClassCastException: scala.runtime.BoxedUnit cannot be cast to org.apache.spark.sql.types.Decimal
	at org.apache.spark.sql.parquet.RowWriteSupport.writePrimitive(ParquetTableSupport.scala:220)
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:192)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:171)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:134)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:671)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:689)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:689)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

I cannot save the dataframe. Please help.",Ubuntu 14.0 LTS,bipin,davies,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 06 23:13:24 UTC 2015,,,,,,,,,,"0|i2ftiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/15 14:33;viirya;Can you provide more information? Such as example codes, your data schema, etc.;;;","06/Aug/15 23:13;davies;This should be fixed 1.5 (not initialized decimal). If not, please re-open it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark: infinite loop during external sort ,SPARK-8202,12836396,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,09/Jun/15 06:30,18/Jun/15 20:50,14/Jul/23 06:26,18/Jun/15 20:50,1.4.0,,,,,,1.4.1,1.5.0,,,,,PySpark,,,,0,,,,,,"The batch size during external sort will grow up to max 10000, then shrink down to zero, causing infinite loop.

Given the assumption that the items usually have similar size, so we don't need to adjust the batch size after first spill.",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 18 20:50:33 UTC 2015,,,,,,,,,,"0|i2ft13:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"09/Jun/15 06:34;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6714;;;","09/Jun/15 06:40;davies;Workaround: increase the number of partitions during sort, or increase memory of python worker by: spark.python.worker.memory (default is 512M);;;","18/Jun/15 20:50;joshrosen;Fixed by Davies' PR for 1.4.1 and 1.5.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Exception in StreamingLinearAlgorithm on Stream with Empty RDD.,SPARK-8200,12836393,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pparkkin,pparkkin,pparkkin,09/Jun/15 06:28,10/Jun/15 22:28,14/Jul/23 06:26,10/Jun/15 22:27,1.3.1,,,,,,1.4.1,1.5.0,,,,,DStreams,MLlib,,,0,,,,,,"When training a streaming logistic regression model or a streaming linear regression model, any empty RDDs in a stream will cause an exception.

  java.lang.UnsupportedOperationException: empty collection
  at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1288)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
  at org.apache.spark.rdd.RDD.first(RDD.scala:1285)
  at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:215)
  at org.apache.spark.mllib.regression.StreamingLinearAlgorithm$$anonfun$trainOn$1.apply(StreamingLinearAlgorithm.scala:91)
  at org.apache.spark.mllib.regression.StreamingLinearAlgorithm$$anonfun$trainOn$1.apply(StreamingLinearAlgorithm.scala:85)
  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
","Ubuntu 14.04.2 LTS
Linux 3.13.0-45-generic #74-Ubuntu SMP Tue Jan 13 19:36:28 UTC 2015
java version ""1.8.0_25""
Java(TM) SE Runtime Environment (build 1.8.0_25-b17)
Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)
Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL
",apachespark,pparkkin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 22:27:42 UTC 2015,,,,,,,,,,"0|i2ft0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/15 06:30;pparkkin;PR: https://github.com/apache/spark/pull/6713;;;","09/Jun/15 06:31;apachespark;User 'pparkkin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6713;;;","10/Jun/15 22:27;srowen;Issue resolved by pull request 6713
[https://github.com/apache/spark/pull/6713];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Tasks that fail due to YARN preemption can cause job failure,SPARK-8167,12836250,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mcheah,pwoody,pwoody,08/Jun/15 19:26,17/May/20 17:48,14/Jul/23 06:26,10/Sep/15 18:59,1.3.1,,,,,,1.6.0,,,,,,Scheduler,Spark Core,YARN,,1,,,,,,"Tasks that are running on preempted executors will count as FAILED with an ExecutorLostFailure. Unfortunately, this can quickly spiral out of control if a large resource shift is occurring, and the tasks get scheduled to executors that immediately get preempted as well.

The current workaround is to increase spark.task.maxFailures very high, but that can cause delays in true failures. We should ideally differentiate these task statuses so that they don't count towards the failure limit.",,aash,andrewor14,apachespark,ashwinshankar77,cheolsoo,dougb,dubovsky,mcheah,mkim,nemccarthy,pwoody,radimk,sb58,snemeth,Steven Rand,vanzin,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 08 14:53:59 UTC 2016,,,,,,,,,,"0|i2fs5b:",9223372036854775807,,,,,,,,,,,,,,1.6.0,,,,,,,,,,,,,"08/Jun/15 19:47;mcheah;To be clear this is independent of SPARK-7451. SPARK-7451 helps for the case that executors die too many times from preemption, but it doesn't not help if the exact same task gets preempted many times.;;;","23/Jun/15 17:03;mcheah;I'm starting to work on this now, sorry for the delay.;;;","24/Jun/15 19:42;mcheah;What's curious here as I'm trying to design this is that it's not immediately obvious how to transfer the exit code of the executor from the remote machine back to the driver. If the Executor dies, the driver immediately sees the connection as dropped and just removes the Executor without question as to what the exit code was; it is hard to know what the exit code is in YARN mode in particular.

Does anyone have any thoughts as to how to get the exit code of the executor to the driver, in yarn-client mode?;;;","25/Jun/15 18:26;mcheah;[~joshrosen] any thoughts on this?;;;","27/Jun/15 02:34;mcheah;One thought is to have, whenever a task fails from an executor lost failure, logic specific to YARN to ask the YarnAllocator (ApplicationMaster) if the executor that was just lost had been preempted. There might be some nasty race conditions here though, and would require invoking a blocking RPC call inside of TaskSetManager.executorLost, or something similar - which is on the message loop of RpcEndpoint. And invoking a blocking RPC call in the message loop is something that's probably not desirable.;;;","30/Jul/15 08:01;zjffdu;[~mcheah] What's the status of this ticket ?  I don't think blocking RPC call is a good idea.  I think we could just send executor preempted message to driver when the container is preempted. And let driver to decrease the numTaskAttemptFails. Although we lose some consistency here, at least we could avoid job failures due to preemption. And I think there's some gap between 2 consecutive failed task attempt, very likely in the gap the driver has received the executor preempted message.  Thoughts ?;;;","06/Aug/15 22:04;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/8007;;;","13/Aug/15 00:23;vanzin;Sorry, reviewing your PR is on my list but this won't make it into 1.5.0.;;;","08/Mar/16 14:53;dubovsky;I'd like to have a question about this fix. In SparkUI in active stages view there is an entry saying this:

Tasks: Succeeded/Total
1480/2880 (1311 failed)

Does this number of failed tasks include those which ""failed"" because of preemption?
It's useful to know whether my job is failing (I should fix it) or resources are taken only (I should wait).

Thank you;;;",,,,,,,,,,,,,,,,,,,,
Run spark-shell cause NullPointerException,SPARK-8162,12836113,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,Sephiroth-Lin,Sephiroth-Lin,08/Jun/15 11:04,11/Feb/16 21:14,14/Jul/23 06:26,09/Jun/15 01:12,1.4.1,1.5.0,,,,,1.4.1,1.5.0,,,,,Build,Spark Shell,,,0,,,,,,"run spark-shell on latest master branch, then failed, details are:

{noformat}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.5.0-SNAPSHOT
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_40)
Type in expressions to have them evaluated.
Type :help for more information.
error: error while loading JobProgressListener, Missing dependency 'bad symbolic reference. A signature in JobProgressListener.class refers to term annotations
in package com.google.common which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling JobProgressListener.class.', required by /opt/apache/spark/lib/spark-assembly-1.5.0-SNAPSHOT-hadoop2.7.0.jar(org/apache/spark/ui/jobs/JobProgressListener.class)
java.lang.NullPointerException
	at org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:193)
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:68)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.spark.repl.SparkILoop.createSQLContext(SparkILoop.scala:1028)
	at $iwC$$iwC.<init>(<console>:9)
	at $iwC.<init>(<console>:18)
	at <init>(<console>:20)
	at .<init>(<console>:24)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:130)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:324)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:974)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:157)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:991)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:663)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

<console>:10: error: not found: value sqlContext
       import sqlContext.implicits._
              ^
<console>:10: error: not found: value sqlContext
       import sqlContext.sql
              ^
{noformat}

JDK: 1.8.0_40
Hadoop: 2.7.0",,apachespark,beloblotskiy,michael_han,mtthwcmpbll,Sephiroth-Lin,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 11 21:14:44 UTC 2016,,,,,,,,,,"0|i2fr93:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"08/Jun/15 11:12;apachespark;User 'Sephiroth-Lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6704;;;","09/Jun/15 00:59;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6711;;;","09/Jun/15 16:26;vanzin;So, I finally got some time to look at this. I see you guys are reverting the change that caused this, but I really would like to understand the underlying reason here.

I have Spark compiled with maven, which does shading, and this is what I get in my assembly ({{javap -v -l -p -constants -classpath assembly/target/spark-assembly_2.10-1.5.0-SNAPSHOT.jar org.apache.spark.ui.jobs.JobProgressListener}}):

{code}
Classfile jar:file:/work/cdh/spark/assembly/target/spark-assembly_2.10-1.5.0-SNAPSHOT.jar!/org/apache/spark/ui/jobs/JobProgressListener.class
  Last modified Jun 8, 2015; size 44441 bytes
  MD5 checksum 4b1b5b6036d729cfafbe2dbb97b4abe5
  Compiled from ""JobProgressListener.scala""
public class org.apache.spark.ui.jobs.JobProgressListener implements org.apache.spark.scheduler.SparkListener,org.apache.spark.Logging
  [blah blah blah]
  #1357 = Utf8               Lorg/spark-project/guava/annotations/VisibleForTesting;
  [blah blah blah]
  public void waitUntilExecutorsUp(int, long);
  [blah blah blah]
    RuntimeVisibleAnnotations:
      0: #1357()
{code}

So it seems like shading is working as it should. Do you have any env variables (such as ""SPARK_PREPEND_CLASSPATH"") set when running this?

Anyway, I have to run more tests to understand what's going on.

;;;","09/Jun/15 17:13;vanzin;Built and ran spark-shell, was able to hit the problem... I can't find a single reference to the unshaded annotation in the final assembly - I really wonder how the repl interpreter is finding it. I ran into a similar issue recently where scalac was mixing shaded and non-shaded guava classes and generating bytecode referencing the wrong type. I really don't know what's going on.

Anyway, removing the annotation sounds like the best solution currently.;;;","09/Dec/15 17:59;beloblotskiy;Just got the same problem with spark-1.5.2-bin-hadoop2.6 on Win7;;;","10/Dec/15 08:33;michael_han;Same with Aliaksei, Just got the same problem with spark-1.5.2-bin-hadoop2.6 on Win7;;;","11/Feb/16 21:14;mtthwcmpbll;I'm also running into this problem with the latest spark-1.6.0-bin-hadoop2.6 on a Windows 7 machine.;;;",,,,,,,,,,,,,,,,,,,,,,
externalBlockStoreInitialized is never set to be true,SPARK-8161,12836094,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shimingfei,shimingfei,shimingfei,08/Jun/15 09:28,17/May/20 18:21,14/Jul/23 06:26,17/Jun/15 20:40,1.4.0,,,,,,1.4.1,1.5.0,,,,,Block Manager,Spark Core,,,0,,,,,,"externalBlockStoreInitialized is never set to be true, which causes the blocks stored in ExternalBlockStore can not be removed.",,apachespark,shimingfei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 09:31:05 UTC 2015,,,,,,,,,,"0|i2fr4v:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"08/Jun/15 09:31;apachespark;User 'shimingfei' has created a pull request for this issue:
https://github.com/apache/spark/pull/6702;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect current database when creating datasource tables,SPARK-8156,12836059,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,baishuo,baishuo,baishuo,08/Jun/15 04:43,23/Jun/15 18:41,14/Jul/23 06:26,17/Jun/15 00:11,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,,,apachespark,baishuo,dougb,jakajancar,marmbrus,zhichao-li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 18:41:57 UTC 2015,,,,,,,,,,"0|i2fqpz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/15 04:57;baishuo;the following code:
hiveContext.sql(""""""use testdb"""""")
val df = (1 to 3).map(i => (i, s""val_$i"", i * 2)).toDF(""a"", ""b"", ""c"")
 df.write
            .format(""parquet"")
            .mode(SaveMode.Overwrite)
            .saveAsTable(""ttt3"")

the table ttt3 will be created under the database ""default"";;;","08/Jun/15 08:01;apachespark;User 'baishuo' has created a pull request for this issue:
https://github.com/apache/spark/pull/6695;;;","17/Jun/15 00:11;marmbrus;Issue resolved by pull request 6695
[https://github.com/apache/spark/pull/6695];;;","23/Jun/15 02:25;jakajancar;Can this be backported into 1.4?
I can prepare a pull request, if needed.;;;","23/Jun/15 18:41;marmbrus;We typically won't backport things that change behavior into a maintenance branches as we want to make it very easy for people to upgrade.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pipeline components should correctly implement copy,SPARK-8151,12836036,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mengxr,mengxr,mengxr,07/Jun/15 23:31,19/Jun/15 17:06,14/Jul/23 06:26,19/Jun/15 17:06,1.4.0,,,,,,1.4.1,1.5.0,,,,,ML,,,,0,,,,,,Some pipeline components (models and meta-algorithms) should correctly implement copy in order to work properly in pipeline fitting.,,apachespark,mengxr,yipjustin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8123,SPARK-8150,SPARK-8087,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 00:41:09 UTC 2015,,,,,,,,,,"0|i2fqlb:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"08/Jun/15 00:41;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/6622;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
IDFModel must implement copy,SPARK-8150,12836033,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,josephkb,josephkb,07/Jun/15 21:47,19/Jun/15 17:08,14/Jul/23 06:26,19/Jun/15 17:08,1.4.0,,,,,,1.4.1,1.5.0,,,,,ML,,,,0,,,,,,,,holden,josephkb,yuu.ishikawa@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-07 21:47:15.0,,,,,,,,,,"0|i2fqkn:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AM link download test can be flaky,SPARK-8136,12835865,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hshreedharan,hshreedharan,hshreedharan,05/Jun/15 23:30,07/Jun/15 10:34,14/Jul/23 06:26,07/Jun/15 10:34,,,,,,,1.5.0,,,,,,Tests,YARN,,,0,,,,,,"Sometimes YARN does not replace the link (or replaces it too soon) causing the YarnClusterSuite to fail. On a real cluster, the NM automatically redirects once the app is complete. So we should make the test less strict and have it only check the link's format rather than try to download the logs.",,apachespark,hshreedharan,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 06 01:25:22 UTC 2015,,,,,,,,,,"0|i2fpif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/15 01:25;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/6680;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bucketizer must implement copy,SPARK-8123,12835632,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,05/Jun/15 08:13,19/Jun/15 17:07,14/Jul/23 06:26,19/Jun/15 17:07,1.4.0,,,,,,1.4.1,1.5.0,,,,,ML,,,,0,,,,,,,,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-06-05 08:13:16.0,,,,,,,,,,"0|i2fo1j:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When using with Hadoop 1.x, ""spark.sql.parquet.output.committer.class"" is overriden by ""spark.sql.sources.outputCommitterClass""",SPARK-8121,12835596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,05/Jun/15 02:35,08/Jun/15 18:37,14/Jul/23 06:26,08/Jun/15 18:37,1.4.0,,,,,,1.4.1,,,,,,SQL,,,,0,,,,,,"When using Spark with Hadoop 1.x (the version I tested is 1.2.0) and {{spark.sql.sources.outputCommitterClass}} is configured, {{spark.sql.parquet.output.committer.class}} will be overriden. 

For example, if {{spark.sql.parquet.output.committer.class}} is set to {{FileOutputCommitter}}, while {{spark.sql.sources.outputCommitterClass}} is set to {{DirectParquetOutputCommitter}}, neither {{_metadata}} nor {{_common_metadata}} will be written because {{FileOutputCommitter}} overrides {{DirectParquetOutputCommitter}}.

The reason is that, {{InsertIntoHadoopFsRelation}} initializes the {{TaskAttemptContext}} before calling {{ParquetRelation2.prepareForWriteJob()}}, which sets up Parquet output committer class. In the meanwhile, in Hadoop 1.x, {{TaskAttempContext}} constructor clones the job configuration, thus doesn't share the job configuration passed to {{ParquetRelation2.prepareForWriteJob()}}.

This issue can be fixed by simply [switching these two lines|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/commands.scala#L285-L286].

Here is a Spark shell snippet for reproducing this issue:
{code}
import sqlContext._

sc.hadoopConfiguration.set(
  ""spark.sql.sources.outputCommitterClass"",
  ""org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter"")

sc.hadoopConfiguration.set(
  ""spark.sql.parquet.output.committer.class"",
  ""org.apache.spark.sql.parquet.DirectParquetOutputCommitter"")

range(0, 1).write.mode(""overwrite"").parquet(""file:///tmp/foo"")
{code}
Then check {{/tmp/foo}}, Parquet summary files are missing:
{noformat}
/tmp/foo
├── _SUCCESS
├── part-r-00001.gz.parquet
├── part-r-00002.gz.parquet
├── part-r-00003.gz.parquet
├── part-r-00004.gz.parquet
├── part-r-00005.gz.parquet
├── part-r-00006.gz.parquet
├── part-r-00007.gz.parquet
└── part-r-00008.gz.parquet
{noformat}",,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 18:37:15 UTC 2015,,,,,,,,,,"0|i2fo0f:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"05/Jun/15 09:30;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6669;;;","08/Jun/15 11:58;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6705;;;","08/Jun/15 18:37;yhuai;Issue resolved by pull request 6705
[https://github.com/apache/spark/pull/6705];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
HeartbeatReceiver should not adjust application executor resources,SPARK-8119,12835591,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,carlmartin,carlmartin,05/Jun/15 01:29,17/Feb/16 08:38,14/Jul/23 06:26,02/Jan/16 14:22,1.4.0,,,,,,1.5.0,,,,,,Spark Core,,,05/Jun/15 00:00,3,,,,,,"DynamicAllocation will set the total executor to a little number when it wants to kill some executors.
But in no-DynamicAllocation scenario, Spark will also set the total executor.
So it will cause such problem: sometimes an executor fails down, there is no more executor which will be pull up by spark.

=== EDIT by andrewor14 ===
The issue is that the AM forgets about the original number of executors it wants after calling sc.killExecutor. Even if dynamic allocation is not enabled, this is still possible because of heartbeat timeouts.

I think the problem is that sc.killExecutor is used incorrectly in HeartbeatReceiver. The intention of the method is to permanently adjust the number of executors the application will get. In HeartbeatReceiver, however, this is used as a best-effort mechanism to ensure that the timed out executor is dead.",,andrewor14,apachespark,azotcsit,carlmartin,damageboy,darabos,mkim,neelesh77,nemccarthy,ponkin,roji,saucam,zhpengg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8374,SPARK-9375,SPARK-11181,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 17 08:38:17 UTC 2016,,,,,,,,,,"0|i2fnzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/15 02:07;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/6662;;;","30/Jun/15 02:08;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/7107;;;","16/Jul/15 21:39;roji;Will this really not be fixed before 1.5? This issue makes Spark 1.4 unusable in a Yarn environment where preemption may happen....;;;","16/Jul/15 23:43;andrewor14;Hi [~roji], yes, we should fix it for 1.4.2 as well. Thanks for bringing that up.;;;","17/Jul/15 05:31;roji;Thanks Andrew!;;;","02/Aug/15 08:59;srowen;I attempted a back-port but this depends on SPARK-7835 and possibly other prior changes, which I'm not so familiar with.;;;","12/Aug/15 08:22;damageboy;Does this mean it's already fixed for the upcoming 1.5.0?
The only outstanding issue is for the 1.4.2 backport? ;;;","12/Aug/15 08:29;srowen;Yes, committed for 1.5.0. I don't know if it will actually go back into 1.4.x since it depends on other changes that aren't in 1.4.x. ;;;","12/Aug/15 16:15;andrewor14;It will be in 1.4.2. I just need to backport it.;;;","10/Sep/15 16:06;damageboy;Why was the target version moved to 1.5.1?
Wasn't this already marked as fixed for 1.5.0?
Is now pushed back?;;;","10/Sep/15 16:16;srowen;No, it's marked as Fixed for 1.5.0 which remains true. I did a bulk change of Target=1.5.0 to Target=1.5.1 which changed this one too, but then I noticed that didn't make sense; it's only left to be integrated into 1.4.2, so I restored that.;;;","02/Jan/16 14:22;srowen;I don't think this will be back-ported to 1.4.x at this point;;;","17/Feb/16 07:43;zhpengg;Hi [~srowen], I think it's really a serious bug, do you have any reason for not back-porting it to 1.4.x?;;;","17/Feb/16 08:38;srowen;I don't think there are any more 1.4.x releases to come. Still, if you open a clean back port to the branch I'll look at merging it.;;;",,,,,,,,,,,,,,,
sc.range() doesn't match python range(),SPARK-8116,12835560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,belisarius222,belisarius222,belisarius222,04/Jun/15 22:49,08/Jun/15 18:23,14/Jul/23 06:26,08/Jun/15 18:23,1.4.0,1.4.1,,,,,1.4.1,1.5.0,,,,,PySpark,,,,0,easyfix,,,,,"Python's built-in range() and xrange() functions can take 1, 2, or 3 arguments. Ranges with just 1 argument are probably used the most frequently, e.g.:
for i in range(len(myList)): ...

However, in pyspark, the SparkContext range() method throws an error when called with a single argument, due to the way its arguments get passed into python's range function.

There's no good reason that I can think of not to support the same syntax as the built-in function. To fix this, we can set the default of the sc.range() method's `stop` argument to None, and then inside the method, if it is None, replace `stop` with `start` and set `start` to 0, which is what the c implementation of range() does:
https://github.com/python/cpython/blob/master/Objects/rangeobject.c#L87",,apachespark,belisarius222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 23:03:07 UTC 2015,,,,,,,,,,"0|i2fnsn:",9223372036854775807,,,,,,,,,,,,,,1.4.1,,,,,,,,,,,,,"04/Jun/15 23:03;apachespark;User 'belisarius222' has created a pull request for this issue:
https://github.com/apache/spark/pull/6656;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Received block event count through the StreamingListener can be negative,SPARK-8112,12835535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,tdas,tdas,04/Jun/15 20:57,05/Jun/15 19:46,14/Jul/23 06:26,05/Jun/15 19:46,1.4.0,,,,,,1.4.1,1.5.0,,,,,DStreams,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8080,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 23:30:05 UTC 2015,,,,,,,,,,"0|i2fnn3:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"04/Jun/15 20:59;tdas;Take a look at SPARK-8080;;;","04/Jun/15 23:30;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6659;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
move the auto alias logic into Analyzer,SPARK-8104,12835451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,04/Jun/15 16:17,09/Dec/15 13:46,14/Jul/23 06:26,22/Jun/15 19:14,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Currently we auto alias expression in parser. However, during parser phase we don't have enough information to do the right alias. For example, Generator that has more than 1 kind of element need MultiAlias, ExtractValue don't need Alias if it's in middle of a ExtractValue chain.",,apachespark,cloud_fan,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6929,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 19:14:03 UTC 2015,,,,,,,,,,"0|i2fn53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/15 16:34;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/6647;;;","22/Jun/15 19:14;marmbrus;Issue resolved by pull request 6647
[https://github.com/apache/spark/pull/6647];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
DAGScheduler should not launch multiple concurrent attempts for one stage on fetch failures,SPARK-8103,12835449,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,irashid,irashid,04/Jun/15 16:13,21/Jul/15 19:22,14/Jul/23 06:26,20/Jul/15 17:30,1.4.0,,,,,,1.5.0,,,,,,Scheduler,Spark Core,,,0,,,,,,"When there is a fetch failure, {{DAGScheduler}} is supposed to fail the stage, retry the necessary portions of the preceding shuffle stage which generated the shuffle data, and eventually rerun the stage.  

We generally expect to get multiple fetch failures together, but only want to re-start the stage once.  The code already makes an attempt to address this https://github.com/apache/spark/blob/10ba1880878d0babcdc5c9b688df5458ea131531/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1108 .  

{code}
       // It is likely that we receive multiple FetchFailed for a single stage (because we have
        // multiple tasks running concurrently on different executors). In that case, it is possible
        // the fetch failure has already been handled by the scheduler.
        if (runningStages.contains(failedStage)) {
{code}

However, this logic is flawed because the stage may have been **resubmitted** by the time we get these fetch failures.  In that case, {{runningStages.contains(failedStage)}} will be true, but we've already handled these failures.

This results in multiple concurrent non-zombie attempts for one stage.  In addition to being very confusing, and a waste of resources, this also can lead to later stages being submitted before the previous stage has registered its map output.  This happens because

(a) when one attempt finishes all its tasks, it may not register its map output because the stage still has pending tasks, from other attempts https://github.com/apache/spark/blob/10ba1880878d0babcdc5c9b688df5458ea131531/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1046

{code}
            if (runningStages.contains(shuffleStage) && shuffleStage.pendingTasks.isEmpty) {
{code}

and (b) {{submitStage}} thinks the following stage is ready to go, because {{getMissingParentStages}} thinks the stage is complete as long it has all of its map outputs: https://github.com/apache/spark/blob/10ba1880878d0babcdc5c9b688df5458ea131531/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L397

{code}
                if (!mapStage.isAvailable) {
                  missing += mapStage
                }
{code}


So the following stage is submitted repeatedly, but it is doomed to fail because its shuffle output has never been registered with the map output tracker.  Here's an example failure in this case:
{noformat}
WARN TaskSetManager: Lost task 5.0 in stage 3.2 (TID 294, 192.168.1.104): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing output locations for shuffle ...
{noformat}


Note that this is a subset of the problems originally described in SPARK-7308, limited to just the issues effecting the DAGScheduler",,apachespark,darabos,irashid,lianhuiwang,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7308,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 21 19:22:03 UTC 2015,,,,,,,,,,"0|i2fn4n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/15 20:20;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/6750;;;","21/Jul/15 19:22;apachespark;User 'markhamstra' has created a pull request for this issue:
https://github.com/apache/spark/pull/7572;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"In yarn-cluster mode, ""--executor-cores"" can't be setted into SparkConf",SPARK-8099,12835376,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,meiyoula,meiyoula,meiyoula,04/Jun/15 12:31,05/Jun/15 18:47,14/Jul/23 06:26,05/Jun/15 18:46,1.0.0,,,,,,1.5.0,,,,,,YARN,,,,0,,,,,,"While testing dynamic executor allocation function, I set the executor cores with *--executor-cores 4* in spark-submit command. But in *ExecutorAllocationManager*, the *private val tasksPerExecutor =conf.getInt(""spark.executor.cores"", 1) / conf.getInt(""spark.task.cpus"", 1)* is still to be 1.",,apachespark,meiyoula,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 12:44:02 UTC 2015,,,,,,,,,,"0|i2fmon:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/15 12:44;apachespark;User 'XuTingjun' has created a pull request for this issue:
https://github.com/apache/spark/pull/6643;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Show correct length of bytes on log page,SPARK-8098,12835347,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,carsonwang,carsonwang,carsonwang,04/Jun/15 09:39,06/Jun/15 09:20,14/Jul/23 06:26,04/Jun/15 23:29,1.3.1,,,,,,1.3.2,1.4.1,1.5.0,,,,Web UI,,,,0,,,,,,"The log page should only show desired length of bytes. Currently it shows bytes from the startIndex to the end of the file. The ""Next"" button on the page is always disabled.",,apachespark,carsonwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 09:44:03 UTC 2015,,,,,,,,,,"0|i2fmi7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/15 09:44;apachespark;User 'carsonwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6640;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark package dependencies not resolved when package is in local-ivy-cache,SPARK-8095,12835288,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,brkyvz,eronwright,eronwright,04/Jun/15 04:37,24/Jun/15 16:59,14/Jul/23 06:26,24/Jun/15 16:59,1.4.0,,,,,,1.4.1,1.5.0,,,,,Spark Submit,,,,0,backport-needed,,,,,"Given a dependency expressed with '--packages', the transitive dependencies are supposed to be automatically included. This is true for most repository types including local-m2-cache, Spark Packages, and central.   For ivy-local-cache, it is not.
",,apachespark,brkyvz,eronwright,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7205,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 21 03:46:29 UTC 2015,,,,,,,,,,"0|i2fm5b:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.1,1.5.0,,,,,,,,,,,"04/Jun/15 04:38;eronwright;Note that IBiblioResolver::setUsepoms is not set to true on the local-ivy-cache resolver, yet is set on all other resolvers.   Speculating about the root cause.;;;","04/Jun/15 04:43;brkyvz;In the local ivy cache, it should use the `ivy.xml` instead of the pom. That's why, setUsePoms is set to false. At least that was my understanding... Doesn't it work?;;;","04/Jun/15 05:22;eronwright;Tested with 1.4.0RC4.

When package is in local-ivy-cache (incorrect behavior):
{code}
        :: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found EronWright#mypackage;0.0.7 in local-ivy-cache
        :: resolution report :: resolve 1028ms :: artifacts dl 11ms
	:: modules in use:
	EronWright#mypackage;0.0.7 from local-ivy-cache in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   1   |   1   |   0   |   0   ||   1   |   1   |
	---------------------------------------------------------------------
{code}

When package is in local-m2-cache (correct behavior):
{code}
        :: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
	confs: [default]
	found EronWright#mypackage;0.0.7 in local-m2-cache
	found org.deeplearning4j#dl4j-spark;0.0.3.3.3.alpha1 in list
	...
        :: resolution report :: resolve 2509ms :: artifacts dl 101ms
	:: modules in use:
	EronWright#mypackage;0.0.7 from local-m2-cache in [default]
	org.deeplearning4j#dl4j-spark;0.0.3.3.3.alpha1 from list in [default]
	...
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   37  |   1   |   1   |   2   ||   34  |   1   |
	---------------------------------------------------------------------
{code};;;","04/Jun/15 23:24;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6658;;;","12/Jun/15 19:13;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/6788;;;","21/Jun/15 03:46;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/6923;;;",,,,,,,,,,,,,,,,,,,,,,,
Spark 1.4 branch's new JSON schema inference has changed the behavior of handling inner empty JSON object.,SPARK-8093,12835260,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,NathanHowell,rhbutani,rhbutani,04/Jun/15 01:04,19/Jun/15 23:28,14/Jul/23 06:26,19/Jun/15 23:23,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"This is similar to SPARK-3365. Sample json is attached. Code to reproduce
{code}
var jsonDF = read.json(""/tmp/t1.json"")
jsonDF.write.parquet(""/tmp/t1.parquet"")
{code}

The 'integration' object is empty in the json.
StackTrace:
{code}
....
Caused by: java.io.IOException: Could not read footer: java.lang.IllegalStateException: Cannot build an empty group
	at parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:238)
	at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newParquet.scala:369)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$metadataCache$lzycompute(newParquet.scala:154)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$metadataCache(newParquet.scala:152)
	at org.apache.spark.sql.parquet.ParquetRelation2.refresh(newParquet.scala:197)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:134)
	... 69 more
Caused by: java.lang.IllegalStateException: Cannot build an empty group
{code}",,apachespark,rhbutani,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jun/15 01:04;rhbutani;t1.json;https://issues.apache.org/jira/secure/attachment/12737427/t1.json",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 23:28:37 UTC 2015,,,,,,,,,,"0|i2flz3:",9223372036854775807,,,,,yhuai,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"05/Jun/15 18:09;yhuai;[~rhbutani] Is your test based on RC4?;;;","05/Jun/15 18:53;rhbutani;yes;;;","05/Jun/15 18:57;yhuai;When you get time, can you try it with master? We just bumped the parquet to 1.7. I am wondering if the logic of ParquetFileReader.readAllFootersInParallel has been changed to handle it or now.;;;","05/Jun/15 23:55;rhbutani;I get the same error with master;;;","06/Jun/15 05:01;yhuai;Seems the new JSON type infer code has a behavior change.
For the following command,
{code}
val df = sqlContext.jsonRDD(sc.parallelize(""""""{""a"":{}, ""b"":1}"""""" :: Nil))
{code}
Spark 1.3 returns
{code}
df: org.apache.spark.sql.DataFrame = [b: bigint]
{code}
But Spark 1.4 returns
{code}
df: org.apache.spark.sql.DataFrame = [a: struct<>, b: bigint]
{code};;;","06/Jun/15 05:06;yhuai;For now, to keep the same behavior with 1.3, the workaround is to set ""spark.sql.json.useJacksonStreamingAPI"" to false (use the old code path).;;;","13/Jun/15 17:53;apachespark;User 'NathanHowell' has created a pull request for this issue:
https://github.com/apache/spark/pull/6799;;;","19/Jun/15 23:23;yhuai;Issue resolved by pull request 6799
[https://github.com/apache/spark/pull/6799];;;","19/Jun/15 23:28;yhuai;With https://github.com/apache/spark/pull/6799, we have changed the behavior back to Spark 1.3's behavior. Empty inner structs will not be in the schema. ;;;",,,,,,,,,,,,,,,,,,,,
OneVsRest doesn't allow flexibility in label/ feature column renaming,SPARK-8092,12835258,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rams,rams,rams,04/Jun/15 00:58,24/Jul/15 05:38,14/Jul/23 06:26,24/Jul/15 05:38,,,,,,,1.5.0,,,,,,ML,,,,0,,,,,,,,apachespark,rams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8799,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 08:48:27 UTC 2015,,,,,,,,,,"0|i2flyn:",9223372036854775807,,,,,josephkb,,,,,,,,,1.5.0,,,,,,,,,,,,,"04/Jun/15 01:05;apachespark;User 'harsha2010' has created a pull request for this issue:
https://github.com/apache/spark/pull/6631;;;","24/Jun/15 08:48;srowen;[~rams] let's not set fix version until it's resolved
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SerializationDebugger does not handle classes with writeObject method,SPARK-8091,12835221,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,03/Jun/15 21:53,19/Jun/15 18:07,14/Jul/23 06:26,19/Jun/15 17:53,1.4.0,,,,,,1.4.1,1.5.0,,,,,Spark Core,,,,0,,,,,,"SerializationDebugger skips testing an object whose class has writeObject(), as it was not trivial to test the serializability all the arbitrary stuff that writeObject() could write. ",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 01:50:07 UTC 2015,,,,,,,,,,"0|i2flqf:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"04/Jun/15 01:50;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6625;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SerializationDebugger does not handle classes with writeReplace correctly,SPARK-8090,12835220,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,03/Jun/15 21:50,19/Jun/15 18:07,14/Jul/23 06:26,19/Jun/15 17:53,1.4.0,,,,,,1.4.1,1.5.0,,,,,Spark Core,,,,0,,,,,,"The following class with not serializable object used through writeReplace will not be caught correctly by the SerializationDebugger
{code}
class SerializableClassWithWriteReplace()
  extends Serializable {
  private def writeReplace(): Object = {
    new NotSerializableObjectI()
  }
}
{code}

The reason is that SerializationDebugger does not check the type of the replaced object (whether serializable or not). ",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 22:00:07 UTC 2015,,,,,,,,,,"0|i2flq7:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"03/Jun/15 22:00;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6625;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ExecutionAllocationManager spamming INFO logs about ""Lowering target number of executors""",SPARK-8088,12835211,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdub,rdub,rdub,03/Jun/15 21:27,03/Jun/15 23:55,14/Jul/23 06:26,03/Jun/15 23:55,1.3.1,1.4.0,,,,,1.4.1,1.5.0,,,,,Spark Core,,,,0,,,,,,"I am running a {{spark-shell}} built at 1.4.0-rc4, with:

{code}
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=5 \
  --conf spark.dynamicAllocation.maxExecutors=300 \
  --conf spark.dynamicAllocation.schedulerBacklogTimeout=3 \
  --conf spark.dynamicAllocation.executorIdleTimeout=600 \
{code}

I can't really type any commands because I am getting 10 of these per second:

{code}
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
{code}

It should not print anything if it is not in fact lowering the number of executors / is already at the minimum, right?",,apachespark,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 21:52:04 UTC 2015,,,,,,,,,,"0|i2flo7:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"03/Jun/15 21:52;apachespark;User 'ryan-williams' has created a pull request for this issue:
https://github.com/apache/spark/pull/6624;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PipelineModel.copy didn't copy the stages,SPARK-8087,12835202,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,mengxr,mengxr,mengxr,03/Jun/15 21:02,19/Jun/15 17:08,14/Jul/23 06:26,19/Jun/15 17:08,1.4.0,,,,,,1.4.1,1.5.0,,,,,ML,,,,0,,,,,,So extra params in transform do not work.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 21:19:02 UTC 2015,,,,,,,,,,"0|i2flm7:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"03/Jun/15 21:19;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/6622;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pass in user-specified schema in read.df,SPARK-8085,12835155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,shivaram,shivaram,03/Jun/15 19:20,06/Jun/15 09:20,14/Jul/23 06:26,05/Jun/15 17:19,1.4.0,,,,,,1.4.1,1.5.0,,,,,SparkR,,,,0,,,,,,This will help cases where we use the CSV reader and want each column to be of a specific type,,aeskilson,apachespark,dsiegel,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 05 17:19:28 UTC 2015,,,,,,,,,,"0|i2flev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/15 20:49;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/6620;;;","05/Jun/15 17:19;shivaram;Issue resolved by pull request 6620
[https://github.com/apache/spark/pull/6620];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix return to drivers link in Mesos driver page,SPARK-8083,12835138,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tnachen,tnachen,tnachen,03/Jun/15 18:27,03/Jun/15 21:58,14/Jul/23 06:26,03/Jun/15 21:58,1.4.0,,,,,,1.4.1,1.5.0,,,,,Mesos,Web UI,,,0,,,,,,"The current path is set to ""/"" but this doesn't work with a proxy. We need to prepend the proxy base uri if it's set.",,apachespark,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 18:28:05 UTC 2015,,,,,,,,,,"0|i2flbb:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"03/Jun/15 18:28;apachespark;User 'tnachen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6615;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom Receiver.store with Iterator type do not give correct count at Spark UI,SPARK-8080,12835075,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dibbhatt,dibbhatt,dibbhatt,03/Jun/15 14:51,19/Jun/15 03:09,14/Jul/23 06:26,19/Jun/15 03:01,1.2.2,1.3.1,,,,,1.4.1,1.5.0,,,,,DStreams,Web UI,,,0,,,,,,"In Custom receiver if I call store with Iterator type (store(dataIterator: Iterator[T]): Unit ) , Spark UI does not show the correct count of records in block which leads to wrong value for Input Rate, Scheduling Delay and Input SIze. ",,apachespark,dibbhatt,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8112,,,,,,,,,,,,,,,"03/Jun/15 14:56;dibbhatt;screenshot.png;https://issues.apache.org/jira/secure/attachment/12737292/screenshot.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 15:15:03 UTC 2015,,,,,,,,,,"0|i2fkxj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/15 14:54;apachespark;User 'dibbhatt' has created a pull request for this issue:
https://github.com/apache/spark/pull/6614;;;","04/Jun/15 20:56;tdas;[~zsxwing] Take a look at the screenshot attached to the JIRA. We should not be showing negative numbers in the input size. I am guessing that this is happening because the num of records reported by ReceivedBlockInfo is -1 (to signify lack of information), which gets added up to become -4). This should not happen I am filing a separate JIRA for this, can you take a look at the issue?;;;","06/Jun/15 08:50;srowen;[~dibbhatt] Don't set fix version.;;;","07/Jun/15 22:05;tdas;Just to add to what Sean Owen said, the model we follow is that we set ""Target version"" to denote what is target version that this fix is *planned* to be available. And ""Fix version"" is set to confirm that this fix is actually available in those versions. The semantic difference helps us disambiguated between planned features and achieved features.;;;","08/Jun/15 15:15;apachespark;User 'dibbhatt' has created a pull request for this issue:
https://github.com/apache/spark/pull/6707;;;",,,,,,,,,,,,,,,,,,,,,,,,
NPE when HadoopFsRelation.prepareForWriteJob throws exception,SPARK-8079,12835059,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,03/Jun/15 13:58,06/Jun/15 10:57,14/Jul/23 06:26,06/Jun/15 09:26,1.4.0,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"Take {{ParquetRelation2}} as an example, the following Spark shell code may cause an unexpected NPE:
{code}
import sqlContext._
import sqlContext.implicits._

range(1, 3).select($""id"" as ""a b"").write.format(""parquet"").save(""file:///tmp/foo"")
{code}
Exceptions thrown:
{noformat}
import sqlContext._
import sqlContext.implicits._

range(1, 3).select($""id"" as ""a b"").write.format(""parquet"").save(""file:///tmp/foo"")

java.lang.RuntimeException: Attribute name ""a b"" contains invalid character(s) among "" ,;{}()   ="". Please use alias to rename it.
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$checkSpecialCharacters$2.apply(ParquetTypes.scala:414)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$checkSpecialCharacters$2.apply(ParquetTypes.scala:412)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.checkSpecialCharacters(ParquetTypes.scala:412)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.convertToString(ParquetTypes.scala:423)
        at org.apache.spark.sql.parquet.RowWriteSupport$.setSchema(ParquetTableSupport.scala:383)
        at org.apache.spark.sql.parquet.ParquetRelation2.prepareJobForWrite(newParquet.scala:230)
        ...
java.lang.NullPointerException
        at org.apache.spark.sql.sources.BaseWriterContainer.abortJob(commands.scala:372)
        at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:137)
        at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:114)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
        ...
{noformat}
Note that the first {{RuntimeException}} is expected, while the following NPE is not.

The reason of the NPE is that, {{BaseWriterContainer.driverSideSetup()}} calls {{relation.prepareForWriteJob()}} AND initializes the {{OutputCommitter}} used for the subsequent write job. However, if the former throws an exception, the latter is not properly initialized, thus an NPE is thrown when aborting the job because the {{OutputCommitter}} is still null.",,apachespark,lian cheng,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 06 09:26:45 UTC 2015,,,,,,,,,,"0|i2fktz:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"03/Jun/15 14:12;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6612;;;","06/Jun/15 08:33;lian cheng;Bascially, in {{InsertIntoHadoopFsRelation}} we need to be careful whenever a user defined method is invoked.  Because they may throw unexpected exception and causes the write job/task to abort.  More specifically, we need to keep an eye on the following methods:

- {{HadoopFsRelation.prepareForWriteJob}}: This method is called on the driver side *before* the write job is issued. It initializes the write job by decorating the {{job}} instance passed in, and returning an {{OutputWriterFactory}} which produces {{OutputWriter}} instances on the executor side. Note that, if this method throws any exception, we should *not* abort the job since the job hasn't been issued yet. Especially, the {{OutputCommitter}} used to commit/abort the job may not be properly initialized yet (which is the case described in this ticket).
- {{OutputWriter.<init>}}, {{OutputWriter.write}}, and {{OutputWriter.close}}: These methods are called on executor side while a write task is being executed. Failures of these methods require task abortion.;;;","06/Jun/15 09:26;lian cheng;Issue resolved by pull request 6612
[https://github.com/apache/spark/pull/6612];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Spark master URL conflict between MASTER env variable and --master command line option,SPARK-8063,12834924,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sunrui,sunrui,sunrui,03/Jun/15 04:31,03/Jun/15 18:58,14/Jul/23 06:26,03/Jun/15 18:57,1.4.0,,,,,,1.4.1,1.5.0,,,,,SparkR,,,,0,,,,,,"Currently, Spark supports several ways to specify the Spark master URL, like --master option for spark-submit, spark.master configuration option, MASTER env variable. They have different precedences, for example, --master overrides MASTER if both are specified.

However, for SparkR shell, it always use the master URL specified by MASTER, not honoring --master.",,apachespark,shivaram,sunrui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 18:57:24 UTC 2015,,,,,,,,,,"0|i2fk1z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/15 04:45;apachespark;User 'sun-rui' has created a pull request for this issue:
https://github.com/apache/spark/pull/6605;;;","03/Jun/15 18:57;shivaram;Issue resolved by pull request 6605
[https://github.com/apache/spark/pull/6605];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in SparkHadoopUtil.getFileSystemThreadStatistics,SPARK-8062,12834911,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,03/Jun/15 02:27,08/Jun/15 17:52,14/Jul/23 06:26,08/Jun/15 17:51,1.2.1,,,,,,1.2.3,,,,,,Spark Core,,,,0,,,,,,"I received the following error report from a user:

While running a Spark Streaming job that reads from MapRfs and writes to HBase using Spark 1.2.1, the job intermittently experiences a total job failure due to the following errors:

{code}
15/05/28 10:35:50 ERROR executor.Executor: Exception in task 1.1 in stage 6.0 (TID 24) 
java.lang.NullPointerException 
at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$4.apply(SparkHadoopUtil.scala:178) 
at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$4.apply(SparkHadoopUtil.scala:178) 
at scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264) 
at scala.collection.Iterator$class.foreach(Iterator.scala:727) 
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) 
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) 
at scala.collection.AbstractIterable.foreach(Iterable.scala:54) 
at scala.collection.TraversableLike$class.filter(TraversableLike.scala:263) 
at scala.collection.AbstractTraversable.filter(Traversable.scala:105) 
at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatistics(SparkHadoopUtil.scala:178) 
at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:139) 
at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:116) 
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:107) 
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:69) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280) 
at org.apache.spark.rdd.RDD.iterator(RDD.scala:247) 
at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280) 
at org.apache.spark.rdd.RDD.iterator(RDD.scala:247) 
at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280) 
at org.apache.spark.rdd.RDD.iterator(RDD.scala:247) 
at org.apache.spark.rdd.FilteredRDD.compute(FilteredRDD.scala:34) 
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280) 
at org.apache.spark.rdd.RDD.iterator(RDD.scala:247) 
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68) 
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) 
at org.apache.spark.scheduler.Task.run(Task.scala:56) 
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200) 
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 
at java.lang.Thread.run(Thread.java:744) 
15/05/28 10:35:50 INFO executor.CoarseGrainedExecutorBackend: Got assigned task 25 
15/05/28 10:35:50 INFO executor.Executor: Running task 2.1 in stage 6.0 (TID 25) 
15/05/28 10:35:50 INFO rdd.NewHadoopRDD: Input split: hdfs:/[REDACTED] 
15/05/28 10:35:50 ERROR executor.Executor: Exception in task 2.1 in stage 6.0 (TID 25) 
java.lang.NullPointerException 
at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$4.apply(SparkHadoopUtil.scala:178) 
at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$4.apply(SparkHadoopUtil.scala:178) 
at scala.collection.TraversableLike$$anonfun$filter$1.apply(TraversableLike.scala:264) 
at scala.collection.Iterator$class.foreach(Iterator.scala:727) 
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) 
at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) 
{code}

Diving into the code here:

The NPE is occurring on this line of SparkHadoopUtil (in 1.2.1.): https://github.com/apache/spark/blob/v1.2.1/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala#L178

Here's that block of code from 1.2.1 (it's the same in 1.2.2):

{code}
  private def getFileSystemThreadStatistics(path: Path, conf: Configuration): Seq[AnyRef] = {
    val qualifiedPath = path.getFileSystem(conf).makeQualified(path)
    val scheme = qualifiedPath.toUri().getScheme()
    val stats = FileSystem.getAllStatistics().filter(_.getScheme().equals(scheme))   // <--- exception occurs at this line
    stats.map(Utils.invoke(classOf[Statistics], _, ""getThreadStatistics""))
  }
{code}

Since the top call on the stack was {{org.apache.spark.deploy.SparkHadoopUtil$$anonfun$4}}, I'm assuming that the _.getScheme().equals(scheme) call here is failing because FileSystem.getAllStatistics() is returning a collection that has a null element or that _.getScheme() is null.

Diving into the Hadoop source, it looks like FileSystem.getAllStatistics() accesses some synchronized static state to return statistics for all Hadoop filesystems created within the JVM. I wonder if it's possible that some code is nondeterministically creating a new FIleSystem instance for a FileSystem that lacks a scheme, causing entires to be stored in the statistics map that will return null when we call getScheme() on them.

I am unable to reproduce this issue myself, but I think that we can fix it for the user by adding try-catch blocks to prevent errors in metrics collection from leading to task failures.","MapR 4.0.1, Hadoop 2.4.1, Yarn",apachespark,joshrosen,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 17:51:48 UTC 2015,,,,,,,,,,"0|i2fjz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/15 02:28;joshrosen;[~sandyr], do you have any thoughts on what might be causing this error? If you can't identify any obvious causes, then I think we should move forward with defensive checks to log and ignore exceptions here.;;;","03/Jun/15 06:57;sandyr;[~joshrosen] nothing sticks out to me past what you've found.  My other suspicion was that somehow the scheme from the path was coming in as null, but the implementation of Path#makeQualified seems like it should always set a scheme.  Given that we've never come across it, it seems possible that it's related to something maprfs is doing?  Defensive checks seem reasonable to me.;;;","03/Jun/15 18:11;joshrosen;I don't have a copy of MapRFS to play around with, so I'm going to see if I can reproduce this by creating a buggy mocked FileSystem and registering it with the JVM-wide FileSystem registry.  I think that just adding some defensive checks is definitely the way to go, though; patch pending soon.;;;","03/Jun/15 20:48;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6618;;;","08/Jun/15 17:51;joshrosen;Issue resolved by pull request 6618
[https://github.com/apache/spark/pull/6618];;;",,,,,,,,,,,,,,,,,,,,,,,,
Call TaskAttemptContext.getTaskAttemptID using Reflection,SPARK-8057,12834891,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,03/Jun/15 00:16,28/Aug/15 16:08,14/Jul/23 06:26,07/Aug/15 04:42,1.3.1,,,,,,1.4.2,1.5.0,,,,,Spark Core,,,,1,,,,,,"Someone may use the Spark core jar in the maven repo with hadoop 1. SPARK-2075 has already resolved the compatibility issue to support it. But ""SparkHadoopMapRedUtil.commitTask"" broke it recently.",,apachespark,rdub,shivaram,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8311,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 00:20:03 UTC 2015,,,,,,,,,,"0|i2fjun:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/15 00:20;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6599;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive on Spark: CAST string AS BIGINT produces wrong value,SPARK-8052,12834837,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,andrew.kurochkin,andrew.kurochkin,02/Jun/15 22:12,20/Jul/15 04:01,14/Jul/23 06:26,13/Jun/15 23:41,1.3.1,,,,,,1.4.2,1.5.0,,,,,,,,,0,,,,,,"Example hive query:
SELECT CAST(""775983671874188101"" as BIGINT)
produces:           775983671874188160L
Look at: last 2 digits.",Spark Standalone mode,andrew.kurochkin,apachespark,glenn.strycker@gmail.com,marmbrus,rxin,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8892,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 20 04:00:33 UTC 2015,,,,,,,,,,"0|i2fjin:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.5.0,,,,,,,,,,,,,"02/Jun/15 22:28;andrew.kurochkin;Applies to Hive. Wrong project.;;;","03/Jun/15 20:22;andrew.kurochkin;""spark-sql bug is not a hive bug, can you please open a jira for spark project?"";;;","04/Jun/15 16:21;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6645;;;","13/Jun/15 23:41;marmbrus;Issue resolved by pull request 6645
[https://github.com/apache/spark/pull/6645];;;","20/Jul/15 04:00;rxin;I also backported this into 1.4 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,
StringIndexerModel (and other models) shouldn't complain if the input column is missing.,SPARK-8051,12834812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,02/Jun/15 21:06,03/Jun/15 22:16,14/Jul/23 06:26,03/Jun/15 22:16,1.4.0,,,,,,1.4.1,1.5.0,,,,,ML,,,,0,,,,,,"If a transformer is not used during transformation, it should keep silent if the input column is missing.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 22:16:53 UTC 2015,,,,,,,,,,"0|i2fjdr:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"02/Jun/15 21:30;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/6595;;;","03/Jun/15 22:16;josephkb;Issue resolved by pull request 6595
[https://github.com/apache/spark/pull/6595];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
OneVsRest's output includes a temp column,SPARK-8049,12834793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,02/Jun/15 20:24,24/Jun/15 08:49,14/Jul/23 06:26,24/Jun/15 08:49,1.4.0,,,,,,1.4.1,1.5.0,,,,,ML,,,,0,,,,,,"The temp accumulator column ""mbc$acc"" is included in the output which should be removed with withoutColumn.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 20:39:10 UTC 2015,,,,,,,,,,"0|i2fj9j:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"02/Jun/15 20:39;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/6592;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack overflow in query parser when there is too many where,SPARK-8045,12834631,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,LabOctoCat,LabOctoCat,02/Jun/15 15:26,11/Aug/16 02:45,14/Jul/23 06:26,11/Aug/16 02:45,1.3.0,1.3.1,,,,,2.0.0,,,,,,SQL,,,,0,,,,,,"A while ago we ran into a stack overflow in the parsing with one our query. It's still an issue (tested this morning) so we decided to report it as a bug.

If run the query in attachment you get a stack overflow error probably because there to many where. Granted, this query isn't pretty, but still the parser shouldn't stack overflow.

FYI, the where clause where used for partition pruning.

Attachments: https://gist.github.com/anonymous/9aa852b7796b3918013c",,dongjoon,LabOctoCat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/16 02:40;dongjoon;longSql.txt;https://issues.apache.org/jira/secure/attachment/12823162/longSql.txt",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 11 02:45:50 UTC 2016,,,,,,,,,,"0|i2firj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/16 02:43;dongjoon;Hi, [~LabOctoCat].

Spark 2.0 now runs your long query with many WHERE correctly. I attached the test file from your Gist and the following is the result.

{code}
val longSQL = """"""
SELECT *
FROM VALUES (2015, 1, 1, 1) T(year, month, day, is_active)
WHERE (year = 2015 AND month = 1 AND day = 1 AND is_active = 1) OR
      (year = 2014 AND month = 12 AND day = 7 AND is_active = 1) OR
      (year = 2014 AND month = 11 AND day = 25 AND is_active = 1) OR
      (year = 2014 AND month = 8 AND day = 10 AND is_active = 1) OR
      (year = 2014 AND month = 7 AND day = 25 AND is_active = 1) OR
      (year = 2014 AND month = 6 AND day = 11 AND is_active = 1) OR
      (year = 2014 AND month = 5 AND day = 18 AND is_active = 1) OR
      (year = 2014 AND month = 11 AND day = 20 AND is_active = 1) OR
      (year = 2014 AND month = 8 AND day = 9 AND is_active = 1) OR
      (year = 2014 AND month = 11 AND day = 30 AND is_active = 1) OR
      (year = 2014 AND month...
""""""
sql(longSQL).show

// Exiting paste mode, now interpreting.

+----+-----+---+---------+
|year|month|day|is_active|
+----+-----+---+---------+
|2015|    1|  1|        1|
+----+-----+---+---------+
{code};;;","11/Aug/16 02:45;dongjoon;New parser can handle this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
update NaiveBayes and SVM examples in doc,SPARK-8043,12834569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,02/Jun/15 12:55,03/Jun/15 06:18,14/Jul/23 06:26,03/Jun/15 06:16,1.4.0,,,,,,1.4.1,1.5.0,,,,,MLlib,,,,0,,,,,,"I found some issues during testing the save/load examples in markdown Documents, as a part of 1.4 QA plan

",,apachespark,mengxr,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7541,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 06:16:49 UTC 2015,,,,,,,,,,"0|i2fidr:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"02/Jun/15 12:58;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/6584;;;","03/Jun/15 06:16;mengxr;Issue resolved by pull request 6584
[https://github.com/apache/spark/pull/6584];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM when using DataFrame join operation,SPARK-8039,12834541,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,WillCup,WillCup,02/Jun/15 10:13,07/Jul/15 10:45,14/Jul/23 06:26,07/Jul/15 10:45,1.3.0,,,,,,,,,,,,Spark Submit,,,,0,,,,,,"Code:
val sumRdd = sqlContext.sql(""select d.id,changeType, sum(p.pointsNum) as total from "" +
      ""app_driver_point_change p join app_driver_points dp join app_user_basic d join app_user_register_log reg "" +
      ""on reg.add_time between \""2015-05-30\"" and \""2015-06-01\"" and  p.pointId = dp.Id and dp.driverId = d.id"" +
      "" and reg.id = d.id group by d.id, changeType"")

    /*
     * [2135031,DRIVER_ADD_CAR_LENGTH,5]
     * StructType(StructField(id,LongType,false), StructField(changeType,StringType,true), StructField(total,LongType,true))
     */

    sumRdd.registerTempTable(""sum_rdd"")

    val re = sumRdd.as(""sr"").join(app_user_basic.as(""ub""), $""sr.id"" === $""ub.id"")
      .select(""sr.*"", ""ub.phone"").as(""d"")
      .join(app_vehicles.as(""av""), $""d.id"" === $""av.belong_uid"", ""left"")
      .selectExpr(""d.id"", ""d.phone"",""av.type as car_type"", ""av.length as car_length"",""av.capacity as car_capacity"",
        ""av.number as car_number"", ""av.license_auth as car_auth"").as(""tmp"")
      .join(app_user_info.as(""ui""))
      .select(""tmp.*"", ""ui.id_card"", ""ui.id_card_auth"").as(""tmp"") // driver + car
      .join(getCarAreaDF(sc, sqlContext), $""tmp.car_number"".startsWith($""car_num""), ""left"")
      .select(""tmp.*"", ""car_area"").as(""tmp"")
      .join(getMobileAreaDF(sc, sqlContext).as(""ma""), $""tmp.phone"".startsWith($""ma.phonePrefix""), ""left"")
      .select(""tmp.*"", ""ma.phone_area"") // driver + car + driver_area + car)_area


There will be tow kinds of OOM exception thrown by this code... Does the join action invoked in distributed spark? I wonder if the broadcast operation delivered too much data...How should I solve this problen
Exception:
15/06/02 18:02:44 WARN TaskSetManager: Lost task 17.0 in stage 12.0 (TID 431, eunke-dp-ana-005): java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.nio.ByteBuffer.wrap(ByteBuffer.java:369)
        at com.mysql.jdbc.StringUtils.toString(StringUtils.java:1871)
        at com.mysql.jdbc.ResultSetRow.getString(ResultSetRow.java:821)
        at com.mysql.jdbc.ByteArrayRow.getString(ByteArrayRow.java:70)
        at com.mysql.jdbc.ResultSetImpl.getStringInternal(ResultSetImpl.java:5816)
        at com.mysql.jdbc.ResultSetImpl.getString(ResultSetImpl.java:5693)
        at org.apache.spark.sql.jdbc.JDBCRDD$$anon$1.getNext(JDBCRDD.scala:346)
        at org.apache.spark.sql.jdbc.JDBCRDD$$anon$1.hasNext(JDBCRDD.scala:399)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoin$$anonfun$2.apply(BroadcastNestedLoopJoin.scala:80)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoin$$anonfun$2.apply(BroadcastNestedLoopJoin.scala:71)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

15/06/02 18:02:44 ERROR TaskSchedulerImpl: Lost executor 5 on eunke-dp-ana-005: remote Akka client disassociated
15/06/02 18:02:44 INFO TaskSetManager: Re-queueing tasks for 5 from TaskSet 12.0
15/06/02 18:02:44 WARN TaskSetManager: Lost task 30.0 in stage 12.0 (TID 450, eunke-dp-ana-005): ExecutorLostFailure (executor 5 lost)
15/06/02 18:02:44 WARN TaskSetManager: Lost task 8.0 in stage 12.0 (TID 422, eunke-dp-ana-005): ExecutorLostFailure (executor 5 lost)
15/06/02 18:02:44 WARN TaskSetManager: Lost task 26.0 in stage 12.0 (TID 440, eunke-dp-ana-005): ExecutorLostFailure (executor 5 lost)
15/06/02 18:02:44 INFO DAGScheduler: Executor lost: 5 (epoch 26)
15/06/02 18:02:44 INFO AppClient$ClientActor: Executor updated: app-20150602175801-0477/5 is now EXITED (Command exited with code 52)
15/06/02 18:02:44 INFO SparkDeploySchedulerBackend: Executor app-20150602175801-0477/5 removed: Command exited with code 52
15/06/02 18:02:44 INFO BlockManagerMasterActor: Trying to remove executor 5 from BlockManagerMaster.
15/06/02 18:02:44 ERROR SparkDeploySchedulerBackend: Asked to remove non-existent executor 5
15/06/02 18:02:44 INFO BlockManagerMasterActor: Removing block manager BlockManagerId(5, eunke-dp-ana-005, 57455)
15/06/02 18:02:44 INFO BlockManagerMaster: Removed 5 successfully in removeExecutor
15/06/02 18:02:44 INFO AppClient$ClientActor: Executor added: app-20150602175801-0477/11 on worker-20150527101123-eunke-dp-ana-005-7078 (eunke-dp-ana-005:7078) with 3 cores
15/06/02 18:02:44 INFO SparkDeploySchedulerBackend: Granted executor ID app-20150602175801-0477/11 on hostPort eunke-dp-ana-005:7078 with 3 cores, 2.0 GB RAM
15/06/02 18:02:44 INFO Stage: Stage 8 is now unavailable on executor 5 (167/200, false)
15/06/02 18:02:44 INFO Stage: Stage 9 is now unavailable on executor 5 (141/200, false)
15/06/02 18:02:44 INFO AppClient$ClientActor: Executor updated: app-20150602175801-0477/11 is now RUNNING
15/06/02 18:02:44 INFO AppClient$ClientActor: Executor updated: app-20150602175801-0477/11 is now LOADING
15/06/02 18:02:45 INFO MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 9 to sparkExecutor@eunke-dp-ana-002:55246
15/06/02 18:02:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 9 is 148 bytes
15/06/02 18:02:45 INFO MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 8 to sparkExecutor@eunke-dp-ana-002:55246
15/06/02 18:02:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 8 is 170 bytes
15/06/02 18:02:45 INFO MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 3 to sparkExecutor@eunke-dp-ana-002:55246
15/06/02 18:02:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 3 is 1140 bytes
15/06/02 18:02:45 INFO TaskSetManager: Starting task 26.1 in stage 12.0 (TID 451, eunke-dp-ana-002, PROCESS_LOCAL, 1978 bytes)
15/06/02 18:02:45 WARN TaskSetManager: Lost task 18.1 in stage 12.0 (TID 446, eunke-dp-ana-002): FetchFailed(null, shuffleId=3, mapId=-1, reduceId=18, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:385)
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:382)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:381)
        at org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:177)
        at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:42)
        at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CartesianRDD.compute(CartesianRDD.scala:75)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

)
15/06/02 18:02:45 INFO DAGScheduler: Marking Stage 12 (count at BroadcastNestedLoopJoin.scala:114) as failed due to a fetch failure from Stage 9 (mapPartitions at Exchange.scala:64)
15/06/02 18:02:45 WARN TaskSetManager: Lost task 9.1 in stage 12.0 (TID 445, eunke-dp-ana-002): FetchFailed(null, shuffleId=3, mapId=-1, reduceId=9, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:385)
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:382)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:381)
        at org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:177)
        at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:42)
        at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CartesianRDD.compute(CartesianRDD.scala:75)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

)
15/06/02 18:02:45 WARN TaskSetManager: Lost task 0.1 in stage 12.0 (TID 444, eunke-dp-ana-002): FetchFailed(null, shuffleId=3, mapId=-1, reduceId=0, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:385)
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:382)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:381)
        at org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:177)
        at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:42)
        at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CartesianRDD.compute(CartesianRDD.scala:75)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

)
15/06/02 18:02:45 INFO DAGScheduler: Stage 12 (count at BroadcastNestedLoopJoin.scala:114) failed in 237.459 s
15/06/02 18:02:45 INFO DAGScheduler: Resubmitting Stage 9 (mapPartitions at Exchange.scala:64) and Stage 12 (count at BroadcastNestedLoopJoin.scala:114) due to fetch failure
15/06/02 18:02:45 WARN TaskSetManager: Lost task 26.1 in stage 12.0 (TID 451, eunke-dp-ana-002): FetchFailed(null, shuffleId=3, mapId=-1, reduceId=26, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 3
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:385)
        at org.apache.spark.MapOutputTracker$$anonfun$org$apache$spark$MapOutputTracker$$convertMapStatuses$1.apply(MapOutputTracker.scala:382)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        at org.apache.spark.MapOutputTracker$.org$apache$spark$MapOutputTracker$$convertMapStatuses(MapOutputTracker.scala:381)
        at org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:186)
        at org.apache.spark.shuffle.hash.BlockStoreShuffleFetcher$.fetch(BlockStoreShuffleFetcher.scala:42)
        at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:40)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CartesianRDD.compute(CartesianRDD.scala:75)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

)
15/06/02 18:02:45 INFO DAGScheduler: Resubmitting failed stages
15/06/02 18:02:45 INFO DAGScheduler: Submitting Stage 8 (MapPartitionsRDD[60] at mapPartitions at Exchange.scala:64), which has no missing parents
15/06/02 18:02:45 INFO MemoryStore: ensureFreeSpace(9040) called with curMem=45430098, maxMem=278302556
15/06/02 18:02:45 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 8.8 KB, free 222.1 MB)
15/06/02 18:02:45 INFO MemoryStore: ensureFreeSpace(4506) called with curMem=45439138, maxMem=278302556
15/06/02 18:02:45 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.4 KB, free 222.1 MB)
15/06/02 18:02:45 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on eunke-dp-ana-001:48259 (size: 4.4 KB, free: 261.7 MB)
15/06/02 18:02:45 INFO BlockManagerMaster: Updated info of block broadcast_19_piece0
15/06/02 18:02:45 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:839
15/06/02 18:02:45 INFO DAGScheduler: Submitting 33 missing tasks from Stage 8 (MapPartitionsRDD[60] at mapPartitions at Exchange.scala:64)
15/06/02 18:02:45 INFO TaskSchedulerImpl: Adding task set 8.1 with 33 tasks
15/06/02 18:02:45 INFO TaskSetManager: Starting task 0.0 in stage 8.1 (TID 452, eunke-dp-ana-002, PROCESS_LOCAL, 1600 bytes)
15/06/02 18:02:45 INFO TaskSetManager: Starting task 1.0 in stage 8.1 (TID 453, eunke-dp-ana-002, PROCESS_LOCAL, 1600 bytes)
15/06/02 18:02:45 INFO TaskSetManager: Starting task 2.0 in stage 8.1 (TID 454, eunke-dp-ana-002, PROCESS_LOCAL, 1600 bytes)
15/06/02 18:02:45 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on eunke-dp-ana-002:55132 (size: 4.4 KB, free: 1060.3 MB)
15/06/02 18:02:45 INFO MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 6 to sparkExecutor@eunke-dp-ana-002:55246
15/06/02 18:02:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 6 is 148 bytes
15/06/02 18:02:45 INFO MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 5 to sparkExecutor@eunke-dp-ana-002:55246
15/06/02 18:02:45 INFO MapOutputTrackerMaster: Size of output statuses for shuffle 5 is 218 bytes
15/06/02 18:02:47 INFO SparkDeploySchedulerBackend: Registered executor: Actor[akka.tcp://sparkExecutor@eunke-dp-ana-005:35440/user/Executor#292835265] with ID 11
15/06/02 18:02:47 INFO TaskSetManager: Starting task 3.0 in stage 8.1 (TID 455, eunke-dp-ana-005, PROCESS_LOCAL, 1600 bytes)
15/06/02 18:02:47 INFO TaskSetManager: Starting task 4.0 in stage 8.1 (TID 456, eunke-dp-ana-005, PROCESS_LOCAL, 1600 bytes)
15/06/02 18:02:47 INFO TaskSetManager: Starting task 5.0 in stage 8.1 (TID 457, eunke-dp-ana-005, PROCESS_LOCAL, 1600 bytes)
15/06/02 18:02:47 INFO BlockManagerMasterActor: Registering block manager eunke-dp-ana-005:37342 with 1060.3 MB RAM, BlockManagerId(11, eunke-dp-ana-005, 37342)
15/06/02 18:02:48 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on eunke-dp-ana-005:37342 (size: 4.4 KB, free: 1060.3 MB)
15/06/02 18:02:49 INFO MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 6 to sparkExecutor@eunke-dp-ana-005:35440
15/06/02 18:02:49 INFO MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 5 to sparkExecutor@eunke-dp-ana-005:35440
15/06/02 18:03:00 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(2, eunke-dp-ana-003, 51001) with no recent heart beats: 167084ms exceeds 120000ms
15/06/02 18:03:00 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(0, eunke-dp-ana-010, 34792) with no recent heart beats: 154521ms exceeds 120000ms
15/06/02 18:03:00 INFO BlockManagerMasterActor: Removing block manager BlockManagerId(0, eunke-dp-ana-010, 34792)
15/06/02 18:03:00 INFO BlockManagerMasterActor: Removing block manager BlockManagerId(2, eunke-dp-ana-003, 51001)
15/06/02 18:03:18 INFO TaskSetManager: Starting task 6.0 in stage 8.1 (TID 458, eunke-dp-ana-007, PROCESS_LOCAL, 1600 bytes)
15/06/02 18:03:18 WARN TaskSetManager: Lost task 13.0 in stage 12.0 (TID 427, eunke-dp-ana-007): java.lang.OutOfMemoryError: GC overhead limit exceeded
        at org.apache.spark.sql.catalyst.expressions.SpecificMutableRow.copy(SpecificMutableRow.scala:230)
        at org.apache.spark.sql.execution.joins.CartesianProduct$$anonfun$2.apply(CartesianProduct.scala:33)
        at org.apache.spark.sql.execution.joins.CartesianProduct$$anonfun$2.apply(CartesianProduct.scala:33)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoin$$anonfun$2.apply(BroadcastNestedLoopJoin.scala:80)
        at org.apache.spark.sql.execution.joins.BroadcastNestedLoopJoin$$anonfun$2.apply(BroadcastNestedLoopJoin.scala:71)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
",,glenn.strycker@gmail.com,WillCup,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 19 10:36:19 UTC 2015,,,,,,,,,,"0|i2fi7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/15 10:36;srowen;[~WillCup] don't set target version please. 1.3.0 was released already anyway. 
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark SQL when functions is broken on Column,SPARK-8038,12834534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ogirardot,ogirardot,ogirardot,02/Jun/15 09:26,02/Jun/15 20:38,14/Jul/23 06:26,02/Jun/15 20:38,1.4.0,,,,,,1.4.0,,,,,,PySpark,SQL,,,0,,,,,,"

{code}
In [1]: df = sqlCtx.createDataFrame([(1, ""1""), (2, ""2""), (1, ""2""), (1, ""2"")], [""key"", ""value""])


In [2]: from pyspark.sql import functions as F

In [8]: df.select(df.key, F.when(df.key > 1, 0).when(df.key == 0, 2).otherwise(1)).show()

+---+---------------------------------+
| key |CASE WHEN (key = 0) THEN 2 ELSE 1|
+---+---------------------------------+
| 1| 1|
| 2| 1|
| 1| 1|
| 1| 1|
+---+---------------------------------+
{code}

When in Scala I get the expected expression and behaviour : 

{code}
scala> val df = sqlContext.createDataFrame(List((1, ""1""), (2, ""2""), (1, ""2""), (1, ""2""))).toDF(""key"", ""value"")

scala> import org.apache.spark.sql.functions._

scala> df.select(df(""key""), when(df(""key"") > 1, 0).when(df(""key"") === 2, 2).otherwise(1)).show()

+---+-------------------------------------------------------+

|key|CASE WHEN (key > 1) THEN 0 WHEN (key = 2) THEN 2 ELSE 1|
+---+-------------------------------------------------------+
| 1| 1|
| 2| 0|
| 1| 1|
| 1| 1|
+---+-------------------------------------------------------+
{code}

This is coming from the ""column.py"" file with the Column class definition of **when** and the fix is coming.",Spark 1.4.0 RC3,apachespark,ogirardot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 17:22:01 UTC 2015,,,,,,,,,,"0|i2fi5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/15 09:32;apachespark;User 'ogirardot' has created a pull request for this issue:
https://github.com/apache/spark/pull/6580;;;","02/Jun/15 17:19;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6590;;;","02/Jun/15 17:22;srowen;(Please set Component);;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Ignores files whose name starts with ""."" while enumerating files in HadoopFsRelation",SPARK-8037,12834523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,02/Jun/15 08:45,06/Aug/15 14:36,14/Jul/23 06:26,02/Jun/15 17:15,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Temporary files like {{.DS_Store}} generated by Mac OS X finder may cause trouble for partition discovery. A directory whose layout looks like the following
{noformat}
> find parquet_partitioned
parquet_partitioned
parquet_partitioned/._common_metadata.crc
parquet_partitioned/._metadata.crc
parquet_partitioned/._SUCCESS.crc
parquet_partitioned/_common_metadata
parquet_partitioned/_metadata
parquet_partitioned/_SUCCESS
parquet_partitioned/year=2014/.DS_Store
parquet_partitioned/year=2014/month=9
parquet_partitioned/year=2014/month=9/.DS_Store
parquet_partitioned/year=2014/month=9/day=1/.DS_Store
parquet_partitioned/year=2014/month=9/day=1/.part-r-00008.gz.parquet.crc
parquet_partitioned/year=2014/month=9/day=1/part-r-00008.gz.parquet
parquet_partitioned/year=2015
parquet_partitioned/year=2015/month=10
parquet_partitioned/year=2015/month=10/day=25
parquet_partitioned/year=2015/month=10/day=25/.part-r-00002.gz.parquet.crc
parquet_partitioned/year=2015/month=10/day=25/.part-r-00004.gz.parquet.crc
parquet_partitioned/year=2015/month=10/day=25/part-r-00002.gz.parquet
parquet_partitioned/year=2015/month=10/day=25/part-r-00004.gz.parquet
parquet_partitioned/year=2015/month=10/day=26
parquet_partitioned/year=2015/month=10/day=26/.part-r-00005.gz.parquet.crc
parquet_partitioned/year=2015/month=10/day=26/part-r-00005.gz.parquet
parquet_partitioned/year=2015/month=9
parquet_partitioned/year=2015/month=9/day=1
parquet_partitioned/year=2015/month=9/day=1/.part-r-00007.gz.parquet.crc
parquet_partitioned/year=2015/month=9/day=1/part-r-00007.gz.parquet
{noformat}
causes exception like this:
{noformat}
scala> val df = sqlContext.read.parquet(""parquet_partitioned"")
java.lang.AssertionError: assertion failed: Conflicting partition column names detected:
    ArrayBuffer(year, month)
ArrayBuffer(year)
ArrayBuffer(year, month, day)
    at scala.Predef$.assert(Predef.scala:179)
    at org.apache.spark.sql.sources.PartitioningUtils$.resolvePartitions(PartitioningUtils.scala:189)
    at org.apache.spark.sql.sources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:87)
    at org.apache.spark.sql.sources.HadoopFsRelation.org$apache$spark$sql$sources$HadoopFsRelation$$discoverPartitions(interfaces.scala:492)
    at org.apache.spark.sql.sources.HadoopFsRelation$$anonfun$partitionSpec$3.apply(interfaces.scala:449)
    at org.apache.spark.sql.sources.HadoopFsRelation$$anonfun$partitionSpec$3.apply(interfaces.scala:448)
{noformat}
This is because {{.DS_Store}} files are considered as a data file.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8036,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 09:47:04 UTC 2015,,,,,,,,,,"0|i2fi3j:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"02/Jun/15 09:47;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6581;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make NumPy version checking in mllib/__init__.py,SPARK-8032,12834498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,MechCoder,MechCoder,MechCoder,02/Jun/15 07:00,03/Jun/15 06:29,14/Jul/23 06:26,03/Jun/15 06:27,1.0.2,1.1.1,1.2.2,1.3.1,1.4.0,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.1,1.5.0,MLlib,PySpark,,,0,,,,,,"The current checking does version `1.x' is less than `1.4' this will fail if x has greater than 1 digit, since x > 4, however `1.x` < `1.4`",,apachespark,MechCoder,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6192,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 06:27:12 UTC 2015,,,,,,,,,,"0|i2fhy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/15 07:05;apachespark;User 'MechCoder' has created a pull request for this issue:
https://github.com/apache/spark/pull/6579;;;","03/Jun/15 06:27;mengxr;Issue resolved by pull request 6579
[https://github.com/apache/spark/pull/6579];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Version number written to Hive metastore is ""0.13.1aa"" instead of ""0.13.1a""",SPARK-8031,12834490,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,lian cheng,lian cheng,lian cheng,02/Jun/15 06:26,01/Jul/15 11:44,14/Jul/23 06:26,30/Jun/15 02:27,1.2.0,1.2.1,1.2.2,1.3.0,1.3.1,1.4.0,1.5.0,,,,,,SQL,,,,0,,,,,,"While debugging {{CliSuite}} for 1.4.0-SNAPSHOT, noticed the following WARN log line:
{noformat}
15/06/02 13:40:29 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
{noformat}
The problem is that, the version of Hive dependencies 1.4.0-SNAPSHOT uses is {{0.13.1a}} (the one shaded by [~pwendell]), but the version showed in this line is {{0.13.1aa}} (one more {{a}}). The WARN log itself is OK since {{CliSuite}} initializes a brand new temporary Derby metastore.

While initializing Hive metastore, Hive calls {{ObjectStore.checkSchema()}} and may write the ""short"" version string to metastore. This short version string is defined by {{hive.version.shortname}} in the POM. However, [it was defined as {{0.13.1aa}}|https://github.com/pwendell/hive/commit/32e515907f0005c7a28ee388eadd1c94cf99b2d4#diff-600376dffeb79835ede4a0b285078036R62]. Confirmed with [~pwendell] that it should be a typo.

This doesn't cause any trouble for now, but we probably want to fix this in the future if we ever need to release another shaded version of Hive 0.13.1.",,lian cheng,rekhajoshm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 02:27:25 UTC 2015,,,,,,,,,,"0|i2fhwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/15 02:58;rekhajoshm;Hi. This issue is not in 1.5.0-SNAPSHOT where hive.version are correctly set to 0.13.1a, and hive.version.short to 0.13.1.Thanks
;;;","30/Jun/15 02:27;rekhajoshm;Fixed in 1.5.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ShuffleMapTasks must be robust to concurrent attempts on the same executor,SPARK-8029,12834470,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,irashid,irashid,02/Jun/15 05:17,09/Jan/17 20:54,14/Jul/23 06:26,13/Nov/15 06:45,1.4.0,,,,,,1.5.3,1.6.0,,,,,Spark Core,,,,2,,,,,,"When stages get retried, a task may have more than one attempt running at the same time, on the same executor.  Currently this causes problems for ShuffleMapTasks, since all attempts try to write to the same output files.

This is finally resolved through https://github.com/apache/spark/pull/9610, which uses the first writer wins approach.",,amcelwee,apachespark,darabos,davies,diederik,irashid,lianhuiwang,rdub,richardatcloudera,rxin,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7829,SPARK-18113,,,,,,,SPARK-7308,,,,,,,"02/Jun/15 05:19;irashid;AlternativesforMakingShuffleMapTasksRobusttoMultipleAttempts.pdf;https://issues.apache.org/jira/secure/attachment/12736740/AlternativesforMakingShuffleMapTasksRobusttoMultipleAttempts.pdf",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 19:08:10 UTC 2015,,,,,,,,,,"0|i2fhs7:",9223372036854775807,,,,,,,,,,,,,,1.5.3,1.6.0,,,,,,,,,,,,"02/Jun/15 05:19;irashid;[~joshrosen] and I discussed this a bit, I'm uploading a doc with a discussion of some alternatives.  I am working on exploring the options a little, but would appreciate any feedback on the various options.;;;","02/Jun/15 05:27;irashid;This is a subset of the issues originally reported in SPARK-7308, to have an issue with a smaller scope, but hopefully still large enough to consider the design.

SPARK-7829 is the ""ad-hoc"" proposal of the fix for this issue.;;;","04/Jun/15 18:02;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/6648;;;","19/Aug/15 18:56;rxin;I have retargeted this and downgraded it from Blocker to Critical since it's been there for a while and not a regression.;;;","20/Oct/15 21:43;rxin;It'd be really good to fix this in 1.6, and maybe even backport it to older branches.

[~irashid] Would you have time to give ""Executors Commit ShuffleMapOutput: First Attempt Wins"" in your design proposal a try? It seems like a much smaller fix needed, and the chance of that fix having problems is pretty low (despite you think it is ""optimistic"").


;;;","22/Oct/15 03:21;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/9214;;;","11/Nov/15 00:11;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/9610;;;","13/Nov/15 06:45;davies;Issue resolved by pull request 9610
[https://github.com/apache/spark/pull/9610];;;","13/Nov/15 07:38;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/9686;;;","13/Nov/15 19:08;rxin;[~davies]  can you update the jira ticket description with the high level approach used in the fix?;;;",,,,,,,,,,,,,,,,,,,
SparkR does not work with `--jars` and `--packages`,SPARK-8028,12834451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shivaram,shivaram,shivaram,02/Jun/15 01:58,02/Jun/15 04:01,14/Jul/23 06:26,02/Jun/15 04:01,1.4.0,,,,,,1.4.0,,,,,,SparkR,,,,0,,,,,,"While trying to use the CSV reader with SparkR I found that the spark.jars property is being cleared while creating the SparkContext.

This means that `--jars` flag also doesn't work right now",,apachespark,Elie A.,saurai3h,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 04:01:43 UTC 2015,,,,,,,,,,"0|i2fhnz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/15 02:27;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/6568;;;","02/Jun/15 04:01;shivaram;Issue resolved by pull request 6568
[https://github.com/apache/spark/pull/6568];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR docs should be included in the distribution,SPARK-8027,12834450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,shivaram,shivaram,shivaram,02/Jun/15 01:31,04/Jun/15 00:38,14/Jul/23 06:26,02/Jun/15 04:22,1.4.0,,,,,,1.4.0,,,,,,SparkR,,,,0,,,,,,"Right now the SparkR docs are not available in the binary distributions. So for example running `?read.df` after launching `bin/sparkR` does not work.

We should generate man pages as a part of the distribution build for this to work",,apachespark,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 00:38:02 UTC 2015,,,,,,,,,,"0|i2fhnr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/15 02:28;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/6567;;;","02/Jun/15 04:22;shivaram;Issue resolved by pull request 6567
[https://github.com/apache/spark/pull/6567];;;","04/Jun/15 00:38;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/6593;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Random Number Generation inconsistent in projections in DataFrame,SPARK-8023,12834436,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,brkyvz,brkyvz,02/Jun/15 00:05,02/Jun/15 07:22,14/Jul/23 06:26,02/Jun/15 07:22,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"to reproduce (in python):

{code}
df = sqlContext.range(0, 10).withColumn('uniform', rand(seed=10))
df.select('uniform', 'uniform' + 1)
{code}

You should see that the first column + 1 doesn't equal the second column.",,apachespark,brkyvz,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 05:28:10 UTC 2015,,,,,,,,,,"0|i2fhkn:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"02/Jun/15 00:05;brkyvz;cc [~yhuai];;;","02/Jun/15 03:53;yhuai;To fix this issue, I am adding a {{def deterministic: Boolean}} to {{Expression}}.;;;","02/Jun/15 03:57;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6570;;;","02/Jun/15 05:28;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6573;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL conf in spark-defaults.conf make metadataHive get constructed too early,SPARK-8020,12834388,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,01/Jun/15 21:42,18/Apr/16 15:19,14/Jul/23 06:26,02/Jun/15 07:17,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"To correctly construct a {{metadataHive}} object, we need two settings, {{spark.sql.hive.metastore.version}} and {{spark.sql.hive.metastore.jars}}. If users want to use Hive 0.12's metastore, they need to set {{spark.sql.hive.metastore.version}} to {{0.12.0}} and set {{spark.sql.hive.metastore.jars}} to {{maven}} or a classpath containing Hive and Hadoop's jars. However, any spark sql setting in the {{spark-defaults.conf}} will trigger the construction of {{metadataHive}} and cause Spark SQL connect to the wrong metastore (e.g. connect to the local derby metastore instead of a remove mysql Hive 0.12 metastore). Also, if {{spark.sql.hive.metastore.version 0.12.0}} is the first conf set to SQL conf, we will get
{code}
Exception in thread ""main"" java.lang.IllegalArgumentException: Builtin jars can only be used when hive execution version == hive metastore version. Execution: 0.13.1 != Metastore: 0.12.0. Specify a vaild path to the correct hive jars using $HIVE_METASTORE_JARS or change spark.sql.hive.metastore.version to 0.13.1.
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:186)
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:175)
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:358)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:186)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:185)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:185)
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:71)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:53)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:248)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:136)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

{code}",,apachespark,cheolsoo,jeanlyn,nemccarthy,qwertymaniac,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7851,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 18 15:19:11 UTC 2016,,,,,,,,,,"0|i2fhan:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"01/Jun/15 23:55;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6563;;;","02/Jun/15 00:47;yhuai;[~cheolsoo] Can you try https://github.com/apache/spark/pull/6563?;;;","02/Jun/15 02:46;jeanlyn;[~yhuai],I set *spark.sql.hive.metastore.jars* in spark-defaults.conf i got errors like yours.But when i set *spark.sql.hive.metastore.jars* in *hive-site.xml* i got
{code}
5/06/02 10:42:04 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/06/02 10:42:04 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:41416 with 706.6 MB RAM, BlockManagerId(driver, localhost, 41416)
15/06/02 10:42:04 INFO storage.BlockManagerMaster: Registered BlockManager
SET spark.sql.hive.metastore.version=0.12.0
15/06/02 10:42:04 WARN conf.HiveConf: DEPRECATED: Configuration property hive.metastore.local no longer has any effect. Make sure to provide a valid value for hive.metastore.u
ris if you are connecting to a remote metastore.
15/06/02 10:42:04 WARN conf.HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead
15/06/02 10:42:04 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 0.12.0 using maven.
Ivy Default Cache set to: /home/dd_edw/.ivy2/cache
The jars for the packages stored in: /home/dd_edw/.ivy2/jars
http://www.datanucleus.org/downloads/maven2 added as a remote repository with the name: repo-1
:: loading settings :: url = jar:file:/data0/spark-1.3.0-bin-2.2.0/lib/spark-assembly-1.4.0-SNAPSHOT-hadoop2.2.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.apache.hive#hive-metastore added as a dependency
org.apache.hive#hive-exec added as a dependency
org.apache.hive#hive-common added as a dependency
org.apache.hive#hive-serde added as a dependency
com.google.guava#guava added as a dependency
org.apache.hadoop#hadoop-client added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
       confs: [default]
       found org.apache.hive#hive-metastore;0.12.0 in central
       found org.antlr#antlr;3.4 in central
       found org.antlr#antlr-runtime;3.4 in central
....
xception in thread ""main"" java.lang.ClassNotFoundException: java.lang.NoClassDefFoundError: com/google/common/base/Preconditions when creating Hive client using classpath: fi
le:/tmp/hive3795822184995995241vv12/aopalliance_aopalliance-1.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-exec-0.12.0.jar, file:/tmp/hive3795822184995995
241vv12/org.apache.thrift_libfb303-0.9.0.jar, file:/tmp/hive3795822184995995241vv12/commons-digester_commons-digester-1.8.jar, file:/tmp/hive3795822184995995241vv12/com.sun.je
rsey_jersey-client-1.9.jar, file:/tmp/hive3795822184995995241vv12/org.apache.httpcomponents_httpclient-4.2.5.jar, file:/tmp/hive3795822184995995241vv12/org.antlr_stringtemplat
e-3.2.1.jar, file:/tmp/hive3795822184995995241vv12/commons-logging_commons-logging-1.1.3.jar, file:/tmp/hive3795822184995995241vv12/org.antlr_antlr-runtime-3.4.jar, file:/tmp/
hive3795822184995995241vv12/org.mockito_mockito-all-1.8.2.jar, file:/tmp/hive3795822184995995241vv12/org.apache.derby_derby-10.4.2.0.jar, file:/tmp/hive3795822184995995241vv12
/antlr_antlr-2.7.7.jar, file:/tmp/hive3795822184995995241vv12/commons-net_commons-net-3.1.jar, file:/tmp/hive3795822184995995241vv12/org.slf4j_slf4j-log4j12-1.7.5.jar, file:/t
mp/hive3795822184995995241vv12/junit_junit-3.8.1.jar, file:/tmp/hive3795822184995995241vv12/org.codehaus.jackson_jackson-jaxrs-1.8.8.jar, file:/tmp/hive3795822184995995241vv12
/commons-cli_commons-cli-1.2.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-serde-0.12.0.jar, file:/tmp/hive3795822184995995241vv12/org.codehaus.jettison_jett
ison-1.1.jar, file:/tmp/hive3795822184995995241vv12/javax.xml.stream_stax-api-1.0-2.jar, file:/tmp/hive3795822184995995241vv12/org.apache.avro_avro-1.7.4.jar, file:/tmp/hive37
95822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-app-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-common-2.4.0.jar
, file:/tmp/hive3795822184995995241vv12/org.codehaus.jackson_jackson-xc-1.8.8.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-annotations-2.4.0.jar, file:/
tmp/hive3795822184995995241vv12/org.mortbay.jetty_jetty-util-6.1.26.jar, file:/tmp/hive3795822184995995241vv12/org.apache.commons_commons-math3-3.1.1.jar, file:/tmp/hive379582
2184995995241vv12/javax.transaction_jta-1.1.jar, file:/tmp/hive3795822184995995241vv12/commons-httpclient_commons-httpclient-3.1.jar, file:/tmp/hive3795822184995995241vv12/xml
enc_xmlenc-0.52.jar, file:/tmp/hive3795822184995995241vv12/org.sonatype.sisu.inject_cglib-2.2.1-v20090111.jar, file:/tmp/hive3795822184995995241vv12/com.google.code.findbugs_j
sr305-1.3.9.jar, file:/tmp/hive3795822184995995241vv12/commons-codec_commons-codec-1.4.jar, file:/tmp/hive3795822184995995241vv12/com.google.guava_guava-14.0.1.jar, file:/tmp/
hive3795822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-shuffle-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/org.jboss.netty_netty-3.2.2.Final.jar, file:/tmp
/hive3795822184995995241vv12/org.apache.commons_commons-compress-1.4.1.jar, file:/tmp/hive3795822184995995241vv12/org.apache.avro_avro-mapred-1.7.1.jar, file:/tmp/hive37958221
84995995241vv12/org.slf4j_slf4j-api-1.7.5.jar, file:/tmp/hive3795822184995995241vv12/javolution_javolution-5.5.1.jar, file:/tmp/hive3795822184995995241vv12/com.sun.xml.bind_ja
xb-impl-2.2.3-1.jar, file:/tmp/hive3795822184995995241vv12/org.iq80.snappy_snappy-0.2.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-yarn-client-2.4.0.jar
, file:/tmp/hive3795822184995995241vv12/log4j_log4j-1.2.17.jar, file:/tmp/hive3795822184995995241vv12/commons-pool_commons-pool-1.5.4.jar, file:/tmp/hive3795822184995995241vv1
2/io.netty_netty-3.4.0.Final.jar, file:/tmp/hive3795822184995995241vv12/org.apache.avro_avro-ipc-1.7.1.jar, file:/tmp/hive3795822184995995241vv12/org.apache.zookeeper_zookeepe
r-3.4.3.jar, file:/tmp/hive3795822184995995241vv12/org.json_json-20090211.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-metastore-0.12.0.jar, file:/tmp/hive3
795822184995995241vv12/org.datanucleus_datanucleus-api-jdo-3.2.1.jar, file:/tmp/hive3795822184995995241vv12/org.mortbay.jetty_servlet-api-2.5-20081211.jar, file:/tmp/hive37958
22184995995241vv12/org.apache.hadoop_hadoop-auth-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/javax.xml.bind_jaxb-api-2.2.2.jar, file:/tmp/hive3795822184995995241vv12/com.
sun.jersey_jersey-server-1.9.jar, file:/tmp/hive3795822184995995241vv12/asm_asm-3.2.jar, file:/tmp/hive3795822184995995241vv12/javax.activation_activation-1.1.jar, file:/tmp/h
ive3795822184995995241vv12/org.datanucleus_datanucleus-core-3.2.2.jar, file:/tmp/hive3795822184995995241vv12/com.jolbox_bonecp-0.7.1.RELEASE.jar, file:/tmp/hive379582218499599
5241vv12/org.tukaani_xz-1.0.jar, file:/tmp/hive3795822184995995241vv12/org.mortbay.jetty_jetty-6.1.26.jar, file:/tmp/hive3795822184995995241vv12/com.sun.jersey.contribs_jersey
-guice-1.9.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-hdfs-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/commons-collections_commons-collections-3.
2.1.jar, file:/tmp/hive3795822184995995241vv12/commons-beanutils_commons-beanutils-1.7.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-c
ore-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-common-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/com.googlecode.javaewah_JavaEWAH-0.3.2.ja
r, file:/tmp/hive3795822184995995241vv12/com.sun.jersey_jersey-json-1.9.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-mapreduce-client-jobclient-2.4.0.ja
r, file:/tmp/hive3795822184995995241vv12/com.sun.jersey_jersey-core-1.9.jar, file:/tmp/hive3795822184995995241vv12/org.datanucleus_datanucleus-rdbms-3.2.1.jar, file:/tmp/hive3
795822184995995241vv12/javax.jdo_jdo-api-3.0.1.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-common-0.12.0.jar, file:/tmp/hive3795822184995995241vv12/commons
-beanutils_commons-beanutils-core-1.8.0.jar, file:/tmp/hive3795822184995995241vv12/org.codehaus.jackson_jackson-mapper-asl-1.8.8.jar, file:/tmp/hive3795822184995995241vv12/com
.thoughtworks.paranamer_paranamer-2.3.jar, file:/tmp/hive3795822184995995241vv12/com.google.protobuf_protobuf-java-2.5.0.jar, file:/tmp/hive3795822184995995241vv12/javax.servl
et_servlet-api-2.5.jar, file:/tmp/hive3795822184995995241vv12/org.apache.velocity_velocity-1.7.jar, file:/tmp/hive3795822184995995241vv12/org.apache.thrift_libthrift-0.9.0.jar
, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-yarn-server-common-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/jline_jline-0.9.94.jar, file:/tmp/hive3795
822184995995241vv12/commons-logging_commons-logging-api-1.0.4.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hive_hive-shims-0.12.0.jar, file:/tmp/hive37958221849959952
41vv12/org.apache.hadoop_hadoop-yarn-api-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/commons-io_commons-io-2.4.jar, file:/tmp/hive3795822184995995241vv12/org.antlr_ST4-4.
0.4.jar, file:/tmp/hive3795822184995995241vv12/org.codehaus.jackson_jackson-core-asl-1.8.8.jar, file:/tmp/hive3795822184995995241vv12/commons-lang_commons-lang-2.6.jar, file:/
tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-yarn-common-2.4.0.jar, file:/tmp/hive3795822184995995241vv12/org.apache.httpcomponents_httpcore-4.2.5.jar, file:/tmp/h
ive3795822184995995241vv12/org.antlr_antlr-3.4.jar, file:/tmp/hive3795822184995995241vv12/commons-configuration_commons-configuration-1.6.jar, file:/tmp/hive379582218499599524
1vv12/com.google.inject_guice-3.0.jar, file:/tmp/hive3795822184995995241vv12/javax.inject_javax.inject-1.jar, file:/tmp/hive3795822184995995241vv12/org.xerial.snappy_snappy-ja
va-1.0.4.1.jar, file:/tmp/hive3795822184995995241vv12/org.apache.hadoop_hadoop-client-2.4.0.jar
Please make sure that jars for your version of hive and hadoop are included in the paths passed to spark.sql.hive.metastore.jars.
       at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:174)
       at org.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:166)
       at org.apache.spark.sql.hive.client.IsolatedClientLoader$.forVersion(IsolatedClientLoader.scala:45)
       at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)
       at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:174)
       at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:349)
       at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:188)
       at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:187)
       at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
       at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
       at org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:187)
       at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:70)
       at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:53)
       at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:248)
       at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:136)
       at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
       at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
       at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
       at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
       at java.lang.reflect.Method.invoke(Method.java:597)
       at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
       at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
       at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
       at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
       at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/06/02 10:42:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
{code} as i show in https://github.com/apache/spark/pull/5876
shall we move this discusstion to user mail list?
;;;","02/Jun/15 03:14;cheolsoo;[~yhuai], the patch seems to fix the original error. Thank you!

But it doesn't make it easy for me to use Hive 0.12 metastore. Now the challenge is that I set {{spark.sql.hive.metastore.jars}} to {{/home/cheolsoop/hive-0.12.0-bin/lib/*:$(hadoop classpath)}}, and that brings in all sorts of class conflicts that I didn't have when using the built-in Hive metastore. For now, I'll probably continue to use my workaround (i.e. commenting out [this code|https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala#L174]) and use the built-in Hive metastore. Btw, Hive 0.13 client is almost compatible with Hive 0.12 metastore server except one introduced by HIVE-6330. It is not too bad as long as users can build their own jars.;;;","02/Jun/15 04:03;yhuai;Is guava causing the problem?;;;","02/Jun/15 05:07;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6571;;;","02/Jun/15 06:09;yhuai;Can you put those settings back to spark-defaults.conf and try again once this one is resolved? I feel putting them in hive-site is not the right way.;;;","02/Jun/15 07:17;yhuai;Issue resolved by pull request 6571
[https://github.com/apache/spark/pull/6571];;;","03/Jun/15 05:37;jeanlyn;I had tried to put the settings back to *spark-defaults.conf* just now,and i builded spark with rc4.I still got the same *ClassNotFoundException* excption as i mentioned about;;;","18/Apr/16 15:19;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/12471;;;",,,,,,,,,,,,,,,,,,,
flume-sink should not depend on Guava.,SPARK-8015,12834300,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,01/Jun/15 17:05,02/Jun/15 18:46,14/Jul/23 06:26,02/Jun/15 18:46,1.4.0,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,"The flume-sink module, due to the shared shading code in our build, ends up depending on the {{org.spark-project}} Guava classes. That means users who deploy the sink in Flume will also need to provide those classes somehow, generally by also adding the Spark assembly, which means adding a whole bunch of other libraries to Flume, which may or may not cause other unforeseen problems.

It's better to not have that dependency in the flume-sink module instead.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 17:10:07 UTC 2015,,,,,,,,,,"0|i2fgs7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/15 17:10;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6555;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DataFrame.write.mode(""error"").save(...) should not scan the output folder",SPARK-8014,12834297,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,huangjs,huangjs,01/Jun/15 17:00,02/Jun/15 20:32,14/Jul/23 06:26,02/Jun/15 20:32,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"When saving a DataFrame with {{ErrorIfExists}} as save mode, we shouldn't do metadata discovery if the destination folder exists. This also applies to {{SaveMode.Overwrite}} and {{SaveMode.Ignore}}.

To reproduce this issue, we may make an empty directory {{/tmp/foo}} and leave an empty file {{bar}} there, then execute the following code in Spark shell:
{code}
import sqlContext._
import sqlContext.implicits._

Seq(1 -> ""a"").toDF(""i"", ""s"").write.format(""parquet"").mode(""error"").save(""file:///tmp/foo"")
{code}
From the exception stack trace we can see that metadata discovery code path is executed:
{noformat}
java.io.IOException: Could not read footer: java.lang.RuntimeException: file:/tmp/foo/bar is not a Parquet file (too small)
        at parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:238)
        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newParquet.scala:369)
        at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$metadataCache$lzycompute(newParquet.scala:154)
        at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$metadataCache(newParquet.scala:152)
        at org.apache.spark.sql.parquet.ParquetRelation2.dataSchema(newParquet.scala:193)
        at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:502)
        at org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:501)
        at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:331)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)
        ...
Caused by: java.lang.RuntimeException: file:/tmp/foo/bar is not a Parquet file (too small)
        at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:408)
        at parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:228)
        at parquet.hadoop.ParquetFileReader$2.call(ParquetFileReader.java:224)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}",,apachespark,huangjs,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 20:32:54 UTC 2015,,,,,,,,,,"0|i2fgrj:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"02/Jun/15 12:41;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6583;;;","02/Jun/15 20:32;yhuai;Issue resolved by pull request 6583
[https://github.com/apache/spark/pull/6583];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Implict promote Numeric type to String type in HiveTypeCoercion,SPARK-8010,12834198,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,OopsOutOfMemory,OopsOutOfMemory,OopsOutOfMemory,01/Jun/15 09:31,19/Jun/15 10:39,14/Jul/23 06:26,17/Jun/15 20:38,1.3.1,,,,,,1.5.0,,,,,,SQL,,,02/Jun/15 00:00,0,,,,,,"1. Given a query
`select coalesce(null, 1, '1') from dual` will cause exception:
  
  java.lang.RuntimeException: Could not determine return type of Coalesce for IntegerType,StringType

2. Given a query:
`select case when true then 1 else '1' end from dual` will cause exception:

  java.lang.RuntimeException: Types in CASE WHEN must be the same or coercible to a common type: StringType != IntegerType

I checked the code, the main cause is the HiveTypeCoercion doesn't do implicit convert when there is a IntegerType and StringType.

Numeric types can be promoted to string type in case throw exceptions.

Since Hive will always do this. It need to be fixed.",,apachespark,marmbrus,OopsOutOfMemory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 20:38:12 UTC 2015,,,,,,,,,,"0|i2fg7r:",9223372036854775807,,,,,yhuai,,,,,,,,,1.3.1,,,,,,,,,,,,,"01/Jun/15 09:39;apachespark;User 'OopsOutOfMemory' has created a pull request for this issue:
https://github.com/apache/spark/pull/6551;;;","17/Jun/15 20:38;marmbrus;Issue resolved by pull request 6551
[https://github.com/apache/spark/pull/6551];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark does not enclose column names when fetchting from jdbc sources,SPARK-8004,12834177,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,rtreffer,rtreffer,01/Jun/15 08:27,08/Oct/16 19:10,14/Jul/23 06:26,07/Jun/15 17:52,,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"Spark failes to load tables that have a keyword as column names

Sample error:
{code}

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 157.0 failed 1 times, most recent failure: Lost task 0.0 in stage 157.0 (TID 4322, localhost): com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'key,value FROM [XXXXXX]'
{code}

A correct query would have been
{code}
SELECT `key`.`value` FROM ....
{code}",,apachespark,jmrr,rtreffer,rxin,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6649,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jun 07 06:07:08 UTC 2015,,,,,,,,,,"0|i2fg3b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/15 15:50;viirya;I think backticks are only working for MySQL?;;;","01/Jun/15 20:59;rxin;Some other databases use double quotes, i.e. ""key"". We'd need to set this in the dialect class.

[~viirya] do you want to continue your streak of JDBC data source fixes and submit a patch for this?  ;;;","01/Jun/15 21:40;srowen;[~rtreffer] please read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark -- you need to set Component ,etc.;;;","02/Jun/15 01:30;viirya;Yes. I will submit a patch later.;;;","02/Jun/15 06:55;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6577;;;","07/Jun/15 06:07;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6689;;;",,,,,,,,,,,,,,,,,,,,,,,
Fix flaky tests in ExternalShuffleServiceSuite and SparkListenerWithClusterSuite,SPARK-7989,12834128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,zsxwing,zsxwing,01/Jun/15 02:59,21/Sep/15 18:39,14/Jul/23 06:26,21/Sep/15 18:39,,,,,,,1.4.1,1.5.0,1.6.0,,,,Spark Core,Tests,,,0,flaky-test,,,,,The flaky tests in ExternalShuffleServiceSuite and SparkListenerWithClusterSuite will fail if there are not enough executors up before running the jobs.,,apachespark,mengxr,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Sep 21 15:22:04 UTC 2015,,,,,,,,,,"0|i2ffsn:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,1.6.0,,,,,,,,,,,"01/Jun/15 03:05;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6546;;;","18/Sep/15 00:18;mengxr;Saw more failures recently:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/HADOOP_VERSION=1.2.1,label=spark-test/4164/testReport/junit/org.apache.spark.scheduler/SparkListenerWithClusterSuite/SparkListener_sends_executor_added_message/

{code}
org.apache.spark.scheduler.SparkListenerWithClusterSuite.SparkListener sends executor added message

Failing for the past 1 build (Since Failed#4164 )
Took 15 sec.
Error Message

Can't find 2 executors before 10000 milliseconds elapsed
Stacktrace

      java.util.concurrent.TimeoutException: Can't find 2 executors before 10000 milliseconds elapsed
      at org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:561)
      at org.apache.spark.scheduler.SparkListenerWithClusterSuite$$anonfun$2.apply$mcV$sp(SparkListenerWithClusterSuite.scala:46)
      at org.apache.spark.scheduler.SparkListenerWithClusterSuite$$anonfun$2.apply(SparkListenerWithClusterSuite.scala:40)
      at org.apache.spark.scheduler.SparkListenerWithClusterSuite$$anonfun$2.apply(SparkListenerWithClusterSuite.scala:40)
      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
      at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
      at org.scalatest.Transformer.apply(Transformer.scala:22)
      at org.scalatest.Transformer.apply(Transformer.scala:20)
      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
      at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
      at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
      at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
      at org.apache.spark.scheduler.SparkListenerWithClusterSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkListenerWithClusterSuite.scala:30)
      at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
      at org.apache.spark.scheduler.SparkListenerWithClusterSuite.org$scalatest$BeforeAndAfter$$super$runTest(SparkListenerWithClusterSuite.scala:30)
      at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
      at org.apache.spark.scheduler.SparkListenerWithClusterSuite.runTest(SparkListenerWithClusterSuite.scala:30)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
      at scala.collection.immutable.List.foreach(List.scala:318)
      at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
      at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
      at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
      at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
      at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
      at org.scalatest.Suite$class.run(Suite.scala:1424)
      at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
      at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
      at org.apache.spark.scheduler.SparkListenerWithClusterSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkListenerWithClusterSuite.scala:30)
      at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
      at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
      at org.apache.spark.scheduler.SparkListenerWithClusterSuite.org$scalatest$BeforeAndAfter$$super$run(SparkListenerWithClusterSuite.scala:30)
      at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
      at org.apache.spark.scheduler.SparkListenerWithClusterSuite.run(SparkListenerWithClusterSuite.scala:30)
      at org.scalatest.Suite$class.callExecuteOnSuite$1(Suite.scala:1492)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
      at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
      at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
      at org.scalatest.Suite$class.runNestedSuites(Suite.scala:1526)
      at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
      at org.scalatest.Suite$class.run(Suite.scala:1421)
      at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
      at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
      at scala.collection.immutable.List.foreach(List.scala:318)
      at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
      at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
      at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
      at org.scalatest.tools.Runner$.main(Runner.scala:860)
      at org.scalatest.tools.Runner.main(Runner.scala)
{code};;;","21/Sep/15 15:22;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/8813;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Remove ""fittingParamMap"" references. Update ML Doc ""Estimator, Transformer, and Param"" examples.",SPARK-7985,12834090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dusenberrymw,dusenberrymw,dusenberrymw,31/May/15 18:25,14/Jul/15 22:49,14/Jul/23 06:26,02/Jun/15 19:38,,,,,,,1.4.0,,,,,,Documentation,ML,,,0,,,,,,"Update ML Doc's ""Estimator, Transformer, and Param"" Scala & Java examples to use model.extractParamMap instead of model.fittingParamMap, which no longer exists.  Remove all other references to fittingParamMap throughout Spark.",,apachespark,dusenberrymw,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 19:38:50 UTC 2015,,,,,,,,,,"0|i2ffk7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/15 18:27;apachespark;User 'dusenberrymw' has created a pull request for this issue:
https://github.com/apache/spark/pull/6514;;;","02/Jun/15 19:38;josephkb;Issue resolved by pull request 6514
[https://github.com/apache/spark/pull/6514];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
DecimalType should not be singleton,SPARK-7978,12834042,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,31/May/15 05:16,02/Jun/15 00:30,14/Jul/23 06:26,01/Jun/15 02:56,1.4.0,,,,,,1.4.0,,,,,,PySpark,SQL,,,0,,,,,,"The DecimalType can not be constructed with parameters. When it's constructed without parameters, we always get same objects, which is wrong.

{code}
>>> from pyspark.sql.types import *
>>> DecimalType(1, 2)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: __call__() takes exactly 1 argument (3 given)
>>> DecimalType()
DecimalType()
{code}",,airhorns,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 31 05:20:03 UTC 2015,,,,,,,,,,"0|i2ff9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/May/15 05:20;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6532;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When parse window spec frame, we need to do case insensitive matches.",SPARK-7972,12834021,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,30/May/15 23:32,02/Jun/15 04:43,14/Jul/23 06:26,02/Jun/15 04:43,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"For window frame, PRECEDING, FOLLOWING, and CURRENT ROW do not have pre-defined tokens. So, Hive Parser returns the user input directly (e.g. {{preCeDING}}). We need to do case insensitive matches in {{HiveQl.scala}}..",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 04:43:16 UTC 2015,,,,,,,,,,"0|i2ff5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/15 23:37;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6524;;;","02/Jun/15 04:43;yhuai;It has been resolved by https://github.com/apache/spark/pull/6524.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
cannot resolve 'count' given input columns when using DataFrame.withColumn,SPARK-7967,12833958,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,WillCup,WillCup,30/May/15 08:05,05/Nov/18 17:39,14/Jul/23 06:26,02/Jun/15 10:07,1.3.0,,,,,,,,,,,,SQL,,,,0,dataFrame,sparksql,,,,"Code:

val userDF = app_user_register_log.filter($""add_time"" > startDay).filter($""add_time"" < endDay)
      .select(""id"").as(""userReg"")
      .join(activeDF.as(""ad""), $""userReg.id"" === $""ad.uid"")
      .select(""ad.uid"",""ad.clientVerion"",""ad.loc"",""ad.auth_status""
        ,""ad.channel"",""ad.bd_area"",""ad.mobile_area"",""ad.idcard_area"")
      .withColumn(""count"", $""count"") // Exception came from this line",spark 1.3.0 standalone,mon,WillCup,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Nov 05 17:39:00 UTC 2018,,,,,,,,,,"0|i2ferb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/15 08:09;WillCup;the stack :

Exception in thread ""main"" org.apache.spark.sql.AnalysisException: cannot resolve 'count' given input columns auth_status, uid, channel, loc, clientVerion, mobile_area, idcard_area, bd_area;
        at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:48)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:45)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:50)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:249)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:263)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:292)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:247)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:103)
        at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:117)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:116)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:121)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3.apply(CheckAnalysis.scala:45)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3.apply(CheckAnalysis.scala:43)
        at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:88)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.apply(CheckAnalysis.scala:43)
        at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1069)
        at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
        at org.apache.spark.sql.DataFrame.logicalPlanToDataFrame(DataFrame.scala:157)
        at org.apache.spark.sql.DataFrame.select(DataFrame.scala:465)
        at org.apache.spark.sql.DataFrame.withColumn(DataFrame.scala:739)
        at com.eunke.bi.ContinuousLoginOwner$.main(ContinuousLoginOwner.scala:111)
        at com.eunke.bi.ContinuousLoginOwner.main(ContinuousLoginOwner.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala);;;","30/May/15 08:13;WillCup;BTW, how could I give the new column a default value? 
I checked the source code for withColumn:

def withColumn(colName: String, col: Column): DataFrame = select(Column(""*""), col.as(colName))

It seems like this method just add a new column without any default value.Can anyone help me? For now, I just use RDD's map and join;;;","05/Nov/18 17:39;mon;Hi, did you solve this issue? I am having the same problem now ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong answers for queries with multiple window specs in the same expression,SPARK-7965,12833955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,joshrosen,joshrosen,30/May/15 07:46,31/Aug/16 21:20,14/Jul/23 06:26,02/Jun/15 04:42,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,correctness,,,,,"I think that Spark SQL may be returning incorrect answers for queries that use multiple window specifications within the same expression.  Here's an example that illustrates the problem.

Say that I have a table with a single numeric column and that I want to compute a cumulative distribution function over this column.  Let's call this table {{nums}}:

{code}
val nums = sc.parallelize(1 to 10).map(x => (x)).toDF(""x"")
nums.registerTempTable(""nums"")
{code}

It's easy to compute a running sum over this column:

{code}
sqlContext.sql(""""""
    select sum(x) over (rows between unbounded preceding and current row) from nums
"""""").collect()

nums: org.apache.spark.sql.DataFrame = [x: int]
res29: Array[org.apache.spark.sql.Row] = Array([1], [3], [6], [10], [15], [21], [28], [36], [45], [55])
{code}

It's also easy to compute a total sum over all rows:

{code}
sqlContext.sql(""""""
    select sum(x) over (rows between unbounded preceding and unbounded following) from nums
"""""").collect()

res34: Array[org.apache.spark.sql.Row] = Array([55], [55], [55], [55], [55], [55], [55], [55], [55], [55])
{code}

Let's say that I combine these expressions to compute a CDF:

{code}
sqlContext.sql(""""""
	select (sum(x) over (rows between unbounded preceding and current row))
    /
    (sum(x) over (rows between unbounded preceding and unbounded following)) from nums
"""""").collect()

res31: Array[org.apache.spark.sql.Row] = Array([1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0], [1.0])
{code}

This seems wrong.  Note that if we combine the running total, global total, and combined expression in the same query, then we see that the first two values are computed correctly / but the combined expression seems to be incorrect:

{code}
sqlContext.sql(""""""
    select
    sum(x) over (rows between unbounded preceding and current row) as running_sum,
    (sum(x) over (rows between unbounded preceding and unbounded following)) as total_sum,
    ((sum(x) over (rows between unbounded preceding and current row))
    /
    (sum(x) over (rows between unbounded preceding and unbounded following))) as combined
    from nums 
"""""").collect()

res40: Array[org.apache.spark.sql.Row] = Array([1,55,1.0], [3,55,1.0], [6,55,1.0], [10,55,1.0], [15,55,1.0], [21,55,1.0], [28,55,1.0], [36,55,1.0], [45,55,1.0], [55,55,1.0])
{code}

/cc [~yhuai]",,apachespark,hvanhovell,joshrosen,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 04:42:55 UTC 2015,,,,,,,,,,"0|i2feqn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/15 23:37;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6524;;;","02/Jun/15 04:42;yhuai;It has been resolved by https://github.com/apache/spark/pull/6524.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
The BooleanCasts rule in HiveTypeCoercion is useless,SPARK-7964,12833953,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,30/May/15 07:19,30/May/15 07:27,14/Jul/23 06:26,30/May/15 07:27,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 30 07:20:06 UTC 2015,,,,,,,,,,"0|i2feq7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/15 07:20;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/6516;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mesos cluster mode is broken,SPARK-7962,12833950,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tnachen,tnachen,tnachen,30/May/15 06:05,05/Aug/15 23:55,14/Jul/23 06:26,30/May/15 11:58,1.4.0,,,,,,1.4.0,,,,,,Mesos,Spark Submit,,,0,,,,,,Rest submission client prepends extra spark:// for non standalone master urls,,apachespark,tnachen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 30 11:53:40 UTC 2015,,,,,,,,,,"0|i2fepj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/15 06:07;apachespark;User 'tnachen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6517;;;","30/May/15 11:53;srowen;Looks resolved now, but can you make a better title in the future?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Failed StreamingContext.start() can leak active actors,SPARK-7958,12833897,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,29/May/15 22:14,02/Jun/15 19:03,14/Jul/23 06:26,02/Jun/15 03:05,1.1.1,1.2.2,1.3.1,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,"StreamingContext.start() can throw exception because DStream.validateAtStart() fails (say, checkpoint directory not set for StateDStream). But by then JobScheduler, JobGenerator, and ReceiverTracker has already started, along with their actors. But those cannot be shutdown because the only way to do that is call StreamingContext.stop() which cannot be called as the context has not been marked as ACTIVE.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 21:00:04 UTC 2015,,,,,,,,,,"0|i2feef:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"01/Jun/15 21:00;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6559;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use Janino to compile SQL expression,SPARK-7956,12833888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,29/May/15 21:25,10/Jun/15 18:12,14/Jul/23 06:26,04/Jun/15 17:29,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"The overhead of current implementation of codegen is to high (50ms - 500ms), which blocks us from turning it on by default.

We should try to investigate using Janino to compile the SQL expressions into JVM bytecode, which should be much faster to compile (about 10ms).",,apachespark,barrybecker4,davies,glenn.strycker@gmail.com,maropu,roczei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7812,,,,,,,SPARK-6419,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-7075,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 17:29:38 UTC 2015,,,,,,,,,,"0|i2fecf:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"29/May/15 21:26;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6479;;;","04/Jun/15 17:29;davies;Issue resolved by pull request 6479
[https://github.com/apache/spark/pull/6479];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic allocation: longer timeout for executors with cached blocks,SPARK-7955,12833882,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hshreedharan,hshreedharan,hshreedharan,29/May/15 20:50,06/May/17 22:25,14/Jul/23 06:26,09/Jun/15 17:24,1.4.0,,,,,,1.4.1,1.5.0,,,,,Spark Core,,,,0,,,,,,"When dynamic allocation is enabled, executor idle time is currently the only parameter considered. This can be annoying if executors get removed but have cached blocks. This can cause sever performance degradation.",,andrewor14,apachespark,ashwinshankar77,cheolsoo,dweeks,hammer,hshreedharan,lianhuiwang,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9197,SPARK-4280,,,,,,,,SPARK-20624,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 09 17:24:36 UTC 2015,,,,,,,,,,"0|i2feb3:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"29/May/15 20:58;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/6508;;;","09/Jun/15 16:59;cheolsoo;Can we close the jira since Hari's PR is committed?;;;","09/Jun/15 17:24;andrewor14;Done, thanks for the reminder.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"HiveThriftServer2.startWithContext() doesn't set ""spark.sql.hive.version""",SPARK-7950,12833829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,lian cheng,lian cheng,29/May/15 16:00,29/May/15 17:43,14/Jul/23 06:26,29/May/15 17:43,1.2.0,1.2.1,1.2.2,1.3.0,1.3.1,1.4.0,1.4.0,,,,,,SQL,,,,0,,,,,,"While testing the newly released Simba Spark SQL ODBC driver 1.0.8.1006 against 1.4.0-SNAPSHOT, we found that if {{HiveThriftServer2}} is started with {{HiveThriftServer2.startWithContext()}}, then simple queries like
{code:sql}
SELECT * FROM src
{code}
fail with the following error message (need to turn on ODBC trace log):
{noformat}
DIAG [S0002] [Simba][SQLEngine] (31740) Table or view not found: SPARK..src
{noformat}
However, JDBC client like Beeline is fine. Also, if the server is started via {{sbin/start-thriftserver.sh}}, both ODBC and JDBC work fine.

The reason for this failure is that, {{HiveThriftServer2.startWithContext()}} doesn't properly set the ""spark.sql.hive.version"" property. It seems that Simba ODBC driver 1.0.8.1006 behaves differently when this property is missing. What I observed is that, in this case, the ODBC driver issues a {{GetColumns}} command, which isn't overriden in Spark {{HiveThriftServer2}}, and this falls back to original Hive code path, which results in unexpected behavior.",Simba Spark SQL ODBC driver 1.0.8.1006,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 17:43:59 UTC 2015,,,,,,,,,,"0|i2fdyv:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"29/May/15 16:01;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6500;;;","29/May/15 17:43;yhuai;Issue resolved by pull request 6500
[https://github.com/apache/spark/pull/6500];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
DecayFactor wrongly set in StreamingKMeans,SPARK-7946,12833748,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,MechCoder,MechCoder,MechCoder,29/May/15 10:56,29/May/15 18:38,14/Jul/23 06:26,29/May/15 18:37,1.3.0,,,,,,1.2.3,1.3.2,1.4.0,,,,DStreams,MLlib,,,0,,,,,,,,apachespark,MechCoder,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6192,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 18:37:23 UTC 2015,,,,,,,,,,"0|i2fdgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/15 10:59;apachespark;User 'MechCoder' has created a pull request for this issue:
https://github.com/apache/spark/pull/6497;;;","29/May/15 12:21;srowen;Set component please
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark;;;","29/May/15 18:37;mengxr;Issue resolved by pull request 6497
[https://github.com/apache/spark/pull/6497];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Spark-Shell 2.11 1.4.0-RC-03 does not add jars to class path,SPARK-7944,12833742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dragos,anakos@gmail.com,anakos@gmail.com,29/May/15 10:30,10/Jul/15 15:22,14/Jul/23 06:26,10/Jul/15 15:22,1.3.1,1.4.0,,,,,1.5.0,,,,,,Spark Shell,,,,0,,,,,,"When I run the spark-shell with the --jars argument and supply a path to a single jar file, none of the classes in the jar are available in the REPL.

I have encountered this same behaviour in both 1.3.1 and 1.4.0_RC-03 builds for scala 2.11. I have yet to do a 1.4.0 RC-03 build for scala 2.10, but the contents of the jar are available in the 1.3.1_2.10 REPL.
",scala 2.11,alexbaretta,anakos@gmail.com,apachespark,brkyvz,dragos,irashid,tdas,vohprecio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/15 12:24;anakos@gmail.com;spark_shell_output.txt;https://issues.apache.org/jira/secure/attachment/12736125/spark_shell_output.txt","29/May/15 17:03;anakos@gmail.com;spark_shell_output_2.10.txt;https://issues.apache.org/jira/secure/attachment/12736187/spark_shell_output_2.10.txt",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 10 15:22:34 UTC 2015,,,,,,,,,,"0|i2fdfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/15 11:55;srowen;Can you give more detail, like what you tried to run and what the error is?;;;","29/May/15 12:21;anakos@gmail.com;Surely - sorry about that.  I started up the shell as follows:

{code}
bin/spark-shell --master yarn-client --jars /Users/alex/.m2/repository/com/twitter/algebird-core_2.11/0.9.0/algebird-core_2.11-0.9.0.jar
{code}

There was no obvious error present in the shell output. When I got the scala prompt from the console, I tried to import some classes and got the following error:

{code}
scala> import com.twitter.algebird._
<console>:20: error: object algebird is not a member of package com.twitter
       import com.twitter.algebird._
{code}

Trying to manually add the jar to the class path also proved to be fruitless:

{code}
scala> :cp /Users/alex/.m2/repository/com/twitter/algebird-core_2.11/0.9.0/algebird-core_2.11-0.9.0.jar
Added '/Users/alex/.m2/repository/com/twitter/algebird-core_2.11/0.9.0/algebird-core_2.11-0.9.0.jar'.  Your new classpath is:
"".:/opt/spark/spark-1.4.0-RC3-hadoop2.3:/Users/alex/.m2/repository/com/twitter/algebird-core_2.11/0.9.0/algebird-core_2.11-0.9.0.jar""
Nothing to replay.

scala> import com.twitter.algebird._
<console>:8: error: object algebird is not a member of package com.twitter
       import com.twitter.algebird._
                          ^
{code}

I've attached the output on shell startup to the issue as well.;;;","29/May/15 12:30;srowen;I assume the file does exist locally. I think --jars should make it available to the driver, yeah. How about 2.10?;;;","29/May/15 12:46;anakos@gmail.com;Yes, the file definitely exists locally. I have not built the 1.4.0-RC for 2.10 since 2.11 is our target scala version

In the past few months, I have built the 1.3.0 and 1.3.1 releases for both scala 2.10 and 2.11 and can confirm that I was able to load jars in the 2.10 REPL but encountered the same problems I'm citing here with the 2.11 REPL.

If you want I can try to build 1.4.0-RC03, target 2.10 and report back in a few hours.;;;","29/May/15 17:02;anakos@gmail.com;I have built 1.4.0-RC03 for scala 2.10 and can confirm that the contents of the jar file are available in the REPL when supplied the same way on the command line. I've attached the console output to this issue.;;;","31/May/15 19:39;tdas;Tagging [~brkyvz] as he is familiar with these --jar stuff. This would be good to fix.
;;;","31/May/15 19:46;brkyvz;I saw this issue with Yarn when using Scala 2.11 for Spark CSV as well. For some reason, the combinations of Yarn, Scala 2.11, and either the flag `\-\-jars` or `\-\-packages` doesn't work. It seems to add the JAR to the classpath 
{code}
15/05/29 17:58:12 INFO spark.SparkContext: Added JAR file:/Users/alex/.m2/repository/com/twitter/algebird-core_2.10/0.9.0/algebird-core_2.10-0.9.0.jar at http://192.168.0.11:49198/jars/algebird-core_2.10-0.9.0.jar with timestamp 1432918692788
{code}

For some reason, it's not there though.;;;","16/Jun/15 18:21;alexbaretta;Bug confirmed on Spark 1.4.0 with Scala 2.11.6. The --jars option to spark-shell is properly passed on to the SparkSubmit class, and the jars seem to be loaded, but the classes are not available in the REPL.

spark-shell --jars commons-csv-1.0.jar
...
15/06/16 17:57:32 INFO SparkContext: Added JAR file:/home/alex/commons-csv-1.0.jar at http://10.240.57.53:38821/jars/commons-csv-1.0.jar with timestamp 1434477452978
...
scala> org.apache.commons.csv.CSVFormat.DEFAULT
<console>:21: error: object csv is not a member of package org.apache.commons
              org.apache.commons.csv.CSVFormat.DEFAULT
                                 ^
;;;","16/Jun/15 19:10;vohprecio;just compiled version 1.5.0-SNAPSHOT Using Scala version 2.10.4 from github.
~/dev/spark(master) $build/mvn -DskipTests clean package
[INFO] BUILD SUCCESS ...

~/dev/spark(master) $bin/spark-shell --jars /Users/antigen/Downloads/algebird-core_2.10-0.10.2.jar

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_67)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context available as sc.
SQL context available as sqlContext.

scala> import com.twitter.algebird._
import com.twitter.algebird._;;;","16/Jun/15 20:41;dragos;I'll have a look tomorrow, I vaguely remember a bug in the Scala REPL that was fixed. Since the code is forked, the fix may not be in there...;;;","18/Jun/15 16:59;dragos;I can confirm this is working if the Spark REPL is based on a more recent version of the Scala REPL. I already have a branch that does away with 99% of the code, basically keeping only the ""initializeSpark"" and ""printWelcome"" calls.

You can assign this to me.;;;","18/Jun/15 17:25;srowen;I think you can just go ahead with a PR;;;","19/Jun/15 13:54;apachespark;User 'dragos' has created a pull request for this issue:
https://github.com/apache/spark/pull/6903;;;","24/Jun/15 16:01;irashid;[~dragos] just for reference for anyone else that runs into this, and can't wait to upgrade spark and / or scala -- do you know of any workaround without your patch?;;;","24/Jun/15 16:31;dragos;Unfortunately I don't think there's can be any workaround. The classpath is simply ignored in the current version.;;;","07/Jul/15 10:27;srowen;[~brkyvz] Can you comment on the PR? https://github.com/apache/spark/pull/6903
This seems to be blocked on making a new plugin.;;;","10/Jul/15 15:22;srowen;Issue resolved by pull request 6903
[https://github.com/apache/spark/pull/6903];;;",,,,,,,,,,,,
Receiver's life cycle is inconsistent with streaming job.,SPARK-7942,12833716,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,carlmartin,carlmartin,29/May/15 08:23,12/Sep/15 12:56,14/Jul/23 06:26,12/Sep/15 00:55,1.4.0,,,,,,1.5.0,,,,,,DStreams,,,,0,,,,,,"Streaming consider the receiver as a common spark job, thus if an error occurs in the receiver's  logical(after 4 times(default) retries ), streaming will no longer get any data but the streaming job is still running. 
A general scenario is that: we config the `spark.streaming.receiver.writeAheadLog.enable` as true to use the `ReliableKafkaReceiver` but do not set the checkpoint dir. Then the receiver will soon be shut down but the streaming is alive.",,carlmartin,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8882,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Sep 12 00:53:17 UTC 2015,,,,,,,,,,"0|i2fd9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/15 08:28;carlmartin;[~tdas] Should we add a logic to shut down the StreamingContext if the Receiver had been down?;;;","30/May/15 22:09;tdas;That is a very good idea. In fact please update the JIRA title to describe that feature. If there were receivers started, and all the receivers have shutdown, then stop the StreamingContext and throw error such that ssc.awaitTermination exits. This will be a good feature to add. ;;;","04/Jun/15 05:57;carlmartin;[~tdas] Now I'm not clear about how to deal with only some of the receivers had broken down, should we shutdown the StreamingContext ? Or ignore this and leave the alive receivers still running;;;","06/Jun/15 01:13;tdas;At the very least, we should throw route an exception to the streamingcontext (so that awaitTermination throws that exception) when ALL of them have exited. That is strictly better than what it is right now.;;;","12/Sep/15 00:53;tdas;This has been resolved in Spark 1.5.0 where we reimplemented the receiver scheduling to keep relaunching and not be limited by the max task retries;;;",,,,,,,,,,,,,,,,,,,,,,,,
Make URL partition recognition return String by default for all partition column types and values,SPARK-7939,12833687,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,huangjs,huangjs,29/May/15 06:20,08/Jun/15 10:20,14/Jul/23 06:26,08/Jun/15 09:51,1.4.0,,,,,,1.5.0,,,,,,SQL,,,,0,backport-needed,,,,,"Imagine the following HDFS paths:

/data/split=00
/data/split=01
...
/data/split=FF

If I have less than or equal to 10 partitions (00, 01, ... 09), currently partition recognition will treat column 'split' as integer column. 

If I have more than 10 partitions, column 'split' will be recognized as String...

This is very confusing. *So I'm suggesting to treat partition columns as String by default*, and allow user to specify types if needed.

Another example is date:
/data/date=2015-04-01 => 'date' is String
/data/date=20150401 => 'date' is Int

Jianshi",,apachespark,huangjs,lian cheng,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 09:51:24 UTC 2015,,,,,,,,,,"0|i2fd3r:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"29/May/15 07:15;lian cheng;We can probably provide a data source option to disable partition column type inference and default to {{StringType}}.;;;","29/May/15 08:29;huangjs;That would be nice, also consider disabling type inference as default behavior.

Jianshi;;;","29/May/15 17:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6503;;;","08/Jun/15 09:51;lian cheng;Issue resolved by pull request 6503
[https://github.com/apache/spark/pull/6503];;;",,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot compare Hive named_struct. (when using argmax, argmin)",SPARK-7937,12833684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,huangjs,huangjs,29/May/15 06:13,03/Aug/15 00:53,14/Jul/23 06:26,03/Aug/15 00:53,1.4.0,,,,,,1.5.0,,,,,,SQL,,,,0,releasenotes,,,,,"Imagine the following SQL:

Intention: get last used bank account country.
 
{code:sql}
select bank_account_id, 
  max(named_struct(
    'src_row_update_ts', unix_timestamp(src_row_update_ts,'yyyy/M/D HH:mm:ss'), 
    'bank_country', bank_country)).bank_country 
from bank_account_monthly
where year_month='201502' 
group by bank_account_id
{code}

=> 
{noformat}
Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 94 in stage 96.0 failed 4 times, most recent failure: Lost task 94.3 in stage 96.0 (TID 22281, xxxx): java.lang.RuntimeException: Type StructType(StructField(src_row_update_ts,LongType,true), StructField(bank_country,StringType,true)) does not support ordered operations
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.catalyst.expressions.LessThan.ordering$lzycompute(predicates.scala:222)
        at org.apache.spark.sql.catalyst.expressions.LessThan.ordering(predicates.scala:215)
        at org.apache.spark.sql.catalyst.expressions.LessThan.eval(predicates.scala:235)
        at org.apache.spark.sql.catalyst.expressions.MaxFunction.update(aggregates.scala:147)
        at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:165)
        at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:149)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{noformat}",,apachespark,huangjs,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 02 22:27:04 UTC 2015,,,,,,,,,,"0|i2fd33:",9223372036854775807,,,,,rxin,,,,,,,,,,,,,,,,,,,,,,"29/May/15 06:15;huangjs;Blog for describing Hive's argmax, argmin feature: https://www.joefkelley.com/?p=727

HIVE JIRA: https://issues.apache.org/jira/browse/HIVE-1128

Jianshi;;;","30/May/15 13:45;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6519;;;","02/Aug/15 22:27;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/7877;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
The default merge script JIRA username / password should be empty,SPARK-7933,12833647,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,kayousterhout,kayousterhout,29/May/15 02:01,29/May/15 02:06,14/Jul/23 06:26,29/May/15 02:05,,,,,,,1.4.0,,,,,,Project Infra,,,,0,,,,,,It looks like this was changed accidentally a few months ago.,,apachespark,kayousterhout,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 02:06:15 UTC 2015,,,,,,,,,,"0|i2fcv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/May/15 02:03;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/6485;;;","29/May/15 02:06;pwendell;Thanks - this was a dummy password I added in there, but yeah fine to have it be the empty string.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Scheduler delay shown in event timeline is incorrect,SPARK-7932,12833644,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,kayousterhout,kayousterhout,29/May/15 01:50,29/May/15 05:10,14/Jul/23 06:26,29/May/15 05:10,,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"In StagePage.scala, we round *down* to the nearest percent when computing the proportion of a task's time spend in each phase of execution.  Scheduler delay is computed by taking 100 - sum(all other proportions), which means that a few extra percent may go into the scheduler delay.  As a result, scheduler delay can appear larger in the visualization than it actually is.

cc [~shivaram]",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 05:10:40 UTC 2015,,,,,,,,,,"0|i2fcuf:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"29/May/15 01:56;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/6484;;;","29/May/15 05:10;kayousterhout;Issue resolved by pull request 6484
[https://github.com/apache/spark/pull/6484];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not restart a socket receiver when the receiver is being shutdown,SPARK-7931,12833636,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,29/May/15 01:07,30/May/15 22:10,14/Jul/23 06:26,30/May/15 22:10,,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,Attempts to restart the socket receiver when it is supposed to be stopped causes undesirable error messages.,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 01:12:03 UTC 2015,,,,,,,,,,"0|i2fcsn:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"29/May/15 01:12;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6483;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Shutdown hook deletes rool local dir before SparkContext is stopped, throwing errors",SPARK-7930,12833634,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,29/May/15 01:01,29/May/15 05:29,14/Jul/23 06:26,29/May/15 05:29,,,,,,,1.4.0,,,,,,DStreams,Spark Core,,,0,,,,,,"Shutdown hook for temp directories had priority 100 while SparkContext was 50. So the local root directory was deleted before SparkContext was shutdown. This leads to scary errors on running jobs, at the time of shutdown. This is especially a problem when running streaming examples, where Ctrl-C is the only way to  shutdown. ",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 01:05:04 UTC 2015,,,,,,,,,,"0|i2fcs7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"29/May/15 01:05;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6482;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make MLlib ChiSqSelector Serializable (& Fix Related Documentation Example).,SPARK-7920,12833507,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dusenberrymw,dusenberrymw,dusenberrymw,28/May/15 17:38,14/Jul/15 22:49,14/Jul/23 06:26,30/May/15 23:51,1.3.1,1.4.0,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,"The MLlib ChiSqSelector class is not serializable, and so the example in the ChiSqSelector documentation fails.  Also, that example is missing the import of ChiSqSelector.  ChiSqSelector should just extend Serializable.

Steps:
1. Locate the MLlib ChiSqSelector documentation example.
2. Fix the example by adding an import statement for ChiSqSelector.
3. Attempt to run -> notice that it will fail due to ChiSqSelector not being serializable. ",,apachespark,dusenberrymw,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 30 23:51:26 UTC 2015,,,,,,,,,,"0|i2fc13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/15 17:42;apachespark;User 'dusenberrymw' has created a pull request for this issue:
https://github.com/apache/spark/pull/6462;;;","30/May/15 23:51;josephkb;Issue resolved by pull request 6462
[https://github.com/apache/spark/pull/6462];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support specifying the column list for target table in CTAS,SPARK-7915,12833461,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,28/May/15 15:30,11/Jun/15 21:11,14/Jul/23 06:26,11/Jun/15 21:11,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"{code}
create table t1 (a int, b string) as select key, value from src;

desc t1;
key	int	NULL
value	string	NULL
{code}

Thus Hive doesn't support specifying the column list for target table in CTAS, however, we should either throwing exception explicitly, or supporting the this feature, we just pick up the later, which seems useful and straightforward.",,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 15:33:06 UTC 2015,,,,,,,,,,"0|i2fbr3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/15 15:33;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/6458;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rename ThriftServer tab in Spark UI,SPARK-7907,12833237,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,27/May/15 23:12,28/May/15 03:05,14/Jul/23 06:26,28/May/15 03:05,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"For users, it is confusing to see ThriftServer at there.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 03:05:11 UTC 2015,,,,,,,,,,"0|i2fadz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"27/May/15 23:43;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6448;;;","28/May/15 03:05;yhuai;Issue resolved by pull request 6448
[https://github.com/apache/spark/pull/6448];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL UDF doesn't support UDT in PySpark,SPARK-7902,12833147,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,mengxr,mengxr,27/May/15 19:09,14/Jan/16 23:49,14/Jul/23 06:26,09/Jul/15 21:43,1.4.0,,,,,,1.5.0,,,,,,PySpark,SQL,,,1,,,,,,"We don't convert Python SQL internal types to Python types in SQL UDF execution. This causes problems if the input arguments contain UDTs or the return type is a UDT. Right now, the raw SQL types are passed into the Python UDF and the return value is not converted to Python SQL types.

This is the code (from [~rams]) to produce this bug. (Actually, it triggers another bug first right now.)
{code}
from pyspark.mllib.linalg import SparseVector
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

df = sqlContext.createDataFrame([(SparseVector(2, {0: 0.0}),)], [""features""])
sz = udf(lambda s: s.size, IntegerType())
df.select(sz(df.features).alias(""sz"")).collect()
{code}",,apachespark,davies,josephkb,mengxr,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7903,,,,,,,SPARK-7903,,SPARK-7186,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 21:43:52 UTC 2015,,,,,,,,,,"0|i2f9uf:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"09/Jul/15 00:54;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/7301;;;","09/Jul/15 21:43;davies;Issue resolved by pull request 7301
[https://github.com/apache/spark/pull/7301];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark sql/tests breaks pylint validation,SPARK-7899,12833078,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mnazario,mnazario,mnazario,27/May/15 15:34,02/Jun/15 17:24,14/Jul/23 06:26,29/May/15 21:13,1.4.0,,,,,,1.4.0,,,,,,PySpark,Tests,,,0,,,,,,"The pyspark.sql.types module is dynamically named {{types}} from {{_types}} which messes up pylint validation

From [~justin.uang] below:

In commit 04e44b37, the migration to Python 3, {{pyspark/sql/types.py}} was renamed to {{pyspark/sql/\_types.py}} and then some magic in {{pyspark/sql/\_\_init\_\_.py}} dynamically renamed the module back to {{types}}. I imagine that this is some naming conflict with Python 3, but what was the error that showed up?

The reason why I'm asking about this is because it's messing with pylint, since pylint cannot now statically find the module. I tried also importing the package so that {{\_\_init\_\_}} would be run in a init-hook, but that isn't what the discovery mechanism is using. I imagine it's probably just crawling the directory structure.

One way to work around this would be something akin to this (http://stackoverflow.com/questions/9602811/how-to-tell-pylint-to-ignore-certain-imports), where I would have to create a fake module, but I would probably be missing a ton of pylint features on users of that module, and it's pretty hacky.",,apachespark,davies,mnazario,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 17:24:28 UTC 2015,,,,,,,,,,"0|i2f9f3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/15 15:41;mnazario;The problem is that pyspark/sql/types conflicts with the built-in Python 3 types module which causes tests to fail.

The Python documentation (https://docs.python.org/3/using/cmdline.html#interface-options) says that by calling ""python path/to/script.py"", the path of the script is automatically added to sys.path. This causes the conflict with the built-in Python 3 types module.

You can fix this by using ""-m"" in running pyspark tests instead since this will run by a module name on sys.path and not add the directory of the script to the python path.
;;;","27/May/15 18:41;apachespark;User 'mnazario' has created a pull request for this issue:
https://github.com/apache/spark/pull/6439;;;","27/May/15 20:54;justin.uang;Building upon michael's comment, the reason it fails is because if we run

{code}
run-test pyspark/sql/types.py
{code}

which runs

{code}
python pyspark/sql/types.py
{code}

then the directory containing the script is adding to the front of the PYTHONPATH, so now types.py is actually absolutely overriding the built-in python3 types module

https://docs.python.org/2/using/cmdline.html
{quote}
If the script name refers directly to a Python file, the directory containing that file is added to the start of sys.path, and the file is executed as the __main__ module.
{quote}
;;;","27/May/15 21:03;justin.uang;Building upon michael's comment, the reason it fails is because if we run

{code}
run-test pyspark/sql/types.py
{code}

which runs

{code}
python pyspark/sql/types.py
{code}

then the directory containing the script is adding to the front of the PYTHONPATH, so now types.py is actually absolutely overriding the built-in python3 types module

https://docs.python.org/2/using/cmdline.html
{quote}
If the script name refers directly to a Python file, the directory containing that file is added to the start of sys.path, and the file is executed as the __main__ module.
{quote}
;;;","29/May/15 21:13;davies;Issue resolved by pull request 6439
[https://github.com/apache/spark/pull/6439];;;","30/May/15 16:35;justin.uang;Can we get this back ported into spark 1.4 or is it too late for that.;;;","02/Jun/15 14:55;mnazario;[~davies] Could we get this backported to Spark 1.4?;;;","02/Jun/15 17:24;davies;I had done it yesterday.;;;",,,,,,,,,,,,,,,,,,,,,
Column with an unsigned bigint should be treated as DecimalType in JDBCRDD,SPARK-7897,12833049,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,27/May/15 14:03,06/Aug/15 23:34,14/Jul/23 06:26,06/Aug/15 23:32,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,,,apachespark,rtreffer,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7697,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 12 19:26:02 UTC 2015,,,,,,,,,,"0|i2f98n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/May/15 14:04;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6438;;;","12/Jun/15 19:24;rtreffer;I think this should be Decimal(20,0), not Decimal.Unlimited;;;","12/Jun/15 19:26;apachespark;User 'rtreffer' has created a pull request for this issue:
https://github.com/apache/spark/pull/6789;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
IndexOutOfBoundsException in ChainedBuffer,SPARK-7896,12833046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sandyr,arahuja,arahuja,27/May/15 13:47,28/May/15 05:30,14/Jul/23 06:26,28/May/15 05:30,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"I've run into this on two tasks that use the same dataset.

The dataset is a collection of strings where the most common string appears ~200M times and the next few appear ~50M times each.

for this rdd: RDD[String], I can do rdd.map( x => (x, 1)).reduceByKey( _ + _) to get the counts (how I got the number above), but I hit the error on rdd.groupByKey().

Also, I have a second RDD of strings rdd2: RDD[String] and I cannot do rdd2.leftOuterJoin(rdd) without hitting this error

{code}
15/05/26 23:27:55 WARN scheduler.TaskSetManager: Lost task 3169.1 in stage 5.0 (TID 4843, demeter-csmaz10-19.demeter.hpc.mssm.edu): java.lang.IndexOutOfBoundsException: 512
        at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)
        at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:47)
        at org.apache.spark.util.collection.ChainedBuffer.write(ChainedBuffer.scala:110)
        at org.apache.spark.util.collection.ChainedBufferOutputStream.write(ChainedBuffer.scala:141)
        at com.esotericsoftware.kryo.io.Output.flush(Output.java:155)
        at org.apache.spark.serializer.KryoSerializationStream.flush(KryoSerializer.scala:147)
        at org.apache.spark.util.collection.PartitionedSerializedPairBuffer.insert(PartitionedSerializedPairBuffer.scala:78)
        at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:219)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:62)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,arahuja,joshrosen,rdub,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 19:05:05 UTC 2015,,,,,,,,,,"0|i2f97z:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"27/May/15 17:14;joshrosen;I'm bumping this up to Blocker and assigning to [~sandyr] to investigate, since he wrote this code originally.  I can also look into this myself later this afternoon.;;;","27/May/15 17:16;arahuja;Tested with 1.2 and I do not hit the same error (the job has not completed - seems to be hung otherwise) but it does get much further and not produce the error above.;;;","27/May/15 17:17;sandyr;[~joshrosen] I'll take a look;;;","27/May/15 17:40;sandyr;This must be because we're overflowing the 2 GB limit of the ChainedBuffer.  512 * 4 MB  = 2 GB.

I'll post a patch that uses long indices.  Would it be easy for you to try it out today [~arahuja]?;;;","27/May/15 17:42;arahuja;Sure, happy to try it out - where is the 4mb coming from?;;;","27/May/15 19:02;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/6440;;;","27/May/15 19:05;sandyr;ChainedBuffer splits data into smaller buffers.  The default size for these buffers is 4 MB.;;;",,,,,,,,,,,,,,,,,,,,,,
Fixing broken trainImplicit example in MLlib Collaborative Filtering documentation.,SPARK-7883,12832902,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dusenberrymw,dusenberrymw,dusenberrymw,26/May/15 23:29,14/Jul/15 22:49,14/Jul/23 06:26,27/May/15 01:10,1.0.2,1.1.1,1.2.2,1.3.1,1.4.0,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.0,,Documentation,MLlib,,,0,,,,,,"The trainImplicit Scala example near the end of the MLlib Collaborative Filtering documentation refers to an ALS.trainImplicit function signature that does not exist.  Rather than add an extra function, let's just fix the example.

Currently, the example refers to a function that would have the following signature: 
def trainImplicit(ratings: RDD[Rating], rank: Int, iterations: Int, alpha: Double) : MatrixFactorizationModel

Instead, let's change the example to refer to this function, which does exist (notice the addition of the lambda parameter):
def trainImplicit(ratings: RDD[Rating], rank: Int, iterations: Int, lambda: Double, alpha: Double) : MatrixFactorizationModel",,apachespark,dusenberrymw,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 01:10:06 UTC 2015,,,,,,,,,,"0|i2f8c7:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.2,1.2.3,1.3.2,1.4.0,,,,,,,,,"26/May/15 23:31;apachespark;User 'dusenberrymw' has created a pull request for this issue:
https://github.com/apache/spark/pull/6422;;;","27/May/15 01:10;mengxr;Issue resolved by pull request 6422
[https://github.com/apache/spark/pull/6422];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Silent failure if assembly jar is corrupted,SPARK-7880,12832871,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,andrewor14,andrewor14,26/May/15 21:41,10/Sep/15 14:56,14/Jul/23 06:26,10/Sep/15 14:55,1.3.0,,,,,,1.5.0,,,,,,Spark Submit,,,,0,,,,,,"If you try to run `bin/spark-submit` with a corrupted jar, you get no output and your application does not run. We should have an informative message that indicates the failure to open the jar instead of silently swallowing it.

This is caused by this line:
https://github.com/apache/spark/blob/61664732b25b35f94be35a42cde651cbfd0e02b7/bin/spark-class#L75",,andrewor14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 10 14:56:03 UTC 2015,,,,,,,,,,"0|i2f83z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/15 22:26;srowen;BTW this line goes away in SPARK-7733 when we update to Java 7. I had held off merging that to master just while we're dealing with 1.4, but it could go in now. If that's really all there is to it you could consider it a subset of that issue, at least for 1.5. For prior branches, yeah, I don't know -- do we often have corrupted jars? this is intended to catch a Java 6 corner case.;;;","26/May/15 23:57;andrewor14;I was testing out RC2 for 1.4 and somehow ended up with a corrupted one. I thought it was a java 6 java 7 incompatibility issue but turns out something's just wrong with the way I downloaded it (?). Either way we should not hide the error message.;;;","10/Sep/15 14:55;srowen;So, I think this was actually no longer a problem in 1.4.0, because of the change I alluded to. If you have a bad assembly jar, you'll see that you do get an error (e.g. ""couldn't find class"").;;;","10/Sep/15 14:56;srowen;.... I meant 1.5 there of course.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Exception when using CLUSTER BY or ORDER BY,SPARK-7875,12832799,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tomdz,tomdz,26/May/15 17:47,07/Oct/16 22:14,14/Jul/23 06:26,07/Oct/16 22:14,1.3.1,,,,,,,,,,,,SQL,,,,0,,,,,,"Under certain circumstances that I haven't yet been able to isolate, I get the following error when doing a HQL query using HiveContext:

org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:746)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:56)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:259)
	at org.apache.spark.RangePartitioner$$anonfun$8.apply(Partitioner.scala:257)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:647)
	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:647)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)",Mesos scheduler with fine-grained mode,@madhu,smilegator,tomdz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 22:14:30 UTC 2016,,,,,,,,,,"0|i2f7of:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 22:14;smilegator;This should have been resolved in the latest branch. Close it now and please reopen it if you still hit it. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serializer re-use + Kryo autoReset disabled leads to AraryIndexOutOfBounds exception in sort-shuffle bypassMergeSort path,SPARK-7873,12832783,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,26/May/15 17:15,28/May/15 03:24,14/Jul/23 06:26,28/May/15 03:24,1.4.0,,,,,,1.4.0,,,,,,Shuffle,Spark Core,,,0,,,,,,"This is a somewhat obscure bug, but I think that it will seriously impact KryoSerializer users who use custom registrators which disabled auto-reset.  When auto-reset is disabled, then this breaks things in some of our shuffle paths which actually end up creating multiple OutputStreams from the same shared SerializerInstance (which is unsafe).  To illustrate this, the following test fails in 1.4:

{code}
class KryoSerializerAutoResetDisabledSuite extends FunSuite with SharedSparkContext {
  conf.set(""spark.serializer"", classOf[KryoSerializer].getName)
  conf.set(""spark.kryo.registrator"", classOf[RegistratorWithoutAutoReset].getName)

  test(""sort-shuffle with bypassMergeSort"") {
    val myObject = (""Hello"", ""World"")
    assert(sc.parallelize(Seq.fill(100)(myObject)).repartition(2).collect().toSet === Set(myObject))
  }
}
{code}

This was introduced by a patch (SPARK-3386) which enables serializer re-use in some of the shuffle paths, since constructing new serializer instances is actually pretty costly for KryoSerializer.  We had already fixed another corner-case (SPARK-7766) bug related to this, but missed this one.  From an engineering risk management perspective, we probably should have just reverted the original serializer reuse patch and added a big cross-product-of-configurations-and-shuffle-managers test suite before attempting to fix the defects.

I think that I have a pretty simple fix for this, but we still might want to consider a revert for 1.4 just to be safe.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3386,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 17:27:08 UTC 2015,,,,,,,,,,"0|i2f7lj:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"26/May/15 17:27;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6415;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark Data Frame Fails to Load Postgres Tables with JSONB DataType Columns,SPARK-7869,12832720,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,0x0fff,brdwrd,brdwrd,26/May/15 13:45,22/Feb/17 15:28,14/Jul/23 06:26,08/Oct/15 06:12,1.3.0,1.3.1,,,,,1.6.0,,,,,,PySpark,SQL,,,2,,,,,,"Most of our tables load into dataframes just fine with postgres. However we have a number of tables leveraging the JSONB datatype. Spark will error and refuse to load this table. While asking for Spark to support JSONB might be a tall order in the short term, it would be great if Spark would at least load the table ignoring the columns it can't load or have it be an option.
{code}
pdf = sql_context.load(source=""jdbc"", url=url, dbtable=""table_of_json"")

Py4JJavaError: An error occurred while calling o41.load.
: java.sql.SQLException: Unsupported type 1111
    at org.apache.spark.sql.jdbc.JDBCRDD$.getCatalystType(JDBCRDD.scala:78)
    at org.apache.spark.sql.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:112)
    at org.apache.spark.sql.jdbc.JDBCRelation.<init>(JDBCRelation.scala:133)
    at org.apache.spark.sql.jdbc.DefaultSource.createRelation(JDBCRelation.scala:121)
    at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:219)
    at org.apache.spark.sql.SQLContext.load(SQLContext.scala:697)
    at org.apache.spark.sql.SQLContext.load(SQLContext.scala:685)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
    at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
    at py4j.Gateway.invoke(Gateway.java:259)
    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
    at py4j.commands.CallCommand.execute(CallCommand.java:79)
    at py4j.GatewayConnection.run(GatewayConnection.java:207)
    at java.lang.Thread.run(Thread.java:745)
{code}",Spark 1.3.1,0x0fff,apachespark,brdwrd,ceefour,ibnipun10@gmail.com,ophiry,praveen.tallapudi,simeons,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10186,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 22 15:12:46 UTC 2017,,,,,,,,,,"0|i2f787:",9223372036854775807,,,,,,,,,,,,,,1.6.0,,,,,,,,,,,,,"24/Aug/15 17:15;simeons;This is also a problem for {{json}} columns, not just {{jsonb}} ones. 

It would be nice to get the JSON as a String column, instead of an error.
;;;","30/Sep/15 10:43;apachespark;User '0x0FFF' has created a pull request for this issue:
https://github.com/apache/spark/pull/8948;;;","20/Jun/16 13:53;ibnipun10@gmail.com;Still not resolves in spark version 1.6. I am seeing the same issue in spark;;;","22/Feb/17 15:12;praveen.tallapudi;Hi Nipun, I am using Spark. Is there a way to insert the Jsonb data into postgres. We have a new project in design phase. We are thinking of using Apache Spark + Postgres DB. But we are facing issues while inserting JSONB data type. 
Is there a support for Postgres-JSONB from spark? Can you please help us ?  I have posted this question in the issues but no response. We really need help, can you please let us know if there is a way of inserting?? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"Ignores ""_temporary"" directories while listing files in HadoopFsRelation",SPARK-7868,12832704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,26/May/15 12:40,27/May/15 03:49,14/Jul/23 06:26,27/May/15 03:49,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"In some cases, failed tasks/jobs may leave uncommitted partial/corrupted data in {{_temporary}} directory. These files should be counted as input files of a {{HadoopFsRelation}}.",,apachespark,lian cheng,neelesh77,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 03:49:17 UTC 2015,,,,,,,,,,"0|i2f74v:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"26/May/15 13:27;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6411;;;","27/May/15 03:49;yhuai;Issue resolved by pull request 6411
[https://github.com/apache/spark/pull/6411];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Clicking a job's DAG graph on Web UI kills the job as the link is broken,SPARK-7864,12832673,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,carsonwang,carsonwang,26/May/15 09:07,26/May/15 23:32,14/Jul/23 06:26,26/May/15 23:32,1.4.0,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"When clicking a job's DAG graph on Web UI, the user is expected to be redirected to the corresponding stage page. The link is got from the stage table by selecting the first link. But there are two links in each row, the first one is the killLink, the second is the nameLink.",,andrewor14,apachespark,carsonwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 21:27:05 UTC 2015,,,,,,,,,,"0|i2f6y7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"26/May/15 09:18;apachespark;User 'carsonwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6407;;;","26/May/15 21:17;andrewor14;Bumping this up to blocker because it kills innocent stages unwittingly.;;;","26/May/15 21:27;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6419;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleDateParam should not use SimpleDateFormat in multiple threads because SimpleDateFormat is not thread-safe,SPARK-7863,12832662,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,26/May/15 08:13,29/May/15 09:19,14/Jul/23 06:26,29/May/15 09:18,,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 09:18:14 UTC 2015,,,,,,,,,,"0|i2f6vr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/15 08:16;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6406;;;","29/May/15 09:18;srowen;Issue resolved by pull request 6406
[https://github.com/apache/spark/pull/6406];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Query would hang when the using script has error output in SparkSQL,SPARK-7862,12832649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,zhichao-li,zhichao-li,26/May/15 07:38,29/Jun/15 19:46,14/Jul/23 06:26,29/Jun/15 19:46,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Steps to reproduce:

val data = (1 to 100000).map { i => (i, i, i) }
data.toDF(""d1"", ""d2"", ""d3"").registerTempTable(""script_trans"")
 sql(""SELECT TRANSFORM (d1, d2, d3) USING 'cat 1>&2' AS (a,b,c) FROM script_trans"")",,apachespark,marmbrus,zhichao-li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 29 19:46:54 UTC 2015,,,,,,,,,,"0|i2f6sv:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.5.0,,,,,,,,,,,,,"26/May/15 07:41;apachespark;User 'zhichao-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/6404;;;","12/Jun/15 05:28;marmbrus;Issue resolved by pull request 6404
[https://github.com/apache/spark/pull/6404];;;","18/Jun/15 12:24;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/6882;;;","18/Jun/15 19:55;marmbrus;Reopened so we can fix the test output.;;;","29/Jun/15 19:46;marmbrus;Issue resolved by pull request 6882
[https://github.com/apache/spark/pull/6882];;;",,,,,,,,,,,,,,,,,,,,,,,,
Collect_SET behaves different under different version of JDK,SPARK-7859,12832602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,26/May/15 03:07,23/Jun/15 03:05,14/Jul/23 06:26,23/Jun/15 03:05,,,,,,,1.4.1,1.5.0,,,,,SQL,,,,0,,,,,,"To reproduce 
{code}
JAVA_HOME=/home/hcheng/Java/jdk1.8.0_45 | build/sbt -Phadoop-2.3 -Phive  'test-only org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryWithoutCodeGenSuite'
{code}

{panel}
- windowing.q -- 20. testSTATs *** FAILED ***
  Results do not match for windowing.q -- 20. testSTATs:
...

Manufacturer#1	almond antique burnished rose metallic	2	258.10677784349235	258.10677784349235	[34,2,6]	66619.10876874991	0.811328754177887	2801.7074999999995               
Manufacturer#1	almond antique burnished rose metallic	2	258.10677784349235	258.10677784349235	[2,34,6]	66619.10876874991	0.811328754177887	2801.7074999999995
{panel}",,apachespark,chenghao,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 03:05:15 UTC 2015,,,,,,,,,,"0|i2f6in:",9223372036854775807,,,,,yhuai,,,,,,,,,1.5.0,,,,,,,,,,,,,"26/May/15 03:11;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/6402;;;","23/Jun/15 03:05;yhuai;Issue resolved by pull request 6402
[https://github.com/apache/spark/pull/6402];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"DataSourceStrategy.createPhysicalRDD should use output schema when performing row conversions, not relation schema",SPARK-7858,12832596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,26/May/15 02:27,27/May/15 03:25,14/Jul/23 06:26,27/May/15 03:25,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"In {{DataSourceStrategy.createPhysicalRDD}}, we use the relation schema as the target schema for converting incoming rows into Catalyst rows.  However, we should be using the output schema instead, since our scan might return a subset of the relation's columns.",,apachespark,joshrosen,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7449,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 03:25:04 UTC 2015,,,,,,,,,,"0|i2f6hj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/May/15 02:34;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6400;;;","27/May/15 03:25;yhuai;Issue resolved by pull request 6400
[https://github.com/apache/spark/pull/6400];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException for SparkSQL,SPARK-7853,12832478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,chenghao,chenghao,25/May/15 08:45,29/May/15 00:12,14/Jul/23 06:26,29/May/15 00:12,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Reproduce steps:
{code}
bin/spark-sql --jars ./sql/hive/src/test/resources/hive-hcatalog-core-0.13.1.jar
CREATE TABLE t1(a string, b string) ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe';
{code}

Throws Exception like:
{noformat}
15/05/26 00:16:33 ERROR SparkSQLDriver: Failed in [CREATE TABLE t1(a string, b string) ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe']
org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Cannot validate serde: org.apache.hive.hcatalog.data.JsonSerDe
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:333)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:310)
	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:139)
	at org.apache.spark.sql.hive.client.ClientWrapper.runHive(ClientWrapper.scala:310)
	at org.apache.spark.sql.hive.client.ClientWrapper.runSqlHive(ClientWrapper.scala:300)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:457)
	at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:33)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:922)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:922)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:147)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:727)
	at org.apache.spark.sql.hive.thriftserver.AbstractSparkSQLDriver.run(AbstractSparkSQLDriver.scala:57)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:283)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:218)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{noformat}",,apachespark,chenghao,lian cheng,sb58,yhuai,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 29 00:12:49 UTC 2015,,,,,,,,,,"0|i2f5rb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"25/May/15 15:03;yuzhihong@gmail.com;Subject says ClassNotFoundException.
Which class couldn't be found ?;;;","25/May/15 15:06;chenghao;ClassNotFound is actually I got after investigation,and the class `org.apache.hadoop.hive.serde2.TestSerDe` can not be found.;;;","25/May/15 15:23;chenghao;And it seems the bug introduced by `IsolatedClientLoader` of Spark SQL, I am working on it for a workaround fixing.;;;","25/May/15 16:18;chenghao;Update the description, seesm 'TestSerDe` is not a good example as it's removed since hive-0.13;;;","25/May/15 16:21;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/6396;;;","27/May/15 05:24;lian cheng;OT: [~chenghao] Just edited the JIRA description. When pasting exception stack trace {{noformat}} can be more preferable than {{panel}} since it uses monospace font :);;;","27/May/15 11:05;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6435;;;","27/May/15 21:31;yhuai;Issue resolved by pull request 6435
[https://github.com/apache/spark/pull/6435];;;","28/May/15 15:52;yhuai;Seems my change in https://github.com/apache/spark/pull/6435 makes hive context fail to create in spark shell. Will submit a pr soon.;;;","28/May/15 16:01;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6459;;;","29/May/15 00:12;yhuai;Issue resolved by pull request 6459
[https://github.com/apache/spark/pull/6459];;;",,,,,,,,,,,,,,,,,,
Hive 0.12.0 profile in POM should be removed,SPARK-7850,12832451,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cheolsoo,cheolsoo,cheolsoo,25/May/15 03:33,27/May/15 07:18,14/Jul/23 06:26,27/May/15 07:18,1.4.0,,,,,,1.4.0,,,,,,Build,Documentation,,,0,,,,,,"Spark 1.4 supports the multiple metastore versions in a single build (hive-0.13.1) by introducing the IsolatedClientLoader, so {{-Phive-0.12.0}} is no longer needed.",,apachespark,ashwinshankar77,cheolsoo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 03:45:07 UTC 2015,,,,,,,,,,"0|i2f5lb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/May/15 03:45;apachespark;User 'piaozhexiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/6393;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix dynamic partition path escaping,SPARK-7847,12832409,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,lian cheng,lian cheng,24/May/15 12:46,27/May/15 17:09,14/Jul/23 06:26,27/May/15 17:09,1.3.0,1.3.1,1.4.0,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Background: when writing dynamic partitions, partition values are converted to string and escaped if necessary. For example, a partition column {{p}} of type {{String}} may have a value {{A/B}}, then the corresponding partition directory name is escaped into {{p=A%2fB}}.

Currently, there are two issues regarding to dynamic partition path escaping. The first issue is that, when reading back partition values, escaped strings are not unescaped. This one is easy to fix.

The second issue is more subtle. In [PR #5381|https://github.com/apache/spark/pull/5381/files#diff-c69b9e667e93b7e4693812cc72abb65fR492] we tried to use {{Path.toUri.toString}} to fix an escaping issue related to S3 credentials with {{/}} character. Unfortunately, {{Path.toUri.toString}} also escapes {{%}} characters in the path. Thus, using the dynamic partitioning case mentioned above, {{p=A%2fB}} is double escaped into {{p=A%252fB}} ({{%}} escaped into {{%25}}).

The expected behavior here should be, only escaping the URI user info part (S3 key and secret) but leave all other components untouched.",,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 17:09:31 UTC 2015,,,,,,,,,,"0|i2f5cf:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"24/May/15 12:59;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6389;;;","27/May/15 17:09;yhuai;Issue resolved by pull request 6389
[https://github.com/apache/spark/pull/6389];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Use different way to pass spark.yarn.keytab and spark.yarn.principal in different modes,SPARK-7846,12832408,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,24/May/15 12:34,29/May/15 16:08,14/Jul/23 06:26,29/May/15 16:08,,,,,,,1.5.0,,,,,,YARN,,,,0,,,,,,"--principal and --keytabl options are passed to client but when we started thrift server or spark-shell these two are also passed into the Main class (org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 and org.apache.spark.repl.Main).

In these two main class, arguments passed in will be processed with some 3rd libraries, which will lead to some error: ""Invalid option: --principal"" or ""Unrecgnised option: --principal"".

We should pass these command args in different forms, say system properties.",,apachespark,mdominguez@cloudera.com,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 24 13:14:03 UTC 2015,,,,,,,,,,"0|i2f5c7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/15 13:14;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/6051;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken tests in KernelDensity,SPARK-7844,12832395,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,MechCoder,MechCoder,MechCoder,24/May/15 06:27,27/May/15 07:03,14/Jul/23 06:26,26/May/15 20:22,,,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,"The densities in KernelDensity are scaled down by (number of parallel processes X number of points). This results in broken tests in KernelDensitySuite which haven't been tested properly. I think it should just be scaled down by (number of samples, i.e number of gaussian distributions)",,apachespark,MechCoder,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6192,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 20:22:57 UTC 2015,,,,,,,,,,"0|i2f59b:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"24/May/15 06:31;MechCoder;ping [~josephkb];;;","24/May/15 06:32;apachespark;User 'MechCoder' has created a pull request for this issue:
https://github.com/apache/spark/pull/6383;;;","26/May/15 20:22;mengxr;Issue resolved by pull request 6383
[https://github.com/apache/spark/pull/6383];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"For InsertIntoHadoopFsRelation, if an exception is thrown while committing a task, the task is not aborted",SPARK-7842,12832351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,lian cheng,lian cheng,23/May/15 14:04,25/May/15 16:30,14/Jul/23 06:26,25/May/15 16:30,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"This is related to spark-7838, where an exception is thrown when committing a task which writes a Parquet file. To be more specific, an exception is thrown from {{OutputWriter.close()}}. In this case, we should catch the exception and call {{abortTask()}} accordingly.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 16:30:02 UTC 2015,,,,,,,,,,"0|i2f4zz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"23/May/15 14:09;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6378;;;","25/May/15 16:30;lian cheng;Issue resolved by pull request 6378
[https://github.com/apache/spark/pull/6378];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark build should not use lib_managed for dependencies,SPARK-7841,12832340,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,dragos,dragos,23/May/15 10:45,18/Nov/15 02:08,14/Jul/23 06:26,10/Nov/15 18:14,1.3.1,,,,,,1.6.0,,,,,,Build,,,,0,easyfix,sbt,,,,"- unnecessary duplication (I will have those libraries under ./m2, via maven anyway)
- every time I call make-distribution I lose lib_managed (via mvn clean install) and have to wait to download again all jars next time I use sbt
- Eclipse does not handle relative paths very well (source attachments from lib_managed don’t always work)
- it's not the default configuration. If we stray from defaults I think there should be a clear advantage.

Digging through history, the only reference to `retrieveManaged := true` I found was in f686e3d, from July 2011 (""Initial work on converting build to SBT 0.10.1""). My guess this is purely an accident of porting the build form Sbt 0.7.x and trying to keep the old project layout.

If there are reasons for keeping it, please comment (I didn't get any answers on the [dev mailing list|http://apache-spark-developers-list.1001551.n3.nabble.com/Why-use-quot-lib-managed-quot-for-the-Sbt-build-td12361.html])",,apachespark,dougb,dragos,joshrosen,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11798,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 10 18:14:49 UTC 2015,,,,,,,,,,"0|i2f4xj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/15 20:17;joshrosen;I agree that we can probably fix this, but note that we'll have to do something about how lib_managed is used in the dev/mima script.;;;","17/Sep/15 06:55;dragos;Yes, there are a few build scripts (including make-distribution IIRC) that depend on having things in `lib_managed`. For the moment I'm applying a patch locally, I hope to have some time to look at this in the next week or two.;;;","09/Nov/15 22:18;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/9575;;;","10/Nov/15 18:14;marmbrus;Issue resolved by pull request 9575
[https://github.com/apache/spark/pull/9575];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Move Python DataFrame.insertInto into DataFrameWriter,SPARK-7840,12832324,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,23/May/15 06:26,23/May/15 16:09,14/Jul/23 06:26,23/May/15 16:09,,,,,,,1.4.0,,,,,,PySpark,SQL,,,0,,,,,,,,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 23 06:27:05 UTC 2015,,,,,,,,,,"0|i2f4tz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"23/May/15 06:27;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6375;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when save as parquet in speculative tasks,SPARK-7837,12832306,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,yhuai,yhuai,23/May/15 00:15,21/Jul/22 12:46,14/Jul/23 06:26,17/Aug/15 16:59,1.4.0,,,,,,1.5.0,,,,,,SQL,,,,2,,,,,,"The query is like {{df.orderBy(...).saveAsTable(...)}}.

When there is no partitioning columns and there is a skewed key, I found the following exception in speculative tasks. After these failures, seems we could not call {{SparkHadoopMapRedUtil.commitTask}} correctly.

{code}
java.lang.NullPointerException
	at parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:146)
	at parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:112)
	at parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:73)
	at org.apache.spark.sql.parquet.ParquetOutputWriter.close(newParquet.scala:115)
	at org.apache.spark.sql.sources.DefaultWriterContainer.abortTask(commands.scala:385)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:150)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:122)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:122)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,lian cheng,mengxr,mkanchwala,saurfang,sb58,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9072,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 21 12:46:17 UTC 2022,,,,,,,,,,"0|i2f4pz:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"29/May/15 00:25;yhuai;We have made the parquet reader side robust to files left in _temporary. So, this problem should have a much smaller impact. 

I am re-targeting it to 1.5. Will keep an eye on it and investigate the root cause.;;;","16/Jun/15 05:12;yhuai;Seems https://www.mail-archive.com/user@spark.apache.org/msg30327.html is about the same issue.;;;","15/Jul/15 13:04;mkanchwala;Facing the Same Issue at my End, I am processing around 68TB of data and transforming it to Parquet and I am also facing it. And this App is on Production, will really like it be resolved on highest priority in 1.5.0.

Thanks Guys;;;","15/Jul/15 14:48;yhuai;[~mkanchwala] One quick clarification question. Did your spark job fail because of the NPE? Or, your job succeeded but you saw the NPE in some speculative tasks? ;;;","15/Jul/15 16:46;mkanchwala;Job Succeeded But shown NPE, I am worried about any data loss between this transformation;;;","15/Jul/15 17:10;mkanchwala;Also I notice that I completed my transformation to Parquet but after the job completion it is taking too much time to save the data to s3.

Input : s3://<same-bucket>/input
Output : s3://<same-bucket>/output

I have around 27K files and it is writing the data at a speed of 1k / 20 mins and my job completed in an hour, can you share the details with me why it's so slow for parquet?;;;","15/Jul/15 18:05;yhuai;[~mkanchwala] There is a bug (https://issues.apache.org/jira/browse/SPARK-8406), which potentially may cause data loss for a large job. Please use Spark 1.4.1 as soon as it is released (I think today or tomorrow) or manually apply the fix (https://github.com/nemccarthy/spark/commit/ba365909b964fe5a5851d88f5f7b7edcd1998142) to your Spark 1.4.0 source code.

Regarding the slowness of saving parquet files in S3, a possible cause is that Parquet's original output committer ({{org.apache.parquet.hadoop.ParquetOutputCommitter}}) will first write data in the temporary dir and then move them to the right place when it commits tasks. This behavior is not necessary in most of the cases for S3 because ""S3 supports multiple writers outputting to the same file, where visibility is guaranteed to be atomic"" (https://gist.github.com/aarondav/c513916e72101bbe14ec). Once you upgrade to Spark 1.4.1, you can set {{spark.sql.parquet.output.committer.class}} to {{org.apache.spark.sql.parquet.DirectParquetOutputCommitter}} in your hadoop conf, which will write output files directly to their final locations. The only case that is not safe to use DirectParquetOutputCommitter is when you append data to an existing table. In this case, Spark 1.4.1 will internally switch back to the original Parquet output committer.;;;","15/Jul/15 18:23;mkanchwala;Thanks [~yhuai] for the update. Will be looking forward on Spark 1.4.1 release. Also I've filed a seperate issue to the Parent Bug as SPARK-9072 : Parquet : Writing data to S3 very slowly . Can you plan this and make the neccessary changes for the Same.

Thanks

;;;","14/Aug/15 20:46;yhuai;When speculation is on, I can reproduce it every time I run
{code}
sc.parallelize((1 to 100), 20).map { i =>
  if (i == 4 || i == 29) Thread.sleep(10000) else Thread.sleep(100)
  i
}.map(i => Tuple1(i)).toDF(""i"").write.mode(""overwrite"").format(""parquet"").save(""/home/yin/outputCommitter"")
{code};;;","16/Aug/15 17:20;lian cheng;Just a note to people who want to reproduce this issue:

# You need to start a Spark cluster with at least two workers running on two distinct nodes. Speculation isn't enabled when running in local mode or single node cluster. If you only have a single machine, you'll probably have to resort to VMs
# Don't forget to set {{spark.speculation}} to {{true}} (it's {{false}} by default);;;","17/Aug/15 02:54;yhuai;ah i see the reason of the NPE. We actually called close twice. In DefaultWriterContainer's writeRows, we start to write out rows and at the end we call commitTask. In commitTask, we first call writer.close and then we call super.commitTask(). In writer.close, we triggered ParquetRecordWriter's close, which sets columnStore to null. Then, because the speculative task's commit is rejected (i.e. super.commitTask() is rejected by OutputCommitCoordinator), we cal labortTask, which triggers writer.close again. Inside writer.close we call ParquetRecordWriter's close and then we get NPE because columnStore is already set to null.;;;","17/Aug/15 05:37;lian cheng;Good job!;;;","17/Aug/15 09:43;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/8236;;;","17/Aug/15 16:59;lian cheng;Issue resolved by pull request 8236
[https://github.com/apache/spark/pull/8236];;;","17/Aug/15 18:01;yhuai;[~mkanchwala] Based on our investigate, the NPE was caused by calling {{close}} of a Parquet record writer twice. The first time we call {{close}}, parquet sets {{columnStore}} (an parquet internal variable) to null and the second time we call {{close}}, the NPE is triggered.

This issue does not cause data loss.;;;","21/Jul/22 12:45;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37245;;;","21/Jul/22 12:46;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37245;;;",,,,,,,,,,,,
Refactor HeartbeatReceiverSuite for coverage and clean up,SPARK-7835,12832282,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,22/May/15 21:55,02/Jul/15 21:00,14/Jul/23 06:26,02/Jul/15 21:00,1.4.0,,,,,,1.5.0,,,,,,Spark Core,Tests,,,0,,,,,,"As of the writing of this description, the existing test suite has a lot of duplicate code and doesn't even cover the most fundamental feature of the HeartbeatReceiver, which is expiring hosts that have not responded in a while.

https://github.com/apache/spark/blob/31d5d463e76b6611c854c6cf27059fec8198adc9/core/src/test/scala/org/apache/spark/HeartbeatReceiverSuite.scala

We should rewrite this test suite to increase coverage and decrease duplicate code.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 02 01:49:03 UTC 2015,,,,,,,,,,"0|i2f4kn:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"22/May/15 21:56;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6310;;;","02/Jul/15 01:49;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/7173;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SortShuffleWriter writes inconsistent data & index files on stage retry,SPARK-7829,12832174,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,irashid,irashid,22/May/15 14:46,13/Nov/15 21:57,14/Jul/23 06:26,13/Nov/15 21:57,1.3.1,,,,,,1.5.3,1.6.0,,,,,Shuffle,Spark Core,,,0,,,,,,"When a stage is retried, even if a shuffle map task was successful, it may get retried in any case.  If it happens to get scheduled on the same executor, the old data file is *appended*, while the index file still assumes the data starts in position 0.  This leads to an apparently corrupt shuffle map output, since when the data file is read, the index file points to the wrong location.",,amcelwee,andrewor14,apachespark,diederik,irashid,joshrosen,kamilmroczek,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7308,,SPARK-8029,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 21:57:24 UTC 2015,,,,,,,,,,"0|i2f3xz:",9223372036854775807,,,,,,,,,,,,,,1.5.3,1.6.0,,,,,,,,,,,,"22/May/15 14:46;irashid;I'll submit a PR shortly;;;","24/May/15 04:03;joshrosen;Thanks for splitting this off as a sub-issue from SPARK-7308.  This issue might be one of the last remaining pieces for explaining some of the shuffle corruption issues that we've seen in sort-based shuffle.  A bug here would actually be consistent with some of the non-determinism of that issue, since it sounds like this issue is only triggered in certain stage retry cases when using certain shuffle paths.

As I commented over at SPARK-7308, the best way to address this might be with a sort of commit protocol in the ShuffleMapTask code. Some of the fixes that you've included for this as part of your other patch seem okay, but I think that they're a little messy compared to avoiding the appends in the first place.  I'm was wondering whether we could just delete the old file rather than appending to it, but that might mess things up if another concurrent downstream stage is attempting to fetch from those map output partitions while we're recomputing them.;;;","26/May/15 21:50;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/6420;;;","13/Nov/15 21:57;andrewor14;I believe this is now fixed due to https://github.com/apache/spark/pull/9610. Let me know if this is not the case.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Java8-tests suite compile error under SBT,SPARK-7820,12832046,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,jerryshao,jerryshao,jerryshao,22/May/15 05:05,02/Jul/15 18:37,14/Jul/23 06:26,01/Jul/15 19:35,1.4.0,,,,,,1.4.1,1.5.0,,,,,Build,DStreams,,,0,,,,,,"Lots of compilation error is shown when java 8 test suite is enabled in SBT:

{{JAVA_HOME=/usr/java/jdk1.8.0_45 ./sbt/sbt -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 -Pjava8-tests}}

{code}
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:43: error: cannot find symbol
[error] public class Java8APISuite extends LocalJavaStreamingContext implements Serializable {
[error]                                    ^
[error]   symbol: class LocalJavaStreamingContext
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:55: error: cannot find symbol
[error]     JavaDStream<String> stream = JavaTestUtils.attachTestInputStream(ssc, inputData, 1);
[error]                                                                      ^
[error]   symbol:   variable ssc
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:55: error: cannot find symbol
[error]     JavaDStream<String> stream = JavaTestUtils.attachTestInputStream(ssc, inputData, 1);
[error]                                  ^
[error]   symbol:   variable JavaTestUtils
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:57: error: cannot find symbol
[error]     JavaTestUtils.attachTestOutputStream(letterCount);
[error]     ^
[error]   symbol:   variable JavaTestUtils
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:58: error: cannot find symbol
[error]     List<List<Integer>> result = JavaTestUtils.runStreams(ssc, 2, 2);
[error]                                                           ^
[error]   symbol:   variable ssc
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:58: error: cannot find symbol
[error]     List<List<Integer>> result = JavaTestUtils.runStreams(ssc, 2, 2);
[error]                                  ^
[error]   symbol:   variable JavaTestUtils
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:73: error: cannot find symbol
[error]     JavaDStream<String> stream = JavaTestUtils.attachTestInputStream(ssc, inputData, 1);
[error]                                                                      ^
[error]   symbol:   variable ssc
[error]   location: class Java8APISuite
{code}

The class {{JavaAPISuite}} relies on {{LocalJavaStreamingContext}} which exists in streaming test jar. It is OK for maven compile, since it will generate test jar, but will be failed in sbt test compile, sbt do not generate test jar by default.",,apachespark,jerryshao,joshrosen,pwendell,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7818,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 01 19:35:10 UTC 2015,,,,,,,,,,"0|i2f35r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/15 07:29;srowen;You have a typo: java8-test -> java8-tests;;;","22/May/15 07:38;srowen;I meant your test fails because your command line has a typo, right? -Pjava8-test does not activate a profile.;;;","22/May/15 07:48;jerryshao;I checked the command line, it actually uses ""-Pjava8-tests"", only typo on the JIRA, also if this profile is not activated, the exception in this profile will not be met.;;;","22/May/15 07:54;srowen;This still can't be your command line as it doesn't do anything (no targets). What are you running exactly?
(PS you should use build/sbt, and -Phadoop2.6 now, but this won't matter here.);;;","22/May/15 07:58;jerryshao;Sorry for not mentioning so clearly, I just run {{test:compile}} which will meet such error, only {{compile}} is ok, since it does not compile test files.;;;","22/May/15 08:18;srowen;Yeah, the Maven build is correct and works. Is this an issue of SBT not reading the test dependencies?
Or, don't we have to make the assembly first for reasons like this before running tests?
It sounds like it's an SBT issue or at least a limitation of the SBT build as we use it here.;;;","22/May/15 08:53;jerryshao;I think it is the limitation of SBT, I tried different ways like {{assembly}}, {{package}}, {{test:package}} before running {{test:compile}}, but still failed. I think a common way like in Spark SQL is to move the shared test code into src/main if we want to share between modules, or copy/paste the same code twice for two modules.;;;","22/May/15 09:12;srowen;Test code shouldn't go into {{src/main}} just to work around this.;;;","22/May/15 18:31;tdas;Seems like this should get fixed before the next Spark 1.4 RC;;;","23/May/15 00:51;jerryshao;Hi [~tdas], what is your suggestion on fixing this? Currently there has two classes {{LocalJavaStreamingContext}} and {{JavaTestUtils}} which is in spark streaming test jar, also {{JavaTestUtils}} relies on {{TestSuiteBase}}. So there's no simple to handle this, either move these to src/main, or copy in to java8-tests module, both way are so elegant, what is your suggestion?;;;","24/May/15 00:15;pwendell;Since this only affects tests I'm de-escalating it, but I'd like to see it fixed as well before 1.4.0 ships if possible.
;;;","30/Jun/15 10:07;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/7120;;;","01/Jul/15 19:35;joshrosen;Issue resolved by pull request 7120
[https://github.com/apache/spark/pull/7120];;;",,,,,,,,,,,,,,,,
Isolated Hive Client Loader appears to cause Native Library libMapRClient.4.0.2-mapr.so already loaded in another classloader error,SPARK-7819,12832045,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,coderfi,coderfi,22/May/15 05:01,28/Jul/15 05:12,14/Jul/23 06:26,12/Jun/15 22:17,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,1,,,,,,"In reference to the pull request: https://github.com/apache/spark/pull/5876

I have been running the Spark 1.3 branch for some time with no major hiccups, and recently switched to the Spark 1.4 branch.

I build my spark distribution with the following build command:
{noformat}
make-distribution.sh --tgz --skip-java-test --with-tachyon -Phive -Phive-0.13.1 -Pmapr4 -Pspark-ganglia-lgpl -Pkinesis-asl -Phive-thriftserver
{noformat}
When running a python script containing a series of smoke tests I use to validate the build, I encountered an error under the following conditions:

* start a spark context
* start a hive context
* run any hive query
* stop the spark context
* start a second spark context
* run any hive query
** ERROR

From what I can tell, the Isolated Class Loader is hitting a MapR class that is loading its native library (presumedly as part of a static initializer).

Unfortunately, the JVM prohibits this the second time around.

I would think that shutting down the SparkContext would clear out any vestigials of the JVM, so I'm surprised that this would even be a problem.

Note: all other smoke tests we are running passes fine.

I will attach the stacktrace and a python script reproducing the issue (at least for my environment and build).",,coderfi,mandoskippy,marmbrus,nemccarthy,yhuai,zhpengg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9275,,,,,,,,,,,,,,,,,,,,,,"04/Jun/15 05:50;coderfi;invalidClassException.log;https://issues.apache.org/jira/secure/attachment/12737475/invalidClassException.log","22/May/15 05:05;coderfi;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12734742/stacktrace.txt","22/May/15 05:02;coderfi;test.py;https://issues.apache.org/jira/secure/attachment/12734740/test.py",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 16:26:19 UTC 2015,,,,,,,,,,"0|i2f35j:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"22/May/15 05:13;coderfi;FYI, I believe I have worked around the problem for now by disabling isolation by hacking:

sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala

-        isolationOn = true,
+       isolationOn = false,

I suppose this is good enough for us since we only need the pre-built version of hive (as provided by the mapr4 profile).;;;","22/May/15 07:27;srowen;Shutting down the SparkContext doesn't affect other things in the JVM. Isn't this a problem with how the native code is loaded? it should be able to check that it's loaded and avoid this if needed.;;;","23/May/15 00:19;marmbrus;I think the right solution here is probably to configure all of the MapR classes as shared across the isolation boundary.  Can you try adding any MapR packages prefixes to [{{spark.sql.hive.metastore.sharedPrefixes}}|https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L127]?;;;","23/May/15 04:16;coderfi;>>>  Isn't this a problem with how the native code is loaded?
Indeed, unfortunately, it's MapR's code, not mine :(

>>> Can you try adding any MapR packages prefixes to spark.sql.hive.metastore.sharedPrefixes
Ah, didn't think of that (even though I saw such a capability in the code), I'll try it out and let you know.;;;","23/May/15 06:05;coderfi;Adding the prefixes did not seem to work.

I tried adding this to my test.py script

    conf.set('spark.sql.hive.metastore.sharedPrefixes', 'com.mapr,mapr')

I also tried adding to spark-defaults.conf

spark.sql.hive.metastore.sharedPrefixes com.mapr,mapr
;;;","28/May/15 17:45;coderfi;Would it be possible to make the isolationOn parameter configurable?
Setting it to False (via a code change) worked around the problem for me.
I imagine there will be other cases where people are packaging other libraries (maybe third party hive UDFs) where they would find difficulty tracking down what prefixes to exclude.;;;","29/May/15 00:22;yhuai;[~coderfi] I just checked in a bug fix related to the class loader and the spark sql conf set in spark conf (e.g. spark-default). Can you try to the latest 1.4 branch and put the following entry in the {{conf/spark-defaults.conf}} 

{{spark.sql.hive.metastore.sharedPrefixes com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc,com.mapr.fs.shim.LibraryLoader,com.mapr.security.JNISecurity,com.mapr.fs.jni}}

Basically, it contains a few packages for JDBC drivers and a few mapr packages.

We will try to figure out a way to let JNI libs work with our two classloaders.;;;","29/May/15 00:26;yhuai;btw, I fix I did is https://github.com/apache/spark/commit/572b62cafe4bc7b1d464c9dcfb449c9d53456826.;;;","30/May/15 18:58;coderfi;Will do, thanks.;;;","31/May/15 05:50;coderfi;FYI, seems to work on a basic test, thanks!

However, I am running into a Out of PermGen space error on other tests further down in the same process.
I'll try increasing the memory settings in the JVM OPTS to see if it goes away. Hopefully it's not due to some sort of a resource leak, but simply because more classes need to be kept in memory now.

Fi
;;;","31/May/15 05:56;yhuai;[~coderfi] Can you provide some details on those other tests?;;;","04/Jun/15 05:50;coderfi;Hello, sorry for not responding sooner, been quite hectic at work.

We have a smoke test that I run whenever I'm testing a new Spark custom build.

Basically it's a python script that test various parts of the Spark API.
During the course of the execution, several Spark Contexts are created, as is HiveContext and SQLContext wrappers.
The test is rather light, but it does a decent job of giving me a heads up when an API changes underneath me so I can give our developers fair warning. :)
It does things like reading/writing parquet files, reading/writing files to MARPFS, word count jobs, hive queries, DataFrame API calls, etc.
It also serves as a light benchmark suite, so that I can keep an eye on performance that may have been introduced by the spark distribution, or by regular operational shenanigans on our Mesos cluster.

The test takes a simple 4-node dev/integration cluster about 200 seconds to run, moving around 100 GB of data from a non-local MAPRFS cluster via raw textFile and HiveContext/SQLContext queries.

Anyway, per my last comment, we ran out of PermGen in this script.

I created an even newer Spark 1.4 build, git 84da653192a2d9edb82d0dbe50f577c4dc6a0c78 and deployed it to our test cluster.
I then updated the spark-defaults.conf per your suggestions, as well as increasing the JVM PermGen settings:

    spark.sql.hive.metastore.sharedPrefixes com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc,com.mapr.fs.shim.LibraryLoader,com.mapr.security.JNISecurity,com.mapr.fs.jni

    spark.driver.extraJavaOptions -XX:+CMSClassUnloadingEnabled -XX:+CMSPermGenSweepingEnabled -XX:MaxPermSize=512M

I'm not sure if CMSClassUnloadingEnabled and CMSPermGenSweepingEnabled is needed. I came across these settings on StackOverflow, and it sounded like it wouldn't hurt, considering what the Isolated Hive Client Loader might be trying to do.

Incidentally, I typically run this smoke test script as an Ipython Notebook, this lets me also do smoke tests on non-spark related apis (such as using matplotlib).

With the above settings, I was able to get through the smoke test without errors.
Just for kicks, I ran it a second time (WITHIN the same running kernel), hoping (or not) to see a OOM.
It worked! So a third time, and it still worked.
I kicked it off a fourth time (still within the same ipython kernel) and was about to declare this a success, when the script failed with an InvalidClassCastException (attached).

Very strange! Not sure what could cause it.

Anyway, I tried a fifth time (still within the same kernel), and it passed just fine.

Considering the smoke tests worked fine 4 out of 5 times, I'm satisfied enough, and will chalk this up as some flakiness in the JVM and all the funky class loading. Also, did I mention that this ipython Notebook is also running in a docker container on a XEN Hypervisor VM ? Maybe that had something to do with it. :) 

So it would appear that increasing the PermGen space should be highly recommended (and maybe a default stock setting) in order to avoid the PermGen OOM error.





;;;","04/Jun/15 05:52;coderfi;Also, it is quite possible that the VM host itself was under a low memory condition at the time of my testing last time around, silly me, forgot to check....;;;","04/Jun/15 06:06;nemccarthy;Has anyone tried this with spark-submit? It works in the shell but it fails with a compiled app... 

{code}
  val conf = new SparkConf().set(""spark.sql.hive.metastore.sharedPrefixes"", ""com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc,com.mapr.fs.shim.LibraryLoader,com.mapr.security.JNISecurity,com.mapr.fs.jni"") //spark HC mapr4 workaround
  val sc = new SparkContext(conf)
  val hc = new HiveContext(sc)
  println("" Res = "" + hc.sql(""select pw_end_date from pz.sample_dates"").count())
{code}

@Fi - have you tried with spark-submit? ;;;","04/Jun/15 06:07;yhuai;For the InvalidClassCastException, is there any chance that the driver and workers were running different versions of Spark 1.4.0?;;;","04/Jun/15 06:08;yhuai;[~nemccarthy] Can you post the stack trace? Also, are you using 1.4.0 rc4?;;;","04/Jun/15 06:12;nemccarthy;Building of spark 1.4 branch, built 2 days ago. I can try building from the RC4 tag.

Spark-submit command;

/apps/spark/spark-1.4.0-SNAPSHOT-bin-mapr4.0.2_yarn_j6_2.10/bin/spark-submit --class com.myapp.TestMain --master yarn-client --conf spark.sql.hive.metastore.sharedPrefixes=com.mysql.jdbc,org.postgresql,com.microsoft.sqlserver,oracle.jdbc,com.mapr.fs.shim.LibraryLoader,com.mapr.security.JNISecurity,com.mapr.fs.jni  ~/app-jar_2.10-0.1.0-SNAPSHOT.jar 

Stack;
{code}
15/06/04 06:03:13 INFO metastore: Connected to metastore.
15/06/04 06:03:13 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:323)
	at com.mapr.fs.ShimLoader.load(ShimLoader.java:198)
	at org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:59)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:274)
	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1857)
	at org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2072)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2282)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2234)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2151)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1002)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:974)
	at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:518)
	at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:536)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:430)
	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:1366)
	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:1332)
	at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:99)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:170)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:166)
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:212)
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:175)
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:358)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:186)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:185)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:185)
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:71)
	at au.com.quantium.personalisation.sampling.TestMain$delayedInit$body.apply(TestMain.scala:26)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:40)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.App$$anonfun$main$1.apply(App.scala:71)
	at scala.App$$anonfun$main$1.apply(App.scala:71)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)
	at scala.App$class.main(App.scala:71)
	at au.com.quantium.personalisation.sampling.TestMain$.main(TestMain.scala:11)
	at au.com.quantium.personalisation.sampling.TestMain.main(TestMain.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.UnsatisfiedLinkError: Native Library /tmp/mapr-nathanm-libMapRClient.1.4.0-SNAPSHOT.so already loaded in another classloader
	at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1931)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1890)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1851)
	at java.lang.Runtime.load0(Runtime.java:795)
	at java.lang.System.load(System.java:1062)
	at com.mapr.fs.shim.LibraryLoader.load(LibraryLoader.java:29)
	... 56 more
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at com.mapr.fs.ShimLoader.load(ShimLoader.java:215)
	at org.apache.hadoop.conf.CoreDefaultProperties.<clinit>(CoreDefaultProperties.java:59)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:274)
	at org.apache.hadoop.conf.Configuration.getClassByNameOrNull(Configuration.java:1857)
	at org.apache.hadoop.conf.Configuration.getProperties(Configuration.java:2072)
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:2282)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:2234)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:2151)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:1002)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:974)
	at org.apache.hadoop.mapred.JobConf.setJar(JobConf.java:518)
	at org.apache.hadoop.mapred.JobConf.setJarByClass(JobConf.java:536)
	at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:430)
	at org.apache.hadoop.hive.conf.HiveConf.initialize(HiveConf.java:1366)
	at org.apache.hadoop.hive.conf.HiveConf.<init>(HiveConf.java:1332)
	at org.apache.spark.sql.hive.client.ClientWrapper.<init>(ClientWrapper.scala:99)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:170)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.<init>(IsolatedClientLoader.scala:166)
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:212)
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:175)
	at org.apache.spark.sql.hive.HiveContext.setConf(HiveContext.scala:358)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:186)
	at org.apache.spark.sql.SQLContext$$anonfun$3.apply(SQLContext.scala:185)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.sql.SQLContext.<init>(SQLContext.scala:185)
	at org.apache.spark.sql.hive.HiveContext.<init>(HiveContext.scala:71)
	at au.com.quantium.personalisation.sampling.TestMain$delayedInit$body.apply(TestMain.scala:26)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:40)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.App$$anonfun$main$1.apply(App.scala:71)
	at scala.App$$anonfun$main$1.apply(App.scala:71)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)
	at scala.App$class.main(App.scala:71)
	at au.com.quantium.personalisation.sampling.TestMain$.main(TestMain.scala:11)
	at au.com.quantium.personalisation.sampling.TestMain.main(TestMain.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.mapr.fs.ShimLoader.loadNativeLibrary(ShimLoader.java:323)
	at com.mapr.fs.ShimLoader.load(ShimLoader.java:198)
	... 50 more
Caused by: java.lang.UnsatisfiedLinkError: Native Library /tmp/mapr-nathanm-libMapRClient.1.4.0-SNAPSHOT.so already loaded in another classloader
	at java.lang.ClassLoader.loadLibrary1(ClassLoader.java:1931)
	at java.lang.ClassLoader.loadLibrary0(ClassLoader.java:1890)
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1851)
	at java.lang.Runtime.load0(Runtime.java:795)
	at java.lang.System.load(System.java:1062)
	at com.mapr.fs.shim.LibraryLoader.load(LibraryLoader.java:29)
	... 56 more
{code};;;","04/Jun/15 06:26;yhuai;[~nemccarthy] It will be great if you can try RC4 tag. There was a bug (SPARK-8020) that probably affected your test.;;;","04/Jun/15 06:30;coderfi;Actually, maybe the InvalidClassCastException might be a little too flaky.

I am running a spark job which queries an ORC table daily partition via the HiveContext.
A single context is created, and I spin up about ten threads (one for each day).

I am seeing plenty of the same errors in various tasks, enough to kill the job for that day.

Curiously, the exact same serialVersionUID are logged in each failure:

     org.apache.spark.sql.hive.MetastoreRelation; local class incompatible: stream classdesc serialVersionUID = 2590680563934099718, local class serialVersionUID = -8650941563091306200

So the interesting thing is that some jobs (for a particular day) work perfectly fine, but others fail.
I tried running this multi-threaded job again, and the same error occurs, but in different places. 
It looks like it usually works fine when Spark automatically schedules the task to be rerun.
So a workaround might be to bump up the number of allowable failures before giving up on the job.

This job works perfectly fine on our Spark 1.3 builds, unfortunately, this issue is occurring too often in a larger job in Spark 1.4 :(
;;;","04/Jun/15 06:45;coderfi;@Nathan: No, haven't tried spark-submit. That's for Scala/Java based jobs, right? I am running pyspark jobs, so no. However, while poking around in how this system works, I believe most of the bits that makes spark-submit work is done in the python case as well (i.e. load up spark-env.sh and read in spark-defaults.conf, as well as setup various classpaths).
BTW, the stacktrace is attached to this ticket as invalidClassException.log

@Yin: The driver and workers (running as a mesos framework), were all running the exact same spark tarball, deployed to SPARK_HOME=/opt/spark/current on the driver and each of the slaves.
I built from the most recent revision in https://github.com/apache/spark/commits/branch-1.4 (84da653192a2d9edb82d0dbe50f577c4dc6a0c78)
which already contains the SPARK-8020 fix.
Anyway, looking at that ticket, I don't recall seeing error messages in the log matching any of the noted stacktraces.
;;;","05/Jun/15 01:47;nemccarthy;@Yin - looks like might build was just a little out of date! RC4 is running well! Thanks!;;;","05/Jun/15 02:04;yhuai;[~nemccarthy] Thank you for the update! Glad to hear that :);;;","17/Jun/15 11:50;mandoskippy;Yin -

This occurred with my 1.4.0 release (not an earlier release, the official release) I had to put the prefixes in my spark-defaults.conf for it to work. Is this the official way to work around the problem or will something be changing in the code so the prefixes are not required?

Thanks

;;;","17/Jun/15 16:26;yhuai;[~mandoskippy] Yeah, it is the official way to work around the problem. Since 1.4.0, we introduced isolated class loaders to make Spark SQL be able to connect to different versions of Hive metastore. Basically, our metastore client part is using a different class loader than the other part of the Spark SQL. I am attaching the doc for this conf ({{spark.sql.hive.metastore.sharedPrefixes}}), hope this is helpful.

{quote}
A comma separated list of class prefixes that should be loaded using the classloader that is shared between Spark SQL and a specific version of Hive. An example of classes that should be shared is JDBC drivers that are needed to talk to the metastore. Other classes that need to be shared are those that interact with classes that are already shared.  For example, custom appenders that are used by log4j.
{quote}
;;;",,,,,
Fix typo on slf4j configuration on metrics.properties.template,SPARK-7811,12832011,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,judynash,judynash,judynash,22/May/15 00:55,24/May/15 20:49,14/Jul/23 06:26,24/May/15 20:49,,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,"There are a minor typo on slf4jsink configuration at metrics.properties.template. 

slf4j is mispelled as sl4j on 2 of the configuration. 

Correcting the typo so users' custom settings will be loaded correctly. ",,apachespark,judynash,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 24 20:49:08 UTC 2015,,,,,,,,,,"0|i2f2xz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/15 07:36;srowen;That's fine, do you want to make a PR?;;;","22/May/15 20:03;apachespark;User 'judynash' has created a pull request for this issue:
https://github.com/apache/spark/pull/6362;;;","24/May/15 20:49;srowen;Issue resolved by pull request 6362
[https://github.com/apache/spark/pull/6362];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
spark-ec2 launch script fails for Python3,SPARK-7806,12831987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,meawoppl,meawoppl,meawoppl,21/May/15 22:52,28/May/15 01:12,14/Jul/23 06:26,27/May/15 18:11,1.3.1,,,,,,1.4.0,,,,,,EC2,PySpark,,,0,,,,,,"Depending on the options used the spark-ec2 script will terminate ungracefully.  

Relevant buglets include:
 - urlopen() returning bytes vs. string
 - floor division change for partition calculation
 - filter() iteration behavior change in module calculation

I have a fixed version that I wish to contribute.  ",All platforms.  ,apachespark,meawoppl,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 01:12:59 UTC 2015,,,,,,,,,,"0|i2f2sv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/15 23:18;apachespark;User 'meawoppl' has created a pull request for this issue:
https://github.com/apache/spark/pull/6336;;;","26/May/15 18:03;meawoppl;This is mostly fixed by the PR above.  There needs to be a single line change in the deploy_scripts.py which needs a wrapping of its single print call here: https://github.com/mesos/spark-ec2/blob/branch-1.4/deploy_templates.py#L88

;;;","27/May/15 18:09;shivaram;Merged https://github.com/mesos/spark-ec2/pull/117 which fixes the issue on spark-ec2 side.;;;","27/May/15 18:11;shivaram;[~srowen] Could you add [~meawoppl] to the Developers group and assign this issue ? ;;;","27/May/15 20:29;meawoppl;There are a couple lingering issues server side, that I am triaging today: 
 - The config for the http server seems slightly broken, though this may be unrelated.  
 - Ganglia has a similar issue, but it may be part of the other config issue
 - The per-module init.sh and setup.sh seem to function subtly wrong on the master node.

Also notably, the macro-expand of the spark config ""templates"" seem very brittle and duplicated in a couple of places.
Should I reopen this issue or start a new one?;;;","27/May/15 20:38;shivaram;[~meawoppl] Lets open new issues for the other ones. ;;;","28/May/15 01:12;meawoppl;I started one here.  https://issues.apache.org/jira/browse/SPARK-7909

I feel like I am a half hour from another PR, but I am having some issues getting everything spun up nicely.  ;;;",,,,,,,,,,,,,,,,,,,,,,
Move SQLTestUtils.scala form src/main,SPARK-7805,12831976,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,pwendell,pwendell,21/May/15 22:25,24/May/15 17:00,14/Jul/23 06:26,24/May/15 16:52,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"These trigger binary compatibility issues when changed. In general we shouldn't be putting test code in src/main. If it's needed by multiple modules, IIRC we have a way to do that (look elsewhere in Spark).",,apachespark,pwendell,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 24 17:00:02 UTC 2015,,,,,,,,,,"0|i2f2qn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/15 22:56;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6334;;;","24/May/15 16:52;yhuai;Issue resolved by pull request 6334
[https://github.com/apache/spark/pull/6334];;;","24/May/15 17:00;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6391;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
isDefined should not marked too early in putNewKey,SPARK-7800,12831891,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,21/May/15 17:57,21/May/15 22:20,14/Jul/23 06:26,21/May/15 22:12,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,isDefined is marked as true twice in Location.putNewKey. The first one is unnecessary and will cause problem because it is too early and before some assert checking.,,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 22:12:27 UTC 2015,,,,,,,,,,"0|i2f28n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/15 18:00;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6324;;;","21/May/15 22:12;srowen;Issue resolved by pull request 6324
[https://github.com/apache/spark/pull/6324];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Use getOrElse for getting the threshold of SVM model,SPARK-7793,12831866,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,coderxiang,coderxiang,coderxiang,21/May/15 17:02,21/May/15 19:11,14/Jul/23 06:26,21/May/15 19:11,,,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,same issue and fix as in Spark-7694,,apachespark,coderxiang,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 19:11:27 UTC 2015,,,,,,,,,,"0|i2f233:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/15 17:04;apachespark;User 'coderxiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6321;;;","21/May/15 19:11;mengxr;Issue resolved by pull request 6321
[https://github.com/apache/spark/pull/6321];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveContext registerTempTable not thread safe,SPARK-7792,12831852,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,navis,yanakad,yanakad,21/May/15 16:06,10/Jun/15 02:33,14/Jul/23 06:26,10/Jun/15 02:33,1.3.1,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"{code:java}
public class ThreadRepro {
    public static void main(String[] args) throws Exception{
       new ThreadRepro().sparkPerfTest();
    }

    public void sparkPerfTest(){

        final AtomicLong counter = new AtomicLong();
        SparkConf conf = new SparkConf();
        conf.setAppName(""My Application"");
        conf.setMaster(""local[7]"");
        SparkContext sc = new SparkContext(conf);

        org.apache.spark.sql.hive.HiveContext hc = new org.apache.spark.sql.hive.HiveContext(sc);
        int poolSize = 10;
        ExecutorService pool = Executors.newFixedThreadPool(poolSize);
        for (int i=0; i<poolSize;i++ )
            pool.execute(new QueryJob(hc, i, counter));

        pool.shutdown();
        try {
            pool.awaitTermination(60, TimeUnit.MINUTES);
        }catch(Exception e){
            System.out.println(""Thread interrupted"");
        }
        System.out.println(""All jobs complete"");
        System.out.println("" Counter is ""+counter.get());

    }
}

class QueryJob implements Runnable{
    String threadId;
    org.apache.spark.sql.hive.HiveContext sqlContext;
    String key;
    AtomicLong counter;
    final AtomicLong local_counter = new AtomicLong();

    public QueryJob(org.apache.spark.sql.hive.HiveContext _sqlContext,int id,AtomicLong ctr){

        threadId = ""thread_""+id;
        this.sqlContext= _sqlContext;
        this.counter = ctr;
    }
    public void run() {
        for (int i = 0; i < 100; i++) {
            String tblName = threadId +""_""+i;
            DataFrame df = sqlContext.emptyDataFrame();
            df.registerTempTable(tblName);
            String _query = String.format(""select count(*) from %s"",tblName);
            System.out.println(String.format("" registered table %s; catalog (%s) "",tblName,debugTables()));
            List<Row> res;
            try {
                res = sqlContext.sql(_query).collectAsList();
            }catch (Exception e){
                System.out.println(""*Exception ""+ debugTables() +""**"");
                throw e;
            }
            sqlContext.dropTempTable(tblName);
            System.out.println("" dropped table ""+tblName);
            try {
                Thread.sleep(3000);//lets make this a not-so-tight loop
            }catch(Exception e){
                System.out.println(""Thread interrupted"");
            }
        }
    }

    private String debugTables(){
        String v = Joiner.on(',').join(sqlContext.tableNames());
        if (v==null)return """"; else return v;
    }
}
{code}

this will periodically produce the following:

{quote}
 registered table thread_0_50; catalog (thread_1_50)
 registered table thread_4_50; catalog (thread_4_50,thread_1_50)
 registered table thread_1_50; catalog (thread_1_50)
 dropped table thread_1_50
 dropped table thread_4_50
*Exception **
Exception in thread ""pool-6-thread-1"" java.lang.Error: org.apache.spark.sql.AnalysisException: no such table thread_0_50; line 1 pos 21
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1151)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.sql.AnalysisException: no such table thread_0_50; line 1 pos 21
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:177)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:186)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:181)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:187)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:208)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
  at scala.collection.AbstractIterator.to(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:238)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:193)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:178)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:181)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:171)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
  at scala.collection.immutable.List.foldLeft(List.scala:84)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
  at scala.collection.immutable.List.foreach(List.scala:318)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
  at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:1082)
  at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:1082)
  at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1080)
  at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
  at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
  at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:101)
  at test.unit.QueryJob.run(ThreadRepro.java:93)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
{quote}

Line 93 is the .sql call...",,apachespark,nemccarthy,yanakad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 08 07:08:03 UTC 2015,,,,,,,,,,"0|i2f207:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/15 07:08;apachespark;User 'navis' has created a pull request for this issue:
https://github.com/apache/spark/pull/6699;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"when use dynamic partitions, the partition string can be wrong without looking at the type",SPARK-7790,12831678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,21/May/15 10:05,27/May/15 19:44,14/Jul/23 06:26,27/May/15 19:42,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,,,adrian-wang,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6784,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 19:42:38 UTC 2015,,,,,,,,,,"0|i2f1f3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/15 10:08;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6318;;;","27/May/15 19:42;yhuai;Issue resolved by pull request 6318
[https://github.com/apache/spark/pull/6318];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Streaming | Kinesis | KinesisReceiver blocks in onStart",SPARK-7788,12831652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,aniket,aniket,21/May/15 08:29,23/May/15 00:39,14/Jul/23 06:26,23/May/15 00:39,1.3.0,1.3.1,,,,,1.4.0,,,,,,DStreams,,,,0,kinesis,,,,,"KinesisReceiver calls worker.run() which is a blocking call (while loop) as per source code of kinesis-client library - https://github.com/awslabs/amazon-kinesis-client/blob/v1.2.1/src/main/java/com/amazonaws/services/kinesis/clientlibrary/lib/worker/Worker.java.

This results in infinite loop while calling sparkStreamingContext.stop(stopSparkContext = false, stopGracefully = true) perhaps because ReceiverTracker is never able to register the receiver (it's receiverInfo field is a empty map) causing it to be stuck in infinite loop while waiting for running flag to be set to false. 

Also, we should investigate a way to have receiver restart in case of failures. ",,aniket,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 07:06:02 UTC 2015,,,,,,,,,,"0|i2f19b:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"21/May/15 08:42;tdas;This may be a good catch. I will try to look into it very soon and try to solve it for 1.4;;;","22/May/15 07:06;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6348;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SerializableAWSCredentials in KinesisReceiver cannot be deserialized ,SPARK-7787,12831648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,cfregly,tdas,21/May/15 08:04,21/May/15 18:40,14/Jul/23 06:26,21/May/15 18:40,,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,Lack of default constructor causes deserialization to fail. This occurs only when the AWS credentials are explicitly specified through KinesisUtils.,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 08:11:03 UTC 2015,,,,,,,,,,"0|i2f18f:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"21/May/15 08:11;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6316;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
A small problem on history server webpage,SPARK-7782,12831596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sb58,Xia Hu,Xia Hu,21/May/15 04:09,28/May/15 06:36,14/Jul/23 06:26,28/May/15 06:13,1.2.1,1.3.1,,,,,1.4.0,,,,,,Web UI,,,,0,starter,,,,,"A very little problem on spark history server webpage.

we can click on head of each row to sort the app lists, for example sort by ""start time"" or ""completed time"". But when it shows with the down arrow, it's actually with ascending order. 

",,apachespark,sb58,Xia Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 06:36:13 UTC 2015,,,,,,,,,,"0|i2f0wv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/15 07:58;srowen;Seems OK, do you want to submit a PR?;;;","27/May/15 13:54;apachespark;User 'zuxqoj' has created a pull request for this issue:
https://github.com/apache/spark/pull/6437;;;","27/May/15 20:16;srowen;PS you should make a better title; ""small problem"" doesn't describe anything to someone browsing.;;;","28/May/15 06:36;Xia Hu;Yeah, ok , I will use a more detail title next time. And I see zuxqoj had already solved this issue, that would be ok. Thank you~;;;",,,,,,,,,,,,,,,,,,,,,,,,,
GradientBoostedTrees is missing maxBins parameter in pyspark,SPARK-7781,12831594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,dondrake,dondrake,21/May/15 03:50,23/Jun/15 05:41,14/Jul/23 06:26,23/Jun/15 05:40,1.3.1,,,,,,1.4.1,1.5.0,,,,,MLlib,,,,0,,,,,,"I'm running Spark v1.3.1 and when I run the following against my dataset:

{code}
model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catFeatures, maxDepth=6, numIterations=3)

The job will fail with the following message:
Traceback (most recent call last):
  File ""/Users/drake/fd/spark/mltest.py"", line 73, in <module>
    model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catFeatures, maxDepth=6, numIterations=3)
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/tree.py"", line 553, in trainRegressor
    loss, numIterations, learningRate, maxDepth)
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/tree.py"", line 438, in _train
    loss, numIterations, learningRate, maxDepth)
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py"", line 120, in callMLlibFunc
    return callJavaFunc(sc, api, *args)
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py"", line 113, in callJavaFunc
    return _java2py(sc, func(*args))
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
15/05/20 16:40:12 INFO BlockManager: Removing block rdd_32_95
py4j.protocol.Py4JJavaError: An error occurred while calling o69.trainGradientBoostedTreesModel.
: java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 32) >= max categories in categorical features (= 1895)
	at scala.Predef$.require(Predef.scala:233)
	at org.apache.spark.mllib.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:128)
	at org.apache.spark.mllib.tree.RandomForest.run(RandomForest.scala:138)
	at org.apache.spark.mllib.tree.DecisionTree.run(DecisionTree.scala:60)
	at org.apache.spark.mllib.tree.GradientBoostedTrees$.org$apache$spark$mllib$tree$GradientBoostedTrees$$boost(GradientBoostedTrees.scala:150)
	at org.apache.spark.mllib.tree.GradientBoostedTrees.run(GradientBoostedTrees.scala:63)
	at org.apache.spark.mllib.tree.GradientBoostedTrees$.train(GradientBoostedTrees.scala:96)
	at org.apache.spark.mllib.api.python.PythonMLLibAPI.trainGradientBoostedTreesModel(PythonMLLibAPI.scala:595)
{code}

So, it's complaining about the maxBins, if I provide maxBins=1900 and re-run it:

{code}
model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catFeatures, maxDepth=6, numIterations=3, maxBins=1900)

Traceback (most recent call last):
  File ""/Users/drake/fd/spark/mltest.py"", line 73, in <module>
    model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catF
eatures, maxDepth=6, numIterations=3, maxBins=1900)
TypeError: trainRegressor() got an unexpected keyword argument 'maxBins'
{code}

It now says it knows nothing of maxBins.

If I run the same command against DecisionTree or RandomForest (with maxBins=1900) it works just fine.

Seems like a bug in GradientBoostedTrees. ",,apachespark,dondrake,holden,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 05:40:51 UTC 2015,,,,,,,,,,"0|i2f0wf:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"21/May/15 04:11;holden;I can take this :);;;","21/May/15 21:05;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6331;;;","23/Jun/15 05:40;josephkb;Issue resolved by pull request 6331
[https://github.com/apache/spark/pull/6331];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
The intercept in LogisticRegressionWithLBFGS should not be regularized,SPARK-7780,12831586,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,dbtsai,dbtsai,21/May/15 02:45,27/Jan/16 01:59,14/Jul/23 06:26,27/Jan/16 01:59,,,,,,,2.0.0,,,,,,MLlib,,,,1,,,,,,"The intercept in Logistic Regression represents a prior on categories which should not be regularized. In MLlib, the regularization is handled through `Updater`, and the `Updater` penalizes all the components without excluding the intercept which resulting poor training accuracy with regularization.

The new implementation in ML framework handles this properly, and we should call the implementation in ML from MLlib since majority of users are still using MLlib api. 

Note that both of them are doing feature scalings to improve the convergence, and the only difference is ML version doesn't regularize the intercept. As a result, when lambda is zero, they will converge to the same solution.",,apachespark,dbtsai,holden,josephkb,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6683,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 27 01:59:22 UTC 2016,,,,,,,,,,"0|i2f0un:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/15 21:10;holden;I can take a crack at this if that would be cool :);;;","21/May/15 22:20;dbtsai;@holdenk Sure. This one will be fun to work on. If it's binary LoR, just call the one in ML. Need to be careful about the updater. If the updater is L1Updater, just set the elasticNetParam to 1.0, and L2Updater, set the elasticNetParam to 0.0. ;;;","22/May/15 08:01;holden;Rad :) Yay for long weekends :);;;","22/May/15 21:01;holden;I was thinking that since the user could override this with any updater a reasonable plan would be to switch on the updater and if it was not one of the known/supported ones default to the old behaviour. Does that sound reasonable?;;;","22/May/15 21:03;josephkb;That sounds reasonable to me.;;;","22/May/15 21:04;dbtsai;++1
Although I don't think there are many users overriding the updater now, and most of the common use-cases are L1 and L2.;;;","22/May/15 23:08;josephkb;Linked related issue on correctness of LogisticRegressionWithLBFGS;;;","24/May/15 08:07;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6386;;;","11/Jun/15 23:57;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6771;;;","19/Aug/15 05:24;rxin;I'm assuming this is going to 1.6 / 1.5.1?
;;;","19/Aug/15 17:28;josephkb;Yep, 1.6 I'd say;;;","16/Jan/16 17:11;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/10788;;;","27/Jan/16 01:59;dbtsai;Issue resolved by pull request 10788
[https://github.com/apache/spark/pull/10788];;;",,,,,,,,,,,,,,,,
Dynamic allocation: confusing message when canceling requests,SPARK-7779,12831582,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,21/May/15 02:01,23/May/15 09:35,14/Jul/23 06:26,23/May/15 09:35,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Right now a long-running job with DA outputs the following. This is somewhat confusing to the user (why is my new desired total dropping??)
{code}
INFO ExecutorAllocationManager: Requesting 0 new executor(s) because tasks are backlogged (new desired total will be 46)
INFO ExecutorAllocationManager: Requesting 0 new executor(s) because tasks are backlogged (new desired total will be 44)
INFO ExecutorAllocationManager: Requesting 0 new executor(s) because tasks are backlogged (new desired total will be 42)
INFO ExecutorAllocationManager: Requesting 0 new executor(s) because tasks are backlogged (new desired total will be 40)
INFO ExecutorAllocationManager: Requesting 0 new executor(s) because tasks are backlogged (new desired total will be 38)
INFO ExecutorAllocationManager: Requesting 0 new executor(s) because tasks are backlogged (new desired total will be 36)
...
{code}",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 02:03:05 UTC 2015,,,,,,,,,,"0|i2f0tz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"21/May/15 02:03;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6301;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add shutdown hook to stop StreamingContext,SPARK-7776,12831560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,21/May/15 00:18,22/May/15 00:45,14/Jul/23 06:26,22/May/15 00:45,,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,"Shutdown hook to stop SparkContext was added recently. This results in ugly errors when a streaming application is terminated by ctrl-C.

{code}
Exception in thread ""Thread-27"" org.apache.spark.SparkException: Job cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:736)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:735)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:735)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1468)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1403)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1642)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:559)
	at org.apache.spark.util.SparkShutdownHook.run(Utils.scala:2266)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(Utils.scala:2236)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2236)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(Utils.scala:2236)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1764)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(Utils.scala:2236)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2236)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(Utils.scala:2236)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(Utils.scala:2236)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$6.run(Utils.scala:2218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
{code}

This is because the Spark's shutdown hook stops the context, and the streaming jobs fail in the middle. The correct solution is to stop the streaming context before the spark context. ",,apachespark,dibbhatt,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 01:05:13 UTC 2015,,,,,,,,,,"0|i2f0pb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"21/May/15 01:05;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6307;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN AM tried to sleep negative milliseconds,SPARK-7775,12831559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,21/May/15 00:15,21/May/15 19:34,14/Jul/23 06:26,21/May/15 19:34,1.5.0,,,,,,1.5.0,,,,,,YARN,,,,0,,,,,,"{code}
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Exception in thread ""Reporter"" java.lang.IllegalArgumentException: timeout value is negative
  at java.lang.Thread.sleep(Native Method)
  at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:356)
{code}

This kills the ""reporter thread"", which does some allocating too.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 19:34:38 UTC 2015,,,,,,,,,,"0|i2f0p3:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"21/May/15 00:40;andrewor14;Just realized the bug was not merged into 1.4, so I'm downgrading this from a blocker.;;;","21/May/15 00:46;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6305;;;","21/May/15 19:34;srowen;Issue resolved by pull request 6305
[https://github.com/apache/spark/pull/6305];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
KryoSerializerInstance reuse is not safe when auto-reset is disabled,SPARK-7766,12831503,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,20/May/15 20:16,26/May/15 17:24,14/Jul/23 06:26,22/May/15 20:29,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"SPARK-3386 modified the shuffle write path to re-use serializer instances across multiple calls to DiskBlockObjectWriter.  It turns out that this introduced a very rare bug when using KryoSerializer: if auto-reset is disabled and reference-tracking is enabled, then we'll end up re-using the same serializer instance to write multiple output streams without calling {{reset()}} between write calls, which can lead to cases where objects in one file may contain references to objects that are in previous files, which can cause errors during deserialization.

The fix should be simple: add {{reset}} calls at the end of {{serialize}} and {{serializeStream}}.

Thanks to John Carrino for reporting this issue on GItHub: https://github.com/apache/spark/pull/5606#issuecomment-103995103",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3386,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 26 17:24:17 UTC 2015,,,,,,,,,,"0|i2f0db:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"20/May/15 20:27;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6293;;;","22/May/15 20:29;joshrosen;Issue resolved by pull request 6293
[https://github.com/apache/spark/pull/6293];;;","26/May/15 17:24;joshrosen;I found another bug related to KryoSerializer re-use that our patch + tests missed: SPARK-7873;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Partition columns of data source tables should be persisted into metastore when creating persisted tables,SPARK-7763,12831429,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,20/May/15 16:52,21/May/15 20:52,14/Jul/23 06:26,21/May/15 20:52,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Partition columns of {{HadoopFsRelation}} should be persisted into Hive metastore together with relation schema. However, storing paths of all partitions into metastore might become a bottleneck. Instead, we always do a partition discovery while loading back the relation.",,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 20:52:01 UTC 2015,,,,,,,,,,"0|i2ezx3:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"20/May/15 17:16;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6285;;;","20/May/15 17:22;lian cheng;This bug also disables metastore relation cache for partitioned tables, because loaded tables lost partition information.;;;","21/May/15 20:52;yhuai;Issue resolved by pull request 6285
[https://github.com/apache/spark/pull/6285];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Master & Worker json endpoints missing,SPARK-7760,12831390,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,irashid,irashid,irashid,20/May/15 14:33,22/May/15 23:05,14/Jul/23 06:26,22/May/15 23:05,1.4.0,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"the standalone cluster master & worker ""/json"" endpoints were accidentally removed in https://issues.apache.org/jira/browse/SPARK-3454.  Need to add them back in",,apachespark,irashid,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 23:05:37 UTC 2015,,,,,,,,,,"0|i2ezof:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"20/May/15 15:38;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/6284;;;","20/May/15 20:29;joshrosen;I've added 1.4.0 as a target version so that this shows up on our release blockers dashboard.;;;","22/May/15 23:05;joshrosen;Issue resolved by pull request 6284
[https://github.com/apache/spark/pull/6284];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Failed to start thrift server when metastore is postgre sql,SPARK-7758,12831376,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,20/May/15 13:24,22/May/15 21:44,14/Jul/23 06:26,22/May/15 21:44,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"I am using today's master branch to start thrift server with setting metastore to postgre sql, and it shows error like:
{code}
15/05/20 20:43:57 DEBUG Schema: DROP TABLE DELETEME1432125837197 CASCADE
15/05/20 20:43:57 ERROR Datastore: Error thrown executing DROP TABLE DELETEME1432125837197 CASCADE : Syntax error: Encountered ""CASCADE"" at line 1, column 34.
java.sql.SQLSyntaxErrorException: Syntax error: Encountered ""CASCADE"" at line 1, column 34.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at org.datanucleus.store.rdbms.datasource.dbcp.DelegatingStatement.execute(DelegatingStatement.java:264)
	at org.datanucleus.store.rdbms.datasource.dbcp.DelegatingStatement.execute(DelegatingStatement.java:264)`

But it works well with earlier master branch (on 7th, April).
After printing their debug level log, I found current branch tries to connect with derby but didn't know why, maybe the big reconstructure in sql module cause this issue.

The Datastore shows in current branch:
15/05/20 20:43:57 DEBUG Datastore: ======================= Datastore =========================
15/05/20 20:43:57 DEBUG Datastore: StoreManager : ""rdbms"" (org.datanucleus.store.rdbms.RDBMSStoreManager)
15/05/20 20:43:57 DEBUG Datastore: Datastore : read-write
15/05/20 20:43:57 DEBUG Datastore: Schema Control : AutoCreate(None), Validate(None)
15/05/20 20:43:57 DEBUG Datastore: Query Languages : [JDOQL, JPQL, SQL, STOREDPROC]
15/05/20 20:43:57 DEBUG Datastore: Queries : Timeout=0
15/05/20 20:43:57 DEBUG Datastore: ===========================================================
15/05/20 20:43:57 DEBUG Datastore: Datastore Adapter : org.datanucleus.store.rdbms.adapter.PostgreSQLAdapter
15/05/20 20:43:57 DEBUG Datastore: Datastore : name=""Apache Derby"" version=""10.10.1.1 - (1458268)""
15/05/20 20:43:57 DEBUG Datastore: Datastore Driver : name=""Apache Derby Embedded JDBC Driver"" version=""10.10.1.1 - (1458268)""
15/05/20 20:43:57 DEBUG Datastore: Primary Connection Factory : URL[jdbc:derby:;databaseName=/tmp/spark-8b38e943-01e5-4341-9c92-7c250f2dec96/metastore;create=true]
15/05/20 20:43:57 DEBUG Datastore: Secondary Connection Factory : URL[jdbc:derby:;databaseName=/tmp/spark-8b38e943-01e5-4341-9c92-7c250f2dec96/metastore;create=true]
15/05/20 20:43:57 DEBUG Datastore: Datastore Identifiers : factory=""datanucleus1"" case=UPPERCASE catalog= schema=SPARK
15/05/20 20:43:57 DEBUG Datastore: Supported Identifier Cases : ""MixedCase"" UPPERCASE ""MixedCase-Sensitive"" 
15/05/20 20:43:57 DEBUG Datastore: Supported Identifier Lengths (max) : Table=128 Column=128 Constraint=128 Index=128 Delimiter=""
15/05/20 20:43:57 DEBUG Datastore: Support for Identifiers in DDL : catalog=false schema=true
15/05/20 20:43:57 DEBUG Datastore: Datastore : checkTableViewExistence, rdbmsConstraintCreateMode=DataNucleus, initialiseColumnInfo=ALL
15/05/20 20:43:57 DEBUG Datastore: Support Statement Batching : yes (max-batch-size=50)
15/05/20 20:43:57 DEBUG Datastore: Queries : Results direction=forward, type=forward-only, concurrency=read-only
15/05/20 20:43:57 DEBUG Datastore: Java-Types : string-default-length=255
15/05/20 20:43:57 DEBUG Datastore: JDBC-Types : [id=2009], BLOB, CLOB, TIME, DATE, BOOLEAN, VARCHAR, DECIMAL, NUMERIC, CHAR, BINARY, FLOAT, LONGVARBINARY, VARBINARY, JAVA_OBJECT
15/05/20 20:43:57 DEBUG Datastore: ===========================================================

The Datastore in earlier master branch:
15/05/20 20:18:10 DEBUG Datastore: ======================= Datastore =========================
15/05/20 20:18:10 DEBUG Datastore: StoreManager : ""rdbms"" (org.datanucleus.store.rdbms.RDBMSStoreManager)
15/05/20 20:18:10 DEBUG Datastore: Datastore : read-write
15/05/20 20:18:10 DEBUG Datastore: Schema Control : AutoCreate(None), Validate(None)
15/05/20 20:18:10 DEBUG Datastore: Query Languages : [JDOQL, JPQL, SQL, STOREDPROC]
15/05/20 20:18:10 DEBUG Datastore: Queries : Timeout=0
15/05/20 20:18:10 DEBUG Datastore: ===========================================================
15/05/20 20:18:10 DEBUG Datastore: Datastore Adapter : org.datanucleus.store.rdbms.adapter.PostgreSQLAdapter
15/05/20 20:18:10 DEBUG Datastore: Datastore : name=""GaussDB"" version=""9.2.1""
15/05/20 20:18:10 DEBUG Datastore: Datastore Driver : name=""PostgreSQL Native Driver"" version=""PostgreSQL 9.1 JDBC4 (build 902)""
15/05/20 20:18:10 DEBUG Datastore: Primary Connection Factory : URL[jdbc:postgresql://9.91.88.98:20051/sparkhivemeta]
15/05/20 20:18:10 DEBUG Datastore: Secondary Connection Factory : URL[jdbc:postgresql://9.91.88.98:20051/sparkhivemeta]
15/05/20 20:18:10 DEBUG Datastore: Datastore Identifiers : factory=""datanucleus1"" case=UPPERCASE schema=PUBLIC
15/05/20 20:18:10 DEBUG Datastore: Supported Identifier Cases : ""MixedCase"" UPPERCASE ""MixedCase-Sensitive"" 
15/05/20 20:18:10 DEBUG Datastore: Supported Identifier Lengths (max) : Table=63 Column=63 Constraint=63 Index=63 Delimiter=""
15/05/20 20:18:10 DEBUG Datastore: Support for Identifiers in DDL : catalog=false schema=true
15/05/20 20:18:10 DEBUG Datastore: Datastore : checkTableViewExistence, rdbmsConstraintCreateMode=DataNucleus, initialiseColumnInfo=ALL
15/05/20 20:18:10 DEBUG Datastore: Support Statement Batching : yes (max-batch-size=50)
15/05/20 20:18:10 DEBUG Datastore: Queries : Results direction=forward, type=forward-only, concurrency=read-only
15/05/20 20:18:10 DEBUG Datastore: Java-Types : string-default-length=255
15/05/20 20:18:10 DEBUG Datastore: JDBC-Types : BLOB, CLOB, TIMESTAMP, TIME, STRUCT, DATE, [id=3838], ARRAY, VARCHAR, NUMERIC, CHAR, REAL, LONGVARCHAR, LONGVARBINARY, SMALLINT, INTEGER, BIGINT, BIT, DOUBLE, DISTINCT
15/05/20 20:18:10 DEBUG Datastore: ===========================================================
{code}

I pasted the two logs and hive-site.xml here, the launch command is just ""./start-thriftserver.sh"".

Hope someone can help solve this. THanks!",,apachespark,marmbrus,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7759,,,,,,,,,,,,,,,,,,"20/May/15 13:30;WangTaoTheTonic;hive-site.xml;https://issues.apache.org/jira/secure/attachment/12734130/hive-site.xml","20/May/15 17:04;WangTaoTheTonic;with error.log;https://issues.apache.org/jira/secure/attachment/12734158/with+error.log","20/May/15 17:04;WangTaoTheTonic;with no error.log;https://issues.apache.org/jira/secure/attachment/12734159/with+no+error.log",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 21:44:45 UTC 2015,,,,,,,,,,"0|i2ezlb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"21/May/15 07:37;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/6314;;;","22/May/15 21:44;marmbrus;Issue resolved by pull request 6314
[https://github.com/apache/spark/pull/6314];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure Spark runs clean on IBM Java implementation,SPARK-7756,12831335,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tellison,t.p.ellison@gmail.com,t.p.ellison@gmail.com,20/May/15 10:33,26/Jun/15 13:49,14/Jul/23 06:26,29/May/15 09:15,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,Spark should run successfully on the IBM Java implementation.  This issue is to gather any minor issues seen running the tests and examples that are attributable to differences in Java vendor.,,apachespark,t.p.ellison@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8438,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 26 13:49:28 UTC 2015,,,,,,,,,,"0|i2ezc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/15 11:30;apachespark;User 'tellison' has created a pull request for this issue:
https://github.com/apache/spark/pull/6282;;;","29/May/15 09:15;srowen;Issue resolved by pull request 6282
[https://github.com/apache/spark/pull/6282];;;","10/Jun/15 08:39;apachespark;User 'a-roberts' has created a pull request for this issue:
https://github.com/apache/spark/pull/6740;;;","26/Jun/15 11:49;apachespark;User 'tellison' has created a pull request for this issue:
https://github.com/apache/spark/pull/7043;;;","26/Jun/15 11:53;t.p.ellison@gmail.com;Using a single JIRA to gather a disparate set of issues found using IBM's Java is not going to work.

I suggest keeping my original SSL failure and a-roberts' ""RDDOperationScope fix"" here, but subsequent bugs should have their own JIRA and be tagged with ""ibmjdk"" so I can easily find them.
;;;","26/Jun/15 13:49;srowen;I agree, https://github.com/apache/spark/pull/6740 should have been separate. The other two PRs are logically related.;;;",,,,,,,,,,,,,,,,,,,,,,,
"Rename ""json"" endpoints to ""api"" endpoints",SPARK-7750,12831233,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,hshreedharan,hshreedharan,hshreedharan,20/May/15 00:30,21/May/15 02:19,14/Jul/23 06:26,21/May/15 02:14,1.3.0,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"Sorry for bringing this in at the last moment, but we should rename the endpoints from <address>/json/v1... to <address>/api/v1.. since it is likely we will add API that can do more than return JSON, like being able to download event logs. In that case the current api can be misleading.

This is a compatibility issue, so it has to go into 1.4",,apachespark,hshreedharan,irashid,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 02:14:32 UTC 2015,,,,,,,,,,"0|i2eypj:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"20/May/15 00:54;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/6273;;;","20/May/15 03:04;markhamstra;Including `@Produces` annotations is also probably a good idea, especially if we are anticipating producing more than one kind of output.;;;","21/May/15 02:14;irashid;Issue resolved by pull request 6273
[https://github.com/apache/spark/pull/6273];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet metastore conversion does not use metastore cache,SPARK-7749,12831225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,yhuai,yhuai,20/May/15 00:14,21/May/15 17:56,14/Jul/23 06:26,21/May/15 17:56,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,Seems https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala#L462-467 is wrong.,,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 17:56:39 UTC 2015,,,,,,,,,,"0|i2eynr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"20/May/15 17:39;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6287;;;","21/May/15 17:56;yhuai;Issue resolved by pull request 6287
[https://github.com/apache/spark/pull/6287];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ContextCleaner not used by many DStream operations,SPARK-7741,12831155,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,tdas,tdas,19/May/15 20:24,20/May/15 22:40,14/Jul/23 06:26,20/May/15 22:40,1.1.1,1.2.2,1.3.1,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,"In the 1.4 branch, this results in java.io.NotSerializableExceptions when trying to use operators wrapped in `withScope` but do not clean closures.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7644,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 21:41:05 UTC 2015,,,,,,,,,,"0|i2ey8v:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"19/May/15 20:31;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6268;;;","19/May/15 21:41;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6269;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
parquet schema discovery should not fail because of empty _temporary dir ,SPARK-7737,12831067,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,19/May/15 16:29,28/May/15 11:42,14/Jul/23 06:26,21/May/15 23:13,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Parquet schema discovery will fail when the dir is like 
{code}
/partitions5k/i=2/_SUCCESS
/partitions5k/i=2/_temporary/
/partitions5k/i=2/part-r-00001.gz.parquet
/partitions5k/i=2/part-r-00002.gz.parquet
/partitions5k/i=2/part-r-00003.gz.parquet
/partitions5k/i=2/part-r-00004.gz.parquet
{code}

{code}
java.lang.AssertionError: assertion failed: Conflicting partition column names detected:
	
	at scala.Predef$.assert(Predef.scala:179)
	at org.apache.spark.sql.sources.PartitioningUtils$.resolvePartitions(PartitioningUtils.scala:159)
	at org.apache.spark.sql.sources.PartitioningUtils$.parsePartitions(PartitioningUtils.scala:71)
	at org.apache.spark.sql.sources.HadoopFsRelation.org$apache$spark$sql$sources$HadoopFsRelation$$discoverPartitions(interfaces.scala:468)
	at org.apache.spark.sql.sources.HadoopFsRelation$$anonfun$partitionSpec$3.apply(interfaces.scala:424)
	at org.apache.spark.sql.sources.HadoopFsRelation$$anonfun$partitionSpec$3.apply(interfaces.scala:423)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.sources.HadoopFsRelation.partitionSpec(interfaces.scala:422)
	at org.apache.spark.sql.sources.HadoopFsRelation.schema$lzycompute(interfaces.scala:482)
	at org.apache.spark.sql.sources.HadoopFsRelation.schema(interfaces.scala:480)
	at org.apache.spark.sql.sources.LogicalRelation.<init>(LogicalRelation.scala:30)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:134)
	at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:118)
	at org.apache.spark.sql.SQLContext.load(SQLContext.scala:1135)
{code}

1.3 works fine.",,apachespark,lian cheng,sranga,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 23:13:04 UTC 2015,,,,,,,,,,"0|i2expr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"21/May/15 18:10;yhuai;Seems https://github.com/apache/spark/pull/6287 still not fix partition discovery completely. For the case in the description, the following case works
{code}
load(""/partitions5k/i=2/"", ""parquet"")
{code}
However, for this case, we fail...
{code}
load(""/partitions5k/"", ""parquet"")
{code};;;","21/May/15 20:17;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6329;;;","21/May/15 23:13;lian cheng;Issue resolved by pull request 6329
[https://github.com/apache/spark/pull/6329];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Exception not failing Python applications (in yarn cluster mode),SPARK-7736,12831058,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,roji,roji,19/May/15 16:12,08/Dec/17 00:01,14/Jul/23 06:26,09/Sep/15 22:50,,,,,,,1.5.1,1.6.0,,,,,YARN,,,,0,,,,,,"It seems that exceptions thrown in Python spark apps after the SparkContext is instantiated don't cause the application to fail, at least in Yarn: the application is marked as SUCCEEDED.

Note that any exception right before the SparkContext correctly places the application in FAILED state.","Spark 1.3.1, Yarn 2.7.0, Ubuntu 14.04",apachespark,dmreshet,edadashov,neelesh77,roji,shivaram,storpipfugl,vanzin,yash360@gmail.com,ztoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9416,SPARK-8612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 08 00:01:12 UTC 2017,,,,,,,,,,"0|i2exnr:",9223372036854775807,,,,,,,,,,,,,,1.5.1,1.6.0,,,,,,,,,,,,"25/Jun/15 20:51;neelesh77;Could you add more context to the issue? 
What is the return value / output expected on the applications?

;;;","25/Jun/15 20:55;roji;The problem is simply with the YARN status for the application. If a Spark application throws an exception after having instantiated the SparkContext, the application obviously terminates but YARN lists the job as SUCCEEDED. This makes it hard for users to see what happened to their jobs in the YARN UI.

Let me know if this is still unclear.;;;","08/Jul/15 07:06;storpipfugl;Platform: spark 1.3.0, CDH 5.4.1

To reproduce with pyspark:

---
from pyspark import SparkContext
with SparkContext(appName=""raise_uncaught"") as sc:
    raise Exception('Fail')
---
$ spark-submit --master yarn-cluster /path/to/my/pythonscript.py

This ends up with the following YARN status:
State:	FINISHED
FinalStatus:	SUCCEEDED
Diagnostics:	Shutdown hook called before final status was reported.

If the exception is thrown before the SparkContext is initialized YARN status displays as expected:
---
from pyspark import SparkContext
raise Exception('Fail')
with SparkContext(appName=""raise_caught"") as sc:
    pass
---

This ends up with the following YARN status:
State:	FAILED
FinalStatus:	FAILED
Diagnostics: <trace>

It seems (from the Diagnostics message) that  https://github.com/apache/spark/blob/19834fa9184f0365a160bcb54bcd33eaa87c70dc/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala : L118 is hit when exceptions are raised after initializing SparkContext. This also means applications are not retried when failures happen after SparkContext initialization.
;;;","08/Jul/15 16:37;neelesh77;My 2 cents:

To have a YARN failed, ApplicationMaster running the driver needs to fail. 

Scenario:
1) It fails once, YARN retries and succeeds if the exception has been handled correctly. This results in a Successful YARN job (assuming the child tasks (executors) succeeded).
2) The retries fail and the YARN job fails completely.
You need the Spark Application to coz a failure in YARN to mark it as a Failure.

Moreover, the ApplicationMaster.java code from the: 
/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java in the Hadoop project should help. 

Reference: 
[1] http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html 

So, I would say this is expected behavior.
Hope that helps. 

Please add/correct me if needed.;;;","09/Jul/15 07:09;storpipfugl;Thanks for the comment. I don't understand how it apply here however as both listed pyspark programs (In my understanding) should result in step 2) of your scenario:

p1) Unhandled execption raised before SparkContext initialization:
---
from pyspark import SparkContext
raise Exception('Fail')
sc = SparkContext(appName=""raise_seen_by_yarn"")
---
This results in an AM retry (total 2 AM tries as per YARN default) and subsequent marking of the application YARN status as FAILED. This is what I expect for a ""designed to fail AM"".

p2) Unhandled execption raised after SparkContext initialization:
---
from pyspark import SparkContext
sc = SparkContext(appName=""raise_not_seen_by_yarn""):
raise Exception('Fail')
---
This results in the the application being marked as SUCCEEDED (total of 1 AM try) which is not what I expect for a ""designed to fail AM"".

I've tried to look in the spark documentation if there should be taken special actions to signal failure to YARN but I haven't found anything? And looking at src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala : L118 where all sys.exit calls are considered successful termination regardless of exit code I can't see a way to signal failure to YARN after SparkContext initialization?

Both p1 and p2 return with non-zero exit code when run with spark-submit --master yarn-client which is what I would expect.
;;;","10/Jul/15 06:41;roji;Neelesh, not sure I understood what you're saying exactly... I agree with Esben that at the end of the day, if a Spark application fails (by throwing an exception), and does so on all Yarn application attempts, that the Yarn status of that application definitely should be FAILED...;;;","03/Aug/15 20:23;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/7751;;;","17/Aug/15 17:35;vanzin;Leaving open for 1.5.1 backport.;;;","17/Aug/15 23:44;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/8258;;;","25/Sep/15 13:06;ztoth;As I see, this is also a problem for SparkR applications in yarn-cluster mode. Is there an open JIRA for that?;;;","25/Sep/15 17:48;shivaram;[~ztoth] Could you open a new JIRA for the SparkR problem ?;;;","28/Sep/15 08:43;ztoth;Created SPARK-10851.;;;","11/Oct/15 11:31;roji;Have just tested this with Spark 1.5.1 on Yarn 2.7.1 and the problem is still there - an exception thrown after the SparkContext has been created terminates the application but Yarn reports it as succeeded.;;;","16/Mar/17 00:18;yash360@gmail.com;This does not seem Fixed. The application still completes with SUCCESS status even when an exception is thrown from the application.
Spark version 2.0.2.;;;","01/Dec/17 22:08;dmreshet;Spark 2.2 still facing that issue.
In my case Azkaban executes Spark Job and finalStatus of this job in Resource Manager is SUCCESS in anycase.;;;","08/Dec/17 00:01;vanzin;Make sure all you guys are running apps in cluster mode if you want to see the proper status. I just ran a failing pyspark app in cluster mode to double check, and all seems fine.;;;",,,,,,,,,,,,,
Raise Exception on non-zero exit from pyspark pipe commands,SPARK-7735,12831045,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,svt,svt,svt,19/May/15 15:24,11/Jul/15 15:06,14/Jul/23 06:26,11/Jul/15 02:30,1.3.0,1.3.1,,,,,1.5.0,,,,,,PySpark,,,,1,newbie,patch,,,,"In pyspark errors are ignored when using the rdd.pipe function. This is different to the scala behaviour where abnormal exit of the piped command is raised. I have submitted a pull request on github which I believe will bring the pyspark behaviour closer to the scala behaviour.

A simple case of where this bug may be problematic is using a network bash utility to perform computations on an rdd. Currently, network errors will be ignored and blank results returned when it would be more desirable to raise an exception so that spark can retry the failed task.",,apachespark,davies,lucamartinetti,svt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://github.com/apache/spark/pull/6262,,Patch,,,,,,,,9223372036854775807,,,Sat Jul 11 02:30:39 UTC 2015,,,,,,,,,,"0|i2exkv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/15 15:26;apachespark;User 'megatron-me-uk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6262;;;","11/Jul/15 02:30;davies;Issue resolved by pull request 6262
[https://github.com/apache/spark/pull/6262];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Complex Teradata queries throwing Analysis Exception when running on spark,SPARK-7730,12831007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pankhuri,pankhuri,19/May/15 13:26,07/Oct/16 22:21,14/Jul/23 06:26,07/Oct/16 22:21,1.3.1,,,,,,,,,,,,SQL,,,,0,,,,,,"Connected spark wth tearadata. When running below TeraData query on spark-shell:

select substr(w_warehouse_name,1,20) as xx,sm_type,cc_name
,sum(case when (cs_ship_date_sk - cs_sold_date_sk <= 30 ) then 1 else 0 end)  as days
  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 30) and 
                 (cs_ship_date_sk - cs_sold_date_sk <= 60) then 1 else 0 end )  as sdays 
  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 60) and 
                 (cs_ship_date_sk - cs_sold_date_sk <= 90) then 1 else 0 end)  as rdays 
  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk > 90) and
                 (cs_ship_date_sk - cs_sold_date_sk <= 120) then 1 else 0 end)  as ndays
  ,sum(case when (cs_ship_date_sk - cs_sold_date_sk  > 120) then 1 else 0 end)  as dfdays
from test
where d_month_seq between 1193 and 1193 + 11
and cs_ship_date_sk   = d_date_sk
and cs_warehouse_sk   = w_warehouse_sk
and cs_ship_mode_sk   = sm_ship_mode_sk
and cs_call_center_sk = cc_call_center_sk
group by xx ,sm_type ,cc_name order by xx,sm_type,cc_name

org.apache.spark.sql.AnalysisException: cannot resolve 'xx' given input columns cc_name, sdays, days, sm_type, rdays, xx, ndays, dfdays;",develeopement,pankhuri,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7731,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 22:21:26 UTC 2016,,,,,,,,,,"0|i2excf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 22:21;smilegator;It should have been fixed. Please reopen it, if you still hit it. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL: cannot add timestamp partition as string,SPARK-7728,12830989,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kjsingh,kjsingh,19/May/15 11:44,07/Oct/16 22:22,14/Jul/23 06:26,07/Oct/16 22:22,1.2.1,,,,,,,,,,,,SQL,,,,0,,,,,,"create table spark_test(i int) partitioned by (p timestamp);
alter table spark_test add partition(p=""2013-01-01 01:00"");
alter table spark_test add partition(p=CAST(""2013-01-01 01:00"" AS TIMESTAMP));

Will not work",,kjsingh,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 22:22:20 UTC 2016,,,,,,,,,,"0|i2ex8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 22:22;smilegator;It should have been fixed in the native DDL support. Please check it. If you still hit it, please reopen this JIRA. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven Install Breaks When Upgrading Scala 2.11.2-->[2.11.3 or higher],SPARK-7726,12830956,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dragos,pwendell,pwendell,19/May/15 09:21,11/Aug/15 05:39,14/Jul/23 06:26,19/May/15 19:15,,,,,,,1.4.0,,,,,,Build,,,,0,,,,,,"This one took a long time to track down. The Maven install phase is part of our release process. It runs the ""scala:doc"" target to generate doc jars. Between Scala 2.11.2 and Scala 2.11.3, the behavior of this plugin changed in a way that breaks our build. In both cases, it returned an error (there has been a long running error here that we've always ignored), however in 2.11.3 that error became fatal and failed the entire build process. The upgrade occurred in SPARK-7092. Here is a simple reproduction:

{code}
./dev/change-version-to-2.11.sh
mvn clean install -pl network/common -pl network/shuffle -DskipTests -Dscala-2.11
{code} 

This command exits success when Spark is at Scala 2.11.2 and fails with 2.11.3 or higher. In either case an error is printed:

{code}
[INFO] 
[INFO] --- scala-maven-plugin:3.2.0:doc-jar (attach-scaladocs) @ spark-network-shuffle_2.11 ---
/Users/pwendell/Documents/spark/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/UploadBlock.java:56: error: not found: type Type
  protected Type type() { return Type.UPLOAD_BLOCK; }
            ^
/Users/pwendell/Documents/spark/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/StreamHandle.java:37: error: not found: type Type
  protected Type type() { return Type.STREAM_HANDLE; }
            ^
/Users/pwendell/Documents/spark/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/RegisterExecutor.java:44: error: not found: type Type
  protected Type type() { return Type.REGISTER_EXECUTOR; }
            ^
/Users/pwendell/Documents/spark/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/OpenBlocks.java:40: error: not found: type Type
  protected Type type() { return Type.OPEN_BLOCKS; }
            ^
model contains 22 documentable templates
four errors found
{code}

Ideally we'd just dig in and fix this error. Unfortunately it's a very confusing error and I have no idea why it is appearing. I'd propose reverting SPARK-7092 in the mean time.",,apachespark,dragos,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7670,,,,,,,,,,,,SPARK-7092,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 05:39:04 UTC 2015,,,,,,,,,,"0|i2ex13:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"19/May/15 09:50;dragos;This fix in 2.11.3 looks like the culprit (meaning that the error was always there, but ignored): https://issues.scala-lang.org/browse/SI-8885;;;","19/May/15 10:10;dragos;The problem is different visibility rules in Scala and Java w.r.t to statics (and the fact that scaladoc is invoked on Java sources).

In this particular case, `Type` is a static enumeration inherited from `BlockTransferMessage`. In Scala statics are part of the companion object, and not visible without an import. Since scaladoc needs to resolve all types it documents, this comes up as an error.

I fixed this by adding a static import in all those files

{code}
import static org.apache.spark.network.shuffle.protocol.BlockTransferMessage.Type;
{code}

{code}
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Spark Project Networking ........................... SUCCESS [ 11.395 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  5.460 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 17.582 s
[INFO] Finished at: 2015-05-19T12:07:01+02:00
[INFO] Final Memory: 42M/456M
[INFO] ------------------------------------------------------------------------
{code}

Should I open a pull request reverting the reverting commit and add this fix? Or you'd like to suppress scaladoc running on Java files (assuming that's not intentional...);;;","19/May/15 10:22;apachespark;User 'dragos' has created a pull request for this issue:
https://github.com/apache/spark/pull/6260;;;","19/May/15 17:07;srowen;Hoping to get this back in for 1.4.0.;;;","10/Aug/15 22:33;pwendell;[~srowen] [~dragos] This is cropping up again when trying to create a release candidate for Spark 1.5:

https://amplab.cs.berkeley.edu/jenkins/view/Spark-Packaging/job/Spark-Release-All-Java7/26/console;;;","11/Aug/15 05:39;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/8095;;;",,,,,,,,,,,,,,,,,,,,,,,
Style checks do not run for Kinesis on Jenkins,SPARK-7722,12830927,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,pwendell,pwendell,19/May/15 07:04,21/May/15 20:50,14/Jul/23 06:26,21/May/15 20:50,,,,,,,1.4.0,,,,,,DStreams,Project Infra,,,0,,,,,,"This caused the release build to fail late in the game. We should make sure jenkins is proactively checking it:

https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commitdiff;h=23cf897112624ece19a3b5e5394cdf71b9c3c8b3;hp=9ebb44f8abb1a13f045eed60190954db904ffef7",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 18:23:02 UTC 2015,,,,,,,,,,"0|i2ewun:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/15 18:23;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6325;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java 6 code in UnsafeShuffleWriterSuite,SPARK-7719,12830920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,pwendell,pwendell,19/May/15 06:04,21/May/15 00:53,14/Jul/23 06:26,21/May/15 00:53,,,,,,,1.4.0,,,,,,Spark Core,Tests,,,0,,,,,,"This was causing a compile failure because emptyIterator() is not exposed in some versions of Java 6. I lost the exact compile error along the way in the console, but it's just a simple visibility issue.

https://github.com/apache/spark/commit/9ebb44f8abb1a13f045eed60190954db904ffef7

I've removed the test code for now, but we probably want to use something from Guava instead for this:
http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/collect/Iterators.html",,apachespark,joshrosen,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 00:53:28 UTC 2015,,,,,,,,,,"0|i2ewt3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/15 22:01;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6298;;;","21/May/15 00:53;joshrosen;Issue resolved by pull request 6298
[https://github.com/apache/spark/pull/6298];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Speed up data source partitioning by avoiding cleaning closures,SPARK-7718,12830919,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,19/May/15 06:01,21/May/15 21:33,14/Jul/23 06:26,21/May/15 21:33,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"The new partitioning support strategy creates a bunch of RDDs (1 per partition, could be up to several thousands), then calls `mapPartitions` on every single one of these RDDs. This causes us to clean the same closure many times. Since we provide the closure in Spark we know for sure it is serializable, so we can bypass the cleaning for performance.

According to [~yhuai] cleaning 5000 closures take up to 6-7 seconds in a 12 seconds job that involves data source partitioning.",,andrewor14,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 21:33:35 UTC 2015,,,,,,,,,,"0|i2ewsv:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"19/May/15 06:05;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6256;;;","21/May/15 21:33;yhuai;Issue resolved by pull request 6256
[https://github.com/apache/spark/pull/6256];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark Standalone Web UI showing incorrect total memory, workers and cores",SPARK-7717,12830871,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhichao-li,swaranga,swaranga,18/May/15 23:42,30/May/15 12:07,14/Jul/23 06:26,30/May/15 12:07,1.3.1,,,,,,1.5.0,,,,,,Web UI,,,,0,web-ui,,,,,"I launched a Spark master in standalone mode in one of my host and then launched 3 workers on three different hosts. The workers successfully connected to my master and the Web UI showed the correct details. Specifically, the Web UI correctly shows that the total memory and the total cores available for the cluster.

However on one of the worker, I did a ""kill -9 <worker process id>"" and restarted the worker again. This time though, the master's Web UI shows incorrect total memory and number of cores. The total memory is shown to be 4*n, where ""n"" is the memory in each worker. Also the total workers is shown as 4 and the total number of cores shown is incorrect, it shows 4*c, where ""c"" is the number of cores on each worker.",RedHat,apachespark,swaranga,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/15 23:49;swaranga;JIRA.PNG;https://issues.apache.org/jira/secure/attachment/12733667/JIRA.PNG",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 30 12:07:15 UTC 2015,,,,,,,,,,"0|i2ewif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/15 23:49;swaranga;Attached screenshot highlighting issue;;;","21/May/15 08:40;apachespark;User 'zhichao-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/6317;;;","30/May/15 12:07;srowen;Issue resolved by pull request 6317
[https://github.com/apache/spark/pull/6317];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Use shared broadcast hadoop conf for partitioned table scan.,SPARK-7713,12830806,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,18/May/15 19:54,20/May/15 18:24,14/Jul/23 06:26,20/May/15 18:24,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"While debugging SPARK-7673, we also found that we are broadcasting a hadoop conf for every Partition (backed by a Hadoop RDD). It also causes the performance regression of compiling a query involving a large number of partitions.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 20 18:24:04 UTC 2015,,,,,,,,,,"0|i2ew3z:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"18/May/15 20:03;srowen;Same as / similar to https://issues.apache.org/jira/browse/SPARK-7410 ?;;;","18/May/15 20:13;yhuai;Yes. It is the same thing. I will have a fix just for SQL because we just migrated our parquet to our new API for partitioned data sources and I am worry about the performance regression for users having parquet tables with lots of partitions. Then, we can have a general purpose fix under SPARK-7410.;;;","19/May/15 02:24;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6252;;;","20/May/15 18:24;yhuai;Issue resolved by pull request 6252
[https://github.com/apache/spark/pull/6252];;;",,,,,,,,,,,,,,,,,,,,,,,,,
startTime() is missing,SPARK-7711,12830793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,holdenk_amp,sds,sds,18/May/15 19:01,21/May/15 21:09,14/Jul/23 06:26,21/May/15 21:09,1.3.1,,,,,,1.4.0,,,,,,PySpark,,,,0,,,,,,"In PySpark 1.3, there appears to be no [startTime|https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#startTime%28%29]:
{code}
>>> sc.startTime  
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
AttributeError: 'SparkContext' object has no attribute 'startTime'
{code}
Scala has the method:
{code}
scala> sc.startTime
res1: Long = 1431974499272
{code}",,apachespark,holden,joshrosen,sds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,http://stackoverflow.com/q/30310630/850781,,,,,,,,,,9223372036854775807,,,Thu May 21 21:09:24 UTC 2015,,,,,,,,,,"0|i2ew13:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/15 23:53;holden;I can add this.;;;","20/May/15 07:23;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/6275;;;","21/May/15 21:09;joshrosen;Issue resolved by pull request 6275
[https://github.com/apache/spark/pull/6275];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Column with an unsigned int should be treated as long in JDBCRDD,SPARK-7697,12830556,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,teppeid,teppeid,18/May/15 02:53,27/May/15 14:05,14/Jul/23 06:26,27/May/15 07:28,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Columns with an unsigned numeric type in JDBC should be treated as the next 'larger' Java type
in JDBCRDD#getCatalystType .
https://github.com/apache/spark/blob/517eb37a85e0a28820bcfd5d98c50d02df6521c6/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JDBCRDD.scala#L49

{code:title=q.sql}
create table t1 (id int unsigned);
insert into t1 values (4234567890);
{code}

{code:title=T1.scala}
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.sql.SQLContext

object T1 {
  def main(args: Array[String]) {
    val sc = new SparkContext(new SparkConf())
    val s = new SQLContext(sc)
    val url = ""jdbc:mysql://localhost/test""
    val t1 = s.jdbc(url, ""t1"")
    t1.printSchema()
    t1.collect().foreach(println)
  }
}
{code}

This code caused error like below.
{noformat}
15/05/18 11:39:51 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, xxx): com.mysql.jdbc.exceptions.jdbc4.MySQLDataException: '4.23456789E9' in column '1' is outside valid range for the datatype INTEGER.
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)
        at com.mysql.jdbc.Util.getInstance(Util.java:360)
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:963)
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:935)
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:924)
        at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:870)
        at com.mysql.jdbc.ResultSetImpl.throwRangeException(ResultSetImpl.java:7090)
        at com.mysql.jdbc.ResultSetImpl.parseIntAsDouble(ResultSetImpl.java:6364)
        at com.mysql.jdbc.ResultSetImpl.getInt(ResultSetImpl.java:2484)
        at org.apache.spark.sql.jdbc.JDBCRDD$$anon$1.getNext(JDBCRDD.scala:344)
        at org.apache.spark.sql.jdbc.JDBCRDD$$anon$1.hasNext(JDBCRDD.scala:399)
...
{noformat}",,apachespark,joehalliwell,rtreffer,teppeid,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7897,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 14:05:55 UTC 2015,,,,,,,,,,"0|i2eukn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/May/15 08:30;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6229;;;","27/May/15 09:52;rtreffer;I've had a similar problem, especially with ""unsigned bigint"". Java has no type for that. (It only fails if a value actually exceeds the java long range).

I worked around the problem by extending DriverQuirks, now JDBCDialects. The idea is that you can map problematic types to whatever you'd want:
https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/jdbc/JDBCSuite.scala#L395

I am mapping unsigned bigint to string in order to load it. This works at some post-processing overhead (basically a udf to map unsigned long stored in a string to signed long).;;;","27/May/15 14:05;viirya;[~treffer] Thanks for reporting the problem. I open another [ticket|https://issues.apache.org/jira/browse/SPARK-7897] and PR for that. I will use DecimalType for unsigned bigint. It would be better if you can test it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Use getOrElse for getting the threshold of LR model ,SPARK-7694,12830550,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,coderxiang,coderxiang,coderxiang,18/May/15 00:14,18/May/15 04:18,14/Jul/23 06:26,18/May/15 04:18,,,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,"The toString method of LogisticRegressionModel calls get method on an Option (threshold) without a safeguard. In spark-shell, the following code 
{code:title=lbfgs.scala|borderStyle=solid}
val model = algorithm.run(data).clearThreshold()
{code}
 in lbfgs code will fail as toString method will be called right after clearThreshold() to show the results in the REPL.",,apachespark,coderxiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 00:15:06 UTC 2015,,,,,,,,,,"0|i2eujb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"18/May/15 00:15;apachespark;User 'coderxiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/6224;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame.describe() should cast all aggregates to String,SPARK-7687,12830480,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,17/May/15 08:53,19/May/15 04:54,14/Jul/23 06:26,19/May/15 04:54,1.3.1,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"In DataFrame.describe(), the count aggregate produces an integer, the avg and stdev aggregates produce doubles, and min and max aggregates can produce varying types depending on what type of column they're applied to. As a result, we should cast all aggregate results to String so that describe()'s output types match its declared output schema.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 17 08:57:02 UTC 2015,,,,,,,,,,"0|i2eu3r:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"17/May/15 08:57;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6218;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Runnable DescribeCommand is assigned wrong physical plan output attributes in SparkStrategies ,SPARK-7686,12830479,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,17/May/15 08:19,17/May/15 19:00,14/Jul/23 06:26,17/May/15 19:00,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"In SparkStrategies, RunnableDescribeCommand is passed the output attributes of the table being described rather than the attributes of the describe command's output.  I noticed this when it caused problems in some UnsafeRow conversion code that I'm working on.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 17 08:25:11 UTC 2015,,,,,,,,,,"0|i2eu3j:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"17/May/15 08:25;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6217;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
TestHive.reset complains Database does not exist: default,SPARK-7684,12830462,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,yhuai,yhuai,17/May/15 03:05,28/May/15 14:37,14/Jul/23 06:26,28/May/15 11:28,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"To see the error, try {{test-only org.apache.spark.sql.hive.MetastoreDataSourcesSuite}}. You will see
{code}
19:23:30.487 ERROR org.apache.spark.sql.hive.test.TestHive: FATAL ERROR: Failed to reset TestDB state.
org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database does not exist: default
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:333)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$runHive$1.apply(ClientWrapper.scala:310)
	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:139)
	at org.apache.spark.sql.hive.client.ClientWrapper.runHive(ClientWrapper.scala:310)
	at org.apache.spark.sql.hive.client.ClientWrapper.runSqlHive(ClientWrapper.scala:300)
	at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:425)
	at org.apache.spark.sql.hive.test.TestHiveContext.runSqlHive(TestHive.scala:94)
	at org.apache.spark.sql.hive.test.TestHiveContext.reset(TestHive.scala:433)
	at org.apache.spark.sql.hive.MetastoreDataSourcesSuite.afterEach(MetastoreDataSourcesSuite.scala:43)
	at org.scalatest.BeforeAndAfterEach$class.afterEach(BeforeAndAfterEach.scala:205)
	at org.apache.spark.sql.hive.MetastoreDataSourcesSuite.afterEach(MetastoreDataSourcesSuite.scala:40)
	at org.scalatest.BeforeAndAfterEach$class.afterEach(BeforeAndAfterEach.scala:220)
	at org.apache.spark.sql.hive.MetastoreDataSourcesSuite.afterEach(MetastoreDataSourcesSuite.scala:40)
	at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:264)
	at org.apache.spark.sql.hive.MetastoreDataSourcesSuite.runTest(MetastoreDataSourcesSuite.scala:40)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.scalatest.FunSuite.run(FunSuite.scala:1555)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

{code}",,apachespark,chenghao,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 14:37:01 UTC 2015,,,,,,,,,,"0|i2etzz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/15 03:05;yhuai;cc [~lian cheng] [~chenghao];;;","19/May/15 09:39;lian cheng;This looks quite similar to this one https://github.com/apache/spark/pull/2352#issuecomment-55440621;;;","22/May/15 10:44;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6353;;;","22/May/15 18:17;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6359;;;","22/May/15 18:18;lian cheng;A minimum test suite that reproduces this issue:
{code}
class ResetSuite extends QueryTest with BeforeAndAfterEach {
  import TestHive._
  override protected def afterEach(): Unit = {
    reset()
  }
  test(""foo"") {
    sql(""CREATE TABLE xxx (foo INT)"")
    catalog.lookupRelation(""xxx"" :: Nil)
  }
}
{code};;;","25/May/15 16:17;lian cheng;Issue resolved by pull request 6359
[https://github.com/apache/spark/pull/6359];;;","27/May/15 20:11;yhuai;I have merged the workaround. Let's keep this one open. So, we can continue the investigation of the real cause. Since it is not quite clear the real cause. I will drop the target version for now. ;;;","28/May/15 11:26;lian cheng;By ""workaround"" did you mean PR [#6353|https://github.com/apache/spark/pull/6353]? Actually PR [#6359|https://github.com/apache/spark/pull/6359] figured out and fixed this issue ({{HiveContext.newTemporaryConfiguration()}} keeps creating new empty temporary metastore). The workaround PR was created before #6359, but merged a few days later, which is a bit confusing.

I'm resolving this.;;;","28/May/15 14:37;yhuai;Yeah, I missed that. Thanks!;;;",,,,,,,,,,,,,,,,,,,,
DataSourceStrategy's buildPartitionedTableScan always list file status for all data files ,SPARK-7673,12830278,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,yhuai,yhuai,15/May/15 19:37,30/Oct/15 09:40,14/Jul/23 06:26,18/May/15 19:51,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,See https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/DataSourceStrategy.scala#L134-141,,apachespark,joshrosen,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 19:51:43 UTC 2015,,,,,,,,,,"0|i2esvz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"15/May/15 19:39;yhuai;This cause pretty significant performance regression for partitioned parquet tables. table().explain() can take 80s for a table stored in s3 with around 2000 partitions.;;;","18/May/15 01:50;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6225;;;","18/May/15 19:51;yhuai;Issue has been addressed by https://github.com/apache/spark/commit/9dadf019b93038e1e18336ccd06c5eecb4bae32f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Number format exception with spark.kryoserializer.buffer.mb,SPARK-7672,12830268,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,nravi,nravi,nravi,15/May/15 19:10,16/May/15 07:25,14/Jul/23 06:26,16/May/15 07:24,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"With spark.kryoserializer.buffer.mb  1000 : 

Exception in thread ""main"" java.lang.NumberFormatException: Size must be specified as bytes (b), kibibytes (k), mebibytes (m), gibibytes (g), tebibytes (t), or pebibytes(p). E.g. 50b, 100k, or 250m.
Fractional values are not supported. Input was: 1000000.0
        at org.apache.spark.network.util.JavaUtils.parseByteString(JavaUtils.java:238)
        at org.apache.spark.network.util.JavaUtils.byteStringAsKb(JavaUtils.java:259)
        at org.apache.spark.util.Utils$.byteStringAsKb(Utils.scala:1037)
        at org.apache.spark.SparkConf.getSizeAsKb(SparkConf.scala:245)
        at org.apache.spark.serializer.KryoSerializer.<init>(KryoSerializer.scala:53)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.spark.SparkEnv$.instantiateClass$1(SparkEnv.scala:269)
        at org.apache.spark.SparkEnv$.instantiateClassFromConf$1(SparkEnv.scala:280)
        at org.apache.spark.SparkEnv$.create(SparkEnv.scala:283)
        at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:188)
        at org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:267)
",,apachespark,nravi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 16 07:24:50 UTC 2015,,,,,,,,,,"0|i2estr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/15 19:11;nravi;In translating deprecated spark.kryoserializer.buffer.mb to spark.kryoserializer.buffer, double conversion would lead to this problem.;;;","15/May/15 19:15;apachespark;User 'nishkamravi2' has created a pull request for this issue:
https://github.com/apache/spark/pull/6198;;;","16/May/15 07:24;srowen;Issue resolved by pull request 6198
[https://github.com/apache/spark/pull/6198];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Builds against Hadoop 2.6+ get inconsistent curator dependencies,SPARK-7669,12830190,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,15/May/15 15:05,17/May/15 16:03,14/Jul/23 06:26,17/May/15 16:03,1.3.1,1.4.0,,,,,1.4.0,,,,,,Build,,,,0,,,,,,"If you build spark against Hadoop 2.6 you end up with an inconsistent set of curator dependencies -curator-recipe 2.4.0 with curator 2.6.0.

A dedicated hadoop-2.6 profile along with extraction of curator version into a property can keep the curator versions in sync, along with ZK.",Hadoop 2.6,apachespark,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 17 16:03:31 UTC 2015,,,,,,,,,,"0|i2escv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/15 15:12;stevel@apache.org;snippet of the maven dependencies with {{-Phadoop-2.4 -Dhadoop.version=2.6.0 }}

 {code}
 [INFO] |  |  |  +- org.apache.curator:curator-client:jar:2.6.0:compile
 [INFO] |  |  |  |  +- (org.slf4j:slf4j-api:jar:1.7.10:compile - version managed from 1.7.6; omitted for duplicate)
 [INFO] |  |  |  |  +- (org.apache.zookeeper:zookeeper:jar:3.4.5:compile - version managed from 3.4.6; omitted for duplicate)
 [INFO] |  |  |  |  \- (com.google.guava:guava:jar:14.0.1:provided - version managed from 11.0.2; scope managed from compile; omitted for duplicate)
 [INFO] |  |  |  +- (org.apache.curator:curator-recipes:jar:2.4.0:compile - version managed from 2.6.0; omitted for duplicate)
 {code}
 
What's happened is that the curator-recipes version is being set by spark, but no version of curator-client is set. Nor can you fix the versions on the CLI, as the curator version isn't yet property driven.

I have a patch to make things consistent, with a hadoop 2.6 profile which sets curator.version=2.6,0,  zookeeper.version=3.4.6, and drives the curator-recipes version off that curator.version property. There's one possible enhancement to this: declare the version of curator-client &c directly, putting the spark build in control of which version gets picked up.;;;","15/May/15 15:16;srowen;Yeah I'm familiar with this flavor of problem, and it's a real pain.
Can we just set the version for both of these artifacts rather than make a profile, or does the curator version need to vary with Hadoop version? if the latter, yes, a new profile is OK.;;;","15/May/15 15:43;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/6191;;;","17/May/15 16:03;srowen;Issue resolved by pull request 6191
[https://github.com/apache/spark/pull/6191];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Matrix.map should preserve transpose property,SPARK-7668,12830184,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,15/May/15 14:41,15/May/15 17:04,14/Jul/23 06:26,15/May/15 17:04,1.3.1,1.4.0,,,,,1.3.2,1.4.0,,,,,MLlib,,,,0,,,,,,Currently calling map on both DenseMatrix and SparseMatrix will throw original transpose property away. It should be preserved.,,apachespark,mengxr,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 17:04:10 UTC 2015,,,,,,,,,,"0|i2esbj:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"15/May/15 14:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6188;;;","15/May/15 17:04;mengxr;Issue resolved by pull request 6188
[https://github.com/apache/spark/pull/6188];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
DAG visualization: Fix incorrect link paths of DAG.,SPARK-7664,12830139,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,15/May/15 10:53,15/May/15 18:55,14/Jul/23 06:26,15/May/15 18:55,1.4.0,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"In JobPage, we can jump a StagePage when we click corresponding box of DAG viz but the link path is incorrect.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 10:57:02 UTC 2015,,,,,,,,,,"0|i2es1j:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"15/May/15 10:57;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/6184;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception of multi-attribute generator anlysis in projection,SPARK-7662,12830091,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,chenghao,chenghao,chenghao,15/May/15 07:11,21/May/15 21:25,14/Jul/23 06:26,19/May/15 22:21,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"{code}
select explode(map(value, key)) from src;
{code}

It throws exception like
{panel}
org.apache.spark.sql.AnalysisException: The number of aliases supplied in the AS clause does not match the number of columns output by the UDTF expected 2 aliases but got _c0 ;
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:38)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:43)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveGenerate$$makeGeneratorOutput(Analyzer.scala:605)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$$anonfun$apply$16$$anonfun$22.apply(Analyzer.scala:562)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$$anonfun$apply$16$$anonfun$22.apply(Analyzer.scala:548)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$$anonfun$apply$16.applyOrElse(Analyzer.scala:548)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$$anonfun$apply$16.applyOrElse(Analyzer.scala:538)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)
{panel}",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 22:21:00 UTC 2015,,,,,,,,,,"0|i2erqv:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"15/May/15 07:18;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/6178;;;","19/May/15 22:21;marmbrus;Issue resolved by pull request 6178
[https://github.com/apache/spark/pull/6178];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Snappy-java buffer-sharing bug leads to data corruption / test failures,SPARK-7660,12830079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,15/May/15 06:05,17/May/15 16:42,14/Jul/23 06:26,17/May/15 16:42,1.4.0,,,,,,1.2.3,1.3.2,1.4.0,,,,Shuffle,Spark Core,,,0,,,,,,"snappy-java contains a bug that can lead to situations where separate SnappyOutputStream instances end up sharing the same input and output buffers, which can lead to data corruption issues.  See https://github.com/xerial/snappy-java/issues/107 for my upstream bug report and https://github.com/xerial/snappy-java/pull/108 for my patch to fix this issue.

I discovered this issue because the buffer-sharing was leading to a test failure in JavaAPISuite: one of the repartition-and-sort tests was returning the wrong answer because both tasks wrote their output using the same compression buffers and one task won the race, causing its output to be written to both shuffle output files. As a result, the test returned the result of collecting one partition twice (see https://github.com/apache/spark/pull/5868#issuecomment-101954962 for more details).

The buffer-sharing can only occur if {{close()}} is called twice on the same SnappyOutputStream _and_ the JVM experiences little GC / memory pressure (for a more precise description of when this issue may occur, see my upstream tickets).  I think that this double-close happens somewhere in some test code that was added as part of my Tungsten shuffle patch, exposing this bug (to see this, download a recent build of master and run https://gist.github.com/JoshRosen/eb3257a75c16597d769f locally in order to force the test execution order that triggers the bug).

I think that it's rare that this bug would lead to silent failures like this. In more realistic workloads that aren't writing only a handful of bytes per task, I would expect this issue to lead to stream corruption issues like SPARK-4105.",,amcelwee,apachespark,joshrosen,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4105,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 17 16:42:40 UTC 2015,,,,,,,,,,"0|i2ero7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"15/May/15 06:06;joshrosen;Note that this affects more than just Spark 1.4.0; I'll trace back and figure out the complete list of affected versions tomorrow, but I think that any version that relied on a Snappy-java library published after mid June or July 2014 may be affected.;;;","15/May/15 06:20;joshrosen;I pushed https://github.com/apache/spark/commit/7da33ce5057ff965eec19ce662465b64a3564019 as a hotfix, which masks the bug in a way that fixes the JavaAPISuite Jenkins failures.  We'll still fix this bug before 1.4, but in the meantime this will make it easy to recognize new Jenkins failures.;;;","15/May/15 06:31;joshrosen;If we're wary of upgrading to a new Snappy version and don't want to wait for a new release / backport, one option is to just wrap SnappyOutputStream with our own code to make close() idempotent.  I don't think that this will have any significant overhead if done right, since the JIT should be able to inline the SnappyOutputStream calls.;;;","15/May/15 06:48;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6176;;;","17/May/15 16:42;joshrosen;Fixed by my workaround PR in 1.2.3, 1.3.2, and 1.4.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Akka timeout exception from ask and table broadcast,SPARK-7655,12830044,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zsxwing,yhuai,yhuai,15/May/15 01:26,24/May/15 04:40,14/Jul/23 06:26,16/May/15 07:44,1.4.0,,,,,,1.4.0,,,,,,Scheduler,Spark Core,,,0,,,,,,"I got the following exception when I was running a query with broadcast join.
{code}
15/05/15 01:15:49 [WARN] AkkaRpcEndpointRef: Error sending message [message = UpdateBlockInfo(BlockManagerId(driver, 10.0.171.162, 54870),broadcast_758_piece0,StorageLevel(false, false, false, false, 1),0,0,0)] in 1 attempts
java.util.concurrent.TimeoutException: Futures timed out after [120 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:107)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.storage.BlockManagerMaster.updateBlockInfo(BlockManagerMaster.scala:58)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$tryToReportBlockStatus(BlockManager.scala:374)
	at org.apache.spark.storage.BlockManager.reportBlockStatus(BlockManager.scala:350)
	at org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1107)
	at org.apache.spark.storage.BlockManager$$anonfun$removeBroadcast$2.apply(BlockManager.scala:1083)
	at org.apache.spark.storage.BlockManager$$anonfun$removeBroadcast$2.apply(BlockManager.scala:1083)
	at scala.collection.immutable.Set$Set2.foreach(Set.scala:94)
	at org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1083)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4.apply$mcI$sp(BlockManagerSlaveEndpoint.scala:65)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4.apply(BlockManagerSlaveEndpoint.scala:65)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$4.apply(BlockManagerSlaveEndpoint.scala:65)
	at org.apache.spark.storage.BlockManagerSlaveEndpoint$$anonfun$1.apply(BlockManagerSlaveEndpoint.scala:78)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,nrstott,yhuai,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 21:32:08 UTC 2015,,,,,,,,,,"0|i2ergf:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"15/May/15 18:26;zsxwing;Found the following stack track that a thread holding the TaskSchedulerImpl lock may block Akka threads:

{code}
""task-result-getter-1"" daemon prio=10 tid=0x00007f460400b000 nid=0x3b73 runnable [0x00007f45a70ee000]

   java.lang.Thread.State: RUNNABLE

        at org.apache.spark.util.ByteBufferInputStream.read(ByteBufferInputStream.scala:38)

        at java.io.ObjectInputStream$PeekInputStream.peek(ObjectInputStream.java:2293)

        at java.io.ObjectInputStream$BlockDataInputStream.peek(ObjectInputStream.java:2586)

        at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2596)

        at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1505)

        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)

        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)

        at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706)

        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344)

        at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)

        at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)

        at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)

        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)

        at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1706)

        at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1344)

        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)

        at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)

        at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:89)

        at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)

        at org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:624)

        at org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:378)

        - locked <0x0000000270cddd18> (a org.apache.spark.scheduler.TaskSchedulerImpl)

        at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:82)

        at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:51)

        at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:51)

        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1769)

        at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:50)

        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

        at java.lang.Thread.run(Thread.java:745)
{code}

And two Akka threads are waiting for the TaskSchedulerImpl lock.

We should not call {{DirectTaskResult.value}} when holding the TaskSchedulerImpl lock. It may cost dozens of seconds to deserialize a large object.

I will move it out of the lock.

Thank [~yhuai] for the stack track.;;;","15/May/15 18:28;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6195;;;","15/May/15 21:32;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6200;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in naive Bayes prediction,SPARK-7652,12830027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,mengxr,mengxr,15/May/15 00:16,19/May/15 20:53,14/Jul/23 06:26,19/May/15 20:53,1.4.0,,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,Saw some performance regression in naive Bayes prediction. Since the multinomial code hasn't been changed since 1.3. I guess the root cause may be the breeze upgrade. So we may consider updating the implementation in prediction as well.,,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7612,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 20:53:28 UTC 2015,,,,,,,,,,"0|i2ercn:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"15/May/15 14:53;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6189;;;","19/May/15 20:53;mengxr;Issue resolved by pull request 6189
[https://github.com/apache/spark/pull/6189];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"PySpark GMM predict, predictSoft should fail on bad input",SPARK-7651,12830009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,MeethuMathew,josephkb,josephkb,14/May/15 23:00,15/May/15 21:53,14/Jul/23 06:26,15/May/15 17:51,1.3.0,1.3.1,1.4.0,,,,1.3.2,1.4.0,,,,,MLlib,PySpark,,,0,,,,,,"In PySpark, GaussianMixtureModel predict and predictSoft test if the argument is an RDD and operate correctly if so.  But if the argument is not an RDD, they fail silently, returning nothing.

[https://github.com/apache/spark/blob/11a1a135d1fe892cd48a9116acc7554846aed84c/python/pyspark/mllib/clustering.py#L176]

Instead, they should raise errors.",,apachespark,josephkb,MeethuMathew,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 08:09:05 UTC 2015,,,,,,,,,,"0|i2er8n:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"14/May/15 23:17;josephkb;Ping [~MeethuMathew]: Could you please make this fix?  It should be quick.  Thank you!;;;","15/May/15 04:31;MeethuMathew;[~josephkb] Yea, I wil fix it asap.;;;","15/May/15 04:50;josephkb;Great, thank you!;;;","15/May/15 04:53;MeethuMathew;Could you please tell me where I should make the changes? In master branch or 1.3.0?;;;","15/May/15 04:58;josephkb;Please base your PR on master.  I can then merge it with master + the other branches.;;;","15/May/15 04:59;MeethuMathew;Ok thank you;;;","15/May/15 04:59;MeethuMathew;Ok thank you;;;","15/May/15 08:09;apachespark;User 'FlytxtRnD' has created a pull request for this issue:
https://github.com/apache/spark/pull/6180;;;",,,,,,,,,,,,,,,,,,,,,
Number of executors and partitions are displayed wrongly in storage tab,SPARK-7643,12829938,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,14/May/15 18:48,14/May/15 23:59,14/Jul/23 06:26,14/May/15 23:59,1.4.0,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"Saw this in the storage tab of an RDD on a 1.4 cluster. An RDD is distributed among many executors, but the web UI says ""1 Executors"". Same the the partitions.
",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/15 18:48;mengxr;1.4 data distribution.png;https://issues.apache.org/jira/secure/attachment/12732923/1.4+data+distribution.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 22:23:03 UTC 2015,,,,,,,,,,"0|i2eqtb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"14/May/15 22:23;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/6157;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContextSchedulerCreationSuite tests may fail due to unrecognized UnsatisfiedLinkError message.,SPARK-7635,12829843,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tellison,mtbrandy,mtbrandy,14/May/15 12:52,14/May/15 13:30,14/Jul/23 06:26,14/May/15 13:05,1.3.1,,,,,,1.5.0,,,,,,Spark Core,,,,0,test,,,,,"When mesos is not available, these tests fail due to the difference in the UnsatisfiedLinkError message with IBM Java vs OpenJDK:

- mesos fine-grained *** FAILED ***
  ""mesos (Not found in java.library.path)"" did not contain ""no mesos in"" (SparkContextSchedulerCreationSuite.scala:162)
- mesos coarse-grained *** FAILED ***
  ""mesos (Not found in java.library.path)"" did not contain ""no mesos in"" (SparkContextSchedulerCreationSuite.scala:162)
- mesos with zookeeper *** FAILED ***
  ""mesos (Not found in java.library.path)"" did not contain ""no mesos in"" (SparkContextSchedulerCreationSuite.scala:162)

PR to be submitted shortly.",IBM Java,mtbrandy,t.p.ellison@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 13:30:03 UTC 2015,,,,,,,,,,"0|i2eq87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/15 13:05;srowen;I think that was literally fixed yesterday by [~tellison]'s commit: https://github.com/apache/spark/commit/51030b8a9d4f3feb7a5d2249cc867fd6a06f0336

Coincidence?

I'm using this as its JIRA, retroactively.;;;","14/May/15 13:30;t.p.ellison@gmail.com;Yep, a happy coincidence!

Full disclosure: Matt and I both work for IBM, but we have never spoken.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
treenode argString should not print children,SPARK-7631,12829808,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,scwf,scwf,scwf,14/May/15 09:32,21/May/15 21:24,14/Jul/23 06:26,18/May/15 19:05,1.3.1,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"spark-sql> explain extended                                                         
         > select * from (                                                          
         > select key from src union all                                            
         > select key from src) t;        

the spark plan will print children in argString		 
		 		 
== Physical Plan ==
Union[ HiveTableScan [key#1], (MetastoreRelation default, src, None), None,
 HiveTableScan [key#3], (MetastoreRelation default, src, None), None]
 HiveTableScan [key#1], (MetastoreRelation default, src, None), None
 HiveTableScan [key#3], (MetastoreRelation default, src, None), None",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 19:05:44 UTC 2015,,,,,,,,,,"0|i2eq0f:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"14/May/15 09:35;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/6144;;;","18/May/15 19:05;marmbrus;Issue resolved by pull request 6144
[https://github.com/apache/spark/pull/6144];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Actions on DataFrame created from HIVE table with newly added column throw NPE ,SPARK-7626,12829767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,zguo,zguo,14/May/15 03:51,07/Oct/16 23:15,14/Jul/23 06:26,07/Oct/16 23:15,1.3.1,,,,,,,,,,,,SQL,,,,0,,,,,,"We recently added a new column page_context to a hive table named ""clicks"", partitioned by data_date.  This leads to NPE being thrown on DataFrame created on older partitions without this column populated. For example:

{code}
val hc = new HiveContext(sc)
val clk = hc.sql(""select * from clicks where data_date=20150302"")
clk.show()
{code}

throws the following error msg:

{code}
java.lang.RuntimeException: cannot find field page_context from [0:log_format_number, .....]
        at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:415)
        at org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.getStructFieldRef(LazySimpleStructObjectInspector.java:173)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$12.apply(TableReader.scala:278)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$12.apply(TableReader.scala:277)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.sql.hive.HadoopTableReader$.fillObject(TableReader.scala:277)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$4$$anonfun$9.apply(TableReader.scala:194)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$4$$anonfun$9.apply(TableReader.scala:188)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
{code}",,apachespark,viirya,zguo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 20 17:30:41 UTC 2015,,,,,,,,,,"0|i2eprb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/15 10:17;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6146;;;","14/May/15 10:24;viirya;I created a PR for this ticket because the code looks problematic. However, I can't reproduce this error when writing test for it.

I simply create a table.
{code}CREATE TABLE table_with_partition(key int, value string) PARTITIONED by (ds string) {code}

Then, insert some test data into a partition.
{code}INSERT OVERWRITE TABLE table_with_partition partition (ds='1') SELECT key, value FROM testData{code}

Add new column to the table:
{code}ALTER TABLE table_with_partition ADD COLUMNS (value2 string){code}

Finally, query the partition:
{code}SELECT * FROM table_with_partition WHERE ds = '1'{code}

Looks like the partition has recognized the newly added column value2.

Do you have any idea about it or can you provide more information?;;;","20/Jun/15 17:30;viirya;By some testing and searching codes, I just found that this problem is solved in another PR. The reason is described in this PR (#6146). So I think this ticket can be closed now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Task scheduler delay is increasing time over time in spark local mode,SPARK-7624,12829759,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,jhu,jhu,14/May/15 03:13,22/May/15 23:03,14/Jul/23 06:26,22/May/15 23:03,1.3.1,,,,,,1.3.2,1.4.0,,,,,Spark Core,,,,0,delay,schedule,,,,"I am running a simple spark streaming program with spark 1.3.1 in local mode, it receives json string from a socket with rate 50 events per second, it can run well in first 6 hours (although the minor gc count per minute is increasing all the time), after that, i can see that the scheduler delay in every task is significant increased from 10 ms to 100 ms, after 10 hours running, the task delay is about 800 ms and cpu is also increased from 2% to 30%. This causes the steaming job can not finish in one batch interval (5 seconds). I dumped the java memory after 16 hours and can see there are about 200000 {{org.apache.spark.scheduler.local.ReviveOffers}} objects in {{akka.actor.LightArrayRevolverScheduler$TaskQueue[]}}. Then i checked the code and see only one place may put the {{ReviveOffers}} to akka {{LightArrayRevolverScheduler}}: the {{LocalActor::reviveOffers}}
{code}
 def reviveOffers() {
    val offers = Seq(new WorkerOffer(localExecutorId, localExecutorHostname, freeCores))
    val tasks = scheduler.resourceOffers(offers).flatten
    for (task <- tasks) {
      freeCores -= scheduler.CPUS_PER_TASK
      executor.launchTask(executorBackend, taskId = task.taskId, attemptNumber = task.attemptNumber,
        task.name, task.serializedTask)
    }

    if (tasks.isEmpty && scheduler.activeTaskSets.nonEmpty) {
      // Try to reviveOffer after 1 second, because scheduler may wait for locality timeout
      context.system.scheduler.scheduleOnce(1000 millis, self, ReviveOffers)
    }
}
{code}

I removed the last three lines in this method (the whole {{if}} block, which is introduced from https://issues.apache.org/jira/browse/SPARK-4939), it worked smooth after 20 hours running, the scheduler delay is about 10 ms all the time. So there should have some conditions that the ReviveOffers will be duplicate scheduled? I am not sure why this happens, but i feel that this is the root cause of this issue. 

My spark settings:
#  Memor: 3G
# CPU: 8 cores 
# Streaming Batch interval: 5 seconds.  

Here are my streaming code:
{code}
val input = ssc.socketTextStream(
      hostname, port, StorageLevel.MEMORY_ONLY_SER).mapPartitions(
      /// parse the json to Order
      Order(_), preservePartitioning = true)
val mresult = input.map(
      v => (v.customer, UserSpending(v.customer, v.count * v.price, v.timestamp.toLong))).cache()
val tempr  = mresult.window(
            Seconds(firstStageWindowSize), 
            Seconds(firstStageWindowSize)
          ).transform(
            rdd => rdd.union(rdd).union(rdd).union(rdd)
          )
tempr.count.print
tempr.cache().foreachRDD((rdd, t) => {
            for (i <- 1 to 5) {
              val c = rdd.filter(x=>scala.util.Random.nextInt(5) == i).count()
              println(""""""T: """""" + t + """""": """""" + c)
            }
          })
{code}

========================================================
Updated at 2015-05-15
I did print some detail schedule times of the suspect lines in {{LocalActor::reviveOffers}}: {color:red}*1685343501*{color} times after 18 hours running.",,apachespark,davies,jhu,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 23:03:15 UTC 2015,,,,,,,,,,"0|i2eppj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/15 07:31;srowen;[~davies] thoughts on the impact of those few lines?;;;","15/May/15 05:36;davies;This is introduced by https://github.com/apache/spark/pull/4147, I will revert it.;;;","15/May/15 05:47;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6172;;;","15/May/15 05:58;davies;In the context of Spark Streaming, there could be some long running tasks (receivers), after the patch [1], LocalBackend will schedule another ReviveOffer if no task could be scheduled, becoming infinite loop. And, each new stage could introduce a new ReviveOffer infinite loop, the scheduler will become slower and slower.

It's not easy to tell when we should schedule another ReviveOffer or not, so I'd like to revert this patch, because the original problem is already resolved by https://github.com/apache/spark/pull/3779

[1] https://github.com/apache/spark/pull/4147;;;","18/May/15 22:07;joshrosen;Does this only affect local mode, or does it also affect cluster modes?;;;","18/May/15 23:57;joshrosen;I've merged Davies' patch for 1.4 but we still need a 1.3.2 backport.;;;","21/May/15 23:35;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6337;;;","22/May/15 23:03;davies;Had merged into 1.3 branch. ;;;",,,,,,,,,,,,,,,,,,,,,
autodoc_docstring_signature doesn't work for some classes/methods under spark.ml,SPARK-7619,12829706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,13/May/15 22:22,15/May/15 01:17,14/Jul/23 06:26,15/May/15 01:17,1.4.0,,,,,,1.4.0,,,,,,ML,PySpark,,,0,,,,,,"We use autodoc_docstring_signature to handle signature for methods wrapped by keyword_only. However, it only works for some methods but not all. Need to look into this issue.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 01:17:12 UTC 2015,,,,,,,,,,"0|i2epdr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"14/May/15 22:53;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/6161;;;","15/May/15 01:17;mengxr;Issue resolved by pull request 6161
[https://github.com/apache/spark/pull/6161];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Column order can be corrupted when saving DataFrame as a partitioned table,SPARK-7616,12829689,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,yhuai,yhuai,13/May/15 21:17,21/May/15 20:52,14/Jul/23 06:26,21/May/15 20:52,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"When saved as a partitioned table, partition columns of a DataFrame are appended after data columns. However, column names are not adjusted accordingly.
{code}
import sqlContext._
import sqlContext.implicits._

val df = (1 to 3).map(i => i -> i * 2).toDF(""a"", ""b"")

df.write
  .format(""parquet"")
  .mode(""overwrite"")
  .partitionBy(""a"")
  .saveAsTable(""t"")

table(""t"").orderBy('a).show()
{code}
Expected output:
{noformat}
+-+-+
|b|a|
+-+-+
|2|1|
|4|2|
|6|3|
+-+-+
{noformat}
Actual output:
{noformat}
+-+-+
|b|a|
+-+-+
|1|2|
|2|4|
|3|6|
+-+-+
{noformat}",,apachespark,huangjs,lian cheng,maropu,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 20:52:26 UTC 2015,,,,,,,,,,"0|i2epa7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"21/May/15 16:37;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6285;;;","21/May/15 20:52;yhuai;Issue resolved by pull request 6285
[https://github.com/apache/spark/pull/6285];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
MLLIB Word2Vec wordVectors divided by Euclidean Norm equals to zero ,SPARK-7615,12829688,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,ezli,ezli,13/May/15 21:16,12/Jan/16 13:28,14/Jul/23 06:26,12/Jan/16 13:28,1.3.1,,,,,,1.6.1,2.0.0,,,,,MLlib,,,,0,,,,,,"In Word2VecModel, wordVecNorms may contains Euclidean Norm equals to zero. This will cause incorrect calculation for cosine distance. when you do cosineVec(ind) / wordVecNorms(ind). Cosine distance should be equal to 0 for norm = 0. ",,amartgon,apachespark,ezli,josephkb,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 12 13:28:25 UTC 2016,,,,,,,,,,"0|i2ep9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/15 16:40;amartgon;Hi, is any body working on this (and SPARK-7617, SPARK-7618)? I have been using Spark for some time, and would like get started contributing. This looks like a trivial issue (good fo a newbie).

Shall I provide a pull request for this? Would it make sense a pull request for the three related issues?

;;;","14/May/15 16:43;srowen;Yes the submitter is about to open another PR, as I understand it. ;;;","18/May/15 21:05;apachespark;User 'ezli' has created a pull request for this issue:
https://github.com/apache/spark/pull/6245;;;","11/Jan/16 10:51;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/10696;;;","12/Jan/16 13:28;srowen;Resolved by https://github.com/apache/spark/pull/10696;;;",,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in RDDOperationGraphListener,SPARK-7608,12829606,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,zsxwing,zsxwing,13/May/15 17:19,22/May/15 20:38,14/Jul/23 06:26,22/May/15 20:38,1.4.0,,,,,,1.4.0,,,,,,Spark Core,Web UI,,,0,,,,,,"{{RDDOperationGraphListener}} does not clean {{jobIdToStageIds}}, which leaks a lot of memory for a long-time application.",,apachespark,joshrosen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 20:38:05 UTC 2015,,,,,,,,,,"0|i2eouf:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"13/May/15 17:19;zsxwing;[~andrewor14] could you take a look at this one?;;;","13/May/15 18:32;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6125;;;","22/May/15 20:38;joshrosen;Whoops, looks like we forgot to resolve this one on merge.  I'm marking it as fixed in 1.4.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support Insert into JDBC Datasource,SPARK-7601,12829566,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gvramana,gvramana,gvramana,13/May/15 14:23,16/May/15 11:08,14/Jul/23 06:26,14/May/15 00:24,1.3.1,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Support Insert into JDBCDataSource. Following are usage examples
{code}
sqlContext.sql(
      s""""""
        |CREATE TEMPORARY TABLE testram1
        |USING org.apache.spark.sql.jdbc
        |OPTIONS (url '$url', dbtable 'testram1', user 'xx', password 'xx', driver 'com.h2.Driver')
      """""".stripMargin.replaceAll(""\n"", "" ""))

sqlContext.sql(""insert into table testram1 select * from testsrc"").show
{code}",,apachespark,gvramana,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 00:24:55 UTC 2015,,,,,,,,,,"0|i2eolj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/15 14:31;apachespark;User 'gvramana' has created a pull request for this issue:
https://github.com/apache/spark/pull/6121;;;","14/May/15 00:24;marmbrus;Issue resolved by pull request 6121
[https://github.com/apache/spark/pull/6121];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't restrict customized FSBasedRelation OutputCommitter to be subclass of FileOutputFormat,SPARK-7599,12829514,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,13/May/15 10:31,13/May/15 14:36,14/Jul/23 06:26,13/May/15 14:36,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Used {{FileOutputCommitter}} here because we need to retrieve the actual path of file being written, which is returned by {{FileOutputCommitter.getWorkPath}}. This implies customized output committers must be subclasses of {{FileOutputCommitter}}, which was true for {{DirectParquetOutputCommitter}}. But this restriction is too strict. Should resort to {{OutputCommitter}} rather than {{FileOutputCommitter}}.",,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 14:36:20 UTC 2015,,,,,,,,,,"0|i2eo9r:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"13/May/15 14:31;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6118;;;","13/May/15 14:36;yhuai;Issue resolved by pull request 6118
[https://github.com/apache/spark/pull/6118];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Window will cause resolve failed with self join,SPARK-7595,12829472,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Sephiroth-Lin,Sephiroth-Lin,Sephiroth-Lin,13/May/15 06:55,16/May/15 11:07,14/Jul/23 06:26,14/May/15 07:23,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"for example:
table: src(key string, value string)
sql: with v1 as(select key, count(value) over (partition by key) cnt_val from src), v2 as(select v1.key, v1_lag.cnt_val from v1, v1 v1_lag where v1.key = v1_lag.key) select * from v2 limit 5;

then will analyze fail when resolving conflicting references in Join:
'Limit 5
 'Project [*]
  'Subquery v2
   'Project ['v1.key,'v1_lag.cnt_val]
    'Filter ('v1.key = 'v1_lag.key)
     'Join Inner, None
      Subquery v1
       Project [key#95,cnt_val#94L]
        Window [key#95,value#96], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount(value#96) WindowSpecDefinition [key#95], [], ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING AS cnt_val#94L], WindowSpecDefinition [key#95], [], ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
         Project [key#95,value#96]
          MetastoreRelation default, src, None
      Subquery v1_lag
       Subquery v1
        Project [key#97,cnt_val#94L]
         Window [key#97,value#98], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCount(value#98) WindowSpecDefinition [key#97], [], ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING AS cnt_val#94L], WindowSpecDefinition [key#97], [], ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
          Project [key#97,value#98]
           MetastoreRelation default, src, None

Conflicting attributes: cnt_val#94L",,apachespark,marmbrus,Sephiroth-Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 07:23:56 UTC 2015,,,,,,,,,,"0|i2eo0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/15 07:01;apachespark;User 'Sephiroth-Lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6114;;;","14/May/15 07:23;marmbrus;Issue resolved by pull request 6114
[https://github.com/apache/spark/pull/6114];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Resolution set to ""Pending Closed"" when using PR merge script",SPARK-7592,12829417,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,13/May/15 00:48,13/May/15 01:24,14/Jul/23 06:26,13/May/15 01:21,,,,,,,1.4.0,,,,,,Project Infra,,,,0,,,,,,"I noticed this was happening. The issue is that the behavior of the ASF JIRA silently changed. Now when the ""Resolve Issue"" transition occurs, the default resolution is ""Pending Closed"". We used to count on the default behavior being to set the resolution as ""Fixed"".

The solution is to explicitly set the resolution as ""Fixed"" and not count on default behavior.",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INFRA-9646,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 00:53:06 UTC 2015,,,,,,,,,,"0|i2eno7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"13/May/15 00:53;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/6103;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve error for binary expressions,SPARK-7569,12829279,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,12/May/15 18:39,12/May/15 20:37,14/Jul/23 06:26,12/May/15 20:37,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"This is not a great error:
{code}
scala> Seq((1,1)).toDF(""a"", ""b"").select(lit(1) + new java.sql.Date(1)) 
org.apache.spark.sql.AnalysisException: invalid expression (1 + 0) between Literal 1, IntegerType and Literal 0, DateType;
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 18:41:04 UTC 2015,,,,,,,,,,"0|i2emvj:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"12/May/15 18:41;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/6089;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ml.LogisticRegression doesn't output the right prediction,SPARK-7568,12829272,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dbtsai,mengxr,mengxr,12/May/15 18:33,14/May/15 08:26,14/Jul/23 06:26,14/May/15 08:26,1.4.0,,,,,,1.4.0,,,,,,ML,,,,0,,,,,,"`bin/spark-submit examples/src/main/python/ml/simple_text_classification_pipeline.py`

{code}
Row(id=4, text=u'spark i j k', words=[u'spark', u'i', u'j', u'k'], features=SparseVector(262144, {105: 1.0, 106: 1.0, 107: 1.0, 62173: 1.0}), rawPrediction=DenseVector([0.1629, -0.1629]), probability=DenseVector([0.5406, 0.4594]), prediction=0.0)
Row(id=5, text=u'l m n', words=[u'l', u'm', u'n'], features=SparseVector(262144, {108: 1.0, 109: 1.0, 110: 1.0}), rawPrediction=DenseVector([2.6407, -2.6407]), probability=DenseVector([0.9334, 0.0666]), prediction=0.0)
Row(id=6, text=u'mapreduce spark', words=[u'mapreduce', u'spark'], features=SparseVector(262144, {62173: 1.0, 140738: 1.0}), rawPrediction=DenseVector([1.2651, -1.2651]), probability=DenseVector([0.7799, 0.2201]), prediction=0.0)
Row(id=7, text=u'apache hadoop', words=[u'apache', u'hadoop'], features=SparseVector(262144, {128334: 1.0, 134181: 1.0}), rawPrediction=DenseVector([3.7429, -3.7429]), probability=DenseVector([0.9769, 0.0231]), prediction=0.0)
{code}

In Scala

{code}
$ bin/run-example ml.SimpleTextClassificationPipeline

(4, spark i j k) --> prob=[0.5406433544851436,0.45935664551485655], prediction=0.0
(5, l m n) --> prob=[0.9334382627383263,0.06656173726167364], prediction=0.0
(6, mapreduce spark) --> prob=[0.7799076868203896,0.22009231317961045], prediction=0.0
(7, apache hadoop) --> prob=[0.9768636139518304,0.023136386048169616], prediction=0.0
{code}

All predictions are 0, while some should be one based on the probability. It seems to be an issue with regularization.",,apachespark,dbtsai,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 08:26:25 UTC 2015,,,,,,,,,,"0|i2emtz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"13/May/15 04:34;dbtsai;This is because we regularize the intercept before which effectively regularizing less on the weights. Now, we follow the R's standard without regularizing the intercept, so we need to decrease the regularization. BTW, we need to implement efficient cross validation soon to help users to pick up lambda. 

with lambda = 0.001 in the current code, the result looks correct.
```
(4, spark i j k) --> prob=[0.1596407738787411,0.8403592261212589], prediction=1.0
(5, l m n) --> prob=[0.8378325685476612,0.16216743145233883], prediction=0.0
(6, mapreduce spark) --> prob=[0.6693126798261013,0.3306873201738986], prediction=0.0
(7, apache hadoop) --> prob=[0.9821575333444208,0.01784246665557917], prediction=0.0
```;;;","13/May/15 04:49;mengxr;The 3rd example still has prediction 0.0. Maybe we should try an even smaller regParam.;;;","13/May/15 05:08;dbtsai;Well, the third example is 0.0 in the old code.
```
(4, spark i j k) --> prob=[0.18764577263047177,0.8123542273695282], prediction=1.0
(5, l m n) --> prob=[0.6508848199790638,0.3491151800209362], prediction=0.0
(6, mapreduce spark) --> prob=[0.561585214970727,0.43841478502927295], prediction=0.0
(7, apache hadoop) --> prob=[0.9118076920593474,0.08819230794065264], prediction=0.0
```

Of course, I can tune lambda such that the third example is 1.0. ;;;","13/May/15 05:22;dbtsai;Actually, with lambda = 0.001, the training accuracy is perfect.

```
(0, a b c d e spark) --> prob=[0.0021342419881406746,0.9978657580118594], prediction=1.0
(1, b d) --> prob=[0.9959176174854043,0.004082382514595685], prediction=0.0
(2, spark f g h) --> prob=[0.0014541569986711233,0.9985458430013289], prediction=1.0
(3, hadoop mapreduce) --> prob=[0.9982978367343561,0.0017021632656438518], prediction=0.0
```;;;","13/May/15 05:26;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6109;;;","13/May/15 05:28;mengxr;Instance 6 still has prediction 0.0, which was 1.0 in Spark 1.3 (if I remember it correctly). Did you test Spark 1.3 or Spark 1.4 before elastic-net change?;;;","13/May/15 05:36;dbtsai;In 1.3, https://github.com/apache/spark/blob/branch-1.3/mllib/src/main/scala/org/apache/spark/ml/classification/LogisticRegression.scala
the intercept is false!

I just check that in 1.4 before I introduced new LOR, we changed the intercept to default of true. In this case, instance 6 will has prediction 0.0.

I confirmed that if we turn off the intercept and with the same regularization, the prob is the same as 1.3. Maybe we should just turn off the intercept since it's high dim problem, and intercept is not important most of time.;;;","13/May/15 05:39;mengxr;What is the default value in R for fitIntercept? Just checked LIBLINEAR and its default is false.;;;","13/May/15 05:42;dbtsai;Default for R is true as you can see in the unit-test R script.;;;","13/May/15 06:05;dbtsai;`fitIntercept = false`, or in Spark 1.3, the training likehood is lower compared with the one with `intercept`.

```
(0, a b c d e spark) --> prob=[0.08957171964937032,0.9104282803506297], prediction=1.0
(1, b d) --> prob=[0.8126200119520359,0.1873799880479641], prediction=0.0
(2, spark f g h) --> prob=[0.056941677968997864,0.9430583220310021], prediction=1.0
(3, hadoop mapreduce) --> prob=[0.8935982881732397,0.10640171182676032], prediction=0.0
```

;;;","13/May/15 06:11;mengxr;That is expected because intercept adds one degree of freedom and hence it is more likely to overfit. I would prefer disabling fitIntercept by default.;;;","14/May/15 08:26;mengxr;Issue resolved by pull request 6109
[https://github.com/apache/spark/pull/6109];;;",,,,,,,,,,,,,,,,,
Migrating Parquet data source to FSBasedRelation,SPARK-7567,12829271,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,12/May/15 18:31,13/May/15 20:39,14/Jul/23 06:26,13/May/15 18:05,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,,,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 20:39:02 UTC 2015,,,,,,,,,,"0|i2emtr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"12/May/15 18:43;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/6090;;;","13/May/15 18:04;marmbrus;Issue resolved by pull request 6090
[https://github.com/apache/spark/pull/6090];;;","13/May/15 20:39;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/6130;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
HiveContext.analyzer cannot be overriden,SPARK-7566,12829238,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smolav,smolav,smolav,12/May/15 16:22,19/May/15 01:12,14/Jul/23 06:26,13/May/15 06:45,1.3.1,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,"Trying to override HiveContext.analyzer will give the following compilation error:

{code}
Error:(51, 36) overriding lazy value analyzer in class HiveContext of type org.apache.spark.sql.catalyst.analysis.Analyzer{val extendedResolutionRules: List[org.apache.spark.sql.catalyst.rules.Rule[org.apache.spark.sql.catalyst.plans.logical.LogicalPlan]]};
 lazy value analyzer has incompatible type
  override protected[sql] lazy val analyzer: Analyzer = {
                                   ^
{code}

That is because the type changed inadvertedly when omitting the type declaration of the return type.",,apachespark,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 07:12:03 UTC 2015,,,,,,,,,,"0|i2emmv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/15 16:26;apachespark;User 'smola' has created a pull request for this issue:
https://github.com/apache/spark/pull/6086;;;","15/May/15 07:12;apachespark;User 'smola' has created a pull request for this issue:
https://github.com/apache/spark/pull/6177;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken maps in jsonRDD,SPARK-7565,12829165,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,tailhook,tailhook,12/May/15 11:00,21/May/15 16:59,14/Jul/23 06:26,21/May/15 16:59,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"When I use the following JSON:
{code}
{""obj"": {""a"": ""hello""}}
{code}

And load it with the following python code:
{code}
tf = sc.textFile('test.json')
v = sqlContext.jsonRDD(tf, StructType([StructField(""obj"", MapType(StringType(), StringType()), True)]))
v.save('test.parquet', mode='overwrite')
{code}

I get the following error in spark master branch:
{code}
Py4JJavaError: An error occurred while calling o78.save.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 5.0 failed 1 times, most recent failure: Lost task 1.0 in stage 5.0 (TID 11, localhost): java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.spark.sql.types.UTF8String
        at org.apache.spark.sql.parquet.RowWriteSupport.writePrimitive(ParquetTableSupport.scala:201)
        at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:192)
        at org.apache.spark.sql.parquet.RowWriteSupport$$anonfun$writeMap$2.apply(ParquetTableSupport.scala:284)
        at org.apache.spark.sql.parquet.RowWriteSupport$$anonfun$writeMap$2.apply(ParquetTableSupport.scala:281)
        at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:109)
        at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
        at org.apache.spark.sql.parquet.RowWriteSupport.writeMap(ParquetTableSupport.scala:281)
        at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:186)
        at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:171)
        at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:134)
        at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
        at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:699)
        at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:717)
        at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:717)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

{code}

This worked well in spark 1.3",,apachespark,davies,joshrosen,tailhook,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 21 16:59:10 UTC 2015,,,,,,,,,,"0|i2em6v:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"12/May/15 12:53;srowen;Set component please.
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark;;;","12/May/15 15:18;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6084;;;","18/May/15 13:00;tailhook;The pull request is pretty trivial. I've tested it and it solves the problem for me.;;;","20/May/15 22:02;davies;[~tailhook] The patch is kind of workaround, it does not really fix the root cause, I will sending out another PR to fix it. Thanks for reporting this.;;;","20/May/15 22:52;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6299;;;","21/May/15 16:59;yhuai;Issue resolved by pull request 6299
[https://github.com/apache/spark/pull/6299];;;",,,,,,,,,,,,,,,,,,,,,,,
OutputCommitCoordinator.stop() should only be executed in driver,SPARK-7563,12829124,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,wenhailong1988,wenhailong1988,12/May/15 08:20,28/Sep/15 08:13,14/Jul/23 06:26,28/Sep/15 08:13,1.3.1,,,,,,1.3.2,1.4.0,,,,,Spark Core,,,,0,,,,,,"I am from IBM Platform Symphony team and we are integrating Spark 1.3.1 with EGO (a resource management product).

In EGO we uses fine-grained dynamic allocation policy, and each Executor will exit after its tasks are all done. When testing *spark-shell*, we find that when executor of first job exit, it will stop OutputCommitCoordinator, which result in all future jobs failing. Details are as follows:

We got the following error in executor when submitting job in *spark-shell* the second time (the first job submission is successful):
{noformat}
15/05/11 04:02:31 INFO spark.util.AkkaUtils: Connecting to OutputCommitCoordinator: akka.tcp://sparkDriver@whlspark01:50452/user/OutputCommitCoordinator
Exception in thread ""main"" akka.actor.ActorNotFound: Actor not found for: ActorSelection[Anchor(akka.tcp://sparkDriver@whlspark01:50452/), Path(/user/OutputCommitCoordinator)]
        at akka.actor.ActorSelection$$anonfun$resolveOne$1.apply(ActorSelection.scala:65)
        at akka.actor.ActorSelection$$anonfun$resolveOne$1.apply(ActorSelection.scala:63)
        at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
        at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:67)
        at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:82)
        at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)
        at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)
        at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
        at akka.dispatch.BatchingExecutor$Batch.run(BatchingExecutor.scala:58)
        at akka.dispatch.ExecutionContexts$sameThreadExecutionContext$.unbatchedExecute(Future.scala:74)
        at akka.dispatch.BatchingExecutor$class.execute(BatchingExecutor.scala:110)
        at akka.dispatch.ExecutionContexts$sameThreadExecutionContext$.execute(Future.scala:73)
        at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
        at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
        at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:267)
        at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:89)
        at akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
        at akka.dispatch.Mailbox.run(Mailbox.scala:220)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}

And in driver side, we see a log message telling that the OutputCommitCoordinator is stopped after the first submission:
{noformat}
15/05/11 04:01:23 INFO spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorActor: OutputCommitCoordinator stopped!
{noformat}

We examine the code of OutputCommitCoordinator, and find that executor will reuse the ref of driver's OutputCommitCoordinatorActor. So when an executor exits, it will eventually call SparkEnv.stop():
{noformat}
  private[spark] def stop() {
    isStopped = true
    pythonWorkers.foreach { case(key, worker) => worker.stop() }
    Option(httpFileServer).foreach(_.stop())
    mapOutputTracker.stop()
    shuffleManager.stop()
    broadcastManager.stop()
    blockManager.stop()
    blockManager.master.stop()
    metricsSystem.stop()
    outputCommitCoordinator.stop()      <--------------- 
    actorSystem.shutdown()
    ......
{noformat} 

and in OutputCommitCoordinator.stop():
{noformat}
  def stop(): Unit = synchronized {
    coordinatorActor.foreach(_ ! StopCoordinator)
    coordinatorActor = None
    authorizedCommittersByStage.clear()
  }
{noformat}

We now work this problem around by adding an attribute ""isDriver"" in OutputCommitCoordinator and judge whether the ""stop"" command comes from driver or executor:
{noformat}
diff SparkEnv.scala
360c360
<       new OutputCommitCoordinator(conf, isDriver)
---
>       new OutputCommitCoordinator(conf)


diff OutputCommitCoordinator.scala
43c43
< private[spark] class OutputCommitCoordinator(conf: SparkConf, isDriver: Boolean = false) extends Logging {
---
> private[spark] class OutputCommitCoordinator(conf: SparkConf) extends Logging {
137,141c137,139
<     if (isDriver) {
<       coordinatorActor.foreach(_ ! StopCoordinator)
<       coordinatorActor = None
<       authorizedCommittersByStage.clear()
<     }
---
>     coordinatorActor.foreach(_ ! StopCoordinator)
>     coordinatorActor = None
>     authorizedCommittersByStage.clear()
{noformat}
We propose to apply this fix in future release since it may affects all *spark-shell* function of dynamic allocation model.
","Red Hat Enterprise Linux Server release 7.0 (Maipo)
Spark 1.3.1 Release",apachespark,glenn.strycker@gmail.com,joshrosen,pwendell,wenhailong1988,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 02 08:51:06 UTC 2015,,,,,,,,,,"0|i2elxz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/May/15 20:15;pwendell;/cc [~joshrosen] I think this is caused by the output committer change you worked on. Probably just a corner case here when executors die in the spark shell.;;;","14/May/15 20:23;joshrosen;Yep, looks like a pretty clear bug.  I think that this messiness was introduced because we decided to use this confusing pattern where the OutputCommitCoordinator component is used both on the master and executors.  My original version of this patch separated the driver and executor componenets more explicitly, but if I recall this was changed in response to some comments during offline code review.  I should have probably fought harder to keep the separation, since having classes with sets of methods that are unsafe to call from certain locations has been a source of multiple types of bugs like this in the past.

Want me to put together a pull request to apply this patch?;;;","15/May/15 19:03;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6197;;;","16/May/15 01:08;pwendell;I pulled the fix into 1.4.0, but not yet 1.3.2 (didn't feel comfortable doing the backport).;;;","22/Jun/15 19:00;joshrosen;This still needs a backport.  I'll do it eventually but it would be nice if someone else could submit a PR; I'll review.;;;","02/Aug/15 08:51;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7865;;;",,,,,,,,,,,,,,,,,,,,,,,
Close files correctly when iteration is finished in WAL recovery,SPARK-7552,12829042,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,jerryshao,jerryshao,12/May/15 01:55,13/May/15 01:18,14/Jul/23 06:26,13/May/15 01:18,1.3.1,1.4.0,,,,,1.3.2,1.4.0,,,,,DStreams,,,,0,,,,,,,,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 02:43:05 UTC 2015,,,,,,,,,,"0|i2elfz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/15 02:43;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/6069;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka stream fails: java.lang.NoClassDefFound com/yammer/metrics/core/Gauge,SPARK-7538,12828942,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,splee,splee,11/May/15 20:00,12/May/15 15:48,14/Jul/23 06:26,12/May/15 02:03,1.3.1,,,,,,,,,,,,DStreams,,,,0,,,,,,"We have a simple streaming job, the components of which work fine in a batch environment reading from a cassandra table as the source.

We adapted it to work with streaming using the Python libs.

Submit command line:

{code}
/home/ubuntu/spark/spark-1.3.1/bin/spark-submit \
    --packages TargetHolding/pyspark-cassandra:0.1.4,org.apache.spark:spark-streaming-kafka_2.10:1.3.1 \
    --conf spark.cassandra.connection.host=10.10.103.172,10.10.102.160,10.10.101.79 \
    --master spark://127.0.0.1:7077 \
    affected_hosts.py
{code}

When we run the streaming job everything starts just fine, then we see the following in the logs:

{code}
15/05/11 19:50:46 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 70, ip-10-10-102-53.us-west-2.compute.internal): java.lang.NoClassDefFoundError: com/yammer/metrics/core/Gauge
        at kafka.consumer.ZookeeperConsumerConnector.createFetcher(ZookeeperConsumerConnector.scala:151)
        at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:115)
        at kafka.consumer.ZookeeperConsumerConnector.<init>(ZookeeperConsumerConnector.scala:128)
        at kafka.consumer.Consumer$.create(ConsumerConnector.scala:89)
        at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:100)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.startReceiver(ReceiverSupervisor.scala:121)
        at org.apache.spark.streaming.receiver.ReceiverSupervisor.start(ReceiverSupervisor.scala:106)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$8.apply(ReceiverTracker.scala:298)
        at org.apache.spark.streaming.scheduler.ReceiverTracker$ReceiverLauncher$$anonfun$8.apply(ReceiverTracker.scala:290)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1498)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1498)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: com.yammer.metrics.core.Gauge
        at java.net.URLClassLoader$1.run(URLClassLoader.java:372)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:360)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 17 more
{code}","Ubuntu 14.04 LTS
java version ""1.7.0_79""
OpenJDK Runtime Environment (IcedTea 2.5.5) (7u79-2.5.5-0ubuntu0.14.04.2)
OpenJDK 64-Bit Server VM (build 24.79-b02, mixed mode)

Spark 1.3.1 release.",pwendell,splee,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 15:48:40 UTC 2015,,,,,,,,,,"0|i2ekuf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/15 20:39;srowen;This usually indicates classpath problems with the app. Are you using a dependency, like this metrics lib, that might be conflicting? I think streaming does, so that might be why it shows up in streaming. Did you try setting the user classpath first settings?;;;","12/May/15 02:03;pwendell;This was a cross post from the mailing list. The poster closed the thread with the following:

{code}
Ted, many thanks.  I'm not used to Java dependencies so this was a real head-scratcher for me.

Downloading the two metrics packages from the maven repository (metrics-core, metrics-annotation) and supplying it on the spark-submit command line worked.

My final spark-submit for a python project using Kafka as an input source:

/home/ubuntu/spark/spark-1.3.1/bin/spark-submit \
    --packages TargetHolding/pyspark-cassandra:0.1.4,org.apache.spark:spark-streaming-kafka_2.10:1.3.1 \
    --jars /home/ubuntu/jars/metrics-core-2.2.0.jar,/home/ubuntu/jars/metrics-annotation-2.2.0.jar \
    --conf spark.cassandra.connection.host=10.10.103.172,10.10.102.160,10.10.101.79 \
    --master spark://127.0.0.1:7077 \
    affected_hosts.py

Now we're seeing data from the stream.  Thanks again!
{code};;;","12/May/15 15:48;yuzhihong@gmail.com;As mentioned by Cody Koeninger on the mailing list, using spark-streaming-kafka-assembly_2.10:1.3.1 would resolve the issue:
{code}
$ jar tvf ~/Downloads/spark-streaming-kafka-assembly_2.10-1.3.1.jar | grep yammer | grep Gauge
  1329 Sat Apr 11 04:25:50 PDT 2015 com/yammer/metrics/core/Gauge.class
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Make StreamingContext.start() idempotent,SPARK-7532,12828870,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,11/May/15 16:45,15/May/15 19:45,14/Jul/23 06:26,12/May/15 15:49,,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,"Currently calling StreamingContext.start() throws error when the context is already started. This is inconsistent with the the StreamingContext.stop() which is idempotent, that is, called stop() on a stopped context is a no-op. ",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 17:14:35 UTC 2015,,,,,,,,,,"0|i2ekf3:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"11/May/15 17:14;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6060;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add API to get the current state of a StreamingContext,SPARK-7530,12828863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,11/May/15 16:23,12/May/15 01:55,14/Jul/23 06:26,12/May/15 01:55,,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 16:30:17 UTC 2015,,,,,,,,,,"0|i2ekdj:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"11/May/15 16:30;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6058;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong detection of REPL mode in ClosureCleaner,SPARK-7527,12828831,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,preeze,preeze,11/May/15 14:38,08/Oct/15 18:27,14/Jul/23 06:26,10/Jun/15 20:27,1.3.1,,,,,,1.4.2,1.5.0,,,,,Spark Core,,,,0,,,,,,"If REPL class is not present on the classpath, the {{inIntetpreter}} boolean switch shall be {{false}}, not {{true}} at: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala#L247",,apachespark,Bosoon,preeze,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10773,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 02:58:02 UTC 2015,,,,,,,,,,"0|i2ek6f:",9223372036854775807,,,,,,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"11/May/15 14:40;preeze;Being addressed together with SPARK-7233;;;","11/May/15 14:50;apachespark;User 'preeze' has created a pull request for this issue:
https://github.com/apache/spark/pull/5835;;;","16/May/15 18:57;preeze;In the end, due to a bigger complexity decided to be fixed separately from SPARK-7233;;;","10/Jun/15 02:58;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6735;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ML Examples option for dataFormat should not be enclosed in angle brackets,SPARK-7522,12828736,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bryanc,bryanc,bryanc,11/May/15 06:32,13/May/15 11:05,14/Jul/23 06:26,13/May/15 11:05,1.1.1,1.2.2,1.3.1,1.4.0,,,1.2.3,1.3.2,1.4.0,,,,Examples,,,,0,,,,,,"Some ML examples include an option for specifying the data format, such as DecisionTreeExample, but the option is enclosed in angle brackets like ""opt[String](""<dataFormat>"").""  This is probably just a typo but makes it awkward to use the option.",,apachespark,bryanc,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 05:55:08 UTC 2015,,,,,,,,,,"0|i2ejlb:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.3,1.3.2,,,,,,,,,,,"11/May/15 06:45;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/6049;;;","11/May/15 07:12;bryanc;pinging [~josephkb] to verify;;;","11/May/15 16:25;mengxr;Issue resolved by pull request 6049
[https://github.com/apache/spark/pull/6049];;;","11/May/15 16:29;mengxr;re-open for branch-1.3/1.2/1.1.;;;","13/May/15 05:55;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/6111;;;",,,,,,,,,,,,,,,,,,,,,,,,
Some minor UI bugs in the new HiveThriftServer2 web UI,SPARK-7519,12828714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tianyi,tianyi,tianyi,11/May/15 02:16,11/May/15 06:09,14/Jul/23 06:26,11/May/15 06:09,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Bugs description:

# There are extra commas on the top of session list.
# The format of time in ""Start at:"" is not the same as others.
# The total number of online sessions is wrong.",,apachespark,lian cheng,tianyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 06:09:36 UTC 2015,,,,,,,,,,"0|i2ejgf:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"11/May/15 03:52;apachespark;User 'tianyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/6048;;;","11/May/15 06:09;lian cheng;Issue resolved by pull request 6048
[https://github.com/apache/spark/pull/6048];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documentation for PySpark on YARN with cluster mode,SPARK-7515,12828650,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,10/May/15 10:17,16/Jun/15 14:46,14/Jul/23 06:26,11/May/15 21:21,1.4.0,,,,,,1.4.1,1.5.0,,,,,Documentation,,,,0,,,,,,Now PySpark on YARN with cluster mode is supported so let's update doc.,,apachespark,sandyr,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 16 14:26:04 UTC 2015,,,,,,,,,,"0|i2ej27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/15 10:19;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/6040;;;","16/Jun/15 14:26;apachespark;User 'punya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6842;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR RDD show method doesn't work after transformations,SPARK-7512,12828627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,shivaram,shivaram,10/May/15 00:00,11/May/15 02:50,14/Jul/23 06:26,11/May/15 02:50,1.4.0,,,,,,1.4.0,,,,,,SparkR,,,,0,,,,,,Detailed description of the bug at http://stackoverflow.com/questions/30057702/sparkr-filterrdd-and-flatmap-not-working,,apachespark,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 02:50:04 UTC 2015,,,,,,,,,,"0|i2eix3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/15 00:05;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/6035;;;","11/May/15 02:50;shivaram;Issue resolved by pull request 6035
[https://github.com/apache/spark/pull/6035];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when initializing SparkContext in YARN-cluster mode,SPARK-7504,12828579,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ehnalis,Ehnalis,Ehnalis,09/May/15 14:31,16/May/15 11:07,14/Jul/23 06:26,15/May/15 19:17,,,,,,,1.4.0,,,,,,Deploy,YARN,,,0,deployment,yarn,yarn-client,,,"It is not clear for most users that, while running Spark on YARN a {{SparkContext}} with a given execution plan can be run locally as {{yarn-client}}, but can not deploy itself to the cluster. This is currently performed using {{org.apache.spark.deploy.yarn.Client}}. {color:gray} I think we should support deployment through {{SparkContext}}, but this is not the point I wish to make here. {color}

Configuring a {{SparkContext}} to deploy itself currently will yield an {{ERROR}} while accessing {{spark.yarn.app.id}} in  {{YarnClusterSchedulerBackend}}, and after that a {{NullPointerException}} while referencing the {{ApplicationMaster}} instance.

Spark should clearly inform the user that it might be running in {{yarn-cluster}} mode without a proper submission using {{Client}} and that deploying is not supported directly from {{SparkContext}}.",,andrewor14,apachespark,Ehnalis,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 15:18:39 UTC 2015,,,,,,,,,,"0|i2eim7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"09/May/15 15:19;apachespark;User 'ehnalis' has created a pull request for this issue:
https://github.com/apache/spark/pull/6029;;;","09/May/15 15:36;srowen;I don't understand the scenario. How do you accidentally run in yarn-cluster mode?;;;","09/May/15 15:46;Ehnalis;I did not run in by accident. I was writing applications using IntelliJ IDEA and changed {{spark.master}} on each run to test and debug in {{local}} or {{yarn-client}} mode. Both modes are supported by {{SparkContext}}, but it can not deploy itself, so as I've changed to {{yarn-cluster}} (to deploy my app for a longer run), {{NullPointerExceptions}} were thrown. I did not realize on the first and second time, deploying is only available through {{org.apache.spark.deploy.yarn.Client}}. I thought there's an issue with my remote YARN cluster or the configuration.;;;","09/May/15 15:52;srowen;I don't think that's supported, but in any event the error you seem to be solving is that you tried to run the AM but not through main()? This doesn't sound like something that should work.;;;","09/May/15 16:06;Ehnalis;I did not run ApplicationMaster directly myself. You can easily reproduce the problem simply by:
- picking a Spark example and
- adding {{-Dspark.master=yarn-cluster}} to VM options, or setting {{spark.master}} through {{SparkContext}}'s configuration
- then run it.

This seems to be a very common scenario to me.;;;","09/May/15 18:05;andrewor14;Zoltan, what do you mean configuring a SparkContext to deploy itself? Did you run the application using the following command?

bin/spark-submit --master yarn-cluster [... other options ...];;;","09/May/15 18:31;Ehnalis;No, I did not use {{spark-submit}}, but I ran a Spark program with {{java}} (using IntelliJ basically) - as I've said a few comments earlier.

The problem from user point of view:
There is a user-facing API provided by {{SparkContext}}, which can be configured with a {{SparkConf}} that has a valid parameter named {{spark.master}} with a valid value {{yarn-cluster}}. If I use {{SparkContext}} with this configuration and define a valid chain of RDDs, compile and run my JAVA code, it simply blows up. For example:

- pick any Spark example,
- set {{spark.master}} to {{yarn-cluster}}
- run your code with JAVA.

Instead of blowing up with {{NullPointerExceptions}}, Spark should warn the user that he or she can not deploy his or her application that way, because it is not supported.;;;","10/May/15 20:23;srowen;{{SparkContext}} isn't an API for launching a Spark app; {{spark-submit}} is. Although you can get this to work it often takes more manual footwork to get it right. There's a new launcher API in 1.4 that is supposed to be a proper programmatic submission API.

I can appreciate giving better errors in any event, even if it's something one isn't supposed to do. This seems pretty harmless as an improved error message, just in case.;;;","10/May/15 20:57;Ehnalis;{{SparkContext}} can be easily configured to operate on local mode without any additional manual change, and can run in {{yarn-client}} mode directly from IntelliJ {{java}}, just needs an additional {{datanucleus-*}} dependency to class-path and a JAR to be set.

As far as I see this problem: if I buy a washing machine, configure it at home and if it blows up with {{ERROR}} on the display, well, I would return it ASAP. But if it tells me {{The configured mode is not supported!}}, I wouldn't think the object is malfunctioning.

I'm pleased that you think this could be improved.;;;","11/May/15 16:48;vanzin;If I understand correctly, what you're doing is running the equivalent of this in you code, right:

{code}
    new SparkContext(new SparkConf().set(""spark.master"", ""yarn-cluster""))
{code}

That's not really supported, since that will not work in yarn-cluster mode, even if the ApplicationMaster launches successfully. I also took a look at your PR and that won't help. The fix here, if any, is to not allow the above code to work by throwing an exception early.;;;","11/May/15 18:14;Ehnalis;You are making a point. I will extend my solution to throw an error message earlier.;;;","12/May/15 15:18;apachespark;User 'ehnalis' has created a pull request for this issue:
https://github.com/apache/spark/pull/6083;;;","12/May/15 15:18;Ehnalis;I've extended my solution. It will throw an exception early by detecting if it has been run by the AM.;;;",,,,,,,,,,,,,,,,
Resources in .sparkStaging directory can't be cleaned up on error,SPARK-7503,12828567,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,09/May/15 10:04,28/Jul/15 06:21,14/Jul/23 06:26,15/May/15 10:38,1.4.0,,,,,,1.4.0,,,,,,YARN,,,,0,,,,,,"When we run applications on YARN with cluster mode, uploaded resources on .sparkStaging directory can't be cleaned up in case of failure of uploading local resources.

You can see this issue by running following command.
{code}
bin/spark-submit --master yarn --deploy-mode cluster --class <someClassName> <non-existing-jar>
{code}",,apachespark,sarutak,sumit.nigam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7439,,,,,,,SPARK-7705,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 06:20:27 UTC 2015,,,,,,,,,,"0|i2eijr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"09/May/15 10:07;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/6026;;;","15/May/15 10:38;srowen;Issue resolved by pull request 6026
[https://github.com/apache/spark/pull/6026];;;","28/Jul/15 06:20;sumit.nigam;For spark on yarn, would yarn not remove all local directories created by container processes upon their exit even if it were unclean exit? In such a case, even if spark fails to remove them, yarn should be able to clear them. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Params.setDefault should keep varargs annotation,SPARK-7498,12828518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,josephkb,josephkb,08/May/15 22:58,23/Jul/15 00:21,14/Jul/23 06:26,21/May/15 20:07,1.4.0,,,,,,1.4.0,,,,,,Java API,ML,,,0,,,,,,"In [SPARK-7429] and PR [https://github.com/apache/spark/pull/5960], I added the varargs annotation to Params.setDefault which takes a variable number of ParamPairs.  It worked locally and on Jenkins for me.

However, @mengxr reported issues compiling on his machine.  So I'm reverting the change introduced in [https://github.com/apache/spark/pull/5960] by removing varargs.",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7429,,SPARK-9268,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 30 02:02:48 UTC 2015,,,,,,,,,,"0|i2ei8v:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"09/May/15 00:24;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/6021;;;","09/May/15 04:56;mengxr;Issue resolved by pull request 6021
[https://github.com/apache/spark/pull/6021];;;","11/May/15 04:41;mengxr;This could be a false alarm. I couldn't reproduce the issue after `sbt clean`. I will submit a PR to revert the change.;;;","11/May/15 05:08;josephkb;Oh Java....;;;","21/May/15 16:59;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/6320;;;","21/May/15 16:59;mengxr;I renamed to JIRA title to `keep varargs`. ;;;","21/May/15 20:07;mengxr;Issue resolved by pull request 6320
[https://github.com/apache/spark/pull/6320];;;","30/May/15 02:02;josephkb;[~mengxr] I'm frequently running into this issue when I don't do a clean build.  As you noted above, doing a clean build solves this issue.  But somehow it makes it more likely than an unclean build will fail.  We'll have to see how much this annoys developers.;;;",,,,,,,,,,,,,,,,,,,,,
test_count_by_value_and_window is flaky,SPARK-7497,12828514,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,mengxr,mengxr,08/May/15 22:34,25/Aug/15 06:42,14/Jul/23 06:26,25/Aug/15 02:14,1.4.0,,,,,,1.5.0,,,,,,DStreams,PySpark,,,0,flaky-test,,,,,"Saw this test failure in https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/32268/console

{code}
======================================================================
FAIL: test_count_by_value_and_window (__main__.WindowFunctionTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""pyspark/streaming/tests.py"", line 418, in test_count_by_value_and_window
    self._test_func(input, func, expected)
  File ""pyspark/streaming/tests.py"", line 133, in _test_func
    self.assertEqual(expected, result)
AssertionError: Lists differ: [[1], [2], [3], [4], [5], [6], [6], [6], [6], [6]] != [[1], [2], [3], [4], [5], [6], [6], [6]]

First list contains 2 additional elements.
First extra element 8:
[6]

- [[1], [2], [3], [4], [5], [6], [6], [6], [6], [6]]
?                                     ----------

+ [[1], [2], [3], [4], [5], [6], [6], [6]]

----------------------------------------------------------------------
{code}",,apachespark,mengxr,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7908,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 19:47:06 UTC 2015,,,,,,,,,,"0|i2ei7z:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"09/May/15 01:07;shivaram;I've seen this a couple of times as well very recently;;;","18/May/15 19:47;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6239;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TABLE statement,SPARK-7493,12828469,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,ssemichev,ssemichev,08/May/15 19:43,22/Aug/16 14:07,14/Jul/23 06:26,22/Aug/16 12:25,,,,,,,,,,,,,SQL,,,,0,,,,,,"Full table name (database_name.table_name) cannot be used with ""ALTER TABLE"" statement 
It works with CREATE TABLE

""ALTER TABLE database_name.table_name ADD PARTITION (source_year='2014', source_month='01').....""

Error in SQL statement: java.lang.RuntimeException: org.apache.spark.sql.AnalysisException: mismatched input 'ADD' expecting KW_EXCHANGE near 'test_table' in alter exchange partition;",Databricks cloud,dongjoon,ssemichev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 22 14:07:43 UTC 2016,,,,,,,,,,"0|i2ehy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/16 21:46;dongjoon;Hi, [~ssemichev].
Just FYI, Spark 2.0 supports this now.
{code}
scala> sql(""create table default.t(i int) partitioned by (p timestamp)"")
scala> sql(""alter table default.t add partition(p='2013-01-02 01:00')"")
scala> sql(""select * from default.t"").show
+---+---+
|  i|  p|
+---+---+
+---+---+
scala> sql(""show partitions default.t"").show
+--------------------+
|           partition|
+--------------------+
|p=2013-01-02 01%3A00|
+--------------------+
{code};;;","21/Aug/16 21:57;dongjoon;Also, this seems to be supported in 1.6.2 for non-timestamp types.
{code}
scala> sc.version
res7: String = 1.6.2
scala> sql(""create table default.t2(i int) partitioned by (p int)"")
scala> sql(""alter table default.t2 add partition(p=2)"")
scala> sql(""show partitions default.t2"").show
+------+
|result|
+------+
|   p=2|
+------+
{code};;;","22/Aug/16 12:25;ssemichev;Good to know, thanks;;;","22/Aug/16 14:07;dongjoon;Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
MapOutputTracker: close input streams to free native memory,SPARK-7490,12828463,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Evan Jones,Evan Jones,Evan Jones,08/May/15 18:55,09/May/15 02:08,14/Jul/23 06:26,09/May/15 02:08,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"GZIPInputStream allocates native memory that is not freed until close() or when the finalizer runs. It is best to close() these streams explicitly to avoid native memory leaks

Pull request here: https://github.com/apache/spark/pull/5982",,apachespark,Evan Jones,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 21:01:45 UTC 2015,,,,,,,,,,"0|i2ehwv:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"08/May/15 18:56;apachespark;User 'evanj' has created a pull request for this issue:
https://github.com/apache/spark/pull/5982;;;","08/May/15 21:01;srowen;Issue resolved by pull request 5982
[https://github.com/apache/spark/pull/5982];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark shell crashes when compiled with scala 2.11 and SPARK_PREPEND_CLASSES=true,SPARK-7489,12828438,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vinodkc,vinodkc,vinodkc,08/May/15 17:37,08/May/15 21:08,14/Jul/23 06:26,08/May/15 21:08,,,,,,,1.4.0,,,,,,Spark Shell,,,,0,,,,,,"Steps followed
>export SPARK_PREPEND_CLASSES=true
>dev/change-version-to-2.11.sh
> sbt/sbt -Pyarn -Phadoop-2.4 -Dscala-2.11 -DskipTests clean assembly

>bin/spark-shell
....................
15/05/08 22:31:35 INFO Main: Created spark context..
Spark context available as sc.
java.lang.NoClassDefFoundError: org/apache/hadoop/hive/conf/HiveConf
  at java.lang.Class.getDeclaredConstructors0(Native Method)
  at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)
  at java.lang.Class.getConstructor0(Class.java:3075)
  at java.lang.Class.getConstructor(Class.java:1825)
  at org.apache.spark.repl.Main$.createSQLContext(Main.scala:86)
  ... 45 elided
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf
  at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
  ... 50 more
<console>:11: error: not found: value sqlContext
       import sqlContext.implicits._
              ^
<console>:11: error: not found: value sqlContext
       import sqlContext.sql

There is a similar Resolved JIRA issue  -SPARK-7470 and a PR https://github.com/apache/spark/pull/5997 , which handled same  issue  only in scala 2.10",,apachespark,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 17:41:05 UTC 2015,,,,,,,,,,"0|i2ehrr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"08/May/15 17:41;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/6013;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove python artifacts from the assembly jar,SPARK-7485,12828367,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,tgraves,tgraves,08/May/15 13:49,12/May/15 08:39,14/Jul/23 06:26,12/May/15 08:39,1.4.0,,,,,,1.4.0,,,,,,Build,,,,0,,,,,,We change it so that we distributed the python files via a zip file in SPARK-6869.  With that we should remove the python files from the assembly jar.,,apachespark,gliptak,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 09 00:31:09 UTC 2015,,,,,,,,,,"0|i2ehc7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"09/May/15 00:31;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6022;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[MLLib] Using Kryo with FPGrowth fails with an exception,SPARK-7483,12828334,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mark_yang,kretes,kretes,08/May/15 11:53,19/Jan/18 20:36,14/Jul/23 06:26,27/Feb/16 13:51,1.3.1,,,,,,2.0.0,,,,,,MLlib,,,,4,,,,,,"When using FPGrowth algorithm with KryoSerializer - Spark fails with

{code}
Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 16, localhost): com.esotericsoftware.kryo.KryoException: java.lang.IllegalArgumentException: Can not set final scala.collection.mutable.ListBuffer field org.apache.spark.mllib.fpm.FPTree$Summary.nodes to scala.collection.mutable.ArrayBuffer
Serialization trace:
nodes (org.apache.spark.mllib.fpm.FPTree$Summary)
org$apache$spark$mllib$fpm$FPTree$$summaries (org.apache.spark.mllib.fpm.FPTree)
{code}

This can be easily reproduced in spark codebase by setting 
{code}
conf.set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
{code} and running FPGrowthSuite.


",,apachespark,astral303,josephkb,Kaiyuan Yang,Kralph,kretes,mengxr,mlety2,simonlou,sunrenxu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 27 13:51:22 UTC 2016,,,,,,,,,,"0|i2eh4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/15 21:21;josephkb;(Updated) Maybe this is a bug...will look into it.
;;;","08/May/15 21:25;josephkb;Does it fix anything if you give Kryo more info, such as explicit registration of relevant classes?;;;","11/May/15 09:30;kretes;hmm, class org.apache.spark.mllib.fpm.FPTree is private and in spark-mllib. Spark is registering classes in spark-core in org.apache.spark.serializer.KryoSerializer#toRegister so it is not straightforward how to do that easily.

And I do not think this is the case of registering, looking at spark serialization doc ""Finally, if you don’t register your custom classes, Kryo will still work, but it will have to store the full class name with each object, which is wasteful."";;;","11/May/15 17:56;josephkb;I agree; it should work, but I'm not sure why it's failing.  I'm not that familiar with Kryo, but I'll ask around.  Thanks for reporting this!;;;","02/Jul/15 02:27;Kralph;I encountered the same bug.
Adding 
sparkConf.registerKryoClasses(Array(classOf[ArrayBuffer[String]], classOf[ListBuffer[String]]))
seems to fix the problem.;;;","02/Jul/15 15:13;kretes;this solves the problem for me as well.

maybe those classes should be registered in kryo from spark when user code is using kryo serialization?;;;","02/Jul/15 23:24;Kralph;This does NOT work.

Registering the classes stops it from crashing, but produces a bug in the FP-Growth algorithm.

Specifically, the frequency counts for itemsets are wrong.

:(;;;","03/Jul/15 07:12;kretes;hmm that's right - adding it in FPGrowthSuite in spark fails first spec.;;;","05/Jul/15 22:14;sunrenxu;I've tried to change the nodes property of Summay class inside object FPTree from ListBuffer to ArrayBuffer. And it seems to fix the exception problem as well as producing the right item counts so far. Suspect the problem with KryoSerializer dealing with ListBuffer class, but haven't looked into details.  The code looks like:

private[fpm] object FPTree {

  /** Representing a node in an FP-Tree. */
  class Node[T](val parent: Node[T]) extends Serializable {
    var item: T = _
    var count: Long = 0L
    val children: mutable.Map[T, Node[T]] = mutable.Map.empty

    def isRoot: Boolean = parent == null
  }

  /** Summary of a item in an FP-Tree. */
  private class Summary[T] extends Serializable {
    var count: Long = 0L
    val nodes: mutable.ArrayBuffer[Node[T]] = mutable.ArrayBuffer.empty
  }
}

;;;","24/Sep/15 11:32;simonlou;kyro not support ListBuffer because ListBuffer don't have any ""zero argument constructor"".
refer to : https://github.com/EsotericSoftware/kryo#using-standard-java-serialization

""By default, if a class has a zero argument constructor then it is invoked via ReflectASM or reflection, otherwise an exception is thrown. ""

Is that the reason?

When using kyro , use ArrayBuffer instead of ListBuffer;;;","03/Feb/16 02:20;apachespark;User 'mark800' has created a pull request for this issue:
https://github.com/apache/spark/pull/11041;;;","03/Feb/16 02:24;Kaiyuan Yang;Upgrade Chill to 0.7.2, then everything is fine.

New version registers more Scala classes, including ListBuffer to support Kryo with FPGrowth.;;;","27/Feb/16 13:51;srowen;Issue resolved by pull request 11041
[https://github.com/apache/spark/pull/11041];;;",,,,,,,,,,,,,,,,
Get exception when DataFrame saveAsTable and run sql on the same table at the same time,SPARK-7480,12828291,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pin_zhang,pin_zhang,08/May/15 08:57,07/Oct/16 22:12,14/Jul/23 06:26,07/Oct/16 22:12,1.3.0,1.3.1,,,,,,,,,,,SQL,,,,0,,,,,,"There is a case 
1) In the main thread call  DataFrame.saveAsTable(table,    SaveMode.Overwrite); save json rdd to hive table
2) In another thread run sql the table simultaneously 
You can see many exceptions to indicate the table not exit or table is not complete.
Does Spark SQL support such usage?

Thanks

[Main Thread]
DataFrame df = hiveContext_.jsonFile(""test.json"");
  String table = ""UNIT_TEST"";
 while (true) {
            df = hiveContext_.jsonFile(""test.json"");
            df.saveAsTable(table, SaveMode.Overwrite);
            System.out.println(new Timestamp(System.currentTimeMillis()) + "" [ ""+Thread.currentThread().getName()
                    + ""] override table"");           
            try {
                Thread.sleep(3000);
            } catch (InterruptedException e) {
                e.printStackTrace();
            }
        }

[Query Thread]
   DataFrame query = hiveContext_.sql(""select * from UNIT_TEST"");
                Row[] rows = query.collect();
                System.out.println(new Timestamp(System.currentTimeMillis()) + "" [ ""+ Thread.currentThread().getName()
                        + ""]  [query result count:] "" + rows.length);


[Exceptions in log]

15/05/08 16:05:49 ERROR Hive: NoSuchObjectException(message:default.unit_test table not found)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1560)
	at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy20.get_table(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:997)
	at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy21.getTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:976)
	at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:950)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:201)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$$super$lookupRelation(HiveContext.scala:262)
	at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:161)
	at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:161)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:161)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.lookupRelation(HiveContext.scala:262)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:174)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:186)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:181)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:208)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:238)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:193)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:178)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:181)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:171)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:1082)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:1082)
	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1080)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:101)
	at spark.streaming.test.SparkHiveTest$QueryWorker.query(SparkHiveTest.java:114)
	at spark.streaming.test.SparkHiveTest$QueryWorker.run(SparkHiveTest.java:108)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

15/05/08 16:06:29 ERROR Executor: Exception in task 0.0 in stage 1457.0 (TID 2551)
java.io.FileNotFoundException: File file:/user/hive/warehouse/unit_test/part-r-00002.parquet does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:511)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:724)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:501)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:397)
	at parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:381)
	at parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:155)
	at parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:138)
	at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:133)
	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:104)
	at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)",,pin_zhang,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 22:11:51 UTC 2016,,,,,,,,,,"0|i2egvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 22:11;smilegator;This has been resolved in the latest branch. Thanks! If you still hit the same issue, please reopen it. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark shell not having hive crashes SQLContext,SPARK-7470,12828231,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,08/May/15 02:49,08/May/15 05:33,14/Jul/23 06:26,08/May/15 05:33,1.3.0,,,,,,1.4.0,,,,,,Spark Shell,SQL,,,0,,,,,,"If hive is not found on my class path, I get the following exception and I don't get to use the SQLContext anymore. In fact, we already catch `java.lang.ClassNotFoundException` in case this happens. We just don't also catch `java.lang.NoClassDefFoundError`. This happens if you have `SPARK_PREPEND_CLASSES` set.

{code}
15/05/07 17:07:30 INFO BlockManagerMaster: Registered BlockManager
15/05/07 17:07:30 INFO EventLoggingListener: Logging events to file:/tmp/spark-events/local-1431043649919
15/05/07 17:07:30 INFO SparkILoop: Created spark context..
Spark context available as sc.
java.lang.NoClassDefFoundError: org/apache/hadoop/hive/conf/HiveConf
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2493)
	at java.lang.Class.getConstructor0(Class.java:2803)
	at java.lang.Class.getConstructor(Class.java:1718)
	at org.apache.spark.repl.SparkILoop.createSQLContext(SparkILoop.scala:1026)
	at $iwC$$iwC.<init>(<console>:9)
	at $iwC.<init>(<console>:18)
{code}",,andrewor14,apachespark,glenn.strycker@gmail.com,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 05:33:50 UTC 2015,,,,,,,,,,"0|i2eghz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"08/May/15 02:57;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/5997;;;","08/May/15 05:33;yhuai;Issue resolved by pull request 5997
[https://github.com/apache/spark/pull/5997];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
need to sort when convert to array in topByKey,SPARK-7452,12828148,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,coderxiang,coderxiang,coderxiang,07/May/15 21:54,08/May/15 03:58,14/Jul/23 06:26,08/May/15 03:55,1.4.0,,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,"the toArray function of the PriorityQueue does not necessarily preserve order. Add a counter-example as the test, which would fail the original impl.",,apachespark,coderxiang,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 03:55:37 UTC 2015,,,,,,,,,,"0|i2eg0n:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"07/May/15 22:38;apachespark;User 'coderxiang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5990;;;","08/May/15 03:55;josephkb;Issue resolved by pull request 5990
[https://github.com/apache/spark/pull/5990];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on Yarn : Preemption of executors is counted as failure causing Spark job to fail,SPARK-7451,12828136,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ashwinshankar77,ashwinshankar77,ashwinshankar77,07/May/15 21:22,09/May/15 08:57,14/Jul/23 06:26,09/May/15 08:57,1.3.1,,,,,,1.4.0,,,,,,YARN,,,,0,,,,,,"Preemption of executors by YARN's scheduler is counted as failure in Spark, which causes spark job to fail once failure count crosses 'spark.yarn.max.executor.failures'.",,apachespark,ashwinshankar77,dweeks,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 00:08:58 UTC 2015,,,,,,,,,,"0|i2efy7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/15 21:22;ashwinshankar77;Working on it.;;;","08/May/15 00:08;apachespark;User 'ashwinshankar77' has created a pull request for this issue:
https://github.com/apache/spark/pull/5993;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Large Job submission lag when using Parquet w/ Schema Merging,SPARK-7447,12828095,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,brdwrd,brdwrd,07/May/15 19:38,17/May/15 07:43,14/Jul/23 06:26,17/May/15 07:43,1.3.0,1.3.1,,,,,1.4.0,,,,,,PySpark,Spark Core,Spark Submit,,0,,,,,,"I have 2.6 billion rows in parquet format and I'm trying to use the new schema merging feature (I was enforcing a consistent schema manually before in 0.8-1.2 which was annoying). 

I have approximate 200 parquet files with key=<date>. When I load the dataframe with the sqlcontext that process is understandably slow because I assume it's reading all the meta data from the parquet files and doing the initial schema merging. So that's ok.

However the problem I have is that once I have the dataframe. Doing any operation on the dataframe seems to have a 10-30 second lag before it actually starts processing the Job and shows up as an Active Job in the Spark Manager. This was an instant operation in all previous versions of Spark. Once the job actually is running the performance is fantastic, however this job submission lag is horrible.

I'm wondering if there is a bug with recomputing the schema merging. Running top on the master node shows some thread maxed out on 1 cpu during the lagging time which makes me think it's not net i/o but something pre-processing before job submission.
","Spark 1.3.1, aws, persistent hdfs version 2 with ebs storage, pyspark, 8 x c3.8xlarge nodes. 

spark-conf

spark.executor.memory 50g

spark.driver.cores 32
spark.driver.memory 50g

spark.default.parallelism 512
spark.sql.shuffle.partitions 512

spark.task.maxFailures  30

spark.executor.logs.rolling.maxRetainedFiles 2
spark.executor.logs.rolling.size.maxBytes 102400
spark.executor.logs.rolling.strategy size

spark.shuffle.spill false

spark.sql.parquet.cacheMetadata true
spark.sql.parquet.filterPushdown true
spark.sql.codegen true

spark.akka.threads = 64",apachespark,brdwrd,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 17 07:43:21 UTC 2015,,,,,,,,,,"0|i2efpb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/15 17:26;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/6012;;;","08/May/15 18:39;brdwrd;Thanks, you are a hero.;;;","17/May/15 07:43;lian cheng;Issue resolved by pull request 6012
[https://github.com/apache/spark/pull/6012];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Validation Error while running  countApproxDistinct  with relative accuracy  >= 0.38  ,SPARK-7438,12827950,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vinodkc,vinodkc,vinodkc,07/May/15 07:12,09/May/15 09:04,14/Jul/23 06:26,09/May/15 09:03,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Eg Code: 
val a = sc.parallelize(1 to 10000, 20)
val b = a ++ a ++ a ++ a ++ a
b.countApproxDistinct(0.38)
""java.lang.IllegalArgumentException: requirement failed: p (3) must be at least 4""

Issue 1: When relative accuracy  >= 0.38, IAE is thrown, as the precision p evaluates to 3.
However,same input in countApproxDistinctByKey(0.38), works fine. Usage of relativeSD should be consistent in both countApproxDistinct and countApproxDistinctByKey
Issue 2: Validation error message ""p (3) must be at least 4"" is not giving a clue on what went wrong.
Issue 3: When relative accuracy < 0.000017, a proper validation error message is not shown from countApproxDistinct
",,apachespark,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 09 09:03:50 UTC 2015,,,,,,,,,,"0|i2eetr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/15 07:15;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/5974;;;","09/May/15 09:03;srowen;Issue resolved by pull request 5974
[https://github.com/apache/spark/pull/5974];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot implement nor use custom StandaloneRecoveryModeFactory implementations,SPARK-7436,12827945,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jlewandowski,jlewandowski,jlewandowski,07/May/15 06:56,08/May/15 18:39,14/Jul/23 06:26,08/May/15 18:39,1.3.1,,,,,,1.3.2,1.4.0,,,,,Deploy,,,,0,,,,,,"At least, this code fragment is buggy ({{Master.scala}}):

{code}
      case ""CUSTOM"" =>
        val clazz = Class.forName(conf.get(""spark.deploy.recoveryMode.factory""))
        val factory = clazz.getConstructor(conf.getClass, Serialization.getClass)
          .newInstance(conf, SerializationExtension(context.system))
          .asInstanceOf[StandaloneRecoveryModeFactory]
        (factory.createPersistenceEngine(), factory.createLeaderElectionAgent(this))
{code}

Because here: {{val factory = clazz.getConstructor(conf.getClass, Serialization.getClass)}} it tries to find the constructor which accepts {{org.apache.spark.SparkConf}} and class of companion object of {{akka.serialization.Serialization}} and then it tries to instantiate {{newInstance(conf, SerializationExtension(context.system))}} with instance of {{SparkConf}} and instance of {{Serialization}} class - not the companion objects. 
",,apachespark,jlewandowski,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 18:39:07 UTC 2015,,,,,,,,,,"0|i2eesn:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"07/May/15 09:08;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/5975;;;","07/May/15 09:29;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/5976;;;","07/May/15 09:47;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/5977;;;","07/May/15 17:03;jlewandowski;[~joshrosen] It is awaiting the review now;;;","08/May/15 18:39;joshrosen;Issue resolved by pull request 5975
[https://github.com/apache/spark/pull/5975];;;",,,,,,,,,,,,,,,,,,,,,,,,
Flaky test in PySpark CrossValidator doc test,SPARK-7432,12827914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,josephkb,josephkb,07/May/15 03:27,02/Jun/15 15:51,14/Jul/23 06:26,02/Jun/15 15:51,1.4.0,,,,,,1.4.0,,,,,,ML,PySpark,,,0,,,,,,"There was a test failure in the doc test in Python CrossValidator:
[https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/32058/consoleFull]

Here's the full doc test:
{code}
    >>> from pyspark.ml.classification import LogisticRegression
    >>> from pyspark.ml.evaluation import BinaryClassificationEvaluator
    >>> from pyspark.mllib.linalg import Vectors
    >>> dataset = sqlContext.createDataFrame(
    ...     [(Vectors.dense([0.0, 1.0]), 0.0),
    ...      (Vectors.dense([1.0, 2.0]), 1.0),
    ...      (Vectors.dense([0.55, 3.0]), 0.0),
    ...      (Vectors.dense([0.45, 4.0]), 1.0),
    ...      (Vectors.dense([0.51, 5.0]), 1.0)] * 10,
    ...     [""features"", ""label""])
    >>> lr = LogisticRegression()
    >>> grid = ParamGridBuilder().addGrid(lr.maxIter, [0, 1, 5]).build()
    >>> evaluator = BinaryClassificationEvaluator()
    >>> cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)
    >>> cvModel = cv.fit(dataset)
    >>> expected = lr.fit(dataset, {lr.maxIter: 5}).transform(dataset)
    >>> cvModel.transform(dataset).collect() == expected.collect()
    True
{code}

Here's the failure message:
{code}
Running test: pyspark/ml/tuning.py ... **********************************************************************
File ""pyspark/ml/tuning.py"", line 108, in __main__.CrossValidator
Failed example:
    cvModel.transform(dataset).collect() == expected.collect()
Expected:
    True
Got:
    False
**********************************************************************
   1 of  11 in __main__.CrossValidator
***Test Failed*** 1 failures.
Had test failures; see logs.
[error] Got a return code of 255 on line 240 of the run-tests script.
{code}
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 15:51:26 UTC 2015,,,,,,,,,,"0|i2eelb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"07/May/15 03:33;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5962;;;","07/May/15 05:15;josephkb;It happened again: [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/32067/console];;;","02/Jun/15 05:14;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/6572;;;","02/Jun/15 15:51;mengxr;Issue resolved by pull request 6572
[https://github.com/apache/spark/pull/6572];;;",,,,,,,,,,,,,,,,,,,,,,,,,
PySpark CrossValidatorModel needs to call parent init,SPARK-7431,12827913,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,07/May/15 03:19,10/May/15 20:29,14/Jul/23 06:26,10/May/15 20:29,1.4.0,,,,,,1.4.0,,,,,,ML,PySpark,,,0,,,,,,Try running the CrossValidator doc test in the pyspark shell.  Then type cvModel to print the model.  It will fail in {{Identifiable.__repr__}} since there is no uid defined!,,apachespark,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 10 20:29:58 UTC 2015,,,,,,,,,,"0|i2eel3:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"07/May/15 03:29;josephkb;I'm working on this;;;","07/May/15 06:40;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/5968;;;","10/May/15 20:29;josephkb;Issue resolved by pull request 5968
[https://github.com/apache/spark/pull/5968];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.streaming.CheckpointSuite,SPARK-7419,12827881,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,andrewor14,andrewor14,07/May/15 00:24,09/Jul/15 20:23,14/Jul/23 06:26,09/Jul/15 20:22,,,,,,,1.4.2,1.5.0,,,,,Tests,,,,0,flaky-test,,,,,"Failing with error messages like
{code}
5 did not equal 7 Number of outputs do not match
{code}

Various tests in the suite seem to be failing with similar error messages:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.3,label=centos/2228/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.0,label=centos/2230/",,andrewor14,apachespark,joshrosen,shixiong@databricks.com,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 13:02:06 UTC 2015,,,,,,,,,,"0|i2eedz:",9223372036854775807,,,,,,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"09/Jul/15 01:23;joshrosen;[~tdas], it looks like SPARK-1600 has started re-occurring too.

org.apache.spark.streaming.CheckpointSuite.recovery with file input stream failed recently:

{code}
Set(10, 1, 6, 21, 45, 3, 36, 15) did not equal Set(10, 1, 6, 28, 21, 45, 3, 36, 15)
{code}

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/2886/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.3,label=centos/testReport/junit/org.apache.spark.streaming/CheckpointSuite/recovery_with_file_input_stream/;;;","09/Jul/15 01:39;tdas;Could you look into this please!


;;;","09/Jul/15 01:55;shixiong@databricks.com;OK. I will look at it.

Best Regards,
Shixiong(Ryan) Zhu


;;;","09/Jul/15 13:02;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/7323;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.deploy.SparkSubmitUtilsSuite search for artifacts,SPARK-7418,12827877,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,brkyvz,andrewor14,andrewor14,07/May/15 00:05,04/Jun/15 23:54,14/Jul/23 06:26,04/Jun/15 23:54,1.4.0,,,,,,1.3.2,1.4.0,,,,,Tests,,,,0,,,,,,"{code}
   java.lang.RuntimeException: [unresolved dependency: com.agimatec#agimatec-validation;0.9.3: not found]
      at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:931)
      at org.apache.spark.deploy.SparkSubmitUtilsSuite$$anonfun$5.apply$mcV$sp(SparkSubmitUtilsSuite.scala:108)
      at org.apache.spark.deploy.SparkSubmitUtilsSuite$$anonfun$5.apply(SparkSubmitUtilsSuite.scala:107)
      at 
{code}

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-Maven-with-YARN/2075/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/junit/org.apache.spark.deploy/SparkSubmitUtilsSuite/search_for_artifact_at_other_repositories/
...",,andrewor14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 23:54:23 UTC 2015,,,,,,,,,,"0|i2eed3:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"04/Jun/15 23:54;andrewor14;This should be resolved by:

branch-1.4+: 8014e1f6bb871d9fd4db74106eb4425d0c1e9dd6 (#5892)
branch-1.3: 5b96b6933a1c0f05512823117c8c66f4b44e2937 (#6657);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.deploy.SparkSubmitUtilsSuite neglect dependencies,SPARK-7417,12827875,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,brkyvz,andrewor14,andrewor14,07/May/15 00:04,12/Aug/20 21:18,14/Jul/23 06:26,04/Jun/15 23:55,1.4.0,,,,,,1.3.2,1.4.0,,,,,Tests,,,,0,flaky-test,,,,,"{code}
Expected exception java.lang.RuntimeException to be thrown, but no exception was thrown.
{code}

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/hadoop.version=1.0.4,label=centos/2201/testReport/junit/org.apache.spark.deploy/SparkSubmitUtilsSuite/neglects_Spark_and_Spark_s_dependencies/
...
",,andrewor14,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 23:55:09 UTC 2015,,,,,,,,,,"0|i2eecn:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"04/Jun/15 23:55;andrewor14;This should be resolved by:
branch-1.4+: 8014e1f6bb871d9fd4db74106eb4425d0c1e9dd6 (#5892)
branch-1.3: 5b96b6933a1c0f05512823117c8c66f4b44e2937 (#6657);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS parser is incomplete,SPARK-7411,12827863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,chenghao,marmbrus,marmbrus,06/May/15 23:30,12/May/15 02:21,14/Jul/23 06:26,12/May/15 02:21,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,The change to use an isolated classloader removed the use of the Semantic Analyzer for parsing CTAS queries.  We should fix this before the release.,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 12 02:21:41 UTC 2015,,,,,,,,,,"0|i2ee9z:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"07/May/15 04:26;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/5963;;;","12/May/15 02:21;marmbrus;Issue resolved by pull request 5963
[https://github.com/apache/spark/pull/5963];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug that ReceiverInputDStream doesn't report InputInfo,SPARK-7405,12827798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,06/May/15 20:32,07/May/15 01:07,14/Jul/23 06:26,07/May/15 01:07,,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,The bug is because SPARK-7139 removed some codes from SPARK-7112 unintentionally here: https://github.com/apache/spark/commit/1854ac326a9cc6014817d8df30ed0458eee5d7d1#diff-5c8651dd78abd20439b8eb938175075dL72,,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7397,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 06 20:34:45 UTC 2015,,,,,,,,,,"0|i2edvr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"06/May/15 20:34;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5950;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Link URL in objects on Timeline View is wrong in case of running on YARN ,SPARK-7403,12827769,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,06/May/15 18:50,09/May/15 09:11,14/Jul/23 06:26,09/May/15 09:10,1.4.0,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"When we use Spark on YARN and have AllJobPage via ResourceManager's proxy, the link URL in objects which represent each job on timeline view is wrong.

In timeline-view.js, the link is generated as follows.
{code}
window.location.href = ""job/?id="" + getJobId(this);
{code}
This assumes the URL displayed on the web browser ends with ""jobs/"" but when we access AllJobPage via the proxy, the url displayed does not end with ""jobs/"".",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 09 09:10:49 UTC 2015,,,,,,,,,,"0|i2edpj:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"06/May/15 19:00;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/5947;;;","09/May/15 09:10;srowen;Issue resolved by pull request 5947
[https://github.com/apache/spark/pull/5947];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Master fails on 2.11 with compilation error,SPARK-7399,12827655,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tijoparacka,dragos,dragos,06/May/15 12:55,14/May/15 08:52,14/Jul/23 06:26,14/May/15 08:52,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,1,,,,,,"The current code in master (and 1.4 branch) fails on 2.11 with the following compilation error:

{code}
[error] /home/ubuntu/workspace/Apache Spark (master) on 2.11/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala:78: in object RDDOperationScope, multiple overloaded alternatives of method withScope define default arguments.
[error] private[spark] object RDDOperationScope {
[error]                       ^
{code}",,apachespark,dragos,huangjs,rishitesh,Tijo Paracka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7614,,,,,SPARK-7614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 14 08:52:43 UTC 2015,,,,,,,,,,"0|i2ed0f:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"07/May/15 05:50;apachespark;User 'tijoparacka' has created a pull request for this issue:
https://github.com/apache/spark/pull/5966;;;","13/May/15 20:01;huangjs;Looks like another change makes it broken again.

Jianshi;;;","13/May/15 20:15;srowen;Reopening due to https://github.com/apache/spark/pull/5966#issuecomment-101712549 ; [~andrewor14] is on the case.;;;","13/May/15 20:27;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6129;;;","14/May/15 03:20;Tijo Paracka;Jira Resolution status changed from Fixed to ""Pending Close"" . I think this is due to the bug in JIRA. Could you update the status.;;;","14/May/15 08:52;srowen;Just reopening to mark this Fixed rather than Pending Closed;;;",,,,,,,,,,,,,,,,,,,,,,,
Kryo buffer size can not be larger than 2M,SPARK-7392,12827543,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,liyezhang556520,liyezhang556520,liyezhang556520,06/May/15 05:17,24/May/15 03:27,14/Jul/23 06:26,08/May/15 08:12,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"when set *spark.kryoserializer.buffer* larger than 2048k, *IllegalArgumentException* will be thrown.",,apachespark,liyezhang556520,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 08:12:02 UTC 2015,,,,,,,,,,"0|i2ecan:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/15 05:22;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/5934;;;","08/May/15 08:12;srowen;Resolved by https://github.com/apache/spark/pull/5934;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
CovarianceCounter in StatFunctions might calculate incorrect result,SPARK-7390,12827527,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,06/May/15 03:09,08/May/15 21:42,14/Jul/23 06:26,08/May/15 21:41,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"CovarianceCounter in StatFunctions has a merging stage. In this merge function, the other CovarianceCounter object sometimes has zero count that causes the final CovarianceCounter with incorrect result.

",,apachespark,mengxr,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 08 21:41:51 UTC 2015,,,,,,,,,,"0|i2ec73:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/15 03:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5931;;;","08/May/15 21:41;mengxr;Issue resolved by pull request 5931
[https://github.com/apache/spark/pull/5931];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky tests for distributed mode in BroadcastSuite,SPARK-7384,12827450,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,05/May/15 22:18,06/May/15 21:55,14/Jul/23 06:26,06/May/15 21:50,,,,,,,1.4.0,,,,,,Spark Core,Tests,,,0,flaky-test,,,,,"Fixed the following failure: https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.3-Maven-pre-YARN/hadoop.version=1.0.4,label=centos/452/testReport/junit/org.apache.spark.broadcast/BroadcastSuite/Unpersisting_HttpBroadcast_on_executors_and_driver_in_distributed_mode/

The tests should wait until all slaves are up. Otherwise, there may be only a part of {{BlockManager}}s registered, and fail the tests.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 05 22:20:29 UTC 2015,,,,,,,,,,"0|i2ebuf:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"05/May/15 22:20;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5925;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"HistoryServer does not handle ""deep"" link when lazy loading app",SPARK-7378,12827436,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,vanzin,vanzin,05/May/15 21:27,08/May/15 21:13,14/Jul/23 06:26,08/May/15 21:13,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"This is a regression caused by SPARK-4705. When you go to a deep link into an app that is not loaded yet, that used to work, but now that returns a 404. You need to go into the root of the app first for the app to be loaded, which is not the expected behaviour.

Fix coming up.",,andrewor14,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 19:45:28 UTC 2015,,,,,,,,,,"0|i2ebrr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"05/May/15 21:33;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5922;;;","07/May/15 19:45;andrewor14;Bumping to blocker because it's a regression from 1.3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid defensive copying in SQL exchange operator when sort-based shuffle buffers data in serialized form,SPARK-7375,12827420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,05/May/15 20:41,09/May/15 02:10,14/Jul/23 06:26,09/May/15 02:10,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"The original sort-based shuffle buffers shuffle input records in memory while sorting them. This causes problems when mutable records are presented to the shuffle, which happens in Spark SQL's Exchange operator. To work around this issue, SPARK-2967 and SPARK-4479 added defensive copying of shuffle inputs in the Exchange operator when sort-based shuffle is enabled.

I think that [~sandyr]'s recent patch for enabling serialization of records in sort-based shuffle (SPARK-4550) and my proposed {{unsafe}}-based shuffle path (SPARK-7081) may allow us to avoid this defensive copying in certain cases (since our patches cause records to be serialized one-at-a-time and remove the buffering of deserialized records).

As mentioned in SPARK-4479, a long-term fix for this issue might be to add hooks for informing the shuffle about object (im)mutability in order to allow the shuffle layer to decide whether to copy. In the meantime, though, I think that we should just extend the checks added in SPARK-4479 to avoid copies when these new serialized sort paths are used.

/cc [~rxin] [~marmbrus] [~yhuai]",,apachespark,joshrosen,sandyr,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-7075,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 09 02:10:35 UTC 2015,,,,,,,,,,"0|i2ebo7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"06/May/15 20:05;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5948;;;","09/May/15 02:10;yhuai;Issue resolved by pull request 5948
[https://github.com/apache/spark/pull/5948];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to custom bind the column in cached query result (temp table),SPARK-7365,12827244,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,boorubasar,boorubasar,05/May/15 10:17,07/Oct/16 22:09,14/Jul/23 06:26,07/Oct/16 22:09,1.2.2,,,,,,,,,,,,SQL,,,,0,,,,,,"Doing a query to cassandra table as

SchemaRDD s = cassandraSQLContext.sql(""select user.id as user_id from user"");
// user.id is UUID in table definition
s.registerTempTable( ""my_user"" );
cassandraSQLContext.cacheTable( ""my_user"" );
s.count(); // throws the following exception

Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ClassCastException: java.util.UUID cannot be cast to java.lang.String
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getString(Row.scala:183)
	at org.apache.spark.sql.columnar.StringColumnStats.gatherStats(ColumnStats.scala:208)
	at org.apache.spark.sql.columnar.NullableColumnBuilder$class.appendFrom(NullableColumnBuilder.scala:56)
	at org.apache.spark.sql.columnar.NativeColumnBuilder.org$apache$spark$sql$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:87)
	at org.apache.spark.sql.columnar.compression.CompressibleColumnBuilder$class.appendFrom(CompressibleColumnBuilder.scala:78)
	at org.apache.spark.sql.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:87)
	at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:125)
	at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:112)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:163)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:245)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:198)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


How can I map the query selected row items, something like we do with mapColumnTo() in

rdd = CassandraJavaUtil.javaFunctions(sc)
                    .cassandraTable( ""ks"", ""user"", mapColumnTo(UUID.class) ).select( ""user_id"");",,boorubasar,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 22:09:02 UTC 2016,,,,,,,,,,"0|i2eanb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 22:09;smilegator;Could you retry it using the latest master branch with Cassandra connectors? If still hit the same issue, please reopen this issue. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw unambiguous exception when attempting to start multiple StreamingContexts in the same JVM,SPARK-7361,12827196,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,05/May/15 09:04,11/May/15 17:59,14/Jul/23 06:26,11/May/15 17:59,,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,Currently attempt to start a streamingContext while another one is started throws a confusing exception that the action name JobScheduler is already registered. Instead its best to throw a proper exception as it is not supported.,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6755,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 05 09:06:48 UTC 2015,,,,,,,,,,"0|i2eai7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"05/May/15 09:06;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/5907;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark cannot detect renamed columns using JDBC connector,SPARK-7345,12827039,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,osidorkin,osidorkin,osidorkin,04/May/15 20:26,10/May/15 08:32,14/Jul/23 06:26,10/May/15 08:32,1.3.1,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,"sqlContext.load(""jdbc"", Map(""url"" -> ""some url"", ""dbtable"" -> ""(select column as column1, column as column2 from table)"")) creates DataFrame with wrong schema which fails on action.
 
Most likely JDBC SQL connector uses getColumnName instead of getColumnLabel to deduce DataFrame schema from ResultSetMetaData.",,apachespark,gvramana,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 09 18:10:27 UTC 2015,,,,,,,,,,"0|i2e9jr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/May/15 18:10;apachespark;User 'osidorkin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6032;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
C4 instance types need to be added to spark_ec2.py,SPARK-7343,12827020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shivaram,jey,jey,04/May/15 19:43,25/May/15 07:34,14/Jul/23 06:26,22/May/15 20:44,1.3.0,1.4.0,,,,,1.4.0,,,,,,EC2,,,,0,,,,,,"The C4 instance types have not been added to the spark_ec2.py script, so launching a cluster containing ""c4.4xlarge"" nodes will result in an error that the ""Virtualization type 'hvm' is required for instances of type 'c4.4xlarge'.""",,jey,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 20:44:52 UTC 2015,,,,,,,,,,"0|i2e9fj:",9223372036854775807,,,,,,,,,,,,,,1.3.2,,,,,,,,,,,,,"22/May/15 20:44;joshrosen;This was fixed for 1.4.0 by https://github.com/apache/spark/pull/6014;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Sometimes the status of finished job show on JobHistory UI will be active, and never update.",SPARK-7336,12826884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ArcherShao,ArcherShao,ArcherShao,04/May/15 09:41,03/Nov/15 23:29,14/Jul/23 06:26,02/Sep/15 18:04,,,,,,,1.6.0,,,,,,Web UI,,,,0,,,,,,"When I run a SparkPi job, the status of the job on JobHistory UI was 'active'. After the job finished for a long time, the status on JobHistory UI never update again, and the job keep in the 'Incomplete applications' list. 
This problem appears occasionally. And the configuration of JobHistory is default value.",,apachespark,ArcherShao,eggsby,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Nov 03 23:29:59 UTC 2015,,,,,,,,,,"0|i2e8lz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/15 10:14;ArcherShao;There is a timer in 'FsHistoryProvider', and it will update appliations' status periodically by check log files. 
One value called 'lastModifiedTime' will update every check task,  record the biggest last modified time of all log files. 
Files that last modified time earlier than 'lastModifiedTime'  will be ignored.
Suppose we have N applications, called app1, app2,...,appN. 
At one check task, app1 checked at time T, appN check at time T + t. app1 last modified at time T + t / 4, appN last modified at time T + t / 2.
The status of app1 will not update this time, and 'lastModifiedTime' will update to T + t / 2.
At next check task, the status of app1 will still not update, because of T + t / 4 < T + t / 2.
So we should record every application's last modified time, not the biggest last modified time of all log files.
I will present a PR later.;;;","04/May/15 10:19;apachespark;User 'ArcherShao' has created a pull request for this issue:
https://github.com/apache/spark/pull/5886;;;","03/Nov/15 23:29;eggsby;This consistently breaks the history UI for me in 1.5.x - any reason why this fix is targeted at 1.6.0 instead of 1.5.2 ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC RDD could lead to NPE when the date field is null,SPARK-7330,12826834,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,04/May/15 02:26,07/May/15 17:20,14/Jul/23 06:26,07/May/15 17:08,,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,because we call DateUtils.fromDate(rs.getDate(xx)) no matter it is null or not.,,adrian-wang,apachespark,michaelmalak,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7364,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 17:08:09 UTC 2015,,,,,,,,,,"0|i2e8bb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/15 02:28;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5877;;;","07/May/15 17:08;yhuai;It has been resolved by https://github.com/apache/spark/commit/ed9be06a4797bbb678355b361054c8872ac20b75.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Performing window() on a WindowedDStream doesn't work all the time,SPARK-7326,12826769,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wesleymiao,wesleymiao,wesleymiao,03/May/15 06:17,11/May/15 11:21,14/Jul/23 06:26,11/May/15 11:20,1.3.1,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,"Someone reported similar issues before but got no response.
http://apache-spark-user-list.1001560.n3.nabble.com/Windows-of-windowed-streams-not-displaying-the-expected-results-td466.html

And I met the same issue recently and it can be reproduced in 1.3.1 by the following piece of code:

def main(args: Array[String]) {

    val batchInterval = ""1234""
    val sparkConf = new SparkConf()
      .setAppName(""WindowOnWindowedDStream"")
      .setMaster(""local[2]"")

    val ssc =  new StreamingContext(sparkConf, Milliseconds(batchInterval.toInt))
    ssc.checkpoint(""checkpoint"")

    def createRDD(i: Int) : RDD[(String, Int)] = {

      val count = 1000
      val rawLogs = (1 to count).map{ _ =>
        val word = ""word"" + Random.nextInt.abs % 5

        (word, 1)
      }
      ssc.sparkContext.parallelize(rawLogs)
    }

    val rddQueue = mutable.Queue[RDD[(String, Int)]]()
    val rawLogStream = ssc.queueStream(rddQueue)

    (1 to 300) foreach { i =>
      rddQueue.enqueue(createRDD(i))
    }

    val l1 = rawLogStream.window(Milliseconds(batchInterval.toInt) * 5, Milliseconds(batchInterval.toInt) * 5).reduceByKey(_ + _)

    val l2 = l1.window(Milliseconds(batchInterval.toInt) * 15, Milliseconds(batchInterval.toInt) * 15).reduceByKey(_ + _)

    l1.print()
    l2.print()

    ssc.start()
    ssc.awaitTermination()
}

Here we have two windowed DStream instance l1 and l2. 

l1 is the result DStream by performing a window() on the source DStream with both window and sliding duration 5 times the batch internal of the source stream.

l2 is the result DStream by performing a window() on l1, with both window and sliding duration 3 times l1's batch interval, which is 15 times of the source stream.

From the output of this simple streaming app, I can only see print data output from l1 and no data printed from l2.

Diving into the source code, I found the problem may most likely reside in DStream.slice() implementation, as shown below.

  def slice(fromTime: Time, toTime: Time): Seq[RDD[T]] = {
    if (!isInitialized) {
      throw new SparkException(this + "" has not been initialized"")
    }
    if (!(fromTime - zeroTime).isMultipleOf(slideDuration)) {
      logWarning(""fromTime ("" + fromTime + "") is not a multiple of slideDuration (""
        + slideDuration + "")"")
    }
    if (!(toTime - zeroTime).isMultipleOf(slideDuration)) {
      logWarning(""toTime ("" + fromTime + "") is not a multiple of slideDuration (""
        + slideDuration + "")"")
    }
    val alignedToTime = toTime.floor(slideDuration, zeroTime)
    val alignedFromTime = fromTime.floor(slideDuration, zeroTime)

    logInfo(""Slicing from "" + fromTime + "" to "" + toTime +
      "" (aligned to "" + alignedFromTime + "" and "" + alignedToTime + "")"")

    alignedFromTime.to(alignedToTime, slideDuration).flatMap(time => {
      if (time >= zeroTime) getOrCompute(time) else None
    })
  }

Here after performing floor() on both fromTime and toTime, the result (alignedFromTime - zeroTime) and (alignedToTime - zeroTime) may no longer be multiple of the slidingDuration, thus making isTimeValid() check failed for all the remaining computation.

The fix would be to add a new floor() function in Time.scala to respect the zeroTime while performing the floor :

  def floor(that: Duration, zeroTime: Time): Time = {
    val t = that.milliseconds
    new Time(((this.millis - zeroTime.milliseconds) / t) * t + zeroTime.milliseconds)
  } 

And then change the DStream.slice to call this new floor function by passing in its zeroTime.

    val alignedToTime = toTime.floor(slideDuration, zeroTime)
    val alignedFromTime = fromTime.floor(slideDuration, zeroTime)

This way the alignedToTime and alignedFromTime are *really* aligned in respect to zeroTime whose value is not really a 0.
 
",,apachespark,wesleymiao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 11:20:33 UTC 2015,,,,,,,,,,"0|i2e7wv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/15 12:22;apachespark;User 'wesleymiao' has created a pull request for this issue:
https://github.com/apache/spark/pull/5871;;;","03/May/15 12:48;srowen;Out of curiosity why do you have a window + slide of 5x the batch duration? that is simply the same as making an underlying stream with 5x the batch duration. Maybe this is just a toy example. I know it's not directly relevant to what you're reporting.;;;","03/May/15 22:39;wesleymiao;What I'd like to achieve is to do multiple-level aggregations on the source of logging stream. For example - let's say the source logging stream is at interval 1 second. The first level aggregation would be every 1 minute, which is the 60 intervals of the source stream. The second level would be every 1 hour. The third level would be 1 day. And we can do more levels if we want.

What I hope is that at each level we'll do reduceByKey(_ + _) so that its aggregation can be done against its immediate previous level, instead of always aggregating against the source stream. Level 3's reduceByKey will be based on level 2's result, level 2 is based one level 1 and level 1 is based on the source stream.

I would think this approach will be more efficient than always reduce over the source stream, particularly for the higher level (like daily and weekly) aggregation.  ;;;","04/May/15 05:39;srowen;Makes sense, just interested in whether this was simplifiable, but it is beside the point.;;;","11/May/15 11:20;srowen;Issue resolved by pull request 5871
[https://github.com/apache/spark/pull/5871];;;",,,,,,,,,,,,,,,,,,,,,,,,
DStream isn't cleaning closures correctly,SPARK-7318,12826701,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,02/May/15 05:14,05/May/15 16:38,14/Jul/23 06:26,05/May/15 16:38,1.0.0,,,,,,1.4.0,,,,,,DStreams,Spark Core,,,0,,,,,,"{code}
  def transform[U: ClassTag](transformFunc: RDD[T] => RDD[U]): DStream[U] = {
    transform((r: RDD[T], t: Time) => context.sparkContext.clean(transformFunc(r), false))
  }
{code}
This is cleaning an RDD instead!",,andrewor14,apachespark,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 02 05:27:00 UTC 2015,,,,,,,,,,"0|i2e7hz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"02/May/15 05:27;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/5860;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SPARK-6913 broke jdk6 build,SPARK-7312,12826641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,tgraves,tgraves,01/May/15 21:03,02/May/15 23:43,14/Jul/23 06:26,01/May/15 23:47,1.4.0,,,,,,1.4.0,,,,,,Build,,,,0,,,,,,"https://github.com/apache/spark/pull/5782 uses java.sql.Driver.getParentLogger  which doesn't exist in jdk6, only jdk7

[error] /home/tgraves/tgravescs_spark/sql/core/src/main/scala/org/apache/spark/sql/jdbc/jdbc.scala:198: value getParentLogger is not a member of java.sql.Driver
[error] override def getParentLogger: java.util.logging.Logger = wrapped.getParentLogger
[error] ^",,apachespark,marmbrus,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6913,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 23:47:12 UTC 2015,,,,,,,,,,"0|i2e74n:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"01/May/15 22:01;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/5847;;;","01/May/15 23:47;marmbrus;Issue resolved by pull request 5847
[https://github.com/apache/spark/pull/5847];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Should there be multiple concurrent attempts for one stage?,SPARK-7308,12826627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,irashid,irashid,01/May/15 19:27,13/Nov/15 22:35,14/Jul/23 06:26,13/Nov/15 22:10,1.3.1,,,,,,1.5.3,1.6.0,,,,,Spark Core,,,,1,,,,,,"Currently, when there is a fetch failure, you can end up with multiple concurrent attempts for the same stage.  Is this intended?  At best, it leads to some very confusing behavior, and it makes it hard for the user to make sense of what is going on.  At worst, I think this is cause of some very strange errors we've seen errors we've seen from users, where stages start executing before all the dependent stages have completed.

This can happen in the following scenario:  there is a fetch failure in attempt 0, so the stage is retried.  attempt 1 starts.  But, tasks from attempt 0 are still running -- some of them can also hit fetch failures after attempt 1 starts.  That will cause additional stage attempts to get fired up.

There is an attempt to handle this already https://github.com/apache/spark/blob/16860327286bc08b4e2283d51b4c8fe024ba5006/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1105

but that only checks whether the **stage** is running.  It really should check whether that **attempt** is still running, but there isn't enough info to do that.  

I'll also post some info on how to reproduce this.",,andrewor14,apachespark,carsonwang,darabos,dweeks,irashid,joshrosen,kamilmroczek,lianhuiwang,liyezhang556520,rdub,richardatcloudera,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5945,,,,,,,,,,,,,,,SPARK-7829,,,,,,,,SPARK-8103,SPARK-8029,,,,"21/May/15 19:34;irashid;SPARK-7308_discussion.pdf;https://issues.apache.org/jira/secure/attachment/12734625/SPARK-7308_discussion.pdf",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 22:10:36 UTC 2015,,,,,,,,,,"0|i2e71j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/15 19:27;irashid;ping [~kayousterhout];;;","01/May/15 19:44;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/5844;;;","01/May/15 20:10;irashid;I've got some code to reproduce the error in this branch: https://github.com/squito/spark/tree/f82d184fa0ded36afd056419f7e326651ce561fd

I just hacked the internals to simulate fetch failures, so that is definitely not meant for merging (and the code is really ugly too, sorry about that).  If you run [this test|https://github.com/squito/spark/blob/f82d184fa0ded36afd056419f7e326651ce561fd/core/src/test/scala/org/apache/spark/scheduler/DAGSchedulerSuite.scala#L771] you'll see concurrent attempts for a stage.

Here's what happens:
1) stage 0 (shuffle write) runs normally
2) stage 1 (shuffle read), attempt 0 has some fetch failures
3) stage 1 gets failed, stage 0 gets resubmitted for the map outputs that are now missing
4) stage 0 attempt 1 finishes successfully, filling in the missing map outputs
5) stage 1 attempt 1 starts.  Note that attempt 0 could still have running tasks, so we've already got 2 attempts going -- but its going to get much worse.
6) *stage 1 attempt 0 continues to have fetch failures*.  This will happen as long as stage 1 has a lot of tasks, and they aren't all trivial.  Its a little hard to produce in a test case, but actually not an unusual situation at all in real user code.
7) now we have even more retries of stage 0, still from the fetch failures of stage 1, attempt 0.
8) eventually these other retries of stage 0 finish, and now we've got even more attempts of stage 1 all going at the same time.;;;","01/May/15 20:14;irashid;In our discussion on https://github.com/apache/spark/pull/5636, Kay pointed out that one problem is that tasks aren't cancelled when a stage attempt is failed:

https://github.com/apache/spark/blob/37537760d19eab878a5e1a48641cc49e6cb4b989/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1116

It certainly makes the problem worse, and that would be good to fix even just for efficiency of retries.  But even if that were fixed, I'm still not  sure that alone would be a complete solution to the problem -- what if more fetch failures crossed paths w/ the request to cancel the pending tasks?  maybe it could be made to work, but we certainly need to think about it very carefully.;;;","06/May/15 00:16;irashid;I forgot to take {{TaskSetManager.isZombie}} into account ... a lot of the ""concurrent"" attempts are just zombies.  That is not the only case, but its all that my test case can consistently reproduce, so I'll try to change it around to see if I can consistently create the other cases.;;;","07/May/15 04:28;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/5964;;;","21/May/15 19:34;irashid;problem description and proposed solution, with discussion of some alternatives.;;;","23/May/15 01:39;irashid;Couple of clarifications based on some offline discussions I've had:

a) There are actually two different ""types"" of concurrent stage attempts.
i. when you get a fetch failed, that attempt is marked as a ""zombie"", and eventually a new attempt for that stage is started which is a non-zombie.  However, there may be tasks in the zombie attempt which run for a while, and so you might have tasks running from the zombie attempt and the non-zombie attempt
ii.  with multiple fetch failures from one attempt, you can end up with multiple *non*-zombie attempts for one stage.  This is the problem that I originally opened this JIRA for, and its issue (1) in the doc.  I can reproduce that with a simpler example, without reproducing the other problems.  However, in those simpler examples, somehow jobs still succeed (with the right answer, too).  I could only get true failures from spark when I tried more complicated workloads, which also triggered all of the other problems.

Issues (2), (3), & (4) are all from the multiple attempts that come from (i) -- that is, a zombie and non-zombie together.

b) Marcelo pointed out that there is another possible solution to this problem -- different stage attempts should write to different shuffle map outputs.  That would eliminate all the problems of multiple attempts corrupting each other's output.  But it does complicate the shuffle logic, because now for each map output, you need to know which attempt to look for.

I'll update the doc later to try to make this more clear and include the alternative suggestion, but I wanted to mention it briefly in case anyone is confused by the doc now. ;;;","24/May/15 03:57;joshrosen;I think that properly fixing this set of issues will involve both scheduler and shuffle write path changes.

Spark's task cancellation is best-effort, so even if we fix the scheduler issues it still is possible that a delayed task from an earlier stage might conflict with a task from a subsequent attempt.  I think that we should focus first on making it safe for multiple attempts of the same task to be running concurrently on the same executor, then focus on making the scheduler changes to prevent this scenario from happening. I like Marcelo's suggestion that different task attempts write their output to different files. Note, however, that the name of the shuffle output file is an implicit interface that's used by our external shuffle service. As a result, I think that we need to ensure that the final ""winning"" task attempt renames its temporary / staging files to the filenames that we're using now.  To do this, I think that we can implement some simple synchronization within an Executor JVM to implement last-writer-wins atomic renaming / commit of output files (this is similar in spirit to OutputCommitCoordinator, but _much_ simpler since it's local coordination).

Once we fix the safety issue, we can then address the scheduler logic changes. I think that addressing these two pieces in this order makes the most sense, since scheduler changes have historically been very hard to perform correctly.  Since these sets of changes are largely orthogonal, splitting them into separate patches will significantly lower our review burden and make things easier for component-specific maintainers (e.g. it'll be easier for the scheduler maintainers to review a smaller patch without a bunch of unrelated changes to executor commit coordination).

Since it sounds like you already have a good test that reproduces all of the bugs, I would welcome a patch which commits a failing test (we could just wrap it in a try-catch block or add an expected exception to the test declaration). This will help to keep the testing work that you've done so far from bitrotting while we work on the fix.;;;","25/May/15 18:28;irashid;Sounds good [~joshrosen].  I will continue to split this into separate issues and PRs (sorry got a little sidetracked, I will get back to that soon).  I just wanted some amount of thought put into all of these issues together, rather than totally in isolation, as it makes sense to consider the design for how we solve some issues together.  I totally understand the flip side, that it makes it tougher to review as well.;;;","04/Jun/15 16:15;irashid;I'm turning this into an umbrella jira (but leaving what is here already, for archiving).  I've broken it into

* SPARK-8103 just the issues w/ the DAGScheduler
* SPARK-8029 making ShuffleMapTask safe with multiple concurrent attempts on one executor;;;","13/Nov/15 22:00;andrewor14;Should this still be open given that all associated JIRAs are closed? I think we've already established that there's no bullet-proof way to do this on the scheduler side so we need to make the write side robust.;;;","13/Nov/15 22:10;irashid;yeah I think you are right, I just marked it as fixed.;;;",,,,,,,,,,,,,,,,
SPARK-7224 broke build with jdk6,SPARK-7306,12826607,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,tgraves,tgraves,01/May/15 18:19,04/Jun/15 23:22,14/Jul/23 06:26,01/May/15 20:00,1.4.0,,,,,,1.4.0,,,,,,Build,,,,0,,,,,,"Lots going on so filing blocker ticket so this doesn't get lost.

https://github.com/apache/spark/pull/5790

broke compatibility with jdk6 because its using  java.nio.file.Path which is only available in jdk7",,apachespark,brkyvz,pwendell,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 23:22:08 UTC 2015,,,,,,,,,,"0|i2e6x3:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"01/May/15 18:25;brkyvz;I'll submit a patch using Guava within an hour.;;;","01/May/15 20:00;pwendell;Fixed by reverting SPARK-7224;;;","04/May/15 18:57;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/5892;;;","04/Jun/15 23:22;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6657;;;",,,,,,,,,,,,,,,,,,,,,,,,,
SPARK building documentation still mentions building for yarn 0.23,SPARK-7302,12826547,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,tgraves,tgraves,01/May/15 13:52,03/May/15 20:23,14/Jul/23 06:26,03/May/15 20:23,1.3.1,,,,,,1.4.0,,,,,,Documentation,,,,0,,,,,,"as of SPARK-3445 we deprecated using hadoop 0.23.  It looks like the building documentation still references it though. We should remove that.

",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3445,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun May 03 20:23:42 UTC 2015,,,,,,,,,,"0|i2e6jr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"01/May/15 19:06;srowen;I think it can be fully removed, like, the build profile can go too.;;;","02/May/15 10:56;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5863;;;","03/May/15 20:23;srowen;Resolved by https://github.com/apache/spark/pull/5863;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
saving Oracle-source DataFrame to Hive changes scale,SPARK-7299,12826521,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,kgeis,kgeis,01/May/15 08:38,18/May/15 08:11,14/Jul/23 06:26,18/May/15 08:11,1.3.1,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"When I load data from Oracle, save it to a table, then select from it, the scale is changed.

For example, I have a column defined as NUMBER(12, 2). I insert 19999 into the column. When I write that to a table and select from it, the result is 199.99.

Some databases (e.g. H2) will return this as 19999.00, but Oracle returns it as 19999. I believe that when the file is written out to parquet, the scale information is taken from the schema, not the value. In an Oracle (at least) JDBC DataFrame, the scale may be different from row to row.",,apachespark,kgeis,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 17:06:21 UTC 2015,,,,,,,,,,"0|i2e6dz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/15 09:04;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5833;;;","01/May/15 09:04;viirya;[~kgeis] I submit a [pr|https://github.com/apache/spark/pull/5833] for this. Can you try it and see if it solves this problem? Thanks.;;;","01/May/15 14:16;kgeis;This passes my test!;;;","01/May/15 17:06;viirya;Great!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Suppress compiler warnings due to use of sun.misc.Unsafe,SPARK-7288,12826447,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,30/Apr/15 22:19,15/Mar/16 12:12,14/Jul/23 06:26,30/Apr/15 22:21,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Our use of sun.misc.Unsafe in Tungsten led to some Java compiler warnings. These warnings can only be suppressed via the -XDignore.symbol.file javac flag; the @SuppressWarnings annotation won't work for these.

We can enable this flag only for the {{unsafe}} module.  To prevent us from having to add similar flags if we call unsafe functionality in other modules, we can place a facade in front of Unsafe so that other modules won't call it directly. This facade also will also help us to avoid accidental usage of deprecated Unsafe methods or methods that aren't supported in Java 6.",,apachespark,joshrosen,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-13395,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-7075,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 22:20:45 UTC 2015,,,,,,,,,,"0|i2e5xr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 22:20;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5814;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.deploy.SparkSubmitSuite --packages,SPARK-7287,12826444,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,andrewor14,andrewor14,30/Apr/15 22:06,30/Jun/15 00:20,14/Jul/23 06:26,30/Jun/15 00:20,1.4.0,,,,,,1.4.1,1.5.0,,,,,Tests,,,,0,flaky-test,,,,,"Error message was not helpful (did not complete within 60 seconds or something).

Observed only in master:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/2239/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.0,label=centos/2238/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/hadoop.version=1.0.4,label=centos/2163/
...",,andrewor14,apachespark,brkyvz,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 25 20:40:06 UTC 2015,,,,,,,,,,"0|i2e5x3:",9223372036854775807,,,,,,,,,,,,,,1.4.2,1.5.0,,,,,,,,,,,,"01/May/15 04:54;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/5826;;;","01/May/15 20:01;pwendell;I had to reopen this due to SPARK-7306;;;","24/May/15 02:42;pwendell;[~brkyvz] I am going to disable this test again, it is still failing even after SPARK-7224:

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.4-Maven-pre-YARN/hadoop.version=2.0.0-mr1-cdh4.1.2,label=centos/235/testReport/junit/org.apache.spark.deploy/SparkSubmitSuite/includes_jars_passed_in_through___packages/;;;","24/May/15 05:05;brkyvz;I don't understand why that's failing. It's not related to --packages per se, it's more like, we can't run Spark Submit anymore. That's why the test above, --jars also fails sometimes.;;;","18/Jun/15 20:12;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6886;;;","19/Jun/15 17:58;andrewor14;I'm going to keep this open for a while since we haven't actually done anything to fix the test itself. If it remains flaky then we should ignore the tests again.;;;","19/Jun/15 17:58;andrewor14;+ and spend some time to actually rewrite it.;;;","25/Jun/15 20:40;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/7027;;;",,,,,,,,,,,,,,,,,,,,,
Precedence of operator not behaving properly,SPARK-7286,12826425,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,jodersky,jethalal,jethalal,30/Apr/15 20:59,09/Mar/16 02:11,14/Jul/23 06:26,09/Mar/16 02:11,1.3.1,,,,,,2.0.0,,,,,,SQL,,,,1,,,,,,"The precedence of the operators ( especially with !== and && ) in Dataframe Columns seems to be messed up.

Example Snippet

.where( $""col1"" === ""val1"" && ($""col2""  !== ""val2"")  ) works fine.

whereas .where( $""col1"" === ""val1"" && $""col2""  !== ""val2""  )
evaluates as ( $""col1"" === ""val1"" && $""col2"" ) !== ""val2""",Linux,apachespark,dragos,hotou,jethalal,jodersky,krcz,rxin,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 08 22:07:04 UTC 2016,,,,,,,,,,"0|i2e5sv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/15 22:24;smilegator;I can reproduce it. Let me investigate it. ;;;","18/Nov/15 00:13;smilegator;This is affected by the operator precedence in Scala. There does not exist a simple fix to resolve this issue. 

For another example, the Scala compiler is unable to recognize the following syntax:
where ( $""col1"" !== ""val1"" && $""col2"" === ""val2"" );;;","19/Nov/15 01:14;jodersky;The problem is that !== is recognized as an assignment operator (according to §6.12.4 of the scala specification http://www.scala-lang.org/docu/files/ScalaReference.pdf) and thus has lower precedence than any other operator.
A potential fix could be to rename !== to
 =!=;;;","19/Nov/15 02:18;jodersky;Going through the code, I saw that catalyst also defines !== in its dsl, so it seems this operator has quite wide-spread usage.
Would deprecating it in favor of something else be a viable option?;;;","19/Nov/15 22:24;smilegator;Hi, [~marmbrus] and [~rxin] 
Is that possible we can change 
!== 
to 
`=!=` in Spark 2.0?

Thank you! ;;;","19/Nov/15 22:26;rxin;Is <> taken?
;;;","19/Nov/15 22:34;smilegator;It works! Do you want we fix this by PR?;;;","19/Nov/15 22:36;rxin;cc [~dragos]

If this is indeed a problem, we should deprecate ""!=="" and create a new function ""<>"".
;;;","19/Nov/15 22:41;jodersky;If we want the same precedence of equals and non-equals operators, they should both start with a symbol of same-precedence.
= has the same precedence as !, however as per spec, if the operator also ends with a = and does not start with =, it is considered an assignment operator with a lower precedence than others.

So I see a couple of options:
1) abandon the wish of same precedence and accept the status-quo
2) adopt Reynold's solution or define another operator
3) remove non-equals comparison altogether, instead rely on negating equals comparison. I.e (A !== B becomes !(A === B))

Personally, I am against choosing option 1 as it leads to confusing behaviour (like this JIRA demonstrates). Not sure what I prefer between option 2 or 3 though;;;","19/Nov/15 22:50;jodersky;I just realized that <> would also have a different precedence that ===

That pretty much limits our options to 1 or 3

Thinking about option 1 again, it might actually not be that bad since (in-)equality comparisons typically happen between expressions using higher precedence characters (*/+-:%). If the dollar column operator were removed it would become a very rare situation.

On a side note, why was the dollar character actually chosen? from what I know its use is discouraged as it can clash with compiler generated identifiers;;;","20/Nov/15 08:32;dragos;I agree with Jakob's analysis. For reference, Slick uses {{=!=}}, which wouldn't be a bad option and gives you the same precedence.;;;","21/Nov/15 04:31;smilegator;Last week, [~jodersky] and I had a discussion. It sounds like their precedence are still different, even if we use 
=!=
;;;","21/Nov/15 04:36;jodersky;Hey @xiao, I think we had a misunderstanding, =!= solves all the problems,
I was suggesting that <> would not solve all problems but that =!= doesn't
""look"" as good (my subjective opinion ;))

;;;","21/Nov/15 04:46;smilegator;: ) That is perfect! The problem is solved! See you next Monday and we can discuss the PR issue.  ;;;","24/Nov/15 01:50;apachespark;User 'jodersky' has created a pull request for this issue:
https://github.com/apache/spark/pull/9925;;;","08/Mar/16 22:07;apachespark;User 'jodersky' has created a pull request for this issue:
https://github.com/apache/spark/pull/11588;;;",,,,,,,,,,,,,
Fix a flaky test in StreamingListenerSuite,SPARK-7282,12826380,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,30/Apr/15 18:10,01/May/15 04:32,14/Jul/23 06:26,01/May/15 04:32,,,,,,,1.4.0,,,,,,DStreams,Tests,,,0,flaky-test,,,,,"Fixed the following flaky test:

{code}
[info] StreamingListenerSuite:
[info] - batch info reporting (782 milliseconds)
[info] - receiver info reporting *** FAILED *** (3 seconds, 911 milliseconds)
[info]   The code passed to eventually never returned normally. Attempted 10 times over 3.4735783689999997 seconds. Last failure message: 0 did not equal 1. (StreamingListenerSuite.scala:104)
[info]   org.scalatest.exceptions.TestFailedDueToTimeoutException:
[info]   at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:420)
[info]   at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:438)
[info]   at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:478)
[info]   at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:307)
[info]   at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:478)
[info]   at org.apache.spark.streaming.StreamingListenerSuite$$anonfun$2.apply$mcV$sp(StreamingListenerSuite.scala:104)
[info]   at org.apache.spark.streaming.StreamingListenerSuite$$anonfun$2.apply(StreamingListenerSuite.scala:94)
[info]   at org.apache.spark.streaming.StreamingListenerSuite$$anonfun$2.apply(StreamingListenerSuite.scala:94)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.apache.spark.streaming.StreamingListenerSuite.org$scalatest$BeforeAndAfter$$super$runTest(StreamingListenerSuite.scala:34)
[info]   at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
[info]   at org.apache.spark.streaming.StreamingListenerSuite.runTest(StreamingListenerSuite.scala:34)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.apache.spark.streaming.StreamingListenerSuite.org$scalatest$BeforeAndAfter$$super$run(StreamingListenerSuite.scala:34)
[info]   at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
[info]   at org.apache.spark.streaming.StreamingListenerSuite.run(StreamingListenerSuite.scala:34)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
[info]   Cause: org.scalatest.exceptions.TestFailedException: 0 did not equal 1
[info]   at org.scalatest.MatchersHelper$.newTestFailedException(MatchersHelper.scala:160)
[info]   at org.scalatest.Matchers$ShouldMethodHelper$.shouldMatcher(Matchers.scala:6231)
[info]   at org.scalatest.Matchers$AnyShouldWrapper.should(Matchers.scala:6277)
[info]   at org.apache.spark.streaming.StreamingListenerSuite$$anonfun$2$$anonfun$apply$mcV$sp$1.apply$mcV$sp(StreamingListenerSuite.scala:105)
[info]   at org.apache.spark.streaming.StreamingListenerSuite$$anonfun$2$$anonfun$apply$mcV$sp$1.apply(StreamingListenerSuite.scala:104)
[info]   at org.apache.spark.streaming.StreamingListenerSuite$$anonfun$2$$anonfun$apply$mcV$sp$1.apply(StreamingListenerSuite.scala:104)
[info]   at org.scalatest.concurrent.Eventually$class.makeAValiantAttempt$1(Eventually.scala:394)
[info]   at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:408)
[info]   at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:438)
[info]   at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:478)
[info]   at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:307)
[info]   at org.scalatest.concurrent.Eventually$.eventually(Eventually.scala:478)
[info]   at org.apache.spark.streaming.StreamingListenerSuite$$anonfun$2.apply$mcV$sp(StreamingListenerSuite.scala:104)
[info]   at org.apache.spark.streaming.StreamingListenerSuite$$anonfun$2.apply(StreamingListenerSuite.scala:94)
[info]   at org.apache.spark.streaming.StreamingListenerSuite$$anonfun$2.apply(StreamingListenerSuite.scala:94)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.apache.spark.streaming.StreamingListenerSuite.org$scalatest$BeforeAndAfter$$super$runTest(StreamingListenerSuite.scala:34)
[info]   at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
[info]   at org.apache.spark.streaming.StreamingListenerSuite.runTest(StreamingListenerSuite.scala:34)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.apache.spark.streaming.StreamingListenerSuite.org$scalatest$BeforeAndAfter$$super$run(StreamingListenerSuite.scala:34)
[info]   at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
[info]   at org.apache.spark.streaming.StreamingListenerSuite.run(StreamingListenerSuite.scala:34)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 18:14:57 UTC 2015,,,,,,,,,,"0|i2e5j3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 18:14;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5812;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
No option for AM native library path in yarn-client mode.,SPARK-7281,12826376,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,30/Apr/15 17:58,01/May/15 20:21,14/Jul/23 06:26,01/May/15 20:21,1.3.1,1.4.0,,,,,1.4.0,,,,,,YARN,,,,0,,,,,,"There are many options to control the AM in yarn-client mode, but the option for the native library path is missing. Without that, users have to change the process's environment directly (using {{spark.yarn.appMasterEnv.LD_LIBRARY_PATH}} on Linux, for example) to add things to the native library path.

Depending on the cluster's configuration, native libraries are needed in the AM (for example when using the JNI-based group mapper in Hadoop).

There's also no option to control the AM's classpath, but since the AM is not running user code, that seems less important.",,apachespark,sandyr,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 20:21:07 UTC 2015,,,,,,,,,,"0|i2e5i7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 18:29;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5813;;;","01/May/15 20:21;srowen;Issue resolved by pull request 5813
[https://github.com/apache/spark/pull/5813];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent handling of dates in PySparks Row object,SPARK-7278,12826342,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kalle,kalle,kalle,30/Apr/15 15:51,14/May/15 22:46,14/Jul/23 06:26,14/May/15 22:28,1.3.1,,,,,,1.3.2,1.4.0,,,,,PySpark,,,,0,,,,,,"Consider the following Python code:

{code:none}
import datetime

rdd = sc.parallelize([[0, datetime.date(2014, 11, 11)], [1, datetime.date(2015,6,4)]])
df = rdd.toDF(schema=['rid', 'date'])
row = df.first()
{code}

Accessing the {{date}} column via {{\_\_getitem\_\_}} returns a {{datetime.datetime}} instance

{code:none}
>>>row[1]
datetime.datetime(2014, 11, 11, 0, 0)
{code}

while access via {{getattr}} returns a {{datetime.date}} instance:

{code:none}
>>>row.date
datetime.date(2014, 11, 11)
{code}

The problem seems to be that that Java deserializes the {{datetime.date}} objects to {{datetime.datetime}}. This is taken care of [here|https://github.com/apache/spark/blob/master/python/pyspark/sql/_types.py#L1027] when using {{getattr}}, but is overlooked when directly accessing the tuple by index.

Is there an easy way to fix this?",,apachespark,kalle,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 11 15:35:49 UTC 2015,,,,,,,,,,"0|i2e5an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 16:20;kalle;This is probably a duplicate of [this issue|https://issues.apache.org/jira/browse/SPARK-6289]. Is there anything dangerous about simply adopting {{datetime.datetime}} as PySparks generic date type?;;;","11/May/15 15:28;kalle;Shouldn't {{DateType}} at least find {{datetime.datetime}} acceptable?;;;","11/May/15 15:32;apachespark;User 'yepzen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6056;;;","11/May/15 15:35;apachespark;User 'ksonj' has created a pull request for this issue:
https://github.com/apache/spark/pull/6057;;;",,,,,,,,,,,,,,,,,,,,,,,,,
property mapred.reduce.task replaced by spark.sql.shuffle.partitions,SPARK-7277,12826285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,tannin,tannin,30/Apr/15 12:38,16/May/15 11:04,14/Jul/23 06:26,07/May/15 23:23,1.3.1,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"When I use ""SET mapred.reduce.task"" I get the warning ""SetCommand: Property mapred.reduce.tasks is deprecated, automatically converted to spark.sql.shuffle.partitions instead.""

It's true that mapred.reduce.task is deprecated but this replacement causes serious trouble:

Setting mapred.reduce.task to -1 (negative one) is valid and causes hadoop/hive to automatically determine the required number of reducers.

Setting spark.sql.shuffle.partitions to negative can cause spark to produce incorrect results.
In my system (spark-sql 1.3.1 running on a single machine in ""local"" mode), with this setting, any outer join produces no output (whereas an inner join does)",,apachespark,glenn.strycker@gmail.com,marmbrus,tannin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 23:23:06 UTC 2015,,,,,,,,,,"0|i2e4yn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 12:46;srowen;(Set Components please);;;","30/Apr/15 13:04;tannin;Apologies, I wasn't sure which component of Spark is actually responsible for converting those parameters.;;;","30/Apr/15 18:09;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5811;;;","07/May/15 23:23;marmbrus;Issue resolved by pull request 5811
[https://github.com/apache/spark/pull/5811];;;",,,,,,,,,,,,,,,,,,,,,,,,,
StringType dynamic partition cast to DecimalType in Spark Sql Hive ,SPARK-7270,12826196,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,CurryGaifan,CurryGaifan,30/Apr/15 03:10,25/May/15 07:33,14/Jul/23 06:26,23/May/15 00:22,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Create a hive table with two partitons,the first type is bigint and the second type is string.When insert overwrite the table with one static partiton and one dynamic partiton, the second StringType dynamic partition will be cast to DecimalType.
{noformat}
desc test;                                                                 
OK
a                   	string              	None                
b                   	bigint              	None                
c                   	string              	None                
	 	 
# Partition Information	 	 
# col_name            	data_type           	comment             
	 	 
b                   	bigint              	None                
c                   	string              	None·
{noformat}

when run following hive sql in HiveContext
{noformat}sqlContext.sql(""insert overwrite table test partition (b=1,c) select 'a','c' from ptest""){noformat}

get the result of partition is
{noformat}test[1,__HIVE_DEFAULT_PARTITION__]{noformat}

spark log
{noformat}15/04/30 10:38:09 WARN HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead
15/04/30 10:38:09 INFO ParseDriver: Parsing command: insert overwrite table test partition (b=1,c) select 'a','c' from ptest
15/04/30 10:38:09 INFO ParseDriver: Parse Completed
15/04/30 10:38:09 WARN HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead
15/04/30 10:38:10 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
15/04/30 10:38:10 INFO ObjectStore: ObjectStore, initialize called
15/04/30 10:38:10 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
15/04/30 10:38:10 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
15/04/30 10:38:10 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
15/04/30 10:38:10 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
15/04/30 10:38:11 WARN HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead
15/04/30 10:38:11 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes=""Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order""
15/04/30 10:38:11 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
15/04/30 10:38:11 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
15/04/30 10:38:12 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MFieldSchema"" is tagged as ""embedded-only"" so does not have its own datastore table.
15/04/30 10:38:12 INFO Datastore: The class ""org.apache.hadoop.hive.metastore.model.MOrder"" is tagged as ""embedded-only"" so does not have its own datastore table.
15/04/30 10:38:12 INFO Query: Reading in results for query ""org.datanucleus.store.rdbms.query.SQLQuery@0"" since the connection used is closing
15/04/30 10:38:12 INFO ObjectStore: Initialized ObjectStore
15/04/30 10:38:12 INFO HiveMetaStore: Added admin role in metastore
15/04/30 10:38:12 INFO HiveMetaStore: Added public role in metastore
15/04/30 10:38:12 INFO HiveMetaStore: No user is added in admin role, since config is empty
15/04/30 10:38:12 INFO SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/04/30 10:38:13 INFO HiveMetaStore: 0: get_table : db=default tbl=test
15/04/30 10:38:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test	
15/04/30 10:38:13 INFO HiveMetaStore: 0: get_partitions : db=default tbl=test
15/04/30 10:38:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=default tbl=test	
15/04/30 10:38:13 INFO HiveMetaStore: 0: get_table : db=default tbl=ptest
15/04/30 10:38:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=ptest	
15/04/30 10:38:13 INFO HiveMetaStore: 0: get_partitions : db=default tbl=ptest
15/04/30 10:38:13 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partitions : db=default tbl=ptest	
15/04/30 10:38:13 INFO deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
15/04/30 10:38:13 INFO MemoryStore: ensureFreeSpace(451930) called with curMem=0, maxMem=2291041566
15/04/30 10:38:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 441.3 KB, free 2.1 GB)
15/04/30 10:38:13 INFO MemoryStore: ensureFreeSpace(71321) called with curMem=451930, maxMem=2291041566
15/04/30 10:38:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 69.6 KB, free 2.1 GB)
15/04/30 10:38:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.134.72.169:45859 (size: 69.6 KB, free: 2.1 GB)
15/04/30 10:38:13 INFO BlockManagerMaster: Updated info of block broadcast_0_piece0
15/04/30 10:38:13 INFO SparkContext: Created broadcast 0 from broadcast at TableReader.scala:68
15/04/30 10:38:13 INFO deprecation: mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress
15/04/30 10:38:13 INFO deprecation: mapred.output.compression.codec is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.codec
15/04/30 10:38:13 INFO deprecation: mapred.output.compression.type is deprecated. Instead, use mapreduce.output.fileoutputformat.compress.type
15/04/30 10:38:14 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
15/04/30 10:38:14 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
15/04/30 10:38:14 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
15/04/30 10:38:14 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
15/04/30 10:38:14 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
15/04/30 10:38:14 INFO GPLNativeCodeLoader: Loaded native gpl library
15/04/30 10:38:14 INFO LzoCodec: Successfully loaded & initialized native-lzo library [hadoop-lzo rev 7041408c0d57cb3b6f51d004772ccf5073ecc95e]
15/04/30 10:38:14 INFO FileInputFormat: Total input paths to process : 1
15/04/30 10:38:14 INFO SparkContext: Starting job: runJob at InsertIntoHiveTable.scala:93
15/04/30 10:38:14 INFO DAGScheduler: Got job 0 (runJob at InsertIntoHiveTable.scala:93) with 1 output partitions (allowLocal=false)
15/04/30 10:38:14 INFO DAGScheduler: Final stage: Stage 0(runJob at InsertIntoHiveTable.scala:93)
15/04/30 10:38:14 INFO DAGScheduler: Parents of final stage: List()
15/04/30 10:38:14 INFO DAGScheduler: Missing parents: List()
15/04/30 10:38:14 INFO DAGScheduler: Submitting Stage 0 (MapPartitionsRDD[5] at mapPartitions at basicOperators.scala:43), which has no missing parents
15/04/30 10:38:14 INFO MemoryStore: ensureFreeSpace(125560) called with curMem=523251, maxMem=2291041566
15/04/30 10:38:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 122.6 KB, free 2.1 GB)
15/04/30 10:38:14 INFO MemoryStore: ensureFreeSpace(82648) called with curMem=648811, maxMem=2291041566
15/04/30 10:38:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 80.7 KB, free 2.1 GB)
15/04/30 10:38:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.134.72.169:45859 (size: 80.7 KB, free: 2.1 GB)
15/04/30 10:38:14 INFO BlockManagerMaster: Updated info of block broadcast_1_piece0
15/04/30 10:38:14 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:838
15/04/30 10:38:14 INFO DAGScheduler: Submitting 1 missing tasks from Stage 0 (MapPartitionsRDD[5] at mapPartitions at basicOperators.scala:43)
15/04/30 10:38:14 INFO YarnClientClusterScheduler: Adding task set 0.0 with 1 tasks
15/04/30 10:38:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, rsync.slave006.yarn.hadoop.sjs.sogou-op.org, NODE_LOCAL, 1794 bytes)
15/04/30 10:38:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on rsync.slave006.yarn.hadoop.sjs.sogou-op.org:55678 (size: 80.7 KB, free: 5.3 GB)
15/04/30 10:38:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on rsync.slave006.yarn.hadoop.sjs.sogou-op.org:55678 (size: 69.6 KB, free: 5.3 GB)
15/04/30 10:38:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 3152 ms on rsync.slave006.yarn.hadoop.sjs.sogou-op.org (1/1)
15/04/30 10:38:17 INFO DAGScheduler: Stage 0 (runJob at InsertIntoHiveTable.scala:93) finished in 3.162 s
15/04/30 10:38:17 INFO YarnClientClusterScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/04/30 10:38:17 INFO DAGScheduler: Job 0 finished: runJob at InsertIntoHiveTable.scala:93, took 3.369777 s
15/04/30 10:38:17 INFO HiveMetaStore: 0: partition_name_has_valid_characters
15/04/30 10:38:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
15/04/30 10:38:17 INFO HiveMetaStore: 0: partition_name_has_valid_characters
15/04/30 10:38:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=partition_name_has_valid_characters	
15/04/30 10:38:17 WARN UserGroupInformation: No groups available for user root
15/04/30 10:38:17 WARN UserGroupInformation: No groups available for user root
15/04/30 10:38:17 WARN HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead
15/04/30 10:38:17 INFO HiveMetaStore: 0: get_table : db=default tbl=test
15/04/30 10:38:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_table : db=default tbl=test	
15/04/30 10:38:17 INFO HiveMetaStore: 0: get_partition_with_auth : db=default tbl=test[1,__HIVE_DEFAULT_PARTITION__]
15/04/30 10:38:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test[1,__HIVE_DEFAULT_PARTITION__]	
15/04/30 10:38:17 INFO Hive: Replacing src:hdfs://yarncluster/tmp/hive-root/hive_2015-04-30_10-38-13_846_3096248751564356035-1/-ext-10000/c=__HIVE_DEFAULT_PARTITION__;dest: hdfs://yarncluster/user/root/hive/warehouse/test/b=1/c=__HIVE_DEFAULT_PARTITION__;Status:true
15/04/30 10:38:17 INFO HiveMetaStore: 0: get_partition_with_auth : db=default tbl=test[1,__HIVE_DEFAULT_PARTITION__]
15/04/30 10:38:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=get_partition_with_auth : db=default tbl=test[1,__HIVE_DEFAULT_PARTITION__]	
15/04/30 10:38:17 INFO HiveMetaStore: 0: append_partition : db=default tbl=test[1,__HIVE_DEFAULT_PARTITION__]
15/04/30 10:38:17 INFO audit: ugi=root	ip=unknown-ip-addr	cmd=append_partition : db=default tbl=test[1,__HIVE_DEFAULT_PARTITION__]	
15/04/30 10:38:17 WARN log: Updating partition stats fast for: test
15/04/30 10:38:17 WARN log: Updated size to 10
15/04/30 10:38:17 INFO Hive: New loading path = hdfs://yarncluster/tmp/hive-root/hive_2015-04-30_10-38-13_846_3096248751564356035-1/-ext-10000/c=__HIVE_DEFAULT_PARTITION__ with partSpec {b=1, c=__HIVE_DEFAULT_PARTITION__}
res0: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[0] at RDD at SchemaRDD.scala:108
== Query Plan ==
== Physical Plan ==
InsertIntoHiveTable (MetastoreRelation default, test, None), Map(b -> Some(1), c -> None), true
 Project [a AS _c0#0,CAST(CAST(c AS _c1#1, DecimalType()), LongType) AS _c1#8L]
  HiveTableScan [], (MetastoreRelation default, ptest, None), None
{noformat}",,apachespark,CurryGaifan,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 23 00:22:54 UTC 2015,,,,,,,,,,"0|i2e4ev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/May/15 11:23;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5864;;;","23/May/15 00:22;marmbrus;Issue resolved by pull request 5864
[https://github.com/apache/spark/pull/5864];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect aggregation analysis,SPARK-7269,12826188,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,chenghao,chenghao,30/Apr/15 02:34,02/Jun/15 14:31,14/Jul/23 06:26,18/May/15 19:13,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"In a case insensitive analyzer (HiveContext), the attribute name captial differences will fail the analysis check for aggregation.
{code}
test(""check analysis failed in case in-sensitive"") {
    Seq(1,2,3).map(i => (i, i.toString)).toDF(""key"", ""value"").registerTempTable(""df_analysis"")
    sql(""SELECT kEy from df_analysis group by key"")
}
{code}

{noformat}
expression 'kEy' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() if you don't care which value you get.;
org.apache.spark.sql.AnalysisException: expression 'kEy' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() if you don't care which value you get.;
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:38)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:39)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:85)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$4.apply(CheckAnalysis.scala:101)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$4.apply(CheckAnalysis.scala:101)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:101)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:89)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:39)
	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1121)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:97)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$15.apply$mcV$sp(SQLQuerySuite.scala:408)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$15.apply(SQLQuerySuite.scala:406)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$15.apply(SQLQuerySuite.scala:406)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
{noformat}",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7235,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 14:31:07 UTC 2015,,,,,,,,,,"0|i2e4d3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 02:36;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/5798;;;","13/May/15 05:49;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/6110;;;","15/May/15 05:59;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/6173;;;","18/May/15 19:13;marmbrus;Issue resolved by pull request 6173
[https://github.com/apache/spark/pull/6173];;;","02/Jun/15 14:31;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/6587;;;",,,,,,,,,,,,,,,,,,,,,,,,
Many user provided closures are not actually cleaned,SPARK-7237,12826033,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,29/Apr/15 18:01,20/May/15 19:18,14/Jul/23 06:26,05/May/15 16:38,1.0.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"It appears that many operations throughout Spark actually do not actually clean the closures provided by the user.

Simple reproduction:
{code}
def test(): Unit = {
  sc.parallelize(1 to 10).mapPartitions { iter => return; iter }.collect()
}
{code}
Clearly, the inner closure is not serializable, but when we serialize it we should expect the ClosureCleaner to fail fast and complain loudly about return statements. Instead, we get a mysterious stack trace:
{code}
java.io.NotSerializableException: java.lang.Object
Serialization stack:
	- object not serializable (class: java.lang.Object, value: java.lang.Object@6db4b914)
	- field (class: $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1, name: nonLocalReturnKey1$1, type: class java.lang.Object)
	- object (class $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1, <function1>)
	- field (class: org.apache.spark.rdd.RDD$$anonfun$14, name: f$4, type: interface scala.Function1)
	- object (class org.apache.spark.rdd.RDD$$anonfun$14, <function3>)
	at org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:40)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:81)
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:314)
{code}

What might have caused this? If you look at the code for mapPartitions, you'll notice that we never explicitly clean the closure passed in by the user. Instead, we only wrap it in another closure and clean only the outer one:
{code}
def mapPartitions[U: ClassTag](
      f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = {
    val func = (context: TaskContext, index: Int, iter: Iterator[T]) => f(iter)
    new MapPartitionsRDD(this, sc.clean(func), preservesPartitioning)
  }
{code}

This is not sufficient, however, because the user provided closure is actually a field of the outer closure, and this inner closure doesn't get cleaned. If we rewrite the above by cleaning the inner closure preemptively, as we have done in other places:

{code}
def mapPartitions[U: ClassTag](
      f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = {
    val cleanedFunc = clean(f)
    new MapPartitionsRDD(
      this,
      (context: TaskContext, index: Int, iter: Iterator[T]) => cleanedFunc(iter),
      preservesPartitioning)
  }
{code}

Then we get the exception that we would expect by running the test() example above:
{code}
org.apache.spark.SparkException: Return statements aren't allowed in Spark closures
	at org.apache.spark.util.ReturnStatementFinder$$anon$1.visitTypeInsn(ClosureCleaner.scala:357)
	at com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown Source)
	at com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown Source)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:215)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:1759)
	at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:640)
{code}

It seems to me that we simply forgot to do this in a few places (e.g. mapPartitions, keyBy, aggregateByKey), because in other similar places we do this correctly (e.g. groupBy, combineByKey, zipPartitions).",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7644,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 21:41:03 UTC 2015,,,,,,,,,,"0|i2e3fr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"29/Apr/15 18:02;andrewor14;cc [~zsxwing];;;","29/Apr/15 19:58;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/5787;;;","07/May/15 21:14;apachespark;User 'ted-yu' has created a pull request for this issue:
https://github.com/apache/spark/pull/5959;;;","19/May/15 21:41;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6269;;;",,,,,,,,,,,,,,,,,,,,,,,,,
AkkaUtils askWithReply sleeps indefinitely when a timeout exception is thrown,SPARK-7236,12825979,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,bryanc,bryanc,bryanc,29/Apr/15 15:31,05/May/15 01:30,14/Jul/23 06:26,05/May/15 01:30,,,,,,,1.4.0,,,,,,Spark Core,,,,0,quickfix,,,,,"When {{AkkaUtils.askWithReply}} gets a TimeoutException, the default parameters {{maxAttempts = 1}} and {{retryInterval = Int.MaxValue}} lead to the thread sleeping for a good while.

I noticed this issue when testing for SPARK-6980 and using this function without invoking Spark jobs, so perhaps it acts differently in another context.

If this function is on its final attempt to ask and it fails, it should return immediately.  Also, perhaps a better default {{retryInterval}} would be {{0}}.",,apachespark,bryanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/15 15:32;bryanc;SparkLongSleepAfterTimeout.scala;https://issues.apache.org/jira/secure/attachment/12729193/SparkLongSleepAfterTimeout.scala",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 04 22:02:53 UTC 2015,,,,,,,,,,"0|i2e33r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/15 15:32;bryanc;Attaching some code to reproduce this issue.;;;","29/Apr/15 18:10;bryanc;According to git blame, it looks like the default of {{retryInterval = Int.MaxValue}} was added with SPARK-3822 https://github.com/apache/spark/commit/1df05a40ebf3493b0aff46d18c0f30d2d5256c7b.

Maybe [~andrewor14] can comment if this was done for a specific reason?;;;","04/May/15 22:02;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/5896;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
When codegen on DateType defaultPrimitive will throw type mismatch exception,SPARK-7234,12825914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smacat,smacat,smacat,29/Apr/15 11:43,30/Apr/15 01:24,14/Jul/23 06:26,30/Apr/15 01:24,,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,"When codegen on, the defaultPrimitive of DateType is null. This will rise below error.

select COUNT(a) from table
a -> DateType

type mismatch;
 found   : Null(null)
 required: DateType.this.InternalType
",,apachespark,smacat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 11:47:27 UTC 2015,,,,,,,,,,"0|i2e2pb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/15 11:47;apachespark;User 'kaka1992' has created a pull request for this issue:
https://github.com/apache/spark/pull/5778;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClosureCleaner#clean blocks concurrent job submitter threads,SPARK-7233,12825911,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,preeze,preeze,preeze,29/Apr/15 11:35,15/May/15 18:24,14/Jul/23 06:26,15/May/15 18:24,1.3.1,1.4.0,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"{{org.apache.spark.util.ClosureCleaner#clean}} method contains logic to determine if Spark is run in interpreter mode: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala#L120

While this behavior is indeed valuable in particular situations, in addition to this it causes concurrent submitter threads to be blocked on a native call to {{java.lang.Class#forName0}} since it appears only 1 thread at a time can make the call.

This becomes a major issue when you have multiple threads concurrently submitting short-lived jobs. This is one of the patterns how we use Spark in production, and the number of parallel requests is expected to be quite high, up to a couple of thousand at a time.

A typical stacktrace of a blocked thread looks like:
{code}
http-bio-8091-exec-14 [BLOCKED] [DAEMON]

java.lang.Class.forName0(String, boolean, ClassLoader, Class) Class.java (native)
java.lang.Class.forName(String) Class.java:260
org.apache.spark.util.ClosureCleaner$.clean(Object, boolean) ClosureCleaner.scala:122
org.apache.spark.SparkContext.clean(Object, boolean) SparkContext.scala:1623
org.apache.spark.rdd.RDD.reduce(Function2) RDD.scala:883
org.apache.spark.rdd.RDD.takeOrdered(int, Ordering) RDD.scala:1240
org.apache.spark.api.java.JavaRDDLike$class.takeOrdered(JavaRDDLike, int, Comparator) JavaRDDLike.scala:586
org.apache.spark.api.java.AbstractJavaRDDLike.takeOrdered(int, Comparator) JavaRDDLike.scala:46
...
{code}",,andrewor14,apachespark,preeze,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/15 11:38;preeze;blocked_threads_closurecleaner.png;https://issues.apache.org/jira/secure/attachment/12729145/blocked_threads_closurecleaner.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 09:34:17 UTC 2015,,,,,,,,,,"0|i2e2on:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"29/Apr/15 11:38;preeze;To illustrate the issue, I performed a test against local Spark.
Attached is the screenshot from the Threads view in Yourkit profiler.
The test was generating only 20 concurrent requests.
As you can see, job submitter threads mainly spend their time being blocked by each other.;;;","29/Apr/15 20:46;pwendell;Thanks this is a great find. As a simple fix, can we just check once whether we are in the interpreter? It will be either true or not for an entire JVM lifetime.;;;","29/Apr/15 20:49;andrewor14;Yes, it can be a static field since that part is pretty isolated and doesn't have to be in the `clean` method. Great find.;;;","29/Apr/15 21:02;andrewor14;[~preeze] would you like to submit a patch for this? If not, I can take it up.;;;","29/Apr/15 22:01;pwendell;I think we should just create a lazy val in Utils that tells you whether you are in the repl. This will also be needed by SPARK-7261.;;;","30/Apr/15 08:34;preeze;[~andrewor14] With pleasure but afraid I won't be able to start with it in the nearest 2 weeks. If you feel you can do it earlier, I would ask you to take this up, please!

My brief list of the ways to fix it included:
- making this boolean {{inInterpreter}} part of the SparkContext and initializing it during SparkContext creation. I assume the same SparkContext can't be reused from both REPL and non-REPL envs.
- introducing an env variable to indicate if we want to enable REPL-mode detection.

The suggestion by [~pwendell] with lazy val in Utils does seem to be simpler and more concise since REPL presence can be indeed detected once per JVM lifespan.;;;","30/Apr/15 08:53;preeze;[~andrewor14] This got priority on our side as well, so I am starting with the patch right now.;;;","01/May/15 09:34;apachespark;User 'preeze' has created a pull request for this issue:
https://github.com/apache/spark/pull/5835;;;",,,,,,,,,,,,,,,,,,,,,
SpecificMutableRow should take integer type as internal representation for DateType,SPARK-7229,12825844,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,29/Apr/15 07:48,29/Apr/15 23:23,14/Jul/23 06:26,29/Apr/15 23:23,,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,"{code}
  test(""test DATE types in cache"") {
    val rows = TestSQLContext.jdbc(urlWithUserAndPass, ""TEST.TIMETYPES"").collect()
    TestSQLContext.jdbc(urlWithUserAndPass, ""TEST.TIMETYPES"").cache().registerTempTable(""mycached_date"")
    val cachedRows = sql(""select * from mycached_date"").collect()
    assert(rows(0).getAs[java.sql.Date](1) === java.sql.Date.valueOf(""1996-01-01""))
    assert(cachedRows(0).getAs[java.sql.Date](1) === java.sql.Date.valueOf(""1996-01-01""))
  }
{code}
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableAny cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableInt
	at org.apache.spark.sql.catalyst.expressions.SpecificMutableRow.getInt(SpecificMutableRow.scala:252)
	at org.apache.spark.sql.columnar.IntColumnStats.gatherStats(ColumnStats.scala:208)
	at org.apache.spark.sql.columnar.NullableColumnBuilder$class.appendFrom(NullableColumnBuilder.scala:56)
	at org.apache.spark.sql.columnar.NativeColumnBuilder.org$apache$spark$sql$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:87)
	at org.apache.spark.sql.columnar.compression.CompressibleColumnBuilder$class.appendFrom(CompressibleColumnBuilder.scala:78)
	at org.apache.spark.sql.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:87)
	at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:148)
	at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:124)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:277)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:209)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{panel}

{panel}",,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 07:51:06 UTC 2015,,,,,,,,,,"0|i2e29j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/15 07:51;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/5772;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CombineLimits optimizer does not work,SPARK-7225,12825805,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,DoingDone9,DoingDone9,DoingDone9,29/Apr/15 06:04,30/Apr/15 05:44,14/Jul/23 06:26,30/Apr/15 05:44,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"The  optimized logical plan of  ""select key from (select key from src limit 100) t2 limit 10""  like that 
{quote}
== Optimized Logical Plan ==
Limit 10
 Limit 100
  Project [key#3]
   MetastoreRelation default, src, None
{quote}

It did not combineLimits",,apachespark,DoingDone9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 06:27:35 UTC 2015,,,,,,,,,,"0|i2e20v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/15 06:27;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/5770;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unrolling never evicts blocks when MemoryStore is nearly full,SPARK-7214,12825721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andrewor14,woggle,woggle,28/Apr/15 22:46,17/May/20 18:21,14/Jul/23 06:26,16/Oct/15 22:12,,,,,,,1.6.0,,,,,,Block Manager,Spark Core,,,0,,,,,,"When less than spark.storage.unrollMemoryThreshold (default 1MB) is left in the MemoryStore, new blocks that are computed with unrollSafely (e.g. any cached RDD split) will always fail unrolling even if old blocks could be dropped to accommodate it.",,andrewor14,apachespark,woggle,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 16 22:12:08 UTC 2015,,,,,,,,,,"0|i2e1iv:",9223372036854775807,,,,,,,,,,,,,,1.6.0,,,,,,,,,,,,,"29/Apr/15 18:42;apachespark;User 'woggle' has created a pull request for this issue:
https://github.com/apache/spark/pull/5784;;;","16/Oct/15 22:12;andrewor14;Indirectly fixed through https://github.com/apache/spark/pull/9000 (SPARK-10956);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception while copying Hadoop config files due to permission issues,SPARK-7213,12825706,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nravi,nravi,nravi,28/Apr/15 22:04,01/May/15 20:17,14/Jul/23 06:26,01/May/15 20:17,,,,,,,1.4.0,,,,,,YARN,,,,0,,,,,,,,apachespark,nravi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 20:17:01 UTC 2015,,,,,,,,,,"0|i2e1gf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/15 22:05;nravi;Exception in thread ""main"" java.io.FileNotFoundException: /etc/hadoop/conf/container-executor.cfg (Permission denied)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:146)
	at com.google.common.io.Files$FileByteSource.openStream(Files.java:126)
	at com.google.common.io.Files$FileByteSource.openStream(Files.java:116)
	at com.google.common.io.ByteSource.copyTo(ByteSource.java:233)
	at com.google.common.io.Files.copy(Files.java:423)
	at org.apache.spark.deploy.yarn.Client$$anonfun$createConfArchive$2.apply(Client.scala:374)
	at org.apache.spark.deploy.yarn.Client$$anonfun$createConfArchive$2.apply(Client.scala:372)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at org.apache.spark.deploy.yarn.Client.createConfArchive(Client.scala:372)
	at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:288)
	at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:466)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:106)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:58)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:141)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:470)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:155)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:192)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:95)
	at spark.benchmarks.JavaWordCount.main(JavaWordCount.java:41)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:619)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
;;;","28/Apr/15 22:09;nravi;PR: https://github.com/apache/spark/pull/5760/;;;","28/Apr/15 22:10;apachespark;User 'nishkamravi2' has created a pull request for this issue:
https://github.com/apache/spark/pull/5760;;;","01/May/15 20:17;srowen;Resolved by https://github.com/apache/spark/pull/5760;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"Add Matrix, SparseMatrix to __all__ list in linalg.py",SPARK-7208,12825675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,josephkb,josephkb,josephkb,28/Apr/15 21:10,29/Apr/15 04:16,14/Jul/23 06:26,29/Apr/15 04:16,1.4.0,,,,,,1.4.0,,,,,,MLlib,PySpark,,,0,,,,,,,,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 04:16:19 UTC 2015,,,,,,,,,,"0|i2e1an:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"28/Apr/15 21:13;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/5759;;;","29/Apr/15 04:16;mengxr;Issue resolved by pull request 5759
[https://github.com/apache/spark/pull/5759];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new spark.ml subpackages to SparkBuild,SPARK-7207,12825674,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,josephkb,josephkb,josephkb,28/Apr/15 21:07,30/Apr/15 21:39,14/Jul/23 06:26,30/Apr/15 21:39,1.4.0,,,,,,1.4.0,,,,,,Build,ML,,,0,,,,,,"Add to project/SparkBuild.scala list of subpackages for spark.ml:
* ml.recommendation
* ml.regression",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 21:39:41 UTC 2015,,,,,,,,,,"0|i2e1af:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"28/Apr/15 21:10;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/5758;;;","30/Apr/15 21:39;mengxr;Issue resolved by pull request 5758
[https://github.com/apache/spark/pull/5758];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support local ivy cache in --packages,SPARK-7205,12825660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,brkyvz,brkyvz,brkyvz,28/Apr/15 20:32,04/Jun/15 23:22,14/Jul/23 06:26,29/Apr/15 06:03,,,,,,,1.4.0,,,,,,Spark Submit,,,,0,,,,,,,,apachespark,brkyvz,eronwright,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8095,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jun 04 23:22:03 UTC 2015,,,,,,,,,,"0|i2e17b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/15 20:35;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/5755;;;","04/Jun/15 04:40;eronwright;I see a potential issue here and filed SPARK-8095.;;;","04/Jun/15 23:22;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/6657;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Call sites in UI are not accurate for DataFrame operations,SPARK-7204,12825657,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,pwendell,pwendell,pwendell,28/Apr/15 20:26,29/Apr/15 07:35,14/Jul/23 06:26,29/Apr/15 07:35,1.3.1,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,"Spark core computes callsites by climbing up the stack until we reach the stack frame at the boundary of user code and spark code. The way we compute whether a given frame is internal (Spark) or user code does not work correctly with the new dataframe API.

Once the scope work goes in, we'll have a nicer way to express units of operator scope, but until then there is a simple fix where we just make sure the SQL internal classes are also skipped as we climb up the stack.",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 21:02:11 UTC 2015,,,,,,,,,,"0|i2e16n:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"28/Apr/15 21:02;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/5757;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
decimal precision lost when loading DataFrame from JDBC,SPARK-7196,12825583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,kgeis,kgeis,28/Apr/15 16:19,25/Aug/15 12:22,14/Jul/23 06:26,30/Apr/15 22:15,1.3.1,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,"I have a decimal database field that is defined as 10.2 (i.e. ##########.##). When I load it into Spark via sqlContext.jdbc(..), the type of the corresponding field in the DataFrame is DecimalType, with precisionInfo None. Because of that loss of precision information, SPARK-4176 is triggered when I try to .saveAsTable(..).",,apachespark,fang fang chen,jmrr,kgeis,rxin,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4176,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 25 12:21:39 UTC 2015,,,,,,,,,,"0|i2e0qf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/15 11:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5777;;;","30/Apr/15 02:18;rxin;[~kgeis] can you check if this https://github.com/apache/spark/pull/5777 fixes your issue?
;;;","30/Apr/15 07:35;kgeis;This does not fix my issue.

{noformat}
scala> val amounts = sqlContext.jdbc(coeusURL, ""(SELECT total_direct_cost_total FROM osp$proposal WHERE rownum < 2)"")
amounts: org.apache.spark.sql.DataFrame = [TOTAL_DIRECT_COST_TOTAL: decimal(10,0)]

scala> amounts.schema(0).dataType.asInstanceOf[org.apache.spark.sql.types.DecimalType].precision
res8: Int = -1

scala> amounts.schema(0).dataType.asInstanceOf[org.apache.spark.sql.types.DecimalType].scale
res9: Int = -1

scala> amounts.schema(0).dataType.asInstanceOf[org.apache.spark.sql.types.DecimalType].precisionInfo
res10: Option[org.apache.spark.sql.types.PrecisionInfo] = None

scala> amounts.saveAsTable(""amounts"")
...
java.lang.RuntimeException: Unsupported datatype DecimalType()
{noformat}
;;;","30/Apr/15 09:56;viirya;[~kgeis] I can't reproduce your problem. As I test in unit test, after applying the [pr|https://github.com/apache/spark/pull/5777], the decimal type have precision and scale now through jdbc. Can you check if you apply the pr and your data schema in original database? If both are checked, can you give us a snippet of example data for test? Thanks.;;;","30/Apr/15 14:44;kgeis;Sorry, I thought I checked out the jdbc_precision branch, but it wasn't.

Let's try again. Downloaded spark-1-jdbc_precision.zip. Retesting... Still not working.

Sample data (in Oracle):
{noformat}
TOTAL_DIRECT_COST_TOTAL
19999
{noformat}

;;;","30/Apr/15 16:31;viirya;[~kgeis] thanks. I think I know where the problem is. I update the jdbc_precision branch. Please apply it and test again.;;;","30/Apr/15 22:11;kgeis;This is now correct:

{noformat}
scala> amounts.schema(0).dataType.asInstanceOf[org.apache.spark.sql.types.DecimalType].precisionInfo
res4: Option[org.apache.spark.sql.types.PrecisionInfo] = Some(PrecisionInfo(12,2))
{noformat}

but now this happens (might be a separate bug):

{noformat}
scala> amounts.saveAsTable(""amounts"")
...
15/04/30 15:05:31 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal
	at org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:365)
	at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:335)
	at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:321)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:699)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:717)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:717)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
...
{noformat}
;;;","30/Apr/15 22:14;rxin;That's probably a separate bug. Do you mind filing a separate JIRA and cc me on it? I merged the fix for this one and going to resolve this ticket. Thanks for helping out with the fix.
;;;","30/Apr/15 22:37;kgeis;I get closer when I apply [PR5803|https://github.com/apache/spark/pull/5803] from SPARK-5456. The field is a DECIMAL(12, 2) in the database. When I save it to Hive and then query off that, the scale is off.

{noformat}
scala> val amounts = sqlContext.jdbc(coeusURL, """"""(SELECT total_direct_cost_total FROM osp$proposal WHERE rownum < 2)"""""")
scala> amounts.show()
TOTAL_DIRECT_COST_TOTAL
19999                  
scala> amounts.saveAsTable(""amounts"")
scala> sqlContext.sql(""SELECT * FROM amounts"").show()
TOTAL_DIRECT_COST_TOTAL
199.99                 
{noformat}
;;;","01/May/15 07:03;viirya;[~kgeis] I can't reproduce this problem too. Would you mind provide more information, such as the schema of amounts (in returned dataframe)? Are you using ""org.apache.spark.sql.parquet"" as your defaultDataSourceName?;;;","01/May/15 07:22;kgeis;My table has

TOTAL_DIRECT_COST_TOTAL NUMBER(12, 2)

I don't understand your question about defaultDataSourceName. I'm not familiar with that. I've pasted almost the entire script except for setting the Oracle JDBC URL and putting the Oracle JDBC driver in the SPARK_CLASSPATH variable.;;;","01/May/15 07:25;viirya;I think NUMBER(12, 2) should be the schema of your Oracle table. Can you do print out amounts.schema to show its schema in dataframe?
;;;","01/May/15 07:30;kgeis;{noformat}
scala> amounts.schema
res1: org.apache.spark.sql.types.StructType = StructType(StructField(TOTAL_DIRECT_COST_TOTAL,DecimalType(12,2),true))
{noformat};;;","01/May/15 07:52;kgeis;I think the problem may be specific to Oracle. If I insert 19999 into a NUMBER(12,2) Oracle field and retrieve it, the result is 19999. In H2, I do the same thing, and I get back 19999.00.

I think that Spark is assuming incorrectly that BigDecimals coming out of result sets have the same scale as the maximum scale allowed for the column.;;;","01/May/15 07:56;viirya;[~kgeis] Yes. Looks like the BigDecimal returned from Oracle has different scale compared to the field definition.;;;","01/May/15 08:25;kgeis;I'm not sure what to do with this. Reopen the issue or file a new one? I'm not sure which project is to blame and how to word it.;;;","01/May/15 08:30;viirya;I think it is different problem to this JIRA. Please file a new JIRA for it.;;;","21/Aug/15 08:42;fang fang chen;I also encountered the same issue during saveAsParquetFile, but the patch didn't resolve my issue.
Without this patch, the error is:
Unsupported datatype DecimalType()
After patching ,the error is:
Unsupported datatype DecimalType(20,2);;;","21/Aug/15 08:43;fang fang chen;Post the error trace here:
java.lang.RuntimeException: Unsupported datatype DecimalType(20,2)
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$fromDataType$2.apply(ParquetTypes.scala:368)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$fromDataType$2.apply(ParquetTypes.scala:312)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetTypes.scala:311)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$4.apply(ParquetTypes.scala:391)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$4.apply(ParquetTypes.scala:390)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.convertFromAttributes(ParquetTypes.scala:389)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.writeMetaData(ParquetTypes.scala:436)
        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.prepareMetadata(newParquet.scala:240)
        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:256)
        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:251)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newParquet.scala:251)
        at org.apache.spark.sql.parquet.ParquetRelation2.<init>(newParquet.scala:369)
        at org.apache.spark.sql.parquet.DefaultSource.createRelation(newParquet.scala:96)
        at org.apache.spark.sql.parquet.DefaultSource.createRelation(newParquet.scala:125)
        at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:308)
        at org.apache.spark.sql.DataFrame.save(DataFrame.scala:1123)
        at org.apache.spark.sql.DataFrame.saveAsParquetFile(DataFrame.scala:922)
        at org.apache.spark.examples.sql.LoadFromMysql_SqlContext$.main(LoadFromMysql_SqlContext.scala:69)
        at org.apache.spark.examples.sql.LoadFromMysql_SqlContext.main(LoadFromMysql_SqlContext.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606);;;","25/Aug/15 12:21;jmrr;Same error as [~fang fang chen], using Spark 1.4.1 and pySpark to convert some mysql tables to parquet files.

{code}
org.apache.spark.sql.AnalysisException: Unsupported datatype DecimalType(20,2);
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$fromDataType$2.apply(ParquetTypes.scala:372)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$fromDataType$2.apply(ParquetTypes.scala:316)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetTypes.scala:315)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$4.apply(ParquetTypes.scala:396)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$4.apply(ParquetTypes.scala:395)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.convertFromAttributes(ParquetTypes.scala:394)
	at org.apache.spark.sql.parquet.RowWriteSupport.init(ParquetTableSupport.scala:150)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:278)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:252)
	at org.apache.spark.sql.parquet.ParquetOutputWriter.<init>(newParquet.scala:83)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anon$4.newInstance(newParquet.scala:229)
	at org.apache.spark.sql.sources.DefaultWriterContainer.initWriters(commands.scala:470)
	at org.apache.spark.sql.sources.BaseWriterContainer.executorSideSetup(commands.scala:360)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:172)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:160)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:160)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code};;;",,,,,,,,,
Exceptions in SerializationDebugger should not crash user code,SPARK-7187,12825390,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,28/Apr/15 05:37,28/Apr/15 07:38,14/Jul/23 06:26,28/Apr/15 07:38,,,,,,,1.3.2,1.4.0,,,,,Spark Core,,,,0,,,,,,"When issues like SPARK-7180 occurs, it ends up crashing user code through the ClosureCleaner in mysterious ways.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 05:53:48 UTC 2015,,,,,,,,,,"0|i2dzkf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/15 05:53;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/5734;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in netty shuffle with spark standalone cluster,SPARK-7183,12825372,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,jhu,jhu,28/Apr/15 04:10,17/May/20 18:30,14/Jul/23 06:26,01/May/15 19:00,1.3.0,,,,,,1.4.0,,,,,,Shuffle,Spark Core,,,0,memory-leak,netty,shuffle,,,"There is slow leak in netty shuffle with spark cluster in {{TransportRequestHandler.streamIds}}

In spark cluster, there are some reusable netty connections between two block managers to get/send blocks between worker/drivers. These connections are handled by the {{org.apache.spark.network.server.TransportRequestHandler}} in server side. This handler keep tracking all the streamids negotiate by RPC when shuffle data need transform in these two block managers and the streamid is keeping increasing, and never get a chance to be deleted exception this connection is dropped (seems never happen in normal running).

Here are some detail logs of this  {{TransportRequestHandler}} (Note: we add a log a print the total size of {{TransportRequestHandler.streamIds}}, the log is ""Current set size is N of org.apache.spark.network.server.TransportRequestHandler@ADDRESS"", this set size is keeping increasing in our test)
{quote}
15/04/22 21:00:16 DEBUG TransportServer: Shuffle server started on port :46288
15/04/22 21:00:16 INFO NettyBlockTransferService: Server created on 46288
15/04/22 21:00:31 INFO TransportRequestHandler: Created TransportRequestHandler org.apache.spark.network.server.TransportRequestHandler@29a4f3e7
15/04/22 21:00:32 TRACE MessageDecoder: Received message RpcRequest: RpcRequest\{requestId=6655045571437304938, message=[B@59778678\}
15/04/22 21:00:32 TRACE NettyBlockRpcServer: Received request: OpenBlocks\{appId=app-20150422210016-0000, execId=<driver>, blockIds=[broadcast_1_piece0]}
15/04/22 21:00:32 TRACE NettyBlockRpcServer: Registered streamId 1387459488000 with 1 buffers
15/04/22 21:00:33 TRACE TransportRequestHandler: Sent result RpcResponse\{requestId=6655045571437304938, response=[B@d2840b\} to client /10.111.7.150:33802
15/04/22 21:00:33 TRACE MessageDecoder: Received message ChunkFetchRequest: ChunkFetchRequest\{streamChunkId=StreamChunkId\{streamId=1387459488000, chunkIndex=0}}
15/04/22 21:00:33 TRACE TransportRequestHandler: Received req from /10.111.7.150:33802 to fetch block StreamChunkId\{streamId=1387459488000, chunkIndex=0\}
15/04/22 21:00:33 INFO TransportRequestHandler: Current set size is 1 of org.apache.spark.network.server.TransportRequestHandler@29a4f3e7
15/04/22 21:00:33 TRACE OneForOneStreamManager: Removing stream id 1387459488000
15/04/22 21:00:33 TRACE TransportRequestHandler: Sent result ChunkFetchSuccess\{streamChunkId=StreamChunkId\{streamId=1387459488000, chunkIndex=0}, buffer=NioManagedBuffer\{buf=java.nio.HeapByteBuffer[pos=0 lim=3839 cap=3839]}} to client /10.111.7.150:33802
15/04/22 21:00:34 TRACE MessageDecoder: Received message RpcRequest: RpcRequest\{requestId=6660601528868866371, message=[B@42bed1b8\}
15/04/22 21:00:34 TRACE NettyBlockRpcServer: Received request: OpenBlocks\{appId=app-20150422210016-0000, execId=<driver>, blockIds=[broadcast_3_piece0]}
15/04/22 21:00:34 TRACE NettyBlockRpcServer: Registered streamId 1387459488001 with 1 buffers
15/04/22 21:00:34 TRACE TransportRequestHandler: Sent result RpcResponse\{requestId=6660601528868866371, response=[B@7fa3fb60\} to client /10.111.7.150:33802
15/04/22 21:00:34 TRACE MessageDecoder: Received message ChunkFetchRequest: ChunkFetchRequest\{streamChunkId=StreamChunkId\{streamId=1387459488001, chunkIndex=0}}
15/04/22 21:00:34 TRACE TransportRequestHandler: Received req from /10.111.7.150:33802 to fetch block StreamChunkId\{streamId=1387459488001, chunkIndex=0\}
15/04/22 21:00:34 INFO TransportRequestHandler: Current set size is 2 of org.apache.spark.network.server.TransportRequestHandler@29a4f3e7
15/04/22 21:00:34 TRACE OneForOneStreamManager: Removing stream id 1387459488001
15/04/22 21:00:34 TRACE TransportRequestHandler: Sent result ChunkFetchSuccess\{streamChunkId=StreamChunkId\{streamId=1387459488001, chunkIndex=0}, buffer=NioManagedBuffer\{buf=java.nio.HeapByteBuffer[pos=0 lim=4277 cap=4277]}} to client /10.111.7.150:33802
15/04/22 21:00:34 TRACE MessageDecoder: Received message RpcRequest: RpcRequest\{requestId=8454597410163901330, message=[B@19c673d1\}
15/04/22 21:00:34 TRACE NettyBlockRpcServer: Received request: OpenBlocks\{appId=app-20150422210016-0000, execId=<driver>, blockIds=[broadcast_2_piece0]}
15/04/22 21:00:34 TRACE NettyBlockRpcServer: Registered streamId 1387459488002 with 1 buffers
15/04/22 21:00:34 TRACE TransportRequestHandler: Sent result RpcResponse\{requestId=8454597410163901330, response=[B@35dbdac2\} to client /10.111.7.150:33802
15/04/22 21:00:34 TRACE MessageDecoder: Received message ChunkFetchRequest: ChunkFetchRequest\{streamChunkId=StreamChunkId\{streamId=1387459488002, chunkIndex=0}}
15/04/22 21:00:34 TRACE TransportRequestHandler: Received req from /10.111.7.150:33802 to fetch block StreamChunkId\{streamId=1387459488002, chunkIndex=0\}
15/04/22 21:00:34 INFO TransportRequestHandler: Current set size is 3 of org.apache.spark.network.server.TransportRequestHandler@29a4f3e7
15/04/22 21:00:34 TRACE OneForOneStreamManager: Removing stream id 1387459488002
......
15/04/22 23:59:50 TRACE MessageDecoder: Received message RpcRequest: RpcRequest\{requestId=5718124278216696100, message=[B@7ade3ea3\}
15/04/22 23:59:50 TRACE NettyBlockRpcServer: Received request: OpenBlocks\{appId=app-20150422210016-0000, execId=<driver>, blockIds=[broadcast_14679_piece0]}
15/04/22 23:59:50 TRACE NettyBlockRpcServer: Registered streamId 1387459501252 with 1 buffers
15/04/22 23:59:50 TRACE TransportRequestHandler: Sent result RpcResponse\{requestId=5718124278216696100, response=[B@40c07a63\} to client /10.111.7.150:33802
15/04/22 23:59:50 TRACE MessageDecoder: Received message ChunkFetchRequest: ChunkFetchRequest\{streamChunkId=StreamChunkId\{streamId=1387459501252, chunkIndex=0}}
15/04/22 23:59:50 TRACE TransportRequestHandler: Received req from /10.111.7.150:33802 to fetch block StreamChunkId\{streamId=1387459501252, chunkIndex=0\}
15/04/22 23:59:50 INFO TransportRequestHandler: Current set size is 13253 of org.apache.spark.network.server.TransportRequestHandler@29a4f3e7
15/04/22 23:59:50 TRACE OneForOneStreamManager: Removing stream id 1387459501252
15/04/22 23:59:50 TRACE TransportRequestHandler: Sent result ChunkFetchSuccess\{streamChunkId=StreamChunkId\{streamId=1387459501252, chunkIndex=0}, buffer=NioManagedBuffer\{buf=java.nio.HeapByteBuffer[pos=0 lim=31474 cap=31474]}} to client /10.111.7.150:33802
15/04/22 23:59:50 TRACE MessageDecoder: Received message RpcRequest: RpcRequest\{requestId=8663805364150028136, message=[B@5974f9b4\}
15/04/22 23:59:50 TRACE NettyBlockRpcServer: Received request: OpenBlocks\{appId=app-20150422210016-0000, execId=<driver>, blockIds=[broadcast_14688_piece0]}
15/04/22 23:59:50 TRACE NettyBlockRpcServer: Registered streamId 1387459501253 with 1 buffers
15/04/22 23:59:50 TRACE TransportRequestHandler: Sent result RpcResponse\{requestId=8663805364150028136, response=[B@122023c6\} to client /10.111.7.150:33802
15/04/22 23:59:50 TRACE MessageDecoder: Received message ChunkFetchRequest: ChunkFetchRequest\{streamChunkId=StreamChunkId\{streamId=1387459501253, chunkIndex=0}}
15/04/22 23:59:50 TRACE TransportRequestHandler: Received req from /10.111.7.150:33802 to fetch block StreamChunkId\{streamId=1387459501253, chunkIndex=0\}
15/04/22 23:59:50 INFO TransportRequestHandler: Current set size is 13254 of org.apache.spark.network.server.TransportRequestHandler@29a4f3e7
15/04/22 23:59:50 TRACE OneForOneStreamManager: Removing stream id 1387459501253
15/04/22 23:59:50 TRACE TransportRequestHandler: Sent result ChunkFetchSuccess\{streamChunkId=StreamChunkId\{streamId=1387459501253, chunkIndex=0}, buffer=NioManagedBuffer\{buf=java.nio.HeapByteBuffer[pos=0 lim=4047 cap=4047]}} to client /10.111.7.150:33802
{quote}",,apachespark,asloane,ilikerps,jhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 23 16:02:56 UTC 2016,,,,,,,,,,"0|i2dzgf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/15 10:19;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5743;;;","01/May/15 19:08;srowen;1.4.0 right? let's set Assignee too;;;","07/May/15 06:38;jhu;Hi, [~sowen]

Do we plan to add this to 1.3+? If there is any plan to release more minor release for 1.3+ like 1.3.2. 
;;;","07/May/15 11:53;srowen;[~ilikerps] what do you think? ok to back port? ;;;","14/May/15 05:38;ilikerps;Sorry for delay, this should be fine to backport, it's a relatively straightforward fix.;;;","23/Mar/16 16:02;asloane;FWIW, this also affects Spark 1.2.
;;;",,,,,,,,,,,,,,,,,,,,,,,
[SQL] Can't remove columns from DataFrame or save DataFrame from a join due to duplicate columns,SPARK-7182,12825371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,dondrake,dondrake,28/Apr/15 04:08,29/May/17 07:52,14/Jul/23 06:26,07/Oct/16 22:07,1.3.1,,,,,,,,,,,,SQL,,,,0,,,,,,"I'm having trouble saving a dataframe as parquet after performing a simple table join.

Below is a trivial example that demonstrates the issue.


The following is from a pyspark session:

{code}
d1=[{'a':1, 'b':2, 'c':3}]
d2=[{'a':1, 'b':2, 'd':4}]

t1 = sqlContext.createDataFrame(d1)
t2 = sqlContext.createDataFrame(d2)

j = t1.join(t2, t1.a==t2.a and t1.b==t2.b)

>>> j
DataFrame[a: bigint, b: bigint, c: bigint, a: bigint, b: bigint, d: bigint]


{code}

Try to get a unique list of the columns:
{code}
u = sorted(list(set(j.columns)))

>>> nt = j.select(*u)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/sql/dataframe.py"", lin
e 586, in select
    jdf = self._jdf.select(self.sql_ctx._sc._jvm.PythonUtils.toSeq(jcols))
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/
java_gateway.py"", line 538, in __call__
  File ""/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/
protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o829.select.
: org.apache.spark.sql.AnalysisException: Reference 'a' is ambiguous, could be: a#0L, a#3L
.;
    at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:2
29)

{code}

That didn't work, save the file (that works), but reading it back in fails.:
{code}
j.saveAsParquetFile('j')

>>> z = sqlContext.parquetFile('j')
>>> z.take(1)
...
: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 171 in stage 104.0 failed 1 times, most recent failure: Lost task 171.0 in stage 104.0 (TID 1235, localhost): parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/Users/drake/fd/spark/j/part-r-00172.parquet
	at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:213)
{code}",,adrian-wang,dondrake,smilegator,Storm0111,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 29 07:52:38 UTC 2017,,,,,,,,,,"0|i2dzg7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/15 07:02;adrian-wang;you should use like
j = t1.join(t2, t1.a==t2.a and t1.b==t2.b).select(t1.a.alias(""t1a""), t1.b.alias(""t1b""), t2.a.alias(""t2a""), t1.b.alias(""t1b2""), t1.c, t2.d);;;","07/Oct/16 22:07;smilegator;Please try it in the master branch. If it still does not work, please reopen it. Thanks!;;;","29/May/17 07:52;Storm0111;Xiao Li.. I'm getting the same error when I try to do .dropDuplicates. Changing the alias cannot be a solution as , if I have 40 column in each dataframe its merely impossible to change the name of each. So I tried to do drop Dupicates but its giving the same  u""Reference 'xcol14x' is ambiguous exception
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
External Sorter merge with aggregation go to an infinite loop when we have a total ordering,SPARK-7181,12825368,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,chouqin,chouqin,chouqin,28/Apr/15 03:52,29/Apr/15 22:53,14/Jul/23 06:26,29/Apr/15 22:53,1.3.1,,,,,,1.2.3,1.3.2,1.4.0,,,,Spark Core,,,,0,,,,,,"In the function {{mergeWithAggregation}} of {{ExternalSorter.scala}}, when there is a total ordering for keys K, values of the same key in the sorted iterator should be combined. Currently this is done by this:

{code}
  val elem = sorted.next()
  val k = elem._1
  var c = elem._2
  while (sorted.hasNext && sorted.head._1 == k) {
    c = mergeCombiners(c, sorted.head._2)
  }
{code}

This will go to an infinite loop when there are more than 1 values with the same key. `sorted.next()` should be called to fix this.",,apachespark,chouqin,zhangxiongfei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 22:53:03 UTC 2015,,,,,,,,,,"0|i2dzfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/15 06:26;apachespark;User 'chouqin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5737;;;","29/Apr/15 22:53;srowen;Issue resolved by pull request 5737
[https://github.com/apache/spark/pull/5737];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SerializationDebugger fails with ArrayOutOfBoundsException,SPARK-7180,12825364,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,andrewor14,andrewor14,28/Apr/15 03:31,19/Jun/15 18:07,14/Jul/23 06:26,19/Jun/15 17:52,1.3.0,,,,,,1.4.1,1.5.0,,,,,Spark Core,,,,0,,,,,,"Simple reproduction:
{code}
class Parent extends Serializable {
  val a = ""a""
  val b = ""b""
}

class Child extends Parent with Serializable {
  val c = Array(1)
  val d = Array(2)
  val e = Array(3)
  val f = Array(4)
  val g = Array(5)
  val o = new Object
}

// ArrayOutOfBoundsException
SparkEnv.get.closureSerializer.newInstance().serialize(new Child)
{code}

I dug into this a little and found that we are trying to fill the fields of `Parent` with the values of `Child`. See the following output I generated by adding println's everywhere:
{code}
* Visiting object org.apache.spark.serializer.Child@2c3299f6 of type org.apache.spark.serializer.Child
  - Found 2 class data slot descriptions
  - Looking at desc #1: org.apache.spark.serializer.Parent: static final long serialVersionUID = 3254964199136071914L;
    - Found 2 fields
      - Ljava/lang/String; a
      - Ljava/lang/String; b
    - getObjFieldValues: 
      - [I@23faa614
      - [I@1cad7d80
      - [I@420a6d35
      - [I@3a87d472
      - [I@2b8ca663
      - java.lang.Object@1effc3eb
{code}
SerializationDebugger#visitSerializable found two fields that belong to the parents, but it tried to cram the child's values into these two fields. The mismatch of number of fields here throws the ArrayOutOfBoundExceptions as a result. The culprit is this line: https://github.com/apache/spark/blob/4d9e560b5470029143926827b1cb9d72a0bfbeff/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala#L150, which runs reflection on the object `Child` even when it's considering the description for `Parent`.

I ran into this when trying to serialize a test suite that extends `FunSuite` (don't ask why).",,andrewor14,apachespark,pwendell,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8012,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 03 22:00:03 UTC 2015,,,,,,,,,,"0|i2dzen:",9223372036854775807,,,,,,,,,,,,,,1.4.1,1.5.0,,,,,,,,,,,,"28/Apr/15 06:53;pwendell;/cc [~rxin];;;","03/Jun/15 22:00;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/6625;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Launcher error in yarn-client,SPARK-7162,12824171,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gq,gq,gq,27/Apr/15 06:57,27/Apr/15 23:53,14/Jul/23 06:26,27/Apr/15 23:53,1.4.0,,,,,,1.4.0,,,,,,YARN,,,,0,,,,,,"{code:none} 
HADOOP_CONF_DIR=/usr/local/CDH5/hadoop-2.3.0-cdh5.0.1/etc/hadoop/ ./bin/spark-shell --master yarn-client --driver-memory 8g  --driver-libraryath $LD_LIBRARY_PATH:$JAVA_LIBRARY_PATH  --jars lib/hadoop-lzo-0.4.15-gplextras5.0.1-SNAPSHOT.jar
{code} => 
{code:none}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.4.0-SNAPSHOT
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_55)
Type in expressions to have them evaluated.
Type :help for more information.
spark.yarn.driver.memoryOverhead is set but does not apply in client mode.
java.io.FileNotFoundException: /etc/hadoop/hadoop (是一个目录)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:146)
        at org.spark-project.guava.io.Files$FileByteSource.openStream(Files.java:124)
        at org.spark-project.guava.io.Files$FileByteSource.openStream(Files.java:114)
        at org.spark-project.guava.io.ByteSource.copyTo(ByteSource.java:182)
        at org.spark-project.guava.io.Files.copy(Files.java:417)
        at org.apache.spark.deploy.yarn.Client$$anonfun$createConfArchive$2.apply(Client.scala:374)
        at org.apache.spark.deploy.yarn.Client$$anonfun$createConfArchive$2.apply(Client.scala:372)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
        at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
        at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
        at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
        at org.apache.spark.deploy.yarn.Client.createConfArchive(Client.scala:372)
        at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:288)
        at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:466)
        at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:106)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:58)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:141)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:469)
        at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:1016)
        at $iwC$$iwC.<init>(<console>:9)
        at $iwC.<init>(<console>:18)
        at <init>(<console>:20)
        at .<init>(<console>:24)
        at .<clinit>(<console>)
        at .<init>(<console>:7)
        at .<clinit>(<console>)
        at $print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:856)
        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:901)
        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:813)
        at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:123)
        at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
        at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:324)
        at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
        at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:64)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:973)
        at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:157)
        at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:64)
        at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
        at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:64)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:990)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:944)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1058)
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:607)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:167)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:190)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

{code}",,apachespark,gq,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 23:53:31 UTC 2015,,,,,,,,,,"0|i2dsdz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/15 08:16;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/5716;;;","27/Apr/15 11:23;srowen;Looks OK but these dirs typically do not contain subdirectories.;;;","27/Apr/15 23:53;srowen;Resolved by https://github.com/apache/spark/pull/5716;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"SparkContext's newAPIHadoopFile does not support comma-separated list of files, but the other API hadoopFile does.",SPARK-7155,12824104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yongtang,yongtang,yongtang,26/Apr/15 18:02,18/Aug/15 12:26,14/Jul/23 06:26,29/Apr/15 22:57,1.3.1,,,,,,1.3.2,1.4.0,,,,,Spark Core,,,,0,,,,,,"SparkContext's newAPIHadoopFile() does not support comma-separated list of files. For example, the following:

sc.newAPIHadoopFile(""/root/file1.txt,/root/file2.txt"", classOf[TextInputFormat], classOf[LongWritable], classOf[Text])

will throw

org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: file:/root/file1.txt,/root/file2.txt

However, the other API hadoopFile() is able to process comma-separated list of files correctly.

In addition, since sc.textFile() uses hadoopFile(), it is also able to process comma-separated list of files correctly.

The problem is that newAPIHadoopFile() use addInputPath() to add the file path into NewHadoopRDD. See Ln 928-931, master branch:
    val job = new NewHadoopJob(conf)
    NewFileInputFormat.addInputPath(job, new Path(path))
    val updatedConf = job.getConfiguration
    new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf).setName(path)

Change addInputPath(job, new Path(path)) to addInputPaths(job, path) will resolve this issue.",Ubuntu 14.04,apachespark,yongtang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5544,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 29 22:57:40 UTC 2015,,,,,,,,,,"0|i2drzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/15 19:01;apachespark;User 'yongtang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5708;;;","29/Apr/15 22:57;srowen;Resolved by https://github.com/apache/spark/pull/5708;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
support Long type ordinal in GetItem,SPARK-7153,12824074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,26/Apr/15 11:13,23/Jun/15 00:37,14/Jul/23 06:26,23/Jun/15 00:37,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"In GetItem, we will cast the ordinal into Int first. However, if the ordinal is Long type, execution will fail even the value of ordinal meets the requirement. The reason is boxing. In java, we can convert long to int, but can't convert Long to Integer.
{code}
test(""get item"") {
  jsonRDD(sparkContext.makeRDD(
    """"""{""a"": [1,2,3], ""b"": 2}"""""" :: Nil)).registerTempTable(""t"")
  checkAnswer(sql(""SELECT a[b] FROM t""), Row(3))
}
{code}
This test will fail as ""b"" is inferred as Long type.",,apachespark,cloud_fan,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 23 00:37:55 UTC 2015,,,,,,,,,,"0|i2drsn:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.5.0,,,,,,,,,,,,,"26/Apr/15 11:45;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/5706;;;","23/Jun/15 00:37;marmbrus;Issue resolved by pull request 5706
[https://github.com/apache/spark/pull/5706];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Defalt system alias problem,SPARK-7149,12824061,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,haiyang,haiyang,26/Apr/15 06:45,07/Oct/16 22:32,14/Jul/23 06:26,07/Oct/16 22:32,,,,,,,,,,,,,SQL,,,,0,,,,,,"Fix default system alias problem.

execute the sql statement will cause problem: 

select substr(value, 0, 2), key as c0 from testData order by c0


org.apache.spark.sql.AnalysisException: Reference 'c0' is ambiguous, could be: c0#42, c0#41.;",,adrian-wang,apachespark,freiss,haiyang,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 22:32:07 UTC 2016,,,,,,,,,,"0|i2drpr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 03:07;adrian-wang;what's the schema of your testData table?

I have tried the one defined in TestData.scala, which is defined as case class TestData(key: Int, value: String), and I get no exception.

Is this because you have a `c0` in your testData schema?;;;","02/May/15 05:35;apachespark;User 'haiyangsea' has created a pull request for this issue:
https://github.com/apache/spark/pull/5861;;;","02/May/15 06:38;haiyang;If we give no alias to a function in project,the SqlParser will give it a default alias like c0,c1,...

When we execute the statement like ""select substr(value, 0, 2), key as c0 from testData order by c0"" will throw exception.;;;","07/Oct/16 22:32;smilegator;Since we changed the way to generate the alias name. c0 is not the alias name now. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
ClosureCleaner does not handle nesting properly,SPARK-7121,12823711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,24/Apr/15 10:16,02/May/15 06:56,14/Jul/23 06:26,02/May/15 06:56,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"For instance, in SparkContext, I tried to do the following:
{code}
def scope[T](body: => T): T = body // no-op
def myCoolMethod(path: String): RDD[String] = scope {
  parallelize(1 to 10).map { _ => path }
}
{code}
and I got an exception complaining that SparkContext is not serializable. The issue here is that the inner closure is getting its path from the outer closure (the scope), but the outer closure actually references the SparkContext object itself to get the `parallelize` method.

Note, however, that the inner closure doesn't actually need the SparkContext; it just needs a field from the outer closure. If we modify ClosureCleaner to clean the outer closure recursively while using the fields accessed by the inner closure, then we can serialize the inner closure.

This is blocking my effort on a visualization task.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 10:19:32 UTC 2015,,,,,,,,,,"0|i2dplj:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"24/Apr/15 10:19;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/5685;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ScriptTransform doesn't consider the output data type,SPARK-7119,12823692,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zhichao-li,chenghao,chenghao,24/Apr/15 08:49,06/Aug/15 16:45,14/Jul/23 06:26,05/Aug/15 01:26,1.3.0,1.3.1,1.4.0,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"{code:sql}
from (from src select transform(key, value) using 'cat' as (thing1 int, thing2 string)) t select thing1 + 2;
{code}

{noformat}
15/04/24 00:58:55 ERROR CliDriver: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ClassCastException: org.apache.spark.sql.types.UTF8String cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:57)
	at org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:127)
	at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:68)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:52)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:819)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:819)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1618)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1618)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:209)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{noformat}",,apachespark,chenghao,jameszhouyi,marmbrus,maropu,yhuai,zhichao-li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 05 01:26:50 UTC 2015,,,,,,,,,,"0|i2dphb:",9223372036854775807,,,,,joshrosen,,,,,,,,,1.5.0,,,,,,,,,,,,,"24/Apr/15 12:18;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5688;;;","28/May/15 14:13;yhuai;[~chenghao]
Can you try?
{{from (from src select transform(key, value) using 'cat' as (thing1 string, thing2 string)) t select cast(thing1 as int) + 2;}};;;","04/Jun/15 08:29;apachespark;User 'zhichao-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/6638;;;","05/Jun/15 00:46;zhichao-li;This workaround query can be executed correctly and there's a simple fix for this issue by the way :);;;","09/Jul/15 00:50;jameszhouyi;The issue blocked Spark SQL query relative to scriptTransform so hopefully it can be fixed in 1.5.0;;;","04/Aug/15 01:02;chenghao;[~marmbrus] This is actually a bug fixing, and it blocks the bigbench testing for quite a long time, since the code is ready (to be reviewed), can we add it back to the 1.5 target list?;;;","05/Aug/15 01:26;marmbrus;Issue resolved by pull request 6638
[https://github.com/apache/spark/pull/6638];;;",,,,,,,,,,,,,,,,,,,,,,
parse error for DataFrame.filter after aggregate,SPARK-7114,12823671,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,24/Apr/15 07:08,06/Jul/15 20:27,14/Jul/23 06:26,06/Jul/15 20:27,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"DataFrame.filter has 2 overloaded versions. One of it accept String parameter to represent condition expression.
{code}
val df = ... // df has 2 columns: key, value
val agg = df.groupBy(""key"").count()
agg.filter(df(""count"") > 1) // this success
agg.filter(""count > 1"") // this failed
{code}
the error message is:
{code}
[1.7] failure: ``('' expected but `>' found

count > 1
      ^
java.lang.RuntimeException: [1.7] failure: ``('' expected but `>' found

count > 1
      ^
{code}",,apachespark,cloud_fan,holden,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 06 10:58:08 UTC 2015,,,,,,,,,,"0|i2dpcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/15 07:13;cloud_fan;cc [~rxin];;;","24/Apr/15 07:17;rxin;This is a problem with the current simple SQL parser, which treats count as a keyword. I think you can work around it by using `count` > 1.

;;;","24/Apr/15 07:17;rxin;cc [~marmbrus];;;","06/Jul/15 10:58;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/7237;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add parameter for zookeeper.znode.parent to hbase_inputformat.py,SPARK-7107,12823627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tedyu@apache.org,yuzhihong@gmail.com,yuzhihong@gmail.com,24/Apr/15 01:00,27/Apr/15 21:42,14/Jul/23 06:26,27/Apr/15 21:42,,,,,,,1.4.0,,,,,,Examples,PySpark,,,0,,,,,,"[~yeshavora] first reported encountering the following exception running hbase_inputformat.py :
{code}
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.
: java.lang.RuntimeException: java.lang.NullPointerException
at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries(RpcRetryingCaller.java:208)
at org.apache.hadoop.hbase.client.ClientScanner.call(ClientScanner.java:313)
at org.apache.hadoop.hbase.client.ClientScanner.nextScanner(ClientScanner.java:288)
at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction(ClientScanner.java:160)
{code}
It turned out that the hbase cluster has custom znode parent:
{code}
    <property>
      <name>zookeeper.znode.parent</name>
      <value>/hbase-unsecure</value>
    </property>
{code}
hbase_inputformat.py should support specification of custom znode parent.",,apachespark,rxin,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 01:13:11 UTC 2015,,,,,,,,,,"0|i2dp2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/15 01:13;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/5673;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext.union crashed when some RDDs have no partitioner,SPARK-7103,12823568,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,stevenshe,stevenshe,stevenshe,23/Apr/15 21:24,27/Apr/15 22:56,14/Jul/23 06:26,27/Apr/15 22:55,1.3.0,1.3.1,,,,,1.3.2,1.4.0,,,,,Spark Core,,,,0,,,,,,"I encountered a bug where Spark crashes with the following stack trace:

{noformat}
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:313)
	at scala.None$.get(Option.scala:311)
	at org.apache.spark.rdd.PartitionerAwareUnionRDD.getPartitions(PartitionerAwareUnionRDD.scala:69)
{noformat}

Here's a minimal example that reproduces it on the Spark shell:

{noformat}
val x = sc.parallelize(Seq(1->true,2->true,3->false)).partitionBy(new HashPartitioner(1))
val y = sc.parallelize(Seq(1->true))
sc.union(y, x).count() // crashes
sc.union(x, y).count() // This works since the first RDD has a partitioner
{noformat}

We had to resort to instantiating the UnionRDD directly to avoid the PartitionerAwareUnionRDD.",,apachespark,glenn.strycker@gmail.com,pwendell,stevenshe,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 22:55:34 UTC 2015,,,,,,,,,,"0|i2doqn:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"23/Apr/15 22:13;srowen;Looks like the check needs to be expanded. To this:

{code}
require(rdds.flatMap(_.partitioner).toSet.size == 1
{code}

add before:

{code}
require(rdds.count(_.partitioner.isDefined) == rdds.size)
{code}

I think. Want to try a PR?;;;","24/Apr/15 05:22;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/5678;;;","24/Apr/15 06:01;apachespark;User 'stevencanopy' has created a pull request for this issue:
https://github.com/apache/spark/pull/5679;;;","24/Apr/15 06:02;stevenshe;Submitted PR #5679 with an added condition to SparkContext.union and a precondition check to PartitionerAwareUnionRDD.;;;","24/Apr/15 07:58;vinodkc;I closed PR #5678..
thanks;;;","24/Apr/15 20:42;pwendell;Escalated the priority since IMO this is good to fix.;;;","27/Apr/15 22:55;srowen;Issue resolved by pull request 5679
[https://github.com/apache/spark/pull/5679];;;",,,,,,,,,,,,,,,,,,,,,,
GradientBoostTrees leaks a persisted RDD,SPARK-7100,12823526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jimfcarroll,jimfcarroll,jimfcarroll,23/Apr/15 19:15,28/Apr/15 11:52,14/Jul/23 06:26,28/Apr/15 11:51,1.2.2,1.3.1,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,"It appears GradientBoostedTrees.scala can call 'persist' on an RDD and never unpersist it.

In the master branch it's in GradientBoostedTrees.boost method. It ""persists"" the input RDD if it's not already persisted but doesn't unpersist it.

I'll be submitting a PR with a fix.",,apachespark,jimfcarroll,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 11:51:12 UTC 2015,,,,,,,,,,"0|i2dohb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"23/Apr/15 19:21;apachespark;User 'jimfcarroll' has created a pull request for this issue:
https://github.com/apache/spark/pull/5669;;;","23/Apr/15 19:36;srowen;(Don't set fix/target version, and I don't think this is major. Have a look at https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark);;;","23/Apr/15 21:08;jimfcarroll;Sorry about the fix/target version. I should have looked fist.;;;","28/Apr/15 11:51;srowen;Issue resolved by pull request 5669
[https://github.com/apache/spark/pull/5669];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent Timestamp behavior when used in WHERE clause,SPARK-7098,12823521,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,peterhagelund,peterhagelund,23/Apr/15 19:02,15/May/15 03:50,14/Jul/23 06:26,15/May/15 03:50,1.2.1,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,java.sql.Timestamps containing a nano value of 0 are not matched in a WHERE clause like SELECT ... FROM ... WHERE TS_COL = '2015-04-23 02:55:00.0'. When used with an explicit cast like SELECT ... FROM ... WHERE TS_COL = CAST('2015-04-23 02:55:00.0' AS TIMESTAMP) things work as expected.,"Windows, Linux, Mac OS X with Java 1.7",apachespark,peterhagelund,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 08:12:30 UTC 2015,,,,,,,,,,"0|i2dogf:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"24/Apr/15 08:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5682;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[REGRESSION] Spark 1.3.1 breaks analysis of third-party logical plans,SPARK-7088,12823354,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,smolav,smolav,smolav,23/Apr/15 09:22,28/Jun/15 10:20,14/Jul/23 06:26,24/Jun/15 19:29,1.3.1,,,,,,1.5.0,,,,,,SQL,,,,1,regression,,,,,"We're using some custom logical plans. We are now migrating from Spark 1.3.0 to 1.3.1 and found a few incompatible API changes. All of them seem to be in internal code, so we understand that. But now the ResolveReferences rule, that used to work with third-party logical plans just does not work, without any possible workaround that I'm aware other than just copying ResolveReferences rule and using it with our own fix.

The change in question is this section of code:
{code}
        }.headOption.getOrElse { // Only handle first case, others will be fixed on the next pass.
          sys.error(
            s""""""
              |Failure when resolving conflicting references in Join:
              |$plan
              |
              |Conflicting attributes: ${conflictingAttributes.mkString("","")}
              """""".stripMargin)
        }
{code}

Which causes the following error on analysis:

{code}
Failure when resolving conflicting references in Join:
'Project ['l.name,'r.name,'FUNC1('l.node,'r.node) AS c2#37,'FUNC2('l.node,'r.node) AS c3#38,'FUNC3('r.node,'l.node) AS c4#39]
 'Join Inner, None
  Subquery l
   Subquery h
    Project [name#12,node#36]
     CustomPlan H, u, (p#13L = s#14L), [ord#15 ASC], IS NULL p#13L, node#36
      Subquery v
       Subquery h_src
        LogicalRDD [name#12,p#13L,s#14L,ord#15], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:37
  Subquery r
   Subquery h
    Project [name#40,node#36]
     CustomPlan H, u, (p#41L = s#42L), [ord#43 ASC], IS NULL pred#41L, node#36
      Subquery v
       Subquery h_src
        LogicalRDD [name#40,p#41L,s#42L,ord#43], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:37
{code}",,apachespark,marmbrus,opuertas,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6595,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 19:29:23 UTC 2015,,,,,,,,,,"0|i2dng7:",9223372036854775807,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,,,"11/May/15 22:06;smolav;Any thoughts on this?;;;","17/Jun/15 08:31;apachespark;User 'smola' has created a pull request for this issue:
https://github.com/apache/spark/pull/6853;;;","24/Jun/15 19:29;marmbrus;Issue resolved by pull request 6853
[https://github.com/apache/spark/pull/6853];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistent default miniBatchFraction parameters in the train methods of RidgeRegression,SPARK-7085,12823339,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Kuromatsu,Kuromatsu,Kuromatsu,23/Apr/15 08:24,23/Apr/15 21:03,14/Jul/23 06:26,23/Apr/15 21:00,1.3.1,,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,"The miniBatchFraction parameter in the train method called with 4 arguments is 0.01, that is,

{code:title=RidgeRegression.scala|borderStyle=solid}
def train(
      input: RDD[LabeledPoint],
      numIterations: Int,
      stepSize: Double,
      regParam: Double): RidgeRegressionModel = {
    train(input, numIterations, stepSize, regParam, 0.01)
  }
{code}

but, the parameter is 1.0 in the other train methods. For example,
{code:title=RidgeRegression.scala|borderStyle=solid}
  def train(
      input: RDD[LabeledPoint],
      numIterations: Int): RidgeRegressionModel = {
    train(input, numIterations, 1.0, 0.01, 1.0)
  }
{code}


",,apachespark,josephkb,Kuromatsu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,604800,604800,,0%,604800,604800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 21:00:51 UTC 2015,,,,,,,,,,"0|i2dncv:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"23/Apr/15 10:21;apachespark;User 'kuromatsu-nobuyuki' has created a pull request for this issue:
https://github.com/apache/spark/pull/5658;;;","23/Apr/15 21:00;josephkb;Issue resolved by pull request 5658
[https://github.com/apache/spark/pull/5658];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
LDA.setBeta calls itself,SPARK-7070,12823302,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,23/Apr/15 05:25,23/Apr/15 21:47,14/Jul/23 06:26,23/Apr/15 21:47,1.3.1,,,,,,1.3.2,1.4.0,,,,,MLlib,,,,0,,,,,,"Should call setTopicConcentration.

Reported by buring: http://apache-spark-user-list.1001560.n3.nabble.com/LDA-code-little-error-Xiangrui-Meng-td22621.html",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 21:47:14 UTC 2015,,,,,,,,,,"0|i2dn47:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"23/Apr/15 05:28;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5649;;;","23/Apr/15 21:47;mengxr;Issue resolved by pull request 5649
[https://github.com/apache/spark/pull/5649];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't resolve nested column in ORDER BY,SPARK-7067,12823293,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,23/Apr/15 05:01,19/Jun/15 10:38,14/Jul/23 06:26,17/Jun/15 21:46,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"In order to avoid breaking existing HiveQL queries, the current way we resolve column in ORDER BY is: first resolve based on what comes from the select clause and then fall back on its child only when this fails.

However, this case will fail:
{code}
test(""orderby queries"") {
  jsonRDD(sparkContext.makeRDD(
    """"""{""a"": {""b"": [{""c"": 1}]}, ""b"": [{""d"": 1}]}"""""" :: Nil)).registerTempTable(""t"")
  sql(""SELECT a.b FROM t ORDER BY b[0].d"").queryExecution.analyzed
}
{code}

As hive doesn't support resolve ORDER BY attribute not exist in select clause, so this problem is spark sql only.",,apachespark,cloud_fan,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 17 21:46:17 UTC 2015,,,,,,,,,,"0|i2dn27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/15 09:59;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/5659;;;","17/Jun/15 21:46;marmbrus;Issue resolved by pull request 5659
[https://github.com/apache/spark/pull/5659];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"VectorAssembler should use NumericType, not NativeType",SPARK-7066,12823281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,23/Apr/15 02:42,23/Apr/15 04:35,14/Jul/23 06:26,23/Apr/15 04:35,,,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,,,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 02:43:11 UTC 2015,,,,,,,,,,"0|i2dmzj:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"23/Apr/15 02:43;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5642;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Update lz4 for Java 7 to avoid: when lz4 compression is used, it causes core dump",SPARK-7063,12823270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jennyma,jennyma,jennyma,23/Apr/15 01:55,18/May/15 21:48,14/Jul/23 06:26,18/May/15 21:48,1.3.1,,,,,,1.5.0,,,,,,Spark Core,,,,0,,,,,,"this issue is initially noticed by using IBM JDK, below please find the stack track of this issue, caused by violating the rule in critical section. 

#0 0x000000314340f3cb in raise () from /service/pmrs/45638/20/lib64/libpthread.so.0
#1 0x00007f795b0323be in j9dump_create () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9prt27.so
#2 0x00007f795a88ba2a in doSystemDump () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9dmp27.so
#3 0x00007f795b0405d5 in j9sig_protect () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9prt27.so
#4 0x00007f795a88a1fd in runDumpFunction () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9dmp27.so
#5 0x00007f795a88dbab in runDumpAgent () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9dmp27.so
#6 0x00007f795a8a1c49 in triggerDumpAgents () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9dmp27.so
#7 0x00007f795a4518fe in doTracePoint () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9trc27.so
#8 0x00007f795a45210e in j9Trace () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9trc27.so
#9 0x00007f79590e46e1 in MM_StandardAccessBarrier::jniReleasePrimitiveArrayCritical(J9VMThread*, _jarray*, void*, int) ()
from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9gc27.so
#10 0x00007f7938bc397c in Java_net_jpountz_lz4_LZ4JNI_LZ4_1compress_1limitedOutput () from /service/pmrs/45638/20/tmp/liblz4-java7155003924599399415.so
#11 0x00007f795b707149 in VMprJavaSendNative () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9vm27.so
#12 0x0000000000000000 in ?? ()

this is an issue introduced by a bug in net.jpountz.lz4.lz4-1.2.0.jar, and fixed in 1.3.0 version.  Sun JDK /Open JDK doesn't complain this issue, but this issue will trigger assertion failure when IBM JDK is used. here is the link to the fix 
https://github.com/jpountz/lz4-java/commit/07229aa2f788229ab4f50379308297f428e3d2d2 


",IBM JDK,apachespark,jennyma,maropu,pwendell,t.p.ellison@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 18 21:48:02 UTC 2015,,,,,,,,,,"0|i2dmx3:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"23/Apr/15 01:57;jennyma;we should bump the version to net.jpountz.lz4.lz4-1.3.0.jar in Spark parent pom file. ;;;","23/Apr/15 01:57;jennyma;will send out a pull request shortly. ;;;","23/Apr/15 02:24;apachespark;User 'linlin200605' has created a pull request for this issue:
https://github.com/apache/spark/pull/5641;;;","27/Apr/15 19:04;srowen;Targeting this for later when we can take a Java 7 dependency.;;;","14/May/15 21:03;pwendell;[~srowen] so I think maybe we can pull this into master now, given that we'll drop 1.6 in Spark 1.5 (?);;;","14/May/15 22:29;srowen;Yes, this is OK to merge for master / 1.5, but not 1.4 or before.;;;","15/May/15 11:43;t.p.ellison@gmail.com;I can confirm that this failure is no longer seen using LZ4 1.3.0 with IBM Java 7+.;;;","18/May/15 02:55;apachespark;User 'JihongMA' has created a pull request for this issue:
https://github.com/apache/spark/pull/6226;;;","18/May/15 21:48;srowen;Issue resolved by pull request 6226
[https://github.com/apache/spark/pull/6226];;;",,,,,,,,,,,,,,,,,,,,
Task deserialization time metric does not include time to deserialize broadcasted RDDs,SPARK-7058,12823115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,22/Apr/15 17:40,23/Apr/15 20:25,14/Jul/23 06:26,23/Apr/15 20:25,1.1.2,1.2.3,1.3.1,1.4.0,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"The web UI's ""task deserialization time"" metric is slightly misleading because it does not capture the time taken to deserialize the broadcasted RDD.  Currently, this statistic is measured in {{Executor.run}} by measuring the time to deserialize the {{Task}} instance: https://github.com/apache/spark/blob/bdc5c16e76c5d0bc147408353b2ba4faa8e914fc/core/src/main/scala/org/apache/spark/executor/Executor.scala#L193

As of Spark 1.1.0, we transfer RDDs using broadcast variables rather than sending them directly as part of the {{Task}} object (see SPARK-2521 for more details).  As a result, the deserialization of the RDD is performed outside of this block and is not accounted for in this statistic.  As a result, the reported task deserialization time may be a severe underestimate of the actual time.

To measure actual RDD deserialization time, I hacked the following change into ShuffleMapTask:

{code}
diff --git a/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala b/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala
index 6c7d000..adab574 100644
--- a/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala
+++ b/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala
@@ -57,8 +57,10 @@ private[spark] class ShuffleMapTask(
   override def runTask(context: TaskContext): MapStatus = {
     // Deserialize the RDD using the broadcast variable.
     val ser = SparkEnv.get.closureSerializer.newInstance()
+    val deserializeStartTime = System.currentTimeMillis()
     val (rdd, dep) = ser.deserialize[(RDD[_], ShuffleDependency[_, _, _])](
       ByteBuffer.wrap(taskBinary.value), Thread.currentThread.getContextClassLoader)
+    println(s""Deserialized a shuffle map task in ${System.currentTimeMillis() - deserializeStartTime} ms"")
{code}

For one of my benchmark jobs (a SQL aggregation query that used code generation), the actual deserialization time was ~150ms per task even though the UI only reported 1ms.

I think that this should be pretty easy to fix by simply adding additional calls in ShuffleMapTask and ResultTask to increment the deserialization time metric.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 22 18:04:03 UTC 2015,,,,,,,,,,"0|i2dm5b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/15 18:04;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5635;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
getContextOrSparkClassLoader is not used while loading JDBC driver class,SPARK-7055,12822996,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vinodkc,vinodkc,vinodkc,22/Apr/15 11:35,24/Apr/15 00:13,14/Jul/23 06:26,23/Apr/15 19:00,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"In JDBCRDD.scala, getConnector method uses Class.forName to load JDBC driver class .Instead it should use getContextOrSparkClassLoader.loadClass method
There was a resolved JIRA : SPARK-6966 , which handled same issue only in 'JDBCRelation.scala'",,apachespark,marmbrus,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 19:00:41 UTC 2015,,,,,,,,,,"0|i2dlfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/15 11:37;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/5633;;;","23/Apr/15 19:00;marmbrus;Issue resolved by pull request 5633
[https://github.com/apache/spark/pull/5633];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Python Kafka test assembly jar not found issue under Maven build,SPARK-7050,12822931,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,22/Apr/15 07:55,08/Jul/15 11:24,14/Jul/23 06:26,08/Jul/15 11:23,1.3.1,,,,,,1.5.0,,,,,,Build,,,,0,,,,,,"The behavior of {{mvn package}} and {{sbt kafka-assembly/assembly}} under kafka-assembly module is different, sbt will generate an assembly jar under target/scala-version/, while mvn generates this jar under target/, which will make python Kafka streaming unit test fail to find the related jar.",,apachespark,cocoatomo,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 11:23:27 UTC 2015,,,,,,,,,,"0|i2dl1j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/15 09:20;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/5632;;;","06/Jul/15 08:19;srowen;Yeah so [~jerryshao] given the PR I'm suggesting you update the JIRA and PR title to reflect the much more important purpose of the change: actually run Python Kafka tests.;;;","06/Jul/15 08:25;jerryshao;Thanks [~srowen], how about this title?;;;","08/Jul/15 11:23;srowen;Issue resolved by pull request 5632
[https://github.com/apache/spark/pull/5632];;;",,,,,,,,,,,,,,,,,,,,,,,,,
[Spark SQL] query would hang when using scripts in SQL statement,SPARK-7044,12822898,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,jameszhouyi,jameszhouyi,22/Apr/15 03:26,24/Apr/15 05:54,14/Jul/23 06:26,24/Apr/15 05:54,1.3.0,1.4.0,,,,,1.3.0,1.4.0,,,,,SQL,,,,0,,,,,,"Query with 'USING' operator like below would hang when using scripts in SQL statement
{code}
INSERT INTO TABLE ${hiveconf:RESULT_TABLE}
SELECT pid1, pid2, COUNT (*) AS cnt
FROM (
  --Make items basket
  FROM (
    -- Joining two tables
    SELECT s.ss_ticket_number AS oid , s.ss_item_sk AS pid
    FROM store_sales s
    INNER JOIN item i ON (s.ss_item_sk = i.i_item_sk)
    WHERE i.i_category_id in (${hiveconf:q01_i_category_id_IN})
    AND s.ss_store_sk in (${hiveconf:q01_ss_store_sk_IN})
    CLUSTER BY oid
  ) q01_map_output
  REDUCE q01_map_output.oid, q01_map_output.pid
  USING '${env:BIG_BENCH_JAVA} ${env:BIG_BENCH_java_child_process_xmx} -cp bigbenchqueriesmr.jar de.bankmark.bigbench.queries.q01.Red -ITEM_SET_MAX ${hiveconf:q01_NPATH_ITEM_SET_MAX} '
  AS (pid1 BIGINT, pid2 BIGINT)
) q01_temp_basket
{code}",,apachespark,chenghao,jameszhouyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 01:01:08 UTC 2015,,,,,,,,,,"0|i2dku7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/15 04:45;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/5625;;;","24/Apr/15 01:01;chenghao;backport to Spark1.3;;;","24/Apr/15 01:01;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/5671;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcRdd doesn't support java.sql.Types.NVARCHAR,SPARK-7039,12822766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,szheng79,szheng79,szheng79,21/Apr/15 19:35,22/Apr/15 20:03,14/Jul/23 06:26,22/Apr/15 20:03,1.3.1,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,"When create a DataFrame from jdbc method through SqlContext:
{code}
DataFrame df = sql.jdbc(url, fullTableName);
{code}

If there is column type NVARCHAR, below exception will be thrown:

{code}
Caused by: java.sql.SQLException: Unsupported type -9
	at org.apache.spark.sql.jdbc.JDBCRDD$.getCatalystType(JDBCRDD.scala:78)
	at org.apache.spark.sql.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:112)
	at org.apache.spark.sql.jdbc.JDBCRelation.<init>(JDBCRelation.scala:133)
	at org.apache.spark.sql.SQLContext.jdbc(SQLContext.scala:900)
	at org.apache.spark.sql.SQLContext.jdbc(SQLContext.scala:852)
{code}

When comparing the code between JDBCRDD.scala and java.sql.Types.java, the only type is not supported in JDBCRDD.scala is NVARCHAR. Because NCHAR is supported, so I think this is just a small mistake that people skip this type instead of ignore it intentionally. ",Spark 1.3.1,apachespark,szheng79,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 22 14:29:31 UTC 2015,,,,,,,,,,"0|i2dk33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/15 19:43;szheng79;I have created a pull request.

And the related code:

JDBCRDD.scala:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/JdbcRDD.scala

java.sql.Types:
http://docs.oracle.com/javase/7/docs/api/java/sql/Types.html

https://github.com/apache/spark/pull/5618;;;","22/Apr/15 14:29;apachespark;User 'szheng79' has created a pull request for this issue:
https://github.com/apache/spark/pull/5618;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ALS.train should support DataFrames in PySpark,SPARK-7036,12822750,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mengxr,mengxr,mengxr,21/Apr/15 18:31,21/Apr/15 23:45,14/Jul/23 06:26,21/Apr/15 23:45,1.3.1,,,,,,1.3.2,1.4.0,,,,,MLlib,,,,0,,,,,,ALS.train works with SchemaRDDs in 1.2. We should support DataFrames for compatibility.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 23:45:10 UTC 2015,,,,,,,,,,"0|i2djzj:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"21/Apr/15 20:18;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5619;;;","21/Apr/15 23:45;mengxr;Issue resolved by pull request 5619
[https://github.com/apache/spark/pull/5619];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 1.2.2 Hadoop 2.4 download is missing,SPARK-7027,12822576,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,pwendell,msoutier,msoutier,21/Apr/15 06:41,21/Apr/15 18:55,14/Jul/23 06:26,21/Apr/15 18:55,1.2.2,,,,,,1.2.2,,,,,,,,,,0,,,,,,"Can't download Spark 1.2.2 pre-built for Hadoop 2.4 from https://spark.apache.org/downloads.html.
",,msoutier,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 18:55:09 UTC 2015,,,,,,,,,,"0|i2diwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/15 18:55;pwendell;It's fixed now, but will take several hours to propagate through the ASF mirror network:

https://dist.apache.org/repos/dist/release/spark/spark-1.2.2/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
LeftSemiJoin can not work when it  has both equal condition and not equal condition. ,SPARK-7026,12822570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,DoingDone9,DoingDone9,21/Apr/15 06:15,17/Jul/15 23:46,14/Jul/23 06:26,17/Jul/15 23:46,1.3.0,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Run sql like that 
{panel}
select *
from
web_sales ws1
left semi join
web_sales ws2
on ws1.ws_order_number = ws2.ws_order_number
and ws1.ws_warehouse_sk <> ws2.ws_warehouse_sk 
{panel}
 then get an exception
{panel}
Couldn't find ws_warehouse_sk#287 in {ws_sold_date_sk#237,ws_sold_time_sk#238,ws_ship_date_sk#239,ws_item_sk#240,ws_bill_customer_sk#241,ws_bill_cdemo_sk#242,ws_bill_hdemo_sk#243,ws_bill_addr_sk#244,ws_ship_customer_sk#245,ws_ship_cdemo_sk#246,ws_ship_hdemo_sk#247,ws_ship_addr_sk#248,ws_web_page_sk#249,ws_web_site_sk#250,ws_ship_mode_sk#251,ws_warehouse_sk#252,ws_promo_sk#253,ws_order_number#254,ws_quantity#255,ws_wholesale_cost#256,ws_list_price#257,ws_sales_price#258,ws_ext_discount_amt#259,ws_ext_sales_price#260,ws_ext_wholesale_cost#261,ws_ext_list_price#262,ws_ext_tax#263,ws_coupon_amt#264,ws_ext_ship_cost#265,ws_net_paid#266,ws_net_paid_inc_tax#267,ws_net_paid_inc_ship#268,ws_net_paid_inc_ship_tax#269,ws_net_profit#270,ws_sold_date#236}
at scala.sys.package$.error(package.scala:27)
{panel}
",,apachespark,DoingDone9,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 17 23:46:21 UTC 2015,,,,,,,,,,"0|i2divb:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.5.0,,,,,,,,,,,,,"21/Apr/15 11:47;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/5610;;;","21/Apr/15 13:52;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/5612;;;","23/Apr/15 02:47;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5643;;;","17/Jul/15 23:46;marmbrus;Issue resolved by pull request 5643
[https://github.com/apache/spark/pull/5643];;;",,,,,,,,,,,,,,,,,,,,,,,,,
[Spark SQL] Can't populate table size inforamtion into Hive metastore when create table or insert into table,SPARK-7023,12822537,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jameszhouyi,jameszhouyi,21/Apr/15 02:52,07/Oct/16 07:55,14/Jul/23 06:26,07/Oct/16 07:55,1.4.0,,,,,,,,,,,,SQL,,,,0,,,,,,"After run below create tables SQL statement on Spark SQL, there is no 'totalSize, numRows, rawDataSize' properties in 'parameters' field..

CREATE TABLE IF NOT EXISTS customer
STORED AS PARQUET
AS
SELECT * FROM customer_temp;

hive> describe extended customer;
OK
c_customer_sk           bigint
c_customer_id           string
c_current_cdemo_sk      bigint
c_current_hdemo_sk      bigint
c_current_addr_sk       bigint
c_first_shipto_date_sk  bigint
c_first_sales_date_sk   bigint
c_salutation            string
c_first_name            string
c_last_name             string
c_preferred_cust_flag   string
c_birth_day             int
c_birth_month           int
c_birth_year            int
c_birth_country         string
c_login                 string
c_email_address         string
c_last_review_date      string

Detailed Table Information     
... parameters:{transient_lastDdlTime=1429582149}...",,chenghao,ckadner,jameszhouyi,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 07:55:11 UTC 2016,,,,,,,,,,"0|i2dinr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 07:55;smilegator;This should have been resolved. Please reopen it if this is still an issue. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Build fails with scala 2.11 option, because a protected[sql] type is accessed in ml package.",SPARK-7011,12822293,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,prashant,prashant,prashant,20/Apr/15 11:50,21/Apr/15 21:43,14/Jul/23 06:26,21/Apr/15 21:43,,,,,,,1.4.0,,,,,,,,,,0,,,,,,"I am not sure why this does not fail while building with scala 2.10, looks like scala bug ?",,apachespark,marmbrus,prashant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 21:43:55 UTC 2015,,,,,,,,,,"0|i2dh2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/15 12:03;apachespark;User 'ScrapCodes' has created a pull request for this issue:
https://github.com/apache/spark/pull/5593;;;","21/Apr/15 21:43;marmbrus;Issue resolved by pull request 5593
[https://github.com/apache/spark/pull/5593];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve reliability of connection failure detection between Netty block transfer service endpoints,SPARK-7003,12822197,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ilikerps,ilikerps,ilikerps,20/Apr/15 02:49,20/Apr/15 16:55,14/Jul/23 06:26,20/Apr/15 16:55,1.3.1,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Currently we rely on the assumption that an exception will be raised and the channel closed if two endpoints cannot communicate over a Netty TCP channel. However, this guarantee does not hold in all network environments, and SPARK-6962 seems to point to a case where only the server side of the connection detected a fault.

We should improve robustness of fetch/rpc requests by having an explicit timeout in the transport layer which closes the connection if there is a period of inactivity while there are outstanding requests.",,apachespark,hzfeiwang,ilikerps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 03:00:25 UTC 2015,,,,,,,,,,"0|i2dghj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/15 03:00;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/5584;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"infinite recursion with createDataFrame(JavaRDD[Row], java.util.List[String])",SPARK-6999,12822179,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,chenghao,justin.uang,justin.uang,19/Apr/15 21:12,02/May/15 23:44,14/Jul/23 06:26,02/May/15 00:40,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"It looks like 

{code}
  def createDataFrame(rowRDD: JavaRDD[Row], columns: java.util.List[String]): DataFrame = {
    createDataFrame(rowRDD.rdd, columns.toSeq)
  }
{code}

is in fact an infinite recursion because it calls itself. Scala implicit conversions convert the arguments back into a JavaRDD and a java.util.List.

{code}
15/04/19 16:51:24 INFO BlockManagerMaster: Trying to register BlockManager
15/04/19 16:51:24 INFO BlockManagerMasterActor: Registering block manager localhost:53711 with 1966.1 MB RAM, BlockManagerId(<driver>, localhost, 53711)
15/04/19 16:51:24 INFO BlockManagerMaster: Registered BlockManager
Exception in thread ""main"" java.lang.StackOverflowError
    at scala.collection.mutable.AbstractSeq.<init>(Seq.scala:47)
    at scala.collection.mutable.AbstractBuffer.<init>(Buffer.scala:48)
    at scala.collection.convert.Wrappers$JListWrapper.<init>(Wrappers.scala:84)
    at scala.collection.convert.WrapAsScala$class.asScalaBuffer(WrapAsScala.scala:127)
    at scala.collection.JavaConversions$.asScalaBuffer(JavaConversions.scala:53)
    at org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:408)
    at org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:408)
    at org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:408)
    at org.apache.spark.sql.SQLContext.createDataFrame(SQLContext.scala:408)
{code}

Here is the code sample I used to reproduce the issue:

{code}
/**
 * @author juang
 */
public final class InfiniteRecursionExample {

    public static void main(String[] args) {
        JavaSparkContext sc = new JavaSparkContext(""local"", ""infinite_recursion_example"");
        List<Row> rows = Lists.newArrayList();
        JavaRDD<Row> rowRDD = sc.parallelize(rows);

        SQLContext sqlContext = new SQLContext(sc);
        sqlContext.createDataFrame(rowRDD, ImmutableList.of(""myCol""));
    }

}
{code}",,anand,apachespark,irashid,rxin,saucam,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 02 00:40:14 UTC 2015,,,,,,,,,,"0|i2dgdr:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"20/Apr/15 15:09;justin.uang;Looking at the source, it looks like one way to implement this is to extract part of getSchema(), specifically the

{code}
        case c: Class[_] if c.isAnnotationPresent(classOf[SQLUserDefinedType]) =>
          (c.getAnnotation(classOf[SQLUserDefinedType]).udt().newInstance(), true)
        case c: Class[_] if c == classOf[java.lang.String] => (StringType, true)
        case c: Class[_] if c == java.lang.Short.TYPE => (ShortType, false)
        case c: Class[_] if c == java.lang.Integer.TYPE => (IntegerType, false)
        case c: Class[_] if c == java.lang.Long.TYPE => (LongType, false)
        case c: Class[_] if c == java.lang.Double.TYPE => (DoubleType, false)
        case c: Class[_] if c == java.lang.Byte.TYPE => (ByteType, false)
        case c: Class[_] if c == java.lang.Float.TYPE => (FloatType, false)
        case c: Class[_] if c == java.lang.Boolean.TYPE => (BooleanType, false)

        case c: Class[_] if c == classOf[java.lang.Short] => (ShortType, true)
        case c: Class[_] if c == classOf[java.lang.Integer] => (IntegerType, true)
        case c: Class[_] if c == classOf[java.lang.Long] => (LongType, true)
        case c: Class[_] if c == classOf[java.lang.Double] => (DoubleType, true)
        case c: Class[_] if c == classOf[java.lang.Byte] => (ByteType, true)
        case c: Class[_] if c == classOf[java.lang.Float] => (FloatType, true)
        case c: Class[_] if c == classOf[java.lang.Boolean] => (BooleanType, true)
        case c: Class[_] if c == classOf[java.math.BigDecimal] => (DecimalType(), true)
        case c: Class[_] if c == classOf[java.sql.Date] => (DateType, true)
        case c: Class[_] if c == classOf[java.sql.Timestamp] => (TimestampType, true)
{code}

section and then have another method pull out elements from the first {{Row}} using {{Row.get}}, then using the switch statement to identify the type. Are there any gotchas that I'm missing?;;;","23/Apr/15 15:24;justin.uang;We might be able to use sql/core/src/main/scala/org/apache/spark/sql/JavaTypeInference.scala which got merged into master in https://github.com/apache/spark/pull/5578/files.;;;","30/Apr/15 08:23;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/5804;;;","02/May/15 00:40;irashid;Issue resolved by pull request 5804
[https://github.com/apache/spark/pull/5804];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Make StreamingKMeans `Serializable`,SPARK-6998,12822159,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,19/Apr/15 17:55,20/Apr/15 03:34,14/Jul/23 06:26,20/Apr/15 03:34,1.2.2,1.3.1,,,,,1.2.3,1.3.2,1.4.0,,,,MLlib,,,,0,,,,,,"If `StreamingKMeans` is not `Serializable`, we cannot do checkpoint for applications that using `StreamingKMeans`. So we should make it `Serializable`.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6979,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 19 17:58:43 UTC 2015,,,,,,,,,,"0|i2dg9r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/15 17:58;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5582;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL documentation for programmatically adding a Schema is broken for Java API,SPARK-6992,12822061,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ogirardot,ogirardot,ogirardot,18/Apr/15 07:20,18/Apr/15 07:32,14/Jul/23 06:26,18/Apr/15 07:31,1.3.0,1.3.1,1.3.2,,,,1.3.2,1.4.0,,,,,Documentation,,,,0,,,,,,"The Java example for the documentation is not compiling mainly because Row.create and DataType is used instead of RowFactory and DataTypes.

I'll create the corresponding pull request on branch-1.3

Regards, 

Olivier.",,apachespark,ogirardot,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 07:32:31 UTC 2015,,,,,,,,,,"0|i2dfo7:",9223372036854775807,,,,,rxin,,,,,,,,,,,,,,,,,,,,,,"18/Apr/15 07:24;srowen;(You don't need to make a separate PR for other branches, just master.)

Is this the same as SPARK-6988?
https://github.com/apache/spark/commit/d305e686b3d73213784bd75cdad7d168b22a1dc4;;;","18/Apr/15 07:25;apachespark;User 'ogirardot' has created a pull request for this issue:
https://github.com/apache/spark/pull/5569;;;","18/Apr/15 07:32;rxin;[~srowen] it's actually a different patch.;;;","18/Apr/15 07:32;ogirardot;Hi Sean, 
same doc but different place and different issue, c.f. http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-1-3-0-org-apache-spark-sql-Row-Does-not-exist-in-Java-API-td11655.html

The main issue being that the Trait were referenced instead of the Java factory methods : DataType => DataTypes and Row => RowFactory

This time I checked the example (the programmatically defined schema example) and everything seems to work fine in Java.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Receiver maxRate over 1000 causes a StackOverflowError,SPARK-6985,12821938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dmcguire81,dmcguire81,dmcguire81,17/Apr/15 17:38,21/Apr/15 11:24,14/Jul/23 06:26,21/Apr/15 11:23,1.3.0,,,,,,1.3.2,1.4.0,,,,,DStreams,,,,1,,,,,,"Attempting to set the streaming receiver max rate (streaming.receiverMaxRate) for the RateLimiter to anything over 1000 causes a fatal error in the receiver with the following stacktrace:

15/04/16 10:41:50 ERROR KafkaReceiver: Error handling message; exiting
java.lang.StackOverflowError
	at org.apache.spark.streaming.receiver.RateLimiter.waitToPush(RateLimiter.scala:66)
	at org.apache.spark.streaming.receiver.RateLimiter.waitToPush(RateLimiter.scala:66)
	at org.apache.spark.streaming.receiver.RateLimiter.waitToPush(RateLimiter.scala:66)
	at org.apache.spark.streaming.receiver.RateLimiter.waitToPush(RateLimiter.scala:66)
	at org.apache.spark.streaming.receiver.RateLimiter.waitToPush(RateLimiter.scala:66)
	at org.apache.spark.streaming.receiver.RateLimiter.waitToPush(RateLimiter.scala:66)
	at org.apache.spark.streaming.receiver.RateLimiter.waitToPush(RateLimiter.scala:66)
	at org.apache.spark.streaming.receiver.RateLimiter.waitToPush(RateLimiter.scala:66)
	at org.apache.spark.streaming.receiver.RateLimiter.waitToPush(RateLimiter.scala:66)
",,apachespark,dmcguire81,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 11:23:25 UTC 2015,,,,,,,,,,"0|i2dewv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/15 17:52;apachespark;User 'dmcguire81' has created a pull request for this issue:
https://github.com/apache/spark/pull/5559;;;","17/Apr/15 23:14;dmcguire81;How can I trigger the automated build on my associated PR, to move the merge process along?;;;","18/Apr/15 05:04;srowen;Admin needs to tell Jenkins it's OK to test, which I can do. I hadn't yet since it seemed like there were already some things that must change.;;;","20/Apr/15 16:26;dmcguire81;There's also a failure mode for the RateLimiter throttling on rates of 1000 messages / second and under, it just takes longer to reproduce, because the truncation is less drastic. I'd be happy to submit a reproduction case for both failure modes, if that would help the process along. I'm thinking of assembling a blog post on the subject, anyway, to help users who might be negatively affected.;;;","20/Apr/15 17:04;srowen;Hm, so there's still a problem here? is it easy to fix? might still be better to fix even if it's somehow very hard to test.;;;","20/Apr/15 17:19;dmcguire81;Both error modes are fixed in my PR; I'm just bringing it up to make a case for ""blocker"" status.

Suffice it to say that, if your application gets behind and allows a backlog to accumulate in Kafka (i.e.: a recoverable failure mode most teams design for), then the RateLimiter *will* choke when you start back up, and it will crash your Receiver, but not your application. Since the only symptom is that the message rate drops to 0, this is an especially pernicious error to detect and recover from.
;;;","20/Apr/15 17:43;srowen;Re: Priority, don't worry in the sense that this should definitely be committed shortly. I was just waiting on the trivial style fix. 

Priority is not used consistently much here, though as an aside, I was hoping to fix that a bit with the update to our ""Contributing"" wiki. Priority unfortunately conflates ""what is most important to work on"", ""when does this have to be fixed by"", ""how big is the impact"" and ""how hard is it to fix"". It's certainly going to be fixed for the next release so is it a ""Blocker""? it's easyish to fix so is it ""Minor""? Let's say Critical!;;;","20/Apr/15 18:08;dmcguire81;Fair enough! Thanks for the explanation.;;;","21/Apr/15 11:23;srowen;Issue resolved by pull request 5559
[https://github.com/apache/spark/pull/5559];;;",,,,,,,,,,,,,,,,,,,,
"""drop table if exists src"" print ERROR info that should not be printed when ""src"" not exists.",SPARK-6976,12821812,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,DoingDone9,DoingDone9,17/Apr/15 09:01,07/Oct/16 07:51,14/Jul/23 06:26,07/Oct/16 07:51,1.3.0,,,,,,,,,,,,SQL,,,,0,,,,,,"If table ""src"" not exists and run sql ""drop table if exists src"", then some ERROR info will be printed, like that 
{quote}
15/04/17 17:09:53 ERROR Hive: NoSuchObjectException(message:default.src table not found)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table(HiveMetaStore.java:1560)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
        at $Proxy10.get_table(Unknown Source)
{quote}",,apachespark,DoingDone9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 09:21:14 UTC 2015,,,,,,,,,,"0|i2de6f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/15 09:21;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/5553;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Argument checking conflict in Yarn when dynamic allocation is enabled,SPARK-6975,12821784,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,17/Apr/15 07:00,18/Apr/15 02:16,14/Jul/23 06:26,18/Apr/15 02:16,1.3.0,,,,,,1.3.2,1.4.0,,,,,YARN,,,,0,,,,,,"When dynamic allocation is enabled in yarn with default parameter, {{numExecutors}} will be set to 0, but this will be failed in the following {{valideArgs()}}, here {{numExecutors}} must > 0, but for dynamic allocation, this executor number can be 0 (with default setting). The exception is shown as below:

Exception in thread ""main"" java.lang.IllegalArgumentException: You must specify at least 1 executor!

",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 07:09:33 UTC 2015,,,,,,,,,,"0|i2de07:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"17/Apr/15 07:09;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/5551;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
The total stages on the allJobsPage is wrong,SPARK-6973,12821771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,meiyoula,meiyoula,meiyoula,17/Apr/15 06:01,06/Jun/15 08:54,14/Jul/23 06:26,06/Jun/15 08:54,,,,,,,1.5.0,,,,,,Web UI,,,,0,,,,,,"The job has two stages,  map and collect stage. Both two retried two times. The first and second time of map stage is successful, and the third time skipped. Of collect stage, the first and second time is failed, and the third time is successful.
On the allJobs page, the number of total stages is allStages-skippedStages. Mostly it's wright, but here I think total stages should be 2.

The example:
Stage 0: Map Stage
Stage 1: Collect Stage

Stage:     Stage 0 -> Stage 1 -> Stage 0(retry 1) -> Stage 1(retry 1) -> Stage 0(retry 2) -> Stage 1(retry 2)
Status：  Success ->     Fail     ->        Success       ->             Fail      ->                Skipped     ->             Success

Though one of Stage 0 is skipped, actually it's executed. So I think it should be included in the total number.",,apachespark,meiyoula,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/15 06:18;meiyoula;allJobs.png;https://issues.apache.org/jira/secure/attachment/12726081/allJobs.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jun 06 08:54:06 UTC 2015,,,,,,,,,,"0|i2ddxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/15 06:40;apachespark;User 'XuTingjun' has created a pull request for this issue:
https://github.com/apache/spark/pull/5550;;;","17/Apr/15 08:32;srowen;Can you clarify your example? it doesn't quite match what you describe in the PR, and what's in the PR seems like a confusing change for users, so I don't think this should happen.;;;","06/Jun/15 08:54;srowen;Issue resolved by pull request 5550
[https://github.com/apache/spark/pull/5550];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Each Jenkins build should use a distinct Zinc port,SPARK-6971,12821704,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,pwendell,pwendell,pwendell,16/Apr/15 22:29,30/Apr/15 20:03,14/Jul/23 06:26,30/Apr/15 20:03,,,,,,,1.4.0,,,,,,Project Infra,,,,0,,,,,,"Contamination between the builds causes random compiler crashes. Our Spark packaging scripts now use unique zinc ports for each build:

https://github.com/apache/spark/commit/3980ebdf188d77799b55b407b115cdc82f51d532#diff-c849de7463d08ba5f589d5732ef07061R229

It should be too hard to fix this - we can just set ZINC_PORT differently in each build, to a specific number or a random number within a range.",,boyork,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 20:02:55 UTC 2015,,,,,,,,,,"0|i2ddiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/15 20:02;pwendell;I took care of this for the master and 1.3 maven builds. They now do this:

{code}
rm -rf ./work
git clean -fdx
# Generate random point for Zinc
export ZINC_PORT=$(python -S -c ""import random; print random.randrange(3030,4030)"")
build/mvn -DskipTests -P${HADOOP_PROFILE} -Pyarn -Phive clean package
build/mvn test -P${HADOOP_PROFILE} -Pyarn -Phive --fail-at-end
# Clean-up Zinc nailgun process
/usr/sbin/lsof -P |grep $ZINC_PORT | grep LISTEN | awk '{ print $2; }' | xargs kill
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internal DateType not handled correctly in caching,SPARK-6967,12821630,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,adrian-wang,marmbrus,marmbrus,16/Apr/15 18:49,23/Apr/15 02:19,14/Jul/23 06:26,23/Apr/15 02:18,,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,"From the user list.  It looks like data is not implemented correctly in in-memory caching.  We should also check the JDBC datasource support for date.

{code}
Stack trace of an exception being reported since upgrade to 1.3.0:
java.lang.ClassCastException: java.sql.Date cannot be cast to
java.lang.Integer
        at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:105)
~[scala-library-2.11.6.jar:na]
        at
org.apache.spark.sql.catalyst.expressions.GenericRow.getInt(rows.scala:83)
~[spark-catalyst_2.11-1.3.0.jar:1.3.0]
        at
org.apache.spark.sql.columnar.IntColumnStats.gatherStats(ColumnStats.scala:191)
~[spark-sql_2.11-1.3.0.jar:1.3.0]
        at
org.apache.spark.sql.columnar.NullableColumnBuilder$class.appendFrom(NullableColumnBuilder.scala:56)
~[spark-sql_2.11-1.3.0.jar:1.3.0]
        at
org.apache.spark.sql.columnar.NativeColumnBuilder.org$apache$spark$sql$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:87)
~[spark-sql_2.11-1.3.0.jar:1.3.0]
        at
org.apache.spark.sql.columnar.compression.CompressibleColumnBuilder$class.appendFrom(CompressibleColumnBuilder.scala:78)
~[spark-sql_2.11-1.3.0.jar:1.3.0]
        at
org.apache.spark.sql.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:87)
~[spark-sql_2.11-1.3.0.jar:1.3.0]
        at
org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:135)
~[spark-sql_2.11-1.3.0.jar:1.3.0]
        at
{code}",,apachespark,marmbrus,rkrist,romi-totango,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 10:30:28 UTC 2015,,,,,,,,,,"0|i2dd33:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"17/Apr/15 10:06;rkrist;steps to reproduce the same problem in JdbcRDD (hope it helps):

- create a table containig Date field in your favourite DBMS, I used PostgreSQL:
{code}
CREATE TABLE spark_test
(
  pk_spark_test integer NOT NULL,
  text character varying(25),
  date1 date,
  CONSTRAINT pk PRIMARY KEY (pk_spark_test)
)
WITH (
  OIDS=FALSE
);
ALTER TABLE spark_test
  OWNER TO postgres;
GRANT ALL ON TABLE spark_test TO postgres;
GRANT ALL ON TABLE spark_test TO public;
{code}

- fill it with data:
{code}
insert into spark_test(pk_spark_test, text, date1) values (1, 'one', '2014-04-01')
insert into spark_test(pk_spark_test, text, date1) values (2, 'two', '2014-04-02')
{code}

- from scala REPL, try the following:
{code}
import org.apache.spark.sql.SQLContext

val sqc = new SQLContext(sc)
sqc.jdbc(""jdbc:postgresql://localhost:5432/ebx_repository?schema=ebx_repository&user=abc&password=def"", ""spark_test"").cache.registerTempTable(""spark_test"")  // don’t forget the cache method

sqc.sql(""select * from spark_test"").foreach(println)

the last command will produce the following error (if you don’t use cache, it will produce correct results as expected):

11:50:27.306 [Executor task launch worker-0] ERROR org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableAny cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableInt
                at org.apache.spark.sql.catalyst.expressions.SpecificMutableRow.getInt(SpecificMutableRow.scala:248) ~[spark-catalyst_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.sql.columnar.IntColumnStats.gatherStats(ColumnStats.scala:191) ~[spark-sql_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.sql.columnar.NullableColumnBuilder$class.appendFrom(NullableColumnBuilder.scala:56) ~[spark-sql_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.sql.columnar.NativeColumnBuilder.org$apache$spark$sql$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:87) ~[spark-sql_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.sql.columnar.compression.CompressibleColumnBuilder$class.appendFrom(CompressibleColumnBuilder.scala:78) ~[spark-sql_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.sql.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:87) ~[spark-sql_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:135) ~[spark-sql_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:111) ~[spark-sql_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.rdd.RDD.iterator(RDD.scala:242) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.rdd.RDD.iterator(RDD.scala:244) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.scheduler.Task.run(Task.scala:64) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203) ~[spark-core_2.11-1.3.0.jar:1.3.0]
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_11]
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_11]
                at java.lang.Thread.run(Thread.java:745) [na:1.8.0_11]
11:50:27.318 [task-result-getter-0] WARN  o.a.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableAny cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableInt
                at org.apache.spark.sql.catalyst.expressions.SpecificMutableRow.getInt(SpecificMutableRow.scala:248)
                at org.apache.spark.sql.columnar.IntColumnStats.gatherStats(ColumnStats.scala:191)
                at org.apache.spark.sql.columnar.NullableColumnBuilder$class.appendFrom(NullableColumnBuilder.scala:56)
                at org.apache.spark.sql.columnar.NativeColumnBuilder.org$apache$spark$sql$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:87)
                at org.apache.spark.sql.columnar.compression.CompressibleColumnBuilder$class.appendFrom(CompressibleColumnBuilder.scala:78)
                at org.apache.spark.sql.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:87)
                at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:135)
                at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:111)
                at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
                at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
                at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
                at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
                at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
                at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
                at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
                at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
                at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
                at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
                at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
                at org.apache.spark.scheduler.Task.run(Task.scala:64)
                at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                at java.lang.Thread.run(Thread.java:745)

11:50:27.320 [task-result-getter-0] ERROR o.a.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableAny cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableInt
                at org.apache.spark.sql.catalyst.expressions.SpecificMutableRow.getInt(SpecificMutableRow.scala:248)
                at org.apache.spark.sql.columnar.IntColumnStats.gatherStats(ColumnStats.scala:191)
                at org.apache.spark.sql.columnar.NullableColumnBuilder$class.appendFrom(NullableColumnBuilder.scala:56)
                at org.apache.spark.sql.columnar.NativeColumnBuilder.org$apache$spark$sql$columnar$compression$CompressibleColumnBuilder$$super$appendFrom(ColumnBuilder.scala:87)
                at org.apache.spark.sql.columnar.compression.CompressibleColumnBuilder$class.appendFrom(CompressibleColumnBuilder.scala:78)
                at org.apache.spark.sql.columnar.NativeColumnBuilder.appendFrom(ColumnBuilder.scala:87)
                at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:135)
                at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:111)
                at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
                at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:172)
                at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:79)
                at org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
                at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
                at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
                at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
                at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
                at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
                at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
                at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
                at org.apache.spark.scheduler.Task.run(Task.scala:64)
                at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
                at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
  at scala.Option.foreach(Option.scala:257)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}
;;;","17/Apr/15 17:34;yhuai;I think the the problem is https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JDBCRDD.scala#L352. ;;;","20/Apr/15 10:30;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5590;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
JDBC datasources use Class.forName to load driver,SPARK-6966,12821622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,16/Apr/15 18:34,17/Apr/15 01:00,14/Jul/23 06:26,17/Apr/15 01:00,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,,,apachespark,cfregly,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 01:00:13 UTC 2015,,,,,,,,,,"0|i2dd1b:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"16/Apr/15 21:54;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5543;;;","17/Apr/15 01:00;marmbrus;Issue resolved by pull request 5543
[https://github.com/apache/spark/pull/5543];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.ContextCleanerSuite automatically cleanup checkpoint,SPARK-6963,12821594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gq,andrewor14,andrewor14,16/Apr/15 17:17,19/Apr/15 08:38,14/Jul/23 06:26,19/Apr/15 08:37,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,flaky-test,,,,,"Observed on an unrelated streaming PR https://github.com/apache/spark/pull/5428
https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/30389/

{code}
sbt.ForkMain$ForkError: fs.exists(org.apache.spark.rdd.RDDCheckpointData.rddCheckpointDataPath(ContextCleanerSuite.this.sc, rddId).get) was true
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.ContextCleanerSuite$$anonfun$9.apply$mcV$sp(ContextCleanerSuite.scala:252)
	at org.apache.spark.ContextCleanerSuite$$anonfun$9.apply(ContextCleanerSuite.scala:209)
	at org.apache.spark.ContextCleanerSuite$$anonfun$9.apply(ContextCleanerSuite.scala:209)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.ContextCleanerSuiteBase.org$scalatest$BeforeAndAfter$$super$runTest(ContextCleanerSuite.scala:46)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.ContextCleanerSuiteBase.org$scalatest$BeforeAndAfterEach$$super$runTest(ContextCleanerSuite.scala:46)
	at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
	at org.apache.spark.ContextCleanerSuiteBase.runTest(ContextCleanerSuite.scala:46)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
{code}",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 01:58:54 UTC 2015,,,,,,,,,,"0|i2dcvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/15 01:58;apachespark;User 'witgo' has created a pull request for this issue:
https://github.com/apache/spark/pull/5548;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not let Yarn Shuffle Server retry its server port.,SPARK-6955,12821393,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,adav,carlmartin,carlmartin,16/Apr/15 01:36,17/May/20 18:31,14/Jul/23 06:26,09/May/15 00:14,1.2.0,,,,,,1.4.0,,,,,,Shuffle,Spark Core,YARN,,0,,,,,," It's better to let the NodeManager get down rather than take a port retry when `spark.shuffle.service.port` has been conflicted during starting the Spark Yarn Shuffle Server, because the retry mechanism will make the inconsistency of shuffle port and also make client fail to find the port.",,apachespark,carlmartin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 18 17:57:45 UTC 2015,,,,,,,,,,"0|i2dbnb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"16/Apr/15 02:27;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5537;;;","18/Apr/15 17:57;apachespark;User 'aarondav' has created a pull request for this issue:
https://github.com/apache/spark/pull/5575;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorAllocationManager can end up requesting a negative number of executors,SPARK-6954,12821390,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandyr,cheolsoo,cheolsoo,16/Apr/15 01:24,19/Jun/15 07:20,14/Jul/23 06:26,02/May/15 09:59,1.3.1,,,,,,1.3.2,1.4.0,,,,,YARN,,,,0,yarn,,,,,"I have a simple test case for dynamic allocation on YARN that fails with the following stack trace-
{code}
15/04/16 00:52:14 ERROR Utils: Uncaught exception in thread spark-dynamic-executor-allocation-0
java.lang.IllegalArgumentException: Attempted to request a negative number of executor(s) -21 from the cluster manager. Please specify a positive number!
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:338)
	at org.apache.spark.SparkContext.requestTotalExecutors(SparkContext.scala:1137)
	at org.apache.spark.ExecutorAllocationManager.addExecutors(ExecutorAllocationManager.scala:294)
	at org.apache.spark.ExecutorAllocationManager.addOrCancelExecutorRequests(ExecutorAllocationManager.scala:263)
	at org.apache.spark.ExecutorAllocationManager.org$apache$spark$ExecutorAllocationManager$$schedule(ExecutorAllocationManager.scala:230)
	at org.apache.spark.ExecutorAllocationManager$$anon$1$$anonfun$run$1.apply$mcV$sp(ExecutorAllocationManager.scala:189)
	at org.apache.spark.ExecutorAllocationManager$$anon$1$$anonfun$run$1.apply(ExecutorAllocationManager.scala:189)
	at org.apache.spark.ExecutorAllocationManager$$anon$1$$anonfun$run$1.apply(ExecutorAllocationManager.scala:189)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618)
	at org.apache.spark.ExecutorAllocationManager$$anon$1.run(ExecutorAllocationManager.scala:189)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}
My test is as follows-
# Start spark-shell with a single executor.
# Run a {{select count(\*)}} query. The number of executors rises as input size is non-trivial.
# After the job finishes, the number of  executors falls as most of them become idle.
# Rerun the same query again, and the request to add executors fails with the above error. In fact, the job itself continues to run with whatever executors it already has, but it never gets more executors unless the shell is closed and restarted. 

In fact, this error only happens when I configure {{executorIdleTimeout}} very small. For eg, I can reproduce it with the following configs-
{code}
spark.dynamicAllocation.executorIdleTimeout     5
spark.dynamicAllocation.schedulerBacklogTimeout 5
{code}
Although I can simply increase {{executorIdleTimeout}} to something like 60 secs to avoid the error, I think this is still a bug to be fixed.

The root cause seems that {{numExecutorsPending}} accidentally becomes negative if executors are killed too aggressively (i.e. {{executorIdleTimeout}} is too small) because under that circumstance, the new target # of executors can be smaller than the current # of executors. When that happens, {{ExecutorAllocationManager}} ends up trying to add a negative number of executors, which throws an exception.",,apachespark,ashwinshankar77,cheolsoo,dweeks-netflix,roji,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6891,,,,SPARK-7901,,,,,,,,,,,,,,,,,,"21/Apr/15 04:34;cheolsoo;with_fix.png;https://issues.apache.org/jira/secure/attachment/12726785/with_fix.png","21/Apr/15 04:34;cheolsoo;without_fix.png;https://issues.apache.org/jira/secure/attachment/12726786/without_fix.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 02 02:10:18 UTC 2015,,,,,,,,,,"0|i2dbmn:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"16/Apr/15 01:31;apachespark;User 'piaozhexiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/5536;;;","16/Apr/15 02:15;sandyr;Hi [~cheolsoo], are you running with a version of Spark that contains SPARK-6325? (1.3.0 does not).;;;","16/Apr/15 02:33;cheolsoo;Hi [~sandyr], Thank you for the question.

I am actually running 1.3.1-RC3, and I just confirmed that SPARK-6325 is in the commit log of my release branch.

I updated the affects version to 1.3.1 to avoid confusion.;;;","21/Apr/15 04:34;cheolsoo;I am uploading two diagrams that shows how the following variables move over time w/ and w/o my patch-
* numExecutorsPending
* executorIds.size
* executorsPendingToRemove.size
* targetNumExecutors

# The {{with_fix.png}} shows 4 consecutive runs of my query. As can be seen, {{targetNumExecutors}} and {{numExecutorsPending}} stays above zero.
# The {{without_fix.png}} shows a single run of my query. As can be seen, {{targetNumExecutors}} and {{numExecutorsPending}} goes negative after the 1st run.

Here is how I collected data in the source code-
{code}
private def targetNumExecutors(): Int = {
  logInfo(""ZZZ "" +
    numExecutorsPending + "","" +
    executorIds.size + "","" +
    executorsPendingToRemove.size + "","" +
    (numExecutorsPending + executorIds.size - executorsPendingToRemove.size))
  numExecutorsPending + executorIds.size - executorsPendingToRemove.size
}
{code};;;","26/Apr/15 06:42;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/5704;;;","02/May/15 02:10;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/5856;;;",,,,,,,,,,,,,,,,,,,,,,,
spark-daemon.sh PID reuse check fails on long classpath,SPARK-6952,12821373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,punya,punya,punya,15/Apr/15 23:53,17/Apr/15 12:41,14/Jul/23 06:26,17/Apr/15 10:08,1.2.2,1.3.1,,,,,1.2.3,1.3.2,1.4.0,,,,Deploy,,,,0,,,,,,"{{sbin/spark-daemon.sh}} uses {{ps -p ""$TARGET_PID"" -o args=}} to figure out whether the process running with the expected PID is actually a Spark daemon. When running with a large classpath, the output of {{ps}} gets truncated and the check fails spuriously.

I think we should weaken the check to see if it's a java command (which is something we do in other parts of the script) rather than looking for the specific main class name. This means that SPARK-4832 might happen under a slightly broader range of circumstances (a *java* program happened to reuse the same PID), but it seems worthwhile compared to failing consistently with a large classpath.",,apachespark,punya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4832,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 12:41:32 UTC 2015,,,,,,,,,,"0|i2dbiv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/15 00:01;apachespark;User 'punya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5535;;;","17/Apr/15 10:08;srowen;Issue resolved by pull request 5535
[https://github.com/apache/spark/pull/5535];;;","17/Apr/15 11:43;punya;Would it be reasonable to back port this to branch-1.3 or is it too late for that?;;;","17/Apr/15 12:31;srowen;It's too late for 1.3.1, but not too late to put it in branch 1.3 for 1.3.2. I think that's OK. I should also then back-port SPARK-5825, which is the same change for another instance in the file.;;;","17/Apr/15 12:41;srowen;Oops, SPARK-5825 was already back ported to 1.2 and 1.3 actually, so this should too, since it goes together with it naturally. Done.;;;",,,,,,,,,,,,,,,,,,,,,,,,
History server slow startup if the event log directory is large,SPARK-6951,12821345,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,mcheah,mcheah,15/Apr/15 22:37,11/Apr/18 14:49,14/Jul/23 06:26,11/Apr/18 14:49,1.3.0,,,,,,2.4.0,,,,,,Web UI,,,,3,,,,,,"I started my history server, then navigated to the web UI where I expected to be able to view some completed applications, but the webpage was not available. It turned out that the History Server was not finished parsing all of the event logs in the event log directory that I had specified. I had accumulated a lot of event logs from months of running Spark, so it would have taken a very long time for the History Server to crunch through them all. I purged the event log directory and started from scratch, and the UI loaded immediately.

We should have a pagination strategy or parse the directory lazily to avoid needing to wait after starting the history server.",,ajbozarth,anthonyr,apachespark,asukhenko,codingcat,irashid,jdanbrown,jooseong,liushaohui,lixin,mcheah,mkim,stevel@apache.org,Steven Rand,tgraves,vanzin,vladio,WangTaoTheTonic,wzheng,xietingwen,zshao,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-9123,SPARK-11123,,,,,,SPARK-18085,,SPARK-13988,SPARK-11373,SPARK-18010,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 11 14:49:55 UTC 2018,,,,,,,,,,"0|i2dbcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/15 23:12;srowen;Not exactly the same thing, but automatically cleaning up logs is addressed in https://issues.apache.org/jira/browse/SPARK-3562 and kind of fixes the 'freeze' problem.;;;","30/Jun/15 13:43;tgraves;This actually happens more then just at start up.  If you have  large number of files, especially in progress files.  Or even just large history files, it takes forever for the history server to pick up new completed ones and show on the UI. ;;;","24/Aug/15 23:20;vladio;I think this was solved in: https://issues.apache.org/jira/browse/SPARK-5522, but it was released in Spark 1.4, while you were using Spark 1.3 at that moment. I've also tried to reproduce this locally (with ~5000 log files) but I couldn't get the expected behavior. I'm not extremely sure about this, but I thought is good to post this here! (thumbsup);;;","25/Aug/15 13:05;tgraves;I'm running Spark 1.4.1 history server in production and its still very slow to start up and pick up new versions, especially when there are very large history files.  I haven't looked at the implementation but it seems like it would be better for the history server to just read some metadata and display it in the list and then when the user clicks on it load the entire thing.  Especially for very large ones.  Or keep a certain number loaded in the background.  
;;;","25/Aug/15 13:53;tgraves;Sorry I was wrong.. I just went and tested it and it does start up fairly quickly now.  I'm having problems with it getting stuck reading large files which is a separate issue.;;;","29/Sep/15 15:57;tgraves;I'm still seeing history server startup really slow in 1.5.1... SPARK-5522 may have helped although I don't see it showing  list of application names in the UI without the data from those so not sure if that pr did what description says.

It would be really nice is it could get metadata without having to parse the entire log. ;;;","02/Oct/15 13:23;stevel@apache.org;There's a way to address this that I've been thinking of for the SPARK-1537 integration, where there's the extra dependency on the timeline server running.

Essentially, it looks like the bus replay operation can be asynchronous: the UI is returned immediately while some (pooled) thread replays the event log.;;;","27/Oct/15 13:29;tgraves;Unfortunately for us I don't think the current timeline server will scale at this point. But I'll have to try it once things are ready.

There are other ways to make this faster to startup.  

- we could simply do an ls and show applications without any details and then add details as they are loaded.   Then if user clicks on one load that one first.
- we could change file name or directory structures to have some basic information that could be displayed by a simple file listing (like the MapReduce history server does).;;;","25/Nov/15 14:33;stevel@apache.org;Thomas, I think you are right w.r.t simple fixes: add a metadata document to summarise the ongoing app. Essentially the same stuff that I am pushing out to the timeline server as summary info

# timestamp of start
# yarn attempt ID (allows for comparision with running apps and their state)
# timestamp of finished (if set). Unset ==> app is not finished.
# updated time
# spark version (I hope to use this in better incompatibility warnings)

There's a risk of inconsistency with the real log: what if the log saved as finished but the metadata isn't in sync and tagged as open? Maybe when an app is viewed and the full log pulled in any discrepancies could be corrected there by saving a new metadata file.;;;","25/Nov/15 14:37;stevel@apache.org;SPARK-11373 proposes adding metrics to the history server; if the time to list and load files is included then it'll help track this problem. my latest SPARK-7889 adds timings & counts of the getAppUI() invocations in preparation for the monitoring, but does nothing about listings;;;","20/Oct/16 08:36;srowen;Pointing this to a future issue which has an open PR for the same issue.;;;","25/Oct/16 16:44;vanzin;I reopened this after discussion in the bug; the other change (SPARK-18010) makes startup a little faster, but not necessarily fast, for large directories / log files.;;;","01/Mar/17 06:25;zshao;Did we consider using a distributed store to solve the scalability issue?  Single-node LevelDB will hit scalability issue soon.
;;;","01/Mar/17 11:58;stevel@apache.org;Having been downstream of YARN timeline server and its leveldb stuff, I'm not convinced adding a DB will magically solve problems: there's still the need to populate that DB. In a physical cluster it may stay around, but in a virtual world it won't, and it does complicate life all round.

With SPARK-17843 giving the UI hints about ongoing async load, at least people can see that there is a load in progress.;;;","08/Mar/17 07:56;lixin;In my case,the inprogress file is the main reason, so if the inprogress file is necessary for the history server, or if  the  inprogress file can be cleaned up with the 'spark.history.fs.cleaner.maxAge';;;","30/Mar/18 21:34;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/20952;;;","11/Apr/18 14:49;irashid;Issue resolved by pull request 20952
[https://github.com/apache/spark/pull/20952];;;",,,,,,,,,,,,
Tiny bug in PowerIterationClusteringExample in which radius not accepted from command line,SPARK-6937,12821231,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,javadba,javadba,javadba,15/Apr/15 17:06,15/Apr/15 20:28,14/Jul/23 06:26,15/Apr/15 20:28,1.3.0,,,,,,1.3.1,1.4.0,,,,,MLlib,,,,0,,,,,,Tiny bug in PowerIterationClusteringExample in which radius not accepted from command line,,apachespark,javadba,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 20:28:27 UTC 2015,,,,,,,,,,"0|i2danb:",9223372036854775807,,,,,mengxr,,,,,,,,,,,,,,,,,,,,,,"15/Apr/15 17:07;apachespark;User 'javadba' has created a pull request for this issue:
https://github.com/apache/spark/pull/5531;;;","15/Apr/15 20:28;mengxr;Issue resolved by pull request 5531
[https://github.com/apache/spark/pull/5531];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SQLContext.sql() caused deadlock in multi-thread env,SPARK-6936,12821225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,zwu.net@gmail.com,zwu.net@gmail.com,15/Apr/15 16:25,08/Mar/17 23:32,14/Jul/23 06:26,16/Dec/15 16:41,1.3.0,,,,,,1.5.0,,,,,,SQL,,,,0,deadlock,sql,threading,,,"Doing (the same query) in more than one threads with SQLConext.sql may lead to deadlock. Here is a way to reproduce it (since this is multi-thread issue, the reproduction may or may not be so easy).

1. Register a relatively big table.

2.  Create two different classes and in the classes, do the same query in a method and put the results in a set and print out the set size.

3.  Create two threads to use an object from each class in the run method. Start the threads. For my tests,  it can have a deadlock just in a few runs. ","JDK 1.8.x, RedHat
Linux version 2.6.32-431.23.3.el6.x86_64 (mockbuild@x86-027.build.eng.bos.redhat.com) (gcc version 4.4.7 20120313 (Red Hat 4.4.7-4) (GCC) ) #1 SMP Wed Jul 16 06:12:23 EDT 2014
",glenn.strycker@gmail.com,Henry Min,marmbrus,rxin,zwu.net@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,Wed Mar 08 23:32:00 UTC 2017,,,,,,,,,,"0|i2dalz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/15 20:30;marmbrus;What is the query?  Can you provide a thread dump?;;;","15/Apr/15 20:55;zwu.net@gmail.com;1. The query is something like this (sorry since the data is proprietary -- I cannot release it here).

SELECT distinct timezoneind, css.rnc, css.field1,localtime, greenwich  FROM cs , css  WHERE   cs.r = css.r and cs.name = css.name and greenwich >  '2015-03-04'

2.  Maven dependency: 
 <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.11</artifactId>
            <version>1.3.0</version>
            <scope> compile</scope> 
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.11</artifactId>
            <version>1.3.0</version>
            <scope> compile</scope> 
        </dependency>

3. Here is the trace I reproduced on Windows 7/  
C:>java -version
java version ""1.8.0_25""
Java(TM) SE Runtime Environment (build 1.8.0_25-b18)
Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)

=================================================================================
2015-04-15 13:46:13
Full thread dump Java HotSpot(TM) 64-Bit Server VM (25.25-b02 mixed mode):

""DestroyJavaVM"" #98 prio=5 os_prio=0 tid=0x000000005dc8c800 nid=0x2048 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Thread-34"" #97 prio=5 os_prio=0 tid=0x0000000061df6000 nid=0x14b0 runnable [0x00000000646ea000]
   java.lang.Thread.State: RUNNABLE
	at scala.collection.mutable.HashTable$class.elemEquals(HashTable.scala:358)
	at scala.collection.mutable.HashMap.elemEquals(HashMap.scala:40)
	at scala.collection.mutable.HashTable$class.scala$collection$mutable$HashTable$$findEntry0(HashTable.scala:136)
	at scala.collection.mutable.HashTable$class.findEntry(HashTable.scala:132)
	at scala.collection.mutable.HashMap.findEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.get(HashMap.scala:70)
	at scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:186)
	at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:80)
	at scala.util.parsing.combinator.syntactical.StdTokenParsers$class.keyword(StdTokenParsers.scala:37)
	at scala.util.parsing.combinator.syntactical.StandardTokenParsers.keyword(StandardTokenParsers.scala:29)
	at org.apache.spark.sql.catalyst.SqlParser$$anonfun$primary$1$$anonfun$apply$287$$anonfun$apply$288.apply(SqlParser.scala:373)
	at org.apache.spark.sql.catalyst.SqlParser$$anonfun$primary$1$$anonfun$apply$287$$anonfun$apply$288.apply(SqlParser.scala:373)
	at scala.util.parsing.combinator.Parsers$Parser.p$lzycompute$2(Parsers.scala:267)
	- locked <0x00000000dc33a648> (a scala.util.parsing.combinator.Parsers$$anon$3)
	at scala.util.parsing.combinator.Parsers$Parser.scala$util$parsing$combinator$Parsers$Parser$$p$3(Parsers.scala:267)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$$tilde$1.apply(Parsers.scala:268)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$$tilde$1.apply(Parsers.scala:268)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:143)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:197)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.PackratParsers$$anonfun$parser2packrat$1.apply(PackratParsers.scala:147)
	at scala.util.parsing.combinator.PackratParsers$$anonfun$parser2packrat$1.apply(PackratParsers.scala:147)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.PackratParsers$class.grow(PackratParsers.scala:294)
	at scala.util.parsing.combinator.PackratParsers$class.scala$util$parsing$combinator$PackratParsers$$lrAnswer(PackratParsers.scala:219)
	at scala.util.parsing.combinator.PackratParsers$$anon$2.apply(PackratParsers.scala:264)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:197)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:143)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:882)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:882)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:881)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSparkSQLParser.scala:38)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:96)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:95)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:137)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:197)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:882)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:882)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:881)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSparkSQLParser.scala:38)
	at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:134)
	at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:134)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:134)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:915)
	at com.att.nwk.io.Reader.fillZoneMap(Reader.java:260)
	at com.att.nwk.io.UMTSEricCSReader$7.run(UMTSEricCSReader.java:598)

""Thread-33"" #96 prio=5 os_prio=0 tid=0x000000005bc49000 nid=0xe4c runnable [0x000000005e309000]
   java.lang.Thread.State: RUNNABLE
	at scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:206)
	at scala.collection.mutable.HashTable$HashUtils$class.elemHashCode(HashTable.scala:409)
	at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:40)
	at scala.collection.mutable.HashTable$class.resize(HashTable.scala:257)
	at scala.collection.mutable.HashTable$class.scala$collection$mutable$HashTable$$addEntry0(HashTable.scala:154)
	at scala.collection.mutable.HashTable$class.findOrAddEntry(HashTable.scala:166)
	at scala.collection.mutable.HashMap.findOrAddEntry(HashMap.scala:40)
	at scala.collection.mutable.HashMap.put(HashMap.scala:76)
	at scala.collection.mutable.HashMap.update(HashMap.scala:81)
	at scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:188)
	at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:80)
	at scala.util.parsing.combinator.syntactical.StdTokenParsers$class.keyword(StdTokenParsers.scala:37)
	at scala.util.parsing.combinator.syntactical.StandardTokenParsers.keyword(StandardTokenParsers.scala:29)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser$Keyword.parser(AbstractSparkSQLParser.scala:46)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.asParser(AbstractSparkSQLParser.scala:49)
	at org.apache.spark.sql.catalyst.SqlParser$$anonfun$andExpression$1.apply(SqlParser.scala:230)
	at org.apache.spark.sql.catalyst.SqlParser$$anonfun$andExpression$1.apply(SqlParser.scala:230)
	at scala.util.parsing.combinator.Parsers$$anonfun$chainl1$1$$anonfun$apply$10.apply(Parsers.scala:798)
	at scala.util.parsing.combinator.Parsers$$anonfun$chainl1$1$$anonfun$apply$10.apply(Parsers.scala:798)
	at scala.util.parsing.combinator.Parsers$$anonfun$rep1$1.apply(Parsers.scala:727)
	at scala.util.parsing.combinator.Parsers$$anonfun$rep1$1.apply(Parsers.scala:712)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:143)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:197)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.PackratParsers$$anonfun$parser2packrat$1.apply(PackratParsers.scala:147)
	at scala.util.parsing.combinator.PackratParsers$$anonfun$parser2packrat$1.apply(PackratParsers.scala:147)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.PackratParsers$class.grow(PackratParsers.scala:294)
	at scala.util.parsing.combinator.PackratParsers$class.scala$util$parsing$combinator$PackratParsers$$lrAnswer(PackratParsers.scala:219)
	at scala.util.parsing.combinator.PackratParsers$$anon$2.apply(PackratParsers.scala:264)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:197)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Success.flatMapWithNext(Parsers.scala:143)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$flatMap$1.apply(Parsers.scala:234)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:882)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:882)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:881)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSparkSQLParser.scala:38)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
	at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:96)
	at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:95)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:137)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:237)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:197)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:249)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:217)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:882)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:882)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:881)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSparkSQLParser.scala:38)
	at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:134)
	at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:134)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:134)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:915)
	at com.att.nwk.io.Reader.fillZoneMap(Reader.java:260)
	at com.att.nwk.io.UMTSEricCSReader$6.run(UMTSEricCSReader.java:584)

""sparkDriver-akka.remote.default-remote-dispatcher-23"" #95 daemon prio=5 os_prio=0 tid=0x0000000061df5800 nid=0x24d8 waiting on condition [0x000000006739f000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081050ef0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-akka.actor.default-dispatcher-22"" #94 daemon prio=5 os_prio=0 tid=0x0000000061df4800 nid=0x27b8 waiting on condition [0x00000000640af000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-akka.actor.default-dispatcher-21"" #93 daemon prio=5 os_prio=0 tid=0x0000000061df4000 nid=0x1c44 waiting on condition [0x000000006717f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-akka.actor.default-dispatcher-20"" #92 daemon prio=5 os_prio=0 tid=0x000000005bc48000 nid=0x1a08 waiting on condition [0x000000006673f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-akka.remote.default-remote-dispatcher-19"" #91 daemon prio=5 os_prio=0 tid=0x000000005bc47800 nid=0x2594 waiting on condition [0x0000000066f8f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081050ef0> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""task-result-getter-3"" #90 daemon prio=5 os_prio=0 tid=0x0000000061df3000 nid=0x878 waiting on condition [0x0000000066d5e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081455488> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""sparkDriver-akka.actor.default-dispatcher-18"" #89 daemon prio=5 os_prio=0 tid=0x0000000061df2800 nid=0xd98 waiting on condition [0x0000000066e8e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-akka.actor.default-dispatcher-17"" #88 daemon prio=5 os_prio=0 tid=0x0000000061df1800 nid=0x2648 waiting on condition [0x000000005c2ce000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-akka.actor.default-dispatcher-15"" #86 daemon prio=5 os_prio=0 tid=0x0000000061df1000 nid=0x9d8 waiting on condition [0x0000000066c1f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""ForkJoinPool-3-worker-15"" #85 daemon prio=5 os_prio=0 tid=0x0000000061df0000 nid=0x1f60 waiting on condition [0x0000000066a2f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000008ffd19d8> (a scala.concurrent.forkjoin.ForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-akka.actor.default-dispatcher-14"" #84 daemon prio=5 os_prio=0 tid=0x0000000061def800 nid=0x200c waiting on condition [0x000000006687f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""task-result-getter-2"" #83 daemon prio=5 os_prio=0 tid=0x000000005bc4a800 nid=0x18c4 waiting on condition [0x00000000665df000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081455488> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""task-result-getter-1"" #82 daemon prio=5 os_prio=0 tid=0x000000005bc4d800 nid=0x1904 waiting on condition [0x000000006278f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081455488> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""task-result-getter-0"" #81 daemon prio=5 os_prio=0 tid=0x000000005bc46800 nid=0x534 waiting on condition [0x000000006547e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081455488> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1067)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1127)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""SparkListenerBus"" #13 daemon prio=5 os_prio=0 tid=0x000000005bc46000 nid=0x1754 waiting on condition [0x0000000062d3f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000008104f720> (a java.util.concurrent.Semaphore$NonfairSync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
	at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:62)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60)

""Spark Context Cleaner"" #72 daemon prio=5 os_prio=0 tid=0x000000005bc45000 nid=0x260c in Object.wait() [0x0000000062c2e000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:142)
	- locked <0x00000000813f4e20> (a java.lang.ref.ReferenceQueue$Lock)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.scala:136)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
	at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:133)
	at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)

""shuffle-server-0"" #63 daemon prio=5 os_prio=0 tid=0x000000005bc44800 nid=0x20fc runnable [0x000000006288e000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
	at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x0000000081443cf8> (a io.netty.channel.nio.SelectedSelectionKeySet)
	- locked <0x0000000081443d18> (a java.util.Collections$UnmodifiableSet)
	- locked <0x0000000081443c78> (a sun.nio.ch.WindowsSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)
	at java.lang.Thread.run(Thread.java:745)

""Driver Heartbeater"" #54 daemon prio=5 os_prio=0 tid=0x000000005bc43800 nid=0x22c8 waiting on condition [0x000000006101f000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:437)

""dag-scheduler-event-loop"" #53 daemon prio=5 os_prio=0 tid=0x000000005bc43000 nid=0x228c waiting on condition [0x0000000060dde000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081454a70> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
	at java.util.concurrent.LinkedBlockingDeque.takeFirst(LinkedBlockingDeque.java:492)
	at java.util.concurrent.LinkedBlockingDeque.take(LinkedBlockingDeque.java:680)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:46)

""Timer-0"" #52 daemon prio=5 os_prio=0 tid=0x000000005bc42000 nid=0x2308 in Object.wait() [0x0000000060a4f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000081453a20> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:502)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x0000000081453a20> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""qtp1455855843-51"" #51 daemon prio=5 os_prio=0 tid=0x000000005bc41800 nid=0x24cc waiting on condition [0x0000000060f1e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081447f68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1455855843-50"" #50 daemon prio=5 os_prio=0 tid=0x000000005bc40800 nid=0x2518 waiting on condition [0x0000000060cdf000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081447f68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1455855843-49"" #49 daemon prio=5 os_prio=0 tid=0x000000005bc40000 nid=0x780 waiting on condition [0x0000000060b7e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081447f68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1455855843-48"" #48 daemon prio=5 os_prio=0 tid=0x000000005bc3f000 nid=0x26c8 waiting on condition [0x00000000608ef000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000081447f68> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1455855843-47 Acceptor1 SelectChannelConnector@0.0.0.0:4040"" #47 daemon prio=5 os_prio=0 tid=0x000000005bc3e800 nid=0x1aa4 waiting for monitor entry [0x000000005e9ce000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:225)
	- waiting to lock <0x0000000081464aa0> (a java.lang.Object)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.spark-project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)

""qtp1455855843-46 Acceptor0 SelectChannelConnector@0.0.0.0:4040"" #46 daemon prio=5 os_prio=0 tid=0x000000005b8c2000 nid=0x1f44 runnable [0x000000005fccf000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
	at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:241)
	- locked <0x0000000081464aa0> (a java.lang.Object)
	at org.spark-project.jetty.server.nio.SelectChannelConnector.accept(SelectChannelConnector.java:109)
	at org.spark-project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)

""qtp1455855843-45 Selector1"" #45 daemon prio=5 os_prio=0 tid=0x000000005b8c1000 nid=0x2574 runnable [0x000000006074e000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
	at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x0000000081465038> (a sun.nio.ch.Util$2)
	- locked <0x0000000081465028> (a java.util.Collections$UnmodifiableSet)
	- locked <0x0000000081464ec8> (a sun.nio.ch.WindowsSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.spark-project.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.spark-project.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)

""qtp1455855843-44 Selector0"" #44 daemon prio=5 os_prio=0 tid=0x000000005b8c0800 nid=0x6bc runnable [0x00000000605ae000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
	at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x00000000814484d8> (a sun.nio.ch.Util$2)
	- locked <0x00000000814484c8> (a java.util.Collections$UnmodifiableSet)
	- locked <0x0000000081448368> (a sun.nio.ch.WindowsSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.spark-project.jetty.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:569)
	at org.spark-project.jetty.io.nio.SelectorManager$1.run(SelectorManager.java:290)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)

""refresh progress"" #42 daemon prio=5 os_prio=0 tid=0x000000005b8bf800 nid=0x2090 in Object.wait() [0x00000000603ff000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked <0x0000000081468858> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""SPARK_CONTEXT cleanup timer"" #41 daemon prio=5 os_prio=0 tid=0x000000005b8bf000 nid=0x27cc in Object.wait() [0x00000000602df000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000081468d40> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:502)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x0000000081468d40> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""qtp1543043602-40"" #40 daemon prio=5 os_prio=0 tid=0x000000005b8bd000 nid=0x2468 waiting on condition [0x000000005ff2e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000814696c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1543043602-39"" #39 daemon prio=5 os_prio=0 tid=0x000000005b8bc800 nid=0x24f0 waiting on condition [0x000000005fddf000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000814696c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1543043602-38"" #38 daemon prio=5 os_prio=0 tid=0x000000005b8bb800 nid=0x18a4 waiting on condition [0x000000005f66e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000814696c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1543043602-37"" #37 daemon prio=5 os_prio=0 tid=0x000000005b8bb000 nid=0x12a0 waiting on condition [0x000000005fb9e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000814696c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1543043602-36"" #36 daemon prio=5 os_prio=0 tid=0x000000005b8ba000 nid=0x9f4 waiting on condition [0x000000005fa7e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000814696c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1543043602-35"" #35 daemon prio=5 os_prio=0 tid=0x000000005b8b9800 nid=0x21b4 waiting on condition [0x000000005f94e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000814696c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1543043602-34"" #34 daemon prio=5 os_prio=0 tid=0x000000005b8b8800 nid=0xf88 waiting on condition [0x000000005f77f000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000814696c8> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
	at org.spark-project.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)

""qtp1543043602-33 Acceptor0 SocketConnector@0.0.0.0:62108"" #33 daemon prio=5 os_prio=0 tid=0x000000005b8b8000 nid=0x2694 runnable [0x000000005f51e000]
   java.lang.Thread.State: RUNNABLE
	at java.net.DualStackPlainSocketImpl.accept0(Native Method)
	at java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:131)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:404)
	at java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)
	- locked <0x000000008147a4a0> (a java.net.SocksSocketImpl)
	at java.net.ServerSocket.implAccept(ServerSocket.java:545)
	at java.net.ServerSocket.accept(ServerSocket.java:513)
	at org.spark-project.jetty.server.bio.SocketConnector.accept(SocketConnector.java:117)
	at org.spark-project.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.spark-project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.spark-project.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)

""BROADCAST_VARS cleanup timer"" #31 daemon prio=5 os_prio=0 tid=0x000000005b8b7000 nid=0x1fc4 in Object.wait() [0x000000005f38e000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000081489088> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:502)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x0000000081489088> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""BLOCK_MANAGER cleanup timer"" #30 daemon prio=5 os_prio=0 tid=0x000000005b8b6800 nid=0x1380 in Object.wait() [0x000000005f13f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000814894c8> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:502)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000000814894c8> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""MAP_OUTPUT_TRACKER cleanup timer"" #28 daemon prio=5 os_prio=0 tid=0x000000005b8b5800 nid=0x1d54 in Object.wait() [0x000000005f26f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000000813e0500> (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:502)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked <0x00000000813e0500> (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)

""New I/O server boss #6"" #27 daemon prio=5 os_prio=0 tid=0x000000005b8b5000 nid=0x2188 runnable [0x000000005edfe000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
	at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x0000000081035868> (a sun.nio.ch.Util$2)
	- locked <0x0000000081035878> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000810357e8> (a sun.nio.ch.WindowsSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:101)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""New I/O worker #5"" #26 daemon prio=5 os_prio=0 tid=0x000000005b8b4000 nid=0x1504 runnable [0x000000005ebfe000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
	at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x000000008104af20> (a sun.nio.ch.Util$2)
	- locked <0x000000008104af30> (a java.util.Collections$UnmodifiableSet)
	- locked <0x000000008104aea0> (a sun.nio.ch.WindowsSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""New I/O worker #4"" #25 daemon prio=5 os_prio=0 tid=0x000000005b8b3800 nid=0x21ec runnable [0x000000005eafe000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
	at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x0000000081029a40> (a sun.nio.ch.Util$2)
	- locked <0x0000000081029a50> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000810299c0> (a sun.nio.ch.WindowsSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""New I/O boss #3"" #24 daemon prio=5 os_prio=0 tid=0x000000005b8b2800 nid=0x25e4 runnable [0x000000005e8be000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
	at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x000000008102e210> (a sun.nio.ch.Util$2)
	- locked <0x000000008102e220> (a java.util.Collections$UnmodifiableSet)
	- locked <0x000000008102e190> (a sun.nio.ch.WindowsSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""New I/O worker #2"" #22 daemon prio=5 os_prio=0 tid=0x000000005b76d800 nid=0x1f40 runnable [0x000000005e62e000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
	at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x0000000081045640> (a sun.nio.ch.Util$2)
	- locked <0x0000000081045650> (a java.util.Collections$UnmodifiableSet)
	- locked <0x00000000810455c0> (a sun.nio.ch.WindowsSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""New I/O worker #1"" #21 daemon prio=5 os_prio=0 tid=0x000000005b8de000 nid=0x2328 runnable [0x000000005e73e000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll0(Native Method)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.poll(WindowsSelectorImpl.java:296)
	at sun.nio.ch.WindowsSelectorImpl$SubSelector.access$400(WindowsSelectorImpl.java:278)
	at sun.nio.ch.WindowsSelectorImpl.doSelect(WindowsSelectorImpl.java:159)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:86)
	- locked <0x000000008103dd38> (a sun.nio.ch.Util$2)
	- locked <0x000000008103dd48> (a java.util.Collections$UnmodifiableSet)
	- locked <0x000000008103dcb8> (a sun.nio.ch.WindowsSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:97)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:415)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

""sparkDriver-akka.actor.default-dispatcher-4"" #18 daemon prio=5 os_prio=0 tid=0x000000005b6f5800 nid=0x19f8 waiting on condition [0x000000005d0bf000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-akka.actor.default-dispatcher-3"" #17 daemon prio=5 os_prio=0 tid=0x000000005ba6d800 nid=0x2478 waiting on condition [0x000000005d1ff000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-akka.actor.default-dispatcher-2"" #16 daemon prio=5 os_prio=0 tid=0x000000005b6d6800 nid=0x21f0 waiting on condition [0x000000005cf0e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000810458f8> (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

""sparkDriver-scheduler-1"" #15 daemon prio=5 os_prio=0 tid=0x000000005b666800 nid=0x1a00 waiting on condition [0x000000005cb9e000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at akka.actor.LightArrayRevolverScheduler.waitNanos(Scheduler.scala:226)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.nextTick(Scheduler.scala:405)
	at akka.actor.LightArrayRevolverScheduler$$anon$8.run(Scheduler.scala:375)
	at java.lang.Thread.run(Thread.java:745)

""Thread-1"" #14 daemon prio=5 os_prio=0 tid=0x000000005b48f000 nid=0x1b5c runnable [0x000000005c87f000]
   java.lang.Thread.State: RUNNABLE
	at sun.net.dns.ResolverConfigurationImpl.notifyAddrChange0(Native Method)
	at sun.net.dns.ResolverConfigurationImpl$AddressChangeListener.run(ResolverConfigurationImpl.java:144)

""Service Thread"" #10 daemon prio=9 os_prio=0 tid=0x00000000588e9800 nid=0x2450 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C1 CompilerThread3"" #9 daemon prio=9 os_prio=2 tid=0x000000005887e000 nid=0x1150 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread2"" #8 daemon prio=9 os_prio=2 tid=0x000000005886d800 nid=0x106c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread1"" #7 daemon prio=9 os_prio=2 tid=0x0000000058865800 nid=0x23a8 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""C2 CompilerThread0"" #6 daemon prio=9 os_prio=2 tid=0x000000005885d800 nid=0x252c waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Attach Listener"" #5 daemon prio=5 os_prio=2 tid=0x0000000058858000 nid=0x1db4 waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Signal Dispatcher"" #4 daemon prio=9 os_prio=2 tid=0x0000000058856800 nid=0x2098 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE

""Finalizer"" #3 daemon prio=8 os_prio=1 tid=0x000000005779f000 nid=0x1068 in Object.wait() [0x0000000059c2f000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:142)
	- locked <0x000000008107fd48> (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:158)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)

""Reference Handler"" #2 daemon prio=10 os_prio=2 tid=0x0000000057796000 nid=0x2230 in Object.wait() [0x00000000599bf000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	at java.lang.Object.wait(Object.java:502)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:157)
	- locked <0x000000008101ab20> (a java.lang.ref.Reference$Lock)

""VM Thread"" os_prio=2 tid=0x0000000058803000 nid=0x1908 runnable 

""GC task thread#0 (ParallelGC)"" os_prio=0 tid=0x0000000001dd3800 nid=0x2580 runnable 

""GC task thread#1 (ParallelGC)"" os_prio=0 tid=0x0000000001dd5000 nid=0x202c runnable 

""GC task thread#2 (ParallelGC)"" os_prio=0 tid=0x0000000001dd6800 nid=0x2560 runnable 

""GC task thread#3 (ParallelGC)"" os_prio=0 tid=0x0000000001dd8000 nid=0x27d4 runnable 

""GC task thread#4 (ParallelGC)"" os_prio=0 tid=0x0000000001ddb000 nid=0x1da4 runnable 

""GC task thread#5 (ParallelGC)"" os_prio=0 tid=0x0000000001ddc800 nid=0x2114 runnable 

""GC task thread#6 (ParallelGC)"" os_prio=0 tid=0x0000000001ddd800 nid=0xc4c runnable 

""GC task thread#7 (ParallelGC)"" os_prio=0 tid=0x0000000001ddf000 nid=0x25c0 runnable 

""VM Periodic Task Thread"" os_prio=2 tid=0x00000000588f3000 nid=0x2284 waiting on condition 

JNI global references: 235

;;;","15/Apr/15 21:03;marmbrus;Thanks, that dump is helpful.  As a workaround, could you use a HiveContext with HiveQL?  It seems like you are hitting a bug in our toy SQL parser, which is really only there for people who can't use the Hive parser due to dependency conflicts.;;;","15/Apr/15 23:12;zwu.net@gmail.com;Currently, I synchronized the part, which seems to be working so far.  ;;;","16/Apr/15 16:41;marmbrus;NoSuchMethodError almost always means you are mixing incompatible versions
of libraries (in this case probably scala?) on your classpath.

;;;","16/Apr/15 17:03;zwu.net@gmail.com;You are right: I used spark-hive_2.10 instead of spark-hive_2.11 in my building . Sorry I deleted my comment before reading your comment.  Thanks,;;;","16/Dec/15 08:42;rxin;Is this still a problem on the latest Spark? I think we have fixed it already right?
;;;","08/Mar/17 23:32;Henry Min;This issue seems has been fixed on the version 1.5.0. The information here is extremely important. Can anyone provide the merged details and show me the merged source code?  Because the similar issue still exists on the latest version.;;;",,,,,,,,,,,,,,,,,,,,,
Fix the bug that using a wrong configuration for “ask” timeout in RpcEnv,SPARK-6934,12821168,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,15/Apr/15 14:10,16/Apr/15 18:49,14/Jul/23 06:26,16/Apr/15 18:49,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"I just found that I used a wrong configuration for the ""ask"" timeout in SPARK-5124. It should have been ""spark.akka.askTimeout"".",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 14:16:41 UTC 2015,,,,,,,,,,"0|i2da9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/15 14:16;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5529;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"python: struct.pack('!q', value) in write_long(value, stream) in serializers.py require int(but doesn't raise exceptions in common cases)",SPARK-6931,12821114,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,bryanc,zhangcx93,zhangcx93,15/Apr/15 10:36,11/Sep/15 10:09,14/Jul/23 06:26,10/Sep/15 18:21,1.3.0,,,,,,1.2.3,1.3.2,,,,,PySpark,,,,0,easyfix,,,,,"when I map my own feature calculation module's function, sparks raises:
Traceback (most recent call last):
  File ""/usr/local/Cellar/apache-spark/1.3.0/libexec/python/pyspark/daemon.py"", line 162, in manager
    code = worker(sock)
  File ""/usr/local/Cellar/apache-spark/1.3.0/libexec/python/pyspark/daemon.py"", line 60, in worker
    worker_main(infile, outfile)
  File ""/usr/local/Cellar/apache-spark/1.3.0/libexec/python/pyspark/worker.py"", line 115, in main
    report_times(outfile, boot_time, init_time, finish_time)
  File ""/usr/local/Cellar/apache-spark/1.3.0/libexec/python/pyspark/worker.py"", line 40, in report_times
    write_long(1000 * boot, outfile)
  File ""/usr/local/Cellar/apache-spark/1.3.0/libexec/python/pyspark/serializers.py"", line 518, in write_long
    stream.write(struct.pack(""!q"", value))
DeprecationWarning: integer argument expected, got float

so I turn on the serializers.py, and tried to print the value out, which is a float, came from 1000 * time.time()

when I removed my lib, or add a rdd.count() before mapping my lib, this bug won’t appear.

so I edited the function to :

def write_long(value, stream):
    stream.write(struct.pack(""!q"", int(value))) # added a int(value)

everything seem fine…

According to python’s doc for struct(https://docs.python.org/2/library/struct.html)’s Note(3), the value should be a int(for q), and if it’s a float, it’ll try use __index__(), else, try __int__, but since __int__ is deprecated, it’ll raise DeprecationWarning. And float doesn’t have __index__, but has __int__, so it should raise the exception every time.

But, as you can see, in normal cases, it won’t raise the exception, and the code works perfectly, and exec struct.pack('!q', 111.1) in console or a clean file won't raise any exception…I can hardly tell how my lib might effect a time.time()'s value passed to struct.pack()... it might a python's original bug or what.

Anyway, this value should be a int, so add a int() to it.",,apachespark,bryanc,davies,joshrosen,zhangcx93,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Thu Sep 10 18:21:00 UTC 2015,,,,,,,,,,"0|i2d9xz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/15 00:24;joshrosen;It looks like this was also reported on the mailing list a year ago, but was reported to be benign: https://mail-archives.apache.org/mod_mbox/spark-user/201406.mbox/%3CCANR-kKdNSR=4+z=vfpK0tKS_7qjYTM_fRO6+qaLArTNvtEx_Wg@mail.gmail.com%3E;;;","17/Apr/15 02:55;bryanc;I just checked and it looks like some int() casting was done in worker.py  to the calls to write_long() as part of Python 3 support in this commit a few hours ago https://github.com/apache/spark/commit/04e44b37cc04f62fbf9e08c7076349e0a4d12ea8#diff-d33eea00c68dfd120f4ceae6381f34cd

I believe this issue should be fixed by that.;;;","18/Apr/15 23:59;joshrosen;[~bryanc], good catch.  I've marked this as fixed in 1.4.0.  It should be pretty easy to backport this to 1.3.x / 1.2.x as well, although I haven't done that yet.;;;","03/Sep/15 22:39;bryanc;I can backport the fix for this;;;","03/Sep/15 23:44;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/8593;;;","04/Sep/15 00:26;apachespark;User 'BryanCutler' has created a pull request for this issue:
https://github.com/apache/spark/pull/8594;;;","10/Sep/15 18:21;davies;Issue resolved by pull request 8594
[https://github.com/apache/spark/pull/8594];;;",,,,,,,,,,,,,,,,,,,,,,
Sorting Error when codegen on,SPARK-6927,12821072,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smacat,smacat,smacat,15/Apr/15 06:56,24/Apr/15 00:24,14/Jul/23 06:26,17/Apr/15 00:32,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"When code gen is on, some unit test in SqlQuerySuit failed.

 test(""sorting"") {
    val before = conf.externalSortEnabled
    setConf(SQLConf.EXTERNAL_SORT, ""false"")
    sortTest()
    setConf(SQLConf.EXTERNAL_SORT, before.toString)
  }

  test(""external sorting"") {
    val before = conf.externalSortEnabled
    setConf(SQLConf.EXTERNAL_SORT, ""true"")
    sortTest()
    setConf(SQLConf.EXTERNAL_SORT, before.toString)
  }

GenerateOrding can't deal BinaryType.

",,apachespark,marmbrus,smacat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 00:32:56 UTC 2015,,,,,,,,,,"0|i2d9on:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/15 10:31;apachespark;User 'kaka1992' has created a pull request for this issue:
https://github.com/apache/spark/pull/5524;;;","17/Apr/15 00:32;marmbrus;Issue resolved by pull request 5524
[https://github.com/apache/spark/pull/5524];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken data returned to PySpark dataframe if any large numbers used in Scala land,SPARK-6917,12821008,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,airhorns,airhorns,14/Apr/15 23:25,02/Jun/15 06:12,14/Jul/23 06:26,02/Jun/15 06:12,1.3.0,,,,,,1.4.0,,,,,,PySpark,SQL,,,0,,,,,,"When trying to access data stored in a Parquet file with an INT96 column (read: TimestampType() encoded for Impala), if the INT96 column is included in the fetched data, other, smaller numeric types come back broken.

{code}
In [1]: sql.parquetFile(""/Users/hornairs/Downloads/part-r-00001.parquet"").select('int_col', 'long_col').first()
Out[1]: Row(int_col=Decimal('1'), long_col=Decimal('10'))

In [2]: sql.parquetFile(""/Users/hornairs/Downloads/part-r-00001.parquet"").first()
Out[2]: Row(long_col={u'__class__': u'scala.runtime.BoxedUnit'}, str_col=u'Hello!', int_col={u'__class__': u'scala.runtime.BoxedUnit'}, date_col=datetime.datetime(1, 12, 31, 19, 0, tzinfo=<DstTzInfo 'America/Toronto' EDT-1 day, 19:00:00 DST>))
{code}

Note the {{\{u'__class__': u'scala.runtime.BoxedUnit'}}} values being returned for the {{int_col}} and {{long_col}} columns in the second loop above. This only happens if I select the {{date_col}} which is stored as {{INT96}}. 

I don't know much about Scala boxing, but I assume that somehow by including numeric columns that are bigger than a machine word I trigger some different, slower execution path somewhere that boxes stuff and causes this problem.

If anyone could give me any pointers on where to get started fixing this I'd be happy to dive in!","Spark 1.3, Python 2.7.6, Scala 2.10",airhorns,apachespark,davies,joshrosen,mengxr,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7314,,,,,,,,,,,,,,,"14/Apr/15 23:26;airhorns;part-r-00001.parquet;https://issues.apache.org/jira/secure/attachment/12725421/part-r-00001.parquet",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 01 20:31:06 UTC 2015,,,,,,,,,,"0|i2d9an:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/15 23:26;airhorns;Attached is the parquet file I used. ;;;","20/Apr/15 23:17;airhorns;[~davies] or [~joshrosen] any idea why this might be happening? I can dig in if you give me some pointers but I don't really know where to start! ;;;","04/May/15 20:10;airhorns;[~pwendell] just checking, would you consider this a blocker for 1.4? It does mean that dataframes are pretty broken for anyone using dates, decimals, or big ints from Python. ;;;","15/May/15 20:57;davies;[~yhuai] It's a bug in SQL or Parquet library:
{code}
scala> sqlContext.parquetFile(""/Users/davies/Downloads/part-r-00001.parquet"")
res1: org.apache.spark.sql.DataFrame = [long_col: decimal(18,0), str_col: string, int_col: decimal(18,0), date_col: timestamp]

scala> sqlContext.parquetFile(""/Users/davies/Downloads/part-r-00001.parquet"").first()
res2: org.apache.spark.sql.Row = [(),Hello!,(),0001-12-31 16:00:00.0]

scala> sqlContext.parquetFile(""/Users/davies/Downloads/part-r-00001.parquet"").select(""long_col"").first()
res3: org.apache.spark.sql.Row = [10]

scala> sqlContext.parquetFile(""/Users/davies/Downloads/part-r-00001.parquet"").select(""long_col"", ""date_col"").first()
res4: org.apache.spark.sql.Row = [(),0001-12-31 16:00:00.0]

scala> sqlContext.parquetFile(""/Users/davies/Downloads/part-r-00001.parquet"").select(""date_col"").first()
res5: org.apache.spark.sql.Row = [0001-12-31 16:00:00.0]
{code};;;","15/May/15 21:29;yhuai;[~adrian-wang] Can you take a look?;;;","01/Jun/15 20:31;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6558;;;",,,,,,,,,,,,,,,,,,,,,,,
"""No suitable driver found"" loading JDBC dataframe using driver added by through SparkContext.addJar",SPARK-6913,12820968,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,slavik.baranov,hotou,hotou,14/Apr/15 21:15,01/May/15 22:01,14/Jul/23 06:26,01/May/15 01:45,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"val sc = new SparkContext(conf)
sc.addJar(""J:\mysql-connector-java-5.1.35.jar"")

val df = sqlContext.jdbc(""jdbc:mysql://localhost:3000/test_db?user=abc&password=123"", ""table1"")
df.show()

Folloing error:
2015-04-14 17:04:39,541 [task-result-getter-0] WARN  org.apache.spark.scheduler.TaskSetManager - Lost task 0.0 in stage 0.0 (TID 0, dev1.test.dc2.com): java.sql.SQLException: No suitable driver found for jdbc:mysql://localhost:3000/test_db?user=abc&password=123
	at java.sql.DriverManager.getConnection(DriverManager.java:689)
	at java.sql.DriverManager.getConnection(DriverManager.java:270)
	at org.apache.spark.sql.jdbc.JDBCRDD$$anonfun$getConnector$1.apply(JDBCRDD.scala:158)
	at org.apache.spark.sql.jdbc.JDBCRDD$$anonfun$getConnector$1.apply(JDBCRDD.scala:150)
	at org.apache.spark.sql.jdbc.JDBCRDD$$anon$1.<init>(JDBCRDD.scala:317)
	at org.apache.spark.sql.jdbc.JDBCRDD.compute(JDBCRDD.scala:309)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
",,397090770,apachespark,hotou,marmbrus,slavik.baranov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7312,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 01 22:01:32 UTC 2015,,,,,,,,,,"0|i2d91r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/15 10:49;slavik.baranov;The problem is in java.sql.DriverManager that doesn't see the drivers loaded by ClassLoaders other than bootstrap ClassLoader.

The solution would be to create a proxy driver included in Spark assembly that forwards all requests to wrapped driver.

I have a working fix for this issue and going to make pull request soon.;;;","29/Apr/15 14:58;apachespark;User 'SlavikBaranov' has created a pull request for this issue:
https://github.com/apache/spark/pull/5782;;;","01/May/15 01:45;marmbrus;Issue resolved by pull request 5782
[https://github.com/apache/spark/pull/5782];;;","01/May/15 22:01;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/5847;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"Throw an AnalysisException when unsupported Java Map<K,V> types used in Hive UDF",SPARK-6912,12820897,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,14/Apr/15 18:33,18/Jan/17 06:55,14/Jul/23 06:26,08/Jul/15 17:34,1.4.0,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"The current implementation can't handle Map<K,V> as a return type in Hive UDF. 
We assume an UDF below;

public class UDFToIntIntMap extends UDF {
    public Map<Integer, Integer> evaluate(Object o);
}

Hive supports this type, and see a link below for details;

https://github.com/kyluka/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java#L163
https://github.com/l1x/apache-hive/blob/master/hive-0.8.1/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorFactory.java#L118

",,apachespark,marmbrus,maropu,zhaiyuyong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-19081,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 17:34:05 UTC 2015,,,,,,,,,,"0|i2d8pj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/15 18:36;maropu;After SPARK-6747 resolved, I'll make a PR for this issue.;;;","07/Jul/15 07:58;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/7257;;;","08/Jul/15 17:34;marmbrus;Issue resolved by pull request 7257
[https://github.com/apache/spark/pull/7257];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade Snappy Java to 1.1.1.7 to fix bug that resulted in worse compression,SPARK-6905,12820881,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,14/Apr/15 17:34,21/Apr/15 06:36,14/Jul/23 06:26,14/Apr/15 20:44,1.2.2,1.3.1,,,,,1.2.3,1.3.2,1.4.0,,,,Spark Core,,,,0,,,,,,"We should upgrade our {{snappy-java}} dependency to 1.1.1.7 in order to include a fix for a bug that resulted in worse compression (see https://github.com/xerial/snappy-java/issues/100).  I believe that this may partially fix SPARK-5081, an issue where the size of shuffle data increased from Spark 1.1.x to 1.2.0.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5081,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 20:44:39 UTC 2015,,,,,,,,,,"0|i2d8m7:",9223372036854775807,,,,,,,,,,,,,,1.2.3,1.3.2,,,,,,,,,,,,"14/Apr/15 17:41;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5512;;;","14/Apr/15 20:44;joshrosen;Issue resolved by pull request 5512
[https://github.com/apache/spark/pull/5512];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Row() object can be mutated even though it should be immutable,SPARK-6902,12820836,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,jarfa,jarfa,14/Apr/15 14:57,08/Aug/15 15:38,14/Jul/23 06:26,08/Aug/15 15:38,1.2.0,,,,,,1.5.0,,,,,,PySpark,SQL,,,1,,,,,,"See the below code snippet, IMHO it shouldn't let you assign {{x.c = 5}} and should just give you an error.

{quote}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 1.2.0-SNAPSHOT
      /_/

Using Python version 2.6.6 (r266:84292, Jan 22 2014 09:42:36)
SparkContext available as sc.
>>> from pyspark.sql import *
>>> x = Row(a=1, b=2, c=3)
>>> x
Row(a=1, b=2, c=3)
>>> x.__dict__
\{'__FIELDS__': ['a', 'b', 'c']\}
>>> x.c
3
>>> x.c = 5
>>> x
Row(a=1, b=2, c=3)
>>> x.__dict__
\{'__FIELDS__': ['a', 'b', 'c'], 'c': 5\}
>>> x.c
5
{quote}",,apachespark,davies,jarfa,joshrosen,mnazario,sds,yuu.ishikawa@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 08 15:38:37 UTC 2015,,,,,,,,,,"0|i2d8c7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/15 22:16;davies;[~jarfa] Python is a dynamic language, it's not common to provide read-only interface (there are many ways to break it), so I'd like to leave it as current (won't fix). Does this work for you?;;;","16/May/15 00:12;jarfa;[~davies] it works for me simply because I *now* know not to mis-use the Row() object like that.

I filed the bug because I didn't want other people to tear their hair out over the same problem. I think editing a Row like I did should throw an error, because had it done so in version 1.2 I would've saved a ton of time by not having to hunt for the bug in my code.;;;","31/May/15 21:45;joshrosen;If you'd like this to raise a warning, can you open a pull request with tests to demonstrate how we can handle this?  There might be an easy way to implement warnings / errors here for the common cases, even though it might not work for some extreme corner-cases.;;;","06/Aug/15 22:06;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/8009;;;","08/Aug/15 15:38;davies;Issue resolved by pull request 8009
[https://github.com/apache/spark/pull/8009];;;",,,,,,,,,,,,,,,,,,,,,,,,
Type mismatch when using codegen,SPARK-6899,12820763,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,smacat,smacat,14/Apr/15 09:22,24/Apr/15 00:30,14/Jul/23 06:26,17/Apr/15 00:50,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"When I run tests in DataFrameSuite with codegen on, some type mismatched error occured.

{code}
test(""average"") {
    checkAnswer(
      decimalData.agg(avg('a)),
      Row(new java.math.BigDecimal(2.0)))
  }

type mismatch;
 found   : Int(0)
 required: org.apache.spark.sql.types.DecimalType#JvmType
{code}

",,apachespark,gvramana,marmbrus,smacat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 00:50:30 UTC 2015,,,,,,,,,,"0|i2d7vz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"15/Apr/15 05:33;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5517;;;","17/Apr/15 00:50;marmbrus;Issue resolved by pull request 5517
[https://github.com/apache/spark/pull/5517];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Special chars in column names is broken,SPARK-6898,12820754,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,14/Apr/15 08:41,27/Jul/15 16:36,14/Jul/23 06:26,15/Apr/15 20:39,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"This function is added a long time ago, but it's not complete, it will fail if we have ""."" inside column name.
{code}
test(""SPARK-3483 Special chars in column names"") {
    val data = sparkContext.parallelize(
      Seq(""""""{""key?number1"": ""value1"", ""key.number2"": ""value2""}""""""))
    jsonRDD(data).registerTempTable(""records"")
    sql(""SELECT `key?number1`, `key.number2` FROM records"")
  }
{code}
this test will fail.",,apachespark,cloud_fan,dsdinter,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2775,SPARK-5632,,,,,,,,SPARK-9371,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jul 27 05:18:03 UTC 2015,,,,,,,,,,"0|i2d7u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/15 15:24;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/5511;;;","15/Apr/15 20:39;marmbrus;Issue resolved by pull request 5511
[https://github.com/apache/spark/pull/5511];;;","24/Jul/15 16:28;dsdinter;I hope I am missing something here but I am facing this issue in my current cluster (Currently running compiled version of Spark 1.4.0 with Hive support)
(I tried with 1.4.0 and 1.4.1 binaries locally and had same issue).
{noformat}
sqlContext.jsonRDD(sc.makeRDD(""""""{""a"": {""c.b"": 1}, ""b.$q"": [{""a@!.q"": 1}], ""q.w"": {""w.i&"": [1]}}"""""" :: Nil)).registerTempTable(""t"")
sqlContext.sql(""SELECT `key?number1`, `key.number2` FROM records"")
sqlContext.sql(""SELECT a.`c.b`, `b.$q`[0].`a@!.q`, `q.w`.`w.i&`[0] FROM t"")

Either select is throwing the same error:
scala> sqlContext.sql(""SELECT a.`c.b`, `b.$q`[0].`a@!.q`, `q.w`.`w.i&`[0] FROM t"")
15/07/24 17:23:12 INFO ParseDriver: Parsing command: SELECT a.`c.b`, `b.$q`[0].`a@!.q`, `q.w`.`w.i&`[0] FROM t
15/07/24 17:23:12 INFO ParseDriver: Parse Completed
org.apache.spark.sql.AnalysisException: cannot resolve 'b.$q' given input columns a, b.$q, q.w; line 1 pos 16
        at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.
scala:63)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.
scala:52)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
{noformat};;;","25/Jul/15 07:35;cloud_fan;[~dsdinter] I tried your case on 1.4.1
{code}
sqlContext.jsonRDD(sc.makeRDD(""""""{""a"": {""c.b"": 1}, ""b.$q"": [{""a@!.q"": 1}], ""q.w"": {""w.i&"": [1]}}"""""" :: Nil)).registerTempTable(""t"")
sqlContext.sql(""SELECT a.`c.b`, `b.$q`[0].`a@!.q`, `q.w`.`w.i&`[0] FROM t"")
{code}
It has no problem, did you use jdbc or thrift-server to run the query? can you copy and paste the full stack-trace of this issue?;;;","25/Jul/15 08:08;dsdinter;[~cloud_fan] I tried from spark-shell (It works from thrift-server as I had Tableau connected to a big denormilized table which includes dots in column names).
Here is the full stack (Maybe my locale has something to do? 

{noformat}
org.apache.spark.sql.AnalysisException: cannot resolve 'b.$q' given input columns a, b.$q, q.w; line 1 pos 16
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:63)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:52)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:285)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:299)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:283)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:299)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:283)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:299)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:329)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:283)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:108)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:123)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:127)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:52)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:98)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:42)
	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)
	at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:131)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:755)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:20)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:27)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:29)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:31)
	at $iwC$$iwC$$iwC.<init>(<console>:33)
	at $iwC$$iwC.<init>(<console>:35)
	at $iwC.<init>(<console>:37)
	at <init>(<console>:39)
	at .<init>(<console>:43)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{noformat}
;;;","25/Jul/15 09:12;dsdinter;[~cloud_fan] I literally copy&pasted the snipped of code from your reply and used Spark 1.4.1 from brew on my Mac and I am getting the error I just included in my earlier reply. For reference I am in UK, in case the locale is doing something weird with backticks...
{noformat}
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
2015-07-25 10:09:05.510 java[6618:141238] Unable to load realm info from SCDynamicStore
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/07/25 10:09:06 INFO SecurityManager: Changing view acls to: davidsabater
15/07/25 10:09:06 INFO SecurityManager: Changing modify acls to: davidsabater
15/07/25 10:09:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(davidsabater); users with modify permissions: Set(davidsabater)
15/07/25 10:09:06 INFO HttpServer: Starting HTTP Server
15/07/25 10:09:06 INFO Utils: Successfully started service 'HTTP class server' on port 60987.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.4.1
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.7.0_45)
Type in expressions to have them evaluated.
Type :help for more information.
15/07/25 10:09:09 INFO SparkContext: Running Spark version 1.4.1
15/07/25 10:09:09 INFO SecurityManager: Changing view acls to: davidsabater
15/07/25 10:09:09 INFO SecurityManager: Changing modify acls to: davidsabater
15/07/25 10:09:09 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(davidsabater); users with modify permissions: Set(davidsabater)
{noformat}

My locale:
LANG=""en_GB.UTF-8""
LC_COLLATE=""en_GB.UTF-8""
LC_CTYPE=""en_GB.UTF-8""
LC_MESSAGES=""en_GB.UTF-8""
LC_MONETARY=""en_GB.UTF-8""
LC_NUMERIC=""en_GB.UTF-8""
LC_TIME=""en_GB.UTF-8"";;;","27/Jul/15 03:58;cloud_fan;It's really weird that I can run this case in test suite, but fail in spark-shell, I'll investigate it. cc [~marmbrus];;;","27/Jul/15 04:35;cloud_fan;I build spark from code v1.4.1 and can handle this case, however the 1.4.1 pre-build version will fail, does anybody know the reason? cc [~rxin];;;","27/Jul/15 04:55;rxin;See if it is broken in HiveContext.
;;;","27/Jul/15 05:18;cloud_fan;OK finally figured it out. It's the problem in hive context and I have opened a JIRA to fix it: https://issues.apache.org/jira/browse/SPARK-9371.

[~dsdinter] thanks for reporting this!;;;",,,,,,,,,,,,,,,,,,,
building error because of guava import,SPARK-6896,12820739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,14/Apr/15 07:33,15/Apr/15 09:35,14/Jul/23 06:26,15/Apr/15 09:24,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"[INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[info] Compiling 8 Scala sources to /root/projects/spark/sql/hive-thriftserver/target/scala-2.10/classes...
[error] bad symbolic reference. A signature in Utils.class refers to term util
[error] in package com.google.common which is not available.
[error] It may be completely missing from the current classpath, or the version on
[error] the classpath might be incompatible with the version used when compiling Utils.class.
[error]
[error] while compiling: /root/projects/spark/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLEnv.scala
[error] during phase: erasure
[error] library version: version 2.10.4
[error] compiler version: version 2.10.4
[error] reconstructed args: -deprecation -classpath 
",,adrian-wang,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6930,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 09:24:10 UTC 2015,,,,,,,,,,"0|i2d7qv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/15 07:36;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5507;;;","15/Apr/15 09:24;srowen;Issue resolved by pull request 5507
[https://github.com/apache/spark/pull/5507];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
typo spark.executor.extraLibraryOptions => spark.executor.extraLibraryPath,SPARK-6894,12820726,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,14/Apr/15 06:38,14/Apr/15 19:02,14/Jul/23 06:26,14/Apr/15 19:02,1.4.0,,,,,,1.4.0,,,,,,Spark Submit,,,,0,,,,,,Looks like it's an occasional turnover in rewriting.,,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 06:48:45 UTC 2015,,,,,,,,,,"0|i2d7nz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"14/Apr/15 06:48;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/5506;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Local cluster mode is broken with SPARK_PREPEND_CLASSES,SPARK-6890,12820680,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vanzin,davies,davies,13/Apr/15 23:56,16/Apr/15 23:42,14/Jul/23 06:26,15/Apr/15 01:52,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"In master, local cluster mode is broken. If I run `bin/spark-submit --master local-cluster[2,1,512]`, my executors keep failing with class not found exception. It appears that the assembly jar is not added to the executors' class paths. I suspect that this is caused by https://github.com/apache/spark/pull/5085.

{code}
Exception in thread ""main"" java.lang.NoClassDefFoundError: scala/Option
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2531)
	at java.lang.Class.getMethod0(Class.java:2774)
	at java.lang.Class.getMethod(Class.java:1663)
	at sun.launcher.LauncherHelper.getMainMethod(LauncherHelper.java:494)
	at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:486)
Caused by: java.lang.ClassNotFoundException: scala.Option
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
{code}",,andrewor14,apachespark,davies,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6668,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 17:50:10 UTC 2015,,,,,,,,,,"0|i2d7ef:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"14/Apr/15 01:46;vanzin;Do you have `SPARK_PREPEND_CLASSES` set by any chance?

BTW, personally, I was against the change that causes this failure, and again personally I wouldn't really be against reverting it. It seems to cause more issues than it solves. [/cc [~nravi]];;;","14/Apr/15 02:02;vanzin;Also, another possible way to fix this is to pass the location of the assembly jar to the {{Main}} class, instead of the current code. That was my original suggestion when Nishkam was working on this. It makes the code a little uglier (to allow for plumbing that path through the code), but it would allow maintaining the behavior added by that patch while probably fixing this issue.

Let me know if you're working on this, Andrew, otherwise I can do that.;;;","14/Apr/15 02:04;andrewor14;I'm not actively working on this. Feel free to fix it since you and Nishkam have more experience in that part of the code.;;;","14/Apr/15 02:52;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5504;;;","14/Apr/15 17:48;vanzin;FYI it's broken on Linux too (and most probably on Windows) if you set SPARK_PREPEND_CLASSES. If you don't, it should work (at least it does for me).;;;","14/Apr/15 17:50;andrewor14;ah I see. I will update the title again ;;;",,,,,,,,,,,,,,,,,,,,,,,
ColumnBuilder misses FloatType,SPARK-6887,12820651,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yhuai,yhuai,yhuai,13/Apr/15 22:14,15/Apr/15 20:04,14/Jul/23 06:26,15/Apr/15 20:04,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"To reproduce ...
{code}
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

val schema = StructType(StructField(""c"", FloatType, true) :: Nil)

val rdd = sc.parallelize(1 to 100).map(i => Row(i.toFloat))

sqlContext.createDataFrame(rdd, schema).registerTempTable(""test"")

sqlContext.sql(""cache table test"")

sqlContext.table(""test"").show
{code}
The exception is ...
{code}
15/04/13 15:00:12 INFO DAGScheduler: Job 0 failed: collect at SparkPlan.scala:88, took 0.474392 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 0.0 failed 1 times, most recent failure: Lost task 5.0 in stage 0.0 (TID 5, localhost): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableFloat cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableLong
	at org.apache.spark.sql.catalyst.expressions.SpecificMutableRow.setLong(SpecificMutableRow.scala:292)
	at org.apache.spark.sql.columnar.compression.LongDelta$Decoder.next(compressionSchemes.scala:539)
	at org.apache.spark.sql.columnar.compression.CompressibleColumnAccessor$class.extractSingle(CompressibleColumnAccessor.scala:37)
	at org.apache.spark.sql.columnar.NativeColumnAccessor.extractSingle(ColumnAccessor.scala:64)
	at org.apache.spark.sql.columnar.BasicColumnAccessor.extractTo(ColumnAccessor.scala:54)
	at org.apache.spark.sql.columnar.NativeColumnAccessor.org$apache$spark$sql$columnar$NullableColumnAccessor$$super$extractTo(ColumnAccessor.scala:64)
	at org.apache.spark.sql.columnar.NullableColumnAccessor$class.extractTo(NullableColumnAccessor.scala:52)
	at org.apache.spark.sql.columnar.NativeColumnAccessor.extractTo(ColumnAccessor.scala:64)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8$$anonfun$13$$anon$2.next(InMemoryColumnarTableScan.scala:295)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8$$anonfun$13$$anon$2.next(InMemoryColumnarTableScan.scala:290)
	at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$6.apply(Aggregate.scala:130)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$6.apply(Aggregate.scala:126)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:640)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:640)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:210)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 20:04:16 UTC 2015,,,,,,,,,,"0|i2d77z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/15 22:17;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/5499;;;","15/Apr/15 20:04;marmbrus;Issue resolved by pull request 5499
[https://github.com/apache/spark/pull/5499];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Big closure in PySpark will fail during shuffle,SPARK-6886,12820626,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,13/Apr/15 20:22,15/Apr/15 20:36,14/Jul/23 06:26,15/Apr/15 20:36,1.2.1,1.3.0,1.4.0,,,,1.2.3,1.3.2,1.4.0,,,,PySpark,,,,0,,,,,,"Reported by  beifei.zhou <beifei.zhou at ximalaya.com>: 

I am using spark to process bid datasets. However, there is always problem when executing reduceByKey on a large dataset, whereas with a smaller dataset.  May I asked you how could I solve this issue?

The error is always like this:
{code}
15/04/09 11:27:46 ERROR Executor: Exception in task 3.0 in stage 1.0 (TID 5)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/Users/nali/Softwares/spark/python/pyspark/worker.py"", line 90, in main
    command = pickleSer.loads(command.value)
  File ""/Users/nali/Softwares/spark/python/pyspark/broadcast.py"", line 106, in value
    self._value = self.load(self._path)
  File ""/Users/nali/Softwares/spark/python/pyspark/broadcast.py"", line 87, in load
    with open(path, 'rb', 1 << 20) as f:
IOError: [Errno 2] No such file or directory: '/private/var/folders/_x/n59vb1b54pl96lvldz2lr_v40000gn/T/spark-37d8ecbc-9ac9-4aa2-be23-12823f4cd1ed/pyspark-1e3d5904-a5b6-4222-a146-91bfdb4a33a7/tmp8XMhgG'
{code}

Here I attach my code:
{code}
import codecs
from pyspark import SparkContext, SparkConf
from operator import add 
import operator
from pyspark.storagelevel import StorageLevel

def combine_dict(a,b):
    a.update(b)
    return a
conf = SparkConf()
sc = SparkContext(appName = ""tag"")
al_tag_dict = sc.textFile('albumtag.txt').map(lambda x: x.split(',')).map(lambda x: {x[0]: x[1:]}).reduce(lambda a, b: combine_dict(a,b))

result = sc.textFile('uidAlbumscore.txt')\
        .map(lambda x: x.split(','))\
        .filter(lambda x: x[1] in al_tag_dict.keys())\
        .map(lambda x: (x[0], al_tag_dict[x[1]], float(x[2])))\
        .map(lambda x: map(lambda a: ((x[0], a), x[2]), x[1]))\
        .flatMap(lambda x: x)\ 
        .map(lambda x: (str(x[0][0]), x[1]))\
        .reduceByKey(add)\
#        .map(lambda x: x[0][0]+','+x[0][1]+','+str(x[1])+'\n')\
#        .reduce(add)
#codecs.open('tag_score.txt','w','utf-8').write(result)
print result.first()
{code}",,apachespark,apetresc,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 20:36:58 UTC 2015,,,,,,,,,,"0|i2d72f:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"13/Apr/15 20:59;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/5496;;;","15/Apr/15 20:36;joshrosen;This should be fixed for 1.2.3, 1.3.2, and 1.4.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark ThriftServer2 Kerberos failed encountering java.lang.IllegalArgumentException: Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]",SPARK-6882,12820577,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,stevel@apache.org,alee526,alee526,13/Apr/15 16:44,02/Oct/15 15:18,14/Jul/23 06:26,02/Oct/15 13:16,1.2.1,1.3.0,1.4.0,,,,1.5.1,,,,,,SQL,,,,0,,,,,,"When Kerberos is enabled, I get the following exceptions. 
{code}
2015-03-13 18:26:05,363 ERROR org.apache.hive.service.cli.thrift.ThriftCLIService (ThriftBinaryCLIService.java:run(93)) - Error: 
java.lang.IllegalArgumentException: Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]
{code}

I tried it in
* Spark 1.2.1 git commit b6eaf77d4332bfb0a698849b1f5f917d20d70e97
* Spark 1.3.0 rc1 commit label 0dcb5d9f31b713ed90bcec63ebc4e530cbb69851

with
* Apache Hive 0.13.1
* Apache Hadoop 2.4.1

Build command
{code}
mvn -U -X -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Phive-thriftserver -Dhadoop.version=2.4.1 -Dyarn.version=2.4.1 -Dhive.version=0.13.1 -DskipTests install
{code}

When starting Spark ThriftServer in {{yarn-client}} mode, the command to start thriftserver looks like this

{code}
./start-thriftserver.sh --hiveconf hive.server2.thrift.port=20000 --hiveconf hive.server2.thrift.bind.host=$(hostname) --master yarn-client
{code}

{{hostname}} points to the current hostname of the machine I'm using.

Error message in {{spark.log}} from Spark 1.2.1 (1.2 rc1)
{code}
2015-03-13 18:26:05,363 ERROR org.apache.hive.service.cli.thrift.ThriftCLIService (ThriftBinaryCLIService.java:run(93)) - Error: 
java.lang.IllegalArgumentException: Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]
        at org.apache.hive.service.auth.SaslQOP.fromString(SaslQOP.java:56)
        at org.apache.hive.service.auth.HiveAuthFactory.getSaslProperties(HiveAuthFactory.java:118)
        at org.apache.hive.service.auth.HiveAuthFactory.getAuthTransFactory(HiveAuthFactory.java:133)
        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:43)
        at java.lang.Thread.run(Thread.java:744)
{code}

I'm wondering if this is due to the same problem described in HIVE-8154 HIVE-7620 due to an older code based for the Spark ThriftServer?

Any insights are appreciated. Currently, I can't get Spark ThriftServer2 to run against a Kerberos cluster (Apache 2.4.1).

My hive-site.xml looks like the following for spark/conf.
The kerberos keytab and tgt are configured correctly, I'm able to connect to metastore, but the subsequent steps failed due to the exception.
{code}
<property>
  <name>hive.semantic.analyzer.factory.impl</name>
  <value>org.apache.hcatalog.cli.HCatSemanticAnalyzerFactory</value>
</property>
<property>
  <name>hive.metastore.execute.setugi</name>
  <value>true</value>
</property>
<property>
  <name>hive.stats.autogather</name>
  <value>false</value>
</property>
<property>
  <name>hive.session.history.enabled</name>
  <value>true</value>
</property>
<property>
  <name>hive.querylog.location</name>
  <value>/tmp/home/hive/log/${user.name}</value>
</property>
<property>
  <name>hive.exec.local.scratchdir</name>
  <value>/tmp/hive/scratch/${user.name}</value>
</property>
<property>
  <name>hive.metastore.uris</name>
  <value>thrift://somehostname:9083</value>
</property>
<!-- HIVE SERVER 2 -->
<property>
  <name>hive.server2.authentication</name>
  <value>KERBEROS</value>
</property>
<property>
  <name>hive.server2.authentication.kerberos.principal</name>
  <value>***</value>
</property>
<property>
  <name>hive.server2.authentication.kerberos.keytab</name>
  <value>***</value>
</property>
<property>
  <name>hive.server2.thrift.sasl.qop</name>
  <value>auth</value>
  <description>Sasl QOP value; one of 'auth', 'auth-int' and 'auth-conf'</description>
</property>
<property>
  <name>hive.server2.enable.impersonation</name>
  <description>Enable user impersonation for HiveServer2</description>
  <value>true</value>
</property>
<!-- HIVE METASTORE -->
<property>
  <name>hive.metastore.sasl.enabled</name>
  <value>true</value>
</property>
<property>
  <name>hive.metastore.kerberos.keytab.file</name>
  <value>***</value>
</property>
<property>
  <name>hive.metastore.kerberos.principal</name>
  <value>***</value>
</property>
<property>
  <name>hive.metastore.cache.pinobjtypes</name>
  <value>Table,Database,Type,FieldSchema,Order</value>
</property>
<property>
  <name>hdfs_sentinel_file</name>
  <value>***</value>
</property>
<property>
  <name>hive.metastore.warehouse.dir</name>
  <value>/hive</value>
</property>
<property>
  <name>hive.metastore.client.socket.timeout</name>
  <value>600</value>
</property>
<property>
  <name>hive.warehouse.subdir.inherit.perms</name>
  <value>true</value>
</property>
{code}

Here, I'm attaching a more detail logs from Spark 1.3 rc1.
{code}
2015-04-13 16:37:20,688 INFO  org.apache.hadoop.security.UserGroupInformation (UserGroupInformation.java:loginUserFromKeytab(893)) - Login successful for user hiveserver/alee-vpc2-dt.test.testhost.com@TEST.TESTHOST.COM using keytab file /etc/testhost/secrets/hiveserver.keytab
2015-04-13 16:37:20,689 INFO  org.apache.hive.service.AbstractService (SparkSQLSessionManager.scala:init(43)) - HiveServer2: Async execution pool size 100
2015-04-13 16:37:20,691 INFO  org.apache.hive.service.AbstractService (AbstractService.java:init(89)) - Service:OperationManager is inited.
2015-04-13 16:37:20,691 INFO  org.apache.hive.service.AbstractService (SparkSQLCLIService.scala:initCompositeService(85)) - Service: SessionManager is inited.
2015-04-13 16:37:20,692 INFO  org.apache.hive.service.AbstractService (SparkSQLCLIService.scala:initCompositeService(85)) - Service: CLIService is inited.
2015-04-13 16:37:20,692 INFO  org.apache.hive.service.AbstractService (AbstractService.java:init(89)) - Service:ThriftBinaryCLIService is inited.
2015-04-13 16:37:20,692 INFO  org.apache.hive.service.AbstractService (SparkSQLCLIService.scala:initCompositeService(85)) - Service: HiveServer2 is inited.
2015-04-13 16:37:20,692 INFO  org.apache.hive.service.AbstractService (AbstractService.java:start(104)) - Service:OperationManager is started.
2015-04-13 16:37:20,693 INFO  org.apache.hive.service.AbstractService (AbstractService.java:start(104)) - Service:SessionManager is started.
2015-04-13 16:37:20,693 INFO  org.apache.hive.service.AbstractService (AbstractService.java:start(104)) - Service:CLIService is started.
2015-04-13 16:37:20,758 INFO  hive.metastore (HiveMetaStoreClient.java:open(297)) - Trying to connect to metastore with URI thrift://alee-vpc2-gw.test.testhost.com:9083
2015-04-13 16:37:20,784 INFO  hive.metastore (HiveMetaStoreClient.java:open(385)) - Connected to metastore.
2015-04-13 16:37:20,801 INFO  org.apache.hive.service.AbstractService (AbstractService.java:start(104)) - Service:ThriftBinaryCLIService is started.
2015-04-13 16:37:20,801 INFO  org.apache.hive.service.AbstractService (AbstractService.java:start(104)) - Service:HiveServer2 is started.
2015-04-13 16:37:20,802 INFO  org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 (Logging.scala:logInfo(59)) - HiveThriftServer2 started
2015-04-13 16:37:20,923 INFO  org.apache.hadoop.security.UserGroupInformation (UserGroupInformation.java:loginUserFromKeytab(893)) - Login successful for user hiveserver/alee-vpc2-dt.test.testhost.com@TEST.TESTHOST.COM using keytab file /etc/testhost/secrets/hiveserver.keytab
2015-04-13 16:37:20,930 INFO  org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager (AbstractDelegationTokenSecretManager.java:updateCurrentKey(222)) - Updating the current master key for generating delegation tokens
2015-04-13 16:37:20,936 INFO  org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:logUpdateMasterKey(209)) - New master key with key id=0
2015-04-13 16:37:20,944 ERROR org.apache.hive.service.cli.thrift.ThriftCLIService (ThriftBinaryCLIService.java:run(93)) - Error: 
java.lang.IllegalArgumentException: Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]
  at org.apache.hive.service.auth.SaslQOP.fromString(SaslQOP.java:56)
  at org.apache.hive.service.auth.HiveAuthFactory.getSaslProperties(HiveAuthFactory.java:118)
  at org.apache.hive.service.auth.HiveAuthFactory.getAuthTransFactory(HiveAuthFactory.java:133)
  at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:43)
  at java.lang.Thread.run(Thread.java:744)
2015-04-13 16:37:20,947 INFO  org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:run(299)) - Starting expired delegation token remover thread, tokenRemoverScanInterval=60 min(s)
2015-04-13 16:37:20,964 INFO  org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager (AbstractDelegationTokenSecretManager.java:updateCurrentKey(222)) - Updating the current master key for generating delegation tokens
2015-04-13 16:37:20,965 INFO  org.apache.hive.service.AbstractService (AbstractService.java:stop(125)) - Service:ThriftBinaryCLIService is stopped.
2015-04-13 16:37:20,966 INFO  org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:logUpdateMasterKey(209)) - New master key with key id=1
{code}","* Apache Hadoop 2.4.1 with Kerberos Enabled
* Apache Hive 0.13.1
* Spark 1.2.1 git commit b6eaf77d4332bfb0a698849b1f5f917d20d70e97
* Spark 1.3.0 rc1 commit label 0dcb5d9f31b713ed90bcec63ebc4e530cbb69851",alee526,gogototo,gweidner,ilovesoup,mdominguez@cloudera.com,stevel@apache.org,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8064,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 02 13:15:52 UTC 2015,,,,,,,,,,"0|i2d6rr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/15 07:15;WangTaoTheTonic;Did you try to set hive.server2.thrift.sasl.qop to ""auth-conf""?;;;","06/Jul/15 20:16;alee526;Hi [~WangTaoTheTonic], I'm still getting the same exception. This is from Spark 1.4 so it is impacting 1.4 as well. You will notice that, it is showing {{null}} in the exception, so it looks like it is having trouble to pick up that property form the beginning.

{code}
2015-07-06 20:12:00,882 ERROR org.apache.hive.service.cli.thrift.ThriftCLIService (ThriftBinaryCLIService.java:run(93)) - Error: 
java.lang.IllegalArgumentException: Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]
	at org.apache.hive.service.auth.SaslQOP.fromString(SaslQOP.java:56)
	at org.apache.hive.service.auth.HiveAuthFactory.getSaslProperties(HiveAuthFactory.java:118)
	at org.apache.hive.service.auth.HiveAuthFactory.getAuthTransFactory(HiveAuthFactory.java:133)
	at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:43)
	at java.lang.Thread.run(Thread.java:744)
{code}

;;;","09/Jul/15 04:03;gogototo;We have the same problem, how to solve it?;;;","09/Jul/15 10:16;ilovesoup;What we do is:
Make Hive 13.1 applied with this patch (and related patch in Jira) https://issues.apache.org/jira/browse/HIVE-6741
In Spark-env.sh point to Hive lib's jar that with patch. It will make Spark Thrift server load hive's class instead of the ones in assemble first.
And, it shall work.;;;","10/Jul/15 02:38;gogototo;Thanks for reply. We add it through --jar options, don't work. How you append it? ;;;","10/Jul/15 07:37;ilovesoup;Can you try add in spark-env.sh's classpath and make sure it stay before other jars.;;;","10/Jul/15 07:38;ilovesoup;Can you try add in spark-env.sh's classpath and make sure it stay before other jars.;;;","11/Jul/15 04:35;alee526;I don't think updating spark-env.sh {{SPARK_CLASSPATH}} will be a good idea since this conflicts with {{--driver-class-path}} in yarn-client mode.
But if this is the current work around, I can specify it with a different directory with SPARK_CONF_DIR just to get it up and running.

Regarding Bin's approach, I believe you will need to enable {{spark.yarn.user.classpath.first}} according to SPARK-939, but I think it should be picking up user JAR by default now, isn't?;;;","07/Aug/15 17:29;stevel@apache.org;Spark 1.5 will use Hive 1.2.1; this problem shouldn't occur then;;;","02/Oct/15 13:15;stevel@apache.org;This will have gone away with Spark 1.5 and the upgrade of hive done in SPARK-8064; closing as fixed. ;;;",,,,,,,,,,,,,,,,,,,
Spark Shutdowns with NoSuchElementException when running parallel collect on cachedRDD,SPARK-6880,12820563,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pankajarora12,pankajarora12,pankajarora12,13/Apr/15 15:51,17/Sep/15 17:21,14/Jul/23 06:26,14/Apr/15 19:08,1.2.1,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Spark Shutdowns with NoSuchElementException when running parallel collect on cachedRDDs

Below is the stack trace

15/03/27 11:12:43 ERROR DAGSchedulerActorSupervisor: eventProcesserActor failed; shutting down SparkContext
java.util.NoSuchElementException: key not found: 28
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at scala.collection.AbstractMap.default(Map.scala:58)
        at scala.collection.mutable.HashMap.apply(HashMap.scala:64)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:808)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:778)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:781)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:780)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:780)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:781)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:780)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:780)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:781)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$submitStage$4.apply(DAGScheduler.scala:780)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:780)
        at org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:762)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1389)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
","CentOs6.0, java7",apachespark,irashid,markhamstra,pankajarora12,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Sep 17 17:21:37 UTC 2015,,,,,,,,,,"0|i2d6on:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"13/Apr/15 16:06;srowen;(Don't assign Target / Fix Version)

This is not a valid JIRA, as there is no detail. If you intend to add detail later, OK, but please next time wait until you have all of that information ready before opening a JIRA. Otherwise I'm going to close this.;;;","13/Apr/15 16:44;apachespark;User 'pankajarora12' has created a pull request for this issue:
https://github.com/apache/spark/pull/5494;;;","13/Apr/15 16:50;pankajarora12;Sean,
Sorry for missing stack trace. Added that in description.;;;","20/May/15 02:00;markhamstra;This fix should also be applied as far back as we care to go in the maintenance branches, since the bug also exists in branch-1.0, branch-1.1, branch-1.2 and branch-1.3.;;;","20/May/15 20:02;apachespark;User 'markhamstra' has created a pull request for this issue:
https://github.com/apache/spark/pull/6291;;;","02/Sep/15 16:50;irashid;Explanation of the remaining issue from [~markhamstra] on why the previous fix was an improvement, and did completely remove the NPE, but wasn't quite the right fix, because it left some minor issues:

bq. Tasks for a Stage that was previously part of a Job that is no longer active would be re-submitted as though they were part of the prior Job and with no properties set. Since properties are what are used to set an other-than-default scheduling pool, this would affect FAIR scheduler usage, but it would also affect anything else that depends on the settings of the properties (which would be just user code at this point, since Spark itself doesn't really use the properties for anything else other than Job Group and Description, which end up in the WebUI, can be used to kill by JobGroup, etc.) Even the default, FIFO scheduling would be affected, however, since the resubmission of the Tasks under the earlier jobId would effectively give them a higher priority/greater urgency than the ActiveJob that now actually needs them.  In any event, the Tasks would generate correct results.;;;","17/Sep/15 17:21;markhamstra;see SPARK-10666;;;",,,,,,,,,,,,,,,,,,,,,,
Check if the app is completed before clean it up,SPARK-6879,12820519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,13/Apr/15 12:03,23/Apr/15 21:21,14/Jul/23 06:26,23/Apr/15 21:20,,,,,,,1.4.0,,,,,,Deploy,,,,0,,,,,,"Now history server deletes the directory whichi expires according to its modification time. It is not good for those long-running applicaitons, as they might be deleted before finished.",,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 21:20:36 UTC 2015,,,,,,,,,,"0|i2d6ev:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/15 12:41;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/5491;;;","23/Apr/15 21:20;srowen;Issue resolved by pull request 5491
[https://github.com/apache/spark/pull/5491];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Sum on empty RDD fails with exception,SPARK-6878,12820506,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,erikvanoosten,erikvanoosten,erikvanoosten,13/Apr/15 10:56,15/Jun/15 13:01,14/Jul/23 06:26,14/Apr/15 11:40,1.2.0,,,,,,1.2.2,1.3.2,1.4.0,,,,Spark Core,,,,0,,,,,,"{{Sum}} on an empty RDD throws an exception. Expected result is {{0}}.

A simple fix is the replace

{noformat}
class DoubleRDDFunctions {
  def sum(): Double = self.reduce(_ + _)
{noformat} 

with:

{noformat}
class DoubleRDDFunctions {
  def sum(): Double = self.aggregate(0.0)(_ + _, _ + _)
{noformat}
",,apachespark,erikvanoosten,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8373,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Tue Apr 14 11:40:51 UTC 2015,,,,,,,,,,"0|i2d6bz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/15 11:02;srowen;Interesting question -- what's the expected sum of nothing at all? although I can see the argument both ways, 0 is probably the better result since {{Array[Double]().sum}} is 0. So {{sc.parallelize(Array[Double]()).sum}} should as well. Want to make a PR?;;;","13/Apr/15 11:15;erikvanoosten;The answer is only defined because the RDD is an {{RDD[Double]}} :)

Sure, I'll make a PR.
Is the proposed solution acceptable?;;;","13/Apr/15 11:20;srowen;Yes, and I think it could even be a little simpler by calling {{fold(0.0)(_ + _)}} ?;;;","13/Apr/15 11:43;erikvanoosten;Ah, yes. I now see that fold also first reduces per partition.;;;","13/Apr/15 12:15;apachespark;User 'erikvanoosten' has created a pull request for this issue:
https://github.com/apache/spark/pull/5489;;;","13/Apr/15 12:15;erikvanoosten;Pull request: https://github.com/apache/spark/pull/5489;;;","14/Apr/15 11:40;srowen;Issue resolved by pull request 5489
[https://github.com/apache/spark/pull/5489];;;",,,,,,,,,,,,,,,,,,,,,,
external sort need to copy,SPARK-6872,12820400,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,12/Apr/15 15:03,24/Apr/15 00:15,14/Jul/23 06:26,13/Apr/15 23:01,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,,,adrian-wang,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 13 23:01:12 UTC 2015,,,,,,,,,,"0|i2d5on:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/15 15:05;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5481;;;","13/Apr/15 23:01;marmbrus;Issue resolved by pull request 5481
[https://github.com/apache/spark/pull/5481];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
WITH clause in CTE can not following another WITH clause,SPARK-6871,12820384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,12/Apr/15 10:10,15/Apr/15 06:49,14/Jul/23 06:26,15/Apr/15 06:49,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"For example, this sql query ""WITH q1 AS (SELECT * FROM testData) WITH q2 AS (SELECT * FROM q1) SELECT * FROM q2"" should not be successfully parsed.",,apachespark,cfregly,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 12 10:12:01 UTC 2015,,,,,,,,,,"0|i2d5l3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/15 10:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5480;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add pyspark archives path to PYTHONPATH,SPARK-6869,12820375,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lianhuiwang,Sephiroth-Lin,Sephiroth-Lin,12/Apr/15 08:00,26/Jun/15 08:55,14/Jul/23 06:27,08/May/15 13:47,1.0.0,,,,,,1.4.0,,,,,,PySpark,,,,0,,,,,,"From SPARK-1920 and SPARK-1520 we know PySpark on Yarn can not work when the assembly jar are package by JDK 1.7+, so ship pyspark archives to executors by Yarn with --py-files. The pyspark archives name must contains ""spark-pyspark"".

1st: zip pyspark to spark-pyspark_2.10.zip
2nd:./bin/spark-submit --master yarn-client/yarn-cluster --py-files spark-pyspark_2.10.zip app.py args",,apachespark,jongyoul,Sephiroth-Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1920,,,,,,,SPARK-1920,SPARK-8646,SPARK-6797,ZEPPELIN-18,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 19 14:31:59 UTC 2015,,,,,,,,,,"0|i2d5j3:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"12/Apr/15 08:06;apachespark;User 'Sephiroth-Lin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5478;;;","19/Apr/15 14:31;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5580;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Container link broken on Spark UI Executors page when YARN is set to HTTPS_ONLY,SPARK-6868,12820371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,deanchen,deanchen,deanchen,12/Apr/15 07:04,13/Apr/15 11:13,14/Jul/23 06:27,13/Apr/15 11:13,1.1.0,1.1.1,1.2.0,1.2.1,1.3.0,,1.3.2,1.4.0,,,,,YARN,,,,0,,,,,,"The stdout and stderr log links on the executor page will use the http:// prefix even if the node manager does not support http and only https via setting yarn.http.policy=HTTPS_ONLY.

Unfortunately the unencrypted http link in that case does not return a 404 but a binary file containing random binary chars. This causes a lot of confusion for the end user since it seems like the log file exists and is just filled with garbage. (see attached screenshot)

The fix is to prefix container log links with https:// instead of http:// if yarn.http.policy=HTTPS_ONLY. YARN's job page has this exact logic as seen here: https://github.com/apache/hadoop/blob/e1109fb65608a668cd53dc324dadc6f63a74eeb9/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/JobBlock.java#L108",,apachespark,deanchen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/15 07:06;deanchen;Screen Shot 2015-04-11 at 11.49.21 PM.png;https://issues.apache.org/jira/secure/attachment/12724800/Screen+Shot+2015-04-11+at+11.49.21+PM.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 12 07:13:43 UTC 2015,,,,,,,,,,"0|i2d5i7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/15 07:13;apachespark;User 'deanchen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5477;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scalastyle config prevents building Maven child modules alone,SPARK-6861,12820315,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,11/Apr/15 14:40,15/Apr/15 14:18,14/Jul/23 06:27,15/Apr/15 14:18,1.3.1,,,,,,1.4.0,,,,,,Build,,,,0,,,,,,"This doesn't work because scalastyle fails to run: {{cd core; mvn package}} It's something that might reasonably be expected to work, and does, except for scalastyle.

scalastyle fails because its config file is declared to be at {{scalastyle-config.xml}}, but this is where it is relative to the project root (parent) only. The fix is to prefix with 

{code}
${project.parent.basedir}
{code}

And while I'm at it:

It's a little funny that {{scalastyle-config.xml}} is at the top level of the project. It should probably live in {{dev/}} with other style-related resources. This requires updating the SBT config too.

The current config writes output in {{scalastyle-output.xml}}, but, this means it ends up in the root directory of every module, where it's not cleaned up. It should go in {{target/}}.

Regarding SPARK-4066, I also note that the config forces scalastyle checks to run in the {{package}} phase, when its default is the later {{verify}} phase. The complaint there was that running something like {{package}} during development with outstanding style problems fails the build. Just letting it default to {{verify}} would push back execution of scalastyle to cases where, say, {{install}} was run (or it was triggered manually).

We can update Scalastyle to 0.7.0, to match the SBT build that actually runs these checks for real.

CC [~rahulsinghal.iitd]",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4066,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 14:18:11 UTC 2015,,,,,,,,,,"0|i2d55r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/15 14:48;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5471;;;","15/Apr/15 14:18;srowen;Issue resolved by pull request 5471
[https://github.com/apache/spark/pull/5471];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the possible inconsistency of StreamingPage,SPARK-6860,12820310,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,11/Apr/15 13:27,15/Apr/15 11:12,14/Jul/23 06:27,13/Apr/15 11:21,,,,,,,1.3.2,1.4.0,,,,,DStreams,Web UI,,,0,,,,,,"Because ""StreamingPage.render"" doesn't hold the ""listener"" lock when generating the content, the different parts of content may have some inconsistent values if ""listener"" updates its status at the same time. And it will confuse people.

We should add ""listener.synchronized"" to make sure we have a consistent view of StreamingJobProgressListener when creating the content.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 13 11:21:42 UTC 2015,,,,,,,,,,"0|i2d54n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/15 13:41;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5470;;;","13/Apr/15 11:21;srowen;Issue resolved by pull request 5470
[https://github.com/apache/spark/pull/5470];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet File Binary column statistics error when reuse byte[] among rows,SPARK-6859,12820297,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,rdblue,yijieshen,yijieshen,11/Apr/15 11:24,31/May/16 22:12,14/Jul/23 06:27,31/May/16 22:12,1.2.0,1.3.0,1.4.0,,,,2.0.0,,,,,,SQL,,,,1,,,,,,"Suppose I create a dataRDD which extends RDD\[Row\], and each row is GenericMutableRow(Array(Int, Array\[Byte\])). A same Array\[Byte\] object is reused among rows but has different content each time. When I convert it to a dataFrame and save it as Parquet File, the file's row group statistic(max & min) of Binary column would be wrong.

\\
\\
Here is the reason: In Parquet, BinaryStatistic just keep max & min as parquet.io.api.Binary references, Spark sql would generate a new Binary backed by the same Array\[Byte\] passed from row.

						     		   	
| |reference| |backed| |	
|max: Binary|---------->|ByteArrayBackedBinary|---------->|Array\[Byte\]|

Therefore, each time parquet updating row group's statistic, max & min would always refer to the same Array\[Byte\], which has new content each time. When parquet decides to save it into file, the last row's content would be saved as both max & min.

\\
\\
It seems it is a parquet bug because it's parquet's responsibility to update statistics correctly.
But not quite sure. Should I report it as a bug in parquet JIRA? ",,ianlcsd,jkleckner,lian cheng,nemccarthy,saucam,yijieshen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-11153,,SPARK-9876,SPARK-11784,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 31 22:12:05 UTC 2016,,,,,,,,,,"0|i2d51r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/15 16:00;lian cheng;[~yijieshen] Thanks for reporting! And yes, please also open a JIRA ticket for Parquet and link it with this one so that it's easier to track.

[~marmbrus] I guess we should disable pushing down filters involving binary type before this bug is fixed in Parquet.;;;","12/Apr/15 16:20;lian cheng;For 1.3 and prior versions, this issue isn't that serious, since strings are immutable. But in 1.4 we are adding mutable UTF8String ([PR #5350|https://github.com/apache/spark/pull/5350]).;;;","12/Apr/15 16:44;lian cheng;A better way can be defensive copy while inserting byte arrays to parquet, so that we don't suffer read performance regression.;;;","13/Apr/15 01:16;yijieshen;I opened a JIRA ticket in Parquet: [PARQUET-251|https://issues.apache.org/jira/browse/PARQUET-251];;;","13/Apr/15 17:00;lian cheng;[~rdblue] pointed out 1 fact that I missed in PARQUET-251: we need to work out a way to ignore (binary) min/max stats for all existing data.

So from Spark SQL side, we have to disable filter push-down for binary columns.;;;","16/Oct/15 20:25;lian cheng;This issue was left unresolved because Parquet filter push-down wasn't enabled by default. But now in 1.5, it's turned on by default. Opened SPARK-11153 to disable filter push-down for strings and binaries.;;;","31/May/16 21:27;ianlcsd;is this one fixed along with SPARK-9876?;;;","31/May/16 22:11;lian cheng;Yea, thanks. I'm closing it.;;;","31/May/16 22:12;lian cheng;Fixed by upgrading parquet-mr to 1.8.1.;;;",,,,,,,,,,,,,,,,,,,,
Set R includes in each file to get right collate order,SPARK-6855,12820252,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,shivaram,shivaram,10/Apr/15 23:47,16/Apr/15 20:06,14/Jul/23 06:27,16/Apr/15 20:06,,,,,,,1.4.0,,,,,,SparkR,,,,0,,,,,,Automated packaging tools like `devtools::document` (which we use to generate docs) can otherwise create invalid collate orders. ,,apachespark,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 20:06:49 UTC 2015,,,,,,,,,,"0|i2d4rr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"10/Apr/15 23:48;apachespark;User 'shivaram' has created a pull request for this issue:
https://github.com/apache/spark/pull/5462;;;","16/Apr/15 20:06;shivaram;Issue resolved by pull request 5462
[https://github.com/apache/spark/pull/5462];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong answers for self joins of converted parquet relations,SPARK-6851,12820190,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,10/Apr/15 20:35,31/Aug/16 21:20,14/Jul/23 06:27,10/Apr/15 23:14,1.3.1,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,correctness,,,,,"From the user list (
/cc [~chinnitv])  When the same relation exists twice in a query plan, our new caching logic replaces both instances with identical replacements.  The bug can be see in the following transformation:

{code}
=== Applying Rule org.apache.spark.sql.hive.HiveMetastoreCatalog$ParquetConversions ===
!Project [state#59,month#60]                                           'Project [state#105,month#106]
! Join Inner, Some(((state#69 = state#59) && (month#70 = month#60)))    'Join Inner, Some(((state#105 = state#105) && (month#106 = month#106)))
!  MetastoreRelation default, orders, None                               Subquery orders
!  Subquery ao                                                            Relation[id#97,category#98,make#99,type#100,price#101,pdate#102,customer#103,city#104,state#105,month#106] org.apache.spark.sql.parquet.ParquetRelation2
!   Distinct                                                             Subquery ao
!    Project [state#69,month#70]                                          Distinct
!     Join Inner, Some((id#81 = id#71))                                    Project [state#105,month#106]
!      MetastoreRelation default, orders, None                              Join Inner, Some((id#115 = id#97))
!      MetastoreRelation default, orderupdates, None                         Subquery orders
!                                                                             Relation[id#97,category#98,make#99,type#100,price#101,pdate#102,customer#103,city#104,state#105,month#106] org.apache.spark.sql.parquet.ParquetRelation2
!                                                                            Subquery orderupdates
!                                                                             Relation[id#115,category#116,make#117,type#118,price#119,pdate#120,customer#121,city#122,state#123,month#124] org.apache.spark.sql.parquet.ParquetRelation2
{code} ",,apachespark,chinnitv,glenn.strycker@gmail.com,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 14 03:17:07 UTC 2015,,,,,,,,,,"0|i2d4ef:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"10/Apr/15 20:48;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5458;;;","10/Apr/15 23:14;marmbrus;Issue resolved by pull request 5458
[https://github.com/apache/spark/pull/5458];;;","14/Jul/15 03:17;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/7387;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
SparkR flaky unit tests when run on Jenkins,SPARK-6850,12820161,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,shivaram,shivaram,10/Apr/15 18:49,10/Apr/15 22:36,14/Jul/23 06:27,10/Apr/15 22:36,,,,,,,1.4.0,,,,,,SparkR,,,,0,,,,,,"From https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/2074/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/console

1. Failure(@test_binaryFile.R#33): saveAsObjectFile()/objectFile() following textFile() works 
collect(rdd) not equal to as.list(mockFile)
Component 1: 1 string mismatch
Component 2: 1 string mismatch

2. Failure(@test_textFile.R#87): textFile() followed by a saveAsTextFile() returns the same content 
collect(rdd) not equal to as.list(mockFile)
Component 1: 1 string mismatch
Component 2: 1 string mismatch
",,apachespark,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 22:36:10 UTC 2015,,,,,,,,,,"0|i2d48f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/15 21:05;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/5460;;;","10/Apr/15 22:36;shivaram;Issue resolved by pull request 5460
[https://github.com/apache/spark/pull/5460];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Stack overflow on updateStateByKey which followed by a dstream with checkpoint set,SPARK-6847,12820006,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,jhu,jhu,10/Apr/15 09:50,01/Feb/16 20:05,14/Jul/23 06:27,01/Feb/16 19:02,1.3.0,1.4.1,1.5.2,1.6.0,,,2.0.0,,,,,,DStreams,,,,1,StackOverflowError,Streaming,,,,"The issue happens with the following sample code: uses {{updateStateByKey}} followed by a {{map}} with checkpoint interval 10 seconds

{code}
    val sparkConf = new SparkConf().setAppName(""test"")
    val streamingContext = new StreamingContext(sparkConf, Seconds(10))
    streamingContext.checkpoint(""""""checkpoint"""""")
    val source = streamingContext.socketTextStream(""localhost"", 9999)
    val updatedResult = source.map(
        (1,_)).updateStateByKey(
            (newlist : Seq[String], oldstate : Option[String]) =>     newlist.headOption.orElse(oldstate))
    updatedResult.map(_._2)
    .checkpoint(Seconds(10))
    .foreachRDD((rdd, t) => {
      println(""Deep: "" + rdd.toDebugString.split(""\n"").length)
      println(t.toString() + "": "" + rdd.collect.length)
    })
    streamingContext.start()
    streamingContext.awaitTermination()
{code}

From the output, we can see that the dependency will be increasing time over time, the {{updateStateByKey}} never get check-pointed,  and finally, the stack overflow will happen. 

Note:
* The rdd in {{updatedResult.map(_._2)}} get check-pointed in this case, but not the {{updateStateByKey}} 
* If remove the {{checkpoint(Seconds(10))}} from the map result ( {{updatedResult.map(_._2)}} ), the stack overflow will not happen",,397090770,apachespark,dgreco,glyton.camilleri,jhu,Yunjie,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 01 20:05:40 UTC 2016,,,,,,,,,,"0|i2d3an:",9223372036854775807,,,,,,,,,,,,,,2.0.0,,,,,,,,,,,,,"10/Apr/15 11:31;srowen;I suspect that's exactly it: {{newlist.headOption.orElse(oldstate)}}

Your state has an object graph that grows without bound. Eventually it will cause a problem when, for example, you serialize the state for a checkpoint and the object graph creates an equally deeply nested method call.

I think you simply should not write your state this way, as a lazy evaluation that has to hold on to old state forever. Return either new or old state immediately in the method. I don't think this is a Spark problem.;;;","12/Apr/15 12:31;srowen;I'm fairly sure this is NotAProblem, for reasons above. If there's a good and different argument otherwise then we could reopen later.;;;","13/Apr/15 03:33;jhu;Hi, [~sowen]

I tested more cases:
# only change the {{newlist.headOption.orElse(oldstate)}} to {{Some(""a"")}}, the issue still exists
# only change the streaming batch interval to {{2 seconds}}, keep the  {{newlist.headOption.orElse(oldstate)}} and checkpoint interval 10 seconds, the issue does not exist. 

So this issue may be related to the checkpoint interval and batch interval. ;;;","13/Apr/15 09:34;srowen;Can you provide (the top part of) the stack overflow stack? so we can see where it's occurring. I think it's something building a very long object graph but that is the first step to confirm.;;;","14/Apr/15 03:44;jhu;Here is the part of the stack (Full stack at: https://gist.github.com/jhu-chang/38a6c052aff1d666b785)
{quote}
15/04/14 11:28:20 [Executor task launch worker-1] ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 27554.0 (TID 3801)
java.lang.StackOverflowError
	at java.io.ObjectStreamClass.setPrimFieldValues(ObjectStreamClass.java:1243)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1984)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:366)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
{quote};;;","14/Apr/15 07:53;srowen;Yeah, doesn't quite help since it is not clear where it starts, but that top may be lost.

Given the observations, the problem may be putting all of the input data into one key, effectively making all RDDs one record, then checkpointing that infrequently, which means it goes to serialize a large object. ""Large"" isn't the problem but whatever it is seems to have a long object dependency graph, maybe a linked list of blocks for example. This would explain why no checkpointing or smaller intervals, could be the difference. How about also turning down the checkpoint interval?

It shouldn't occur ideally but this might be pushing the intended usage a bit far by having as skewed a data distribution as possible. Does this come up in real usage? You'd generally expect the data per key per interval to be smallish.;;;","14/Apr/15 08:46;jhu;Hi, [~sowen]

The checkpoint interval can not be turn down (smaller than 10 seconds) since it must be bigger or equal than the batch interval. I will try to more checkpoint interval like 20 seconds, 30 seconds...

We have a real case that has the same problem, it only updates small set of values per key per interval (one event per key per interval)

One observation is that: the {{updateStateByKey}} is automatically checkpointed;;;","14/Apr/15 09:50;srowen;I think it's worth reopening although I still don't know what the issue is, but it's not obviously a usage problem. Yes I meant turn down the batch interval too but I suppose you've covered that case. I don't recall hearing other issues like this, and what you're describing sounds like it would affect most any use of checkpointing, so that's surprising. Still could be some issue in here but you may have to lead debugging if you're seeking a resolution. I don't have bright ideas from here.;;;","15/Apr/15 09:23;jhu;I did a little more investigation about this issue, that appears to be a problem with some operations({{updateStateByKey}}, {{reduceByKeyAndWindow}} with in-reduce function) which must be check-pointed and followed by a operation with checkpoint (either manual added like the code of this JIRA description or an operation which must be check-pointed) and the checkpoint interval of these two operation is the same (or the followed operation has a checkpoint interval the same with batch interval).
The following code will have this issue: assume default batch interval is 2 seconds, the default checkpoint interval is 10 seconds
# {{source.updateStateByKey(func).map(f).checkpoint(10 seconds)}} 
# {{source.updateStateByKey(func).map(f).updateStateByKey(func2)}}
# {{source.updateStateByKey(func).map(f).checkpoint(2 seconds)}} 

These DO NOT have this issue
# {{source.updateStateByKey(func).map(f).checkpoint(4 seconds)}} 
# {{source.updateStateByKey(func).map(f).updateStateByKey(func2).checkpoint(4 seconds)}}

A rdd graph which contains two rdds needs to be check-pointed would be generated from these sample codes. 

If the child(ren) rdd(s) also need to do the checkpoint at the same time the parent needs to do, then the parent will not do checkpoint according the {{rdd.doCheckpoint}}. In this case, the rdd comes from {{updateStateByKey}} will never be check-pointed at the issued sample code, that leads the stack overflow. ({{updateStateByKey}} needs checkpoint to break the dependency in this operation) 

If the child(ren) rdd(s) is not always check-pointed at the same time of the parent needs to do, there is a chance that the parent rdd (comes from {{updateStateByKey}}) can do some successful checkpoint to break the dependency, although the checkpoint may have some delay. So no stack overflow will happen.

So, currently, we got a workaround of this issue by setting the checkpoint interval to different values if we use operations that must be check-pointed in streaming project. Maybe this is not a easy fix here, hope we can add some validation at least;;;","09/Oct/15 10:00;glyton.camilleri;Hi, 

I've also bumped into this very same issue but couldn't find a good value for {{checkpoint}}; our setup consists of a kafka-stream with 10s time-window, trying various values for the checkpoint interval (default, 10s, and 15s). 

It always takes a long time for the exception to appear, often in the range of 10 hours or so, making the problem relatively painful to debug. We'll be trying to investigate further, but it would be great if someone could shed some more light on the issue.;;;","10/Oct/15 04:30;jhu;Hi [~glyton.camilleri]
You can check whether there are two dstreams in the DAG need to be checkpointed (updateStateByKey, reduceByKeyAndWindow), it yes, you can workaround this to use some output for the previous DStream which needs to checkpointed. 

{code}
val d1 = input.updateStateByKey(func)
val d2 = d1.map(...).updateStateByKey(func)
d2.foreachRDD(rdd => print(rdd.count))
/// workaround the stack over flow listed in this JIRA
d1.foreachRDD(rdd => rdd.foreach(_ => Unit))
{code}
;;;","23/Oct/15 13:47;glyton.camilleri;Hi,
we managed to actually get rid of the overflow issues by settings checkpoints on more streams than we thought we needed to, in addition to implementing a small change following your suggestion; before the fix, the setup was similar to what you describe:

{code}
val dStream1 = // create kafka stream and do some preprocessing
val dStream2 = dStream1.updateStateByKey { func }.checkpoint(timeWindow * 2)
val dStream3 = dStream2.map { ... }

// (1) perform some side-effect on the state
if (certainConditionsAreMet) dStream2.foreachRDD { 
  _.foreachPartition { ... }
}

// (2) publish final results to a set of Kafka topics
dStream3.transform { ... }.foreachRDD {
  _.foreachPartition { ... }
}
{code}

There were two things we did:
a) set different checkpoints for {{dStream2}} and {{dStream3}}, whereas before we were only setting the checkpoint for {{dStream2}}
b) changed (1) above such then when {{!certainConditionsAreMet}}, we just consume the stream like you describe in your suggestion

I honestly think that b) was more likely to be influential in removing the StackOverflowError really, but we decided to leave the checkpoint settings in a) there anyway.
Apologies for the late follow-up, but we needed to make sure the issue had actually been resolved.;;;","17/Nov/15 16:10;Yunjie;Hi Glyton Camilleri & Jack Hu,
  We also ran into the same StackOverflow issue in our application, where we wrote like
{code}
val dStream1 = context.union(kafkaStreams).updateStateByKey(updateFunc).checkpoint(Seconds(50))
{code}
  I've read about your comments, does it mean that I can get rid of this issue by simply add an extra meaningless map() step to dStream1? Or I should do something like
{code}
val workaroundStream = dStream1.map(...).checkpoint(Seconds(some_value_other_than_50))
{code}
  I was confused by what certainConditionsAreMet refers to, or what kind of the content should be filled in {code}_.foreachPartition { ... }{code} so that I had to ask here for detail.

Best Regards.;;;","17/Nov/15 16:47;glyton.camilleri;Hi Yunjie,

Whether 50 seconds is good or not as a checkpoint interval depends largely on the time-window the stream is acting on; so if the stream is set to execute jobs every 10 seconds, then 50 seconds could be fine.

In my example code, {{certainConditionsAreMet}} was just a place-holder: the conditions met were application-specific in that case; so in other words, there were conditions under which we would perform the side-effect on the stream, which in our case ((1) above) was saving the contents of the stream to HDFS. So the fix looked something like this:

{code}
  def isTimeToSave: Boolean = ... // this function decides whether it's time to store the contents of the stream to HDFS

  def saveData[A](stream: DStream[A]) = if (isTimeToSave) stream.foreachRDD { 
    ... // write data in HDFS
  } else stream.foreachRDD { 
    _.foreachPartition { _ => () } // just do nothing 
  }   
{code}

The {{else}} part is what i'm referring to above.;;;","21/Jan/16 03:02;zsxwing;I think this one has been fixed by https://github.com/apache/spark/pull/10623

Could you try the latest codes of master or 1.6?;;;","22/Jan/16 03:22;jhu;Hi [~zsxwing]

I just test a simple case with 1.6, it still exists:
{code}
batch interval = 2 seconds
source.updateStateByKey(func).map(f).checkpoint(2 seconds)
{code};;;","23/Jan/16 02:32;zsxwing;It has not yet been released. You need to use the master branch or 1.6 branch to test it.;;;","25/Jan/16 04:13;jhu;Test on latest 1.6 branch (f913f7e [SPARK-12120][PYSPARK] Improve exception message when failing to init), it still exists.;;;","25/Jan/16 21:04;zsxwing;I found the issue. The problem is in the following line:

{code}
    updatedResult.map(_._2)
      .checkpoint(Seconds(10))
{code}

Because `checkpoint` is called after `map`, only MapPartitionsRDD will do checkpoint and the state RDD in updateStateByKey will be skipped. When an RDD executes checkpointing, its dependencies (parent RDDs) will be skipped. You can just switch `map` and `checkpoint` like this:

{code}
    updatedResult.checkpoint(Seconds(10)).map(_._2)
{code}
;;;","25/Jan/16 21:06;zsxwing;The user should not checkpoint DStream/RDDs after `updateStateByKey`. Otherwise, the state RDD of `updateStateByKey` cannot be checkpointed.;;;","26/Jan/16 02:43;jhu;Hi [~zsxwing]

Even user does not implicit do checkpoint after the {{upstateByKey}}, this issue still will happen in following cases
# {{updateStateByKey().filter().updateStateByKey()}}
# {{updateStateByKey().filter().reduceByKeyAndWindow(reduce, inreduce, ...)}}
# {{reduceByKeyAndWindow(reduce,inreduce,...).filter().udateStateByKey()}}

If do not plan to fix this issue, may be an implicit workaround/warning should give to user to such usage. 
It will be very hard to find the real cause if the application is complicate. 
;;;","26/Jan/16 20:00;zsxwing;Looks several operators have this issue, such as, updateStateByKey, mapWithState, reduceByKeyAndWindow. Let's try to find out a solution.;;;","27/Jan/16 00:25;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/10934;;;","01/Feb/16 20:05;zsxwing;As my PR changed internal semantics, it's only merged to master branch (2.0.0). 

For pre 2.0.0, you may need to trigger the checkpoint by yourself. E.g., for {{updateStateByKey().filter().updateStateByKey()}}, you can update to {{dstream.updateStateByKey().count(); dstream.updateStateByKey().filter().updateStateByKey()}} to trigger the checkpoint for the first ""updateStateByKey"".;;;",,,,,
Memory leak occurs when register temp table with cache table on,SPARK-6844,12819991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,jhu,jhu,10/Apr/15 09:28,24/Apr/15 00:16,14/Jul/23 06:27,15/Apr/15 20:16,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,Memory,SQL,,,,"There is a memory leak in register temp table with cache on

This is the simple code to reproduce this issue:
{code}
    val sparkConf = new SparkConf().setAppName(""LeakTest"")
    val sparkContext = new SparkContext(sparkConf)
    val sqlContext = new SQLContext(sparkContext)
    val tableName = ""tmp""
    val jsonrdd = sparkContext.textFile(""""""sample.json"""""")
    var loopCount = 1L
    while(true) {
      sqlContext.jsonRDD(jsonrdd).registerTempTable(tableName)
      sqlContext.cacheTable(tableName)
      println(""L: "" +loopCount + "" R:"" + sqlContext.sql(""""""select count(*) from tmp"""""").count())
      sqlContext.uncacheTable(tableName)
      loopCount += 1
    }
{code}

The cause is that the {{InMemoryRelation}}. {{InMemoryColumnarTableScan}} uses the accumulator ({{InMemoryRelation.batchStats}},{{InMemoryColumnarTableScan.readPartitions}}, {{InMemoryColumnarTableScan.readBatches}} ) to get some information from partitions or for test. These accumulators will register itself into a static map in {{Accumulators.originals}} and never get cleaned up.
",,apachespark,Haopu Wang,jhu,marmbrus,saucam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 23 07:32:08 UTC 2015,,,,,,,,,,"0|i2d37b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/15 19:32;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5475;;;","15/Apr/15 20:16;marmbrus;Issue resolved by pull request 5475
[https://github.com/apache/spark/pull/5475];;;","16/Apr/15 06:38;jhu;Hi, [~marmbrus]

Do we have a plan to port this to 1.3.X branch? 
;;;","16/Apr/15 19:52;marmbrus;I was not planning to.  I do not think that it is a regression from 1.2 and it is a little risky to backport changes to the way we initialize cached relations.;;;","23/Apr/15 07:32;jhu;Hi, [~marmbrus]]
I mean 1.3.X, like 1.3.2. The master seems not much different with branch 1.3 (May be i am wrong);;;",,,,,,,,,,,,,,,,,,,,,,,,
"Potential visibility problem for the ""state"" of Executor",SPARK-6843,12819971,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,zhichao-li,zhichao-li,zhichao-li,10/Apr/15 07:47,12/Apr/15 12:42,14/Jul/23 06:27,12/Apr/15 12:41,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,,,apachespark,zhichao-li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 12 12:41:56 UTC 2015,,,,,,,,,,"0|i2d32v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/15 07:51;apachespark;User 'zhichao-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/5448;;;","10/Apr/15 11:56;srowen;[~zhichao-li] This isn't useful as a JIRA since it does not describe the problem, and that's what the JIRA is for. This doesn't have value otherwise. You put some description in the PR, but PRs describe the implementation of a solution, not the problem.;;;","12/Apr/15 12:41;srowen;Issue resolved by pull request 5448
[https://github.com/apache/spark/pull/5448];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
filter() on DataFrame does not work as expected,SPARK-6812,12819800,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sunrui,davies,davies,09/Apr/15 19:30,08/May/15 10:03,14/Jul/23 06:27,07/May/15 05:50,,,,,,,1.4.0,,,,,,SparkR,,,,0,,,,,,"{code}
> filter(df, df$age > 21)
Error in filter(df, df$age > 21) :
  no method for coercing this S4 class to a vector
{code}",,apachespark,davies,shivaram,sunrui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7482,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 07 22:26:12 UTC 2015,,,,,,,,,,"0|i2d1yv:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"09/Apr/15 19:50;shivaram;Hmm - don't we have a unit test for this ? I'm wondering if this is because of the generics not resolving correctly.;;;","05/May/15 14:12;sunrui;I think this is due to function conflict with existing R filter() function. 
If you enforce to call filter() inside SparkR scope, that works.
SparkR:::filter(df, ""age > 20"")

Originally, in RDD API, we rename RDD filter() to filterRDD() and Filter() . But in DataFrame, filter() name is still used. So we can rename filter() to filterDF() and Filter() as RDD API does.;;;","06/May/15 04:02;shivaram;Thanks [~sunrui] for investigating - It looks like `dplyr` successfully overrides filter in their API. Could you try to see if we can do something similar ? https://github.com/hadley/dplyr/blob/master/NAMESPACE#L104;;;","06/May/15 05:07;sunrui;Interestingly, we have a unit test case for filter() and the test passes. In R, if multiple packages have a same name, the name in the package loaded lastly overwrites that in the packages loaded before. 
If you use bin/sparkR to start a SparkR shell, the environment list is as follows:
 [1] "".GlobalEnv""        ""package:stats""     ""package:graphics""
 [4] ""package:grDevices"" ""package:datasets""  ""package:SparkR""
 [7] ""package:utils""     ""package:methods""   ""Autoloads""
[10] ""package:base""

You can see that ""package:stats"" is before ""package:SparkR"", so its filter() function overwrites the one in SparkR.

While in the test procedure, the environment list is different:
.GlobalEnv package:plyr package:SparkR package:testthat package:methods package:stats package:graphics package:grDevices package:utils package:datasets Autoloads package:base

You can see that package:SparkR is before package:stats. That why filter() in SparkR passes the test.

Don't know why the package loading order is different now.;;;","06/May/15 05:15;shivaram;Ah I see - We ran into a similar issue with `head` before and the workaround was to include utils before SparkR -- See R/pkg/inst/profile/shell.R 
We could do a similar fix for stats;;;","06/May/15 07:49;sunrui;According to the R manual: https://stat.ethz.ch/R-manual/R-devel/library/base/html/Startup.html,
"" if a function .First is found on the search path, it is executed as .First(). Finally, function .First.sys() in the base package is run. This calls require to attach the default packages specified by options(""defaultPackages"").""

In .First() in profile/shell.R, we load SparkR package. This means SparkR package is loaded before default packages. If there are same names in default packages, they will overwrite those in SparkR. This is why filter() in SparkR is masked by filter() in stats, which is usually in the default package list.

We need to make sure SparkR is loaded after default packages. The solution is to append SparkR to default packages, instead of loading SparkR in .First().
;;;","06/May/15 09:49;apachespark;User 'sun-rui' has created a pull request for this issue:
https://github.com/apache/spark/pull/5938;;;","06/May/15 16:55;shivaram;Regarding the name conflicts occurring between the Scala API, other R packages etc. there are a couple of points

- IMHO we don't need to mimic the Scala API always as SparkR is more geared towards R users than towards users who are switching to it from Scala. So we should still look to override common R functions for DataFrames or Machine learning so that we can maintain familiarity for R-users
- Also I agree with you that naming conflicts exist even when you load three or four R related packages (without SparkR), so I think the best we can do is to load SparkR at the top of default packages list as you mention.

Regarding sortDF, I was actually planning to name it `arrange` after the dply call as part of https://issues.apache.org/jira/browse/SPARK-7231 -- Let me know if this sounds good to you.;;;","07/May/15 05:49;shivaram;Fixed by https://github.com/apache/spark/pull/5938;;;","07/May/15 05:59;sunrui;[~shivaram], Yes I agree. Seems there are still two methods, sampleDF() and saveDF(), we can change them back to sample() and save()?
;;;","07/May/15 22:26;shivaram;Yeah lets change them to sample(), load() and save() -- and we can remove showDF as I described in https://issues.apache.org/jira/browse/SPARK-7435;;;",,,,,,,,,,,,,,,,,,
"Reading from JDBC with SQLContext, using lower/upper bounds and numPartitions gives incorrect results.",SPARK-6800,12819637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,capitao,capitao,09/Apr/15 11:22,24/Apr/15 00:27,14/Jul/23 06:27,15/Apr/15 20:03,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"Having a Derby table with people info (id, name, age) defined like this:

{code}
val jdbcUrl = ""jdbc:derby:memory:PeopleDB;create=true""
val conn = DriverManager.getConnection(jdbcUrl)
val stmt = conn.createStatement()
stmt.execute(""CREATE TABLE Person (person_id INT NOT NULL GENERATED ALWAYS AS IDENTITY CONSTRAINT person_pk PRIMARY KEY, name VARCHAR(50), age INT)"")
stmt.execute(""INSERT INTO Person(name, age) VALUES('Armando Carvalho', 50)"")
stmt.execute(""INSERT INTO Person(name, age) VALUES('Lurdes Pereira', 23)"")
stmt.execute(""INSERT INTO Person(name, age) VALUES('Ana Rita Costa', 12)"")
stmt.execute(""INSERT INTO Person(name, age) VALUES('Armando Pereira', 32)"")
stmt.execute(""INSERT INTO Person(name, age) VALUES('Miguel Costa', 15)"")
stmt.execute(""INSERT INTO Person(name, age) VALUES('Anabela Sintra', 13)"")
{code}

If I try to read that table from Spark SQL with lower/upper bounds, like this:

{code}
val people = sqlContext.jdbc(url = jdbcUrl, table = ""Person"",
      columnName = ""age"", lowerBound = 0, upperBound = 40, numPartitions = 10)
people.show()
{code}

I get this result:
{noformat}
PERSON_ID NAME             AGE
3         Ana Rita Costa   12 
5         Miguel Costa     15 
6         Anabela Sintra   13 
2         Lurdes Pereira   23 
4         Armando Pereira  32 
1         Armando Carvalho 50 
{noformat}

Which is wrong, considering the defined upper bound has been ignored (I get a person with age 50!).
Digging the code, I've found that in {{JDBCRelation.columnPartition}} the WHERE clauses it generates are the following:
{code}
(0) age < 4,0
(1) age >= 4  AND age < 8,1
(2) age >= 8  AND age < 12,2
(3) age >= 12 AND age < 16,3
(4) age >= 16 AND age < 20,4
(5) age >= 20 AND age < 24,5
(6) age >= 24 AND age < 28,6
(7) age >= 28 AND age < 32,7
(8) age >= 32 AND age < 36,8
(9) age >= 36,9
{code}

The last condition ignores the upper bound and the other ones may result in repeated rows being read.

Using the JdbcRDD (and converting it to a DataFrame) I would have something like this:
{code}
val jdbcRdd = new JdbcRDD(sc, () => DriverManager.getConnection(jdbcUrl),
      ""SELECT * FROM Person WHERE age >= ? and age <= ?"", 0, 40, 10,
      rs => (rs.getInt(1), rs.getString(2), rs.getInt(3)))
val people = jdbcRdd.toDF(""PERSON_ID"", ""NAME"", ""AGE"")
people.show()
{code}

Resulting in:
{noformat}
PERSON_ID NAME            AGE
3         Ana Rita Costa  12 
5         Miguel Costa    15 
6         Anabela Sintra  13 
2         Lurdes Pereira  23 
4         Armando Pereira 32 
{noformat}

Which is correct!

Confirming the WHERE clauses generated by the JdbcRDD in the {{getPartitions}} I've found it generates the following:
{code}
(0) age >= 0  AND age <= 3
(1) age >= 4  AND age <= 7
(2) age >= 8  AND age <= 11
(3) age >= 12 AND age <= 15
(4) age >= 16 AND age <= 19
(5) age >= 20 AND age <= 23
(6) age >= 24 AND age <= 27
(7) age >= 28 AND age <= 31
(8) age >= 32 AND age <= 35
(9) age >= 36 AND age <= 40
{code}

This is the behaviour I was expecting from the Spark SQL version. Is the Spark SQL version buggy or is this some weird expected behaviour?","Windows 8.1, Apache Derby DB, Spark 1.3.0 CDH5.4.0, Scala 2.10",apachespark,capitao,marmbrus,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 20:03:12 UTC 2015,,,,,,,,,,"0|i2d0z3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/15 10:26;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5488;;;","13/Apr/15 12:53;capitao;The above pull request seem to only fix the upper and lower bounds issue. There is still the intermediate queries issue that may result in repeated rows being fetched from a DB.;;;","15/Apr/15 02:32;viirya;Why the other ones will result in repeated rows being read?;;;","15/Apr/15 02:44;viirya;And according to the explanation from Michael Armbrust, {{lowerBound}} and {{upperBound}} are just used to decide partition stride, not for filtering. So all table rows are partitioned. So this is not a bug. But of course the document needs to be updated to clearly state this.
;;;","15/Apr/15 09:29;viirya;The ranges for partitions 1 to 8 are not overlapped. They are:
(1) 8 > age >= 4
(2) 12 > age >= 8
(3) 16 > age >= 12
(4) 20 > age >= 16
(5) 24 > age >= 20
(6) 28 > age >= 24
(7) 32 > age >= 28
(8) 36 > age >= 32;;;","15/Apr/15 09:32;capitao;My fault. I'm sorry. I have looked to the wrong place to retrieve the where clauses.
""age >= 8  AND age < 12,2"" is where=""age >= 8  AND age < 12"" and partition=2

Fixing the lower/upper bounds issue is all is needed.;;;","15/Apr/15 09:34;viirya;About the upper and lower bounds issue, please refer to the pr page, Michael Armbrust gives the explanation why it is not a bug. Just the document needs to be updated for this issue.

Thanks.;;;","15/Apr/15 09:43;capitao;Could you please point me to that page?
I'm bit confused because not considering the upper/lower bounds would cause the fetch of the whole table and cause the issue I've pointed here in which a person with an age out of bound is retrieved.
Thanks.;;;","15/Apr/15 09:45;viirya;PR: https://github.com/apache/spark/pull/5488;;;","15/Apr/15 20:03;marmbrus;Issue resolved by pull request 5488
[https://github.com/apache/spark/pull/5488];;;",,,,,,,,,,,,,,,,,,,
Avoid reading Parquet footers on driver side when an global arbitrative schema is available,SPARK-6795,12819551,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,lian cheng,lian cheng,09/Apr/15 03:13,13/Aug/15 08:40,14/Jul/23 06:27,06/Aug/15 14:47,1.0.2,1.1.1,1.2.1,1.3.1,,,1.5.0,,,,,,SQL,,,,0,,,,,,"With the help of [Parquet MR PR #91|https://github.com/apache/incubator-parquet-mr/pull/91] which will be included in the official release of Parquet MR 1.6.0, now it's possible to avoid reading footers on the driver side completely when an global arbitrative schema is available.

Currently, the global schema can be either Hive metastore schema or specified via data sources DDL. All tasks should verify Parquet data files and reconcile possible schema conflicts locally against this global schema.

However, when no global schema is available and schema merging is enabled, we still need to read schemas from all data files to infer a valid global schema.",,copris,LabOctoCat,lian cheng,saucam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7340,,,SPARK-8125,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 13 08:40:49 UTC 2015,,,,,,,,,,"0|i2d0fz:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"06/Aug/15 14:47;lian cheng;This has been fixed together with SPARK-8125.;;;","12/Aug/15 15:23;LabOctoCat;This doesn't seem to be fixed.  I built the latest branch-1.4 and removed our custom fix for this, and we still experience this issue. On a query to a table with a lot of files, the driver hang for a while will it's reading partitions. In the UI, if you check the timeline it's pretty clear, with our branch there is almost no empty space, with branch-1.4, there is a  50s void in our worst case.

The assumed culprit =>

1. readAllFootersInParallelUsingSummaryFiles, will default reading all footers, if no summary file is available. So most of the times we probably read all footers even if schema merging is off.

https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/parquet/newParquet.scala#L357

2. Why do we read schema if there is metastore schema available?

Shouldn't it be 

          val dataSchema0 = maybeDataSchema
            .orElse(maybeMetastoreSchema)
            .orElse(readSchema())

?

https://github.com/apache/spark/blob/branch-1.4/sql/core/src/main/scala/org/apache/spark/sql/parquet/newParquet.scala#L370;;;","12/Aug/15 15:59;LabOctoCat;My bad I didn't see that this wasn't merged => https://github.com/apache/spark/pull/7664

Seeing it closed, I assumed it was merged. But I really think it should be merged.;;;","13/Aug/15 08:40;lian cheng;As explained on GitHub, usually we only backport fixes of severe bugs to maintaining branches, while [PR #7396|https://github.com/apache/spark/pull/7396] isn't a ""bug fix"" and it's quite a big change. Generally we don't want to introduce any regression when we publish a maintenance release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
DateUtils can not handle date before 1970/01/01 correctly,SPARK-6785,12819481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ckadner,davies,davies,08/Apr/15 22:33,29/Jul/16 03:01,14/Jul/23 06:27,30/Jun/15 19:23,,,,,,,1.5.0,,,,,,SQL,,,,2,,,,,,"{code}
scala> val d = new Date(100)
d: java.sql.Date = 1969-12-31

scala> DateUtils.toJavaDate(DateUtils.fromJavaDate(d))
res1: java.sql.Date = 1970-01-01

{code}",,apachespark,ckadner,davies,marmbrus,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16788,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 30 19:23:11 UTC 2015,,,,,,,,,,"0|i2d00v:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.5.0,,,,,,,,,,,,,"13/May/15 20:54;ckadner;Hi Patrick,
I would like to work on this issue. Seems like the date conversion is thrown off by the time-zone adjustments and the fact that the interchange type is days instead of millis. I am preparing a pull-request which will also include test cases to cover more date conversion scenarios.

;;;","18/May/15 19:18;apachespark;User 'ckadner' has created a pull request for this issue:
https://github.com/apache/spark/pull/6236;;;","18/May/15 20:14;apachespark;User 'ckadner' has created a pull request for this issue:
https://github.com/apache/spark/pull/6242;;;","18/May/15 22:15;ckadner;{panel:borderStyle=dashed|borderColor=#ccc|bgColor=#FFFFCE}
Pull Request +[6242|https://github.com/apache/spark/pull/6242]+
{panel}
\\
Before my fix, the from-and-to Java date conversion of dates before 1970 will only work for {{java.sql.Date}} objects that reflect a date and time exactly at midnight in the System's local time zone. 
Otherwise, if the Date's time is just one millisecond before or after midnight, the result of the above conversion will be offset by one day for Dates before 1970 because of a rounding (truncation) flaw in the function {{DateUtils.millisToDays(Long):Int}}

\\

{code}
  scala> val df = new SimpleDateFormat(""yyyy-MM-dd HH:mm:ss"")
  df: java.text.SimpleDateFormat = yyyy-MM-dd HH:mm:ss

  scala> val d1 = new Date(df.parse(""1969-01-01 00:00:00"").getTime)
  d2: java.sql.Date = 1969-01-01
	
  scala> val d2 = new Date(df.parse(""1969-01-01 00:00:01"").getTime)
  d2: java.sql.Date = 1969-01-01

  scala> DateUtils.toJavaDate(DateUtils.fromJavaDate(d1))
  res1: java.sql.Date = 1969-01-01
	
  scala> DateUtils.toJavaDate(DateUtils.fromJavaDate(d2))
  res2: java.sql.Date = 1969-01-02
{code}

\\

What is the code doing and how to fix it:

\\

 - A {{java.util.Date}} is represented by milliseconds ({{Long}}) since the Epoch (1970/01/01 0:00:00 GMT) with positive numbers for dates after and negative numbers for dates before 1970
 
 - The function {{DateUtils.fromJavaDate(java.util.Date):Int}} calculates the number of full days passed since 1970/01/01 00:00:00 (local time, not UTC), but by using the data type {{Long}} (as opposed to {{Double}}) when  converting milliseconds to days it essentially truncates the fractional part of days passed (disregarding the impact of hours, minutes, seconds)
 
 - The function {{DateUtils.toJavaDate(Int):Date}} converts the given number of days into milliseconds and adds it 1970/01/01 00:00:00 (local time, not UTC)

 - _Side note: The time-zone offset from UTC is factored in when converting a Date to days and removed when converting days to Date, so the time-zone shifting is neutralized in the round-trip conversion {{toJavaDate(fromJavaDate(java.util.Date))}}._
 
 - The truncation of partial days is not a problem for dates after 1970 since adding a fraction of a day to any date will not flip the calendar to the next day (since all our Dates start 0:00:00 AM)
 
 - That truncation of partial days however is a problem when subtracting even a second from a {{Date}} with time at 0:00:00 AM which should turn the calender back one day to the previous date
 
 - Ideally the date conversion should be done using milliseconds, but since using days has been established already, the fix is to work with {{Double}} to preserve fractions of days and use {{floor()}} instead of the implicit truncate to round to a full number of days ({{Int}})

\\

Pseudo-code example, adding or subtracting 1 hour to Date ""1970/01/01 0:00:00"" using milliseconds...

{code}
""1970-01-01 0:00:00"" + 1 hr = ""1970-01-01  1:00:00""
""1970-01-01 0:00:00"" - 1 hr = ""1969-12-31 23:00:00""
{code}

\\

Same example, using full days. One hour is about 0.04 days. Using {{trunc()}} versus {{floor()}} we get ...  

{code}
trunc(+0.04) = +0  -->  ""1970-01-01"" + 0 days = ""1970-01-01""    (correct)
floor(+0.04) = +0  -->  ""1970-01-01"" + 0 days = ""1970-01-01""    (correct)

trunc(-0.04) = -0  -->  ""1970-01-01"" + -0 days = ""1970-01-01""   (incorrect, bug)
floor(-0.04) = -1  -->  ""1970-01-01"" + -1 day  = ""1969-12-31""   (correct, fix)
{code}

{code} 
def trunc(d: Dounble): Int = d.toInt
{code};;;","24/Jun/15 11:38;apachespark;User 'ckadner' has created a pull request for this issue:
https://github.com/apache/spark/pull/6983;;;","30/Jun/15 19:23;marmbrus;Issue resolved by pull request 6983
[https://github.com/apache/spark/pull/6983];;;",,,,,,,,,,,,,,,,,,,,,,,
Implement Parquet complex types backwards-compatiblity rules,SPARK-6774,12819296,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,lian cheng,lian cheng,08/Apr/15 12:32,08/Oct/15 23:20,14/Jul/23 06:27,08/Oct/15 23:20,1.0.0,1.1.1,1.2.1,1.3.0,,,,,,,,,SQL,,,,0,,,,,,"[Parquet format PR #17|https://github.com/apache/incubator-parquet-format/pull/17] standardized representation of Parquet complex types and listed backwards-compatibility rules. Spark SQL should implement these compatibility rules to improve interoperatability.

Before, Spark SQL is only compatible with parquet-avro, parquet-hive, and Impala. And it's done in an error prone ad-hoc way, because Parquet format spec didn't explicitly specify complex type structures at the time Spark SQL Parquet support was firstly authored. After fixing this issue, we are expected to be compatible with most (if not all) systems that generated Parquet data in a systematic way by conforming to Parquet format spec and implementing all backwards-compatibility rules.",,glenn.strycker@gmail.com,huangjs,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5508,SPARK-8811,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Oct 08 23:20:45 UTC 2015,,,,,,,,,,"0|i2cywv:",9223372036854775807,,,,,lian cheng,,,,,,,,,1.6.0,,,,,,,,,,,,,"08/Oct/15 23:20;lian cheng;Finally, fixed all the Parquet compatibility issues after 6 months!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
check -license will passed in next time when rat jar download failed.,SPARK-6773,12819294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,June.He,June.He,June.He,08/Apr/15 11:51,10/Apr/15 19:03,14/Jul/23 06:27,10/Apr/15 19:02,1.3.0,,,,,,1.4.0,,,,,,Build,,,10/Apr/15 00:00,0,,,,,,"In dev/check-license, it will download Rat jar if it not exist. if download failed, it will report error:
**************************
Attempting to fetch rat
Our attempt to download rat locally to /home/spark/hejun/sparkgit/spark/lib/apache-rat-0.10.jar failed. Please install rat manually.
*****************************
but if run it again in next cycle, it will check RAT passed and go on building also an error reported:
**************************
Error: Invalid or corrupt jarfile /home/spark/hejun/sparkgit/spark/lib/apache-rat-0.10.jar
RAT checks passed.
*****************************

This is because:
1. The error tmp rat.jar is not removed when rat jar download failed in last time. So it will go on checking license using the error rat.jar
2. The rat-results.txt is empty because rat.jar run failed, so RAT check passed.


Suggest:
1. Add a clean step when rat.jar download faild.
2. Add a error checking logic after run rat checking.

",,apachespark,June.He,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 19:02:48 UTC 2015,,,,,,,,,,"0|i2cywf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/15 11:57;apachespark;User 'sisihj' has created a pull request for this issue:
https://github.com/apache/spark/pull/5421;;;","10/Apr/15 19:02;srowen;Issue resolved by pull request 5421
[https://github.com/apache/spark/pull/5421];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Usage of the ListenerBus in YarnClusterSuite is wrong,SPARK-6769,12819236,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,08/Apr/15 08:58,14/Apr/15 21:02,14/Jul/23 06:27,14/Apr/15 21:02,1.4.0,,,,,,1.4.0,,,,,,Tests,YARN,,,0,,,,,,"In YarnClusterSuite, a test case uses `SaveExecutorInfo`  to handle ExecutorAddedEvent as follows.

{code}
private class SaveExecutorInfo extends SparkListener {
  val addedExecutorInfos = mutable.Map[String, ExecutorInfo]()

  override def onExecutorAdded(executor: SparkListenerExecutorAdded) {
    addedExecutorInfos(executor.executorId) = executor.executorInfo
  }
}

...

    listener = new SaveExecutorInfo
    val sc = new SparkContext(new SparkConf()
      .setAppName(""yarn \""test app\"" 'with quotes' and \\back\\slashes and $dollarSigns""))
    sc.addSparkListener(listener)
    val status = new File(args(0))
    var result = ""failure""
    try {
      val data = sc.parallelize(1 to 4, 4).collect().toSet
      assert(sc.listenerBus.waitUntilEmpty(WAIT_TIMEOUT_MILLIS))
      data should be (Set(1, 2, 3, 4))
      result = ""success""
    } finally {
      sc.stop()
      Files.write(result, status, UTF_8)
    }
{code}

But, the usage is wrong because Executors will spawn during initializing SparkContext and SparkContext#addSparkListener should be invoked after the initialization, thus after Executors spawn, so SaveExecutorInfo cannot handle ExecutorAddedEvent.

Following code refers the result of the handling ExecutorAddedEvent. Because of the reason above, we cannot reach the assertion. 

{code}
    // verify log urls are present
    listener.addedExecutorInfos.values.foreach { info =>
      assert(info.logUrlMap.nonEmpty)
    }
{code}",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 09:00:35 UTC 2015,,,,,,,,,,"0|i2cyjr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"08/Apr/15 09:00;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/5417;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Do not support ""float/double union decimal or decimal(a ,b) union decimal(c, d)""",SPARK-6768,12819235,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,DoingDone9,DoingDone9,08/Apr/15 08:57,12/May/15 03:22,14/Jul/23 06:27,12/May/15 03:22,,,,,,,,,,,,,SQL,,,,0,,,,,,"Do not support sql like that :

select cast(12.2056999 as float) from testData limit 1
union
select cast(12.2041 as decimal(7, 4)) from testData limit 1

select cast(12.2056999 as double) from testData limit 1
union
select cast(12.2041 as decimal(7, 4)) from testData limit 1

select cast(1241.20 as decimal(6, 2)) from testData limit 1
union
select cast(1.204 as decimal(5, 3)) from testData limit 1

",,apachespark,DoingDone9,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 09:03:37 UTC 2015,,,,,,,,,,"0|i2cyjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/15 09:03;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/5418;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation error in Spark SQL Readme file,SPARK-6767,12819231,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,tijoparacka,Tijo Paracka,Tijo Paracka,08/Apr/15 08:43,09/Apr/15 10:31,14/Jul/23 06:27,09/Apr/15 10:31,1.3.0,,,,,,1.3.2,1.4.0,,,,,Documentation,SQL,,,0,,,,,,"Error in Spark SQL Documentation file . The sample script for SQL DSL   throwing below error

scala> query.where('key > 30).select(avg('key)).collect()
<console>:43: error: value > is not a member of Symbol
              query.where('key > 30).select(avg('key)).collect()
",,apachespark,Tijo Paracka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 04:00:34 UTC 2015,,,,,,,,,,"0|i2cyin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/15 08:44;apachespark;User 'tijoparacka' has created a pull request for this issue:
https://github.com/apache/spark/pull/5415;;;","09/Apr/15 04:00;Tijo Paracka;Could you please change the status of this issue and assign this to me ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamingListenerBatchSubmitted isn't sent and StreamingListenerBatchStarted.batchInfo.processingStartTime is a wrong value,SPARK-6766,12819217,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,08/Apr/15 07:35,14/Apr/15 23:01,14/Jul/23 06:27,14/Apr/15 23:01,1.0.2,1.1.1,1.2.1,1.3.0,,,1.3.2,1.4.0,,,,,DStreams,,,,0,,,,,,"1. Now there is no place posting StreamingListenerBatchSubmitted. It should be post when JobSet is submitted.
2. Call JobSet.handleJobStart before posting StreamingListenerBatchStarted will set StreamingListenerBatchStarted.batchInfo.processingStartTime to None, which should have been set to a correct value.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6702,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 09:25:57 UTC 2015,,,,,,,,,,"0|i2cyfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/15 07:43;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5414;;;","10/Apr/15 09:25;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5452;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix potential resource leaks in CheckPoint CheckpointWriter and CheckpointReader,SPARK-6762,12819179,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhichao-li,zhichao-li,zhichao-li,08/Apr/15 04:32,13/Apr/15 11:18,14/Jul/23 06:27,13/Apr/15 11:18,,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,The close action should be placed within finally block to avoid the potential resource leaks,,apachespark,zhichao-li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 13 11:18:17 UTC 2015,,,,,,,,,,"0|i2cy7b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/15 07:18;apachespark;User 'nareshpr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5413;;;","08/Apr/15 07:46;apachespark;User 'zhichao-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/5407;;;","13/Apr/15 11:18;srowen;Issue resolved by pull request 5407
[https://github.com/apache/spark/pull/5407];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test for SPARK-3426 (in ShuffleSuite) doesn't correctly clone the SparkConf,SPARK-6753,12819064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,kayousterhout,kayousterhout,07/Apr/15 20:41,08/Apr/15 17:29,14/Jul/23 06:27,08/Apr/15 17:29,1.1.1,1.2.0,1.3.0,,,,1.1.2,1.2.3,1.3.2,1.4.0,,,Tests,,,,0,,,,,,"As a result, that test always uses the default shuffle settings, rather than using the shuffle manager / other settings set by tests that extend ShuffleSuite.",,apachespark,joshrosen,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 17:29:08 UTC 2015,,,,,,,,,,"0|i2cxk7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/15 20:49;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/5401;;;","08/Apr/15 17:29;joshrosen;Fixed by https://github.com/apache/spark/pull/5401;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
QueryPlan.schema should be a lazy val to avoid creating excessive duplicate StructType objects,SPARK-6748,12818997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,07/Apr/15 17:39,24/Apr/15 00:28,14/Jul/23 06:27,07/Apr/15 23:15,1.3.0,,,,,,1.4.0,,,,,,,,,,0,,,,,,"Spotted this issue while trying to do a simple micro benchmark:
{code}
sc.parallelize(1 to 10000000).
  map(i => (i, s""val_$i"")).
  toDF(""key"", ""value"").
  saveAsParquetFile(""file:///tmp/src.parquet"")

sqlContext.parquetFile(""file:///tmp/src.parquet"").collect()
{code}
YJP profiling result showed that, *10 million {{StructType}}, 10 million {{StructField \[\]}}, and 20 million {{StructField}} were allocated*.

It turned out that {{DataFrame.collect()}} calls {{SparkPlan.executeCollect()}}, which consists of a single line:
{code}
execute().map(ScalaReflection.convertRowToScala(_, schema)).collect()
{code}
The problem is that, {{QueryPlan.schema}} is a function, and since 1.3.0, {{convertRowToScala}} starts returning a {{GenericRowWithSchema}}. These two facts result in 10 million rows, each with a separate schema object.",,apachespark,glenn.strycker@gmail.com,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 23:15:08 UTC 2015,,,,,,,,,,"0|i2cx4v:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"07/Apr/15 18:01;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5398;;;","07/Apr/15 23:15;lian cheng;Issue resolved by pull request 5398
[https://github.com/apache/spark/pull/5398];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw an AnalysisException when unsupported Java list types used in Hive UDF,SPARK-6747,12818987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maropu,maropu,maropu,07/Apr/15 16:58,07/Jul/15 02:45,14/Jul/23 06:27,07/Jul/15 02:45,1.4.0,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"The current implementation can't handle List<> as a return type in Hive UDF and
throws meaningless Match Error.
We assume an UDF below;

public class UDFToListString extends UDF {
    public List<String> evaluate(Object o) {
        return Arrays.asList(""xxx"", ""yyy"", ""zzz"");
    }
}

An exception of scala.MatchError is thrown as follows when the UDF used;

scala.MatchError: interface java.util.List (of class java.lang.Class)
	at org.apache.spark.sql.hive.HiveInspectors$class.javaClassToDataType(HiveInspectors.scala:174)
	at org.apache.spark.sql.hive.HiveSimpleUdf.javaClassToDataType(hiveUdfs.scala:76)
	at org.apache.spark.sql.hive.HiveSimpleUdf.dataType$lzycompute(hiveUdfs.scala:106)
	at org.apache.spark.sql.hive.HiveSimpleUdf.dataType(hiveUdfs.scala:106)
	at org.apache.spark.sql.catalyst.expressions.Alias.toAttribute(namedExpressions.scala:131)
	at org.apache.spark.sql.catalyst.planning.PhysicalOperation$$anonfun$collectAliases$1.applyOrElse(patterns.scala:95)
	at org.apache.spark.sql.catalyst.planning.PhysicalOperation$$anonfun$collectAliases$1.applyOrElse(patterns.scala:94)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.collection.TraversableLike$$anonfun$collect$1.apply(TraversableLike.scala:278)
...

To make udf developers more understood, we need to throw a more suitable exception.",,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 07 00:38:03 UTC 2015,,,,,,,,,,"0|i2cx2n:",9223372036854775807,,,,,marmbrus,,,,,,,,,,,,,,,,,,,,,,"07/Apr/15 17:18;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/5395;;;","15/May/15 07:24;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/6179;;;","07/Jul/15 00:38;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/7248;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Join with empty projection on one side produces invalid results,SPARK-6743,12818951,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,smolav,smolav,07/Apr/15 14:49,14/May/19 06:21,14/Jul/23 06:27,22/May/15 16:45,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,2,correctness,,,,,"{code:java}
val sqlContext = new SQLContext(sc)
val tab0 = sc.parallelize(Seq(
      (83,0,38),
      (26,0,79),
      (43,81,24)
    ))
    sqlContext.registerDataFrameAsTable(sqlContext.createDataFrame(tab0), ""tab0"")
sqlContext.cacheTable(""tab0"")   
val df1 = sqlContext.sql(""SELECT tab0._2, cor0._2 FROM tab0, tab0 cor0 GROUP BY tab0._2, cor0._2"")
val result1 = df1.collect()
val df2 = sqlContext.sql(""SELECT cor0._2 FROM tab0, tab0 cor0 GROUP BY cor0._2"")
val result2 = df2.collect()
val df3 = sqlContext.sql(""SELECT cor0._2 FROM tab0 cor0 GROUP BY cor0._2"")
val result3 = df3.collect()
{code}

Given the previous code, result2 equals to Row(43), Row(83), Row(26), which is wrong. These results correspond to cor0._1, instead of cor0._2. Correct results would be Row(0), Row(81), which are ok for the third query. The first query also produces valid results, and the only difference is that the left side of the join is not empty.",,apachespark,marmbrus,opuertas,rxin,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 16:45:45 UTC 2015,,,,,,,,,,"0|i2cwuv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/May/15 22:07;smolav;Any thoughts on this?;;;","14/May/15 09:39;smolav;Sorry, my first example was not very clear. Here is a more precise one:

{code}
 val sqlc = new SQLContext(sc)

    val tab0 = sc.parallelize(Seq(
      Tuple1(""A1""),
      Tuple1(""A2"")
    ))
    sqlc.registerDataFrameAsTable(sqlc.createDataFrame(tab0), ""tab0"")
    sqlc.cacheTable(""tab0"")

    val tab1 = sc.parallelize(Seq(
      Tuple1(""B1""),
      Tuple1(""B2"")
    ))
    sqlc.registerDataFrameAsTable(sqlc.createDataFrame(tab1), ""tab1"")
    sqlc.cacheTable(""tab1"")

    /* Succeeds */
    val result1 = sqlc.sql(""SELECT tab0._1,tab1._1 FROM tab0, tab1 GROUP BY tab0._1,tab1._1 ORDER BY tab0._1, tab1._1"").collect()
    assertResult(Array(Row(""A1"", ""B1""), Row(""A1"", ""B2""), Row(""A2"", ""B1""), Row(""A2"", ""B2"")))(result1)

    /* Fails. Got: Array([A1], [A2]) */
    val result2 = sqlc.sql(""SELECT tab1._1 FROM tab0, tab1 GROUP BY tab1._1 ORDER BY tab1._1"").collect()
    assertResult(Array(Row(""B1""), Row(""B2"")))(result2)
{code};;;","14/May/15 10:26;smolav;Note that the bug is not related to GROUP BY, that's just a quick way to produce a Project logical plan with an empty projection list from SQL. Builing upon my previous test case, here are some further instances of the bug using logical plans and DataFrames:

{code}
import org.apache.spark.sql.catalyst.dsl.plans._
    import org.apache.spark.sql.catalyst.dsl.expressions._
    
    val plan0 = sqlc.table(""tab0"").logicalPlan.subquery('tab0)
    val plan1 = sqlc.table(""tab1"").logicalPlan.subquery('tab1)
    
    /* Succeeds */
    val planA = plan0.select('_1 as ""c0"")
      .join(plan1.select('_1 as ""c1""))
      .select('c0, 'c1)
      .orderBy('c0.asc, 'c1.asc)
    assertResult(Array(Row(""A1"", ""B1""), Row(""A1"", ""B2""), Row(""A2"", ""B1""), Row(""A2"", ""B2"")))(DataFrame(sqlc, planA).collect())

    /* Fails. Got: Array([A1], [A1], [A2], [A2]) */
    val planB = plan0.select('_1 as ""c0"")
      .join(plan1.select('_1 as ""c1""))
      .select('c1)
      .orderBy('c1.asc)
    assertResult(Array(Row(""B1""), Row(""B1""), Row(""B2""), Row(""B2"")))(DataFrame(sqlc, planB).collect())

    /* Fails. Got: Array([A1], [A1], [A2], [A2]) */
    val planC = plan0.select()
      .join(plan1.select('_1 as ""c1""))
      .select('c1)
      .orderBy('c1.asc)
    assertResult(Array(Row(""B1""), Row(""B1""), Row(""B2""), Row(""B2"")))(DataFrame(sqlc, planC).collect())
{code};;;","14/May/15 12:31;smolav;This problem only happens for cached relations. Here is the root of the problem:

{code}
/* Fails. Got: Array(Row(""A1""), Row(""A2"") */
assertResult(Array(Row(), Row()))(
  InMemoryColumnarTableScan(Nil, Nil, sqlc.table(""tab0"").queryExecution.sparkPlan.asInstanceOf[InMemoryColumnarTableScan].relation)
    .execute().collect()
)
{code}

InMemoryColumnarTableScan returns the narrowest column when no attributes are requested:

{code}
 // Find the ordinals and data types of the requested columns.  If none are requested, use the
 // narrowest (the field with minimum default element size).
      val (requestedColumnIndices, requestedColumnDataTypes) = if (attributes.isEmpty) {
        val (narrowestOrdinal, narrowestDataType) =
          relation.output.zipWithIndex.map { case (a, ordinal) =>
            ordinal -> a.dataType
          } minBy { case (_, dataType) =>
            ColumnType(dataType).defaultSize
          }
        Seq(narrowestOrdinal) -> Seq(narrowestDataType)
      } else {
        attributes.map { a =>
          relation.output.indexWhere(_.exprId == a.exprId) -> a.dataType
        }.unzip
      }
{code}

It seems this is what leads to incorrect results.;;;","15/May/15 02:08;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/6165;;;","15/May/15 02:08;marmbrus;Thanks for narrowing this down.  I've opened a PR with a fix.;;;","22/May/15 16:45;marmbrus;Issue resolved by pull request 6165
[https://github.com/apache/spark/pull/6165];;;",,,,,,,,,,,,,,,,,,,,,,
Spark pushes down filters in old parquet path that reference partitioning columns,SPARK-6742,12818910,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,saucam,saucam,saucam,07/Apr/15 12:21,24/Apr/15 00:28,14/Jul/23 06:27,13/Apr/15 21:43,1.2.1,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Create a table with multiple fields partitioned on 'market' column. run a query like : 

SELECT start_sp_time, end_sp_time, imsi, imei,  enb_common_enbid FROM csl_data_parquet WHERE (((technology = 'FDD') AND (bandclass = '800') AND (region = 'R15') AND (market = 'LA metro')) OR ((technology = 'FDD') AND (bandclass = '1900') AND (region = 'R15') AND (market = 'Indianapolis'))) AND start_sp_time >= 1.4158368E9 AND end_sp_time < 1.4159232E9 AND dt >= '2014-11-13-00-00' AND dt < '2014-11-14-00-00' ORDER BY end_sp_time DESC LIMIT 100

The or filter is pushed down in this case , resulting in column not found exception from parquet 
",,apachespark,marmbrus,saucam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 13 21:43:17 UTC 2015,,,,,,,,,,"0|i2cwm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/15 12:22;saucam;This is same as SPARK-6554 for new parquet path;;;","07/Apr/15 12:27;apachespark;User 'saucam' has created a pull request for this issue:
https://github.com/apache/spark/pull/5390;;;","13/Apr/15 21:43;marmbrus;Issue resolved by pull request 5390
[https://github.com/apache/spark/pull/5390];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
SQL operator and condition precedence is not honoured,SPARK-6740,12818882,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,smolav,smolav,smolav,07/Apr/15 10:15,20/Oct/15 20:38,14/Jul/23 06:27,20/Oct/15 20:38,1.3.0,,,,,,1.6.0,,,,,,SQL,,,,0,,,,,,"The following query from the SQL Logic Test suite fails to parse:

SELECT DISTINCT * FROM t1 AS cor0 WHERE NOT ( - _2 + - 39 ) IS NULL

while the following (equivalent) does parse correctly:

SELECT DISTINCT * FROM t1 AS cor0 WHERE NOT (( - _2 + - 39 ) IS NULL)

SQLite, MySQL and Oracle (and probably most SQL implementations) define IS with higher precedence than NOT, so the first query is valid and well-defined.
",,apachespark,davies,rxin,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 20 20:38:43 UTC 2015,,,,,,,,,,"0|i2cwg7:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.6.0,,,,,,,,,,,,,"21/May/15 18:41;apachespark;User 'smola' has created a pull request for this issue:
https://github.com/apache/spark/pull/6326;;;","05/Sep/15 12:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/8617;;;","20/Oct/15 20:38;davies;Issue resolved by pull request 8617
[https://github.com/apache/spark/pull/8617];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
EstimateSize  is difference with spill file size,SPARK-6738,12818852,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shenhong,shenhong,shenhong,07/Apr/15 09:02,27/Apr/15 23:02,14/Jul/23 06:27,27/Apr/15 23:02,1.2.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"ExternalAppendOnlyMap spill 2.2 GB data to disk:

{code}

15/04/07 20:27:37 INFO collection.ExternalAppendOnlyMap: Thread 54 spilling in-memory map of 2.2 GB to disk (61 times so far)
15/04/07 20:27:37 INFO collection.ExternalAppendOnlyMap: /data11/yarnenv/local/usercache/spark/appcache/application_1423737010718_40455651/spark-local-20150407202613-4e80/11/temp_local_fdb4a583-5d13-4394-bccb-e1217d5db812
{code}

But the file size is only 2.2M.

{code}
ll -h /data11/yarnenv/local/usercache/spark/appcache/application_1423737010718_40455651/spark-local-20150407202613-4e80/11/
total 2.2M
-rw-r----- 1 spark users 2.2M Apr  7 20:27 temp_local_fdb4a583-5d13-4394-bccb-e1217d5db812
{code}

The GC log show that the jvm memory is less than 1GB.
{code}
2015-04-07T20:27:08.023+0800: [GC 981981K->55363K(3961344K), 0.0341720 secs]
2015-04-07T20:27:14.483+0800: [GC 987523K->53737K(3961344K), 0.0252660 secs]
2015-04-07T20:27:20.793+0800: [GC 985897K->56370K(3961344K), 0.0606460 secs]
2015-04-07T20:27:27.553+0800: [GC 988530K->59089K(3961344K), 0.0651840 secs]
2015-04-07T20:27:34.067+0800: [GC 991249K->62153K(3961344K), 0.0288460 secs]
2015-04-07T20:27:40.180+0800: [GC 994313K->61344K(3961344K), 0.0388970 secs]
2015-04-07T20:27:46.490+0800: [GC 993504K->59915K(3961344K), 0.0235150 secs]
{code}

The estimateSize  is hugh difference with spill file size, there is a bug in SizeEstimator.visitArray.",,apachespark,shenhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 23:02:18 UTC 2015,,,,,,,,,,"0|i2cw9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/15 10:44;srowen;Is that the only file spilled though? I'm not an expert but it looks like lots of files are spilled to here.;;;","07/Apr/15 10:44;srowen;(Please use the code tag to format output);;;","07/Apr/15 11:06;shenhong;Yes, it spill lots of files, but each one has only 1.1M. ;;;","07/Apr/15 11:15;srowen;Do you observe a problem? is it possible that you are looking at unserialized objects in memory but serialized representation on disk? what is the nature of the data? More info would be much more helpful;;;","07/Apr/15 12:37;srowen;To be clear I am asking how big the data being spilled is in memory. The GC state isnt relevant. That is, are they just compressing 10x on serialization into the files you see? It is not crazy.;;;","07/Apr/15 12:37;shenhong;I don't think it's serialized cause the problem. the input data is a hive table, and the spark job is a spark SQL.
In the fact, when the log show that spilling in-memory map of 2.2 GB to disk, the file is only 2.2M, and the GC log show the jvm is less than 1GB. the estimateSize also deviation with the jvm memory.
;;;","13/Apr/15 11:06;srowen;We can reopen if there is more detail, but the problem report is focusing on the size of one spill file when there are lots of them. The in-memory size is also not necessarily the on-disk size. I haven't seen a report of a problem here either, like something that then fails.;;;","21/Apr/15 02:54;shenhong;There is a bug in SizeEstimator;;;","21/Apr/15 08:55;apachespark;User 'shenh062326' has created a pull request for this issue:
https://github.com/apache/spark/pull/5608;;;","27/Apr/15 23:02;srowen;Resolved by https://github.com/apache/spark/pull/5608;;;",,,,,,,,,,,,,,,,,,,
OutputCommitCoordinator.authorizedCommittersByStage map out of memory,SPARK-6737,12818844,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,litao1990,litao1990,07/Apr/15 08:48,07/Apr/15 23:28,14/Jul/23 06:27,07/Apr/15 23:28,1.3.0,1.3.1,,,,,1.3.1,1.4.0,,,,,DStreams,Scheduler,Spark Core,,0,Bug,Core,DAGScheduler,OOM,Streaming,"I am using spark streaming(1.3.1)  as a long time running service and out of memory after running for 7 days. 

I found that the field authorizedCommittersByStage in OutputCommitCoordinator class cause the OOM. 
authorizedCommittersByStage is a map, key is StageId, value is Map[PartitionId, TaskAttemptId]. The OutputCommitCoordinator class has a method stageEnd which will remove stageId from authorizedCommittersByStage. But the method stageEnd is never called by DAGSchedule. And it cause the authorizedCommittersByStage's stage info never be cleaned, which cause OOM.

It happens in my spark streaming program(1.3.1), I am not sure if it will appear in other spark components and other spark version.",spark 1.3.1,397090770,aash,apachespark,emres,jhu,jnadler,joshrosen,litao1990,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 23:28:00 UTC 2015,,,,,,,,,,"0|i2cw7r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/15 17:27;joshrosen;This bug is present in Spark 1.3+.  The problem lies in the interaction between DAGScheduler and OutputCommitCoordinator.  DAGScheduler only informs the OutputCommitCoordinator of stage completion for one of the less-common paths in which stages are marked as completed, whereas we should be calling it for every stage completion event.

This should be pretty easy to fix with a patch to the DAGScheduler, so I'll work on putting one together shortly.;;;","07/Apr/15 17:49;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5397;;;","07/Apr/15 23:28;joshrosen;Issue resolved by pull request 5397
[https://github.com/apache/spark/pull/5397];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Support GenericUDTF.close for Generate,SPARK-6734,12818797,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,07/Apr/15 04:31,16/May/15 11:02,14/Jul/23 06:27,13/May/15 16:36,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Some third-party UDTF extension, will generate more rows in the ""GenericUDTF.close()"" method, which is supported by Hive.

https://cwiki.apache.org/confluence/display/Hive/DeveloperGuide+UDTF

However, Spark SQL ignores the ""GenericUDTF.close()"", and it causes bug while porting job from Hive to Spark SQL.",,apachespark,chenghao,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 16:36:17 UTC 2015,,,,,,,,,,"0|i2cvxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/15 04:34;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/5383;;;","13/May/15 16:36;lian cheng;Issue resolved by pull request 5383
[https://github.com/apache/spark/pull/5383];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't have table as identifier in OPTIONS,SPARK-6730,12818782,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,alexliu68,alexliu68,07/Apr/15 02:06,24/Apr/15 00:29,14/Jul/23 06:27,15/Apr/15 20:00,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"The following query fails because there is an  identifier ""table"" in OPTIONS

{code}
CREATE TEMPORARY TABLE ddlTable
USING org.apache.spark.sql.cassandra
OPTIONS (
 table ""test1"",
 keyspace ""test""
)
{code} 

The following error

{code}

]   java.lang.RuntimeException: [1.2] failure: ``insert'' expected but identifier CREATE found
[info] 
[info]  CREATE TEMPORARY TABLE ddlTable USING org.apache.spark.sql.cassandra OPTIONS (  table ""test1"",  keyspace ""dstest""  )       
[info]  ^
[info]   at scala.sys.package$.error(package.scala:27)
[info]   at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSparkSQLParser.scala:40)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$2.apply(SQLContext.scala:130)
[info]   at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:96)
[info]   at org.apache.spark.sql.SparkSQLParser$$anonfun$org$apache$spark$sql$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:95)
[info]   at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
[info]   at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
[info]   at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
[info]   at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
[info]   at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
[info]   at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
[info]   at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
[info]   at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
[info]   at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
[info]   at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
[info]   at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(AbstractSparkSQLParser.scala:38)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:134)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:134)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:134)
{code}",,alexliu68,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 20:00:29 UTC 2015,,,,,,,,,,"0|i2cvtz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/15 07:44;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5520;;;","15/Apr/15 20:00;marmbrus;Issue resolved by pull request 5520
[https://github.com/apache/spark/pull/5520];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
DriverQuirks get can get OutOfBounds exception is some cases,SPARK-6729,12818746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vlyubin,vlyubin,vlyubin,06/Apr/15 23:25,07/Apr/15 01:01,14/Jul/23 06:27,07/Apr/15 01:01,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"The function uses .substring(0, X), which will trigger OutOfBoundsException if string length is less than X. A better way to do this is to use startsWith, which won't error out in this case. I'll propose a patch shortly.",,apachespark,vlyubin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 06 23:27:24 UTC 2015,,,,,,,,,,"0|i2cvlz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/15 23:27;apachespark;User 'vlyubin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5378;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change SparkContext.DRIVER_IDENTIFIER from '<driver>' to 'driver',SPARK-6716,12790277,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,05/Apr/15 21:06,07/Apr/15 06:37,14/Jul/23 06:27,07/Apr/15 06:37,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Currently, the driver's executorId is set to {{<driver>}}.  This choice of ID was present in older Spark versions, but it has started to cause problems now that executorIds are used in more contexts, such as Ganglia metric names or driver thread-dump links the web UI.  The angle brackets must be escaped when embedding this ID in XML or as part of URLs and this has led to multiple problems:

- https://issues.apache.org/jira/browse/SPARK-6484
- https://issues.apache.org/jira/browse/SPARK-4313

The simplest solution seems to be to change this id to something that does not contain any special characters, such as {{driver}}. 

I'm not sure whether we can perform this change in a patch release, since this ID may be considered a stable API by metrics users, but it's probably okay to do this in a major release as long as we document it in the release notes.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 07 06:37:59 UTC 2015,,,,,,,,,,"0|i284an:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"05/Apr/15 21:07;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5372;;;","07/Apr/15 06:37;joshrosen;Issue resolved by pull request 5372
[https://github.com/apache/spark/pull/5372];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong initial bias in GraphX SVDPlusPlus,SPARK-6710,12788294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,michaelmalak,michaelmalak,michaelmalak,04/Apr/15 18:27,11/Apr/15 07:03,14/Jul/23 06:27,11/Apr/15 07:03,1.3.0,,,,,,1.4.0,,,,,,GraphX,,,,0,easyfix,,,,,"In the initialization portion of GraphX SVDPlusPluS, the initialization of biases appears to be incorrect. Specifically, in line 
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/SVDPlusPlus.scala#L96 
instead of 
(vd._1, vd._2, msg.get._2 / msg.get._1, 1.0 / scala.math.sqrt(msg.get._1)) 
it should probably be 
(vd._1, vd._2, msg.get._2 / msg.get._1 - u, 1.0 / scala.math.sqrt(msg.get._1)) 

That is, the biases bu and bi (both represented as the third component of the Tuple4[] above, depending on whether the vertex is a user or an item), described in equation (1) of the Koren paper, are supposed to be small offsets to the mean (represented by the variable u, signifying the Greek letter mu) to account for peculiarities of individual users and items. 

Initializing these biases to wrong values should theoretically not matter given enough iterations of the algorithm, but some quick empirical testing shows it has trouble converging at all, even after many orders of magnitude additional iterations. 

This perhaps could be the source of previously reported trouble with SVDPlusPlus. 
http://apache-spark-user-list.1001560.n3.nabble.com/GraphX-SVDPlusPlus-problem-td12885.html ",,apachespark,josephkb,michaelmalak,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 11 01:31:40 UTC 2015,,,,,,,,,,"0|i27shj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/15 22:11;rxin;[~michaelmalak] would you like to submit a pull request for this?;;;","11/Apr/15 01:31;apachespark;User 'michaelmalak' has created a pull request for this issue:
https://github.com/apache/spark/pull/5464;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Using Hive UDTF may throw ClassNotFoundException,SPARK-6708,12788281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,lian cheng,lian cheng,04/Apr/15 14:31,28/May/15 05:54,14/Jul/23 06:27,11/Apr/15 14:13,1.1.1,1.2.1,1.3.0,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Spark shell session for reproducing this issue:
{code}
import sqlContext._

sql(""create table t1 (str string)"")
sql(""select v.va from t1 lateral view json_tuple(str, 'a') v as va"").queryExecution.analyzed
{code}
Exception thrown:
{noformat}
java.lang.ClassNotFoundException: json_tuple
        at scala.tools.nsc.interpreter.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:83)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.sql.hive.HiveFunctionWrapper.createFunction(Shim13.scala:148)
        at org.apache.spark.sql.hive.HiveGenericUdtf.function$lzycompute(hiveUdfs.scala:274)
        at org.apache.spark.sql.hive.HiveGenericUdtf.function(hiveUdfs.scala:274)
        at org.apache.spark.sql.hive.HiveGenericUdtf.outputInspector$lzycompute(hiveUdfs.scala:280)
        at org.apache.spark.sql.hive.HiveGenericUdtf.outputInspector(hiveUdfs.scala:280)
        at org.apache.spark.sql.hive.HiveGenericUdtf.outputDataTypes$lzycompute(hiveUdfs.scala:285)
        at org.apache.spark.sql.hive.HiveGenericUdtf.outputDataTypes(hiveUdfs.scala:285)
        at org.apache.spark.sql.hive.HiveGenericUdtf.makeOutput(hiveUdfs.scala:291)
        at org.apache.spark.sql.catalyst.expressions.Generator.output(generators.scala:60)
        at org.apache.spark.sql.catalyst.plans.logical.Generate$$anonfun$2.apply(basicOperators.scala:60)
        at org.apache.spark.sql.catalyst.plans.logical.Generate$$anonfun$2.apply(basicOperators.scala:60)
        at scala.Option.map(Option.scala:145)
        at org.apache.spark.sql.catalyst.plans.logical.Generate.generatorOutput(basicOperators.scala:60)
        at org.apache.spark.sql.catalyst.plans.logical.Generate.output(basicOperators.scala:70)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveChildren$1.apply(LogicalPlan.scala:117)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveChildren$1.apply(LogicalPlan.scala:117)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
        at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
        at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:117)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$7$$anonfun$applyOrElse$2$$anonfun$11.apply(Analyzer.scala:292)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$7$$anonfun$applyOrElse$2$$anonfun$11.apply(Analyzer.scala:292)
        at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$7$$anonfun$applyOrElse$2.applyOrElse(Analyzer.scala:292)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$7$$anonfun$applyOrElse$2.applyOrElse(Analyzer.scala:284)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:252)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:252)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:251)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:108)
        at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:123)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:122)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:127)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$7.applyOrElse(Analyzer.scala:284)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$7.applyOrElse(Analyzer.scala:196)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$2.apply(TreeNode.scala:256)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$2.apply(TreeNode.scala:256)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:255)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:196)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:195)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
        at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
        at scala.collection.immutable.List.foldLeft(List.scala:84)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
        at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:1106)
        at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:1106)
        at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1104)
        at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
        at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
        at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:97)
{noformat}
The problem is that we passed the function name rather than the resolved UDTF class name to {{HiveGenericUdtf}} [here|https://github.com/apache/spark/blob/9b40c17ab161b64933539abeefde443cb4f98673/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala#L1288].",,dreamquster,khnd,lian cheng,Saurabh Santhosh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4811,SPARK-6835,SPARK-4854,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 11 14:13:53 UTC 2015,,,,,,,,,,"0|i27sf3:",9223372036854775807,,,,,lian cheng,,,,,,,,,1.4.0,,,,,,,,,,,,,"05/Apr/15 20:31;khnd;is this related to SPARK-4854?;;;","07/Apr/15 08:40;lian cheng;Thanks for pointing out this. I followed SPARK-4854 and found that this issue duplicates SPARK-4811.;;;","11/Apr/15 14:13;lian cheng;Issue resolved by pull request 5444
[https://github.com/apache/spark/pull/5444];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.deploy.yarn.YarnClusterSuite Python application,SPARK-6701,12788097,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,andrewor14,andrewor14,03/Apr/15 20:39,30/Dec/15 22:26,14/Jul/23 06:27,30/Dec/15 22:26,1.3.0,,,,,,,,,,,,Tests,YARN,,,0,,,,,,"Observed in Master and 1.3, both in SBT and in Maven (with YARN).

{code}
Process List(/home/jenkins/workspace/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE/hadoop2.3/label/centos/bin/spark-submit, --master, yarn-cluster, --num-executors, 1, --properties-file, /tmp/spark-ea49597c-2a95-4d8c-a9ea-23861a02c9bd/spark968020731409047027.properties, --py-files, /tmp/spark-ea49597c-2a95-4d8c-a9ea-23861a02c9bd/test2.py, /tmp/spark-ea49597c-2a95-4d8c-a9ea-23861a02c9bd/test.py, /tmp/spark-ea49597c-2a95-4d8c-a9ea-23861a02c9bd/result961582960984674264.tmp) exited with code 1

sbt.ForkMain$ForkError: Process List(/home/jenkins/workspace/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE/hadoop2.3/label/centos/bin/spark-submit, --master, yarn-cluster, --num-executors, 1, --properties-file, /tmp/spark-ea49597c-2a95-4d8c-a9ea-23861a02c9bd/spark968020731409047027.properties, --py-files, /tmp/spark-ea49597c-2a95-4d8c-a9ea-23861a02c9bd/test2.py, /tmp/spark-ea49597c-2a95-4d8c-a9ea-23861a02c9bd/test.py, /tmp/spark-ea49597c-2a95-4d8c-a9ea-23861a02c9bd/result961582960984674264.tmp) exited with code 1
	at org.apache.spark.util.Utils$.executeAndGetOutput(Utils.scala:1122)
	at org.apache.spark.deploy.yarn.YarnClusterSuite.org$apache$spark$deploy$yarn$YarnClusterSuite$$runSpark(YarnClusterSuite.scala:259)
	at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$4.apply$mcV$sp(YarnClusterSuite.scala:160)
	at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$4.apply(YarnClusterSuite.scala:146)
	at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$4.apply(YarnClusterSuite.scala:146)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
{code}",,andrewor14,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6700,,,,,,,SPARK-6506,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 30 22:26:24 UTC 2015,,,,,,,,,,"0|i27ruf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/15 22:26;joshrosen;I'm going to resolve this as fixed since the relevant tests don't appear to have failed in the last month or so:

https://spark-tests.appspot.com/tests/org.apache.spark.deploy.yarn.YarnClusterSuite/run%20Python%20application%20in%20yarn-cluster%20mode

https://spark-tests.appspot.com/tests/org.apache.spark.deploy.yarn.YarnClusterSuite/run%20Python%20application%20in%20yarn-client%20mode;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveContext.refreshTable is missing in PySpark,SPARK-6696,12787991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,03/Apr/15 13:47,09/Apr/15 01:47,14/Jul/23 06:27,09/Apr/15 01:47,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,,,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 01:47:54 UTC 2015,,,,,,,,,,"0|i27r7j:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"03/Apr/15 13:49;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5349;;;","09/Apr/15 01:47;marmbrus;Issue resolved by pull request 5349
[https://github.com/apache/spark/pull/5349];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQL CLI must be able to specify an option --database on the command line.,SPARK-6694,12787943,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,jin,jin,jin,03/Apr/15 08:11,24/Apr/15 00:30,14/Jul/23 06:27,16/Apr/15 15:41,1.3.0,1.4.0,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"SparkSQL CLI has an option --database as follows.
But, the option --database is ignored.

{code:}
$ spark-sql --help
:
CLI options:
    :
    --database <databasename>     Specify the database to use
```
{code}",,apachespark,jin,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 16 15:41:30 UTC 2015,,,,,,,,,,"0|i27qwv:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"03/Apr/15 08:23;apachespark;User 'adachij2002' has created a pull request for this issue:
https://github.com/apache/spark/pull/5345;;;","03/Apr/15 08:38;srowen;What problem do you encounter? You only showed the help message.;;;","03/Apr/15 09:39;jin;SparkSQL CLI doesn't work option --database, and that forced database to ""default"".
For example, It is said that I have a database 'test_db' and a table 't_user'.
I caught the error as follows.

{code:}
$ spark-sql --database test_db -e 'select * from t_user order by id'
:
15/04/03 13:26:30 ERROR metadata.Hive: NoSuchObjectException(message:default.t_user table not found)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result$get_table_resultStandardScheme.read(ThriftHiveMetastore.java:29338)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result$get_table_resultStandardScheme.read(ThriftHiveMetastore.java:29306)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_table_result.read(ThriftHiveMetastore.java:29237)
at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_table(ThriftHiveMetastore.java:1036)
at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_table(ThriftHiveMetastore.java:1022)
at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTable(HiveMetaStoreClient.java:997)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
at com.sun.proxy.$Proxy9.getTable(Unknown Source)
at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:976)
at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:950)
at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:180)
at org.apache.spark.sql.hive.HiveContext$$anon$2.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$$super$lookupRelation(HiveContext.scala:252)
at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:161)
at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:161)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:161)
at org.apache.spark.sql.hive.HiveContext$$anon$2.lookupRelation(HiveContext.scala:252)
at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:175)
at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:187)
at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:182)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:187)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:187)
at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:50)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:186)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:207)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
at scala.collection.AbstractIterator.to(Iterator.scala:1157)
at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:236)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:192)
at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:207)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
at scala.collection.AbstractIterator.to(Iterator.scala:1157)
at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:236)
at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:192)
at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:177)
at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:182)
at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:172)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
at scala.collection.immutable.List.foldLeft(List.scala:84)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
at scala.collection.immutable.List.foreach(List.scala:318)
at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:1071)
at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:1071)
at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1069)
at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:92)
at org.apache.spark.sql.hive.thriftserver.AbstractSparkSQLDriver.run(AbstractSparkSQLDriver.scala:57)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:278)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:359)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:155)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code};;;","16/Apr/15 15:41;lian cheng;Issue resolved by pull request 5345
[https://github.com/apache/spark/pull/5345];;;",,,,,,,,,,,,,,,,,,,,,,,,,
spark-sql script ends up throwing Exception when event logging is enabled.,SPARK-6690,12787903,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,sarutak,sarutak,03/Apr/15 04:59,30/May/15 13:30,14/Jul/23 06:27,27/May/15 19:07,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"When event logging is enabled, spark-sql script ends up throwing Exception like as follows.

{code}
15/04/03 13:51:49 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/jobs,null}
15/04/03 13:51:49 ERROR scheduler.LiveListenerBus: Listener EventLoggingListener threw an exception
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
	at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:144)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:144)
	at org.apache.spark.scheduler.EventLoggingListener.onApplicationEnd(EventLoggingListener.scala:188)
	at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:54)
	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53)
	at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:37)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:79)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1171)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:63)
Caused by: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)
	at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1843)
	at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1804)
	at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:127)
	... 17 more
15/04/03 13:51:49 INFO ui.SparkUI: Stopped Spark web UI at http://sarutak-devel:4040
15/04/03 13:51:49 INFO scheduler.DAGScheduler: Stopping DAGScheduler
Exception in thread ""Thread-6"" java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1760)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1124)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1120)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1120)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1398)
	at org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener.scala:209)
	at org.apache.spark.SparkContext$$anonfun$stop$3.apply(SparkContext.scala:1408)
	at org.apache.spark.SparkContext$$anonfun$stop$3.apply(SparkContext.scala:1408)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1408)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.stop(SparkSQLEnv.scala:66)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$$anon$1.run(SparkSQLCLIDriver.scala:107)
{code}

This is because FileSystem#close is called by the shutdown hook registered in SparkSQLCLIDriver.

{code}
    Runtime.getRuntime.addShutdownHook(
      new Thread() {
        override def run() {
          SparkSQLEnv.stop()
        }
      }
    )
{code}

This issue was resolved by SPARK-3062 but I think, it's brought again by SPARK-2261.",,apachespark,sarutak,WangTaoTheTonic,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6933,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 19:06:34 UTC 2015,,,,,,,,,,"0|i27qnz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"03/Apr/15 05:42;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/5341;;;","27/May/15 19:06;yhuai;Per https://github.com/apache/spark/pull/5341, https://github.com/apache/spark/pull/5560 addressed this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
EventLoggingListener should always operate on resolved URIs,SPARK-6688,12787873,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,03/Apr/15 01:39,03/Apr/15 18:55,14/Jul/23 06:27,03/Apr/15 18:55,1.3.0,,,,,,1.3.1,1.4.0,,,,,Spark Core,,,,0,,,,,,"A small bug was introduced in 1.3.0, where a check in EventLoggingListener.scala is performed on the non-resolved log path. This means that if ""fs.defaultFS"" is not the local filesystem, and the user is trying to store logs in the local filesystem by providing a path with no ""file:"" protocol, thing will fail.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 01:42:32 UTC 2015,,,,,,,,,,"0|i27qhj:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"03/Apr/15 01:42;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5340;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
toDF column rename does not work when columns contain '.',SPARK-6686,12787840,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,02/Apr/15 22:46,03/Apr/15 01:31,14/Jul/23 06:27,03/Apr/15 01:31,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"{code}
  test(""rename nested groupby"") {
    val df = Seq((1,(1,1))).toDF()

    checkAnswer(
      df.groupBy(""_1"").agg(col(""_1""), sum(""_2._1"")).toDF(""key"", ""total""),
      Row(1, 1) :: Nil)
  }
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 01:31:19 UTC 2015,,,,,,,,,,"0|i27qaf:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"02/Apr/15 22:49;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5337;;;","03/Apr/15 01:31;marmbrus;Issue resolved by pull request 5337
[https://github.com/apache/spark/pull/5337];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark.sql nondeterministic issue with row fields,SPARK-6677,12787640,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,parmesan,parmesan,02/Apr/15 10:25,12/Apr/15 13:01,14/Jul/23 06:27,12/Apr/15 05:34,1.3.0,,,,,,1.3.1,1.4.0,,,,,PySpark,,,,0,pyspark,row,sql,,,"The following issue happens only when running pyspark in the python interpreter, it works correctly with spark-submit.

Reading two json files containing objects with a different structure leads sometimes to the definition of wrong Rows, where the fields of a file are used for the other one.

I was able to write a sample code that reproduce this issue one out of three times; the code snippet is available at the following link, together with some (very simple) data samples:

https://gist.github.com/armisael/e08bb4567d0a11efe2db","spark version: spark-1.3.0-bin-hadoop2.4
python version: Python 2.7.6
operating system: MacOS, x86_64 x86_64 x86_64 GNU/Linux",apachespark,davies,joshrosen,parmesan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 12 13:01:38 UTC 2015,,,,,,,,,,"0|i27p2n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/15 06:47;parmesan;An update: on a more complex script I'm using, the same issue arises even with spark-submit.;;;","08/Apr/15 16:52;davies;What is the expected output?

I got this:
{code}
key: a
res1 data as row: [Row(foo=1, key=u'a')]
res2 data as row: [Row(bar=3, key=u'a', other=u'foobar')]
res1 and res2 fields: (u'foo', u'key') (u'bar', u'key', u'other')
res1 data as tuple: 1 a
res2 data as tuple: 3 a foobar
key: c
res1 data as row: []
res2 data as row: [Row(bar=4, key=u'c', other=u'barfoo')]
key: b
res1 data as row: [Row(foo=2, key=u'b')]
res2 data as row: []
{code};;;","08/Apr/15 18:43;parmesan;Hi Davies,

Thanks for taking the time to look into this; that's the expected output, in fact. The point is that back then it happened to me randomly, once every five executions or so. Now I'm not able to reproduce it anymore with the data I posted, but I was able to reproduce it again (100% of the times) with the data I just added to the gist; the exception I'm getting is:

{noformat}
$ ./bin/pyspark ./spark_test.py
[...]
key: 31491
res1 data as row: [Row(foo=31491, key=u'31491')]
res2 data as row: [Row(bar=1574550000, key=u'31491', other=u'foobar', some=u'thing', that=u'this', this=u'that')]
res1 and res2 fields: (u'foo', u'key') (u'bar', u'key', u'other', u'some', u'that', u'this')
res1 data as tuple: 31491 31491
res2 data as tuple: 1574550000 31491 foobar
key: 31497
res1 data as row: []
res2 data as row: [Row(foo=1574850000, key=u'31497')]
key: 31495
res1 data as row: [Traceback (most recent call last):
  File ""/path/to/spark-1.3.0-bin-hadoop2.4/./spark_test.py"", line 25, in <module>
    print ""res1 data as row:"", list(res_x)
  File ""/path/to/spark-1.3.0-bin-hadoop2.4/python/pyspark/sql/types.py"", line 1214, in __repr__
    for n in self.__FIELDS__))
  File ""/path/to/spark-1.3.0-bin-hadoop2.4/python/pyspark/sql/types.py"", line 1214, in <genexpr>
    for n in self.__FIELDS__))
IndexError: tuple index out of range
{noformat}
which is the same I'm getting in my more complex script.

Some considerations:
1) it may be that this particular input leads to the issue only on my machine, therefore I've added another file to generate some random inputs; I've got this exception on two over three randomly-generated samples, please go ahead and run it with different values of {{N}} if the uploaded data does not make pyspark crash in your environment;
2) interestingly, given the sample data, it always crashes on the same key: {{31495}}; however, they do not seem ""magic"" to me (and of course input files containing just those two elements does not make pyspark crash in any way):
{noformat}
data/sample_a.json:{""foo"": 31495, ""key"": ""31495""}
data/sample_b.json:{""other"": ""foobar"", ""bar"": 1574750000, ""key"": ""31495"", ""that"": ""this"", ""this"": ""that"", ""some"": ""thing""}
{noformat}
3) what happens is that either {{res_x.data\[0\].___FIELDS___}} or {{res_y.data\[0\].___FIELDS___}} get the wrong field names, leading to the {{IndexError}} (the fields are too many and the row does not contain enough data).;;;","08/Apr/15 21:22;davies;I still can not reproduce in on master, will retry with 1.3.0.;;;","08/Apr/15 21:27;davies;Spark 1.3.0 also works fine here, tried different N.;;;","09/Apr/15 09:02;parmesan;Uhm, don't know what to say. Let's try with this: I've created a docker that reproduces the issue, its available here:
https://github.com/armisael/SPARK-6677

I tested it on three different machines, and the issue appeared on all of them. Can you give it a try?;;;","09/Apr/15 21:49;davies;That's cool, I will test it.;;;","10/Apr/15 00:58;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/5445;;;","10/Apr/15 00:58;davies;This will be fixed by https://github.com/apache/spark/pull/5445, thanks to help to reproduce it.;;;","12/Apr/15 05:34;joshrosen;Issue resolved by pull request 5445
[https://github.com/apache/spark/pull/5445];;;","12/Apr/15 05:35;joshrosen;Thanks again for the reproduction; this was a tricky issue!;;;","12/Apr/15 13:01;parmesan;glad it helped! we're very eager to try it out;;;",,,,,,,,,,,,,,,,,
HiveContext setConf is not stable,SPARK-6675,12787607,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,invkrh,invkrh,02/Apr/15 08:37,22/Jun/15 21:08,14/Jul/23 06:27,22/Jun/15 21:08,1.3.0,,,,,,,,,,,,SQL,,,,1,,,,,,"I find HiveContext.setConf does not work correctly. Here are some code snippets showing the problem:

snippet 1:

{code}
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.{SparkConf, SparkContext}

object Main extends App {

  val conf = new SparkConf()
    .setAppName(""context-test"")
    .setMaster(""local[8]"")
  val sc = new SparkContext(conf)
  val hc = new HiveContext(sc)

  hc.setConf(""spark.sql.shuffle.partitions"", ""10"")
  hc.setConf(""hive.metastore.warehouse.dir"", ""/home/spark/hive/warehouse_test"")
  hc.getAllConfs filter(_._1.contains(""warehouse.dir"")) foreach println
  hc.getAllConfs filter(_._1.contains(""shuffle.partitions"")) foreach println
}
{code}


Results:
(hive.metastore.warehouse.dir,/home/spark/hive/warehouse_test)
(spark.sql.shuffle.partitions,10)

snippet 2:
{code}
...
  hc.setConf(""hive.metastore.warehouse.dir"", ""/home/spark/hive/warehouse_test"")
  hc.setConf(""spark.sql.shuffle.partitions"", ""10"")
  hc.getAllConfs filter(_._1.contains(""warehouse.dir"")) foreach println
  hc.getAllConfs filter(_._1.contains(""shuffle.partitions"")) foreach println
...
{code}

Results:
(hive.metastore.warehouse.dir,/user/hive/warehouse)
(spark.sql.shuffle.partitions,10)

You can see that I just permuted the two setConf call, then that leads to two different Hive configuration.
It seems that HiveContext can not set a new value on ""hive.metastore.warehouse.dir"" key in one or the first ""setConf"" call.
You need another ""setConf"" call before changing ""hive.metastore.warehouse.dir"". For example, set ""hive.metastore.warehouse.dir"" twice and the snippet 1

snippet 3:
{code}
...
  hc.setConf(""hive.metastore.warehouse.dir"", ""/home/spark/hive/warehouse_test"")
  hc.setConf(""hive.metastore.warehouse.dir"", ""/home/spark/hive/warehouse_test"")
  hc.getAllConfs filter(_._1.contains(""warehouse.dir"")) foreach println
...
{code}

Results:
(hive.metastore.warehouse.dir,/home/spark/hive/warehouse_test)


You can reproduce this if you move to the latest branch-1.3 (1.3.1-snapshot, htag = 7d029cb1eb6f1df1bce1a3f5784fb7ce2f981a33)

I have also tested the released 1.3.0 (htag = 4aaf48d46d13129f0f9bdafd771dd80fe568a7dc). It has the same problem.",AWS ec2 xlarge2 cluster launched by spark's script,apachespark,glenn.strycker@gmail.com,invkrh,smolav,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 22 21:06:44 UTC 2015,,,,,,,,,,"0|i27ovb:",9223372036854775807,,,,,yhuai,,,,,,,,,1.5.0,,,,,,,,,,,,,"10/Apr/15 17:33;apachespark;User 'gvramana' has created a pull request for this issue:
https://github.com/apache/spark/pull/5457;;;","19/Jun/15 01:31;yhuai;[~invkrh] Can you try 1.4 or master and see if this problem still exists?;;;","22/Jun/15 21:06;invkrh;Hi,

I tried branch-1.4.
It works now. Issue closed.

Thank you for your work.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
spark-shell.cmd can't start even when spark was built in Windows,SPARK-6673,12787583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tsudukim,tsudukim,tsudukim,02/Apr/15 06:36,06/Apr/15 09:11,14/Jul/23 06:27,06/Apr/15 09:11,1.3.0,,,,,,1.4.0,,,,,,Windows,,,,0,,,,,,"spark-shell.cmd can't start.

{code}
bin\spark-shell.cmd --master local
{code}
will get
{code}
Failed to find Spark assembly JAR.
You need to build Spark before running this program.
{code}
even when we have built spark.

This is because of the lack of the environment {{SPARK_SCALA_VERSION}} which is used in {{spark-class2.cmd}}.
In linux scripts, this value is set as {{2.10}} or {{2.11}} by default in {{load-spark-env.sh}}, but there are no equivalent script in Windows.

As workaround, by executing
{code}
set SPARK_SCALA_VERSION=2.10
{code}
before execute spark-shell.cmd, we can successfully start it.",,apachespark,avulanov,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 06 09:11:36 UTC 2015,,,,,,,,,,"0|i27opz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"02/Apr/15 06:55;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/5328;;;","03/Apr/15 19:56;avulanov;Probably similar issue: I am trying to execute unit tests in MLlib with LocalClusterSparkContext on Windows 7. I am getting a bunch of error in the log saying that: ""Cannot find any assembly build directories."" If I do set SPARK_SCALA_VERSION=2.10 then I get ""No assemblies found in 'C:\dev\spark\mllib\.\assembly\target\scala-2.10'"";;;","06/Apr/15 02:11;tsudukim;Similar, but might be a different problem.
Because the original problem can be avoided just by setting SPARK_SCALA_VERSION variable.

Could you show the commands how to build Spark, and how to execute it?
And please re-build Spark after ""clean"", try it again, and paste the full output.;;;","06/Apr/15 09:11;srowen;Issue resolved by pull request 5328
[https://github.com/apache/spark/pull/5328];;;",,,,,,,,,,,,,,,,,,,,,,,,,
createDataFrame from RDD[Row] with UDTs cannot be saved,SPARK-6672,12787578,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,02/Apr/15 06:25,23/Apr/15 17:50,14/Jul/23 06:27,02/Apr/15 10:28,1.3.0,,,,,,1.3.1,1.4.0,,,,,MLlib,SQL,,,0,,,,,,"Reported by Jaonary (https://www.mail-archive.com/user@spark.apache.org/msg25218.html):

{code}
import org.apache.spark.mllib.linalg._
import org.apache.spark.mllib.regression._
val df0 = sqlContext.createDataFrame(Seq(LabeledPoint(1.0, Vectors.dense(2.0, 3.0))))
df0.save(""/tmp/df0"") // works
val df1 = sqlContext.createDataFrame(df0.rdd, df0.schema)
df1.save(""/tmp/df1"") // error
{code}

throws

{code}
15/04/01 23:24:16 INFO DAGScheduler: Job 3 failed: runJob at newParquet.scala:686, took 0.288304 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 3.0 failed 1 times, most recent failure: Lost task 3.0 in stage 3.0 (TID 15, localhost): java.lang.ClassCastException: org.apache.spark.mllib.linalg.DenseVector cannot be cast to org.apache.spark.sql.Row
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:191)
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:182)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:171)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:134)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:668)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:686)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:686)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:212)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}",,apachespark,lian cheng,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 10:28:14 UTC 2015,,,,,,,,,,"0|i27oov:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"02/Apr/15 07:03;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5329;;;","02/Apr/15 10:28;lian cheng;Issue resolved by pull request 5329
[https://github.com/apache/spark/pull/5329];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveContext.analyze should throw UnsupportedOperationException instead of NotImplementedError,SPARK-6670,12787570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,yhuai,yhuai,02/Apr/15 05:26,02/Apr/15 23:52,14/Jul/23 06:27,02/Apr/15 23:52,1.2.0,1.3.0,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,,,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-04-02 05:26:50.0,,,,,,,,,,"0|i27on3:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lock metastore client in analyzeTable,SPARK-6669,12787569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,yhuai,yhuai,02/Apr/15 05:16,02/Apr/15 23:49,14/Jul/23 06:27,02/Apr/15 23:49,1.2.0,1.3.0,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,,,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 22:19:39 UTC 2015,,,,,,,,,,"0|i27omv:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"02/Apr/15 22:19;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/5333;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
hang while collect in PySpark,SPARK-6667,12787562,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,02/Apr/15 04:31,03/Aug/15 20:26,14/Jul/23 06:27,02/Apr/15 19:21,1.2.2,1.3.1,1.4.0,,,,1.2.2,1.3.1,1.4.0,,,,PySpark,,,,0,,,,,,"PySpark tests hang while collecting:

",,apachespark,ari.meyer,davies,joshrosen,mnazario,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 03 20:26:05 UTC 2015,,,,,,,,,,"0|i27olb:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.1,1.4.0,,,,,,,,,,,"02/Apr/15 05:08;davies;The server side had timed out:
{code}
15/04/01 22:04:54 ERROR PythonRDD: Error while sending iterator
java.net.SocketTimeoutException: Accept timed out
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)
	at java.net.ServerSocket.implAccept(ServerSocket.java:530)
	at java.net.ServerSocket.accept(ServerSocket.java:498)
	at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:616)
{code}

But client had connected:
{code}
Traceback (most recent call last):
  File ""/Users/davies/work/spark/python/pyspark/rdd.py"", line 2332, in <module>
    _test()
  File ""/Users/davies/work/spark/python/pyspark/rdd.py"", line 2325, in _test
    globs=globs, optionflags=doctest.ELLIPSIS)
  File ""//anaconda/lib/python2.7/doctest.py"", line 1911, in testmod
    runner.run(test)
  File ""//anaconda/lib/python2.7/doctest.py"", line 1454, in run
    return self.__run(test, compileflags, out)
  File ""//anaconda/lib/python2.7/doctest.py"", line 1315, in __run
    compileflags, 1) in test.globs
  File ""<doctest __main__.RDD.take[0]>"", line 1, in <module>
  File ""/Users/davies/work/spark-1.1/python/pyspark/rdd.py"", line 1229, in take
    res = self.context.runJob(self, takeUpToNumLeft, p, True)
  File ""/Users/davies/work/spark-1.1/python/pyspark/context.py"", line 843, in runJob
    return list(_load_from_socket(port, mappedRDD._jrdd_deserializer))
  File ""/Users/davies/work/spark-1.1/python/pyspark/rdd.py"", line 119, in _load_from_socket
    for item in serializer.load_stream(rf):
  File ""/Users/davies/work/spark-1.1/python/pyspark/serializers.py"", line 131, in load_stream
    yield self._read_with_length(stream)
  File ""/Users/davies/work/spark-1.1/python/pyspark/serializers.py"", line 148, in _read_with_length
    length = read_int(stream)
  File ""/Users/davies/work/spark-1.1/python/pyspark/serializers.py"", line 526, in read_int
    length = stream.read(4)
  File ""//anaconda/lib/python2.7/socket.py"", line 380, in read
    data = self._sock.recv(left)
KeyboardInterrupt
{code};;;","02/Apr/15 05:18;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/5324;;;","02/Apr/15 17:48;joshrosen;This was introduced by SPARK-6194, which was merged for 1.2.2, so I'm adding 1.2.2. as an affected / target version.;;;","03/Aug/15 01:42;ari.meyer;I was using 1.2.0 and didn't have this problem.  I just downloaded 1.4.1, and it appears every time I try to run the getting started examples with PySpark.  I reinstalled 1.2.0, and get no socket timeout.  Any ideas?  I'm running on Linux Mint 17.2.;;;","03/Aug/15 20:26;ari.meyer;I just tested with 1.3.1, and it works fine.  I then retested with 1.4.1, and now it's not giving me the socket timeout.  The only change was suspending my laptop overnight.  Any idea why I got the timeout repeatedly before with 1.4.1 only, but not now? ;;;",,,,,,,,,,,,,,,,,,,,,,,,
"Python type errors should print type, not object",SPARK-6661,12787519,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,31z4,josephkb,josephkb,01/Apr/15 23:22,20/Apr/15 17:45,14/Jul/23 06:27,20/Apr/15 17:44,1.3.0,,,,,,1.4.0,,,,,,MLlib,PySpark,,,0,,,,,,"In MLlib PySpark, we sometimes test the type of an object and print an error if the object is of the wrong type.  E.g.:
[https://github.com/apache/spark/blob/f084c5de14eb10a6aba82a39e03e7877926ebb9e/python/pyspark/mllib/regression.py#L173]

These checks should print the type, not the actual object.  E.g., if the object cannot be converted to a string, then the check linked above will give a warning like this:
{code}
TypeError: not all arguments converted during string formatting
{code}
...which is weird for the user.

There may be other places in the codebase where this is an issue, so we need to check through and verify.",,31z4,apachespark,josephkb,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 17:44:31 UTC 2015,,,,,,,,,,"0|i27o9b:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"04/Apr/15 20:45;apachespark;User '31z4' has created a pull request for this issue:
https://github.com/apache/spark/pull/5361;;;","20/Apr/15 17:44;joshrosen;Issue resolved by pull request 5361
[https://github.com/apache/spark/pull/5361];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
MLLibPythonAPI.pythonToJava doesn't recognize object arrays,SPARK-6660,12787516,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,01/Apr/15 23:05,02/Apr/15 01:31,14/Jul/23 06:27,02/Apr/15 01:31,,,,,,,1.3.1,1.4.0,,,,,MLlib,PySpark,,,0,,,,,,"{code}
points = MLUtils.loadLabeledPoints(sc, ""..."")
_to_java_object_rdd(points).count()
{code}

throws exception

{code}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-22-5b481e99a111> in <module>()
----> 1 jrdd.count()

/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--> 300                     format(target_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o510.count.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 18 in stage 114.0 failed 4 times, most recent failure: Lost task 18.3 in stage 114.0 (TID 1133, ip-10-0-130-35.us-west-2.compute.internal): java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to java.util.ArrayList
	at org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:1090)
	at org.apache.spark.mllib.api.python.SerDe$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(PythonMLLibAPI.scala:1087)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1472)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1006)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1497)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 01:31:36 UTC 2015,,,,,,,,,,"0|i27o8n:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"01/Apr/15 23:14;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5318;;;","02/Apr/15 01:31;mengxr;Issue resolved by pull request 5318
[https://github.com/apache/spark/pull/5318];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
We need to read the schema of a data source table stored in spark.sql.sources.schema property,SPARK-6655,12787442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,01/Apr/15 18:54,02/Apr/15 23:02,14/Jul/23 06:27,02/Apr/15 23:02,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,Tagar,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 23:02:48 UTC 2015,,,,,,,,,,"0|i27ns7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/15 19:28;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/5313;;;","02/Apr/15 23:02;marmbrus;Issue resolved by pull request 5313
[https://github.com/apache/spark/pull/5313];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"SQLContext and HiveContext do not handle ""tricky"" names well",SPARK-6652,12787416,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,mhseiden,mhseiden,01/Apr/15 17:44,07/Oct/16 07:32,14/Jul/23 06:27,07/Oct/16 07:31,1.2.1,,,,,,,,,,,,SQL,,,,1,,,,,,"h3. Summary
There are cases where both the SQLContext and HiveContext fail when handling ""tricky names"" (containing UTF-8, tabs, newlines, etc) well. For example, the following string:

{noformat}
val tricky = ""Tricky-\u4E2D[x.][\"",/\\n * ? é\n&$(x)\t(':;#!^-Name""
{noformat}

causes the following exceptions during parsing and resolution (respectively).

h5. SQLContext parse failure
{noformat}
// pseudocode
val data = 0 until 100
val rdd = sc.parallelize(data)
val schema = StructType(StructField(Tricky, IntegerType, false) :: Nil)
val schemaRDD = sqlContext.applySchema(rdd.map(i => Row(i)), schema)
schemaRDD.registerAsTable(Tricky)
sqlContext.sql(s""select `$Tricky` from `$Tricky`"")

java.lang.RuntimeException: [1.33] failure: ``UNION'' expected but ErrorToken(``' expected but 
 found) found

select `Tricky-中[x.]["",/\n * ? é

                                ^
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:33)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:79)
	at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:79)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:174)
	at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:173)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.parsing.combinator.Parsers$$anon$2$$anonfun$apply$14.apply(Parsers.scala:891)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at scala.util.parsing.combinator.Parsers$$anon$2.apply(Parsers.scala:890)
	at scala.util.parsing.combinator.PackratParsers$$anon$1.apply(PackratParsers.scala:110)
	at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:31)
	at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:83)
	at org.apache.spark.sql.SQLContext$$anonfun$parseSql$1.apply(SQLContext.scala:83)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.SQLContext.parseSql(SQLContext.scala:83)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:303)
{noformat}

h5. HiveContext resolution failure
{noformat}
// pseudocode
val data = 0 until 100
val rdd = sc.parallelize(data)
val schema = StructType(StructField(Tricky, IntegerType, false) :: Nil)
val schemaRDD = sqlContext.applySchema(rdd.map(i => Row(i)), schema)
schemaRDD.registerAsTable(Tricky)
sqlContext.sql(s""select `$Tricky` from `$Tricky`"").collect()

// the parse is ok in this case...
15/04/01 10:41:48 WARN HiveConf: DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  Use hive.hmshandler.retry.* instead
15/04/01 10:41:48 INFO ParseDriver: Parsing command: select `Tricky-中[x.]["",/\n * ? é
&$(x)	(':;#!^-Name` from `Tricky-中[x.]["",/\n * ? é
&$(x)	(':;#!^-Name`
15/04/01 10:41:48 INFO ParseDriver: Parse Completed

// but resolution fails
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Unresolved attributes: 'Tricky-中[x.]["",/\n * ? é
&$(x)	(':;#!^-Name, tree:
'Project ['Tricky-中[x.]["",/\n * ? é
&$(x)	(':;#!^-Name]
 Subquery tricky-中[x.]["",/\n * ? é
&$(x)	(':;#!^-name
  LogicalRDD [Tricky-中[x.]["",/\n * ? é
&$(x)	(':;#!^-Name#2], MappedRDD[16] at map at <console>:30

	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$1.applyOrElse(Analyzer.scala:80)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$1.applyOrElse(Analyzer.scala:78)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:135)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$.apply(Analyzer.scala:78)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$.apply(Analyzer.scala:76)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:34)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:412)
	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:412)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:413)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:413)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:418)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:416)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:422)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:422)
	at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:444)
{noformat}",,glenn.strycker@gmail.com,mhseiden,smilegator,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 07:31:50 UTC 2016,,,,,,,,,,"0|i27nmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/16 07:31;smilegator;It should be resolved in the latest master. Please retry it. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorAllocationManager never stops,SPARK-6650,12787399,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,vanzin,vanzin,01/Apr/15 16:47,03/Apr/15 02:51,14/Jul/23 06:27,03/Apr/15 02:51,1.3.0,,,,,,1.3.1,1.4.0,,,,,Spark Core,,,,0,,,,,,"{{ExecutorAllocationManager}} doesn't even have a stop() method. That means that when the owning SparkContext goes away, the internal thread it uses to schedule its activities remains alive.

That means it constantly spams the logs and does who knows what else that could affect any future contexts that are allocated.

It's particularly evil during unit tests, since it slows down everything else after the suite is run, leaving multiple threads behind.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 17:02:37 UTC 2015,,,,,,,,,,"0|i27njr:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"01/Apr/15 17:02;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5311;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame created through SQLContext.jdbc() failed if columns table must be quoted,SPARK-6649,12787383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,frederic.blanc,frederic.blanc,01/Apr/15 15:50,08/Oct/16 19:11,14/Jul/23 06:27,08/Oct/16 19:11,1.3.0,,,,,,,,,,,,SQL,,,,0,,,,,,"If I want to import the content a table from oracle, that contains a column with name COMMENT (a reserved keyword), I cannot use a DataFrame that map all the columns of this table.

{code:title=ddl.sql|borderStyle=solid}
CREATE TABLE TEST_TABLE (
    ""COMMENT"" VARCHAR2(10)
);
{code}

{code:title=test.java|borderStyle=solid}
SQLContext sqlContext = ...

DataFrame df = sqlContext.jdbc(databaseURL, ""TEST_TABLE"");
df.rdd();   // => failed if the table contains a column with a reserved keyword
{code}

The same problem can be encounter if reserved keyword are used on table name.

The JDBCRDD scala class could be improved, if the columnList initializer append the double-quote for each column. (line : 225)


",,apachespark,frederic.blanc,freiss,gvramana,rhillegas,rxin,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8004,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Sep 25 20:13:17 UTC 2015,,,,,,,,,,"0|i27ngf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/May/15 01:33;freiss;I was able to reproduce this problem on Spark 1.3.1. The Spark SQL lexical analyzer treats anything enclosed in double quotes as a string. The Spark SQL parser only allows strings to be treated as literals, not identifiers.

On the spark-sql command line:
{noformat}
spark-sql> select ""hello"" as hello, 'world' as world;
hello	world
Time taken: 0.125 seconds, Fetched 1 row(s)

spark-sql> select ""hello"" as ""hello"", 'world' as ""world"";
15/05/04 18:03:05 ERROR SparkSQLDriver: Failed in [select ""hello"" as ""hello"", 'world' as ""world""]
org.apache.spark.sql.AnalysisException: cannot recognize input near 'as' '""hello""' ',' in selection target; line 1 pos 18
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:254)
        [many lines of stack trace]
{noformat}

The same thing happens from the Spark Scala shell:
{noformat}
scala> val df = sqlContext.sql(""select \""hello\"" as hello, 'world' as world"")
df: org.apache.spark.sql.DataFrame = [hello: string, world: string]

scala> val df2 = sqlContext.sql(""select \""hello\"" as \""hello\"", 'world' as \""world\"""")
org.apache.spark.sql.AnalysisException: cannot recognize input near 'as' '""hello""' ',' in selection target; line 1 pos 18
	at org.apache.spark.sql.hive.HiveQl$.createPlan(HiveQl.scala:254)
	at org.apache.spark.sql.hive.ExtendedHiveQlParser$$anonfun$hiveQl$1.apply(ExtendedHiveQlParser.scala:41)
        [many lines of stack trace]
{noformat}

This behavior is not consistent with the SQL standard, though I suppose it is somewhat consistent with MySQL's default behavior.

According to the grammar in the SQL-92 document ([http://www.contrib.andrew.cmu.edu/~shadow/sql/sql1992.txt], strings should be delimited by single quotes:
{noformat}
<character string literal> ::=
              [ <introducer><character set specification> ]
              <quote> [ <character representation>... ] <quote>
                [ { <separator>... <quote> [ <character representation>... ] <quote> }... ]
...
<national character string literal> ::=
              N <quote> [ <character representation>... ] <quote>
                [ { <separator>... <quote> [ <character representation>... ] <quote> }... ]

<bit string literal> ::=
              B <quote> [ <bit>... ] <quote>
                [ { <separator>... <quote> [ <bit>... ] <quote> }... ]

<hex string literal> ::=
              X <quote> [ <hexit>... ] <quote>
                [ { <separator>... <quote> [ <hexit>... ] <quote> }... ]
...
<quote> ::= '
{noformat}
and identifiers *may* be delimited with double quotes:
{noformat}
<delimited identifier> ::=
              <double quote> <delimited identifier body> <double quote>
...
<double quote> ::= ""
{noformat}

Thoughts? Are there any pull requests in flight that fix this problem already?;;;","12/May/15 23:08;freiss;Did some more digging through the code and edit history.

It looks like the current version of the SQL parser uses the backtick character (`) as a delimiter. This change first appeared in SPARK-3483, with additional fixes done as part of SPARK-6898. I don't see any explanation for the use of a nonstandard quote character, and there doesn't seem to be any relevant discussion in the mailing list archives.

I've only found two test cases under org.apache.spark.sql and child packages that use double quotes to delimit a string literal.

In SQLQuerySuite.scala:
{noformat}
test(""date row"") {
    checkAnswer(sql(
      """"""select cast(""2015-01-28"" as date) from testData limit 1""""""),
...
{noformat}

And in TableScanSuite.scala:
{noformat}
  test(""SPARK-5196 schema field with comment"") {
    sql(
      """"""
       |CREATE TEMPORARY TABLE student(name string comment ""SN"", age int comment ""SA"", grade int)
       |USING org.apache.spark.sql.sources.AllDataTypesScanSource
       |OPTIONS (
       |  from '1',
       |  to '10'
       |)
...
{noformat}

All the other test cases use single quotes per the SQL standard.

I'm going to assume that the use of double quotes to delimit string literals was an oversight and make changes to correct that oversight.;;;","16/May/15 04:39;apachespark;User 'frreiss' has created a pull request for this issue:
https://github.com/apache/spark/pull/6208;;;","25/Sep/15 20:13;rhillegas;Hi Fred,

The backtick syntax seems to be a feature of HiveQL according to this discussion on the developer list: http://apache-spark-developers-list.1001551.n3.nabble.com/column-identifiers-in-Spark-SQL-td14280.html

Thanks,
-Rick;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Reading Parquet files with different sub-files doesn't work,SPARK-6648,12787349,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,msoutier,msoutier,01/Apr/15 14:32,28/Jul/15 05:40,14/Jul/23 06:27,28/Jul/15 05:40,1.2.1,,,,,,,,,,,,SQL,,,,0,,,,,,"When reading from multiple parquet files (via sqlContext.parquetFile(/path/1.parquet,/path/2.parquet), and one of the parquet files is being overwritten using a different coalesce (e.g. one only contains part-r-1.parquet, the other also part-r-2.parquet, part-r-3.parquet), the reading fails with:

ERROR c.w.r.websocket.ParquetReader  efault-dispatcher-63 : Failed reading parquet file
java.lang.IllegalArgumentException: Could not find Parquet metadata at path <path>
at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$readMetaData$4.apply(ParquetTypes.scala:459) ~[org.apache.spark.spark-sql_2.10-1.2.1.jar:1.2.1]

	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$readMetaData$4.apply(ParquetTypes.scala:459) ~[org.apache.spark.spark-sql_2.10-1.2.1.jar:1.2.1]
	at scala.Option.getOrElse(Option.scala:120) ~[org.scala-lang.scala-library-2.10.4.jar:na]
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.readMetaData(ParquetTypes.scala:458) ~[org.apache.spark.spark-sql_2.10-1.2.1.jar:1.2.1]
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.readSchemaFromFile(ParquetTypes.scala:477) ~[org.apache.spark.spark-sql_2.10-1.2.1.jar:1.2.1]
	at org.apache.spark.sql.parquet.ParquetRelation.<init>(ParquetRelation.scala:65) ~[org.apache.spark.spark-sql_2.10-1.2.1.jar:1.2.1]
	at org.apache.spark.sql.SQLContext.parquetFile(SQLContext.scala:165) ~[org.apache.spark.spark-sql_2.10-1.2.1.jar:1.2.1]

I haven't tested with Spark 1.3 yet but will report back after upgrading to 1.3.1 (as soon as it's released).
",,marmbrus,msoutier,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 28 05:40:38 UTC 2015,,,,,,,,,,"0|i27n9b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/15 05:40;marmbrus;I think this has been fixed by a rewrite of our parquet functionality.  Please reopen if you can still reproduce using Spark master (or 1.5-RC1 when that comes out).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make trait StringComparison as BinaryPredicate and throw error when Predicate can't translate to data source Filter,SPARK-6647,12787317,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,01/Apr/15 11:16,24/Apr/15 00:30,14/Jul/23 06:27,03/Apr/15 19:35,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Now trait {{StringComparison}} is a {{BinaryExpression}}. In fact, it should be a {{BinaryPredicate}}.

By making {{StringComparison}} as {{BinaryPredicate}}, we can throw error when a {{expressions.Predicate}} can't translate to a data source {{Filter}} in function {{selectFilters}}.

Without this modification, because we will wrap a {{Filter}} outside the scanned results in {{pruneFilterProjectRaw}}, we can't detect about something is wrong in translating predicates to filters in {{selectFilters}}.

The unit test of SPARK-6625 demonstrates such problem. In that pr, even {{expressions.Contains}} is not properly translated to {{sources.StringContains}}, the filtering is still performed by the {{Filter}} and so the test passes.

Of course, by doing this modification, all {{expressions.Predicate}} classes need to have its data source {{Filter}} correspondingly.
",,apachespark,marmbrus,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 19:35:37 UTC 2015,,,,,,,,,,"0|i27n27:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/15 11:18;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5309;;;","03/Apr/15 19:35;marmbrus;Issue resolved by pull request 5309
[https://github.com/apache/spark/pull/5309];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the lambda weight to number of explicit ratings in implicit ALS,SPARK-6642,12787238,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,01/Apr/15 04:07,01/Apr/15 23:55,14/Jul/23 06:27,01/Apr/15 23:55,1.3.0,,,,,,1.3.1,1.4.0,,,,,MLlib,,,,0,,,,,,"Until SPARK-6637 is resolved, we should switch back to the 1.2 lambda weighting strategy to be consistent.",,apachespark,josephkb,mengxr,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6637,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 23:55:13 UTC 2015,,,,,,,,,,"0|i27mkv:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"01/Apr/15 20:44;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5314;;;","01/Apr/15 23:55;mengxr;Issue resolved by pull request 5314
[https://github.com/apache/spark/pull/5314];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Executor may connect to HeartbeartReceiver before it's setup in the driver side,SPARK-6640,12787230,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,01/Apr/15 02:56,03/Apr/15 18:45,14/Jul/23 06:27,03/Apr/15 18:45,1.3.0,,,,,,1.4.0,,,,,,Spark Core,,,,1,,,,,,"Here is the current code about starting LocalBackend and creating HeartbeatReceiver:

{code}
  // Create and start the scheduler
  private[spark] var (schedulerBackend, taskScheduler) =
    SparkContext.createTaskScheduler(this, master)
  private val heartbeatReceiver = env.actorSystem.actorOf(
    Props(new HeartbeatReceiver(this, taskScheduler)), ""HeartbeatReceiver"")
{code}

When creating LocalBackend, it will start `LocalActor`. `LocalActor` will   create Executor, and Executor's constructor will retrieve `HeartbeatReceiver`.

So we should make sure this line:
{code}
private val heartbeatReceiver = env.actorSystem.actorOf(
    Props(new HeartbeatReceiver(this, taskScheduler)), ""HeartbeatReceiver"")
{code}
happen before ""creating LocalActor"".

However, current codes can not guarantee that. Sometimes, creating Executor will crash. The issue was reported by sparkdi <shopaddr1234@dubna.us> in http://apache-spark-user-list.1001560.n3.nabble.com/Actor-not-found-td22265.html#a22324
",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 01 06:47:01 UTC 2015,,,,,,,,,,"0|i27mjb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"01/Apr/15 06:47;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5306;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use public DNS hostname everywhere in spark_ec2.py,SPARK-6636,12787180,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,aasted,aasted,aasted,31/Mar/15 22:26,09/Apr/15 20:48,14/Jul/23 06:27,07/Apr/15 06:53,,,,,,,1.3.2,1.4.0,,,,,EC2,,,,0,,,,,,"The spark_ec2.py script uses public_dns_name everywhere in the script except for testing ssh availability, which is done using the public ip address of the instances. This breaks the script for users who are deploying the cluster with a private-network-only security group. The fix is to use public_dns_name in the remaining place.

I am submitting a pull-request alongside this bug report.",,aasted,apachespark,joshrosen,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 20:48:49 UTC 2015,,,,,,,,,,"0|i27m87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/15 22:30;apachespark;User 'aasted' has created a pull request for this issue:
https://github.com/apache/spark/pull/5302;;;","07/Apr/15 06:53;joshrosen;Issue resolved by pull request 5302
[https://github.com/apache/spark/pull/5302];;;","09/Apr/15 20:48;nchammas;[~aasted] - Can you elaborate on this? I haven't used private-network-only security groups before. Why wouldn't the IP address work if that kind of security group is used?

Just curious since, naively speaking, the public IP and public DNS name should always be interchangeable.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame.withColumn can create columns with identical names,SPARK-6635,12787090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,josephkb,josephkb,31/Mar/15 17:21,27/Jun/17 01:43,14/Jul/23 06:27,21/Apr/15 01:54,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"DataFrame lets you create multiple columns with the same name, which causes problems when you try to refer to columns by name.

Proposal: If a column is added to a DataFrame with a column of the same name, then the new column should replace the old column.

{code}
scala> val df = sc.parallelize(Array(1,2,3)).toDF(""x"")
df: org.apache.spark.sql.DataFrame = [x: int]

scala> val df3 = df.withColumn(""x"", df(""x"") + 1)
df3: org.apache.spark.sql.DataFrame = [x: int, x: int]

scala> df3.collect()
res1: Array[org.apache.spark.sql.Row] = Array([1,2], [2,3], [3,4])

scala> df3(""x"")
org.apache.spark.sql.AnalysisException: Reference 'x' is ambiguous, could be: x, x.;
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:216)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:121)
	at org.apache.spark.sql.DataFrame.resolve(DataFrame.scala:161)
	at org.apache.spark.sql.DataFrame.col(DataFrame.scala:436)
	at org.apache.spark.sql.DataFrame.apply(DataFrame.scala:426)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:31)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:33)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:35)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:37)
	at $iwC$$iwC$$iwC.<init>(<console>:39)
	at $iwC$$iwC.<init>(<console>:41)
	at $iwC.<init>(<console>:43)
	at <init>(<console>:45)
	at .<init>(<console>:49)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:856)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:901)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:813)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:656)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:664)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:669)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:996)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:944)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1058)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}
",,apachespark,calstad,josephkb,marmbrus,rakeshchalasani,rxin,yichuan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6634,SPARK-10073,SPARK-12204,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 27 01:43:28 UTC 2017,,,,,,,,,,"0|i27lp3:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"16/Apr/15 10:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5541;;;","16/Apr/15 15:37;rxin;Should we throw an exception if the name is identical, or just replace it? For example, what happens if the user does
{code}
df.select(df(""*""), lit(1).as(""col""))
{code}

If df already contains a column named ""col""?;;;","16/Apr/15 17:17;josephkb;[~rxin] Your select statement does highlight how the correct behavior is ambiguous.  What is the best way to replace 1 column while leaving all others the same?  That seems like a useful operation.  (Right now, I can only think of getting all of the column names, removing the one being replaced, and using that list in a select statement.);;;","16/Apr/15 21:03;rxin;cc [~marmbrus] to chime in.

I think about it more, and withName should probably overwrite an existing column (or maybe with an argument to control the behavior?). However, we might want a broader discussion about column names also.
;;;","16/Apr/15 21:09;rakeshchalasani;Join over to two data frames also leads to a similar issue, if the two frames have same named columns.

I can create separate JIRA issue for this, but the resolution should follow a similar behavior I believe.;;;","16/Apr/15 21:10;josephkb;Btw, when I say ""that seems like a useful operation,"" I really mean that I wanted to do that for MLlib.;;;","21/Apr/15 00:33;marmbrus;+1 to {{withColumn}} overwriting existing columns.;;;","21/Apr/15 01:02;josephkb;Just to clarify, does that mean {{withColumn}} does *not* replace columns, but {{withName}} does?  (I'm not sure what {{withName}} is.);;;","21/Apr/15 01:07;marmbrus;Sorry, updated.  I meant {{withColumn}}.;;;","21/Apr/15 01:54;marmbrus;Issue resolved by pull request 5541
[https://github.com/apache/spark/pull/5541];;;","17/Aug/15 21:10;calstad;This still seems to be an issue in the pyspark implementation of {{withColumn}};;;","17/Aug/15 23:20;marmbrus;Filed [SPARK-10073] for python.;;;","27/Jun/17 01:43;yichuan;withColumn have this strange behavior with join, it replaces both columns from the left and right side of the join, and replace them with the new column (Spark 2.0.2)

{code:java}
val left = Seq((1, ""one"", null, ""p1""), (2, ""two"", ""b"", null)).toDF(""id"", ""name"", ""note"", ""note2"")
val right = Seq((1, ""c"", 11, ""n1""), (2,  null, 22, ""n2"")).toDF(""id"",  ""note"", ""seq"", ""note2"")
val j = left.join(right, ""id"")
j.show()
{code}

{code:java}
+---+----+----+-----+----+---+-----+
| id|name|note|note2|note|seq|note2|
+---+----+----+-----+----+---+-----+
|  1| one|null|   p1|   c| 11|   n1|
|  2| two|   b| null|null| 22|   n2|
+---+----+----+-----+----+---+-----+
{code}


{code:java}
val k = j.withColumn(""note"", coalesce(left(""note""), right(""note"")))
k.show()
{code}


{code:java}
+---+----+----+-----+----+---+-----+
| id|name|note|note2|note|seq|note2|
+---+----+----+-----+----+---+-----+
|  1| one|   c|   p1|   c| 11|   n1|
|  2| two|   b| null|   b| 22|   n2|
+---+----+----+-----+----+---+-----+
{code}


{code:java}
val l = k.drop(""note"")
l.show()
{code}



{code:java}
+---+----+-----+---+-----+
| id|name|note2|seq|note2|
+---+----+-----+---+-----+
|  1| one|   p1| 11|   n1|
|  2| two| null| 22|   n2|
+---+----+-----+---+-----+
{code}




;;;",,,,,,,,,,,,,,,,
"Should be ""Contains"" instead of ""EndsWith"" when constructing sources.StringContains",SPARK-6633,12787064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,viirya,viirya,31/Mar/15 15:50,31/Mar/15 20:18,14/Jul/23 06:27,31/Mar/15 20:18,,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,,,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 15:52:08 UTC 2015,,,,,,,,,,"0|i27ljj:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"31/Mar/15 15:52;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5299;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
cancelJobGroup() may not work for jobs whose job groups are inherited from parent threads,SPARK-6629,12786958,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,joshrosen,31/Mar/15 08:16,29/Apr/15 20:32,14/Jul/23 06:27,29/Apr/15 20:32,1.0.2,1.1.2,1.2.2,1.3.0,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"When a job is submitted with a job group and that job group is inherited from a parent thread, there are multiple bugs that may prevent this job from being cancelable via SparkContext.cancelJobGroup():

- When filtering jobs based on their job group properties, DAGScheduler calls get() instead of getProperty(), which does not respect inheritance, so it will skip over jobs whose groups were inherited.
- Properties objects are mutable, but we do not make defensive copies / snapshots, so modifications of the parent thread's job group will cause running jobs' groups to change.

Both of these issues are easy to fix: use `getProperty()` and perform defensive copying.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6862,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 09:10:52 UTC 2015,,,,,,,,,,"0|i27kwn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/15 08:21;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5288;;;","31/Mar/15 09:10;joshrosen;This may very well be ""not an issue"", pending discussion: https://github.com/apache/spark/pull/5288#issuecomment-87996678;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling EventLoop.stop in EventLoop.onReceive and EventLoop.onError should call onStop,SPARK-6621,12786907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,31/Mar/15 03:26,03/Apr/15 05:55,14/Jul/23 06:27,03/Apr/15 05:55,1.3.0,,,,,,1.3.1,1.4.0,,,,,Spark Core,,,,0,,,,,,,,apachespark,joshrosen,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 05:55:29 UTC 2015,,,,,,,,,,"0|i27klb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/15 03:32;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5280;;;","03/Apr/15 05:55;joshrosen;Issue resolved by pull request 5280
[https://github.com/apache/spark/pull/5280];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"OutputCommitCoordinator should clear authorized committers only after authorized committer fails, not after any failure",SPARK-6614,12786848,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,30/Mar/15 22:05,17/May/20 17:47,14/Jul/23 06:27,31/Mar/15 23:22,1.3.0,1.3.1,1.4.0,,,,1.3.1,1.4.0,,,,,Scheduler,Spark Core,,,0,,,,,,"In OutputCommitCoordinator, there is some logic to clear the authorized committer's lock on committing in case it fails.  However, it looks like the current code also clears this lock if _other_ tasks fail, which is an obvious bug: https://github.com/apache/spark/blob/df3550084c9975f999ed370dd9f7c495181a68ba/core/src/main/scala/org/apache/spark/scheduler/OutputCommitCoordinator.scala#L118.  In theory, it's possible that this could allow a new committer to start, run to completion, and commit output before the authorized committer finished, but it's unlikely that this race occurs often in practice due to the complex combination of failure and timing conditions that would be required to expose it.  Still, we should fix this issue.

This was discovered by [~adav] while reading the OutputCommitCoordinator code.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 22:38:26 UTC 2015,,,,,,,,,,"0|i27k87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/15 22:38;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5276;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Aggregation attribute name including special chars '(' and ')' should be replaced before generating Parquet schema,SPARK-6607,12786677,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,30/Mar/15 11:07,27/May/15 19:25,14/Jul/23 06:27,04/Apr/15 16:24,1.1.1,1.2.1,1.3.0,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"'(' and ')' are special characters used in Parquet schema for type annotation. When we run an aggregation query, we will obtain attribute name such as ""MAX(a)"".

If we directly store the generated DataFrame as Parquet file, it causes failure when reading and parsing the stored schema string.

Several methods can be adopted to solve this. This pr uses a simplest one to just replace attribute names before generating Parquet schema based on these attributes.

Another possible method might be modifying all aggregation expression names from ""func(column)"" to ""func[column]"".
",,apachespark,lian cheng,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5192,,,SPARK-4521,,SPARK-4521,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Apr 04 16:24:39 UTC 2015,,,,,,,,,,"0|i27j6v:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"30/Mar/15 11:08;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5263;;;","04/Apr/15 16:24;lian cheng;Issue resolved by pull request 5263
[https://github.com/apache/spark/pull/5263];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
fix the instruction on building scaladoc ,SPARK-6596,12786573,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,codingcat,codingcat,codingcat,29/Mar/15 22:38,30/Mar/15 10:42,14/Jul/23 06:27,30/Mar/15 10:41,1.4.0,,,,,,1.4.0,,,,,,Documentation,,,,0,,,,,,"In README.md under docs/ directory, it says that 

You can build just the Spark scaladoc by running build/sbt doc from the SPARK_PROJECT_ROOT directory.

I guess the right approach is build/sbt unidoc",,apachespark,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 10:41:55 UTC 2015,,,,,,,,,,"0|i27ijz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/15 22:42;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/5253;;;","30/Mar/15 10:41;srowen;Issue resolved by pull request 5253
[https://github.com/apache/spark/pull/5253];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
DataFrame self joins with MetastoreRelations fail,SPARK-6595,12786552,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,29/Mar/15 20:08,15/May/15 19:19,14/Jul/23 06:27,30/Mar/15 14:37,1.3.0,,,,,,1.3.2,1.4.0,,,,,SQL,,,,0,,,,,,,,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7088,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 14:37:17 UTC 2015,,,,,,,,,,"0|i27ifb:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"29/Mar/15 20:11;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5251;;;","30/Mar/15 14:37;lian cheng;Fixed by https://github.com/apache/spark/pull/5251;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
API of Row trait should be presented in Scala doc,SPARK-6592,12786499,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,codingcat,codingcat,codingcat,29/Mar/15 03:46,30/Mar/15 18:56,14/Jul/23 06:27,30/Mar/15 18:56,1.3.0,,,,,,1.3.1,1.4.0,,,,,Documentation,SQL,,,0,,,,,,"Currently, the API of Row class is not presented in Scaladoc, though we have many chances to use it 

the reason is that we ignore all files under catalyst directly in SparkBuild.scala when generating Scaladoc, (https://github.com/apache/spark/blob/f75f633b21faaf911f04aeff847f25749b1ecd89/project/SparkBuild.scala#L369)

What's the best approach to fix this? [~rxin]",,apachespark,codingcat,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 29 22:34:04 UTC 2015,,,,,,,,,,"0|i27i3j:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"29/Mar/15 03:48;codingcat;also cc: [~lian cheng] [~marmbrus];;;","29/Mar/15 03:48;rxin;Can you try change that line to 

spark/sql/catalyst?

then it should only filter out the catalyst package, but not the catalyst module.
;;;","29/Mar/15 11:50;codingcat;?

I don't think that makes any difference, as the path of Row.scala still contains ""spark/sql/catalyst""?

I also tried to rerun build/sbt doc, the same thing...

maybe we need to hack SparkBuild.scala to exclude Row.scala?;;;","29/Mar/15 19:05;rxin;Row.html/class doesn't contain the word catalyst, does it?

./api/java/org/apache/spark/sql/Row.html

;;;","29/Mar/15 21:13;codingcat;it contains....

the reason is that the input of that line is ""file.getCanonicalPath""...which output the absolute path

e.g.

{code}

scala> val f = new java.io.File(""Row.class"")
f: java.io.File = Row.class

scala> f.getCanonicalPath
res0: String = /Users/nanzhu/code/spark/sql/catalyst/target/scala-2.10/classes/org/apache/spark/sql/Row.class

{code}
;;;","29/Mar/15 21:26;rxin;Ok then can't you just add ""apache"" to it?
;;;","29/Mar/15 22:34;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/5252;;;",,,,,,,,,,,,,,,,,,,,,,
"FileServerSuite.test (""HttpFileServer should not work with SSL when the server is untrusted"") failed is some evn.",SPARK-6585,12786439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,June.He,June.He,June.He,28/Mar/15 09:39,29/Mar/15 11:47,14/Jul/23 06:27,29/Mar/15 11:47,1.3.0,,,,,,1.4.0,,,,,,Tests,,,30/Mar/15 00:00,0,,,,,,"In my test machine, FileServerSuite.test (""HttpFileServer should not work with SSL when the server is untrusted"") case throw SSLException not SSLHandshakeException, suggest change to catch SSLException to  improve test case 's robustness.

[info] - HttpFileServer should not work with SSL when the server is untrusted *** FAILED *** (69 milliseconds)
[info]   Expected exception javax.net.ssl.SSLHandshakeException to be thrown, but javax.net.ssl.SSLException was thrown. (FileServerSuite.scala:231)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:496)
[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
[info]   at org.scalatest.Assertions$class.intercept(Assertions.scala:1004)
[info]   at org.scalatest.FunSuite.intercept(FunSuite.scala:1555)
[info]   at org.apache.spark.FileServerSuite$$anonfun$15.apply$mcV$sp(FileServerSuite.scala:231)
[info]   at org.apache.spark.FileServerSuite$$anonfun$15.apply(FileServerSuite.scala:224)
[info]   at org.apache.spark.FileServerSuite$$anonfun$15.apply(FileServerSuite.scala:224)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.apache.spark.FileServerSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(FileServerSuite.scala:34)
[info]   at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)",,apachespark,June.He,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 29 11:47:40 UTC 2015,,,,,,,,,,"0|i27hqf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/15 09:58;apachespark;User 'sisihj' has created a pull request for this issue:
https://github.com/apache/spark/pull/5239;;;","29/Mar/15 11:47;srowen;Issue resolved by pull request 5239
[https://github.com/apache/spark/pull/5239];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Outbound channel in network library is not thread-safe, can lead to fetch failures",SPARK-6578,12786313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,vanzin,vanzin,27/Mar/15 19:45,02/Apr/15 21:51,14/Jul/23 06:27,01/Apr/15 23:07,1.3.0,,,,,,1.2.2,1.3.1,1.4.0,,,,Spark Core,,,,0,,,,,,"There is a very narrow race in the outbound channel of the network library. While netty guarantees that the inbound channel is thread-safe, the same is not true for the outbound channel: multiple threads can be writing and running the pipeline at the same time.

This leads to an issue with MessageEncoder and the optimization it performs for zero-copy of file data: since a single RPC can be broken into multiple buffers (for , example when replying to a chunk request), if you have multiple threads writing these RPCs then they can be mixed up in the final socket. That breaks framing and will cause the receiving side to not understand the messages.

Patch coming up shortly.",,apachespark,rxin,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 20:08:04 UTC 2015,,,,,,,,,,"0|i27gyn:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.1,1.4.0,,,,,,,,,,,"27/Mar/15 19:49;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5234;;;","01/Apr/15 23:21;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5319;;;","01/Apr/15 23:24;rxin;We should patch 1.2.x too. [~vanzin] mind submitting a patch for that branch?
;;;","02/Apr/15 20:08;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5336;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Converted Parquet Metastore tables no longer cache metadata,SPARK-6575,12786296,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,27/Mar/15 18:54,03/Apr/15 03:23,14/Jul/23 06:27,03/Apr/15 03:23,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"Consider a metastore Parquet table that
# doesn't have schema evolution issue
# has lots of data files and/or partitions

In this case, driver schema merging can be both slow and unnecessary. Would be good to have a configuration to let the use disable schema merging when converting such a metastore Parquet table.",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 01:19:57 UTC 2015,,,,,,,,,,"0|i27guv:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"27/Mar/15 18:57;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5231;;;","31/Mar/15 18:22;marmbrus;Issue resolved by pull request 5231
[https://github.com/apache/spark/pull/5231];;;","03/Apr/15 01:19;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/5339;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Python Example sql.py not working in version 1.3,SPARK-6574,12786257,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,27/Mar/15 16:58,27/Mar/15 18:42,14/Jul/23 06:27,27/Mar/15 18:42,1.3.0,,,,,,1.3.1,1.4.0,,,,,PySpark,SQL,,,0,,,,,,"I downloaded spark version spark-1.3.0-bin-hadoop2.4.

When the python version of sql.py is run the following error occurs:

[root@nde-dev8-template python]#
/root/spark-1.3.0-bin-hadoop2.4/bin/spark-submit sql.py
Spark assembly has been built with Hive, including Datanucleus jars on
classpath
Traceback (most recent call last):
  File ""/root/spark-1.3.0-bin-hadoop2.4/examples/src/main/python/sql.py"",
line 22, in <module>
    from pyspark.sql import Row, StructField, StructType, StringType,
IntegerType
ImportError: cannot import name StructField

----------------------------------------------------------------------
The sql.py version, spark-1.2.1-bin-hadoop2.4, does not throw the error:

[root@nde-dev8-template python]#
/root/spark-1.2.1-bin-hadoop2.4/bin/spark-submit sql.py
Spark assembly has been built with Hive, including Datanucleus jars on
classpath
15/03/27 14:18:44 WARN NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable
15/03/27 14:19:41 WARN ThreadLocalRandom: Failed to generate a seed from
SecureRandom within 3 seconds. Not enough entrophy?
root
 |-- age: integer (nullable = true)
 |-- name: string (nullable = true)

root
 |-- person_name: string (nullable = false)
 |-- person_age: integer (nullable = false)

root
 |-- age: integer (nullable = true)
 |-- name: string (nullable = true)

Justin


-------------------------------------------------

The OS/JAVA environments are:

OS: Linux nde-dev8-template 2.6.32-431.17.1.el6.x86_64 #1 SMP Fri Apr 11
17:27:00 EDT 2014 x86_64 x86_64 x86_64 GNU/Linux

JAVA: [root@nde-dev8-template bin]# java -version
java version ""1.7.0_51""
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)

The same error occurs when using bin/pyspark shell.

>>> from pyspark.sql import StructField
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ImportError: cannot import name StructField


---------------------------------------------------

Any advice for resolving? Thanks in advance.

Peter
",,apachespark,davies,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 18:42:45 UTC 2015,,,,,,,,,,"0|i27gm7:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"27/Mar/15 17:10;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/5230;;;","27/Mar/15 18:42;marmbrus;Issue resolved by pull request 5230
[https://github.com/apache/spark/pull/5230];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
MatrixFactorizationModel created by load fails on predictAll,SPARK-6571,12786201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,cchayden,cchayden,27/Mar/15 13:38,30/Mar/15 21:28,14/Jul/23 06:27,30/Mar/15 21:28,1.3.0,,,,,,1.3.1,1.4.0,,,,,MLlib,PySpark,,,0,,,,,,"This code, adapted from the documentation, fails when using a loaded model.
from pyspark.mllib.recommendation import ALS, Rating, MatrixFactorizationModel

r1 = (1, 1, 1.0)
r2 = (1, 2, 2.0)
r3 = (2, 1, 2.0)
ratings = sc.parallelize([r1, r2, r3])
model = ALS.trainImplicit(ratings, 1, seed=10)
print '(2, 2)', model.predict(2, 2)
#    0.43...
testset = sc.parallelize([(1, 2), (1, 1)])
print 'all', model.predictAll(testset).collect()
#    [Rating(user=1, product=1, rating=1.0...), Rating(user=1, product=2, rating=1.9...)]
import os, tempfile
path = tempfile.mkdtemp()
model.save(sc, path)
sameModel = MatrixFactorizationModel.load(sc, path)
print '(2, 2)', sameModel.predict(2,2)
sameModel.predictAll(testset).collect()


This gives
(2, 2) 0.443547642944
all [Rating(user=1, product=1, rating=1.1538351103381217), Rating(user=1, product=2, rating=0.7153473708381739)]
(2, 2) 0.443547642944
---------------------------------------------------------------------------
Py4JError                                 Traceback (most recent call last)
<ipython-input-18-af6612bed9d0> in <module>()
     19 sameModel = MatrixFactorizationModel.load(sc, path)
     20 print '(2, 2)', sameModel.predict(2,2)
---> 21 sameModel.predictAll(testset).collect()
     22 

/home/ubuntu/spark/python/pyspark/mllib/recommendation.pyc in predictAll(self, user_product)
    104         assert len(first) == 2, ""user_product should be RDD of (user, product)""
    105         user_product = user_product.map(lambda (u, p): (int(u), int(p)))
--> 106         return self.call(""predict"", user_product)
    107 
    108     def userFeatures(self):

/home/ubuntu/spark/python/pyspark/mllib/common.pyc in call(self, name, *a)
    134     def call(self, name, *a):
    135         """"""Call method of java_model""""""
--> 136         return callJavaFunc(self._sc, getattr(self._java_model, name), *a)
    137 
    138 

/home/ubuntu/spark/python/pyspark/mllib/common.pyc in callJavaFunc(sc, func, *args)
    111     """""" Call Java Function """"""
    112     args = [_py2java(sc, a) for a in args]
--> 113     return _java2py(sc, func(*args))
    114 
    115 

/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __call__(self, *args)
    536         answer = self.gateway_client.send_command(command)
    537         return_value = get_return_value(answer, self.gateway_client,
--> 538                 self.target_id, self.name)
    539 
    540         for temp_arg in temp_args:

/home/ubuntu/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    302                 raise Py4JError(
    303                     'An error occurred while calling {0}{1}{2}. Trace:\n{3}\n'.
--> 304                     format(target_id, '.', name, value))
    305         else:
    306             raise Py4JError(

Py4JError: An error occurred while calling o450.predict. Trace:
py4j.Py4JException: Method predict([class org.apache.spark.api.java.JavaRDD]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)
	at py4j.Gateway.invoke(Gateway.java:252)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:744)
",,apachespark,cchayden,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 17:02:42 UTC 2015,,,,,,,,,,"0|i27g9r:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"27/Mar/15 19:48;josephkb;Thanks for the detailed report!  I was able to reproduce the bug too.  Looking into it...;;;","28/Mar/15 17:02;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5243;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Spark SQL arrays: ""explode()"" fails and cannot save array type to Parquet",SPARK-6570,12786173,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,jonchase,jonchase,27/Mar/15 11:34,07/Oct/16 22:44,14/Jul/23 06:27,07/Oct/16 22:44,1.3.0,,,,,,,,,,,,SQL,,,,1,,,,,,"{code}
    @Rule
    public TemporaryFolder tmp = new TemporaryFolder();

    @Test
    public void testPercentileWithExplode() throws Exception {
        StructType schema = DataTypes.createStructType(Lists.newArrayList(
                DataTypes.createStructField(""col1"", DataTypes.StringType, false),
                DataTypes.createStructField(""col2s"", DataTypes.createArrayType(DataTypes.IntegerType, true), true)
        ));

        JavaRDD<Row> rowRDD = sc.parallelize(Lists.newArrayList(
                RowFactory.create(""test"", new int[]{1, 2, 3})
        ));

        DataFrame df = sql.createDataFrame(rowRDD, schema);
        df.registerTempTable(""df"");
        df.printSchema();

        List<int[]> ints = sql.sql(""select col2s from df"").javaRDD()
                              .map(row -> (int[]) row.get(0)).collect();
        assertEquals(1, ints.size());
        assertArrayEquals(new int[]{1, 2, 3}, ints.get(0));


        // fails: lateral view explode does not work: java.lang.ClassCastException: [I cannot be cast to scala.collection.Seq
        List<Integer> explodedInts = sql.sql(""select col2 from df lateral view explode(col2s) splode as col2"").javaRDD()
                                        .map(row -> row.getInt(0)).collect();
        assertEquals(3, explodedInts.size());
        assertEquals(Lists.newArrayList(1, 2, 3), explodedInts);


        // fails: java.lang.ClassCastException: [I cannot be cast to scala.collection.Seq
        df.saveAsParquetFile(tmp.getRoot().getAbsolutePath() + ""/parquet"");


        DataFrame loadedDf = sql.load(tmp.getRoot().getAbsolutePath() + ""/parquet"");
        loadedDf.registerTempTable(""loadedDf"");
        List<int[]> moreInts = sql.sql(""select col2s from loadedDf"").javaRDD()
                                  .map(row -> (int[]) row.get(0)).collect();
        assertEquals(1, moreInts.size());
        assertArrayEquals(new int[]{1, 2, 3}, moreInts.get(0));
    }
{code}


{code}
root
 |-- col1: string (nullable = false)
 |-- col2s: array (nullable = true)
 |    |-- element: integer (containsNull = true)

ERROR org.apache.spark.executor.Executor Exception in task 7.0 in stage 1.0 (TID 15)
java.lang.ClassCastException: [I cannot be cast to scala.collection.Seq
	at org.apache.spark.sql.catalyst.expressions.Explode.eval(generators.scala:125) ~[spark-catalyst_2.10-1.3.0.jar:1.3.0]
	at org.apache.spark.sql.execution.Generate$$anonfun$2$$anonfun$apply$1.apply(Generate.scala:70) ~[spark-sql_2.10-1.3.0.jar:1.3.0]
	at org.apache.spark.sql.execution.Generate$$anonfun$2$$anonfun$apply$1.apply(Generate.scala:69) ~[spark-sql_2.10-1.3.0.jar:1.3.0]
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371) ~[scala-library-2.10.4.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) ~[scala-library-2.10.4.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) ~[scala-library-2.10.4.jar:na]
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327) ~[scala-library-2.10.4.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:727) ~[scala-library-2.10.4.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) ~[scala-library-2.10.4.jar:na]
{code}",,jonchase,k.shaposhnikov@gmail.com,lian cheng,smilegator,smolav,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 22:44:23 UTC 2016,,,,,,,,,,"0|i27g3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/15 13:15;jonchase;Stack trace for saveAsParquetFile():

{code}
root
 |-- col1: string (nullable = false)
 |-- col2s: array (nullable = true)
 |    |-- element: integer (containsNull = true)

SLF4J: Failed to load class ""org.slf4j.impl.StaticLoggerBinder"".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
ERROR org.apache.spark.executor.Executor Exception in task 7.0 in stage 1.0 (TID 15)
java.lang.ClassCastException: [I cannot be cast to scala.collection.Seq
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:185) ~[spark-sql_2.10-1.3.0.jar:1.3.0]
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:171) ~[spark-sql_2.10-1.3.0.jar:1.3.0]
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:134) ~[spark-sql_2.10-1.3.0.jar:1.3.0]
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120) ~[parquet-hadoop-1.6.0rc3.jar:na]
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81) ~[parquet-hadoop-1.6.0rc3.jar:na]
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37) ~[parquet-hadoop-1.6.0rc3.jar:na]
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:631) ~[spark-sql_2.10-1.3.0.jar:1.3.0]
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:648) ~[spark-sql_2.10-1.3.0.jar:1.3.0]
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:648) ~[spark-sql_2.10-1.3.0.jar:1.3.0]
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61) ~[spark-core_2.10-1.3.0.jar:1.3.0]
	at org.apache.spark.scheduler.Task.run(Task.scala:64) ~[spark-core_2.10-1.3.0.jar:1.3.0]
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203) ~[spark-core_2.10-1.3.0.jar:1.3.0]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [na:1.8.0_31]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_31]
	at java.lang.Thread.run(Thread.java:745) [na:1.8.0_31]
WARN  o.a.spark.scheduler.TaskSetManager Lost task 7.0 in stage 1.0 (TID 15, localhost): java.lang.ClassCastException: [I cannot be cast to scala.collection.Seq
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:185)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:171)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:134)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:631)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:648)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:648)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

ERROR o.a.spark.scheduler.TaskSetManager Task 7 in stage 1.0 failed 1 times; aborting job

org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 1.0 failed 1 times, most recent failure: Lost task 7.0 in stage 1.0 (TID 15, localhost): java.lang.ClassCastException: [I cannot be cast to scala.collection.Seq
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:185)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:171)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:134)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:631)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:648)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:648)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1203)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1191)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1191)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code};;;","27/May/15 22:49;yhuai;[~jonchase] Can you test our Spark 1.4 branch? I think it has been fixed in 1.4.;;;","07/Oct/16 22:44;smilegator;Thanks! If you still hit the issue, please feel free to reopen it. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
spark-shell.cmd --jars option does not accept the jar that has space in its path,SPARK-6568,12786131,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tsudukim,tsudukim,tsudukim,27/Mar/15 09:14,14/May/15 01:05,14/Jul/23 06:27,13/May/15 08:44,1.3.0,,,,,,1.4.0,,,,,,Spark Shell,,,,0,,,,,,"spark-shell.cmd --jars option does not accept the jar that has space in its path.
The path of jar sometimes containes space in Windows.

{code}
bin\spark-shell.cmd --jars ""C:\Program Files\some\jar1.jar""
{code}
this gets
{code}
Exception in thread ""main"" java.net.URISyntaxException: Illegal character in path at index 10: C:/Program Files/some/jar1.jar
{code}",Windows 8.1,apachespark,jarlhaggerty,stevel@apache.org,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 13 08:44:01 UTC 2015,,,,,,,,,,"0|i27fuf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/15 11:04;stevel@apache.org;Can you show the full stack trace?;;;","03/Apr/15 09:30;tsudukim;{code}
bin\spark-shell.cmd --jars ""C:\Program Files\some\jar1.jar""
{code}
{code}
Exception in thread ""main"" java.net.URISyntaxException: Illegal character in path at index 10: C:/Program Files/some/jar1.jar
        at java.net.URI$Parser.fail(URI.java:2829)
        at java.net.URI$Parser.checkChars(URI.java:3002)
        at java.net.URI$Parser.parseHierarchical(URI.java:3086)
        at java.net.URI$Parser.parse(URI.java:3034)
        at java.net.URI.<init>(URI.java:595)
        at org.apache.spark.util.Utils$.resolveURI(Utils.scala:1721)
        at org.apache.spark.util.Utils$$anonfun$resolveURIs$1.apply(Utils.scala:1745)
        at org.apache.spark.util.Utils$$anonfun$resolveURIs$1.apply(Utils.scala:1745)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        at org.apache.spark.util.Utils$.resolveURIs(Utils.scala:1745)
        at org.apache.spark.deploy.SparkSubmitArguments.handle(SparkSubmitArguments.scala:367)
        at org.apache.spark.launcher.SparkSubmitOptionParser.parse(SparkSubmitOptionParser.java:155)
        at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:92)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:105)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/04/02 14:23:46 DEBUG Utils: Shutdown hook called
{code};;;","03/Apr/15 09:33;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/5347;;;","10/Apr/15 05:20;tsudukim;I removed ""Windows"" from Components since this is not the problem only for Windows. Same problem also occurs in Linux.;;;","10/Apr/15 06:12;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/5447;;;","11/Apr/15 23:18;jarlhaggerty;When I try to run the following command the back slashes in the file names get removed and I get an IllegalArgumentException.   This is under both Spark 1.3.0 and 1.2.1.

PS C:\Users\jarlhaggerty> C:\spark\bin\pyspark.cmd --master local --jars C:\Users\jarlhaggerty\Miniconda3\envs\py27\Lib\
site-packages\thunder\lib\thunder_2.10-0.5.0.jar --driver-class-path C:\Users\jarlhaggerty\Miniconda3\envs\py27\lib\site
-packages\thunder\lib\thunder_2.10-0.5.0.jar
Running C:\Users\jarlhaggerty\Miniconda3\envs\py27\python.exe with PYTHONPATH=C:\spark\bin\..\python\lib\py4j-0.8.2.1-sr
c.zip;C:\spark\bin\..\python;
Python 2.7.9 |Anaconda 2.2.0 (64-bit)| (default, Dec 18 2014, 16:57:52) [MSC v.1500 64 bit (AMD64)] on win32
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Anaconda is brought to you by Continuum Analytics.
Please check out: http://continuum.io/thanks and https://binstar.org
Exception in thread ""main"" java.lang.IllegalArgumentException: Given path is malformed: C:UsersjarlhaggertyMiniconda3env
spy27Libsite-packagesthunderlibthunder_2.10-0.5.0.jar
        at org.apache.spark.util.Utils$.resolveURI(Utils.scala:1665)
        at org.apache.spark.util.Utils$$anonfun$resolveURIs$1.apply(Utils.scala:1687)
        at org.apache.spark.util.Utils$$anonfun$resolveURIs$1.apply(Utils.scala:1687)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
        at org.apache.spark.util.Utils$.resolveURIs(Utils.scala:1687)
        at org.apache.spark.deploy.SparkSubmitArguments.parse$1(SparkSubmitArguments.scala:391)
        at org.apache.spark.deploy.SparkSubmitArguments.parseOpts(SparkSubmitArguments.scala:288)
        at org.apache.spark.deploy.SparkSubmitArguments.<init>(SparkSubmitArguments.scala:87)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:105)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
Traceback (most recent call last):
  File ""C:\spark\bin\..\python\pyspark\shell.py"", line 50, in <module>
    sc = SparkContext(appName=""PySparkShell"", pyFiles=add_files)
  File ""C:\spark\python\pyspark\context.py"", line 108, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway)
  File ""C:\spark\python\pyspark\context.py"", line 222, in _ensure_initialized
    SparkContext._gateway = gateway or launch_gateway()
  File ""C:\spark\python\pyspark\java_gateway.py"", line 80, in launch_gateway
    raise Exception(""Java gateway process exited before sending the driver its port number"")
Exception: Java gateway process exited before sending the driver its port number;;;","13/May/15 08:44;srowen;Issue resolved by pull request 5447
[https://github.com/apache/spark/pull/5447];;;",,,,,,,,,,,,,,,,,,,,,,
PairRDDFunctions suppresses exceptions in writeFile,SPARK-6560,12786082,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,click_stephen,stephen,stephen,27/Mar/15 03:59,03/Apr/15 08:49,14/Jul/23 06:27,03/Apr/15 08:49,1.3.0,,,,,,1.4.0,,,,,,Spark Core,,,,1,,,,,,"In PairRDDFunctions, saveAsHadoopDataset uses a try/finally to manage SparkHadoopWriter. Briefly:

{code}
try {
  ... writer.write(...)
} finally {
  writer.close()
}
{code}

However, if an exception happens in writer.write, and then writer.close is called, and an exception in writer.close happens, the original (real) exception from writer.write is suppressed.

This makes debugging very painful, as the exception that is shown in the logs (from writer.close) is spurious, and the original, real exception has been lost and not logged.",,apachespark,stephen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 08:49:34 UTC 2015,,,,,,,,,,"0|i27fjr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/15 04:17;apachespark;User 'stephenh' has created a pull request for this issue:
https://github.com/apache/spark/pull/5223;;;","03/Apr/15 08:49;srowen;Issue resolved by pull request 5223
[https://github.com/apache/spark/pull/5223];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.getCurrentUserName returns the full principal name instead of login name,SPARK-6558,12786079,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tgraves,tgraves,tgraves,27/Mar/15 03:21,30/Mar/15 12:59,14/Jul/23 06:27,29/Mar/15 11:44,1.3.0,,,,,,1.3.1,1.4.0,,,,,Spark Core,,,,0,,,,,,"Utils.getCurrentUserName returns UserGroupInformation.getCurrentUser().getUserName() when SPARK_USER isn't set.  It should return UserGroupInformation.getCurrentUser().getShortUserName()

getUserName() returns the users full principal name (ie user1@CORP.COM). getShortUserName() returns just the users login name (user1).

This just happens to work on YARN because the Client code sets:
    env(""SPARK_USER"") = UserGroupInformation.getCurrentUser().getShortUserName()
",,apachespark,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 29 11:44:16 UTC 2015,,,,,,,,,,"0|i27fj3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/15 13:43;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/5229;;;","29/Mar/15 11:44;srowen;Issue resolved by pull request 5229
[https://github.com/apache/spark/pull/5229];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong parsing logic of executorTimeoutMs and checkTimeoutIntervalMs in HeartbeatReceiver,SPARK-6556,12785859,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,26/Mar/15 11:53,27/Mar/15 12:34,14/Jul/23 06:27,27/Mar/15 12:34,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"The current reading logic of ""executorTimeoutMs"" is:

{code}
private val executorTimeoutMs = sc.conf.getLong(""spark.network.timeout"", 
    sc.conf.getLong(""spark.storage.blockManagerSlaveTimeoutMs"", 120)) * 1000
{code}

So if ""spark.storage.blockManagerSlaveTimeoutMs"" is 10000 and ""spark.network.timeout"" is not set, executorTimeoutMs will be 10000 * 1000. But the correct value should have been 10000. 

""checkTimeoutIntervalMs"" has the same issue. ",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 11:59:30 UTC 2015,,,,,,,,,,"0|i27e6v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/15 11:59;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5209;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Override equals and hashCode in MetastoreRelation,SPARK-6555,12785846,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,26/Mar/15 11:11,31/Mar/15 18:19,14/Jul/23 06:27,31/Mar/15 18:19,1.0.2,1.1.1,1.2.1,1.3.0,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"This is a follow-up of SPARK-6450.

As explained in [this comment|https://issues.apache.org/jira/browse/SPARK-6450?focusedCommentId=14379499&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14379499] of SPARK-6450, we resorted to a more surgical fix due to the upcoming 1.3.1 release. But overriding {{equals}} and {{hashCode}} is the proper fix to that problem.",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 18:19:41 UTC 2015,,,,,,,,,,"0|i27e3z:",9223372036854775807,,,,,lian cheng,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"31/Mar/15 08:54;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5289;;;","31/Mar/15 18:19;marmbrus;Issue resolved by pull request 5289
[https://github.com/apache/spark/pull/5289];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add PreAnalyzer to keep logical plan consistent across DataFrame,SPARK-6550,12785807,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,viirya,viirya,26/Mar/15 08:30,24/Apr/15 00:31,14/Jul/23 06:27,27/Mar/15 18:40,,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"h2. Problems

In some cases, the expressions in a logical plan will be modified to new ones during analysis, e.g. the handling for self-join cases. If some expressions are resolved based on the analyzed plan, they are referring to changed expression ids, not original ids.

But the transformation of DataFrame will use logical plan to construct new DataFrame, e.g. {{groupBy}} and aggregation. So in such cases, the expressions in these DataFrames will be inconsistent.

The problems are specified as following:

# Expression ids in logical plan are possibly inconsistent if expression ids are changed during analysis and some expressions are resolved after that

When we try to run the following codes:
{code}
val df = Seq(1,2,3).map(i => (i, i.toString)).toDF(""int"", ""str"")
val df2 = df.as('x).join(df.as('y), $""x.str"" === $""y.str"").groupBy(""y.str"").min(""y.int"")
{code}

Because {{groupBy}} and {{min}} will perform resolving based on the analyzed logical plan, their expression ids refer to analyzed plan, instead of logical plan.

So the logical plan of df2 looks like:

{code}
'Aggregate [str#5], [str#5,MIN(int#4) AS MIN(int)#6]
 'Join Inner, Some(('x.str = 'y.str))
  Subquery x
   Project [_1#0 AS int#2,_2#1 AS str#3]
    LocalRelation [_1#0,_2#1], [[1,1],[2,2],[3,3]]
  Subquery y
   Project [_1#0 AS int#2,_2#1 AS str#3]
    LocalRelation [_1#0,_2#1], [[1,1],[2,2],[3,3]]
{code}

As you see, the expression ids in {{Aggregate}} are different to the expression ids in {{Subquery y}}. This is the first problem.

# The {{df2}} can't be performed

The showing logical plan of {{df2}} can't be performed. Because the expression ids of {{Subquery y}} will be modified for self-join handling during analysis, the analyzed plan of {{df2}} becomes:

{code}
Aggregate [str#5], [str#5,MIN(int#4) AS MIN(int)#6]
 Join Inner, Some((str#3 = str#8))
  Subquery x
   Project [_1#0 AS int#2,_2#1 AS str#3]
    LocalRelation [_1#0,_2#1], [[1,1],[2,2],[3,3]]
  Subquery y
   Project [_1#0 AS int#7,_2#1 AS str#8]
    LocalRelation [_1#0,_2#1], [[1,1],[2,2],[3,3]]
{code}

The expressions referred in {{Aggregate}} are not matching to these in {{Subquery y}}. This is the second problem.

h2. Proposed solution

We try to add a {{PreAnalyzer}}. When a logical plan {{rawPlan}} is given to SQLContext, it uses PreAnalyzer to modify the logical plan before assigning to {{QueryExecution.logical}}. Then later operations will based on the pre-analyzed logical plan, instead of the original {{rawPlan}}.
",,apachespark,lian cheng,marmbrus,michaelmalak,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 18:40:22 UTC 2015,,,,,,,,,,"0|i27dvb:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"26/Mar/15 08:35;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5203;;;","26/Mar/15 21:45;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5217;;;","27/Mar/15 18:40;marmbrus;Issue resolved by pull request 5217
[https://github.com/apache/spark/pull/5217];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Build failure caused by PR #5029 together with #4289,SPARK-6546,12785771,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,DoingDone9,DoingDone9,DoingDone9,26/Mar/15 03:19,26/Mar/15 09:12,14/Jul/23 06:27,26/Mar/15 09:04,1.4.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"PR [#4289|https://github.com/apache/spark/pull/4289] was using Guava's {{com.google.common.io.Files}} according to the first commit of that PR, see [here|https://github.com/jeanlyn/spark/blob/3b27af36f82580c2171df965140c9a14e62fd5f0/sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertIntoHiveTableSuite.scala#L22]. However, [PR #5029|https://github.com/apache/spark/pull/5029] was merged earlier, and deprecated Guava {{Files}} by {{Utils.Files}}. These two combined caused this build failure. (There're no conflicts in the eyes of Git, but there do exist semantic conflicts.)",,apachespark,DoingDone9,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 09:12:15 UTC 2015,,,,,,,,,,"0|i27dnb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"26/Mar/15 03:24;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/5198;;;","26/Mar/15 09:04;lian cheng;Issue resolved by pull request 5198
[https://github.com/apache/spark/pull/5198];;;","26/Mar/15 09:12;lian cheng;Updated ticket title and description to refect the root cause.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Problem with Avro and Kryo Serialization,SPARK-6544,12785746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,deanchen,deanchen,deanchen,26/Mar/15 00:31,24/Apr/15 00:31,14/Jul/23 06:27,27/Mar/15 14:33,1.2.0,1.3.0,,,,,1.3.1,1.4.0,,,,,Spark Core,,,,0,,,,,,"We're running in to the following bug with Avro 1.7.6 and the Kryo serializer causing jobs to fail

https://issues.apache.org/jira/browse/AVRO-1476?focusedCommentId=13999249&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13999249

PR here
https://github.com/apache/spark/pull/5193
",,apachespark,deanchen,michaelmalak,nphung,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 27 19:26:38 UTC 2015,,,,,,,,,,"0|i27dhr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/15 00:38;apachespark;User 'deanchen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5193;;;","27/Mar/15 14:33;srowen;Issue resolved by pull request 5193
[https://github.com/apache/spark/pull/5193];;;","27/Mar/15 19:26;pwendell;Back-ported to 1.3.1 per discussion on issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Executor table on Stage page should sort by Executor ID numerically, not lexically",SPARK-6541,12785719,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jbonofre,rdub,rdub,25/Mar/15 23:08,18/Nov/15 08:58,14/Jul/23 06:27,18/Nov/15 08:58,1.3.0,,,,,,1.6.0,,,,,,Web UI,,,,0,,,,,,"Page loads with a table like this:

!http://f.cl.ly/items/0M273s053F2T2K1o441L/Screen%20Shot%202015-03-25%20at%207.07.08%20PM.png!

After clicking ""Executor ID"" to sort by that column, it sorts numerically:

!http://f.cl.ly/items/01161p3s2H070h1K1a0c/Screen%20Shot%202015-03-25%20at%207.08.26%20PM.png!",,apachespark,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 18 08:58:23 UTC 2015,,,,,,,,,,"0|i27dbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/15 06:15;apachespark;User 'jbonofre' has created a pull request for this issue:
https://github.com/apache/spark/pull/9165;;;","18/Nov/15 08:58;srowen;Issue resolved by pull request 9165
[https://github.com/apache/spark/pull/9165];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing nullable Metastore fields when merging a Parquet schema,SPARK-6538,12785662,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,budde,budde,budde,25/Mar/15 20:05,24/Apr/15 00:32,14/Jul/23 06:27,28/Mar/15 01:15,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"When Spark SQL infers a schema for a DataFrame, it will take the union of all field types present in the structured source data (e.g. an RDD of JSON data). When the source data for a row doesn't define a particular field on the DataFrame's schema, a null value will simply be assumed for this field. This workflow makes it very easy to construct tables and query over a set of structured data with a nonuniform schema. However, this behavior is not consistent in some cases when dealing with Parquet files and an external table managed by an external Hive metastore.

In our particular usecase, we use Spark Streaming to parse and transform our input data and then apply a window function to save an arbitrary-sized batch of data as a Parquet file, which itself will be added as a partition to an external Hive table via an ""ALTER TABLE... ADD PARTITION..."" statement. Since our input data is nonuniform, it is expected that not every partition batch will contain every field present in the table's schema obtained from the Hive metastore. As such, we expect that the schema of some of our Parquet files may not contain the same set fields present in the full metastore schema.

In such cases, it seems natural that Spark SQL would simply assume null values for any missing fields in the partition's Parquet file, assuming these fields are specified as nullable by the metastore schema. This is not the case in the current implementation of ParquetRelation2. The mergeMetastoreParquetSchema() method used to reconcile differences between a Parquet file's schema and a schema retrieved from the Hive metastore will raise an exception if the Parquet file doesn't match the same set of fields specified by the metastore.

I propose altering this implementation in order to allow for any missing metastore fields marked as nullable to be merged in to the Parquet file's schema before continuing with the checks present in mergeMetastoreParquetSchema().

Classifying this as a bug as it exposes inconsistent behavior, IMHO. If you feel this should be an improvement or new feature instead, please feel free to reclassify this issue.",,apachespark,budde,glenn.strycker@gmail.com,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 28 01:15:47 UTC 2015,,,,,,,,,,"0|i27czb:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"25/Mar/15 20:13;apachespark;User 'budde' has created a pull request for this issue:
https://github.com/apache/spark/pull/5188;;;","26/Mar/15 17:15;apachespark;User 'budde' has created a pull request for this issue:
https://github.com/apache/spark/pull/5214;;;","28/Mar/15 01:15;lian cheng;Issue resolved by pull request 5214
[https://github.com/apache/spark/pull/5214];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
UIWorkloadGenerator: The main thread should not stop SparkContext until all jobs finish,SPARK-6537,12785644,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,25/Mar/15 18:44,25/Mar/15 20:27,14/Jul/23 06:27,25/Mar/15 20:27,1.0.0,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,The main thread of UIWorkloadGenerator spawn sub threads to launch jobs but the main thread stop SparkContext without waiting for finishing those threads.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 18:47:53 UTC 2015,,,,,,,,,,"0|i27cvb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"25/Mar/15 18:47;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/5187;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
LDAModel.scala fails scalastyle on Windows,SPARK-6532,12785550,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,brianok,brianok,25/Mar/15 15:19,26/Mar/15 18:02,14/Jul/23 06:27,26/Mar/15 18:02,1.3.0,,,,,,1.4.0,,,,,,Build,Windows,,,0,,,,,,"When executing mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.5.2 -DskipTests -X clean package, the build fails with the error:
[DEBUG] Configuring mojo org.scalastyle:scalastyle-maven-plugin:0.4.0:check from plugin realm ClassRealm[plugin>org.sca
astyle:scalastyle-maven-plugin:0.4.0, parent: sun.misc.Launcher$AppClassLoader@1174ec5]
[DEBUG] Configuring mojo 'org.scalastyle:scalastyle-maven-plugin:0.4.0:check' with basic configurator -->
[DEBUG]   (f) baseDirectory = C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib
[DEBUG]   (f) buildDirectory = C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\target
[DEBUG]   (f) configLocation = scalastyle-config.xml
[DEBUG]   (f) failOnViolation = true
[DEBUG]   (f) failOnWarning = false
[DEBUG]   (f) includeTestSourceDirectory = false
[DEBUG]   (f) outputEncoding = UTF-8
[DEBUG]   (f) outputFile = C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\scalastyle-output.xml
[DEBUG]   (f) quiet = false
[DEBUG]   (f) skip = false
[DEBUG]   (f) sourceDirectory = C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\src\main\scala
[DEBUG]   (f) testSourceDirectory = C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\src\test\scala
[DEBUG]   (f) verbose = false
[DEBUG]   (f) project = MavenProject: org.apache.spark:spark-mllib_2.10:1.3.0 @ C:\Users\u6013553\spark-1.3.0\spark-1.3
0\mllib\dependency-reduced-pom.xml
[DEBUG] -- end configuration --
[DEBUG] failOnWarning=false
[DEBUG] verbose=false
[DEBUG] quiet=false
[DEBUG] sourceDirectory=C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\src\main\scala
[DEBUG] includeTestSourceDirectory=false
[DEBUG] buildDirectory=C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\target
[DEBUG] baseDirectory=C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib
[DEBUG] outputFile=C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\scalastyle-output.xml
[DEBUG] outputEncoding=UTF-8
[DEBUG] inputEncoding=null
[DEBUG] processing sourceDirectory=C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\src\main\scala encoding=null
error file=C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\src\main\scala\org\apache\spark\mllib\clustering\LDAModel.sc
la message=Input length = 1
Saving to outputFile=C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\scalastyle-output.xml
Processed 143 file(s)
Found 1 errors
Found 0 warnings
Found 0 infos
Finished in 1571 ms

scalastyle-output.xml reports
<?xml version=""1.0"" encoding=""UTF-8""?>
<checkstyle version=""5.0"">
 <file name=""C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\src\main\scala\org\apache\spark\mllib\clustering\LDAModel.scala"">
  <error severity=""error"" message=""Input length = 1""></error>
 </file>
</checkstyle>","Windows 7, Maven 3.1.0",apachespark,brianok,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6063,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 18:02:32 UTC 2015,,,,,,,,,,"0|i27cbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/15 15:38;srowen;I can't reproduce this. It's not a compile error, but appears to be from scalastyle. Are you sure there's no typo or modifications locally?;;;","25/Mar/15 15:42;brianok;Not that I am aware of, but my Scala knowledge is limited.  For what it is worth, I found this message on the mailing list from another person with the same issue.
http://mail-archives.apache.org/mod_mbox/spark-user/201503.mbox/%3CCAAhzg8RAkw6WkVHvLbZRwWoacWd2FWgp=qmHwQzw5DZMVuH6Hw@mail.gmail.com%3E

From	Ahmed Nawar <ahmed.na...@gmail.com>
Subject	Re: Building Spark on Windows WAS: Any IRC channel on Spark?
Date	Tue, 17 Mar 2015 13:43:10 GMT
Dear Yu,

   Are you mean ""scalastyle-output.xml""? i coped its content below


<?xml version=""1.0"" encoding=""UTF-8""?>
<checkstyle version=""5.0"">
 <file
name=""C:\Nawwar\Hadoop\spark\spark-1.3.0\mllib\src\main\scala\org\apache\spark\mllib\clustering\LDAModel.scala"">
  <error severity=""error"" message=""Input length = 1""></error>
 </file>
</checkstyle>
;;;","25/Mar/15 15:52;srowen;(This is not a Blocker, as it's at best a style violation)

Hm, looks specific to Windows, since both of you were using Windows and I'm not (nor are the Jenkins builds). My guess is that it's a stray character in this file that the Windows default encoding doesn't like or something. I don't see it on casual inspection though. I wonder if you can dig in to get scalastyle to spit out something more useful than ""Input length = 1""?;;;","25/Mar/15 15:55;brianok;Thanks.  I did not understand that scalastyle was not part of the compiler (Scala novice as I said).  I will try to dig in deeper, though honestly my first priority will probably be trying to figure out how to bypass the scalastyle part of the build process now that I understand what it does :);;;","25/Mar/15 15:59;srowen;Yes, scalastyle is just a build plugin. From http://www.scalastyle.org/maven.html I'd guess you might be able to ignore it with {{-DfailOnViolation=false}} or by editing that config into the build `{{pom.xml}}`. It's definitely worth fixing, and I'm sure it's something incredibly trivial, just not sure what it is and I don't have a Windows machine to help figure it out.;;;","25/Mar/15 16:32;brianok;If I add the --inputEncoding UTF-8 parameter to the scalastyle-batch jar (downloaded from the scalastyle site), then it seems to work without warning.  Modifying the root POM file to include the parameter (<inputEncoding>UTF-8</inputEncoding>) appears to have resolved the issue.  I'll add a comment to confirm when build has completed.

C:\Users\u6013553>java -jar C:\Users\u6013553\Downloads\scalastyle_2.10-0.6.0-batch.jar -c C:\Users\u6013553\spark-1.3.0\spark-1.3.0\scalastyle-config
.xml -v true --inputEncoding UTF-8 C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\src\main\scala
Processed 143 file(s)
Found 0 errors
Found 0 warnings
Finished in 2556 ms

C:\Users\u6013553>java -jar C:\Users\u6013553\Downloads\scalastyle_2.10-0.6.0-batch.jar -c C:\Users\u6013553\spark-1.3.0\spark-1.3.0\scalastyle-config
.xml -v true C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\src\main\scala
error file=C:\Users\u6013553\spark-1.3.0\spark-1.3.0\mllib\src\main\scala\org\apache\spark\mllib\clustering\LDAModel.scala message=Input length = 1
Processed 143 file(s)
Found 1 errors
Found 0 warnings
Finished in 2640 ms

pom.xml snippet
      <plugin>
        <groupId>org.scalastyle</groupId>
        <artifactId>scalastyle-maven-plugin</artifactId>
        <version>0.4.0</version>
        <configuration>
          <verbose>false</verbose>
          <failOnViolation>true</failOnViolation>
          <includeTestSourceDirectory>false</includeTestSourceDirectory>
          <failOnWarning>false</failOnWarning>
          <sourceDirectory>${basedir}/src/main/scala</sourceDirectory>
          <testSourceDirectory>${basedir}/src/test/scala</testSourceDirectory>
          <configLocation>scalastyle-config.xml</configLocation>
          <outputFile>scalastyle-output.xml</outputFile>
          <outputEncoding>UTF-8</outputEncoding>
          <inputEncoding>UTF-8</inputEncoding>
        </configuration>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>check</goal>
            </goals>
          </execution>
        </executions>
      </plugin>;;;","25/Mar/15 16:58;srowen;OK, I think it would be pretty safe to specify inputEncoding here if it makes a difference. Really it would be nice to set it to ${project.build.sourceEncoding} and set outputEncoding in this plugin to ${project.reporting.outputEncoding}. If that works (and it should; they're all set to UTF-8), would you care to open a PR?

I'm still really curious what about the file reads differently in Windows, but I bet it is some crazy invisible unicode char that we'll have trouble finding.;;;","25/Mar/15 17:11;brianok;I think I found the issue.  LDAModel.scala:133 =>    * This is often called “theta” in the literature.

The quotes around theta are non-ASCII characters.

I should be able to open a PR if you can point me to instructions on how I can do so.  ;;;","25/Mar/15 17:25;srowen;Aha, I knew this all rang a bell. It was fixed already after 1.3.0: https://github.com/apache/spark/commit/b36b1bc22ea73669b0f69ed21e77d47fb0a7cd5d

I can back-port that one so that it gets into 1.3.1 at least. So the proximate problem is already resolved.

I think the scalastyle change is a small nice-to-have. See https://github.com/apache/spark and https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark  If it's pretty unfamiliar, I can make the change.;;;","25/Mar/15 17:32;brianok;Thanks (PR defaulted to Problem Report in my corporate head rather than Pull Request).  I am more familiar with the inner workings of Scala if nothing else :)  I can try to submit this change on my own time, but if you want to do it because you are already set up to do so, that would be appreciated.  The change you suggested appears to be working, though the build hasn't finished yet.;;;","25/Mar/15 22:07;josephkb;I'll resolve this as a duplicate of [SPARK-6063]  Thanks for reporting it & the back-port!;;;","25/Mar/15 22:17;srowen;[~josephkb] I might go ahead and make the change to the scalastyle plugin since it really should be reading the source code in the same way that javac/scalac does, with a fixed character encoding. If so I might re-resolve this as fixing that very related issue to avoid similar differences between Win / Linux later.;;;","25/Mar/15 22:20;josephkb;Oh, I agree about fixing that; I was thinking it'd be a separate JIRA, but fixing it here sounds fine.  Thank you!;;;","26/Mar/15 13:49;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5211;;;","26/Mar/15 18:02;srowen;Just re-resolving since there was a separate change for this one.;;;",,,,,,,,,,,,,,
python support yarn cluster mode requires SPARK_HOME to be set,SPARK-6506,12785182,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,tgraves,tgraves,24/Mar/15 14:46,08/Apr/15 17:15,14/Jul/23 06:27,08/Apr/15 17:15,1.3.0,,,,,,1.3.2,1.4.0,,,,,YARN,,,,0,,,,,,"We added support for python running in yarn cluster mode in https://issues.apache.org/jira/browse/SPARK-5173, but it requires that SPARK_HOME be set in the environment variables for application master and executor.  It doesn't have to be set to anything real but it fails if its not set.  See the command at the end of: https://github.com/apache/spark/pull/3976",,apachespark,ashwinshankar77,cheolsoo,joshrosen,kostas,lianhuiwang,qwertymaniac,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5173,SPARK-6701,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 17:15:25 UTC 2015,,,,,,,,,,"0|i27a4n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/15 13:13;lianhuiwang;hi [~tgraves] I use 1.3.0 to run. if i donot set SPARK_HOME at every node, i get the following exception in every executor:
Error from python worker:
  /usr/bin/python: No module named pyspark
PYTHONPATH was:
  /data/yarnenv/local/usercache/lianhui/filecache/296/spark-assembly-1.3.0-hadoop2.2.0.jar/python
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:164)
        at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:86)
        at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:62)
        at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:105)
        at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:69)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)

from the exception, i can find that pyspark of the spark.jar in nodeManager cannot be worked. and i donot know why pyspark of spark.jar cannot be worked. [~andrewor14] can you help me?
so i think now we should put spark dirs to PYTHONPATH  or SPARK_HOME at every node.;;;","26/Mar/15 13:22;tgraves;If you are running on yarn you just have to set SPARK_HOME like this:
spark.yarn.appMasterEnv.SPARK_HOME /bogus
spark.executorEnv.SPARK_HOME /bogus

But the error you pasted above isn't about that.  I've seen this when building the assembly with jdk7 or jdk8 due to the python stuff not being packaged properly in the assembly jar.  I have to use jdk6 to package it.  see https://issues.apache.org/jira/browse/SPARK-1920;;;","02/Apr/15 17:16;vanzin;Maybe you're running into SPARK-5808?;;;","02/Apr/15 17:20;tgraves;No it was built with maven and the pyspark artifacts are there.  You just have to set SPARK_HOME to something even though it isn't used for anything.  Something is looking at SPARK_HOME and blows up if its not set.;;;","07/Apr/15 02:03;kostas;I ran into this issue too by running:
bq. spark-submit  --master yarn-cluster examples/pi.py 4

it looks like I only had to set: spark.yarn.appMasterEnv.SPARK_HOME=/bogus to get it going:
bq. spark-submit --conf spark.yarn.appMasterEnv.SPARK_HOME=/bogus --master yarn-cluster pi.py 4
;;;","07/Apr/15 22:45;kostas;Here is the exception I saw when I ran the above job:
{code}
Traceback (most recent call last):
  File ""pi.py"", line 29, in <module>
    sc = SparkContext(appName=""PythonPi"")
  File ""/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.23/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/context.py"", line 108, in __init__
  File ""/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.23/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/context.py"", line 222, in _ensure_initialized
  File ""/opt/cloudera/parcels/CDH-5.4.0-1.cdh5.4.0.p0.23/jars/spark-assembly-1.3.0-cdh5.4.0-hadoop2.6.0-cdh5.4.0.jar/pyspark/java_gateway.py"", line 32, in launch_gateway
  File ""/usr/lib64/python2.6/UserDict.py"", line 22, in __getitem__
    raise KeyError(key)
KeyError: 'SPARK_HOME'
{code};;;","07/Apr/15 22:50;vanzin;Ah, makes sense. In yarn-cluster mode, that code should probably figure out what SPARK_HOME is based on what {code}__file__{code} returns, instead. Not sure what that would be when the file is inside a jar, though.;;;","07/Apr/15 22:52;vanzin;Actually, that may not be enough... what if there really isn't a SPARK_HOME on the host launching the driver? What if there's no ""spark-submit"" to run?;;;","08/Apr/15 01:00;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5405;;;","08/Apr/15 01:01;vanzin;Ignore my previous comments, the fix is much, much simpler than that...  :-/;;;","08/Apr/15 17:15;joshrosen;Issue resolved by pull request 5405
[https://github.com/apache/spark/pull/5405];;;",,,,,,,,,,,,,,,,,,
Remove the reflection call in HiveFunctionWrapper,SPARK-6505,12785177,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lian cheng,lian cheng,lian cheng,24/Mar/15 14:16,02/May/15 23:44,14/Jul/23 06:27,27/Apr/15 06:15,1.2.0,1.2.1,1.3.0,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"While trying to fix SPARK-4785, we introduced {{HiveFunctionWrapper}}, and added two not so necessary reflection calls there. These calls had caused some dependency hell problems for MapR distribution of Spark.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 27 06:15:07 UTC 2015,,,,,,,,,,"0|i27a3j:",9223372036854775807,,,,,lian cheng,,,,,,,,,1.4.0,,,,,,,,,,,,,"24/Mar/15 14:17;lian cheng;Here is [a WiP simpler fix for SPARK-4785|https://github.com/liancheng/spark/commit/2310e23797d6c87f90f81fda1d89adc0813c147d], which doesn't require these reflection call. We can make this a formal PR to fix this issue.;;;","23/Apr/15 10:10;apachespark;User 'baishuo' has created a pull request for this issue:
https://github.com/apache/spark/pull/5660;;;","27/Apr/15 06:15;lian cheng;Issue resolved by pull request 5660
[https://github.com/apache/spark/pull/5660];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot read Parquet files generated from different versions at once,SPARK-6504,12785175,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,msoutier,msoutier,24/Mar/15 14:03,14/Mar/16 09:58,14/Jul/23 06:27,16/Sep/15 20:24,1.2.1,,,,,,1.3.1,,,,,,SQL,,,,0,,,,,,"When trying to read Parquet files generated by Spark 1.1.1 and 1.2.1 at the same time via `sqlContext.parquetFile(""fileFrom1.1.parqut,fileFrom1.2.parquet"")` an exception occurs:

could not merge metadata: key org.apache.spark.sql.parquet.row.metadata has conflicting values: [{""type"":""struct"",""fields"":[{""name"":""date"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""account"",""type"":""string"",""nullable"":true,""metadata"":{}},{""name"":""impressions"",""type"":""long"",""nullable"":false,""metadata"":{}},{""name"":""cost"",""type"":""double"",""nullable"":false,""metadata"":{}},{""name"":""clicks"",""type"":""long"",""nullable"":false,""metadata"":{}},{""name"":""conversions"",""type"":""long"",""nullable"":false,""metadata"":{}},{""name"":""orderValue"",""type"":""double"",""nullable"":false,""metadata"":{}}]}, StructType(List(StructField(date,StringType,true), StructField(account,StringType,true), StructField(impressions,LongType,false), StructField(cost,DoubleType,false), StructField(clicks,LongType,false), StructField(conversions,LongType,false), StructField(orderValue,DoubleType,false)))]

The Schema is exactly equal.
",,marmbrus,msoutier,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Sep 16 20:24:31 UTC 2015,,,,,,,,,,"0|i27a33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/15 15:12;marmbrus;Have you tried this with Spark 1.3?;;;","26/Mar/15 15:15;msoutier;No, as far as I understand, Spark 1.3 cannot read Parquets created with 1.1.x at all.;;;","26/Mar/15 15:17;marmbrus;Ah, I see.  Would you be able to try with branch-1.3, where this has been fixed?;;;","26/Mar/15 15:45;msoutier;Not easily, but 1.3.1 is supposed to be released soon, right?;;;","26/Mar/15 16:50;marmbrus;Yes, hopefully next week.

;;;","16/Sep/15 20:24;marmbrus;This should be fixed.  Please reopen if you are still having problems.;;;",,,,,,,,,,,,,,,,,,,,,,,
Multinomial Logistic Regression failed when initialWeights is not null,SPARK-6496,12785135,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,24/Mar/15 11:10,25/Mar/15 17:07,14/Jul/23 06:27,25/Mar/15 17:06,1.3.0,,,,,,1.3.1,1.4.0,,,,,MLlib,,,,0,,,,,,"This bug is easy to reproduce, when use Multinomial Logistic Regression to train multiclass classification model with non-null initialWeights, it will throw an exception.
When you run
{code}
val lr = new LogisticRegressionWithLBFGS().setNumClasses(3)
val model = lr.run(input, initialWeights)
{code}
It will throw
{code}
requirement failed: LogisticRegressionModel.load with numClasses = 3 and numFeatures = -1 expected weights of length -2 (without intercept) or 0 (with intercept), but was given weights of length 10
java.lang.IllegalArgumentException: requirement failed: LogisticRegressionModel.load with numClasses = 3 and numFeatures = -1 expected weights of length -2 (without intercept) or 0 (with intercept), but was given weights of length 10
{code}
",,apachespark,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 17:07:08 UTC 2015,,,,,,,,,,"0|i279u7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/15 11:22;srowen;The problem is numFeatures = -1, not initialWeights. I'm not clear how you are reproducing this? what are the input and initialWeights? those seem quite relevant.;;;","24/Mar/15 11:49;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5167;;;","24/Mar/15 12:07;yanboliang;[~srowen] I have address this issue at github. I guess you have seen it.;;;","25/Mar/15 17:07;srowen;Resolved by https://github.com/apache/spark/pull/5167;;;",,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext.stop() can deadlock when DAGSchedulerEventProcessLoop dies,SPARK-6492,12785090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ilganeli,joshrosen,joshrosen,24/Mar/15 07:23,03/Apr/15 18:24,14/Jul/23 06:27,03/Apr/15 18:24,1.3.0,1.4.0,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"A deadlock can occur when DAGScheduler death causes a SparkContext to be shut down while user code is concurrently racing to stop the SparkContext in a finally block.

For example:

{code}
try {
      sc = new SparkContext(""local"", ""test"")
      // start running a job that causes the DAGSchedulerEventProcessor to crash
      someRDD.doStuff()
    }
} finally {
  sc.stop() // stop the sparkcontext once the failure in DAGScheduler causes the above job to fail with an exception
}
{code}

This leads to a deadlock.  The event processor thread tries to lock on the {{SparkContext.SPARK_CONTEXT_CONSTRUCTOR_LOCK}} and becomes blocked because the thread that holds that lock is waiting for the event processor thread to join:

{code}
""dag-scheduler-event-loop"" daemon prio=5 tid=0x00007ffa69456000 nid=0x9403 waiting for monitor entry [0x00000001223ad000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1398)
	- waiting to lock <0x00000007f5037b08> (a java.lang.Object)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onError(DAGScheduler.scala:1412)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:52)
{code}

{code}
""pool-1-thread-1-ScalaTest-running-SparkContextSuite"" prio=5 tid=0x00007ffa69864800 nid=0x5903 in Object.wait() [0x00000001202dc000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00000007f4b28000> (a org.apache.spark.util.EventLoop$$anon$1)
	at java.lang.Thread.join(Thread.java:1281)
	- locked <0x00000007f4b28000> (a org.apache.spark.util.EventLoop$$anon$1)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1352)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1405)
	- locked <0x00000007f5037b08> (a java.lang.Object)
[...]
{code}",,apachespark,ilganeli,joshrosen,zhichao-li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 18:24:03 UTC 2015,,,,,,,,,,"0|i279k7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/15 22:01;ilganeli;Would it be reasonable to fix this by adding some timeout/retry logic in the SparkContext shutdown code? If so, I can take care of this. Thanks. ;;;","30/Mar/15 22:59;apachespark;User 'ilganeli' has created a pull request for this issue:
https://github.com/apache/spark/pull/5277;;;","30/Mar/15 23:07;joshrosen;Timeouts are one way to fix this, but I wonder if we could also try to remove the circular wait condition by modifying EventLoop so that {{stopped}} is set before we call {{onError}}.  This would prevent calls to {{EventLoop.stop()}} from blocking while the event loop is in the process of shutting down, which should prevent this race.;;;","03/Apr/15 18:24;srowen;Issue resolved by pull request 5277
[https://github.com/apache/spark/pull/5277];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Spark will put the current working dir to the CLASSPATH,SPARK-6491,12785088,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marsishandsome,marsishandsome,marsishandsome,24/Mar/15 06:41,26/Mar/15 13:30,14/Jul/23 06:27,26/Mar/15 13:30,1.3.0,,,,,,1.3.1,,,,,,Spark Submit,,,,0,,,,,,"When running ""bin/computer-classpath.sh"", the output will be:

:/spark/conf:/spark/assembly/target/scala-2.10/spark-assembly-1.3.0-hadoop2.5.0-cdh5.2.0.jar:/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar:/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/spark/lib_managed/jars/datanucleus-core-3.2.10.jar

Java will add the current working dir to the CLASSPATH, if the first "":"" exists, which is not expected by spark users.

For example, if I call spark-shell in the folder /root. And there exists a ""core-site.xml"" under /root/. Spark will use this file as HADOOP CONF file, even if I have already set HADOOP_CONF_DIR=/etc/hadoop/conf.",,apachespark,marsishandsome,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4831,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 13:30:22 UTC 2015,,,,,,,,,,"0|i279jr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/15 06:57;apachespark;User 'marsishandsome' has created a pull request for this issue:
https://github.com/apache/spark/pull/5156;;;","24/Mar/15 10:19;srowen;[~marsishandsome] please assign a Component to JIRAs;;;","26/Mar/15 13:30;srowen;Resolved by https://github.com/apache/spark/pull/5156;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Ganglia metrics xml reporter doesn't escape correctly,SPARK-6484,12785048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,marmbrus,marmbrus,24/Mar/15 01:27,09/May/18 20:30,14/Jul/23 06:27,09/May/18 20:30,,,,,,,,,,,,,Spark Core,,,,0,,,,,,"The following should be escaped:

{code}
""   &quot;
'   &apos;
<   &lt;
>   &gt;
&   &amp;
{code}",,joshrosen,marmbrus,rxin,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 09 20:30:01 UTC 2018,,,,,,,,,,"0|i279av:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/15 01:51;joshrosen;To provide some extra context for this JIRA, I think the problem here is that the Spark driver's executor ID is {{<driver>}}, which gets included in the metrics names, breaking things when it's not escaped properly.  Given that this {{<driver>}} has caused problems in other places, too, I'd suggest that we change it to something else (maybe just {{driver}}).  If we consider this identifier to be a stable public API, though, then I guess we'd have to resort to fixing the escaping.;;;","24/Mar/15 11:09;srowen;[~joshrosen] see https://github.com/apache/spark/pull/3812 which simply agrees with your assessment that {{<driver>}} is a problem.;;;","29/Jul/15 07:39;joshrosen;IIRC this may have been fixed by a metrics version bump ;;;","20/Sep/15 19:01;rxin;Is this still a problem?;;;","22/Sep/15 21:28;joshrosen;I'm going to untarget this from 1.5.1 because, as far as I know, this is not a problem with newer versions of our Ganglia dependency.;;;","09/May/18 20:30;vanzin;Marking this as fixed given Josh's comments.;;;",,,,,,,,,,,,,,,,,,,,,,,
"Set ""In Progress"" when a PR is opened for an issue",SPARK-6481,12785041,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,nchammas,marmbrus,marmbrus,24/Mar/15 01:05,26/Mar/15 19:27,14/Jul/23 06:27,26/Mar/15 03:43,,,,,,,,,,,,,Project Infra,,,,0,,,,,,"[~pwendell] and I are not sure if this is possible, but it would be really helpful if the JIRA status was updated to ""In Progress"" when we do the linking to an open pull request.",,joshrosen,lian cheng,marmbrus,nchammas,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 19:27:09 UTC 2015,,,,,,,,,,"0|i2799b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/15 01:08;nchammas;I'm guessing this will be done via [github_jira_sync.py|https://github.com/apache/spark/blob/master/dev/github_jira_sync.py]. OK, will take a look this week.;;;","24/Mar/15 16:38;nchammas;[~pwendell] - Where is the GitHub JIRA sync script triggered from? I want to see how it's invoked, as well as get some way to run the script on demand for testing.;;;","24/Mar/15 17:38;joshrosen;This used to be done in github_jira_sync.py, but we ran into some weird issues with AMPLab Jenkins at one point, so I ended up folding this functionality into the spark-prs site.

When updating a PR, we call a function to perform the JIRA linking: https://github.com/databricks/spark-pr-dashboard/blob/0ee9102b7bb30fa61e2f47c3d282c5a018a7d198/sparkprs/models.py#L254

This ends up calling essentially the same code that's in the Spark JIRA sync script:
https://github.com/databricks/spark-pr-dashboard/blob/0ee9102b7bb30fa61e2f47c3d282c5a018a7d198/sparkprs/jira_api.py#L15;;;","24/Mar/15 19:06;nchammas;Ah, thanks for the pointers.

So should that script be removed from the Spark repo?

Also, how would I go about testing changes to {{jira_api.py}} without getting credentials?;;;","24/Mar/15 19:29;joshrosen;Meh, I suppose it's fine to leave the old sync script, since it's an easier-to-use standalone tool; I don't feel strongly either way, though.

I'm pretty sure that you can test the JIRA functions using your own JIRA login.;;;","24/Mar/15 19:38;nchammas;The change Michael/Patrick want is for state transitions, and AFAICT I don't have permission to do that with my personal JIRA account.

If my personal account is given the appropriate permissions (need to trigger state transitions; need to view project workflow), then certainly I can test things out using my personal credentials.;;;","24/Mar/15 22:51;nchammas;Since there is no guaranteed way to map GitHub usernames to JIRA usernames, what should we do about the JIRA assignee?

A JIRA issue needs an assignee in order to be marked ""In Progress"". We can have the script:
# always assign the issue to the Apache Spark user
# keep it assigned to whoever has it assigned, if any (this may be different from the PR user)
# in the case of no current assignee, assign to Apache Spark just to mark the JIRA in progress, then remove assignee

Any preferences [~marmbrus] / [~pwendell]?;;;","25/Mar/15 18:28;pwendell;Hey All,

One issue here, (I think?) right now unfortunately no users have sufficient permission to make the state change into ""In Progress"" because of the way that the JIRA is currently set up. Currently we don't expose the ""Start Progress"" button on any screen, so I think that makes it unavailable from the API call. At least, I just used my own credentials and I was not able to see the ""Start Progress"" transition on a JIRA, even though AFAIK I have the highest permissions possible.

The reason we do this I think was that we wanted to restrict assignment of JIRA's to the committership for now and the ""Start Progress"" button automatically assigns issues to a new person clicking it.

In my ideal world it works such that typical users cannot modify this state transition and it is only possible to put it in progress via a github pull request. If there is such a permission scheme that allows that, then we should see about asking ASF to enable it for our JIRA.

In terms of assignment, I'd say for now just leave the assignment as it was before.;;;","25/Mar/15 18:39;nchammas;The Spark user can initiate state transitions, but the issue needs to be assigned to it in order to do so.

So here's what I'm gonna do, after chatting briefly with Patrick:
* Save the assigned user, if any
* Assign to the Spark user
* Mark as in progress ONLY IF the issue is Open
** I dunno if we want to change the issue state if it doesn't start out as Open. Lemme know if you disagree.
* Restore the original assignee, including Unassigned if that's what it was.

Sound good to everybody? I'm going to implement this in the [jira_api.py|https://github.com/databricks/spark-pr-dashboard/blob/master/sparkprs/jira_api.py] that Josh pointed me to.;;;","25/Mar/15 19:27;nchammas;PR for this: https://github.com/databricks/spark-pr-dashboard/pull/49;;;","26/Mar/15 03:43;joshrosen;This is now live on the Spark PRs site and has been updating issues's statuses as the pull requests are updated.  At some point, I'll trigger a full refresh to link the old PRs.

Therefore, I'm going to mark this issue as ""Fixed.""  Thanks Nick!;;;","26/Mar/15 10:58;lian cheng;Maybe unrelated to this issue, but I saw a lot of JIRA notifications about ""Assignee"" updates, jumping between a normal user and ""Apache Spark"". Is this behavior a side effect of the ""In Progress"" PR? (Seems caused by [this code block|https://github.com/databricks/spark-pr-dashboard/pull/49/files#diff-6f3562e8b8a773341837373ab53b5462R34].);;;","26/Mar/15 12:04;srowen;[~nchammas] Agree, I really like this, though it's generating a lot of extra updates, and I'm one of the fools that actually tries to read `issues@`. If that's avoidable it would be great!;;;","26/Mar/15 12:43;lian cheng;Aha, so I'm not the only one! Although I just started doing this pretty recently :P ;;;","26/Mar/15 14:21;nchammas;Im willing to update this if there is a better approach, but AFAIK we
*have* to assign the issue to the Spark user in order to change the issue's
state. We then change it back to preserve the original assignee if any.

If there is a better flow I missed, let me know and I would love to fix it.
Otherwise I think this is what we're stuck with. An option might be to
filter out these assignee changes in your email client, though I know that
is suboptimal.

Nick

;;;","26/Mar/15 14:25;nchammas;Also, there was a one time mass update for all issues triggered yesterday
by Josh, so this will be the only time you see this volume of assignee
changes at once.
2015년 3월 26일 (목) 오전 10:20, Nicholas Chammas <nicholas.chammas@gmail.com>님이

;;;","26/Mar/15 14:27;joshrosen;Actually, I haven't triggered the mass update quite yet; it's only been updating issues one-by-one as they're updated in GitHub.;;;","26/Mar/15 14:41;nchammas;Oh, never mind then. :-};;;","26/Mar/15 17:59;srowen;So, I'm seeing about 50 new ""Assignee"" emails a day after this change. That does create a fair bit more noise for the few of us trying to keep on top of issues@. Hm. I am wondering at the value of the updated state, vs the extra traffic.;;;","26/Mar/15 18:14;joshrosen;It looks like it should be possible to disable issues@ emails for assignee changes, but I think we'd have to contact INFRA to do this since I don't have the required JIRA permissions.  JIRA gives pretty granular control over which events trigger emails on which lists.

Alternatively, we might be able to use a custom JIRA workflow that allows us to change assignees without changing assignment, but I'm not sure how to configure this (and also don't have the required permissions).

Finally, I could just disable this auto-linking while we figure things out if it's really annoying.;;;","26/Mar/15 19:11;srowen;The thing is, for some reason it's making changes on JIRAs that don't have any activity. Like, see https://issues.apache.org/jira/browse/SPARK-3441 and look at ""All"" activity. I don't know if that indicates something else is a bit off, which could be more easily fixed.

I think that one of these days, soon, we'll have to get more control over JIRA anyway. (Example: to make Component required? doesn't seem to be possible without being allowed to add or change workflows.) Do we know anything about that beyond contacting INFRA? Like, has anyone been told we just can't?;;;","26/Mar/15 19:14;joshrosen;For SPARK-3441, this issue was updated because the linked GitHub pull request was updated.;;;","26/Mar/15 19:15;nchammas;SPARK-3441 looks fine to me. It was open, so the script assigned to Spark in order to mark it in progress and then assigned it back to Sandy. That's all reflected correctly in the ""All"" activity. It did that because the issue has a linked, open PR.;;;","26/Mar/15 19:27;srowen;Got it. So we may see a lot of this activity as issues get tagged In Progress, but obviously they don't get changed every time. Well I can filter for now, it's not critical.;;;",,,,,
histogram() bucket function is wrong in some simple edge cases,SPARK-6480,12785020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,23/Mar/15 23:17,26/Mar/15 15:01,14/Jul/23 06:27,26/Mar/15 15:01,1.3.0,,,,,,1.2.2,1.3.1,1.4.0,,,,Spark Core,,,,1,,,,,,"(Credit to a customer report here) This test would fail now: 

{code}
    val rdd = sc.parallelize(Seq(1, 1, 1, 2, 3, 3))
    assert(Array(3, 1, 2) === rdd.map(_.toDouble).histogram(3)._2)
{code}

Because it returns 3, 1, 0. The problem ultimately traces to the 'fast' bucket function that judges buckets based on a multiple of the gap between first and second elements. Errors multiply and the end of the final bucket fails to include the max.

Fairly plausible use case actually.

This can be tightened up easily with a slightly better expression. It will also fix this test, which is actually expecting the wrong answer:

{code}
    val rdd = sc.parallelize(6 to 99)
    val (histogramBuckets, histogramResults) = rdd.histogram(9)
    val expectedHistogramResults =
      Array(11, 10, 11, 10, 10, 11, 10, 10, 11)
{code}

(Should be {{Array(11, 10, 10, 11, 10, 10, 11, 10, 11)}})",,apachespark,frosner,michaelmalak,neelesh77,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 15:01:14 UTC 2015,,,,,,,,,,"0|i27953:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/15 23:23;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5148;;;","24/Mar/15 10:19;frosner;Thanks for picking it up [~srowen]!;;;","25/Mar/15 14:19;srowen;[~frosner] can you have a peek at the PR and see if it makes sense to you? I'd like to get another set of eyes on it before committing;;;","26/Mar/15 08:55;frosner;[~srowen] will do today!;;;","26/Mar/15 15:01;srowen;Resolved by https://github.com/apache/spark/pull/5148;;;",,,,,,,,,,,,,,,,,,,,,,,,
Launcher lib shouldn't try to figure out Scala version when not in dev mode,SPARK-6473,12784977,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,23/Mar/15 20:24,05/Dec/17 23:15,14/Jul/23 06:27,24/Mar/15 13:48,1.4.0,,,,,,1.4.0,,,,,,Spark Submit,,,,0,,,,,,"Thanks to [~nravi] for pointing this out.

The launcher library currently always tries to figure out what's the build's scala version, even when it's not needed. That code is only used when setting some dev options, and relies on the layout of the build directories, so it doesn't work with the directory layout created by make-distribution.sh.

Right now this works on Linux because bin/load-spark-env.sh sets the Scala version explicitly, but it would break the distribution on Windows, for example.

Fix is pretty straight-forward.",,apachespark,peng,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 05 23:15:56 UTC 2017,,,,,,,,,,"0|i278vz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/15 20:26;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5143;;;","24/Mar/15 13:48;srowen;Issue resolved by pull request 5143
[https://github.com/apache/spark/pull/5143];;;","05/Dec/17 23:15;peng;Looks like this issue reappear at some point:

getScalaVersion() will be called anyway even if not in dev/test mode:

https://github.com/apache/spark/blob/master/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java#L196

It was suppressed simply because SPARK_SCALA_VERSION is always set by shell script.

Should we add a test to ensure that it never happens?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the race condition of subDirs in DiskBlockManager,SPARK-6468,12784811,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,23/Mar/15 12:55,17/May/20 18:21,14/Jul/23 06:27,26/Mar/15 12:55,1.3.0,,,,,,1.4.0,,,,,,Block Manager,Spark Core,,,0,,,,,,"There are two race conditions of subDirs in DiskBlockManager:

1. `getAllFiles` does not use correct locks to read the contents in `subDirs`. Although it's designed for testing, it's still worth to add correct locks to eliminate the race condition.

2. The double-check has a race condition in `getFile(filename: String)`. If a thread finds `subDirs(dirId)(subDirId)` is not null out of the `synchronized` block, it may not be able to see the correct content of the File instance pointed by `subDirs(dirId)(subDirId)` according to the Java memory model (there is no volatile variable here).",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 12:55:01 UTC 2015,,,,,,,,,,"0|i2783b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/15 12:58;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5136;;;","26/Mar/15 12:55;srowen;Issue resolved by pull request 5136
[https://github.com/apache/spark/pull/5136];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
GenericRowWithSchema: KryoException: Class cannot be created (missing no-arg constructor):,SPARK-6465,12784772,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,EarthsonLu,EarthsonLu,23/Mar/15 09:11,22/May/15 00:11,14/Jul/23 06:27,26/Mar/15 10:47,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"I can not find a issue for this. 

register for GenericRowWithSchema is lost in  org.apache.spark.sql.execution.SparkSqlSerializer.

Is this the only thing we need to do?

Here is the log

{code}
15/03/23 16:21:00 WARN TaskSetManager: Lost task 9.0 in stage 20.0 (TID 31978, datanode06.site): com.esotericsoftware.kryo.KryoException: Class cannot be created (missing no-arg constructor): org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema
        at com.esotericsoftware.kryo.Kryo.newInstantiator(Kryo.java:1050)
        at com.esotericsoftware.kryo.Kryo.newInstance(Kryo.java:1062)
        at com.esotericsoftware.kryo.serializers.FieldSerializer.create(FieldSerializer.java:228)
        at com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:217)
        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
        at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:42)
        at com.twitter.chill.Tuple2Serializer.read(TupleSerializers.scala:33)
        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
        at org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:138)
        at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
        at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
        at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.execution.joins.HashJoin$$anon$1.hasNext(HashJoin.scala:66)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:217)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:64)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{code}","Spark 1.3, YARN 2.6.0, CentOS",apachespark,EarthsonLu,glenn.strycker@gmail.com,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,SPARK-5494,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 10:47:45 UTC 2015,,,,,,,,,,"0|i277un:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"24/Mar/15 08:26;EarthsonLu;I'm confused.

https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L94

{code}
  def convertRowToScala(r: Row, schema: StructType): Row = {
    // TODO: This is very slow!!!
    new GenericRowWithSchema( //Why we need GenericRowWithSchema? It seems to be the only use of GenericRowWithSchema
      r.toSeq.zip(schema.fields.map(_.dataType))
        .map(r_dt => convertToScala(r_dt._1, r_dt._2)).toArray, schema)
  }
{code};;;","25/Mar/15 12:50;lian cheng;I believe for now this is only used for testing purposes. It's used in [{{DataFrameSuite}}|https://github.com/apache/spark/blob/64262ed99912e780b51f240a14dc98fc3cdf916d/sql/core/src/test/scala/org/apache/spark/sql/DataFrameSuite.scala#L90];;;","25/Mar/15 18:42;marmbrus;We don't keep the schema around internally because it would just take up space in the row.  However, when we give the rows to users we add it back in so that they can do things like: call .toDF on a RDD[Row] without specifying the schema or access columns by name instead of ordinal.  These aren't implemented yet, but that is why we are passing around this information.;;;","25/Mar/15 23:48;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5191;;;","26/Mar/15 10:47;lian cheng;Issue resolved by pull request 5191
[https://github.com/apache/spark/pull/5191];;;",,,,,,,,,,,,,,,,,,,,,,,,
AttributeSet.equal should compare size,SPARK-6463,12784760,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,June.He,June.He,June.He,23/Mar/15 08:13,26/Mar/15 02:22,14/Jul/23 06:27,26/Mar/15 02:22,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"AttributeSet.equal should compare both member and size, otherwise it could return true when the left is a subset of the right.
",,apachespark,June.He,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 02:22:44 UTC 2015,,,,,,,,,,"0|i277rz:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"23/Mar/15 08:40;apachespark;User 'sisihj' has created a pull request for this issue:
https://github.com/apache/spark/pull/5133;;;","26/Mar/15 00:39;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5194;;;","26/Mar/15 02:22;marmbrus;Issue resolved by pull request 5194
[https://github.com/apache/spark/pull/5194];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Bad error message for invalid data sources,SPARK-6458,12784690,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,22/Mar/15 19:31,24/Mar/15 21:11,14/Jul/23 06:27,24/Mar/15 21:11,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"Create a table where the datasources is some random class.  The error message is not very helpful:

{code}
Caused by: scala.MatchError: org.postgresql.Driver@2e0d0929 (of class org.postgresql.Driver)
	at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:288)
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 07:13:28 UTC 2015,,,,,,,,,,"0|i277cf:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"24/Mar/15 07:13;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5158;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when calling Pyspark RandomForestModel.load,SPARK-6457,12784676,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,josephkb,josephkb,josephkb,22/Mar/15 17:26,24/Apr/15 00:33,14/Jul/23 06:27,23/Mar/15 02:31,1.3.0,,,,,,1.3.1,1.4.0,,,,,MLlib,PySpark,,,0,,,,,,"Reported by [https://github.com/catmonkeylee]:

Summary: PySpark RandomForestModel.load fails in test script.  It appears that the saved model file is empty.

{quote}
When I run the sample code in cluster mode, there is an error.

Traceback (most recent call last):
File ""/data1/s/apps/spark-app/app/sample_rf.py"", line 25, in 
sameModel = RandomForestModel.load(sc, model_path)
File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 254, in load
java_model = cls.load_java(sc, path)
File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 250, in _load_java
return java_obj.load(sc._jsc.sc(), path)
File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call
File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.mllib.tree.model.RandomForestModel.load.
: java.lang.UnsupportedOperationException: empty collection
at org.apache.spark.rdd.RDD.first(RDD.scala:1191)
at org.apache.spark.mllib.util.Loader$.loadMetadata(modelSaveLoad.scala:125)
at org.apache.spark.mllib.tree.model.RandomForestModel$.load(treeEnsembleModels.scala:65)
at org.apache.spark.mllib.tree.model.RandomForestModel.load(treeEnsembleModels.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{quote}

{quote}
I run the code on a spark cluster , spark version is 1.3.0

The test code:
===================================
from pyspark import SparkContext, SparkConf
from pyspark.mllib.tree import RandomForest, RandomForestModel
from pyspark.mllib.util import MLUtils

conf = SparkConf().setAppName('LocalTest')
sc = SparkContext(conf=conf)
data = MLUtils.loadLibSVMFile(sc, 'data/mllib/sample_libsvm_data.txt')
print data.count()
(trainingData, testData) = data.randomSplit([0.7, 0.3])
model = RandomForest.trainClassifier(trainingData, numClasses=2, categoricalFeaturesInfo={},
                                     numTrees=3, featureSubsetStrategy=""auto"",
                                     impurity='gini', maxDepth=4, maxBins=32)

# Evaluate model on test instances and compute test error
predictions = model.predict(testData.map(lambda x: x.features))
labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)
testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())
print('Test Error = ' + str(testErr))
print('Learned classification forest model:')
print(model.toDebugString())

# Save and load model
_model_path = ""/home/s/apps/spark-app/data/myModelPath""
model.save(sc, _model_path)
sameModel = RandomForestModel.load(sc, _model_path)
sc.stop()

===================
run command:
spark-submit --master spark://t0.q.net:7077 --executor-memory 1G sample_rf.py

======================
Then I get this error :


Traceback (most recent call last):
  File ""/data1/s/apps/spark-app/app/sample_rf.py"", line 25, in <module>
    sameModel = RandomForestModel.load(sc, _model_path)
  File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 254, in load
    java_model = cls._load_java(sc, path)
  File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 250, in _load_java
    return java_obj.load(sc._jsc.sc(), path)
  File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.mllib.tree.model.RandomForestModel.load.
: java.lang.UnsupportedOperationException: empty collection
at org.apache.spark.rdd.RDD.first(RDD.scala:1191)
at org.apache.spark.mllib.util.Loader$.loadMetadata(modelSaveLoad.scala:125)
at org.apache.spark.mllib.tree.model.RandomForestModel$.load(treeEnsembleModels.scala:65)
at org.apache.spark.mllib.tree.model.RandomForestModel.load(treeEnsembleModels.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
at py4j.Gateway.invoke(Gateway.java:259)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:207)
at java.lang.Thread.run(Thread.java:724)
{quote}
",,josephkb,lee.xiaobo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6330,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 02:31:03 UTC 2015,,,,,,,,,,"0|i2779b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/15 17:33;josephkb;I tried to reproduce this error myself using Spark 1.3 RC3, and I could not.

CC: [~mengxr]  Do you know if saveAsParquetFile() blocks?  I wonder if save() had not finished yet before load() was called.;;;","22/Mar/15 17:36;josephkb;To catmonkeylee: Can you try inserting a sleep call between save and load as a quick test?  (That may not be the best long-term fix, but may be good enough to tell us what the bug is.);;;","23/Mar/15 02:23;lee.xiaobo;Thanks Joseph K. Bradley,   I change the code when save model,

model.save(sc, ""hdfs://t17.q.net:9000/user/webchecker/output/myModelPath"") 

This time I meet another error, So I searched in JIRA and found it's a bug. The issue had been resolved in https://issues.apache.org/jira/browse/SPARK-6330 , So I checkout the master code build and run the same test code, everything is OK.:

==================================================== 
15/03/20 23:27:06 INFO DAGScheduler: Stage 23 (saveAsTextFile at treeEnsembleModels.scala:310) finished in 1.026 s
15/03/20 23:27:06 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool default
15/03/20 23:27:06 INFO DAGScheduler: Job 11 finished: saveAsTextFile at treeEnsembleModels.scala:310, took 1.101135 s
Traceback (most recent call last):
  File ""/data1/s/apps/spark-app/app/test_rf_classify.py"", line 77, in <module>
    model.save(sc, ""hdfs://wdlog17.safe.bjt.qihoo.net:9000/user/webchecker/output/rf0"")
  File ""/home/s/apps/spark/python/pyspark/mllib/util.py"", line 202, in save
    self._java_model.save(sc._jsc.sc(), path)
  File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/home/s/apps/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o75.save.
: java.lang.IllegalArgumentException: Wrong FS: hdfs://t17.q.net:9000/user/webchecker/output/rf0/data, expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
        at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:463)
        at org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java:118)
        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:252)
        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:251)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.List.foreach(List.scala:318)


;;;","23/Mar/15 02:29;josephkb;[~lee.xiaobo]  Thanks!  I'm glad you figured it out.  I'll close this JIRA.;;;","23/Mar/15 02:31;josephkb;Fixed by [SPARK-6330];;;",,,,,,,,,,,,,,,,,,,,,,,,
Spark Sql throwing exception on large partitioned data,SPARK-6456,12784674,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pankajch,pankajch,22/Mar/15 17:05,25/Jun/15 03:48,14/Jul/23 06:27,24/Mar/15 09:32,,,,,,,1.2.1,,,,,,SQL,,,,0,,,,,,"Spark connects with Hive Metastore. I am able to run simple queries like show table and select. but throws below exception while running query on the hive Table having large number of partitions.
{noformat}
Exception in thread ""main"" java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:40)
        at`enter code here` org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.thrift.transport.TTransportException: java.net.SocketTimeoutException: Read timed out
        at org.apache.hadoop.hive.ql.metadata.Hive.getAllPartitionsOf(Hive.java:1785)
        at org.apache.spark.sql.hive.HiveShim$.getAllPartitionsOf(Shim13.scala:316)
        at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:86)
        at org.apache.spark.sql.hive.HiveContext$$anon$1.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$$super$lookupRelation(HiveContext.scala:253)
        at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:137)
        at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:137)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:137)
        at org.apache.spark.sql.hive.HiveContext$$anon$1.lookupRelation(HiveContext.scala:253)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$5.applyOrElse(Analyzer.scala:143)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$5.applyOrElse(Analyzer.scala:138)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:162)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
{noformat}",,gaurav-aol,lian cheng,marmbrus,pankajch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8595,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 24 16:09:05 UTC 2015,,,,,,,,,,"0|i2778v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/15 19:26;marmbrus;Do you have the logs from the metastore?  It seems likely that that process is OOMing or something.;;;","23/Mar/15 07:20;lian cheng;How many partitions are there? Also, what's the version of the Hive metastore? For now, Spark SQL only support Hive 0.12.0 and 0.13.1. Spark 1.1 and prior versions only support Hive 0.12.0.;;;","24/Mar/15 09:31;pankajch;It was the issue of large number of partition. actually the number was too high. i removed old partitions and it worked for me.
Thanks for the clue. i am closing the issue.

Thanks
Pankaj ;;;","24/Jun/15 16:09;gaurav-aol;I am facing same issue and have only 24 partitions;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Fix several broken links in PySpark docs,SPARK-6454,12784645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,smugakamil,srowen,srowen,22/Mar/15 12:05,22/Mar/15 15:57,14/Jul/23 06:27,22/Mar/15 15:57,1.3.0,,,,,,1.3.1,1.4.0,,,,,Documentation,PySpark,,,0,,,,,,"See https://github.com/apache/spark/pull/5120

There are actually a number of instances of links to PySpark docs that end in {{-class.html}} which don't work.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 22 15:57:28 UTC 2015,,,,,,,,,,"0|i2772n:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"22/Mar/15 13:04;apachespark;User 'kamilsmuga' has created a pull request for this issue:
https://github.com/apache/spark/pull/5120;;;","22/Mar/15 15:57;srowen;Issue resolved by pull request 5120
[https://github.com/apache/spark/pull/5120];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
CheckAnalysis should throw when the Aggregate node contains missing input attribute(s),SPARK-6452,12784634,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,22/Mar/15 07:53,24/Mar/15 08:12,14/Jul/23 06:27,24/Mar/15 08:12,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"Please refer to SPARK-6444 for reproduction steps. The top {{Aggregate}} node in the analyzed plan shown there contains a missing input attribute, but {{CheckAnalysis}} doesn't report it.",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6444,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 08:12:32 UTC 2015,,,,,,,,,,"0|i27707:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"22/Mar/15 14:02;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5129;;;","24/Mar/15 08:12;marmbrus;Issue resolved by pull request 5129
[https://github.com/apache/spark/pull/5129];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support CombineSum in Code Gen,SPARK-6451,12784629,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gvramana,yhuai,yhuai,22/Mar/15 03:09,09/Apr/15 01:42,14/Jul/23 06:27,09/Apr/15 01:42,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Since we are using CombineSum at the reducer side for the SUM function, we need to make it work in code gen. Otherwise, code gen will not convert Aggregates with a SUM function to GeneratedAggregates (the code gen version).",,apachespark,gvramana,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 09 01:42:54 UTC 2015,,,,,,,,,,"0|i276z3:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"23/Mar/15 06:40;gvramana;Working on the same.;;;","23/Mar/15 15:50;apachespark;User 'gvramana' has created a pull request for this issue:
https://github.com/apache/spark/pull/5138;;;","24/Mar/15 19:33;marmbrus;I went ahead and reverted the root cause from branch-1.3 as changes to code gen are a little scare to put in a maintenance branch.  We can still try and get the fix into master.;;;","09/Apr/15 01:42;marmbrus;Issue resolved by pull request 5138
[https://github.com/apache/spark/pull/5138];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Metastore Parquet table conversion fails when a single metastore Parquet table appears multiple times in the query,SPARK-6450,12784628,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,chinnitv,chinnitv,22/Mar/15 02:26,26/Mar/15 00:40,14/Jul/23 06:27,26/Mar/15 00:40,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"The below query was working fine till 1.3 commit 9a151ce58b3e756f205c9f3ebbbf3ab0ba5b33fd.(Yes it definitely works at this commit although this commit is completely unrelated)

It got broken in 1.3.0 release with an AnalysisException: resolved attributes ... missing from .... (although this list contains the fields which it reports missing)
{code}
at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:189)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:218)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy17.executeStatementAsync(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:233)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

{code}
select Orders.Country, Orders.ProductCategory,count(1) from Orders join (select Orders.Country, count(1) CountryOrderCount from Orders where to_date(Orders.PlacedDate) > '2015-01-01' group by Orders.Country order by CountryOrderCount DESC LIMIT 5) Top5Countries on Top5Countries.Country = Orders.Country where to_date(Orders.PlacedDate) > '2015-01-01' group by Orders.Country,Orders.ProductCategory;
{code}

The temporary workaround is to add explicit alias for the table Orders

{code}
select o.Country, o.ProductCategory,count(1) from Orders o join (select r.Country, count(1) CountryOrderCount from Orders r where to_date(r.PlacedDate) > '2015-01-01' group by r.Country order by CountryOrderCount DESC LIMIT 5) Top5Countries on Top5Countries.Country = o.Country where to_date(o.PlacedDate) > '2015-01-01' group by o.Country,o.ProductCategory;
{code}

However this change not only affects self joins, it also seems to affect union queries as well, like the below query which was again working before(commit 9a151ce) got broken

{code}
select Orders.Country,null,count(1) OrderCount from Orders group by Orders.Country,null
union all
select null,Orders.ProductCategory,count(1) OrderCount from Orders group by null, Orders.ProductCategory
{code}
also fails with a Analysis exception.
The workaround is to add different aliases for the tables.",,apachespark,chinnitv,glenn.strycker@gmail.com,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 00:40:38 UTC 2015,,,,,,,,,,"0|i276yv:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.3.1,,,,,,,,,,,,,"22/Mar/15 02:55;marmbrus;What kind of table are you reading from?  Does it happen to be parquet?;;;","22/Mar/15 02:59;chinnitv;Yes it is from Parquet

On Sat, Mar 21, 2015 at 7:56 PM, Michael Armbrust (JIRA) <jira@apache.org>

;;;","22/Mar/15 19:43;marmbrus;[~lian cheng] I think we are failing to attach the table name as a qualifier when converting parquet relations to the native code path.;;;","23/Mar/15 03:25;lian cheng;Ah, I see. Will handle this ASAP. [~chinnitv] Thanks for reporting this!;;;","25/Mar/15 01:11;marmbrus;I am having trouble reproducing this issue.  Can you explain more how you are creating the table in question?;;;","25/Mar/15 01:35;lian cheng;[~chinnitv], could you please provide the DDL of the Orders table to help reproducing this issue?;;;","25/Mar/15 01:44;lian cheng;[~chinnitv], never mind, reproduced this issue with 1.3.0 release and the following Spark shell snippet:
{noformat}
sqlContext.sql(""""""create table if not exists Orders (Country string, ProductCategory string, PlacedDate string) stored as parquet"""""")

sqlContext.sql(""""""
    select
        Orders.Country,
        Orders.ProductCategory,
        count(1)
    from
        Orders
    join (
        select
            Orders.Country,
            count(1) CountryOrderCount
        from
            Orders
        where
            to_date(Orders.PlacedDate) > '2015-01-01'
        group by
            Orders.Country
        order by
            CountryOrderCount DESC
        LIMIT 5
    ) Top5Countries
    on
        Top5Countries.Country = Orders.Country
    where
        to_date(Orders.PlacedDate) > '2015-01-01'
    group by
        Orders.Country,
        Orders.ProductCategory
    """""").queryExecution.analyzed
{noformat};;;","25/Mar/15 08:38;lian cheng;Here is a simpler Spark shell snippet for reproduction:
{noformat}
    import sqlContext._

    sql(
      """"""CREATE TABLE IF NOT EXISTS ms_convert (key INT)
        |STORED AS PARQUET
      """""".stripMargin)

    // This shouldn't throw AnalysisException
    val analyzed = sql(
      """"""SELECT key FROM ms_convert
        |UNION ALL
        |SELECT key FROM ms_convert
      """""".stripMargin).queryExecution.analyzed
{noformat}
[~marmbrus] has nailed down the root cause: the {{ParquetConversions}} analysis rule generates a hash map, which maps from the original {{MetastoreRelation}} instances to the newly created {{ParquetRelation2}} instances. However, {{MetastoreRelation.equals}} doesn't compare output attributes. Thus, if a single metastore Parquet table appears multiple times in a query, only a single entry ends up in the hash map, and the conversion is not correctly performed.

Proper fix for this issue should be overriding {{equals}} and {{hashCode}} for {{MetastoreRelation}}. However, this breaks more tests than expected. It's possible that these tests are ill-formed from the very beginning. But as 1.3.1 release is approaching, we'd like to make the change more surgical to avoid potential regressions. The proposed fix here is to make both the metastore relations and their output attributes as keys in the hash map used in {{ParquetConversions}}.;;;","25/Mar/15 09:20;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5183;;;","26/Mar/15 00:40;marmbrus;Issue resolved by pull request 5183
[https://github.com/apache/spark/pull/5183];;;",,,,,,,,,,,,,,,,,,,
SQL functions (either built-in or UDF) should check for data types of their arguments,SPARK-6444,12783828,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,lian cheng,lian cheng,21/Mar/15 04:03,03/Jun/15 07:48,14/Jul/23 06:27,03/Jun/15 07:48,1.0.2,1.1.1,1.2.1,1.3.0,,,1.5.0,,,,,,SQL,,,,0,,,,,,"SQL functions should remain unresolved if their arguments don't satisfy their argument type requirements. Take {{Sum}} as an example, the data type of {{Sum(Literal(""1""))}} is {{StringType}}, and now it's considered resolved, which may cause problems.

Here is a simplified version of a problematic query reported by [~cenyuhai]. Spark shell session for reproducing this issue:
{code}
import sqlContext._

sql(""""""
    CREATE TABLE IF NOT EXISTS ut (
        c1 STRING,
        c2 STRING
    )
    """""")

sql(""""""
    SELECT SUM(c3) FROM (
        SELECT SUM(c1) AS c3, 0 AS c4 FROM ut     -- (1)
        UNION ALL
        SELECT 0 AS c3, COUNT(c2) AS c4 FROM ut   -- (2)
    ) t
    """""").queryExecution.optimizedPlan
{code}
Exception thrown:
{noformat}
java.util.NoSuchElementException: key not found: c3#10
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at org.apache.spark.sql.catalyst.expressions.AttributeMap.default(AttributeMap.scala:29)
        at scala.collection.MapLike$class.apply(MapLike.scala:141)
        at org.apache.spark.sql.catalyst.expressions.AttributeMap.apply(AttributeMap.scala:29)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$1.applyOrElse(Optimizer.scala:80)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$1.applyOrElse(Optimizer.scala:79)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:50)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:186)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:177)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$.pushToRight(Optimizer.scala:79)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$apply$1$$anonfun$applyOrElse$6.apply(Optimizer.scala:101)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$apply$1$$anonfun$applyOrElse$6.apply(Optimizer.scala:101)
        ...
{noformat}
The analyzed plan of the query is:
{noformat}
== Analyzed Logical Plan ==
!Aggregate [], [SUM(CAST(c3#153, DoubleType)) AS _c0#157]                   (c)
 Union
  Project [CAST(c3#153, StringType) AS c3#164,c4#163L]                      (d)
   Project [c3#153,CAST(c4#154, LongType) AS c4#163L]
    Aggregate [], [SUM(CAST(c1#158, DoubleType)) AS c3#153,0 AS c4#154]     (b)
     MetastoreRelation default, ut, None
  Project [CAST(c3#155, StringType) AS c3#162,c4#156L]                      (a)
   Aggregate [], [0 AS c3#155,COUNT(c2#161) AS c4#156L]
    MetastoreRelation default, ut, None
{noformat}
This case is very interesting. It involves 2 analysis rules, {{WidenTypes}} and {{PromoteStrings}}, and 1 optimizer rule, {{UnionPushdown}}. To see the details, we can turn on TRACE level log and check detailed rule execution process. The TL;DR is:
# Since {{c1}} is STRING, {{SUM(c1)}} is also STRING (which is the root cause of the whole issue).
# {{c3}} in {{(1)}} is STRING, while the one in {{(2)}} is INT. Thus {{WidenTypes}} casts the latter to STRING to ensure both sides of the UNION have the same schema.  See {{(a)}}.
# {{PromoteStrings}} casts {{c1}} in {{SUM(c1)}} to DOUBLE, which consequently changes data type of {{SUM(c1)}} and {{c3}} to DOUBLE.  See {{(b)}}.
# {{c3}} in the top level {{Aggregate}} is resolved as DOUBLE (c)
# Since schemas of the two sides of the UNION are different again, {{WidenTypes}} casts {{SUM(c1) AS c3}} to STRING.  See {{(d)}}.
# Int the top level {{Aggregate}}, {{c3#153}} becomes a missing input attribute because it is hidden by {{(d)}} now.
# In the optimizing phase, {{UnionPushdown}} throws because the top level {{Aggregate}} has missing input attribute.",,apachespark,glenn.strycker@gmail.com,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7562,,,,,,,,,,,,,SPARK-6452,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 27 19:28:58 UTC 2015,,,,,,,,,,"0|i272vj:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"22/Mar/15 07:55;lian cheng;This issue also exposes another issue in the analyzer, which is tracked in SPARK-6452.;;;","22/Mar/15 08:55;lian cheng;Another valid fix for this issue is simply switching the order of {{WidenTypes}} and {{PromoteStrings}}, so that the string argument of {{Sum}} is correctly converted to double first. Note that analysis rules shouldn't be order-dependent. The reason why switching rule execution order fixes this issue is that, in this case, {{PromoteStrings}} does argument type checking and type conversion for {{Sum}}.;;;","22/Mar/15 10:42;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5127;;;","22/Mar/15 10:56;lian cheng;This PR contains the ""quick fix"" I mentioned above. I believe [PR #4685|https://github.com/apache/spark/pull/4685] is heading to the right direction.;;;","14/May/15 18:17;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/6152;;;","26/May/15 08:15;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/6405;;;","27/May/15 19:28;yhuai;Since https://github.com/apache/spark/pull/6405 is the pr for both this one and SPARK-7562, I am re-targeting it to 1.5 to make the target version consistent with SPARK-7562.;;;",,,,,,,,,,,,,,,,,,,,,,
ipv6 URI for HttpServer,SPARK-6440,12783764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nyaapa,nyaapa,nyaapa,20/Mar/15 21:45,13/Apr/15 11:56,14/Jul/23 06:27,13/Apr/15 11:55,1.3.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"In {{org.apache.spark.HttpServer}} uri is generated as {code:java}""spark://"" + localHostname + "":"" + masterPort{code}, where {{localHostname}} is {code:java} org.apache.spark.util.Utils.localHostName() = customHostname.getOrElse(localIpAddressHostname){code}. If the host has an ipv6 address then it would be interpolated into invalid URI:  {{spark://fe80:0:0:0:200:f8ff:fe21:67cf:42}} instead of {{spark://[fe80:0:0:0:200:f8ff:fe21:67cf]:42}}.

The solution is to separate uri and hostname entities.","java 7 hotspot, spark 1.3.0, ipv6 only cluster",achanda,apachespark,nyaapa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 13 11:55:47 UTC 2015,,,,,,,,,,"0|i272h3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/15 07:54;achanda;I'd like to work on this. Please assign it to me.;;;","21/Mar/15 08:09;apachespark;User 'achanda' has created a pull request for this issue:
https://github.com/apache/spark/pull/5115;;;","21/Mar/15 08:48;nyaapa;I have already prepared [this patch|https://github.com/nyaapa/spark/commit/8d793565b5cfd0fc7717c179b1ae5a7926f02512]

May i make a pull request?;;;","21/Mar/15 10:23;nyaapa;Sorry that i didn't give my suggestion in bug description: i didn't think that spark community is so reactive :)
 So, what do we need to do? I aggree with [~sowen] to let callers work with inetaddress as they need, sometimes we need ipv6 ""as is"" (not as uri).
What is the common way: to fix pull request or to close it and make a new one?;;;","21/Mar/15 15:50;srowen;I would first recommend maybe trying to collaborate on one pull request. Since there's already a live one, comment on that maybe. You can open your own if you need to. You don't need to close a pull request to change it, just push more commits to the PR.;;;","08/Apr/15 18:32;apachespark;User 'nyaapa' has created a pull request for this issue:
https://github.com/apache/spark/pull/5424;;;","13/Apr/15 11:55;srowen;Issue resolved by pull request 5424
[https://github.com/apache/spark/pull/5424];;;",,,,,,,,,,,,,,,,,,,,,,
SQL ExternalSort should use CompletionIterator to clean up temp files,SPARK-6437,12783742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,yhuai,yhuai,20/Mar/15 20:59,24/Mar/15 19:10,14/Jul/23 06:27,24/Mar/15 19:10,,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"Right now, temp files used by SQL ExternalSort are not cleaned up.",,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 19:10:49 UTC 2015,,,,,,,,,,"0|i272c7:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"24/Mar/15 08:06;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5161;;;","24/Mar/15 19:10;marmbrus;Issue resolved by pull request 5161
[https://github.com/apache/spark/pull/5161];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-shell --jars option does not add all jars to classpath,SPARK-6435,12783622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tsudukim,vjapache,vjapache,20/Mar/15 14:29,28/Apr/15 11:57,14/Jul/23 06:27,28/Apr/15 11:57,1.3.0,,,,,,1.4.0,,,,,,Spark Shell,Windows,,,0,,,,,,"Not all jars supplied via the --jars option will be added to the driver (and presumably executor) classpath.  The first jar(s) will be added, but not all.

To reproduce this, just add a few jars (I tested 5) to the --jars option, and then try to import a class from the last jar.  This fails.  A simple reproducer: 

Create a bunch of dummy jars:
jar cfM jar1.jar log.txt
jar cfM jar2.jar log.txt
jar cfM jar3.jar log.txt
jar cfM jar4.jar log.txt

Start the spark-shell with the dummy jars and guava at the end:
%SPARK_HOME%\bin\spark-shell --master local --jars jar1.jar,jar2.jar,jar3.jar,jar4.jar,c:\code\lib\guava-14.0.1.jar

In the shell, try importing from guava; you'll get an error:
{code}
scala> import com.google.common.base.Strings
<console>:19: error: object Strings is not a member of package com.google.common.base
       import com.google.common.base.Strings
              ^
{code}


",Win64,apachespark,glenn.strycker@gmail.com,jongyoul,michaelmalak,tsudukim,vanzin,vjapache,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4941,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 11:57:09 UTC 2015,,,,,,,,,,"0|i271lj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/15 14:31;srowen;Hm, does it work without the dummy jars? I suspect a different Windows-related problem in your syntax.;;;","20/Mar/15 14:58;vjapache;It works when guava is the 1st or 2nd jar.  Not sure at what point spark starts dropping jars, but I had this issue with multiple 'real' jars (i.e. containing .class files) in the --jars option: If I move a jar to the front of the list, it works; move it to the back, it fails.;;;","20/Mar/15 15:07;srowen;The weird thing  is the example shows it finds the Guava package, but I suspect that is coming from Spark's one unshaded Guava class, Optional. It doesn't really make sense that it would work in position 1 or 2 only. Is this what you are running, exactly? Can you debug further to find out the exact command line that is launched?;;;","22/Mar/15 20:59;srowen;I tried a simplified version of this with {{spark-shell}}:

{code}
spark-shell --master local --jars android-core.jar,core.jar,javase.jar
{code}

and it worked as expected. I was able to access code in all the JARs. Same worked on YARN.

The JARs were added too:

{code}
15/03/22 13:57:53 INFO SparkContext: Added JAR file:/home/srowen/android-core.jar at http://10.16.180.26:49005/jars/android-core.jar with timestamp 1427057873313
15/03/22 13:57:53 INFO SparkContext: Added JAR file:/home/srowen/core.jar at http://10.16.180.26:49005/jars/core.jar with timestamp 1427057873315
15/03/22 13:57:53 INFO SparkContext: Added JAR file:/home/srowen/javase.jar at http://10.16.180.26:49005/jars/javase.jar with timestamp 1427057873315
{code}

This wasn't Windows though. I suppose it's possible there's something wrong with how the classpath is reassembled for Windows or something. Or could have been fixed along the way.;;;","23/Mar/15 10:45;vjapache;I tested this on Linux with the 1.3.0 release, works fine.  Apparently a windows-specific issue.  Apparently on windows only the 1st jar is picked up.  This appears to be a problem with parsing the command line, introduced by the change in windows scripts between 1.2.0 and 1.3.0.  A simple fix to bin\windows-utils.cmd resolves the issue.

I ran this command to test with 'real' jars:
{code}
%SPARK_HOME%\bin\spark-shell --master local --jars c:\code\elasticsearch-1.4.2\lib\lucene-core-4.10.2.jar,c:\temp\guava-14.0.1.jar
{code}

Here are some snippets from the console - note that only the 1st jar is added; I can load classes from the 1st jar but not the 2nd:
{code}
15/03/23 10:57:41 INFO SparkUI: Started SparkUI at http://vgarla-t440P.fritz.box
:4040
15/03/23 10:57:41 INFO SparkContext: Added JAR file:/c:/code/elasticsearch-1.4.2/lib/lucene-core-4.10.2.jar at http://192.168.178.41:54601/jars/lucene-core-4.10.2.jar with timestamp 1427104661969
15/03/23 10:57:42 INFO Executor: Starting executor ID <driver> on host localhost
...
scala> import org.apache.lucene.util.IOUtils
import org.apache.lucene.util.IOUtils

scala> import com.google.common.base.Strings
<console>:20: error: object Strings is not a member of package com.google.common.base
{code}

Looking at the command line in jvisualvm, I see that only the 1st jar is aded:
{code}
Main class: org.apache.spark.deploy.SparkSubmit
Arguments: --class org.apache.spark.repl.Main --master local --jars c:\code\elasticsearch-1.4.2\lib\lucene-core-4.10.2.jar spark-shell c:\temp\guava-14.0.1.jar
{code}
In spark 1.2.0, spark-shell2.cmd just passed arguments ""as is"" to the java command line:
{code}
cmd /V /E /C %SPARK_HOME%\bin\spark-submit.cmd --class org.apache.spark.repl.Main %* spark-shell
{code}

In spark 1.3.0, spark-shell2.cmd calls windows-utils.cmd to parse arguments into SUBMISSION_OPTS and APPLICATION_OPTS.  Only the first jar in the list passed to --jars makes it into the SUBMISSION_OPTS; latter jars are added to APPLICATION_OPTS:
{code}
call %SPARK_HOME%\bin\windows-utils.cmd %*
if %ERRORLEVEL% equ 1 (
  call :usage
  exit /b 1
)
echo SUBMISSION_OPTS=%SUBMISSION_OPTS%
echo APPLICATION_OPTS=%APPLICATION_OPTS%

cmd /V /E /C %SPARK_HOME%\bin\spark-submit.cmd --class org.apache.spark.repl.Main %SUBMISSION_OPTS% spark-shell %APPLICATION_OPTS%
{code}

The problem is that by the time the command line arguments get to windows-utils.cmd, the windows command line processor has split the comma-separated list into distinct arguments.  The windows way of saying ""treat this as a single arg"" is to surround in double-quotes.  However, when I surround the jars in quotes, I get an error:
{code}
%SPARK_HOME%\bin\spark-shell --master local --jars ""c:\code\elasticsearch-1.4.2\lib\lucene-core-4.10.2.jar,c:\temp\guava-14.0.1.jar""
c:\temp\guava-14.0.1.jar""""==""x"" was unexpected at this time.
{code}
Digging in, I see this is caused by this line from windows-utils.cmd:
{code}
  if ""x%2""==""x"" (
{code}

Replacing the quotes with square brackets does the trick:
{code}
  if [x%2]==[x] (
{code}

Now the command line is processed correctly.

;;;","23/Mar/15 19:20;srowen;Great debugging! [~tsudukim] do you have thoughts on this? I think this bit was part of your change in https://github.com/apache/spark/commit/8d932475e6759e869c16ce6cac203a2e56558716#diff-7ac5881d6bad553b23f5225775c8fde3

So, it sounds like you do need to quote the comma-separated arg? but then quoting doesn't work as expected?

The {{""x%2""==""x""}} idiom is used several places in the Windows scripts. Is the square bracket syntax definitely preferred?;;;","23/Mar/15 19:52;vjapache;I came up with square brackets after 2 minutes of googling/stackoverflowing; a more thorough search/understanding of bat scripts might result in a better/different solution (I can rule myself out of the more thorough bat script understanding).  That being said, this test is used to check for an empty string.  Square brackets is the most upvoted solution: http://stackoverflow.com/questions/2541767/what-is-the-proper-way-to-test-if-variable-is-empty-in-a-batch-file-if-not-1
;;;","26/Mar/15 09:56;tsudukim;I think {{""%~2""==""""}} way is better for this.
But this script no longer exists now because the launching method of spark has changed drastically by [SPARK-4924].;;;","26/Mar/15 12:33;srowen;Yes this is a moot point in 1.4 and after, but I'd love to get a working solution for 1.3, even if it's a bit hacky. [~tsudukim] do you think the square-bracket syntax is going to not work in some cases? at least, it seems to fix this issue. [~vjapache] does the alternative syntax above work, without the ""x""?;;;","27/Mar/15 01:17;tsudukim;{code}
if [%2]==[] (
{code}
I think this square bracket style also works properly.

I tested it on my Windows 8.1. {{x}} is not needed.;;;","27/Mar/15 09:12;tsudukim;I looked into the script of the latest version and unfortunately found that it doesn't work properly too.
We have the same symptom when we specify multiple jars with --jars option in spark-shell.cmd, but the cause is different.

These work fine.
{code}
bin\spark-shell.cmd --jars C:\jar1.jar
bin\spark-shell.cmd --jars ""C:\jar1.jar""
{code}

But this doesn't work.
{code}
bin\spark-shell.cmd --jars ""C:\jar1.jar,C:\jar2.jar""
{code}
this gets
{code}
Exception in thread ""main"" java.net.URISyntaxException: Illegal character in path at index 11: C:/jar1.jar C:/jar2.jar
{code}
;;;","27/Mar/15 09:16;vjapache;Strange - when I test it with multiple jars (with the fixed script) everything works.
Something has changed in some other script wrt the released 1.3.0;;;","27/Mar/15 09:42;tsudukim;This cause is the following code in spark-class2.cmd
{code}
for /f ""tokens=*"" %%i in ('cmd /C """"%RUNNER%"" -cp %LAUNCHER_CP% org.apache.spark.launcher.Main %*""') do (
  set SPARK_CMD=%%i
)
%SPARK_CMD%
{code}

""for"" clause is a little bit complex, but we can get this when we expand its variables between the single quotations.
{code}
'cmd /C """"C:\Program Files\Java\jdk1.7.0_67\bin\java"" -cp C:\Users\tsudukim\Documents\workspace\spark-dev\bin\..\launcher\target\scala-2.10\classes org.apache.spark.launcher.Main org.apache.spark.deploy.SparkSubmit --class org.apache.spark.repl.Main --jars ""C:\jar1.jar,C:\jar2.jar""""'
{code}
and when this is executed, java code (launcher\src\main\java\org\apache\spark\launcher\Main.java) receives the args like followings as ""argsArray"".
{code}
[0] = {java.lang.String@410}""org.apache.spark.deploy.SparkSubmit""
[1] = {java.lang.String@417}""--class""
[2] = {java.lang.String@418}""org.apache.spark.repl.Main""
[3] = {java.lang.String@419}""--jars""
[4] = {java.lang.String@420}""C:\jar1.jar C:\jar2.jar""
{code}
The comma between C:\jar1.jar and C:\jar2.jar disappeared here.

The handling of double quotation in Windows batch is so difficult.
I'm not sure but perhaps it was parsed twice.
The separator of args is only space in bash, but space, semicolon and comma are also used in Windows batch.
So comma was converted to space in 1st parse.

To avoid this problem, I think it's better to parse only once, not twice.
(escaping double-quotations in Windows batch is so painful.);;;","27/Mar/15 09:43;tsudukim;Release 1.3.0 works fine.
But the problem occurs in the latest script in master branch (under developing for 1.4).
;;;","27/Mar/15 09:49;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/5227;;;","27/Mar/15 12:02;srowen;OK, [~vjapache] would you like to submit a PR that changes to use the brackets? You may need two PRs, one for branch-1.3 and one for master, since some occurrences are now gone in master.

[~tsudukim] OK I understand your pull request is to fix a similar issue but in the new 1.4 / master code?;;;","31/Mar/15 07:06;tsudukim;Sorry to confuse you. My PR is for master code, but not for branch-1.3.
I assume [~vjapache] will send the PR for branch-1.3 but if you don't have enough time, I can make it.;;;","31/Mar/15 07:15;tsudukim;Ah, the problems occured in branch-1.3 and in master branch are similar, but their causes are completely different.
[~srowen] if you think it's better we manage these problems in separate JIRA tickets, let's do so. How do you think?;;;","31/Mar/15 12:29;srowen;[~tsudukim] I think this JIRA is a fine place to track this work. No need for a separate issue even if there are different changes for branch 1.3 and master.
You have already started fixing this for master in https://github.com/apache/spark/pull/5227 so I think you can proceed there.
;;;","01/Apr/15 01:21;tsudukim;[~srowen] OK, thank you! Then I'm going to proceed here.;;;","03/Apr/15 01:05;vanzin;I haven't tested this on 1.3 so I can't comment. But are you guys sure this is an issue on master? See the session below:

{noformat}
C:\cygwin64\home\admin\work\spark>bin\spark-shell --jars ..\jar1.jar,..\jar2.jar,..\jar3.jar
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/04/02 18:02:56 INFO SecurityManager: Changing view acls to: admin
15/04/02 18:02:56 INFO SecurityManager: Changing modify acls to: admin
15/04/02 18:02:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
15/04/02 18:02:56 INFO HttpServer: Starting HTTP Server
15/04/02 18:02:57 INFO Utils: Successfully started service 'HTTP class server' on port 49289.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.4.0-SNAPSHOT
      /_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_25)
Type in expressions to have them evaluated.
Type :help for more information.
15/04/02 18:03:44 INFO SparkContext: Running Spark version 1.4.0-SNAPSHOT
15/04/02 18:03:45 INFO SecurityManager: Changing view acls to: admin
15/04/02 18:03:45 INFO SecurityManager: Changing modify acls to: admin
15/04/02 18:03:45 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(admin); users with modify permissions: Set(admin)
15/04/02 18:03:53 INFO Slf4jLogger: Slf4jLogger started
15/04/02 18:03:54 INFO Remoting: Starting remoting
15/04/02 18:04:00 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@vanzin-win7:49302]
15/04/02 18:04:00 INFO Utils: Successfully started service 'sparkDriver' on port 49302.
15/04/02 18:04:00 INFO SparkEnv: Registering MapOutputTracker
15/04/02 18:04:01 INFO SparkEnv: Registering BlockManagerMaster
15/04/02 18:04:01 INFO DiskBlockManager: Created local directory at C:\Users\admin\AppData\Local\Temp\spark-6f29266b-d302-4917-9e7a-cbbc77d87faa\blockmgr-398614d1-f50d-450f-becb-9230aaf5200b
15/04/02 18:04:01 INFO MemoryStore: MemoryStore started with capacity 267.3 MB
15/04/02 18:04:02 INFO HttpFileServer: HTTP File server directory is C:\Users\admin\AppData\Local\Temp\spark-6f29266b-d302-4917-9e7a-cbbc77d87faa\httpd-f4510339-e54b-44cb-a201-22a23d32c8d6
15/04/02 18:04:02 INFO HttpServer: Starting HTTP Server
15/04/02 18:04:02 INFO Utils: Successfully started service 'HTTP file server' on port 49303.
15/04/02 18:04:03 INFO SparkEnv: Registering OutputCommitCoordinator
15/04/02 18:04:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/04/02 18:04:04 INFO SparkUI: Started SparkUI at http://vanzin-win7:4040
15/04/02 18:04:05 INFO SparkContext: Added JAR file:/C:/cygwin64/home/admin/work/spark/../jar1.jar at http://192.168.56.101:49303/jars/jar1.jar with timestamp 1428023045788
15/04/02 18:04:05 INFO SparkContext: Added JAR file:/C:/cygwin64/home/admin/work/spark/../jar2.jar at http://192.168.56.101:49303/jars/jar2.jar with timestamp 1428023045804
15/04/02 18:04:05 INFO SparkContext: Added JAR file:/C:/cygwin64/home/admin/work/spark/../jar3.jar at http://192.168.56.101:49303/jars/jar3.jar with timestamp 1428023045819
15/04/02 18:04:07 INFO Executor: Starting executor ID <driver> on host localhost
15/04/02 18:04:07 INFO Executor: Using REPL class URI: http://192.168.56.101:49289
15/04/02 18:04:07 INFO AkkaUtils: Connecting to HeartbeatReceiver: akka.tcp://sparkDriver@vanzin-win7:49302/user/HeartbeatReceiver
15/04/02 18:04:10 INFO NettyBlockTransferService: Server created on 49310
15/04/02 18:04:10 INFO BlockManagerMaster: Trying to register BlockManager
15/04/02 18:04:10 INFO BlockManagerMasterActor: Registering block manager localhost:49310 with 267.3 MB RAM, BlockManagerId(<driver>, localhost, 49310)
15/04/02 18:04:10 INFO BlockManagerMaster: Registered BlockManager
15/04/02 18:04:15 INFO SparkILoop: Created spark context..
Spark context available as sc.
15/04/02 18:04:18 INFO SparkILoop: Created sql context (with Hive support)..
SQL context available as sqlContext.

scala> getClass().getResource(""/test.txt"")
res0: java.net.URL = jar:file:/C:/cygwin64/home/admin/work/spark/../jar1.jar!/test.txt

scala> getClass().getResource(""/test2.txt"")
res2: java.net.URL = jar:file:/C:/cygwin64/home/admin/work/spark/../jar2.jar!/test2.txt

scala>
{noformat}

The current PR is against master, and I'd like to avoid changing that code unless it's reeeeeally needed. Those batch scripts are already cryptic enough.;;;","03/Apr/15 05:53;tsudukim;Please try with double-quotation. (double-quotation is required when the path includes space etc)
This doesn't work.
{code}
bin\spark-shell.cmd --jars ""..\jar1.jar,..\jar2.jar,..\jar3.jar""
{code}

And in this case, the present code has a bug on another bug. So, it is not working as we intend to, but it just only seems to be working coincidentally in some case.
I think we shouldn't keep the codes that contains bugs. If we keep them, it will be more difficult to change the codes in the future. (If it did NOT contain bugs, I would agree with you.)

And I also agree that the batch scripts are cryptic. To avoid it, It's better to change *.cmd to other language like powershell.;;;","04/Apr/15 07:38;srowen;I feel like it's worth fixing, cryptic or not. Try reproducing with the quotes? and if it fails, let's go with the PR? it seems reasonable and is confined to the Windows support.;;;","28/Apr/15 11:57;srowen;Issue resolved by pull request 5227
[https://github.com/apache/spark/pull/5227];;;",,,,,
Cannot load parquet data with partitions if not all partition columns match data columns,SPARK-6432,12783542,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,huangjs,huangjs,20/Mar/15 09:06,28/May/15 11:20,14/Jul/23 06:27,28/May/15 11:20,1.3.0,1.3.1,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Suppose we have a dataset in the following folder structure:

{noformat}
parquet/source=live/date=2015-03-18/
parquet/source=live/date=2015-03-19/
...
{noformat}

And the data schema has the following columns:
- id
- *event_date*
- source
- value

Where partition key source matches data column source, but partition key date doesn't match any columns in data.

Then we cannot load dataset in Spark using parquetFile. It reports:

{code}
org.apache.spark.sql.AnalysisException: Ambiguous references to source: (source#2,List()),(source#5,List());
...
{code}

Currently if partition columns has overlaps with data columns, partition columns have to be a subset of the data columns.

Jianshi",,huangjs,lian cheng,zzcclp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu May 28 11:18:53 UTC 2015,,,,,,,,,,"0|i2714f:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"20/Mar/15 09:33;zzcclp;[~huangjs], I have some parquet files in partitions path, as follow:
2015-03-18 10:38 /zzc_test/parquetbypartitons/puser=impala.data1
2015-03-18 10:39 /zzc_test/parquetbypartitons/puser=impala.data2
Load dataset correctly:
root
 |-- ltype: integer (nullable = false)
 |-- chan: string (nullable = false)
 |-- ts: integer (nullable = false)
 |-- cip: string (nullable = false)
 |-- rt: string (nullable = false)
 |-- date: string (nullable = false)
 |-- time: string (nullable = false)
 |-- host: string (nullable = false)
 |-- ratio: integer (nullable = false)
 |-- size: long (nullable = false)
 |-- code: integer (nullable = false)
 |-- dltime: long (nullable = false)
 |-- cache: string (nullable = false)
 |-- bsize: long (nullable = false)
 |-- upsize: long (nullable = false)
 |-- url: string (nullable = false)
 |-- referer: string (nullable = false)
 |-- ua: string (nullable = false)
 |-- *puser: string (nullable = true)*
 |-- pdate: string (nullable = true)
 |-- pslice: string (nullable = true)
 |-- pcache: integer (nullable = true);;;","20/Mar/15 09:36;lian cheng;The problem is that, if all partition columns appeared in the path exist in the data files, it's fine. But if only some of the partition columns exist in the data file, it ends up with duplicated columns. You case belongs to the first category.;;;","20/Mar/15 09:48;zzcclp;[~liancheng], My data schema has the following columns:
root
 |-- ltype: integer (nullable = false)
 |-- chan: string (nullable = false)
 |-- ts: integer (nullable = false)
 |-- cip: string (nullable = false)
 |-- rt: string (nullable = false)
 |-- date: string (nullable = false)
 |-- time: string (nullable = false)
 |-- host: string (nullable = false)
 |-- ratio: integer (nullable = false)
 |-- size: long (nullable = false)
 |-- code: integer (nullable = false)
 |-- dltime: long (nullable = false)
 |-- cache: string (nullable = false)
 |-- bsize: long (nullable = false)
 |-- upsize: long (nullable = false)
 |-- url: string (nullable = false)
 |-- referer: string (nullable = false)
 |-- ua: string (nullable = false)

puser column appeared in the path don't exist in the data files. ;;;","20/Mar/15 09:55;huangjs;If no partition column appear in the data columns, then it's fine.


Jianshi;;;","20/Mar/15 10:00;zzcclp;Thanks, [~liancheng], [~huangjs];;;","20/Mar/15 10:37;lian cheng;Yeah, sorry for my carelessness, it's OK if either all or no partition columns appear in the data files.;;;","28/May/15 11:18;lian cheng;This use case is covered by newly introduced {{HadoopFsRelation}}, so I'm resolving this.;;;",,,,,,,,,,,,,,,,,,,,,,
Couldn't find leader offsets exception when creating KafkaDirectStream,SPARK-6431,12783532,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,koeninger,ardlema,ardlema,20/Mar/15 08:39,25/Aug/17 08:33,14/Jul/23 06:27,12/Apr/15 16:37,1.3.0,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,"When I try to create an InputDStream using the createDirectStream method of the KafkaUtils class and the kafka topic does not have any messages yet am getting the following error:

org.apache.spark.SparkException: Couldn't find leader offsets for Set()
org.apache.spark.SparkException: org.apache.spark.SparkException: Couldn't find leader offsets for Set()
	at org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:413)

If I put a message in the topic before creating the DirectStream everything works fine.",,apachespark,ardlema,koeninger,krisden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6434,,,,,,,,,SPARK-12775,SPARK-21836,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Apr 12 16:37:41 UTC 2015,,,,,,,,,,"0|i27127:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/15 14:37;koeninger;I think this got mis-diagnosed on the mailing list, sorry for the confusion.

The only way I've been able to reproduce that exception is by trying to start a stream for a topic that doesn't exist at all.  Alberto, did you actually run kafka-topics.sh --create before starting the job, or in some other way create the topic?  Pretty sure what happened here is that your topic didn't exist the first time you ran the job.  Your brokers were set to auto-create topics, so it did exist the next time you ran the job.  Putting a message into the topic didn't have anything to do with it.

Here's why I think that's what happened.  Following console session is an example, where ""empty"" topic existed prior to starting the console, but had no messages.  Topic ""hasonemesssage"" existed and had one message in it.  Topic ""doesntexistyet"" didn't exist at the beginning of the console.

The metadata apis return the same info for existing-but-empty topics as they do for topics with messages in them:

scala> kc.getPartitions(Set(""empty"")).right
res0: scala.util.Either.RightProjection[org.apache.spark.streaming.kafka.KafkaCluster.Err,Set[kafka.common.TopicAndPartition]] = RightProjection(Right(
Set([empty,0], [empty,1])))

scala> kc.getPartitions(Set(""hasonemessage"")).right
res1: scala.util.Either.RightProjection[org.apache.spark.streaming.kafka.KafkaCluster.Err,Set[kafka.common.TopicAndPartition]] = RightProjection(Right(Set([hasonemessage,0], [hasonemessage,1])))


Leader offsets are both 0 for the empty topic, as you'd expect:

scala> kc.getLatestLeaderOffsets(kc.getPartitions(Set(""empty"")).right.get)
res5: Either[org.apache.spark.streaming.kafka.KafkaCluster.Err,Map[kafka.common.TopicAndPartition,org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset]] = Right(Map([empty,1] -> LeaderOffset(localhost,9094,0), [empty,0] -> LeaderOffset(localhost,9093,0)))

And one of the leader offsets is 1 for the topic with one message:

scala> kc.getLatestLeaderOffsets(kc.getPartitions(Set(""hasonemessage"")).right.get)
res6: Either[org.apache.spark.streaming.kafka.KafkaCluster.Err,Map[kafka.common.TopicAndPartition,org.apache.spark.streaming.kafka.KafkaCluster.LeaderOffset]] = Right(Map([hasonemessage,0] -> LeaderOffset(localhost,9092,1), [hasonemessage,1] -> LeaderOffset(localhost,9093,0)))


The first time a metadata request is made against the non-existing topic, it returns empty:

kc.getPartitions(Set(""doesntexistyet"")).right
res2: scala.util.Either.RightProjection[org.apache.spark.streaming.kafka.KafkaCluster.Err,Set[kafka.common.TopicAndPartition]] = RightProjection(Right(Set()))


But if your brokers are configured with auto.create.topics.enable set to true, that metadata request alone is enough to trigger creation of the topic.  Requesting it again shows that the topic has been created:

scala> kc.getPartitions(Set(""doesntexistyet"")).right
res3: scala.util.Either.RightProjection[org.apache.spark.streaming.kafka.KafkaCluster.Err,Set[kafka.common.TopicAndPartition]] = RightProjection(Right(Set([doesntexistyet,0], [doesntexistyet,1])))


If you don't think that explains what happened, please let me know if you have a way of reproducing that exception against an existing-but-empty topic, because I cant.

As far as what to do about this, my instinct is to just improve the error handling for the getPartitions call.  If the topic doesn't exist yet, It shouldn't be returning an empty set, it should be returning an error.
;;;","07/Apr/15 07:43;ardlema;You're absolutely right Cody. I've been having a look at my code and I've found out that I'm not creating the topic before creating the DirectStream. If you are interested here is the test I am running: https://github.com/ardlema/big-brother/blob/master/src/test/scala/org/ardlema/spark/DwellDetectorTest.scala

I completely agree with you, If the topic doesn't exist it should be returning an error and not a misleading empty set.

;;;","10/Apr/15 14:00;apachespark;User 'koeninger' has created a pull request for this issue:
https://github.com/apache/spark/pull/5454;;;","12/Apr/15 16:37;srowen;Issue resolved by pull request 5454
[https://github.com/apache/spark/pull/5454];;;",,,,,,,,,,,,,,,,,,,,,,,,,
_regression_train_wrapper does not test initialWeights correctly,SPARK-6421,12783421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lewuathe,josephkb,josephkb,19/Mar/15 23:08,20/Mar/15 21:18,14/Jul/23 06:27,20/Mar/15 21:18,1.3.0,,,,,,1.3.1,1.4.0,,,,,MLlib,PySpark,,,0,,,,,,"There is a bug in this line: [https://github.com/apache/spark/blob/f17d43b033d928dbc46aef8e367aa08902e698ad/python/pyspark/mllib/regression.py#L138]

You can reproduce this bug as follows:
{code}
>>> from numpy import array
>>> a = array([1,2,3])
>>> b = a or [1.0] * 3
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()
{code}
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 20 21:18:45 UTC 2015,,,,,,,,,,"0|i270dj:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"20/Mar/15 06:30;apachespark;User 'Lewuathe' has created a pull request for this issue:
https://github.com/apache/spark/pull/5101;;;","20/Mar/15 21:18;mengxr;Issue resolved by pull request 5101
[https://github.com/apache/spark/pull/5101];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
GenerateOrdering does not support BinaryType and complex types.,SPARK-6419,12783407,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,yhuai,yhuai,19/Mar/15 22:38,10/Jun/15 18:12,14/Jul/23 06:27,10/Jun/15 18:12,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"When user want to order by binary columns or columns with complex types and code gen is enabled, there will be a MatchError ([see here|https://github.com/apache/spark/blob/v1.3.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateOrdering.scala#L45]). We can either add supports for these types or have a function to check if we can safely call GenerateOrdering (like the canBeCodeGened for HashAggregation Strategy).",,davies,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7956,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jun 10 18:11:54 UTC 2015,,,,,,,,,,"0|i270an:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"19/Mar/15 22:39;yhuai;For now, the workaround is to disable code gen for queries requiring order by on binary columns or columns with complex types (set spark.sql.codegen=false).;;;","10/Jun/15 18:11;davies;Fixed by SPARK-7956;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Document that RDD.fold() requires the operator to be commutative,SPARK-6416,12783325,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,joshrosen,joshrosen,19/Mar/15 18:03,03/Jan/16 08:55,14/Jul/23 06:27,03/Jan/16 08:55,1.4.0,,,,,,1.5.0,,,,,,Documentation,Spark Core,,,0,,,,,,"Spark's {{RDD.fold}} operation has some confusing behaviors when a non-commutative reduce function is used.

Here's an example, which was originally reported on StackOverflow (https://stackoverflow.com/questions/29150202/pyspark-fold-method-output):

{code}
sc.parallelize([1,25,8,4,2]).fold(0,lambda a,b:a+1 )
8
{code}

To understand what's going on here, let's look at the definition of Spark's `fold` operation.  

I'm going to show the Python version of the code, but the Scala version exhibits the exact same behavior (you can also [browse the source on GitHub|https://github.com/apache/spark/blob/8cb23a1f9a3ed08e57865bcb6cc1cc7902881073/python/pyspark/rdd.py#L780]:

{code}
    def fold(self, zeroValue, op):
        """"""
        Aggregate the elements of each partition, and then the results for all
        the partitions, using a given associative function and a neutral ""zero
        value.""
        The function C{op(t1, t2)} is allowed to modify C{t1} and return it
        as its result value to avoid object allocation; however, it should not
        modify C{t2}.
        >>> from operator import add
        >>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)
        15
        """"""
        def func(iterator):
            acc = zeroValue
            for obj in iterator:
                acc = op(obj, acc)
            yield acc
        vals = self.mapPartitions(func).collect()
        return reduce(op, vals, zeroValue)
{code}

(For comparison, see the [Scala implementation of `RDD.fold`|https://github.com/apache/spark/blob/8cb23a1f9a3ed08e57865bcb6cc1cc7902881073/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L943]).

Spark's `fold` operates by first folding each partition and then folding the results.  The problem is that an empty partition gets folded down to the zero element, so the final driver-side fold ends up folding one value for _every_ partition rather than one value for each _non-empty_ partition.  This means that the result of `fold` is sensitive to the number of partitions:

{code}
    >>> sc.parallelize([1,25,8,4,2], 100).fold(0,lambda a,b:a+1 )
    100
    >>> sc.parallelize([1,25,8,4,2], 50).fold(0,lambda a,b:a+1 )
    50
    >>> sc.parallelize([1,25,8,4,2], 1).fold(0,lambda a,b:a+1 )
    1
{code}

In this last case, what's happening is that the single partition is being folded down to the correct value, then that value is folded with the zero-value at the driver to yield 1.

I think the underlying problem here is that our fold() operation implicitly requires the operator to be commutative in addition to associative, but this isn't documented anywhere.  Due to ordering non-determinism elsewhere in Spark, such as SPARK-5750, I don't think there's an easy way to fix this.  Therefore, I think we should update the documentation and examples to clarify this requirement and explain that our fold acts more like a reduce with a default value than the type of ordering-sensitive fold() that users may expect in functional languages.",,apachespark,joshrosen,markhamstra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7683,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 03 08:55:03 UTC 2016,,,,,,,,,,"0|i26zsn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/15 10:31;srowen;Josh I'm looking at the related SPARK-7683 which will require a behavior change at some point in the future. Is this something you would change the implementation of, in the future (2.x) or sooner? 

I can try to document this in the short term anyway.;;;","17/May/15 14:59;joshrosen;Hey Sean,

I don't think that this will be easy to fix (efficiently) due to the ordering non-determinism issues mentioned above, so I think that we should update the docs to clarify the commutativity requirements.  SPARK-7683, on the other hand, seems like an obvious fix for 2.x.  For 2.x, we could also consider removing fold, since it seems to be a source of confusion.;;;","18/May/15 08:38;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6231;;;","21/May/15 18:44;srowen;The documentation update is done. Possibly worth leaving this open to track maybe removing this op in 2.0.;;;","21/May/15 19:02;markhamstra;Why remove it?  It's very useful when used correctly.;;;","21/May/15 19:15;srowen;I think the argument was that it is just a different form of reduce() since it can't guarantee to fold things serially, in order, into a zero value. I'm not strongly advocating for further action here, myself.;;;","01/Jan/16 15:45;srowen;[~joshrosen] [~markhamstra] WDYT about deprecating fold() for 2.x? or else, I think this can be closed.;;;","01/Jan/16 19:23;markhamstra;I don't see any reason to change the API wrt `fold`.  With operations on RDDs, we generally try to achieve the same semantics as for Scala parallel collections, and that does hold true for `fold`:

  scala> val list = (1 to 10000).toList

  scala> list.fold(0)(_ + _)
  res0: Int = 50005000

  scala> list.par.fold(0)(_ + _)
  res1: Int = 50005000

  scala> list.fold(1)(_ + _)
  res2: Int = 50005001

  scala> list.par.fold(1)(_ + _)
  res3: Int = 50005039


If we need to change anything, it would simply be to change our API documentation to more closely match that of the Scala Standard Library, where the first argument to `fold` is described as: ""a neutral element for the fold operation, it may be added to the result an arbitrary number of times, not changing the result (e.g. Nil for list concatenation, 0 for addition, or 1 for multiplication)"".;;;","02/Jan/16 09:43;srowen;That's an interesting example, in that I wouldn't have though the 3rd and 4th examples would differ. However your example does violate the contract, since you're providing 1 as a neutral element for addition, which isn't valid. In Josh's example, he passes 0 and still gets the differing results depending on partitions. Your 1st and 2nd examples show Scala APIs would give the same answer. Does that change your thinking?;;;","02/Jan/16 23:44;markhamstra;I still don't see RDD#fold as being out of bounds with what should be expected from the Scala parallel collections model -- there, too, you can get confusing results if you don't pay attention to the partitioned nature of the operation:
{code}
scala> val list1 = (1 to 10000).toList

scala> val list2 = (1 to 100).toList

scala> list1.fold(0){ case (a, b) => a + 1 }
res0: Int = 10000

scala> list1.par.fold(0){ case (a, b) => a + 1 }
res1: Int = 162

scala> list2.fold(0){ case (a, b) => a + 1 }
res2: Int = 100

scala> list2.par.fold(0){ case (a, b) => a + 1 }
res3: Int = 7
{code};;;","03/Jan/16 08:55;srowen;Hm! I should have tried that myself. I think that's a good argument that at least it's not inconsistent. The behavior is documented by an earlier change so calling this resolved, retroactively.;;;",,,,,,,,,,,,,,,,,,
Spark driver failed with NPE on job cancelation,SPARK-6414,12783322,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,hunglin,ymakhno,ymakhno,19/Mar/15 17:59,17/May/20 17:47,14/Jul/23 06:27,02/Apr/15 21:21,1.2.1,1.3.0,,,,,1.2.2,1.3.1,1.4.0,,,,Scheduler,Spark Core,,,0,,,,,,"When a job group is cancelled, we scan through all jobs to determine which are members of the group. This scan assumes that the job group property is always set. If 'properties' is null in an active job, you get an NPE.

We just need to make sure we ignore ones where activeJob.properties is null. We should also make sure it works if the particular property is missing.

https://github.com/apache/spark/blob/branch-1.3/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L678",,apachespark,hunglin,joshrosen,ymakhno,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 02 21:21:28 UTC 2015,,,,,,,,,,"0|i26zrz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/15 05:44;apachespark;User 'hunglin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5124;;;","02/Apr/15 21:21;joshrosen;Fixed in 1.2.2, 1.3.1, and 1.4.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
PySpark DataFrames can't be created if any datetimes have timezones,SPARK-6411,12783199,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,airhorns,airhorns,19/Mar/15 11:49,11/Jun/15 08:00,14/Jul/23 06:27,11/Jun/15 08:00,1.3.0,,,,,,1.5.0,,,,,,PySpark,SQL,,,0,,,,,,"I am unable to create a DataFrame with PySpark if any of the {{datetime}} objects that pass through the conversion process have a {{tzinfo}} property set. 

This works fine:

{code}
In [9]: sc.parallelize([(datetime.datetime(2014, 7, 8, 11, 10),)]).toDF().collect()
Out[9]: [Row(_1=datetime.datetime(2014, 7, 8, 11, 10))]
{code}

as expected, the tuple's schema is inferred as having one anonymous column with a datetime field, and the datetime roundtrips through to the Java side python deserialization and then back into python land upon {{collect}}. This however:

{code}
In [5]: from dateutil.tz import tzutc

In [10]: sc.parallelize([(datetime.datetime(2014, 7, 8, 11, 10, tzinfo=tzutc()),)]).toDF().collect()
{code}

explodes with

{code}
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12, localhost): net.razorvine.pickle.PickleException: invalid pickle data for datetime; expected 1 or 7 args, got 2
	at net.razorvine.pickle.objects.DateTimeConstructor.createDateTime(DateTimeConstructor.java:69)
	at net.razorvine.pickle.objects.DateTimeConstructor.construct(DateTimeConstructor.java:32)
	at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:154)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:153)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:119)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:114)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:114)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:114)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:114)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1520)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1520)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1211)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1200)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1199)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1199)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1401)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1362)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}

By the looks of the error, it would appear as though the java depickler isn't expecting the pickle stream to provide that extra timezone constructor argument.

Here's the disassembled pickle stream for a timezone-less datetime:

{code}
>>> object = datetime.datetime(2014, 7, 8, 11, 10)
>>> stream = pickle.dumps(object)
>>> pickletools.dis(stream)
    0: c    GLOBAL     'datetime datetime'
   19: p    PUT        0
   22: (    MARK
   23: S        STRING     '\x07\xde\x07\x08\x0b\n\x00\x00\x00\x00'
   65: p        PUT        1
   68: t        TUPLE      (MARK at 22)
   69: p    PUT        2
   72: R    REDUCE
   73: p    PUT        3
   76: .    STOP
highest protocol among opcodes = 0
{code}

and then for one with a timezone:

{code}
>>> object = datetime.datetime(2014, 7, 8, 11, 10, tzinfo=tzutc())
>>> stream = pickle.dumps(object)
>>> pickletools.dis(stream)
    0: c    GLOBAL     'datetime datetime'
   19: p    PUT        0
   22: (    MARK
   23: S        STRING     '\x07\xde\x07\x08\x0b\n\x00\x00\x00\x00'
   65: p        PUT        1
   68: c        GLOBAL     'copy_reg _reconstructor'
   93: p        PUT        2
   96: (        MARK
   97: c            GLOBAL     'dateutil.tz tzutc'
  116: p            PUT        3
  119: c            GLOBAL     'datetime tzinfo'
  136: p            PUT        4
  139: g            GET        4
  142: (            MARK
  143: t                TUPLE      (MARK at 142)
  144: R            REDUCE
  145: p            PUT        5
  148: t            TUPLE      (MARK at 96)
  149: p        PUT        6
  152: R        REDUCE
  153: p        PUT        7
  156: t        TUPLE      (MARK at 22)
  157: p    PUT        8
  160: R    REDUCE
  161: p    PUT        9
  164: .    STOP
highest protocol among opcodes = 0
{code}

I would bet that the Pyrolite library is missing support for that nested object as a second tuple member in the reconstruction of the datetime object. Has anyone hit this before? Any more information I can provide?",,airhorns,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7314,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 00:41:05 UTC 2015,,,,,,,,,,"0|i26z1j:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"25/Mar/15 14:33;airhorns;I've opened and issue on the upstream Pyrolite library which I think is causing this problem here: https://github.com/irmen/Pyrolite/issues/19;;;","04/May/15 05:04;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/5850;;;","04/May/15 05:56;mengxr;[~airhorns] I'm testing Spark with Pyrolite master branch. With your patch, it is possible to create DFs with datetime that contains tzinfo. However, I found two issues:

1. A tz-unaware date (or maybe datetime) object becomes tz-aware after a round trip, which makes `test_apply_schema` in `sql/tests.py` fail.
2. The tzinfo does not remain the same after a round trip.

Test code:
{code}
    def test_datetime_with_timezone(self):
        """"""
        SPARK-6411
        """"""
        try:
            import pytz
            has_pytz = True
        except:
            has_pytz = False

        if has_pytz:
            tz = pytz.timezone('America/Los_Angeles')
            date = datetime.datetime(2014, 7, 8, 11, 10, tzinfo=tz)
            first = self.sqlCtx.createDataFrame([(date,)]).first()[0]
            self.assertEqual(date, first)
{code}

Output:
{code}
======================================================================
FAIL: test_datetime_with_timezone (__main__.SQLTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/meng/src/spark/python/pyspark/sql/tests.py"", line 536, in test_datetime_with_timezone
    self.assertEqual(date, first)
AssertionError: datetime.datetime(2014, 7, 8, 11, 10, tzinfo=<DstTzInfo 'America/Los_Angeles' PST-1 day, 16:00:00 STD>) != datetime.datetime(2014, 7, 8, 11, 10, tzinfo=<DstTzInfo 'America/Los_Angeles' PDT-1 day, 17:00:00 DST>)
{code}

I will check the conversion for date/datetime. It would be really helpful if you could provide insights.;;;","16/May/15 01:01;davies;Since TimestampType in Spark SQL does not support timezone, there is no way to get the timezone back after a round trip. So we should drop the timezone for datetime before serializing (convert to UTC).

I will send out a PR soon.;;;","19/May/15 00:41;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/6250;;;",,,,,,,,,,,,,,,,,,,,,,,,
"It is not necessary that avoid old inteface of hive, because this will make some UDAF can not work.",SPARK-6409,12783164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,DoingDone9,DoingDone9,DoingDone9,19/Mar/15 09:07,25/Mar/15 18:12,14/Jul/23 06:27,25/Mar/15 18:12,1.2.1,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,starter,,,,,"I run SQL like that 
{code}
CREATE TEMPORARY FUNCTION test_avg AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage'; 
  
SELECT 
    test_avg(1), 
    test_avg(substr(value,5)) 
FROM src; 
{code}


then i get a exception
{code}
15/03/19 09:36:45 ERROR CliDriver: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 6, HPC-3): java.lang.ClassCastException: org.apache.hadoop.hive.ql.udf.generic.GenericUDAFAverage$AverageAggregationBuffer cannot be cast to org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator$AbstractAggregationBuffer 
        at org.apache.spark.sql.hive.HiveUdafFunction.<init>(hiveUdfs.scala:369) 
        at org.apache.spark.sql.hive.HiveGenericUdaf.newInstance(hiveUdfs.scala:214) 
        at org.apache.spark.sql.hive.HiveGenericUdaf.newInstance(hiveUdfs.scala:188) 
{code}


i find that GenericUDAFAverage used a deprecated interface AggregationBuffer that has been instead by AbstractAggregationBuffer. and spark avoid the old interface AggregationBuffer , so GenericUDAFAverage  can not work.I think it is not necessary.

code in spark
{code}
  // Cast required to avoid type inference selecting a deprecated Hive API.
  private val buffer =
    function.getNewAggregationBuffer.asInstanceOf[GenericUDAFEvaluator.AbstractAggregationBuffer]
{code}

",,apachespark,DoingDone9,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 18:12:12 UTC 2015,,,,,,,,,,"0|i26ytr:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"21/Mar/15 21:47;marmbrus;Reasonable.  Want to open a PR?;;;","23/Mar/15 01:09;DoingDone9;yes, i will open a PR;;;","23/Mar/15 01:55;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/5131;;;","25/Mar/15 18:12;marmbrus;Issue resolved by pull request 5131
[https://github.com/apache/spark/pull/5131];;;",,,,,,,,,,,,,,,,,,,,,,,,,
JDBCRDD fails on where clause with string literal,SPARK-6408,12783153,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pllee,pllee,pllee,19/Mar/15 07:45,24/Apr/15 00:46,14/Jul/23 06:27,22/Mar/15 09:18,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"The generated SQL query string is incorrect on filtering string literals.

{code}where foo='bar'{code} results in {code}where foo=bar{code}

The following snippet reproduce the bug:
{code}
$ SPARK_CLASSPATH=h2-1.4.186.jar spark/bin/spark-shell

import java.sql.DriverManager
val url = ""jdbc:h2:mem:testdb0""
Class.forName(""org.h2.Driver"")
val conn = DriverManager.getConnection(url)
conn.prepareStatement(""create schema test"").executeUpdate()
conn.prepareStatement(""create table test.people (name TEXT(32) NOT NULL, theid INTEGER NOT NULL)"").executeUpdate()
conn.prepareStatement(""insert into test.people values ('fred', 1)"").executeUpdate()
conn.commit()
sql(s""""""
CREATE TEMPORARY TABLE foobar
USING org.apache.spark.sql.jdbc
OPTIONS (url '$url', dbtable 'TEST.PEOPLE')
"""""")
sql(""select * from foobar where NAME='fred'"").collect

15/03/19 06:34:38 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
org.h2.jdbc.JdbcSQLException: Column ""FRED"" not found; SQL statement:
SELECT NAME,THEID FROM TEST.PEOPLE WHERE NAME = fred [42122-186]
{code}

Note that it is likely that other data types also have similar problem.
",,lian cheng,pllee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 22 09:18:12 UTC 2015,,,,,,,,,,"0|i26yrb:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"22/Mar/15 09:18;lian cheng;Issue resolved by pull request 5087
[https://github.com/apache/spark/pull/5087];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SQL]class not found exception thows when `add jar` use spark cli ,SPARK-6392,12782743,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,jeanlyn,jeanlyn,18/Mar/15 01:51,09/Aug/15 05:14,14/Jul/23 06:27,20/Jul/15 03:16,1.2.0,,,,,,,,,,,,SQL,,,,0,,,,,,"When we use spark cli to add jar dynamic,we will get the *java.lang.ClassNotFoundException* when we use the class of jar to create udf.For example:
{noformat}
spark-sql> add jar /home/jeanlyn/hello.jar;
spark-sql>create temporary function hello as 'hello';
spark-sql>select hello(name) from person;
Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.ClassNotFoundException: hello
{noformat}",,apachespark,gvramana,jeanlyn,jonathak,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 09 05:14:11 UTC 2015,,,,,,,,,,"0|i26waf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/15 02:22;apachespark;User 'jeanlyn' has created a pull request for this issue:
https://github.com/apache/spark/pull/5079;;;","20/Jul/15 00:34;rxin;[~jeanlyn] is this still a problem with the master branch?
;;;","20/Jul/15 03:15;jeanlyn;I think this issue is fixed by https://github.com/apache/spark/pull/4586.;;;","07/Aug/15 18:13;jonathak;Are you sure this is fixed by https://github.com/apache/spark/pull/4586? I still seem to be hitting this issue with UDFs even when using Spark 1.4.1, which includes https://github.com/apache/spark/pull/4586.;;;","09/Aug/15 05:14;jeanlyn;I thought it fixed in my case, do you have more descriptions about the issue, or how can we reproduce it?;;;",,,,,,,,,,,,,,,,,,,,,,,,
YARN app diagnostics report doesn't report NPEs,SPARK-6389,12782571,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,17/Mar/15 14:18,18/Mar/15 13:11,14/Jul/23 06:27,18/Mar/15 13:11,1.3.0,,,,,,1.4.0,,,,,,YARN,,,,0,,,,,,"{{ApplicationMaster.run()}} catches exceptions and calls {{toMessage()}} to get their message included in the YARN diagnostics report visible in the RM UI.

Except, NPEs don't have a message —if one is raised their report becomes {{Uncaught exception: null}}, which isn't that useful. The full text & stack trace is logged correctly in the AM.",,apachespark,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 13:11:11 UTC 2015,,,,,,,,,,"0|i26v9j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/15 14:24;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/5070;;;","18/Mar/15 13:11;srowen;Resolved by https://github.com/apache/spark/pull/5070;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Few examples on Dataframe operation give compiler errors ,SPARK-6383,12782510,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,tijoparacka,Tijo Paracka,Tijo Paracka,17/Mar/15 09:40,13/May/15 17:40,14/Jul/23 06:27,18/Mar/15 01:52,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,DataFrame,,,,,"The below statements give compiler errors as 
a) the select method doesnot accept String, Column 
df.select(""name"", df(""age"") + 1).show() // Need to convert String to Column

b) Filtering should be based on ""age""  not on ""name""  Column
df.filter(df(""name"") > 21).show()",,apachespark,tijoparacka,Tijo Paracka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 09:32:41 UTC 2015,,,,,,,,,,"0|i26uvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/15 09:40;apachespark;User 'tijoparacka' has created a pull request for this issue:
https://github.com/apache/spark/pull/5068;;;","24/Mar/15 09:32;tijoparacka;The Assignee: for this issues appeared as ""Unassigned"". Could you please change the assignee to my name.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Subqueries are thrown away too early in dataframes,SPARK-6376,12782462,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,17/Mar/15 03:23,14/Apr/15 06:16,14/Jul/23 06:27,24/Mar/15 21:09,,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"Because we throw away aliases as we construct the query plan, you can't reference them later.  For example, this query fails:

{code}
  test(""self join with aliases"") {
    val df = Seq(1,2,3).map(i => (i, i.toString)).toDF(""int"", ""str"")
    checkAnswer(
      df.as('x).join(df.as('y), $""x.str"" === $""y.str"").groupBy(""x.str"").count(),
      Row(""1"", 1) :: Row(""2"", 1) :: Row(""3"", 1) :: Nil)
  }
{code}

{code}
[info]   org.apache.spark.sql.AnalysisException: Cannot resolve column name ""x.str"" among (int, str, int, str);
[info]   at org.apache.spark.sql.DataFrame$$anonfun$resolve$1.apply(DataFrame.scala:162)
[info]   at org.apache.spark.sql.DataFrame$$anonfun$resolve$1.apply(DataFrame.scala:162)
{code}",,apachespark,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 06:16:30 UTC 2015,,,,,,,,,,"0|i26ulb:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"24/Mar/15 08:01;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5160;;;","14/Apr/15 06:16;rxin;Note that SPARK-6865 will break this since $""x.str"" will refer to a column named ""x.str"", rather than a column named ""str"" in table ""x"".
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad formatting in analysis errors,SPARK-6375,12782461,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,17/Mar/15 03:18,24/Mar/15 20:23,14/Jul/23 06:27,24/Mar/15 20:23,,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"{code}
[info]   org.apache.spark.sql.AnalysisException: Ambiguous references to str: (str#3,List()),(str#5,List());
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 06:15:05 UTC 2015,,,,,,,,,,"0|i26ul3:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"24/Mar/15 06:15;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5155;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""spark-submit --conf"" is not being propagated to child processes",SPARK-6372,12782452,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,vanzin,vanzin,17/Mar/15 02:26,18/Mar/15 13:07,14/Jul/23 06:27,18/Mar/15 13:07,1.4.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Thanks to [~irashid] for bringing this up. It seems that the new launcher library is incorrectly handling ""--conf"" and not passing it down to the child processes. Fix is simple, PR coming up.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 13:07:17 UTC 2015,,,,,,,,,,"0|i26uj3:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"17/Mar/15 02:27;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5057;;;","18/Mar/15 13:07;srowen;Issue resolved by pull request 5057
[https://github.com/apache/spark/pull/5057];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
InsertIntoHiveTable and Parquet Relation should use logic from SparkHadoopWriter,SPARK-6369,12782421,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,marmbrus,marmbrus,17/Mar/15 00:01,30/Mar/15 23:52,14/Jul/23 06:27,30/Mar/15 23:52,,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,Right now it is possible that we will corrupt the output if there is a race between competing speculative tasks.,,apachespark,lian cheng,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6067,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 30 23:52:07 UTC 2015,,,,,,,,,,"0|i26ucn:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"20/Mar/15 22:54;yhuai;Seems we need to change SparkHiveWriterContainer.commit to follow the code added by SPARK-4879 in SparkHadoopWriter.commit.;;;","21/Mar/15 00:43;yhuai;Three places that need to be fixed are [hiveWriterContainer|https://github.com/apache/spark/blob/v1.3.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveWriterContainers.scala#L120], [newParquet|https://github.com/apache/spark/blob/v1.3.0/sql/core/src/main/scala/org/apache/spark/sql/parquet/newParquet.scala#L619], and [old parquet code path|https://github.com/apache/spark/blob/v1.3.0/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetTableOperations.scala#L342]. We should follow the fix of SPARK-4879 ([here|https://github.com/apache/spark/blob/v1.3.0/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala#L106]).

Also, seems we can remove [this line|https://github.com/apache/spark/blob/v1.3.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala#L75].

;;;","23/Mar/15 17:29;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5139;;;","30/Mar/15 23:52;lian cheng;Issue resolved by pull request 5139
[https://github.com/apache/spark/pull/5139];;;",,,,,,,,,,,,,,,,,,,,,,,,,
"In Python API, the default save mode for save and saveAsTable should be ""error"" instead of ""append"".",SPARK-6366,12782371,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,16/Mar/15 21:33,18/Mar/15 01:42,14/Jul/23 06:27,18/Mar/15 01:41,,,,,,,1.3.1,,,,,,SQL,,,,0,,,,,,"If a user want to append data, he/she should explicitly specify the save mode. Also, in Scala and Java, the default save mode is ErrorIfExists.",,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 01:41:48 UTC 2015,,,,,,,,,,"0|i26u1r:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"16/Mar/15 21:37;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/5053;;;","18/Mar/15 01:41;lian cheng;Issue resolved by pull request 5053
[https://github.com/apache/spark/pull/5053];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
jetty-security needed for SPARK_PREPEND_CLASSES to work,SPARK-6365,12782362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,irashid,irashid,irashid,16/Mar/15 20:54,17/Mar/15 17:04,14/Jul/23 06:27,17/Mar/15 17:04,,,,,,,1.3.1,,,,,,Build,,,,0,,,,,,"For {{SPARK_PREPEND_CLASSES}} to work, we need the jetty-security jar to also get dumped into the unshaded dir.  Otherwise you get an exception like:

{noformat}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/eclipse/jetty/security/LoginService
        at org.apache.spark.repl.SparkIMain.<init>(SparkIMain.scala:118)
        at org.apache.spark.repl.SparkILoop$SparkILoopInterpreter.<init>(SparkILoop.scala:187)
        at org.apache.spark.repl.SparkILoop.createInterpreter(SparkILoop.scala:216)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:948)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:944)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1058)
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.eclipse.jetty.security.LoginService
        at java.net.URLClassLoader$1.run(URLClassLoader.java:372)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:360)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        ... 20 more
{noformat}

PR is on the way ...",,apachespark,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 17:04:31 UTC 2015,,,,,,,,,,"0|i26tzr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/15 13:51;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/5052;;;","17/Mar/15 14:51;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/5071;;;","17/Mar/15 17:04;irashid;Issue resolved by pull request 5071
[https://github.com/apache/spark/pull/5071];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Broken pipe error when training a RandomForest on a union of two RDDs,SPARK-6362,12782282,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,laskov,laskov,16/Mar/15 17:19,21/Mar/16 22:19,14/Jul/23 06:27,21/Mar/16 22:19,1.2.0,,,,,,1.3.0,,,,,,MLlib,PySpark,,,0,,,,,,"Training a RandomForest classifier on a dataset obtained as a union of two RDDs throws a broken pipe error:

Traceback (most recent call last):
  File ""/home/laskov/code/spark-1.2.1/python/pyspark/daemon.py"", line 162, in manager
    code = worker(sock)
  File ""/home/laskov/code/spark-1.2.1/python/pyspark/daemon.py"", line 64, in worker
    outfile.flush()
IOError: [Errno 32] Broken pipe

Despite an error the job runs to completion. 

The following code reproduces the error:

from pyspark.context import SparkContext
from pyspark.mllib.rand import RandomRDDs
from pyspark.mllib.tree import RandomForest
from pyspark.mllib.linalg import DenseVector
from pyspark.mllib.regression import LabeledPoint
import random

if __name__ == ""__main__"":

    sc = SparkContext(appName=""Union bug test"")

    data1 = RandomRDDs.normalVectorRDD(sc,numRows=10000,numCols=200)
    data1 = data1.map(lambda x: LabeledPoint(random.randint(0,1),\
                                             DenseVector(x)))
    data2 = RandomRDDs.normalVectorRDD(sc,numRows=10000,numCols=200)
    data2 = data2.map(lambda x: LabeledPoint(random.randint(0,1),\
                                            DenseVector(x)))

    training_data = data1.union(data2)
    #training_data = training_data.repartition(2)
    model = RandomForest.trainClassifier(training_data, numClasses=2,
                                         categoricalFeaturesInfo={},
                                         numTrees=50, maxDepth=30)

Interestingly, re-partitioning the data after the union operation rectifies the problem (uncomment the line before training in the code above). 
","Kubuntu 14.04, local driver",josephkb,laskov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 21 22:19:11 UTC 2016,,,,,,,,,,"0|i26tif:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/15 23:46;josephkb;This may be caused by [SPARK-5973], which is fixed in 1.3.  Does upgrading to 1.3 fix it?;;;","21/Mar/16 22:19;josephkb;I'm going to close this since it appears to be fixed (based on running it locally just now on master).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"For Spark 1.1 and 1.2, after any RDD transformations, calling saveAsParquetFile over a SchemaRDD with decimal or UDT column throws",SPARK-6360,12782255,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,lian cheng,lian cheng,16/Mar/15 16:19,07/Aug/15 09:06,14/Jul/23 06:27,06/Aug/15 23:21,1.1.0,1.2.0,,,,,1.5.0,,,,,,SQL,,,,1,,,,,,"Spark shell session for reproduction (use {{:paste}}):
{noformat}
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.catalyst.types.decimal._
import org.apache.spark.sql.catalyst.types._
import org.apache.hadoop.fs._

val sqlContext = new SQLContext(sc)
val fs = FileSystem.get(sc.hadoopConfiguration)

fs.delete(new Path(""a.parquet""))
fs.delete(new Path(""b.parquet""))

import sc._
import sqlContext._

val r1 = parallelize(1 to 10).map(i => Tuple1(Decimal(i, 10, 0))).select('_1 cast DecimalType(10, 0))

// OK
r1.saveAsParquetFile(""a.parquet"")

val r2 = parallelize(1 to 10).map(i => Tuple1(Decimal(i, 10, 0))).select('_1 cast DecimalType(10, 0))

val r3 = r2.coalesce(1)

// Error
r3.saveAsParquetFile(""b.parquet"")
{noformat}
Exception thrown:
{noformat}
java.lang.ClassCastException: scala.math.BigDecimal cannot be cast to org.apache.spark.sql.catalyst.types.decimal.Decimal
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:359)
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:328)
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:314)
        at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:308)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
15/03/17 00:04:13 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.ClassCastException: scala.math.BigDecimal cannot be cast to org.apache.spark.sql.catalyst.types.decimal.Decimal
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:359)
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:328)
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:314)
        at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:308)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
The query plan of {{r1}} is:
{noformat}
== Parsed Logical Plan ==
'Project [CAST('_1, DecimalType(10,0)) AS c0#60]
 LogicalRDD [_1#59], MapPartitionsRDD[71] at mapPartitions at ExistingRDD.scala:36

== Analyzed Logical Plan ==
Project [CAST(_1#59, DecimalType(10,0)) AS c0#60]
 LogicalRDD [_1#59], MapPartitionsRDD[71] at mapPartitions at ExistingRDD.scala:36

== Optimized Logical Plan ==
Project [CAST(_1#59, DecimalType(10,0)) AS c0#60]
 LogicalRDD [_1#59], MapPartitionsRDD[71] at mapPartitions at ExistingRDD.scala:36

== Physical Plan ==
Project [CAST(_1#59, DecimalType(10,0)) AS c0#60]
 PhysicalRDD [_1#59], MapPartitionsRDD[71] at mapPartitions at ExistingRDD.scala:36

Code Generation: false
== RDD ==
{noformat}
while {{r3}}'s query plan is:
{noformat}
== Parsed Logical Plan ==
LogicalRDD [c0#61], CoalescedRDD[74] at coalesce at SchemaRDD.scala:456

== Analyzed Logical Plan ==
LogicalRDD [c0#61], CoalescedRDD[74] at coalesce at SchemaRDD.scala:456

== Optimized Logical Plan ==
LogicalRDD [c0#61], CoalescedRDD[74] at coalesce at SchemaRDD.scala:456

== Physical Plan ==
PhysicalRDD [c0#61], CoalescedRDD[74] at coalesce at SchemaRDD.scala:456

Code Generation: false
== RDD ==
{noformat}
The key difference here is that, {{r3}} wraps an existing {{SchemaRDD}} ({{r2}}, beneath the {{CoalescedRDD}}). While evaluating {{r3}}, {{r2.compute}} is called, which calls {{ScalaReflection.convertRowToScala}}. Here, Catalyst {{Decimal}} values are converted into Java {{BigDecimal}}s, and finally causes the exception.

Note that {{DataFrame}} in Spark 1.3 doesn't suffer this issue.",,davies,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 06 23:21:42 UTC 2015,,,,,,,,,,"0|i26tcf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/May/15 11:46;srowen;[~lian cheng] Given the target version did you already resolve this? I'm cleaning up stale Target Version settings.;;;","05/May/15 05:38;lian cheng;This is actually not fixed yet. And it only affects versions prior to 1.2.x. I'm bumping this to 1.2.3.;;;","06/Aug/15 23:21;davies;This should be fixed in 1.5 (will not back porting to other branches, because there lots of change related to Decimal and InternalRow).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Spark standalone cluster does not support local:/ url for jar file,SPARK-6355,12782157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,koudelka,koudelka,16/Mar/15 09:51,11/Jul/17 22:10,14/Jul/23 06:27,11/Jul/17 22:10,1.2.1,1.3.0,,,,,,,,,,,Spark Core,,,,0,,,,,,"Submitting a new spark application to a standalone cluster with local:/path will result in an exception.


Driver successfully submitted as driver-20150316171157-0004
... waiting before polling master for driver state
... polling master for driver state
State of driver-20150316171157-0004 is ERROR
Exception from cluster was: java.io.IOException: No FileSystem for scheme: local
java.io.IOException: No FileSystem for scheme: local
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2584)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2591)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:296)
	at org.apache.spark.deploy.worker.DriverRunner.org$apache$spark$deploy$worker$DriverRunner$$downloadUserJar(DriverRunner.scala:141)
	at org.apache.spark.deploy.worker.DriverRunner$$anon$1.run(DriverRunner.scala:75)",,koudelka,mtbrandy,steve.ash,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1924,SPARK-6081,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jul 11 22:10:28 UTC 2017,,,,,,,,,,"0|i26ss7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/15 11:23;srowen;Is {{local}} a valid URI scheme? I hadn't seen that before. Do you just mean {{file:}}?
Maybe you could show what command you are running. I'm not clear what URI you are passing.;;;","16/Mar/15 14:13;koudelka;[~srowen] Thank you for your reply. 
I use spark-submit --class class.Main local:/application.jar . https://spark.apache.org/docs/1.2.1/submitting-applications.html under ""Advanced Dependency Management"" mentions local:/ can be used when a jar is pre-distributed instead of uploading using the built in file server. Maybe I am misunderstanding but I believe it is meant to work for the main application jar as well as for --jars config option.

I am running standalone cluster with Zookeeper HA and have on occasion had problem crashing on restart due to the spark fileserver being unavailable to distribute the jar to the worker nodes (I can't reliably reproduce this yet). I intended to use local:/ as a fix but seems this option does not work in standalone cluster.;;;","16/Mar/15 14:16;srowen;Oh, I learned something then. Yeah that looks like the intended behavior and this should work. Maybe it is just not applied in standalone mode for the app jar.;;;","16/Mar/15 14:21;koudelka;I haven't tried to see if --jars is working properly. There is (or at least used to be) an similar issue with spark-submit --master option. The spark standalone cluster documentation says that multiple master hosts can be provided as a comma separated list like ""spark://host1:port1,host2:port2"" but it did not work (at least in < 1.2.0 ). But it does work when setting the master url within the driver code while starting the SparkContext. I am wondering if I can do a similar workaround for this issue.;;;","25/Jun/15 22:31;mtbrandy;This appears to be fixed in 1.4.0:

Using simply <filepath>

{noformat}
15/06/25 17:16:49 INFO Executor: Fetching http://9.41.180.152:41860/jars/simple-project-1.0-SNAPSHOT.jar with timestamp 1435270607309
15/06/25 17:16:50 INFO Utils: Fetching http://9.41.180.152:41860/jars/simple-project-1.0-SNAPSHOT.jar to /tmp/spark-76ae9294-c1e7-4cd5-b242-28ca93684f9e/executor-fdda27da-f4a8-447f-9bfc-8a26e33ded58/fetchFileTemp6323092224818161420.tmp
15/06/25 17:16:50 INFO Utils: Copying /tmp/spark-76ae9294-c1e7-4cd5-b242-28ca93684f9e/executor-fdda27da-f4a8-447f-9bfc-8a26e33ded58/8335963441435270607309_cache to /home/mbrandy/spark/work/app-20150625171647-0003/0/./simple-project-1.0-SNAPSHOT.jar
15/06/25 17:16:50 INFO Executor: Adding file:/home/mbrandy/spark/work/app-20150625171647-0003/0/./simple-project-1.0-SNAPSHOT.jar to class loader
{noformat}

Using local:<filepath>

{noformat}
15/06/25 17:17:10 INFO Executor: Fetching file:/home/mbrandy/simple-project-1.0-SNAPSHOT.jar with timestamp 1435270627993
15/06/25 17:17:10 INFO Utils: Copying /home/mbrandy/simple-project-1.0-SNAPSHOT.jar to /tmp/spark-76ae9294-c1e7-4cd5-b242-28ca93684f9e/executor-17d4c6f3-7c64-4fd8-8c3e-6530df306d3b/14876367751435270627993_cache
15/06/25 17:17:10 INFO Utils: Copying /tmp/spark-76ae9294-c1e7-4cd5-b242-28ca93684f9e/executor-17d4c6f3-7c64-4fd8-8c3e-6530df306d3b/14876367751435270627993_cache to /home/mbrandy/spark/work/app-20150625171708-0004/0/./simple-project-1.0-SNAPSHOT.jar
15/06/25 17:17:10 INFO Executor: Adding file:/home/mbrandy/spark/work/app-20150625171708-0004/0/./simple-project-1.0-SNAPSHOT.jar to class loader
{noformat}

My guess is that it was fixed via SPARK-6081.;;;","11/Jul/17 22:10;vanzin;I'm going to trust the last comment about this being fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,
Model update propagation during prediction in Streaming Regression,SPARK-6345,12782120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,freeman-lab,freeman-lab,freeman-lab,16/Mar/15 03:26,03/Apr/15 04:39,14/Jul/23 06:27,03/Apr/15 04:39,,,,,,,1.3.1,1.4.0,,,,,DStreams,MLlib,,,0,,,,,,"During streaming regression analyses (Streaming Linear Regression and Streaming Logistic Regression), model updates based on training data are not being reflected in subsequent calls to predictOn or predictOnValues, despite updates themselves occurring successfully. It may be due to recent changes to model declaration, and I have a working fix prepared to be submitted ASAP (alongside expanded test coverage).

A temporary workaround is to retrieve and use the updated model within a foreachRDD, as in:

{code}
model.trainOn(trainingData)
testingData.foreachRDD{ rdd =>
    val latest = model.latestModel()
    val predictions = rdd.map(lp => latest.predict(lp.features))
    ...print or other side effects...
}
{code}

Or within a transform, as in:

{code}
model.trainOn(trainingData)
val predictions = testingData.transform { rdd =>
      val latest = model.latestModel()
      rdd.map(lp => (lp.label, latest.predict(lp.features)))
}
{code}

Note that this does not affect Streaming KMeans, which works as expected for combinations of training and prediction.",,apachespark,freeman-lab,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 04:39:31 UTC 2015,,,,,,,,,,"0|i26sjz:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.2,1.3.1,1.4.0,,,,,,,,,,"16/Mar/15 06:51;apachespark;User 'freeman-lab' has created a pull request for this issue:
https://github.com/apache/spark/pull/5037;;;","03/Apr/15 04:39;mengxr;Issue resolved by pull request 5037
[https://github.com/apache/spark/pull/5037];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 1.3 doc fixes,SPARK-6337,12782024,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,vinodkc,josephkb,josephkb,14/Mar/15 17:57,22/Mar/15 20:01,14/Jul/23 06:27,22/Mar/15 20:00,1.3.0,,,,,,1.3.1,1.4.0,,,,,Documentation,SQL,,,0,,,,,,"I'll try to track doc issues to be fixed for the 1.3.1 release in this JIRA.

DataFrame
* [quotes should include ""age > 30"" | https://github.com/apache/spark/blob/b943f5d907df0607ecffb729f2bccfa436438d7e/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala#L92]
* [DeveloperAPI  -> DeveloperApi | https://github.com/apache/spark/blob/c49d156624624a719c0d1262a58933ea3e346963/mllib/src/main/scala/org/apache/spark/ml/Pipeline.scala#L36]
* [SQL Programming Guide: Programmatically Specifying the Schema typo | https://github.com/apache/spark/blob/25e271d9fbb3394931d23822a1b2020e9d9b46b3/docs/sql-programming-guide.md]
** Reported by Vinay: Under the ""Programmatically Specifying the Schema"" section , it's mentioned that SQL data types are in the following package org.apache.spark.sql, but I guess it has changed to org.apache.spark.sql.types
",,apachespark,josephkb,sandeepn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 22 20:00:43 UTC 2015,,,,,,,,,,"0|i26ryn:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"21/Mar/15 05:16;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/5112;;;","22/Mar/15 20:00;srowen;Issue resolved by pull request 5112
[https://github.com/apache/spark/pull/5112];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
New Spark Master URL is not picked up when streaming context is started from checkpoint,SPARK-6331,12781939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tdas,tdas,tdas,13/Mar/15 21:56,17/Mar/15 12:32,14/Jul/23 06:27,17/Mar/15 12:32,1.1.1,1.2.1,1.3.0,,,,,,,,,,DStreams,,,,0,,,,,,"When the SparkConf is reconstructed based on the checkpointed configuration, it recovers the old master URL. This okay if the cluster on which the streaming application is relaunched is the same cluster as it was running before. But if that cluster changes, there is no way to inject the new master URL of the new cluster. As a result, the restarted app tries to connect to the non-existent old cluster and fails. 

The solution is to check whether a master URL is set in the System properties (by Spark submit) before recreating the SparkConf. If a new master url is set in the properties, then use it as that is obviously the most relevant one. Otherwise load the old one (to maintain existing behavior). ",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 14 02:01:07 UTC 2015,,,,,,,,,,"0|i26rfr:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"14/Mar/15 02:01;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/5024;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
newParquetRelation gets incorrect FileSystem,SPARK-6330,12781937,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vlyubin,vlyubin,vlyubin,13/Mar/15 21:52,03/Apr/15 18:35,14/Jul/23 06:27,03/Apr/15 18:35,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"Here's a snippet from newParquet.scala:

    def refresh(): Unit = {
      val fs = FileSystem.get(sparkContext.hadoopConfiguration)

      // Support either reading a collection of raw Parquet part-files, or a collection of folders
      // containing Parquet files (e.g. partitioned Parquet table).
      val baseStatuses = paths.distinct.map { p =>
        val qualified = fs.makeQualified(new Path(p))

        if (!fs.exists(qualified) && maybeSchema.isDefined) {
          fs.mkdirs(qualified)
          prepareMetadata(qualified, maybeSchema.get, sparkContext.hadoopConfiguration)
        }

        fs.getFileStatus(qualified)
      }.toArray

If we are running this locally and path points to S3, fs would be incorrect. A fix is to construct fs for each file separately.",,apachespark,hotou,vlyubin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6351,SPARK-6446,,,,,,,,,,,,,,SPARK-6457,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 18:35:42 UTC 2015,,,,,,,,,,"0|i26rfb:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"13/Mar/15 21:54;apachespark;User 'vlyubin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5020;;;","18/Mar/15 00:34;apachespark;User 'ypcat' has created a pull request for this issue:
https://github.com/apache/spark/pull/5039;;;","03/Apr/15 18:23;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/5353;;;","03/Apr/15 18:24;yhuai;I am reopening the issue since for s3n, {{fs.makeQualified(qualifiedPath)}} does not. It will throw a very confusing error message.
{code}
java.lang.IllegalArgumentException: Wrong FS: s3n://ID:KEY@bucket/path, expected: s3n://ID:KEY@bucket.
{code}

When I put a relative path, it is fine. Also, if I use qualifiedPath.makeQualified(fs). It is fine.;;;","03/Apr/15 18:35;yhuai;Please ignore my comment. ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Minor doc changes for Mesos and TOC,SPARK-6329,12781933,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,boyork,boyork,boyork,13/Mar/15 21:44,14/Mar/15 17:30,14/Jul/23 06:27,14/Mar/15 17:30,,,,,,,1.4.0,,,,,,Documentation,,,,0,,,,,,"Following up from SPARK-1182 there were two more minor doc issues to address:

1. Link to the Mesos configuration table rather than just the Mesos page.
2. If possible, make Mesos/YARN/Standalone appear in the table of contents on top.",,apachespark,boyork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 14 17:30:05 UTC 2015,,,,,,,,,,"0|i26ref:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/15 22:42;apachespark;User 'brennonyork' has created a pull request for this issue:
https://github.com/apache/spark/pull/5022;;;","14/Mar/15 17:30;srowen;Issue resolved by pull request 5022
[https://github.com/apache/spark/pull/5022];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Run PySpark with python directly is broken,SPARK-6327,12781914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,13/Mar/15 20:12,16/Mar/15 23:27,14/Jul/23 06:27,16/Mar/15 23:27,1.4.0,,,,,,1.4.0,,,,,,PySpark,,,,0,,,,,,"It works before, but broken now:
{code}
davies@localhost:~/work/spark$ python r.py
NOTE: SPARK_PREPEND_CLASSES is set, placing locally compiled Spark classes ahead of assembly.
Usage: spark-submit [options] <app jar | python file> [app arguments]
Usage: spark-submit --kill [submission ID] --master [spark://...]
Usage: spark-submit --status [submission ID] --master [spark://...]

Options:
  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.
  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (""client"") or
                              on one of the worker machines inside the cluster (""cluster"")
                              (Default: client).
  --class CLASS_NAME          Your application's main class (for Java / Scala apps).
  --name NAME                 A name of your application.
  --jars JARS                 Comma-separated list of local jars to include on the driver
                              and executor classpaths.
  --packages                  Comma-separated list of maven coordinates of jars to include
                              on the driver and executor classpaths. Will search the local
                              maven repo, then maven central and any additional remote
                              repositories given by --repositories. The format for the
                              coordinates should be groupId:artifactId:version.
  --repositories              Comma-separated list of additional remote repositories to
                              search for the maven coordinates given with --packages.
  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place
                              on the PYTHONPATH for Python apps.
  --files FILES               Comma-separated list of files to be placed in the working
                              directory of each executor.

  --conf PROP=VALUE           Arbitrary Spark configuration property.
  --properties-file FILE      Path to a file from which to load extra properties. If not
                              specified, this will look for conf/spark-defaults.conf.

  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 512M).
  --driver-java-options       Extra Java options to pass to the driver.
  --driver-library-path       Extra library path entries to pass to the driver.
  --driver-class-path         Extra class path entries to pass to the driver. Note that
                              jars added with --jars are automatically included in the
                              classpath.

  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).

  --proxy-user NAME           User to impersonate when submitting the application.

  --help, -h                  Show this help message and exit
  --verbose, -v               Print additional debug output
  --version,                  Print the version of current Spark

 Spark standalone with cluster deploy mode only:
  --driver-cores NUM          Cores for driver (Default: 1).
  --supervise                 If given, restarts the driver on failure.
  --kill SUBMISSION_ID        If given, kills the driver specified.
  --status SUBMISSION_ID      If given, requests the status of the driver specified.

 Spark standalone and Mesos only:
  --total-executor-cores NUM  Total cores for all executors.

 YARN-only:
  --driver-cores NUM          Number of cores used by the driver, only in cluster mode
                              (Default: 1).
  --executor-cores NUM        Number of cores per executor (Default: 1).
  --queue QUEUE_NAME          The YARN queue to submit to (Default: ""default"").
  --num-executors NUM         Number of executors to launch (Default: 2).
  --archives ARCHIVES         Comma separated list of archives to be extracted into the
                              working directory of each executor.
{code}",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 23:27:14 UTC 2015,,,,,,,,,,"0|i26ra7:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"13/Mar/15 21:13;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/5019;;;","16/Mar/15 23:27;joshrosen;Issue resolved by pull request 5019
[https://github.com/apache/spark/pull/5019];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnAllocator crash with dynamic allocation on,SPARK-6325,12781876,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,vanzin,vanzin,vanzin,13/Mar/15 18:03,18/Mar/15 13:19,14/Jul/23 06:27,18/Mar/15 13:18,1.3.0,,,,,,1.3.1,1.4.0,,,,,Spark Core,YARN,,,0,,,,,,"Run spark-shell like this:

{noformat}
spark-shell --conf spark.shuffle.service.enabled=true \
--conf spark.dynamicAllocation.enabled=true  \
--conf spark.dynamicAllocation.minExecutors=1  \
--conf spark.dynamicAllocation.maxExecutors=20 \
--conf spark.dynamicAllocation.executorIdleTimeout=10  \
--conf spark.dynamicAllocation.schedulerBacklogTimeout=5  \
--conf spark.dynamicAllocation.sustainedSchedulerBacklogTimeout=5
{noformat}

Then run this simple test:

{code}
scala> val verySmallRdd = sc.parallelize(1 to 10, 10).map { i => 
     |   if (i % 2 == 0) { Thread.sleep(30 * 1000); i } else 0
     | }
verySmallRdd: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[1] at map at <console>:21

scala> verySmallRdd.collect()
{code}

When Spark starts ramping down the number of allocated executors, it will hit an assert in YarnAllocator.scala:

{code}
assert(targetNumExecutors >= 0, ""Allocator killed more executors than are allocated!"")
{code}

This assert will cause the akka backend to die, but not the AM itself. So the app will be in a zombie-like state, where the driver is alive but can't talk to the AM. Sadness ensues.

I have a working fix, just need to add unit tests. Stay tuned.

Thanks to [~wypoon] for finding the problem, and for the test case.",,apachespark,jongyoul,vanzin,wypoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 13:18:44 UTC 2015,,,,,,,,,,"0|i26r1z:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"13/Mar/15 18:38;wypoon;The testcase actually came from a similar example in a presentation by Andrew Or and Aaron Davidson on elastic scaling. :-)
;;;","13/Mar/15 18:40;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5018;;;","18/Mar/15 13:18;srowen;Issue resolved by pull request 5018
[https://github.com/apache/spark/pull/5018];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS should consider the case where no file format or storage handler is given,SPARK-6322,12781830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,viirya,viirya,13/Mar/15 15:12,07/Oct/16 07:30,14/Jul/23 06:27,07/Oct/16 07:30,,,,,,,,,,,,,SQL,,,,0,,,,,,"When creating CreateTableAsSelect in HiveQl, it doesn't consider the case where no file format or storage handler is given. So later in CreateTables, one of CreateTableAsSelect cases will never be run.
",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 15:14:07 UTC 2015,,,,,,,,,,"0|i26qsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/15 15:14;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/5014;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adding new query plan strategy to SQLContext,SPARK-6320,12781798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,HatY,HatY,13/Mar/15 13:03,13/Jun/16 08:53,14/Jul/23 06:27,10/Jun/16 20:08,1.3.0,,,,,,2.0.0,,,,,,SQL,,,,1,,,,,,"Hi,

I would like to add a new strategy to {{SQLContext}}. To do this I created a new class which extends {{Strategy}}. In my new class I need to call {{planLater}} function. However this method is defined in {{SparkPlanner}} (which itself inherits the method from {{QueryPlanner}}).

To my knowledge the only way to make {{planLater}} function visible to my new strategy is to define my strategy inside another class that extends {{SparkPlanner}} and inherits {{planLater}} as a result, by doing so I will have to extend the {{SQLContext}} such that I can override the {{planner}} field with the new {{Planner}} class I created.

It seems that this is a design problem because adding a new strategy seems to require extending {{SQLContext}} (unless I am doing it wrong and there is a better way to do it).

Thanks a lot,
Youssef",,ajnavarro,apachespark,evacchi,HatY,lianhuiwang,marmbrus,maropu,smolav,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6981,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jun 10 20:08:08 UTC 2016,,,,,,,,,,"0|i26qlj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/15 01:05;marmbrus;Hmm, interesting.  So far I had only considered this interface for planning leaves of the query plan.  Can you tell me more about what you are trying to optimize?;;;","17/Mar/15 15:09;HatY;Thank you Michael for your response,

Actually I am trying to extend SQL syntax with custom constructs which would require altering the workflow of {{SparkSQL}} all the way starting from the lexer and ending with the optimization and physical planning.

I thought this should be possible without having to extend many classes. However not being able to use {{planLater}} forces me to do seemingly unnecessary extension of {{SparkPlanner}} specially that there seem to be some logic to handle these scenarios (i.e. the {{extraStrategies}} sequence).;;;","19/Mar/15 08:12;smolav;[~marmbrus] We could change strategies so that they take a SparkPlanner in their constructor. This should provide enough flexibility for [~H.Youssef]'s use case and might improve code organization of the core strategies in the future.;;;","23/Mar/15 19:20;marmbrus;If that can be done in a minimally invasive way that sounds reasonable to me.;;;","17/Apr/15 09:37;evacchi;My use case is customizing strategies so that operations can be pruned/rewritten, depending on metadata in a custom catalog. For instance, consider the case of a DataFrame where a column X is known to be non-nullable; if the query contains the filter {{where('x is not null)}} the filter can be pruned with a rule of the form:

{code:java}
      case logical.Filter(IsNotNull(e: NamedExpression), child)
        if customCatalog.isNonNull(e) => {
          planLater(child) :: Nil
{code}

I wonder why is {{SparkPlanner}} nested inside {{SQLContext}}. Cass {{SparkPlanner}} may require the {{SQLContext}}, {{SQLConf}} as constructor parameters and be defined in its own class (and the same could be said for QueryExecution).  suggestion would be to refactor out {{SparkPlanner}}. Advanced users would then be able to extend it like any other class.

I have opened SPARK-6981 and a PR to address this issue;;;","17/May/16 10:02;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/13147;;;","31/May/16 23:09;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/13426;;;","10/Jun/16 20:08;marmbrus;Issue resolved by pull request 13147
[https://github.com/apache/spark/pull/13147];;;",,,,,,,,,,,,,,,,,,,,,
Should throw analysis exception when using binary type in groupby/join,SPARK-6319,12781780,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,viirya,lian cheng,lian cheng,13/Mar/15 12:10,01/Feb/16 18:03,14/Jul/23 06:27,31/Jul/15 00:24,1.0.2,1.1.1,1.2.1,1.3.0,,,1.5.0,,,,,,SQL,,,,0,,,,,,"Spark shell session for reproduction:
{noformat}
scala> import sqlContext.implicits._
scala> import org.apache.spark.sql.types._
scala> Seq(1, 1, 2, 2).map(i => Tuple1(i.toString)).toDF(""c"").select($""c"" cast BinaryType).distinct.show()
...
CAST(c, BinaryType)
[B@43f13160
[B@5018b648
[B@3be22500
[B@476fc8a1
{noformat}
Spark SQL uses plain byte arrays to represent binary values. However, arrays are compared by reference rather than by value. On the other hand, the DISTINCT operator uses a {{HashSet}} and its {{.contains}} method to check for duplicated values. These two facts together cause the problem.",,apachespark,chinwei,joshrosen,lian cheng,marmbrus,maropu,michalm,ravi.pesala,viirya,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5553,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 01 18:03:20 UTC 2016,,,,,,,,,,"0|i26qhj:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"15/Mar/15 23:56;yhuai;I believe that our aggregation and join will also be affected. I am changing the Priority to Blocker.;;;","16/Mar/15 10:06;lian cheng;SPARK-5553 is somewhat related to this one. By reimplement the binary type with some more efficient representation with a proper {{equalsTo}} method can fix this issue in a clean way. Though we might not try to resolve SPARK-5553 right away.;;;","16/Mar/15 23:26;yhuai;Had a discussion with [~marmbrus] on it. We agree that the right way to address it is to have a wrapper on the byte array.;;;","16/Mar/15 23:29;marmbrus;I'm going to bump this to 1.4.0.  I don't think most systems will actually let you do this, and its a pretty weird use case that no real user has complained about before.;;;","13/Jul/15 17:59;joshrosen;I think that we should revisit this issue.  It seems that we currently return wrong answers for groupBy queries involving binary typed columns.  If we're not going to support this properly, then I think we should fail-fast with an analysis error rather than returning an incorrect answer.;;;","13/Jul/15 18:15;marmbrus;+1 to throwing an {{AnalysisException}};;;","29/Jul/15 07:05;joshrosen;I think it would be nice to add this AnalysisException + a test before 1.5.0.

[~viirya], given your recent patch for fixing the SortMergeJoin analysis, this might be really quick for you to implement. If you're not interested, I'll try to do it myself this week.;;;","29/Jul/15 07:35;viirya;[~joshrosen] I will do it later. Thanks.;;;","30/Jul/15 07:19;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/7787;;;","01/Feb/16 04:10;chinwei;Hi guys,

Any resolution for group by binary column using Spark.;;;","01/Feb/16 18:03;lian cheng;One possible but not necessarily the best workaround is to have a UDF that maps your binary field to some other data type, which can be used in GROUP BY/JOIN.;;;",,,,,,,,,,,,,,,,,,
Interactive HIVE scala console is not starting,SPARK-6317,12781777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vinodkc,vinodkc,vinodkc,13/Mar/15 12:02,13/Mar/15 23:42,14/Jul/23 06:27,13/Mar/15 23:20,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"{{build/sbt hive/console}}  is failing  

{noformat}
[info] Starting scala interpreter...
[info] 
<console>:15: error: object Dsl is not a member of package org.apache.spark.sql
       import org.apache.spark.sql.Dsl._
                                   ^
{noformat}",,apachespark,lian cheng,vinodkc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 23:20:47 UTC 2015,,,,,,,,,,"0|i26qgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/15 12:04;apachespark;User 'vinodkc' has created a pull request for this issue:
https://github.com/apache/spark/pull/5011;;;","13/Mar/15 13:06;srowen;(Let's reserve ""Blocker"" for confirmed issues that committers have decided need to block a release. I don't think this is especially severe.)

That said I do see the same error so will look into your fix.;;;","13/Mar/15 23:20;lian cheng;Issue resolved by pull request 5011
[https://github.com/apache/spark/pull/5011];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQL 1.3.0 (RC3) fails to read parquet file generated by 1.1.1,SPARK-6315,12781734,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,marmbrus,marmbrus,13/Mar/15 06:40,21/Mar/15 03:20,14/Jul/23 06:27,21/Mar/15 03:20,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"Parquet files generated by Spark 1.1 have a deprecated representation of the schema.  In Spark 1.3 we fail to read these files through the new Parquet code path.  We should continue to read these files until we formally deprecate this representation.

As a workaround:
{code}
SET spark.sql.parquet.useDataSourceApi=false
{code}",,apachespark,avignon,jkleckner,jonathak,lian cheng,marmbrus,Michael Davies,michaelmalak,pllee,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 21 03:20:18 UTC 2015,,,,,,,,,,"0|i26q7z:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"13/Mar/15 14:17;yhuai;Should we change the target version to 1.3.1?;;;","15/Mar/15 13:25;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5034;;;","16/Mar/15 17:17;Michael Davies;We are planning to use Parquet to store files that we want to be accessible, via Spark SQL, for up to 7 years. 

Is there a policy, or view, on how long the ability to read files laid down by earlier versions of Spark will be supported? I had not imagined that files written by earlier versions would potentially become unreadable.;;;","16/Mar/15 17:22;lian cheng;Hi [~Michael Davies], really sorry for the trouble! [PR #5034| https://github.com/apache/spark/pull/5034] was opened to fix this issue, and is expected to be included in 1.3.1. If you need this right away, you may cherry-pick it into your own fork after it's been reviewed and merged (which should be pretty soon).;;;","16/Mar/15 17:45;Michael Davies;Hi [~liancheng]

This is not a problem for us at the moment, we are still in dev and will wait for 1.3.1. 

The main question was whether deprecation and removal of support for reading files written in earlier versions will be something that may be considered in the future.

Thanks for fixing this so promptly;;;","21/Mar/15 03:20;lian cheng;Issue resolved by pull request 5034
[https://github.com/apache/spark/pull/5034];;;",,,,,,,,,,,,,,,,,,,,,,,
Fetch File Lock file creation doesnt work when Spark working dir is on a NFS mount,SPARK-6313,12781713,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,nemccarthy,nemccarthy,nemccarthy,13/Mar/15 04:17,17/Mar/15 16:35,14/Jul/23 06:27,17/Mar/15 16:34,1.2.0,1.2.1,1.3.0,,,,1.2.2,1.3.1,1.4.0,,,,Spark Core,,,,0,,,,,,"When running in cluster mode and mounting the spark work dir on a NFS volume (or some volume which doesn't support file locking), the fetchFile (used for downloading JARs etc on the executors) method in Spark Utils class will fail. This file locking was introduced as an improvement with SPARK-2713. 

See https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L415 

Introduced in 1.2 in commit; https://github.com/apache/spark/commit/7aacb7bfad4ec73fd8f18555c72ef696 

As this locking is for optimisation for fetching files, could we take a different approach here to create a temp/advisory lock file? 

Typically you would just mount local disks (in say ext4 format) and provide this as a comma separated list however we are trying to run Spark on MapR. With MapR we can do a loop back mount to a volume on the local node and take advantage of MapRs disk pools. This also means we dont need specific mounts for Spark and improves the generic nature of the cluster. ",,apachespark,brandonli,joshrosen,nemccarthy,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 16:35:53 UTC 2015,,,,,,,,,,"0|i26q3b:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"13/Mar/15 05:32;nemccarthy;Suggestion along the lines of;

https://github.com/apache/lucene-solr/blob/5314a56924f46522993baf106e6deca0e48a967f/lucene/core/src/java/org/apache/lucene/store/SimpleFSLockFactory.java 
or
https://github.com/graphhopper/graphhopper/blob/master/core/src/main/java/com/graphhopper/storage/SimpleFSLockFactory.java
;;;","13/Mar/15 05:36;nemccarthy;Since the {code}val lockFileName = s""${url.hashCode}${timestamp}_lock""{code} uses a timestamp I can't see there being too many problems with hanging/left over lock files. ;;;","13/Mar/15 18:13;joshrosen;Could you update this ticket with more details on the error-message or symptom that you've observed (such as a stacktrace)?  This would be helpful in order to make this issue more searchable / discoverable.;;;","13/Mar/15 18:40;joshrosen;Thanks for the pointer to the Lucene lock factory code.

It's fine for the locks to be advisory in the sense that things shouldn't break if multiple executors acquire the lock and try to download the same file, but there's potentially a problem if the lock isn't released after the JVM that acquired it exits abnormally, since this could cause other executors to block indefinitely while waiting for the original lock owner to download the file.  One approach might be to write the PID of the original lock owner into the lock file, which would allow blocked executors to timeout and re-attempt the lock acquisition if they detect that the original lock holder died.  This might face its own portability challenges, though, and seems complex.

A simple hotfix might be to add a SparkConf setting to always force this caching to bypassed (this would be a two-line change to Executor.scala).  This might lose the performance benefits of the caching, though.

If you're using NFS and the shared filesystem is mounted at the same path on all nodes, I think that you should be able to use use {{local://path/to/nfs/}} to specify the paths to your files / JARs, which will cause them to be read from the executor-local filesystem rather than fetched remotely.  In this case, this would cause them to be read from NFS, so you may be able to use this technique to recover any performance benefits for large files that would be lost in disabling the caching.

I'd be happy to review patches for this issue.;;;","15/Mar/15 23:13;nemccarthy;Stacktrace;

14/12/12 18:18:24 WARN scheduler.TaskSetManager: Lost task 7.0 in stage 0.0 (TID 8, hadoop-016): java.io.IOException: Permission denied
at sun.nio.ch.FileDispatcherImpl.lock0(Native Method)
at sun.nio.ch.FileDispatcherImpl.lock(FileDispatcherImpl.java:91)
at sun.nio.ch.FileChannelImpl.lock(FileChannelImpl.java:1022)
at java.nio.channels.FileChannel.lock(FileChannel.java:1052)
at org.apache.spark.util.Utils$.fetchFile(Utils.scala:379)
at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:350)
at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$6.apply(Executor.scala:347)
at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:347)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745);;;","16/Mar/15 03:28;pwendell;[~joshrosen] changing default caching behavior seems like it could silently regress performance for the vas majority of users who aren't on NFS. What about a hotfix for 1.3.1 that just exposes the config for NFS users (this is very small population), but doesn't change the default. That may be sufficient in itself... or if we want a real fix that makes it work out-of-the-box on NDFS, we can put it in 1.4.;;;","16/Mar/15 06:46;apachespark;User 'nemccarthy' has created a pull request for this issue:
https://github.com/apache/spark/pull/5036;;;","16/Mar/15 06:47;nemccarthy;Thanks for the feedback guys. The config option workaround seems like the path of least resistance for now with some more testing being required for a different implementation. For us it would be great if we could get a fix ASAP. Ive created PR 5603 https://github.com/apache/spark/pull/5036;;;","17/Mar/15 16:34;joshrosen;Issue resolved by pull request 5036
[https://github.com/apache/spark/pull/5036];;;","17/Mar/15 16:35;joshrosen;I've merged Nathan's patch into 1.4.0, 1.3.1, and 1.2.2.  After this path, users can work around this bug by setting {{spark.files.useFetchCache=false}} in their SparkConf.;;;",,,,,,,,,,,,,,,,,,,
VectorUDT is displayed as `vecto` in dtypes,SPARK-6308,12781675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,MechCoder,mengxr,mengxr,13/Mar/15 01:00,23/Mar/15 20:30,14/Jul/23 06:27,23/Mar/15 20:30,,,,,,,1.4.0,,,,,,MLlib,SQL,,,0,,,,,,VectorUDT should override simpleString instead of relying on the default implementation.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 20:30:34 UTC 2015,,,,,,,,,,"0|i26puv:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"21/Mar/15 19:20;apachespark;User 'MechCoder' has created a pull request for this issue:
https://github.com/apache/spark/pull/5118;;;","23/Mar/15 20:30;mengxr;Issue resolved by pull request 5118
[https://github.com/apache/spark/pull/5118];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Readme points to dead link,SPARK-6306,12781504,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,tvas,tvas,tvas,12/Mar/15 13:47,24/Apr/15 00:34,14/Jul/23 06:27,12/Mar/15 15:01,,,,,,,1.4.0,,,,,,Documentation,,,,0,,,,,,"The link to ""Specifying the Hadoop Version"" now points to http://spark.apache.org/docs/latest/building-with-maven.html#specifying-the-hadoop-version.

The correct link is: http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 15:50:14 UTC 2015,,,,,,,,,,"0|i26otr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/15 13:51;apachespark;User 'thvasilo' has created a pull request for this issue:
https://github.com/apache/spark/pull/4999;;;","12/Mar/15 15:01;srowen;Issue resolved by pull request 4999
[https://github.com/apache/spark/pull/4999];;;","12/Mar/15 15:03;srowen;For a trivial change, a JIRA is just overhead. You don't need one unless there is a meaningful difference between the problem description and the fix itself.;;;","12/Mar/15 15:50;tvas;I'll keep that in mind in the future.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Checkpointing doesn't retain driver port,SPARK-6304,12781465,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,msoutier,msoutier,12/Mar/15 11:13,16/Jul/15 23:58,14/Jul/23 06:27,16/Jul/15 23:58,1.2.1,,,,,,1.5.0,,,,,,DStreams,,,,0,,,,,,"In a check-pointed Streaming application running on a fixed driver port, the setting ""spark.driver.port"" is not loaded when recovering from a checkpoint.

(The driver is then started on a random port.)
",,apachespark,jerryshao,msoutier,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 07:34:54 UTC 2015,,,,,,,,,,"0|i26ol3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/15 08:01;jerryshao;Hi [~msoutier], seldom user will set this {{spark.driver.port}}, Spark itself will randomly choose a port and set it to SparkConf, so according to this, Spark Streaming will remove previously set port when recovering from checkpoint.

Basically why {{spark.driver.port}} need to be fixed? And potentially it needs to be fixed when {{spark.driver.*}} is set by user.;;;","16/Mar/15 08:11;msoutier;Simple, I'm using `actorStream` and want to send data to it via remoting. For that I need to have a fixed port to send data to.

As a workaround I'm now starting a second ActorSystem, but it seems to have issues communicating with Spark's ActorSystem.
;;;","16/Mar/15 08:23;jerryshao;OK, got it. So are you going to fix this issue or just report a bug here?;;;","16/Mar/15 08:28;msoutier;I'm just reporting the bug. As you said, the code explicitly removes ""spark.driver.host"" and ""spark.driver.port"" when recovering from a checkpoint, so I first would like to understand why that is.


;;;","16/Mar/15 08:37;jerryshao;As I said, normally user will not set these two configurations {{spark.driver.host}} and {{spark.driver.port}} to let SparkContext to set. SparkContext will internally choose driver's host name and random port for these two configurations, the reason to do so is to avoid port contention whey multiple driver running on the same machine. 

Spark Streaming takes this assumption to remove these two configurations when recovering from checkpoint file, to avoid port contention. Yes this is a bug for usage scenarios like yours.;;;","16/Mar/15 10:42;msoutier;Yeah but if the user doesn't set the port, why remove it? When Spark deserializes the checkpoint, the port shouldn't be set by default, right?
;;;","17/Mar/15 00:06;jerryshao;Hi [~msoutier], the reason to remove these two configurations, especially ""spark.driver.port"" is that: SparkContext itself will randomly choose a port and set it to configuration even user didn't set it, next time after application is recovered, previous configuration ""spark.driver.port"" need to remove and let SparkContext itself to randomly choose again and set into the SparkConf. So that's why checkpoint need to remove these two configurations.;;;","17/Mar/15 05:05;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/5060;;;","17/Mar/15 07:34;msoutier;Got it, thanks. In my tests it was never set automatically, so this must be set at some later point.;;;",,,,,,,,,,,,,,,,,,,,
sc.addFile(path) does not support the relative path.,SPARK-6300,12781422,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,DoingDone9,DoingDone9,DoingDone9,12/Mar/15 07:23,16/Mar/15 12:27,14/Jul/23 06:27,16/Mar/15 12:27,1.2.1,1.3.0,,,,,1.3.1,1.4.0,,,,,Spark Core,,,,0,,,,,,"when i run cmd like that sc.addFile(""../test.txt""), it did not work and throw an exception

java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:../test.txt
at org.apache.hadoop.fs.Path.initialize(Path.java:206)
at org.apache.hadoop.fs.Path.<init>(Path.java:172) 
........
.......
Caused by: java.net.URISyntaxException: Relative path in absolute URI: file:../test.txt
at java.net.URI.checkPath(URI.java:1804)
at java.net.URI.<init>(URI.java:752)
at org.apache.hadoop.fs.Path.initialize(Path.java:203)",,apachespark,DoingDone9,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 12:27:44 UTC 2015,,,,,,,,,,"0|i26obj:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"12/Mar/15 07:25;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/4993;;;","12/Mar/15 16:10;srowen;(Sandy notes it's a regression so yeah it's more important. I didn't think this was ever supposed to work);;;","16/Mar/15 12:27;srowen;Issue resolved by pull request 4993
[https://github.com/apache/spark/pull/4993];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException in standalone mode when running groupByKey with class defined in REPL.,SPARK-6299,12781406,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,swkimme,swkimme,swkimme,12/Mar/15 04:59,08/Jan/16 13:13,14/Jul/23 06:27,17/Mar/15 06:49,1.2.1,1.3.0,,,,,1.3.1,1.4.0,,,,,Spark Shell,,,,0,,,,,,"Anyone can reproduce this issue by the code below
(runs well in local mode, got exception with clusters)
(it runs well in Spark 1.1.1)

{code}
case class ClassA(value: String)
val rdd = sc.parallelize(List((""k1"", ClassA(""v1"")), (""k1"", ClassA(""v2"")) ))
rdd.groupByKey.collect
{code}


{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 162 in stage 1.0 failed 4 times, most recent failure: Lost task 162.3 in stage 1.0 (TID 1027, ip-172-16-182-27.ap-northeast-1.compute.internal): java.lang.ClassNotFoundException: $iwC$$iwC$$iwC$$iwC$UserRelationshipRow
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:274)
at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:59)
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:133)
at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:91)
at org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:44)
at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:56)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
at akka.actor.ActorCell.invoke(ActorCell.scala:487)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
at akka.dispatch.Mailbox.run(Mailbox.scala:220)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

{code}",,apachespark,dreyco676,jongyoul,kevin@between.us,senkwich,swkimme,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3203,SPARK-7061,,,,,,,,,,,,,ZEPPELIN-158,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Aug 17 14:40:04 UTC 2015,,,,,,,,,,"0|i26o7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/15 15:22;srowen;Hm, is this supposed to work? the class is not defined outside your driver process, and isn't found on the executors as a result.;;;","13/Mar/15 02:05;swkimme;Hi Sean, 
Surely it should work, I guess this is quite common pattern while working with spark shell. 
(This code works in Spark 1.1.1) 
;;;","13/Mar/15 13:42;srowen;OK, could definitely be wrong about that. I tried it on 1.2.1 (YARN) and it worked fine:

{code}
res0: Array[(String, Iterable[ClassA])] = Array((k1,CompactBuffer(ClassA(v1), ClassA(v2))))
{code}

On master, which is roughly 1.3.0 (local), it also works:

{code}
res0: Array[(String, Iterable[ClassA])] = Array((k1,CompactBuffer(ClassA(v1), ClassA(v2))))
{code}

How are you running this? I can't reproduce it.;;;","13/Mar/15 14:11;kevin@between.us;I'm running standalone cluster,
And I confirmed another guy in community can reproduce the error.
For local mode, it works fine on my side too.


;;;","13/Mar/15 14:20;srowen;Yes, I see it happens in standalone mode, so must somehow be specific to this mode. IIRC there are odd problems with case classes in particular, so you might try working around by defining a non-case class instead.;;;","16/Mar/15 05:59;swkimme;I've digging onto this issue, found some clues and workarounds. 

1. The problem came from 'JavaSerializer'  in 'NettyBlockTransferService'
it needs to be initialized with 'replClassLoader' in Executor, but it's just using 'Thread.currentThread.getContextClassLoader'.
I'm thinking about how can we set classloader properly in NettyBlockTransferService.

2. (EDIT) I'm finding workaround. (Tried setting spark.shuffle.blockTransferService to nio but failed) ;;;","16/Mar/15 14:18;apachespark;User 'swkimme' has created a pull request for this issue:
https://github.com/apache/spark/pull/5046;;;","28/Mar/15 20:34;senkwich;FYI, we had the same issue on Mesos for 1.2.1 when the class was defined through the REPL. So, it was not just limited to standalone mode.;;;","17/Aug/15 14:40;dreyco676;In case anyone else is stuck on 1.3.0 for a while I was able to get around this by setting SPARK_CLASSPATH before starting spark-shell. The jar was not actually used in the REPL but by including it seemed to resolve the issue of distributing Classes in my script.

SPARK_CLASSPATH=/path/ojdbc6.jar spark-shell --master yarn --deploy-mode client --num-executors 100 --driver-memory 7500m --executor-memory 7500m
;;;",,,,,,,,,,,,,,,,,,,,
PySpark task may hang while call take() on in Java/Scala,SPARK-6294,12781364,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,11/Mar/15 23:59,28/May/15 03:24,14/Jul/23 06:27,12/Mar/15 22:19,1.2.1,1.3.0,,,,,1.2.2,1.3.1,1.4.0,,,,PySpark,,,,0,,,,,,"{code}
>>> rdd = sc.parallelize(range(1<<20)).map(lambda x: str(x))
>>> rdd._jrdd.first()
{code}

There is the stacktrace while hanging:

{code}
""Executor task launch worker-5"" daemon prio=10 tid=0x00007f8fd01a9800 nid=0x566 in Object.wait() [0x00007f90481d7000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x0000000630929340> (a org.apache.spark.api.python.PythonRDD$WriterThread)
	at java.lang.Thread.join(Thread.java:1281)
	- locked <0x0000000630929340> (a org.apache.spark.api.python.PythonRDD$WriterThread)
	at java.lang.Thread.join(Thread.java:1355)
	at org.apache.spark.api.python.PythonRDD$$anonfun$compute$1.apply(PythonRDD.scala:78)
	at org.apache.spark.api.python.PythonRDD$$anonfun$compute$1.apply(PythonRDD.scala:76)
	at org.apache.spark.TaskContextImpl$$anon$1.onTaskCompletion(TaskContextImpl.scala:49)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:68)
	at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:66)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:58)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 22:19:51 UTC 2015,,,,,,,,,,"0|i26nyn:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.1,1.4.0,,,,,,,,,,,"12/Mar/15 00:25;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4987;;;","12/Mar/15 17:25;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/5003;;;","12/Mar/15 22:19;mengxr;Issue resolved by pull request 5003
[https://github.com/apache/spark/pull/5003];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
spark.ml.param.Params.checkInputColumn bug upon error,SPARK-6290,12781341,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mengxr,josephkb,josephkb,11/Mar/15 22:22,24/Apr/15 21:33,14/Jul/23 06:27,24/Apr/15 21:33,1.3.0,,,,,,1.4.0,,,,,,ML,,,,0,,,,,,"In checkInputColumn, if data types do not match, it tries to print an error message with this in it:
{code}
Column param description: ${getParam(colName)}""
{code}
However, getParam cannot be called on the string colName; it needs the parameter name, which this method is not given.  This causes a weird error which users may find hard to understand.",,gweidner,josephkb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5957,,,,,,,,,,,SPARK-7137,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 21:26:41 UTC 2015,,,,,,,,,,"0|i26ntj:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"23/Apr/15 01:00;gweidner;I would like to work on this.  In a test scenario created to force the error, observed the first part of the message was helpful in describing the failure due to types not matching and included the required vs. actual type information.  However, the second part of the message contained a java.lang.IllegalArgumentException embedded with stack trace.  For example:

java.lang.IllegalArgumentException: requirement failed: 
	at scala.Predef$.require(Predef.scala:233)
	at org.apache.spark.ml.param.Params$class.checkInputColumn(params.scala:206)

Can you confirm the exception stack trace part is the ""weird"" error referred to in the description?  Thanks.;;;","23/Apr/15 01:33;josephkb;There should be a complaint from within the getParam method and/or from string interpolation.  Could you please post what you ran and the full error you saw?  (I apologize for not posting this info when I first saw this error.);;;","23/Apr/15 18:26;gweidner;Thank you Joseph for the quick reply and my apologies for not including test scenario details.  The ""java.lang.IllegalArgumentException: requirement failed:"" was normal for this case since scala.Predef.require throws the exception when the requirement expression is false.  Note another test scenario was created from org.apache.spark.ml.param.ParamsSuite which resulted in an exception being thrown in getParam method:

  test(""SPARK-6290"") {

    val field = new StructField(""missing"", StringType, false)

    val schema = new StructType(Array(field))

    // Force require expression in Params.checkInputColumn to fail since actual is StringType

    solver.checkInputColumn2(schema, ""missing"", BooleanType)

  }

org.apache.spark.ml.param.TestParams.missing()
java.lang.NoSuchMethodException: org.apache.spark.ml.param.TestParams.missing()
	at java.lang.Class.getMethod(Class.java:1670)
	at org.apache.spark.ml.param.Params$class.getParam(params.scala:163)
	at org.apache.spark.ml.param.TestParams.getParam(TestParams.scala:23)
	at org.apache.spark.ml.param.Params$$anonfun$checkInputColumn$1.apply(params.scala:209)
	at org.apache.spark.ml.param.Params$$anonfun$checkInputColumn$1.apply(params.scala:207)
	at scala.Predef$.require(Predef.scala:233)
	at org.apache.spark.ml.param.Params$class.checkInputColumn(params.scala:206)
	at org.apache.spark.ml.param.TestParams.checkInputColumn(TestParams.scala:23)
	at org.apache.spark.ml.param.TestParams.checkInputColumn2(TestParams.scala:32)
	at org.apache.spark.ml.param.ParamsSuite$$anonfun$5.apply$mcV$sp(ParamsSuite.scala:119)
	at org.apache.spark.ml.param.ParamsSuite$$anonfun$5.apply(ParamsSuite.scala:110)
	at org.apache.spark.ml.param.ParamsSuite$$anonfun$5.apply(ParamsSuite.scala:110)

However, using one of the existing parameters (e.g., ""inputCol"") defined in TestParams.scala allowed getParam to complete successfully.
;;;","24/Apr/15 18:55;gweidner;After synchronizing with latest from master, observed (by comparing usage in org.apache.spark.ml.impl.estimator.Predictor.validateAndTransformSchema) that spark.ml.param.Params.checkInputColumn has been replaced with org.apache.spark.ml.util.SchemaUtils.checkColumnType.  In addition, the call to Params.getParam has been removed from the require statement:

new version checkColumnType:

    require(actualDataType.equals(dataType),
      s""Column $colName must be of type $dataType but was actually $actualDataType."")

previous version checkInputColumn:

    require(actualDataType.equals(dataType),
      s""Input column $colName must be of type $dataType"" +
        s"" but was actually $actualDataType.  Column param description: ${getParam(colName)}"")

Since the call to getParam which was causing the issue has been removed, the error can no longer occur.  Can this issue be marked resolved, or would it be helpful if I still added a test case?
;;;","24/Apr/15 21:26;josephkb;Thanks!  I should have realized that.  I'll mark this as resolved.  However, it could be nice to move checkColumnType back to Params so that it can print out info about the incorrectly specified column.  I'll make a JIRA for that.;;;",,,,,,,,,,,,,,,,,,,,,,,,
PySpark doesn't maintain SQL date Types,SPARK-6289,12781309,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,mnazario,mnazario,11/Mar/15 20:37,09/Jul/15 21:43,14/Jul/23 06:27,09/Jul/15 21:43,1.2.1,,,,,,1.5.0,,,,,,PySpark,SQL,,,0,,,,,,"For the DateType, Spark SQL requires a datetime.date in Python. However, if you collect a row based on that type, you'll end up with a returned value which is type datetime.datetime.

I have tried to reproduce this using the pyspark shell, but have been unable to. This is definitely a problem coming from pyrolite though:

https://github.com/irmen/Pyrolite/

Pyrolite is being used for datetime and date serialization, but appears to not map to date objects, but maps to datetime objects.",,apachespark,davies,mnazario,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7314,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jul 09 21:43:58 UTC 2015,,,,,,,,,,"0|i26nnj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/15 02:50;davies;[~mnazario] Is this still a problem after we upgrade Pyrolite to 4.4? Could you help to very that, thanks!;;;","13/May/15 21:57;mnazario;This is the problem I have in my tests in Spark 1.3.1.

I've reproduced this problem with a much simpler piece of code in the pyspark shell:

{code}
>>> import pandas, datetime
>>> df = pandas.DataFrame([[datetime.datetime(1990, 1, 1), datetime.date(2000, 3, 3)]], columns=[""foo"", ""bar""])
>>> sdf = sqlCtx.createDataFrame(df)
>>> sdf
DataFrame[foo: bigint, bar: date]
>>> row = sdf.first()
>>> row
Row(foo=631152000000000000, bar=datetime.date(2000, 3, 3))
>>> row[1]
datetime.datetime(2000, 3, 3, 0, 0)
>>> row.bar
datetime.date(2000, 3, 3)
{code}

I'll test this out in Spark 1.4 soon;;;","15/May/15 00:08;davies;[~mnazario] datetime is a subclass of date, so a datetime object is also a date object (you could use it as date object). Does it work for you?;;;","16/May/15 00:20;mnazario;This does work for me, but it seems odd that this behavior would change based on the way you access data.;;;","21/May/15 23:35;davies;This will be fixed by upgrading to Pyrolite 4.6, which will pickle java.sql.Date as datetime.date;;;","09/Jul/15 00:54;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/7301;;;","09/Jul/15 21:43;davies;Issue resolved by pull request 7301
[https://github.com/apache/spark/pull/7301];;;",,,,,,,,,,,,,,,,,,,,,,
Handle TASK_ERROR in TaskState,SPARK-6286,12781256,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dragos,dragos,dragos,11/Mar/15 17:17,19/Mar/15 09:14,14/Jul/23 06:27,18/Mar/15 13:16,,,,,,,1.3.1,1.4.0,,,,,Spark Core,,,,0,mesos,,,,,"Scala warning:

{code}
match may not be exhaustive. It would fail on the following input: TASK_ERROR
{code}
",,apachespark,dragos,jongyoul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 19 09:14:59 UTC 2015,,,,,,,,,,"0|i26nbr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/15 17:24;srowen;I remember looking at this -- it's a Mesos enum right? I wondered if it were a new-ish value, and handling it would somehow make the code incompatible with older versions of Mesos, so I didn't want to touch it. But that is not based on any real knowledge.

If you know of what to do in this state and confirm that it's not a value specific to only some supported Mesos versions, then I'd go for it and add a handler.;;;","11/Mar/15 18:07;dragos;Good point. It's been [introduced in 0.21.0|http://mesos.apache.org/blog/mesos-0-21-0-released/]. According to [pom.xml|https://github.com/apache/spark/blob/master/pom.xml#L119], Spark depends on `0.21.0`, so it seems safe to handle it. Feel free to close if you think it's going to break something else.;;;","11/Mar/15 18:50;srowen;Probably OK for 1.4.x. Do you know what to do with this case?
[~jongyoul] do you have an opinion? I think you made the update to 0.21.0.;;;","12/Mar/15 06:53;jongyoul;[~srowen] I saw release note and a new protocol {{TASK_ERROR}} which avoids falling in infinite loop by making wrong task description. And I agree with you because this is not compatible with older versions of Mesos, and in case of spark, {{TASK_ERROR}} occurs rarely - personally, none - because taskSchedulerImpl makes mesosTask mechanically. However, if you need to code for {{TASK_ERROR}} to prepare future releases - actually no one wants to downgrade mesos without any reason -, I think there are two options for dealing with {{TASK_ERROR}}. One is to treat it as same as {{TASK_LOST}}, and another one is killing this spark application itself because this might be a kind of system error. I think it's enough to choose the first option for now, and we make this code compatible with older versions.;;;","12/Mar/15 15:13;srowen;[~dragos] I think it would be reasonable to handle this like {{TASK_LOST}}. I agree that there is not a reason to expect Mesos will be downgraded, and the required version is already required by Spark. This is also a little important to make sure this message is handled as intended and does not cause an exception. You want to make the simple PR?;;;","12/Mar/15 16:26;dragos;Sure, I'll issue a PR for handling {{TASK_ERROR => TASK_LOST}};;;","12/Mar/15 16:56;apachespark;User 'dragos' has created a pull request for this issue:
https://github.com/apache/spark/pull/5000;;;","18/Mar/15 13:16;srowen;Resolved by https://github.com/apache/spark/pull/5000;;;","19/Mar/15 09:14;apachespark;User 'jongyoul' has created a pull request for this issue:
https://github.com/apache/spark/pull/5088;;;",,,,,,,,,,,,,,,,,,,,
Duplicated code leads to errors,SPARK-6285,12781248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,dragos,dragos,11/Mar/15 16:57,15/Mar/15 12:45,14/Jul/23 06:27,15/Mar/15 12:45,1.3.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"The following class is duplicated inside [ParquetTestData|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetTestData.scala#L39] and [ParquetIOSuite|https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/parquet/ParquetIOSuite.scala#L44], with exact same code and fully qualified name:

{code}
org.apache.spark.sql.parquet.TestGroupWriteSupport
{code}

The second one was introduced in [3b395e10|https://github.com/apache/spark/commit/3b395e10510782474789c9098084503f98ca4830], but even though it mentions that `ParquetTestData` should be removed later, I couldn't find a corresponding Jira ticket.

This duplicate class causes the Eclipse builder to fail (since src/main and src/test are compiled together in Eclipse, unlike Sbt).",,apachespark,dragos,lian cheng,OopsOutOfMemory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 15 12:45:07 UTC 2015,,,,,,,,,,"0|i26n9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/15 17:34;srowen;I do not observe any compilation problem in Maven or IntelliJ though, so I don't know if it's an actual problem in the source.

That said, I don't see why there are two copies of the same class; one can be removed. But the containing class in the main source tree looks like test code. I think you can try moving it to the test tree too as part of a fix. ParquetTest is only used from test code, and ParquetTestData is... only used in sql's README.md? maybe my IDE is reading that wrong.;;;","11/Mar/15 18:01;dragos;According to the git commit message that introduced the duplicate:

{quote}
 To avoid potential merge conflicts, old testing code are not removed yet. The following classes can be safely removed after most Parquet related PRs are handled:

- `ParquetQuerySuite`
- `ParquetTestData`
{quote}

I mentioned the Eclipse build problem in passing, but I can expand: the class *is* a duplicated name, so the Scala compiler is correct in refusing it. It only compiles in Sbt/Maven because the src/main and src/test are compiled in separate compiler runs, and scalac seems to not notice the duplicate name when it comes from bytecode. Eclipse builds src/main and src/test together, and when both classes originate from sources scalac issues an error message.
;;;","11/Mar/15 18:52;srowen;[~lian cheng] do you think we can now remove these two classes in order to resolve the issue?;;;","13/Mar/15 09:33;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/5010;;;","13/Mar/15 09:43;lian cheng;[~fiberit] and [~srowen], thanks for reporting this! These two classes should have been removed in [#4116|https://github.com/apache/spark/pull/4116]. That PR wasn't associated with a JIRA ticket and was marked as ""Minor"". Now in retrospect, it seems not that minor actually... Just opened a PR to fix this.;;;","13/Mar/15 18:55;dragos;Thanks, [~lian cheng];;;","13/Mar/15 23:16;lian cheng;Issue resolved by pull request 5010
[https://github.com/apache/spark/pull/5010];;;","15/Mar/15 10:06;apachespark;User 'OopsOutOfMemory' has created a pull request for this issue:
https://github.com/apache/spark/pull/5032;;;","15/Mar/15 10:14;OopsOutOfMemory;There is also several references for `ParquetTestData` that need to be removed In SparkBuild.scala and README.md. 
Otherwise `build/sbt hive/console` can not be launched.

PR #5032 will resolve this.;;;","15/Mar/15 10:17;lian cheng;Reopening this because of the issue [PR #5032|https://github.com/baishuo/spark/pull/2] fixes.;;;","15/Mar/15 12:45;lian cheng;Issue resolved by pull request 5032
[https://github.com/apache/spark/pull/5032];;;",,,,,,,,,,,,,,,,,,
"Miss expressions flag ""s"" at logging string ",SPARK-6279,12781114,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,zzcclp,zzcclp,zzcclp,11/Mar/15 08:15,27/Mar/15 11:46,14/Jul/23 06:27,11/Mar/15 12:23,1.3.0,,,,,,1.4.0,,,,,,DStreams,,,,0,,,,,,"In KafkaRDD.scala, Miss expressions flag ""s"" at logging string

In logging file, it print `Beginning offset ${part.fromOffset} is the same as ending offset ` but not `log.warn(""Beginning offset 111 is the same as ending offset ""`.",,apachespark,zzcclp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6569,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 06:34:17 UTC 2015,,,,,,,,,,"0|i26mgf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/15 08:22;srowen;(Don't bother making a JIRA for trivial issues, where the description of the problem is largely identical to the fix.);;;","11/Mar/15 08:23;apachespark;User 'zzcclp' has created a pull request for this issue:
https://github.com/apache/spark/pull/4979;;;","11/Mar/15 12:23;srowen;Issue resolved by pull request 4979
[https://github.com/apache/spark/pull/4979];;;","13/Mar/15 06:34;zzcclp;[~srowen], I am new to Spark and JIRA, Sorry for this;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Types are now reserved words in DDL parser.,SPARK-6250,12780997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,marmbrus,marmbrus,10/Mar/15 21:42,21/Mar/15 20:28,14/Jul/23 06:27,21/Mar/15 20:28,,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,nitay,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 21 20:28:17 UTC 2015,,,,,,,,,,"0|i26lqv:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"17/Mar/15 01:09;marmbrus;We have confirmed that this does work if you escape the field names using backticks.  Since this is pretty standard, I'm going to close this ""wont fix"".  If there is some case where this is not possible, please reopen with details.;;;","17/Mar/15 01:18;nitay;Is it hard to fix this? Seems to me it would be a pretty straightforward thing to do? It's not always easy to change the underlying data model, for example working with existing hive metastore not under your control.;;;","17/Mar/15 01:25;marmbrus;I'm not suggesting you change your datamodel.  Just that if you are going to name your columns after datatypes you escape them with backticks (as you must do when using any reserved word or non-standard characters).  When interacting with the Hive metastore I would expect all required escaping to happen automatically.  Please let me know if you have a counter example.;;;","17/Mar/15 01:26;nitay;How would I do a select * from an existing hive metastores which have types in their names?;;;","17/Mar/15 01:30;yhuai;[~nitay] Have you tried backticks? Does it work?;;;","17/Mar/15 01:59;nitay;Backticks doesn't work for me on existing data. For example I work with a table that is structured like:

...
foo: struct<timestamp:bigint,timezone:string>
...

Selecting *, `foo`, `foo.timestamp`, foo.`timestamp` all don't work.
What am I doing wrong?


;;;","17/Mar/15 02:01;nitay;The error is always the same: https://gist.github.com/nitay/8ba0efd739cf2e22ad23;;;","17/Mar/15 02:12;marmbrus;Okay, thanks for explaining the problem!;;;","17/Mar/15 02:13;nitay;Thanks [~marmbrus] and [~yhuai].;;;","17/Mar/15 05:23;yhuai;[~nitay] Just a quick update. I tried
{code}
HiveMetastoreTypes.toDataType(""struct<`timestamp`:bigint,timezone:string>"")
{code}
and I got
{code}
res1: org.apache.spark.sql.types.DataType = StructType(StructField(timestamp,LongType,true), StructField(timezone,StringType,true))
{code}

Can you post the query and the exception you got after you submitted the query?

I will also take a look at it.;;;","17/Mar/15 14:49;nitay;[~yhuai] that query works, but that's because there is no underlying table. What fails is if you have a table you're querying with that schema. Then no matter how you specify it in the SQL it will fail when it reads the schema.;;;","17/Mar/15 15:20;yhuai;I see. The problem is that the data type string returned by Hive does not have backticks. Will take care it soon.;;;","17/Mar/15 18:36;yhuai;Update: Seems this problem affects struct fields when a field name is using a reserved type keyword as the name. For top level column, it is fine.;;;","17/Mar/15 23:05;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/5078;;;","17/Mar/15 23:12;yhuai;[~nitay] Can you try https://github.com/apache/spark/pull/5078 and see if it works for your case?;;;","19/Mar/15 22:19;nitay;Sorry for delay I'll try it out asap, hopefully by eod tmrw.;;;","21/Mar/15 20:28;marmbrus;Issue resolved by pull request 5078
[https://github.com/apache/spark/pull/5078];;;",,,,,,,,,,,,
LocalRelation needs to implement statistics,SPARK-6248,12780917,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,yhuai,yhuai,10/Mar/15 17:20,20/Mar/15 22:58,14/Jul/23 06:27,20/Mar/15 22:58,,,,,,,1.3.1,,,,,,SQL,,,,0,,,,,,"We need to implement statistics for LocalRelation. Otherwise, we cannot join a LocalRelation with other tables. The following exception will be thrown.

{code}
java.lang.UnsupportedOperationException: LeafNode LocalRelation must implement statistics.
{code}
",,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 23:54:00 UTC 2015,,,,,,,,,,"0|i26l9r:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"17/Mar/15 23:54;yhuai;https://github.com/apache/spark/pull/5062 will also fix it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Certain self joins cannot be analyzed,SPARK-6247,12780915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,yhuai,yhuai,10/Mar/15 17:12,20/Mar/15 23:00,14/Jul/23 06:27,18/Mar/15 02:49,,,,,,,1.3.1,,,,,,SQL,,,,0,,,,,,"When you try the following code
{code}
val df =
   (1 to 10)
      .map(i => (i, i.toDouble, i.toLong, i.toString, i.toString))
      .toDF(""intCol"", ""doubleCol"", ""longCol"", ""stringCol1"", ""stringCol2"")

df.registerTempTable(""test"")

sql(
  """"""
  |SELECT x.stringCol2, avg(y.intCol), sum(x.doubleCol)
  |FROM test x JOIN test y ON (x.stringCol1 = y.stringCol1)
  |GROUP BY x.stringCol2
  """""".stripMargin).explain()
{code}

The following exception will be thrown.
{code}
[info]   java.util.NoSuchElementException: next on empty iterator
[info]   at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
[info]   at scala.collection.Iterator$$anon$2.next(Iterator.scala:37)
[info]   at scala.collection.IndexedSeqLike$Elements.next(IndexedSeqLike.scala:64)
[info]   at scala.collection.IterableLike$class.head(IterableLike.scala:91)
[info]   at scala.collection.mutable.ArrayBuffer.scala$collection$IndexedSeqOptimized$$super$head(ArrayBuffer.scala:47)
[info]   at scala.collection.IndexedSeqOptimized$class.head(IndexedSeqOptimized.scala:120)
[info]   at scala.collection.mutable.ArrayBuffer.head(ArrayBuffer.scala:47)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$7.applyOrElse(Analyzer.scala:247)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$7.applyOrElse(Analyzer.scala:197)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)
[info]   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:50)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:249)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:263)
[info]   at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
[info]   at scala.collection.Iterator$class.foreach(Iterator.scala:727)
[info]   at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
[info]   at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
[info]   at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
[info]   at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
[info]   at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
[info]   at scala.collection.AbstractIterator.to(Iterator.scala:1157)
[info]   at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
[info]   at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
[info]   at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
[info]   at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenUp(TreeNode.scala:292)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:247)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:197)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:196)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
[info]   at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
[info]   at scala.collection.immutable.List.foldLeft(List.scala:84)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
[info]   at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:1071)
[info]   at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:1071)
[info]   at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1069)
[info]   at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
[info]   at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
[info]   at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:915)
[info]   at org.apache.spark.serializer.SparkSqlSerializer2Suite$$anonfun$2.apply$mcV$sp(SparkSqlSerializer2Suite.scala:66)
[info]   at org.apache.spark.serializer.SparkSqlSerializer2Suite$$anonfun$2.apply(SparkSqlSerializer2Suite.scala:48)
[info]   at org.apache.spark.serializer.SparkSqlSerializer2Suite$$anonfun$2.apply(SparkSqlSerializer2Suite.scala:48)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuite.run(FunSuite.scala:1555)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6231,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 02:49:01 UTC 2015,,,,,,,,,,"0|i26l9b:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"12/Mar/15 05:19;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4991;;;","17/Mar/15 05:26;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5062;;;","18/Mar/15 02:49;marmbrus;Issue resolved by pull request 5062
[https://github.com/apache/spark/pull/5062];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
spark-ec2 can't handle clusters with > 100 nodes,SPARK-6246,12780870,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,alyaxey,nchammas,nchammas,10/Mar/15 14:35,22/May/15 17:14,14/Jul/23 06:27,19/May/15 23:46,1.3.0,,,,,,1.5.0,,,,,,EC2,,,,1,,,,,,"This appears to be a new restriction, perhaps resulting from our upgrade of boto. Maybe it's a new restriction from EC2. Not sure yet.

We didn't have this issue around the Spark 1.1.0 time frame from what I can remember. I'll track down where the issue is and when it started.

Attempting to launch a cluster with 100 slaves yields the following:

{code}
Spark AMI: ami-35b1885c
Launching instances...
Launched 100 slaves in us-east-1c, regid = r-9c408776
Launched master in us-east-1c, regid = r-92408778
Waiting for AWS to propagate instance metadata...
Waiting for cluster to enter 'ssh-ready' state.ERROR:boto:400 Bad Request
ERROR:boto:<?xml version=""1.0"" encoding=""UTF-8""?>
<Response><Errors><Error><Code>InvalidRequest</Code><Message>101 exceeds the maximum number of instance IDs that can be specificied (100). Please specify fewer than 100 instance IDs.</Message></Error></Errors><RequestID>217fd6ff-9afa-4e91-86bc-ab16fcc442d8</RequestID></Response>
Traceback (most recent call last):
  File ""./ec2/spark_ec2.py"", line 1338, in <module>
    main()
  File ""./ec2/spark_ec2.py"", line 1330, in main
    real_main()
  File ""./ec2/spark_ec2.py"", line 1170, in real_main
    cluster_state='ssh-ready'
  File ""./ec2/spark_ec2.py"", line 795, in wait_for_cluster_state
    statuses = conn.get_all_instance_status(instance_ids=[i.id for i in cluster_instances])
  File ""/path/apache/spark/ec2/lib/boto-2.34.0/boto/ec2/connection.py"", line 737, in get_all_instance_status
    InstanceStatusSet, verb='POST')
  File ""/path/apache/spark/ec2/lib/boto-2.34.0/boto/connection.py"", line 1204, in get_object
    raise self.ResponseError(response.status, response.reason, body)
boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request
<?xml version=""1.0"" encoding=""UTF-8""?>
<Response><Errors><Error><Code>InvalidRequest</Code><Message>101 exceeds the maximum number of instance IDs that can be specificied (100). Please specify fewer than 100 instance IDs.</Message></Error></Errors><RequestID>217fd6ff-9afa-4e91-86bc-ab16fcc442d8</RequestID></Response>
{code}

This problem seems to be with {{get_all_instance_status()}}, though I am not sure if other methods are affected too.",,alyaxey,apachespark,nchammas,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 19 23:47:59 UTC 2015,,,,,,,,,,"0|i26kzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/15 14:36;nchammas;FYI [~shivaram].;;;","10/Mar/15 14:55;srowen;The funny thing is, the typo in that error message (""specificied"") makes it easy to find some corroboration:

https://github.com/skavanagh/EC2Box/issues/8
https://github.com/worksap-ate/aws-sdk/issues/139

Looks like an AWS SDK limit? ;;;","10/Mar/15 18:23;shivaram;Hmm - This seems like a bad problem. And it looks like a AWS side change rather than a boto change I guess.
[~nchammas] Similar to the EC2Box issue above, can we also batch calls to `get_instances` 100 instances at a time ?;;;","10/Mar/15 20:32;nchammas;I dunno, I haven't looked into the problem yet (been out all day), but I'm surprised that everything else works with > 100 nodes: creating nodes, destroying them, getting them. It's just the status check call.

If we have to, sure I'll batch the calls. But I suspect there's a better way to do things. I'm surprised boto doesn't just abstract this problem away.

Anyway, I'll look into it and report back.;;;","18/May/15 03:42;shivaram;I just ran into this problem as well. This definitely does not happen with some of the older versions of the script. ;;;","19/May/15 15:36;alyaxey;This can be fixed by replacing the line in file ec2/spark_ec2.py

        statuses = conn.get_all_instance_status(instance_ids=[i.id for i in cluster_instances])

with the lines:

        max_batch = 100
        statuses = []
        for j in range((len(cluster_instances) + max_batch - 1) // max_batch):
            statuses.extend(conn.get_all_instance_status(instance_ids=[i.id for i in cluster_instances[j * max_batch:(j + 1) * max_batch]]));;;","19/May/15 15:51;shivaram;Could you send a PR for this ?;;;","19/May/15 20:24;apachespark;User 'alyaxey' has created a pull request for this issue:
https://github.com/apache/spark/pull/6267;;;","19/May/15 20:29;alyaxey;[~shivaram] Done. This is my first PR. Do I have to do anything else to contribute to this ticket?;;;","19/May/15 23:46;shivaram;Issue resolved by pull request 6267
[https://github.com/apache/spark/pull/6267];;;","19/May/15 23:47;shivaram;[~srowen] Could you add [~alyaxey] to the developers group and assign this issue ?;;;",,,,,,,,,,,,,,,,,,
jsonRDD() of empty RDD results in exception,SPARK-6245,12780850,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,farrellee,farrellee,10/Mar/15 13:42,16/Mar/15 19:07,14/Jul/23 06:27,11/Mar/15 14:09,1.2.1,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"converting an empty RDD to a JSON RDD results in an exception. this case is common when using spark streaming.

{code}
from pyspark import SparkContext
from pyspark.sql import SQLContext
sc = SparkContext()
qsc = SQLContext(sc)
qsc.jsonRDD(sc.parallelize([]))
{code}

exception:

{noformat}
Traceback (most recent call last):                                                                              
  File ""/tmp/bug.py"", line 5, in <module>
    qsc.jsonRDD(sc.parallelize([]))
  File ""/usr/share/spark/python/pyspark/sql.py"", line 1605, in jsonRDD
    srdd = self._ssql_ctx.jsonRDD(jrdd.rdd(), samplingRatio)
  File ""/usr/share/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/usr/share/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o27.jsonRDD.
: java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:886)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:886)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:886)
	at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:57)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:232)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
{noformat}
",,apachespark,farrellee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 15:03:38 UTC 2015,,,,,,,,,,"0|i26kv3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/15 13:55;srowen;I zapped a similar problem with {{histogram()}} a while ago... whatever is calling {{reduce}} there needs to first check for an empty RDD.;;;","10/Mar/15 22:27;farrellee;this is an issue for the scala interface as well.

{code}
scala> val qsc = new org.apache.spark.sql.SQLContext(sc)
qsc: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@36c77da5
scala> qsc.jsonRDD(sc.parallelize(List()))
java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:886)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:886)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:886)
	at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:57)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:232)
	at org.apache.spark.sql.SQLContext.jsonRDD(SQLContext.scala:204)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:15)
	at $iwC$$iwC$$iwC.<init>(<console>:20)
	at $iwC$$iwC.<init>(<console>:22)
	at $iwC.<init>(<console>:24)
	at <init>(<console>:26)
	at .<init>(<console>:30)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:852)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1125)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:674)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:705)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:669)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:828)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:873)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:785)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:628)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:636)
	at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:641)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:968)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:916)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:916)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1011)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}

{{org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:57)}} is surely the guilty party;;;","11/Mar/15 00:42;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4971;;;","11/Mar/15 14:09;srowen;Issue resolved by pull request 4971
[https://github.com/apache/spark/pull/4971];;;","16/Mar/15 14:47;farrellee;[~srowen] thanks for fixing this. it's nice to file a bug, go on vacation and see it fixed when you get back!

what do you think about adding this to 1.3.1?;;;","16/Mar/15 15:03;srowen;Seems reasonable to me. I had avoided back-porting less-than-major fixes but that said this is a clean fix, and if you think there's a good use for it, it's possible to cherry pick it.;;;",,,,,,,,,,,,,,,,,,,,,,,
The Operation of match did not conside the scenarios that order.dataType does not match NativeType,SPARK-6243,12780739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,DoingDone9,DoingDone9,DoingDone9,10/Mar/15 07:06,24/Apr/15 00:35,14/Jul/23 06:27,03/Apr/15 00:24,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"It did not conside that order.dataType does not match NativeType.

val comparison = order.dataType match {
     case n: NativeType if order.direction == Ascending =>
            n.ordering.asInstanceOf[Ordering[Any]].compare(left, right)
    case n: NativeType if order.direction == Descending =>
            n.ordering.asInstanceOf[Ordering[Any]].reverse.compare(left, right)
 }",,apachespark,DoingDone9,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 00:24:49 UTC 2015,,,,,,,,,,"0|i26k6n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/15 07:24;apachespark;User 'DoingDone9' has created a pull request for this issue:
https://github.com/apache/spark/pull/4959;;;","03/Apr/15 00:24;marmbrus;Issue resolved by pull request 4959
[https://github.com/apache/spark/pull/4959];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
[STREAMING] All data may not be recovered from WAL when driver is killed,SPARK-6222,12780433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,hshreedharan,hshreedharan,hshreedharan,09/Mar/15 08:46,24/Apr/15 00:36,14/Jul/23 06:27,19/Mar/15 06:17,1.3.0,,,,,,1.3.1,1.4.0,,,,,DStreams,,,,0,,,,,,"When testing for our next release, our internal tests written by [~wypoon] caught a regression in Spark Streaming between 1.2.0 and 1.3.0. The test runs FlumePolling stream to read data from Flume, then kills the Application Master. Once YARN restarts it, the test waits until no more data is to be written and verifies the original against the data on HDFS. This was passing in 1.2.0, but is failing now.

Since the test ties into Cloudera's internal infrastructure and build process, it cannot be directly run on an Apache build. But I have been working on isolating the commit that may have caused the regression. I have confirmed that it was caused by SPARK-5147 (PR # [4149|https://github.com/apache/spark/pull/4149]). I confirmed this several times using the test and the failure is consistently reproducible. 

To re-confirm, I reverted just this one commit (and Clock consolidation one to avoid conflicts), and the issue was no longer reproducible.

Since this is a data loss issue, I believe this is a blocker for Spark 1.3.0
/cc [~tdas], [~pwendell]",,anandriyer,apachespark,hshreedharan,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5142,,,,,,,,,,,,,,,"10/Mar/15 00:29;hshreedharan;AfterPatch.txt;https://issues.apache.org/jira/secure/attachment/12703551/AfterPatch.txt","10/Mar/15 00:29;hshreedharan;CleanWithoutPatch.txt;https://issues.apache.org/jira/secure/attachment/12703550/CleanWithoutPatch.txt","09/Mar/15 22:31;hshreedharan;SPARK-6122.patch;https://issues.apache.org/jira/secure/attachment/12703519/SPARK-6122.patch",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 05:09:13 UTC 2015,,,,,,,,,,"0|i26ibz:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"09/Mar/15 18:39;tdas;Could you upload the stack traces, and logs that show is error? The PR # 4149 is about automatically deleting old log files. Is it an error that WAL files are deleted automatically too early? ;;;","09/Mar/15 22:31;hshreedharan;This patch fixes the issue -- so the issue is basically that somehow when the checkpoint data is being cleared up, it is still too early.

I am not sure of what the exact reason is yet. ;;;","09/Mar/15 23:10;tdas;Which patch fixes the issue?;;;","09/Mar/15 23:11;hshreedharan;The one on the jira. ;;;","09/Mar/15 23:20;srowen;[~hshreedharan] you can make a [WIP] pull request instead of a patch. It's easier to review that way.;;;","09/Mar/15 23:25;hshreedharan;[~srowen] This patch is actually not intended to fix the issue, since this patch will cause the WAL to not be cleaned up - which is not something we want. This was only intended to help isolate the problem -- from this patch it is clear that we are somehow attempting to clear the WAL data prematurely, causing the regression - when and why I am not yet sure.;;;","10/Mar/15 00:29;hshreedharan;Here are the logs:

CleanWithoutPatch -- this is run on an assembly built from a pristine source tree. This is the one where events are missing
AfterPatch - Here I apply the patch attached to this jira (where there is no periodic cleanup). No events are missing in this run.

Each file contains the logs of the same application, with a comment in the middle to show where the AM was killed and YARN restarted it.;;;","10/Mar/15 02:06;tdas;Offline discussion with Hari, these logs dont have the necessary information. He will generate debug/trace level logs.;;;","10/Mar/15 20:29;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4964;;;","10/Mar/15 20:36;hshreedharan;I could not generate TRACE level logs because of some weird issues with the cluster, but I was able to figure out what is happening here and was able to fix it with the PR #[4964|https://github.com/apache/spark/pull/4964]. This my analysis:

* We periodically generate a batch to be processed in the `generateJobs` method, which asynchronously starts a job to process the batch, but we checkpoint at the end of this method, without waiting for the job to complete. Therefore the checkpoint at time t has info about jobs started at t, not completed at t.
* On checkpoint completion, we remove all old checkpoints and any WAL files for time before t - 2 * checkpointDuration. This means that all WAL files for current - minus 2 batches (or 2 checkpoints worth) are deleted. We write a BatchCleared event also into the WAL
* When we recover, to get back to the same state as we were earlier, we throw out all metadata when we see a BatchCleared event. 

As a result when we recover, we never recover the data between the last checkpoint which was completed and the last checkpoint that was initiated, leading to data loss.;;;","10/Mar/15 21:12;hshreedharan;In the direct connector for Kafka, we checkpoint the starting and ending offsets for each batch, but at the checkpoint time, the batch has not actually been processed, but just submitted. If the app dies at that time, the checkpoint will contain the offsets of a batch that was not processed - this will result in data loss too. So this bug is relevant event when no WAL is present.;;;","10/Mar/15 23:45;hshreedharan;Another option is to change the way we delete old block data - we delete the data only for the lastProcessedBatch time. Even that should fix this issue (and does not change much of the checkpoint time logic). I am testing that out now. I, though, prefer the logic currently in the PR because it makes the checkpointing more deterministic - at checkpoint time ""t"", the batch generated at time ""t"" has been processed, while currently at the checkpoint time ""t"" - the batch may or may not have been processed, which is a bit non-deterministic than I'd like.;;;","11/Mar/15 00:07;hshreedharan;Thinking about it again - markBatchFullyProcessed(time) in JobGenerator seems to think a batch is processed when the checkpoint is completed, which is not the right thing to do - since the batch may not have been processed. So this actually seems like a real bug - we have an issue with checkpointing vs processing. We assume checkpoint completion means batch completion which simply is not true. I think the proposed method clears that up. For now, I am going to go ahead with that, fixing tests and 1 bug and updating PR.;;;","13/Mar/15 05:07;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/5008;;;","13/Mar/15 05:09;tdas;I proposed another way to fix this here
https://github.com/apache/spark/pull/5008
Basically, dont clear checkpoint data after the pre-batch-start checkpoint. 

BTW, super thanks to [~hshreedharan] for painstakingly explaining me offline what the problem was.;;;",,,,,,,,,,,,,,
The EXPLAIN output of CTAS only shows the analyzed plan,SPARK-6212,12780225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yijieshen,yhuai,yhuai,07/Mar/15 01:25,09/Aug/15 04:06,14/Jul/23 06:27,09/Aug/15 04:06,1.3.0,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"When you try
{code}
sql(""explain extended create table parquet2 as select * from parquet1"").collect.foreach(println)
{code}
The output will be 
{code}
[== Parsed Logical Plan ==]
['CreateTableAsSelect None, parquet2, false, Some(TOK_CREATETABLE)]
[ 'Project [*]]
[  'UnresolvedRelation [parquet1], None]
[]
[== Analyzed Logical Plan ==]
[CreateTableAsSelect [Database:default, TableName: parquet2, InsertIntoHiveTable]]
[Project [str#44]]
[ Subquery parquet1]
[  Relation[str#44] ParquetRelation2(List(file:/user/hive/warehouse/parquet1),Map(serialization.format -> 1, path -> file:/user/hive/warehouse/parquet1),Some(StructType(StructField(str,StringType,true))),None)]
[]
[]
[== Optimized Logical Plan ==]
[CreateTableAsSelect [Database:default, TableName: parquet2, InsertIntoHiveTable]]
[Project [str#44]]
[ Subquery parquet1]
[  Relation[str#44] ParquetRelation2(List(file:/user/hive/warehouse/parquet1),Map(serialization.format -> 1, path -> file:/user/hive/warehouse/parquet1),Some(StructType(StructField(str,StringType,true))),None)]
[]
[]
[== Physical Plan ==]
[ExecutedCommand (CreateTableAsSelect [Database:default, TableName: parquet2, InsertIntoHiveTable]]
[Project [str#44]]
[ Subquery parquet1]
[  Relation[str#44] ParquetRelation2(List(file:/user/hive/warehouse/parquet1),Map(serialization.format -> 1, path -> file:/user/hive/warehouse/parquet1),Some(StructType(StructField(str,StringType,true))),None)]
[)]
[]
[Code Generation: false]
[== RDD ==]
{code}

Query Plans of the SELECT clause shown in Optimized Plan and Physical Plan are actually analyzed plan.",,apachespark,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Aug 09 04:06:54 UTC 2015,,,,,,,,,,"0|i2fqyn:",9223372036854775807,,,,,yhuai,,,,,,Spark 1.5 release,,,1.5.0,,,,,,,,,,,,,"07/Mar/15 12:46;srowen;(Let's make sure to set component on issues, to help categorization a little bit.);;;","06/Aug/15 01:14;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/7980;;;","06/Aug/15 04:21;apachespark;User 'yjshen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7986;;;","09/Aug/15 04:06;yhuai;Issue resolved by pull request 7986
[https://github.com/apache/spark/pull/7986];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Generated column name should not include id of column in it.,SPARK-6210,12780206,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,06/Mar/15 23:39,21/Mar/15 21:56,14/Jul/23 06:27,21/Mar/15 21:56,1.3.0,,,,,,1.3.1,,,,,,SQL,,,,0,,,,,,"{code}
        >>> df.groupBy().max('age').collect()
        [Row(MAX(age#0)=5)]
        >>> df3.groupBy().max('age', 'height').collect()
        [Row(MAX(age#4L)=5, MAX(height#5L)=85)]
{code}",,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 00:52:58 UTC 2015,,,,,,,,,,"0|i26h0f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/15 00:52;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/5006;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorClassLoader can leak connections after failing to load classes from the REPL class server,SPARK-6209,12780198,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,06/Mar/15 22:56,17/Apr/15 21:08,14/Jul/23 06:27,17/Apr/15 21:08,1.0.0,1.0.3,1.1.2,1.2.1,1.3.0,1.4.0,1.2.3,1.3.1,1.4.0,,,,Spark Core,,,,1,,,,,,"ExecutorClassLoader does not ensure proper cleanup of network connections that it opens.  If it fails to load a class, it may leak partially-consumed InputStreams that are connected to the REPL's HTTP class server, causing that server to exhaust its thread pool, which can cause the entire job to hang.

Here is a simple reproduction:

With

{code}
./bin/spark-shell --master local-cluster[8,8,512] 
{code}

run the following command:

{code}
sc.parallelize(1 to 1000, 1000).map { x =>
  try {
      Class.forName(""some.class.that.does.not.Exist"")
  } catch {
      case e: Exception => // do nothing
  }
  x
}.count()
{code}

This job will run 253 tasks, then will completely freeze without any errors or failed tasks.

It looks like the driver has 253 threads blocked in socketRead0() calls:

{code}
[joshrosen ~]$ jstack 16765 | grep socketRead0 | wc
     253     759   14674
{code}

e.g.

{code}
""qtp1287429402-13"" daemon prio=5 tid=0x00007f868a1c0000 nid=0x5b03 runnable [0x00000001159bd000]
   java.lang.Thread.State: RUNNABLE
    at java.net.SocketInputStream.socketRead0(Native Method)
    at java.net.SocketInputStream.read(SocketInputStream.java:152)
    at java.net.SocketInputStream.read(SocketInputStream.java:122)
    at org.eclipse.jetty.io.ByteArrayBuffer.readFrom(ByteArrayBuffer.java:391)
    at org.eclipse.jetty.io.bio.StreamEndPoint.fill(StreamEndPoint.java:141)
    at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.fill(SocketConnector.java:227)
    at org.eclipse.jetty.http.HttpParser.fill(HttpParser.java:1044)
    at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:280)
    at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)
    at org.eclipse.jetty.server.BlockingHttpConnection.handle(BlockingHttpConnection.java:72)
    at org.eclipse.jetty.server.bio.SocketConnector$ConnectorEndPoint.run(SocketConnector.java:264)
    at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
    at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
    at java.lang.Thread.run(Thread.java:745) 
{code}

Jstack on the executors shows blocking in loadClass / findClass, where a single thread is RUNNABLE and waiting to hear back from the driver and other executor threads are BLOCKED on object monitor synchronization at Class.forName0().

Remotely triggering a GC on a hanging executor allows the job to progress and complete more tasks before hanging again.  If I repeatedly trigger GC on all of the executors, then the job runs to completion:

{code}
jps | grep CoarseGra | cut -d ' ' -f 1 | xargs -I {} -n 1 -P100 jcmd {} GC.run
{code}

The culprit is a {{catch}} block that ignores all exceptions and performs no cleanup: https://github.com/apache/spark/blob/v1.2.0/repl/src/main/scala/org/apache/spark/repl/ExecutorClassLoader.scala#L94

This bug has been present since Spark 1.0.0, but I suspect that we haven't seen it before because it's pretty hard to reproduce. Triggering this error requires a job with tasks that trigger ClassNotFoundExceptions yet are still able to run to completion.  It also requires that executors are able to leak enough open connections to exhaust the class server's Jetty thread pool limit, which requires that there are a large number of tasks (253+) and either a large number of executors or a very low amount of GC pressure on those executors (since GC will cause the leaked connections to be closed).

The fix here is pretty simple: add proper resource cleanup to this class.
",,aash,apachespark,joshrosen,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 21:08:07 UTC 2015,,,,,,,,,,"0|i26gyn:",9223372036854775807,,,,,,,,,,,,,,1.0.2,1.1.2,1.2.2,1.3.1,1.4.0,,,,,,,,,"06/Mar/15 23:01;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4935;;;","09/Mar/15 04:27;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4944;;;","24/Mar/15 22:23;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5174;;;","28/Mar/15 14:55;srowen;Oops, my bulk change shouldn't have caught this one. I see why it is unresolved but has Fix versions;;;","05/Apr/15 21:00;joshrosen;I've merged the backport for 1.2.2.;;;","17/Apr/15 21:08;joshrosen;I'm resolving this as ""Fixed"" since it has been resolved for 1.2.3+.;;;",,,,,,,,,,,,,,,,,,,,,,,
YARN secure cluster mode doesn't obtain a hive-metastore token ,SPARK-6207,12780169,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dougb,dougb,dougb,06/Mar/15 21:03,24/Apr/15 00:36,14/Jul/23 06:27,13/Apr/15 14:51,1.2.0,1.2.1,1.3.0,,,,1.4.0,,,,,,Spark Submit,SQL,YARN,,0,,,,,,"When running a spark job, on YARN in secure mode, with ""--deploy-mode cluster"",  org.apache.spark.deploy.yarn.Client() does not obtain a delegation token to the hive-metastore. Therefore any attempts to talk to the hive-metastore fail with a ""GSSException: No valid credentials provided...""

",YARN,apachespark,crystal_gaoyu,dougb,mdominguez@cloudera.com,michaelmalak,neelesh77,tgraves,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 12:45:22 UTC 2015,,,,,,,,,,"0|i26gsf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/15 21:07;dougb;I have some code that fixes this issue. I'm preparing a pr.
;;;","15/Mar/15 07:32;apachespark;User 'dougb' has created a pull request for this issue:
https://github.com/apache/spark/pull/5031;;;","17/Mar/15 02:25;dougb;Need to catch java.lang.UnsupportedOperationException and ignore,
or check to see if delegation token mode is supported with current configuration 
before trying to get a delegation token.
See  https://issues.apache.org/jira/browse/HIVE-4625
;;;","10/Apr/15 12:45;dougb;I updated the pr a couple days ago. Looking for feedback. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
UISeleniumSuite fails for Hadoop 2.x test with NoClassDefFoundError,SPARK-6205,12780113,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,06/Mar/15 16:29,06/Apr/15 14:45,14/Jul/23 06:27,08/Mar/15 14:09,1.3.0,,,,,,1.3.2,1.4.0,,,,,Tests,,,,0,,,,,,"{code}
mvn -DskipTests -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=2.6.0 clean install
mvn -Pyarn -Phive -Phadoop-2.4 -Dhadoop.version=2.6.0 test -DwildcardSuites=org.apache.spark.ui.UISeleniumSuite -Dtest=none -pl core/ 
{code}

will produce:

{code}
UISeleniumSuite:
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal
  ...
{code}

It doesn't seem to happen without the various profiles set above.

The fix is simple, although sounds weird; Selenium's dependency on {{xml-apis:xml-apis}} must be manually included in core's test dependencies. This probably has something to do with Hadoop 2 vs 1 dependency changes and the fact that Maven test deps aren't transitive, AFAIK.

PR coming...",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 08 14:09:53 UTC 2015,,,,,,,,,,"0|i26gfz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"06/Mar/15 16:31;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4933;;;","08/Mar/15 14:09;srowen;Issue resolved by pull request 4933
[https://github.com/apache/spark/pull/4933];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
INSET should coerce types,SPARK-6201,12780011,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,huangjs,huangjs,06/Mar/15 08:06,06/May/15 17:34,14/Jul/23 06:27,06/May/15 17:34,1.2.0,1.2.1,1.3.0,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Suppose we have the following table:

{code}
sqlc.jsonRDD(sc.parallelize(Seq(""{\""a\"": \""1\""}}"", ""{\""a\"": \""2\""}}"", ""{\""a\"": \""3\""}}""))).registerTempTable(""d"")
{code}

The schema is
{noformat}
root
 |-- a: string (nullable = true)
{noformat}

Then,

{code}
sql(""select * from d where (d.a = 1 or d.a = 2)"").collect
=>
Array([1], [2])
{code}

where d.a and constants 1,2 will be casted to Double first and do the comparison as you can find it out in the plan:

{noformat}
Filter ((CAST(a#155, DoubleType) = CAST(1, DoubleType)) || (CAST(a#155, DoubleType) = CAST(2, DoubleType)))
{noformat}

However, if I use

{code}
sql(""select * from d where d.a in (1,2)"").collect
{code}

The result is empty.

The physical plan shows it's using INSET:
{noformat}
== Physical Plan ==
Filter a#155 INSET (1,2)
 PhysicalRDD [a#155], MappedRDD[499] at map at JsonRDD.scala:47
{noformat}


*It seems INSET implementation in SparkSQL doesn't coerce type implicitly, where Hive does. We should make SparkSQL conform to Hive's behavior, even though doing implicit coercion here is very confusing for comparing String and Int.*

Jianshi
",,adrian-wang,apachespark,huangjs,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 17:39:04 UTC 2015,,,,,,,,,,"0|i26ftb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/15 06:32;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4945;;;","09/Mar/15 08:04;adrian-wang;I have created a pull request for this.;;;","09/Mar/15 14:26;lian cheng;Played Hive type implicit conversion a bit more and found that Hive actually converts integers to strings in your case:
{code:sql}
hive> create table t1 as select '1.00' as c1;
hive> select * from t1 where c1 in (1.0);
{code}
If {{c1}} is converted to numeric, then {{1.00}} should appear in the result. However, the result set is empty. For expression {{""1.00"" IN (1.0)}}, a {{GenericUDFIn}} instance is created and called with argument list {{(""1.00"", 1.0)}}. Then {{GenericUDFIn.initialize}} tries to convert all arguments into a common data type from left to right. Since double is allowed to be translated into string, {{1.0}} is converted into string {{""1.0""}}.

References:
# [Implicit type coercion support in existing database systems|http://chapeau.freevariable.com/2014/08/existing-system-coercion.html] by William Benton
# [{{GenericUDFIn.initialize}}|https://github.com/apache/hive/blob/release-0.13.1/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFIn.java#L84-L100]

;;;","09/Mar/15 15:31;lian cheng;I'm kinda not sure whether / how should we fix this issue. The {{IN}} operator is implemented as a {{GenericUDF}} in Hive, and is governed by the UDF argument type conversion rules, which I'm not sure whether they are standard and proper to be introduced into Spark SQL.;;;","09/Mar/15 17:39;huangjs;Implicit coercion outside the Numeric domain is quite evil. I don't think Hive's behavior makes sense here. 

Raising an exception is fine in this case. And if you want to make it Hive compliant, then pls think about adding an switch, say

bq.  spark.sql.strict_mode = true(default) / false

Jianshi;;;",,,,,,,,,,,,,,,,,,,,,,,,
handle json parse exception for eventlog file not finished writing ,SPARK-6197,12779971,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,liyezhang556520,liyezhang556520,liyezhang556520,06/Mar/15 03:10,02/Jan/16 14:22,14/Jul/23 06:27,02/Jan/16 14:21,1.3.0,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"This is a following JIRA for [SPARK-6107|https://issues.apache.org/jira/browse/SPARK-6107]. In  [SPARK-6107|https://issues.apache.org/jira/browse/SPARK-6107], webUI can display event log files that with suffix *.inprogress*. However, the eventlog file may be not finished writing for some abnormal cases (e.g. Ctrl+C), In which case, the file maybe  truncated in the last line, leading to the line being not in valid Json format. Which will cause Json parse exception when reading the file. 

For this case, we can just ignore the last line content, since the history for abnormal cases showed on web is only a reference for user, it can demonstrate the past status of the app before terminated abnormally (we can not guarantee the history can show exactly the last moment when app encounter the abnormal situation). ",,andrewor14,apachespark,glenn.strycker@gmail.com,liyezhang556520,qwertymaniac,rdub,thale013,Xia Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6314,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 02 14:22:19 UTC 2016,,,,,,,,,,"0|i26fkn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/15 05:37;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/4927;;;","13/Mar/15 14:01;srowen;Issue resolved by pull request 4927
[https://github.com/apache/spark/pull/4927];;;","28/Apr/15 19:37;andrewor14;https://github.com/apache/spark/pull/5736;;;","16/May/15 11:51;srowen;I back-ported to 1.3.x. I infer from the previous target and label that this was the work left to do.;;;","21/May/15 02:57;Xia Hu;this patch seems useless to me. My version is 1.3.1 and env is spark-on-yarn. The problem still exists. ;;;","21/May/15 03:00;andrewor14;It would appear that it's because it was later reverted (https://github.com/apache/spark/commit/bf4ca12) because of a build break, but was never merged back into branch-1.3.;;;","21/May/15 04:21;liyezhang556520;Hi [~Xia Hu], This Patch only solve the problem for standalone Mode, for Yarn mode history server, Please refer to patch in [SPARK-6314|https://issues.apache.org/jira/browse/SPARK-6314];;;","21/May/15 06:22;Xia Hu;Ok, get it. Thank you very much~~;;;","02/Jan/16 14:22;srowen;I don't think this will ever be back-ported to 1.3.x at this point;;;",,,,,,,,,,,,,,,,,,,,
collect() in PySpark will cause memory leak in JVM,SPARK-6194,12779949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,06/Mar/15 01:14,20/Apr/15 19:07,14/Jul/23 06:27,02/Apr/15 17:53,1.0.2,1.1.1,1.2.1,1.3.0,,,1.2.2,1.3.1,1.4.0,,,,PySpark,,,,0,,,,,,"It could be reproduced  by:

{code}
for i in range(40):
    sc.parallelize(range(5000), 10).flatMap(lambda i: range(10000)).collect()
{code}

It will fail after 2 or 3 jobs, and run totally successfully if I add
`gc.collect()` after each job.

We could call _detach() for the JavaList returned by collect
in Java, will send out a PR for this.

Reported by Michael and commented by Josh：

On Thu, Mar 5, 2015 at 2:39 PM, Josh Rosen <joshrosen@databricks.com> wrote:
> Based on Py4J's Memory Model page
> (http://py4j.sourceforge.net/advanced_topics.html#py4j-memory-model):
>
>> Because Java objects on the Python side are involved in a circular
>> reference (JavaObject and JavaMember reference each other), these objects
>> are not immediately garbage collected once the last reference to the object
>> is removed (but they are guaranteed to be eventually collected if the Python
>> garbage collector runs before the Python program exits).
>
>
>>
>> In doubt, users can always call the detach function on the Python gateway
>> to explicitly delete a reference on the Java side. A call to gc.collect()
>> also usually works.
>
>
> Maybe we should be manually calling detach() when the Python-side has
> finished consuming temporary objects from the JVM.  Do you have a small
> workload / configuration that reproduces the OOM which we can use to test a
> fix?  I don't think that I've seen this issue in the past, but this might be
> because we mistook Java OOMs as being caused by collecting too much data
> rather than due to memory leaks.
>
> On Thu, Mar 5, 2015 at 10:41 AM, Michael Nazario <mnazario@palantir.com>
> wrote:
>>
>> Hi Josh,
>>
>> I have a question about how PySpark does memory management in the Py4J
>> bridge between the Java driver and the Python driver. I was wondering if
>> there have been any memory problems in this system because the Python
>> garbage collector does not collect circular references immediately and Py4J
>> has circular references in each object it receives from Java.
>>
>> When I dug through the PySpark code, I seemed to find that most RDD
>> actions return by calling collect. In collect, you end up calling the Java
>> RDD collect and getting an iterator from that. Would this be a possible
>> cause for a Java driver OutOfMemoryException because there are resources in
>> Java which do not get freed up immediately?
>>
>> I have also seen that trying to take a lot of values from a dataset twice
>> in a row can cause the Java driver to OOM (while just once works). Are there
>> some other memory considerations that are relevant in the driver?
>>
>> Thanks,
>> Michael",,apachespark,davies,joshrosen,mnazario,ofermend,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 20 19:07:02 UTC 2015,,,,,,,,,,"0|i26ffz:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"06/Mar/15 02:15;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4923;;;","13/Mar/15 17:45;joshrosen;I've merged this into `master` (1.4.0), `branch-1.3` (1.3.1), and `branch-1.2` (1.2.2).  Not sure if we want to backport it all the way back to 1.1.x or 1.0.x.;;;","28/Mar/15 14:56;srowen;Same, restored Fix versions. I fixed my query now.;;;","02/Apr/15 17:47;joshrosen;This patch introduced a rare bug that can cause a hang while calling {{collect()}} (SPARK-6194); I'm commenting here so that the issues are linked to ensure that the fixes land in the right branches.;;;","02/Apr/15 17:53;joshrosen;Going to resolve this as ""Fixed"" for now, since I don't think we're going to backport prior to 1.2.x.;;;","20/Apr/15 19:07;mnazario;I'm commenting here to fix the link which Josh posted here earlier about a bug which was caused by this. The link he posted was this same ticket when it should have been SPARK-6294.;;;",,,,,,,,,,,,,,,,,,,,,,,
Instance types can be mislabeled when re-starting cluster with default arguments,SPARK-6188,12779839,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tvas,tvas,tvas,05/Mar/15 17:04,09/Mar/15 14:17,14/Jul/23 06:27,09/Mar/15 14:16,1.0.2,1.1.0,1.1.1,1.2.0,1.2.1,,1.4.0,,,,,,EC2,,,,0,,,,,,"This was discovered when investigating https://issues.apache.org/jira/browse/SPARK-5838.

In short, when restarting a cluster that you launched with an alternative instance type, you have to provide the instance type(s) again in the ""/spark-ec2 -i <key-file> --region=<ec2-region> start <cluster-name>"" command. Otherwise it gets set to the default m1.large.

This then affects the setup of the machines.

I'll submit a pull request that takes cares of this, without the user needing to provide the instance type(s) again.

EDIT: 

Example case where this becomes a problem:
1. User launches a cluster with instances with 1 disk, ex. m3.large.
2. The user stops the cluster.
3. When the user restarts the cluster with the start command without providing the instance type, the setup is performed using the default instance type, m1.large, which assumes 2 disks present in the machine.
4. The SPARK_LOCAL_DIRS is then set to ""mnt/spark,mnt2/spark"". /mnt2 corresponds to the snapshot partition in a m3.large instance, which is only 8GB in size. When the user runs jobs that shuffle data, this partition fills up quickly, resulting in failed jobs due to ""No space left on device"" errors.

Apart from this example one could come up with other examples where the setup of the machines is wrong, due to assuming that they are of type m1.large.",,apachespark,tvas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5838,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 14:16:29 UTC 2015,,,,,,,,,,"0|i26esf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/15 17:19;apachespark;User 'thvasilo' has created a pull request for this issue:
https://github.com/apache/spark/pull/4916;;;","09/Mar/15 14:16;srowen;Issue resolved by pull request 4916
[https://github.com/apache/spark/pull/4916];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
spark-parent pom needs to be published for both 2.10 and 2.11,SPARK-6182,12779716,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,pwendell,pwendell,05/Mar/15 07:46,05/Mar/15 19:32,14/Jul/23 06:27,05/Mar/15 19:32,1.3.0,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,"A recent bug report (SPARK-5143) pointed out that the parent pom had the wrong scala version property when building for Scala 2.11. The fix for that issue correctly sets the parent pom property when the manual bash script is invoked to update Spark to scala 2.11.

For published artifacts, however, an issue still remains. Because our parent pom does not have a scala identifier, we can't publish two of them for scala 2.10 and 2.11. The fix is simple, we just need to add a scala version identifier to the parent pom and update all references to that parent.

For more details on how our current publishing process works, see this script:
https://github.com/apache/spark/blob/master/dev/create-release/create-release.sh#L119

We do a mvn install under both builds, then we manually copy the artifacts to the ASF nexus repository (necessary because Maven does not support cross-publishing nicely).",,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 14:22:38 UTC 2015,,,,,,,,,,"0|i26e13:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"05/Mar/15 14:04;srowen;I think there are two ways to resolve this:

1) Replace all occurrences of {{spark-parent}} with {{spark-parent_2.10}}. Use the scripts to simply update this value too during release.
2) Remove use of {{spark.binary.version}} where dependencies are expressed between modules, and hard-code {{2.10}}, updated to {{2.11}} during release. This would mean the parent POM no longer has to change between versions in order to get this right.

I'll submit a PR both ways for consideration. I suppose I feel like #2 is actually nicer, if it works.;;;","05/Mar/15 14:22;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4913;;;","05/Mar/15 14:22;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4912;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Executor log links are using internal addresses in EC2; display `:0` when ephemeral ports are used",SPARK-6175,12779649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,yhuai,yhuai,05/Mar/15 00:58,05/Mar/15 20:04,14/Jul/23 06:27,05/Mar/15 20:04,1.3.0,,,,,,1.3.0,,,,,,Web UI,,,,1,,,,,,See the comment at https://github.com/apache/spark/pull/3486#discussion_r25830508,,apachespark,joshrosen,neelesh77,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2450,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 20:04:25 UTC 2015,,,,,,,,,,"0|i26dmf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Mar/15 07:54;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4903;;;","05/Mar/15 20:04;joshrosen;Issue resolved by pull request 4903
[https://github.com/apache/spark/pull/4903];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Previous Commit Broke BroadcastTest,SPARK-6167,12779595,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,rnowling,rnowling,04/Mar/15 21:28,04/Mar/15 23:26,14/Jul/23 06:27,04/Mar/15 23:15,1.2.1,,,,,,1.1.2,1.2.2,,,,,Examples,,,,0,,,,,,"Commit associated with SPARK-1010 spell class names incorrectly (BroaddcastFactory instead of BroadcastFactory).  As a result, the BroadcastTest doesn't work.",,rnowling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 23:26:40 UTC 2015,,,,,,,,,,"0|i26dav:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/15 21:35;rnowling;This PR fixes the issue in master and the 1.3 branch:

https://github.com/apache/spark/pull/4724

Needs to be merged into 1.2 branch as well.;;;","04/Mar/15 23:15;srowen;I merged this into 1.1 and 1.2 as well.;;;","04/Mar/15 23:26;rnowling;Great! Thanks!



;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Unrolling with MEMORY_AND_DISK should always release memory,SPARK-6157,12779360,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,SuYan,SuYan,04/Mar/15 11:42,17/May/20 18:21,14/Jul/23 06:27,14/Mar/16 21:26,1.2.1,,,,,,2.0.0,,,,,,Block Manager,Spark Core,,,0,,,,,,"=== EDIT by andrewor14 ===

The existing description was somewhat confusing, so here's a more succinct version of it.

If unrolling a block with MEMORY_AND_DISK was unsuccessful, we will drop the block to disk
directly. After doing so, however, we don't need the underlying array that held the partial
values anymore, so we should release the pending unroll memory for other tasks on the same
executor. Otherwise, other tasks may unnecessarily drop their blocks to disk due to the lack
of unroll space, resulting in worse performance.

=== Original comment ===

Current code:
Now we want to cache a Memory_and_disk level block
1. Try to put in memory and unroll unsuccessful. then reserved unroll memory because we got a iterator from an unroll Array 
2. Then put into disk.
3. Get value from get(blockId), and iterator from that value, and then nothing with an unroll Array. So here we should release the reserved unroll memory instead will release  until the task is end.

and also, have somebody already pull a request, for get Memory_and_disk level block, while cache in memory from disk, we should, use file.length to check if we can put in memory store instead just allocate a file.length buffer, may lead to OOM.",,apachespark,nitin2goyal,saucam,SuYan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6156,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 09 21:43:09 UTC 2016,,,,,,,,,,"0|i26cav:",9223372036854775807,,,,,,,,,,,,,,2.0.0,,,,,,,,,,,,,"04/Mar/15 11:43;apachespark;User 'suyanNone' has created a pull request for this issue:
https://github.com/apache/spark/pull/4887;;;","09/Mar/16 21:43;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/11613;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark does not support Java 8 compiled Scala classes,SPARK-6152,12779285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,pyrolistical,pyrolistical,04/Mar/15 03:05,13/Nov/15 06:59,14/Jul/23 06:27,11/Nov/15 19:18,1.2.1,,,,,,1.6.0,,,,,,Spark Core,,,,8,,,,,,"Spark uses reflectasm to check Scala closures which fails if the *user defined Scala closures* are compiled to Java 8 class version

The cause is reflectasm does not support Java 8
https://github.com/EsotericSoftware/reflectasm/issues/35

Workaround:
Don't compile Scala classes to Java 8, Scala 2.11 does not support nor require any Java 8 features

Stack trace:
{code}
java.lang.IllegalArgumentException
	at com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.<init>(Unknown Source)
	at com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.<init>(Unknown Source)
	at com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.<init>(Unknown Source)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$getClassReader(ClosureCleaner.scala:41)
	at org.apache.spark.util.ClosureCleaner$.getInnerClasses(ClosureCleaner.scala:84)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:107)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:1478)
	at org.apache.spark.rdd.RDD.map(RDD.scala:288)
	at ...my Scala 2.11 compiled to Java 8 code calling into spark
{code}","Java 8+
Scala 2.11",anandriyer,apachespark,appaquet,aroberts,chlam4,cscotta,dispalt,dougb,Grigory Turunov,joshrosen,jthakrar,malcolmgreaves,marmbrus,maropu,martin.grotzke,michaelmalak,Oskar Blom,pyrolistical,rxin,stevel@apache.org,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Nov 13 06:59:39 UTC 2015,,,,,,,,,,"0|i26buf:",9223372036854775807,,,,,,,,,,,,,,1.6.0,,,,,,,,,,,,,"04/Mar/15 09:43;srowen;Spark is not compiled with Java 8. What problem are you reporting?;;;","04/Mar/15 17:38;pyrolistical;I've made the description more clear.  The problem is occurs when I create a Scala 2.11 project that imports Spark 1.2.1 and I compile my Scala code to Java 8 using ""scalac -target:jvm-1.8 ..."";;;","19/Mar/15 00:42;srowen;To your deleted comment -- yes indeed it looks like the library explicitly does not work with Java 8 since class file version 52 == Java 8;;;","20/Mar/15 13:01;martin.grotzke;I just released reflectasm-1.10.1 (which now should support java 8 due to the upgrade to asm 5) to maven central.;;;","20/Mar/15 13:04;srowen;Nice one. It looks like {{reflectasm}} comes in via {{chill}}. Do you know if/when {{chill}} might consume the newer version? then we could consume that.;;;","20/Mar/15 16:52;pyrolistical;If you want to get the new version via {{chill}}, you will also need to wait for a release of {{kryo}} as {{chill}} gets the dependency via {{kryo}}

-I think the root cause here is we are using a shaded library in a third party dependency.  Sounds like a bad practise.  We should just depend on an unshaded {{reflectasm}} directly-

{{kyro}} just updated to {{reflectasm-1.10.1}} four hours ago, so this dependency train will take a while to arrive.

EDIT: Actually, that was a silly idea.  We do need to upgrade {{chill}} otherwise we will just move the failure into {{kyro}};;;","20/Mar/15 17:10;martin.grotzke;I'll try to get out a new kryo version as soon as possible...;;;","20/Mar/15 17:17;martin.grotzke;Btw, chill guys are still on kryo 2.21, kryo right now is 3.0.0. IIRC there were some compatibility issues in kryo they complained about. Perhaps you already should open an issue for chill java 8 support to see what they think about it.;;;","20/Mar/15 20:34;pyrolistical;Done: https://github.com/twitter/chill/issues/223;;;","24/Mar/15 23:26;martin.grotzke;Btw, we just released kryo 3.0.1: https://github.com/EsotericSoftware/kryo/blob/master/CHANGES.md#2240---300-2014-0-04;;;","06/Jun/15 03:02;malcolmgreaves;It appears as if progress on updating chill to use a version of reflectasm that is compatible with Java 1.8 has stalled: https://github.com/twitter/chill/pull/224
;;;","29/Jul/15 20:25;stevel@apache.org;Chill and Kryo need to be in sync; there's also the need to be compatible with the version Hive uses, (which has historically been addressed with custom versions of Hive).

If spark could jump to Kryo 3.x, classpath conflict with hive would go away, provided the wire formats of serialized classes were compatible: hive's spark-client JAR uses kryo 2.2.x to talk to spark.;;;","14/Aug/15 20:40;malcolmgreaves;Interesting [~stevel@apache.org]! What kinds of changes do you think this would require -- mostly verifying that there's backward compatibility with those serialized classes?;;;","15/Sep/15 11:31;stevel@apache.org;what changes? Yes, making sure there are no regressions. Hive had to upgrade to deal with bugs Kryo 2.21; for Spark 1.5 there's a special org.spark-project.hive artifact which downgraded to Kryo 2.21; the subset of Hive that spark-hive and spark-hive-thriftserver all work there. Hive would certainly veto any reverting to 2.21; I don't know what their stance would be to a 3.x upgrade on the 1.2 branch ... reluctant would be the default response, I suspect.

Getting everything to 2.24 is more likely, though if 3.x is needed for Java 8 compatibility it could be argued for;;;","05/Nov/15 22:47;joshrosen;Do we actually need reflectasm itself for the closure cleaner or are we just using it as a convenient way to pull in a shaded ASM artifact? Why not publish our own shaded ASM 5 and use that instead?;;;","05/Nov/15 22:59;joshrosen;It turns out that Apache Geronimo has already published shaded ASM 5 artifacts:

http://mvnrepository.com/artifact/org.apache.xbean/xbean-asm5-shaded/4.4

Here's the source that was used to produce that shaded artifact:

https://github.com/apache/geronimo-xbean/tree/xbean-4.4/xbean-asm5-shaded

This corresponds to ASM 5.0.4, which is the latest release:

https://github.com/apache/geronimo-xbean/blob/xbean-4.4/pom.xml#L67

I'll investigate changing Spark to use this instead of reflectASM's shaded copy.;;;","05/Nov/15 23:46;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/9512;;;","10/Nov/15 19:42;joshrosen;Does anyone have a standalone reproduction of this issue that I can use to test my PR? https://github.com/apache/spark/pull/9512
 
EDIT: just realized that this issue pertains to _Scala_ classes that were compiled with Java 8. Will add a new test to try that out.;;;","10/Nov/15 19:57;joshrosen;Yep, was able to reproduce trivially by running Spark's existing Scala unit tests with JDK 8. I'm going to add some plumbing to the build in order to let us test this in Jenkins.;;;","11/Nov/15 19:18;marmbrus;Issue resolved by pull request 9512
[https://github.com/apache/spark/pull/9512];;;","13/Nov/15 06:59;Oskar Blom;Nice - when is 1.6.0 due for release?;;;",,,,,,,,
Spark SQL CLI doesn't work when compiled against Hive 12 with SBT because of runtime incompatibility issues caused by Guava 15,SPARK-6149,12779253,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,lian cheng,lian cheng,04/Mar/15 00:43,05/Mar/15 04:53,14/Jul/23 06:27,05/Mar/15 04:53,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"The following description is based on [a recent master revision|https://github.com/apache/spark/tree/159b24a1e47e4fa8118e4b81049fbc7bc3406433].

{noformat}
$ ./build/sbt -Pyarn,hadoop-2.4,hive,hive-thriftserver,hive-0.12.0,scala-2.10 -Dhadoop.version=2.4.1 clean assembly/assembly
...
$ ./bin/spark-sql
...
spark-sql> CREATE TABLE hive_test(key INT, value STRING);
15/03/03 21:28:08 ERROR exec.DDLTask: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:602)
        at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:3661)
        at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:252)
        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:151)
        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:65)
        at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1414)
        at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1192)
        at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1020)
        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:888)
        at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:308)
        at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:280)
        at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:55)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:55)
        at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:65)
        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:1092)
        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:1092)
        at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:134)
        at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:117)
        at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
        at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:92)
        at org.apache.spark.sql.hive.thriftserver.AbstractSparkSQLDriver.run(AbstractSparkSQLDriver.scala:57)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:275)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:211)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1212)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:62)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:72)
        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2372)
        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2383)
        at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:596)
        ... 34 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1210)
        ... 39 more
Caused by: javax.jdo.JDOFatalInternalException: Error creating transactional connection factory
NestedThrowables:
java.lang.reflect.InvocationTargetException
        at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:587)
        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:781)
        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:326)
        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:195)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)
        at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)
        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)
        at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:275)
        at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:304)
        at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:234)
        at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:209)
        at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
        at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
        at org.apache.hadoop.hive.metastore.RetryingRawStore.<init>(RetryingRawStore.java:64)
        at org.apache.hadoop.hive.metastore.RetryingRawStore.getProxy(RetryingRawStore.java:73)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:415)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:402)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:441)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:326)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.<init>(HiveMetaStore.java:286)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:54)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:59)
        at org.apache.hadoop.hive.metastore.HiveMetaStore.newHMSHandler(HiveMetaStore.java:4060)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:121)
        ... 44 more
Caused by: java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:325)
        at org.datanucleus.store.AbstractStoreManager.registerConnectionFactory(AbstractStoreManager.java:281)
        at org.datanucleus.store.AbstractStoreManager.<init>(AbstractStoreManager.java:239)
        at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:292)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)
        at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)
        at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)
        at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1069)
        at org.datanucleus.NucleusContext.initialise(NucleusContext.java:359)
        at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:768)
        ... 73 more
Caused by: java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class com.jolbox.bonecp.BoneCPDataSource
        at com.jolbox.bonecp.BoneCPDataSource.<init>(BoneCPDataSource.java:64)
        at org.datanucleus.store.rdbms.datasource.BoneCPDataSourceFactory.makePooledDataSource(BoneCPDataSourceFactory.java:73)
        at org.datanucleus.store.rdbms.ConnectionFactoryImpl.generateDataSources(ConnectionFactoryImpl.java:217)
        at org.datanucleus.store.rdbms.ConnectionFactoryImpl.initialiseDataSources(ConnectionFactoryImpl.java:110)
        at org.datanucleus.store.rdbms.ConnectionFactoryImpl.<init>(ConnectionFactoryImpl.java:82)
        ... 91 more
{noformat}
This is because started from Guava 15.0, {{com.google.common.collect.MapMaker.makeComputingMap}} is no longer public.",,apachespark,lian cheng,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 17:31:35 UTC 2015,,,,,,,,,,"0|i26bnj:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"04/Mar/15 05:19;lian cheng;Pointed out by [~pwendell], this is a Maven-vs-SBT issue. In Maven, Guava 15 is properly shaded and Guava 14.0.1 is used. However, SBT still chooses the highest version. Verified that the Maven build is not affected.;;;","04/Mar/15 07:12;pwendell;Yes - because of this I think simply excluding guava from whatever dependency is introducing Guava 15 is fine (I think it was some of the jackson dependencies). It will coerce the SBT build to match the maven behavior.;;;","04/Mar/15 07:16;pwendell;To be more specific, I am suggesting that in the pom files where we add a dependency on jackson (or whatever is recursively depending on Guava 15) we just add an exclusion of guava.;;;","04/Mar/15 13:51;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4890;;;","04/Mar/15 17:31;pwendell;Since this only affects the sbt build and not the reference build (maven), I'm de-escalating it from a blocker.;;;",,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY fails to resolve nested fields,SPARK-6145,12779213,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,marmbrus,marmbrus,03/Mar/15 22:46,31/Mar/15 18:31,14/Jul/23 06:27,31/Mar/15 18:31,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"{code}
sqlContext.jsonRDD(sc.parallelize(
  """"""{""a"": {""b"": 1}, ""c"": 1}"""""" :: Nil)).registerTempTable(""nestedOrder"")

// Works
sqlContext.sql(""SELECT 1 FROM nestedOrder ORDER BY c"")

// Fails now
sqlContext.sql(""SELECT 1 FROM nestedOrder ORDER BY a.b"")

// Fails now
sqlContext.sql(""SELECT a.b FROM nestedOrder ORDER BY a.b"")
{code}

Relatedly the error message for bad get fields should also include the name of the field in question.",,apachespark,chenghao,cloud_fan,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 18:31:39 UTC 2015,,,,,,,,,,"0|i26bev:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"04/Mar/15 15:27;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4892;;;","05/Mar/15 08:02;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4904;;;","05/Mar/15 20:21;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4918;;;","05/Mar/15 22:49;marmbrus;Issue resolved by pull request 4918
[https://github.com/apache/spark/pull/4918];;;","25/Mar/15 22:06;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5189;;;","31/Mar/15 18:31;marmbrus;Issue resolved by pull request 5189
[https://github.com/apache/spark/pull/5189];;;",,,,,,,,,,,,,,,,,,,,,,,
When in cluster mode using ADD JAR with a hdfs:// sourced jar will fail,SPARK-6144,12779210,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tleftwich,tleftwich,tleftwich,03/Mar/15 22:29,04/Mar/15 21:03,14/Jul/23 06:27,04/Mar/15 21:03,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"While in cluster mode if you use ADD JAR with a HDFS sourced jar it will fail trying to source that jar on the worker nodes with the following error:
{code}
15/03/03 04:56:50 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.io.FileNotFoundException: /yarn/nm/usercache/vagrant/appcache/application_1425166832391_0027/-19222735701425358546704_cache (No such file or directory)
        at java.io.FileInputStream.open(Native Method)
        at java.io.FileInputStream.<init>(FileInputStream.java:146)
{code}


PR https://github.com/apache/spark/pull/4880",,andrewor14,apachespark,dougb,jxiang,michaelmalak,tleftwich,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4687,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 18:52:54 UTC 2015,,,,,,,,,,"0|i26be7:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"03/Mar/15 22:40;apachespark;User 'trystanleftwich' has created a pull request for this issue:
https://github.com/apache/spark/pull/4880;;;","04/Mar/15 01:26;apachespark;User 'trystanleftwich' has created a pull request for this issue:
https://github.com/apache/spark/pull/4881;;;","04/Mar/15 04:51;andrewor14;I believe this is a regression from 1.2 caused by https://github.com/apache/spark/pull/3670.;;;","04/Mar/15 17:31;vanzin;Separately, we should check the unit tests to make sure this is properly covered. It probably missed tests because the broken path is the one that handles ""hdfs:"" urls...;;;","04/Mar/15 18:52;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4894;;;",,,,,,,,,,,,,,,,,,,,,,,,
"10-12% Performance regression with ""finalize""",SPARK-6142,12779162,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,nravi,nravi,03/Mar/15 19:52,09/Mar/15 23:04,14/Jul/23 06:27,04/Mar/15 00:23,1.3.0,,,,,,,,,,,,Spark Core,,,,0,,,,,,10-12% performance regression in PageRank (and potentially other workloads) caused due to the use of finalize in ExternalAppendOnlyMap. Introduced by a commit on Feb 19th. ,,michaelmalak,nravi,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 23:04:06 UTC 2015,,,,,,,,,,"0|i26b3z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/15 00:22;nravi;Fixed (master 9af001749a37a86ccbf78063ec514a21801645fa)   (1.3 ee4929d1d38d83382ccdc22bf99f61f24f989c8b);;;","05/Mar/15 09:05;zsxwing;[~nravi] I tested in our environment, and didn't find any performance lost with `finalize`. Did 10-12% performance regression always happen?;;;","05/Mar/15 09:17;srowen;Yeah I'm curious how this change would affect performance. Is it possible that {{cleanup()}} was not being called at all in a common case before, and now this ensures it is? is it possible it causes {{cleanup()}} to happen at a different time, which affects timing? that is I wonder if it's a good thing, or, just an artifact of what was timed.;;;","05/Mar/15 23:55;nravi;[~zsxwing]    Yes, the 10-12% regression was very consistent and is gone after the revert by Andrew. You can reproduce it with PageRank with a large enough input data set so that there is spilling to disk.
[~srowen] An object containing finalizer creates significant overhead for GC/JVM even if the finalizer method contains nothing in it. ;;;","06/Mar/15 08:59;srowen;[~nravi] what's the overhead -- do objects only go on the finalizer queue in the JVM if they override {{finalize()}}?;;;","06/Mar/15 09:35;nravi;[~srowen] GC takes at least one extra cycle for objects with finalize. There are synchronization overheads. Memory pressure increases. And then some.;;;","06/Mar/15 09:48;srowen;Yeah that makes sense. I agree that finalize() isn't something to generally implement anyway, yes.;;;","09/Mar/15 23:04;srowen;Is this resolved by reverting those commits then?;;;",,,,,,,,,,,,,,,,,,,,,
Upgrade Breeze to 0.11 to fix convergence bug,SPARK-6141,12779152,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dbtsai,dbtsai,dbtsai,03/Mar/15 19:15,06/Mar/15 02:56,14/Jul/23 06:27,04/Mar/15 07:52,,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"LBFGS and OWLQN in Breeze 0.10 has convergence check bug. This is fixed in 0.11, see the description in Breeze project for detail:

https://github.com/scalanlp/breeze/pull/373#issuecomment-76879760",,apachespark,dbtsai,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 07:52:21 UTC 2015,,,,,,,,,,"0|i26b1r:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"03/Mar/15 19:18;apachespark;User 'dbtsai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4879;;;","04/Mar/15 07:52;mengxr;Issue resolved by pull request 4879
[https://github.com/apache/spark/pull/4879];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Docker client library introduces Guava 17.0, which causes runtime binary incompatibilities",SPARK-6136,12778987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,03/Mar/15 09:06,11/Aug/15 10:04,14/Jul/23 06:27,04/Mar/15 11:41,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Integration test suites in the JDBC data source ({{MySQLIntegration}} and {{PostgresIntegration}}) depend on docker-client 2.7.5, which transitively depends on Guava 17.0. Unfortunately, Guava 17.0 is causing runtime binary incompatibility issues when Spark is compiled against Hadoop 2.4.
{code}
$ ./build/sbt -Pyarn,hadoop-2.4,hive,hive-0.12.0,scala-2.10 -Dhadoop.version=2.4.1
...
> sql/test-only *.ParquetDataSourceOffIOSuite
...
[info] ParquetDataSourceOffIOSuite:
[info] Exception encountered when attempting to run a suite with class name: org.apache.spark.sql.parquet.ParquetDataSourceOffIOSuite *** ABORTED *** (134 milliseconds)
[info]   java.lang.IllegalAccessError: tried to access method com.google.common.base.Stopwatch.<init>()V from class org.apache.hadoop.mapreduce.lib.input.FileInputFormat
[info]   at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:261)
[info]   at parquet.hadoop.ParquetInputFormat.listStatus(ParquetInputFormat.java:277)
[info]   at org.apache.spark.sql.parquet.FilteringParquetRowInputFormat.getSplits(ParquetTableOperations.scala:437)
[info]   at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:95)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
[info]   at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
[info]   at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:1525)
[info]   at org.apache.spark.rdd.RDD.collect(RDD.scala:813)
[info]   at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:83)
[info]   at org.apache.spark.sql.DataFrame.collect(DataFrame.scala:797)
[info]   at org.apache.spark.sql.QueryTest$.checkAnswer(QueryTest.scala:115)
[info]   at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:60)
[info]   at org.apache.spark.sql.parquet.ParquetIOSuiteBase$$anonfun$checkParquetFile$1.apply(ParquetIOSuite.scala:76)
[info]   at org.apache.spark.sql.parquet.ParquetIOSuiteBase$$anonfun$checkParquetFile$1.apply(ParquetIOSuite.scala:76)
[info]   at org.apache.spark.sql.parquet.ParquetTest$$anonfun$withParquetDataFrame$1.apply(ParquetTest.scala:105)
[info]   at org.apache.spark.sql.parquet.ParquetTest$$anonfun$withParquetDataFrame$1.apply(ParquetTest.scala:105)
[info]   at org.apache.spark.sql.parquet.ParquetTest$$anonfun$withParquetFile$1.apply(ParquetTest.scala:94)
[info]   at org.apache.spark.sql.parquet.ParquetTest$$anonfun$withParquetFile$1.apply(ParquetTest.scala:92)
[info]   at org.apache.spark.sql.parquet.ParquetTest$class.withTempPath(ParquetTest.scala:71)
[info]   at org.apache.spark.sql.parquet.ParquetIOSuiteBase.withTempPath(ParquetIOSuite.scala:67)
[info]   at org.apache.spark.sql.parquet.ParquetTest$class.withParquetFile(ParquetTest.scala:92)
[info]   at org.apache.spark.sql.parquet.ParquetIOSuiteBase.withParquetFile(ParquetIOSuite.scala:67)
[info]   at org.apache.spark.sql.parquet.ParquetTest$class.withParquetDataFrame(ParquetTest.scala:105)
[info]   at org.apache.spark.sql.parquet.ParquetIOSuiteBase.withParquetDataFrame(ParquetIOSuite.scala:67)
[info]   at org.apache.spark.sql.parquet.ParquetIOSuiteBase.checkParquetFile(ParquetIOSuite.scala:76)
[info]   at org.apache.spark.sql.parquet.ParquetIOSuiteBase$$anonfun$1.apply$mcV$sp(ParquetIOSuite.scala:83)
[info]   at org.apache.spark.sql.parquet.ParquetIOSuiteBase$$anonfun$1.apply(ParquetIOSuite.scala:79)
[info]   at org.apache.spark.sql.parquet.ParquetIOSuiteBase$$anonfun$1.apply(ParquetIOSuite.scala:79)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.apache.spark.sql.parquet.ParquetDataSourceOffIOSuite.org$scalatest$BeforeAndAfterAll$$super$run(ParquetIOSuite.scala:346)
[info]   at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
[info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
[info]   at org.apache.spark.sql.parquet.ParquetDataSourceOffIOSuite.run(ParquetIOSuite.scala:346)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}
This is because the default constructor of {{Stopwatch}} is no longer public in Guava 17.0.

Compiling Spark against Hive 0.12.0 also introduces other types of runtime binary incompatibility issue.

Considering {{MySQLIntegration}} and {{PostgresIntegration}} are ignored right now, I'd suggest moving them from the Spark project to the [Spark integration tests||https://github.com/databricks/spark-integration-tests] project.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Aug 11 10:04:08 UTC 2015,,,,,,,,,,"0|i26a27:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"03/Mar/15 09:19;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4872;;;","11/Aug/15 10:04;apachespark;User 'yjshen' has created a pull request for this issue:
https://github.com/apache/spark/pull/8101;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix wrong datatype for casting FloatType and default LongType value in defaultPrimitive,SPARK-6134,12778974,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,03/Mar/15 08:03,24/Apr/15 00:40,14/Jul/23 06:27,04/Mar/15 12:27,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"In CodeGenerator, the casting on FloatType should use FloatType instead of IntegerType.

Besides, defaultPrimitive for LongType should be -1L instead of 1L.
",,apachespark,lian cheng,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 12:27:09 UTC 2015,,,,,,,,,,"0|i269zj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/15 08:05;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/4870;;;","04/Mar/15 12:27;lian cheng;Issue resolved by pull request 4870
[https://github.com/apache/spark/pull/4870];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkContext#stop is not idempotent,SPARK-6133,12778961,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,03/Mar/15 06:24,13/Mar/15 18:34,14/Jul/23 06:27,13/Mar/15 18:33,1.0.0,,,,,,1.2.2,1.3.1,1.4.0,,,,Spark Core,,,,0,,,,,,"If we call sc.stop() twice, the listener bus will attempt to log an event after stop() is called, resulting in a scary error message. This happens if Spark calls sc.stop() internally (it does this on certain error conditions) and the application code calls it again, for instance.",,andrewor14,apachespark,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 09:04:55 UTC 2015,,,,,,,,,,"0|i269wn:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.1,1.4.0,,,,,,,,,,,"03/Mar/15 09:04;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4871;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Context cleaner race condition across SparkContexts,SPARK-6132,12778960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,03/Mar/15 06:21,26/Mar/15 00:47,14/Jul/23 06:27,22/Mar/15 13:09,1.0.0,,,,,,1.1.2,1.2.2,1.3.1,1.4.0,,,Spark Core,,,,0,,,,,,"The context cleaner thread is not stopped properly. If a SparkContext is started immediately after one stops, the context cleaner of the former can clean variables in the latter.

This is because the cleaner.stop() just sets a flag and expects the thread to terminate asynchronously, but the code to clean broadcasts goes through `SparkEnv.get.blockManager`, which could belong to a different SparkContext. This is likely to be the cause of the `JavaAPISuite`, which creates many back-to-back SparkContexts, being flaky.

The right behavior is to wait until all currently running clean up tasks have finished.
{code}
java.io.IOException: org.apache.spark.SparkException: Failed to get broadcast_0_piece0 of broadcast_0
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1180)
        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:164)
        at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
        at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
        ...
Caused by: org.apache.spark.SparkException: Failed to get broadcast_0_piece0 of broadcast_0
        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:137)
        at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$2.apply(TorrentBroadcast.scala:137)
        at scala.Option.getOrElse(Option.scala:120)
{code}",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5812,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 26 00:47:13 UTC 2015,,,,,,,,,,"0|i269wf:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.2,1.3.1,1.4.0,,,,,,,,,,"03/Mar/15 06:48;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4869;;;","04/Mar/15 02:18;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4882;;;","13/Mar/15 18:31;srowen;Also in 1.3.1 now; Andrew mentioned he'd like to back-port after seeing how this goes in master a while longer.;;;","26/Mar/15 00:47;andrewor14;Looks like this is back ported in all target branches now. Thanks [~srowen].;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Kafka API not visible in Python API docs,SPARK-6127,12778927,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,03/Mar/15 01:06,03/Mar/15 02:41,14/Jul/23 06:27,03/Mar/15 02:41,,,,,,,1.3.0,,,,,,,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 01:09:46 UTC 2015,,,,,,,,,,"0|i269pb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"03/Mar/15 01:09;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/4860;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet reader should use the schema of every file to create converter,SPARK-6123,12778907,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,yhuai,yhuai,02/Mar/15 23:37,09/Jul/15 11:36,14/Jul/23 06:27,08/Jul/15 22:51,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"For two parquet files for the same table having an array column, if values of the array in one file was created when containsNull was true and those in another file was created when containsNull was false, the containsNull in the merged schema will be true and we cannot correctly read data from the table created with containsNull=false. ",,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 22:51:25 UTC 2015,,,,,,,,,,"0|i269l3:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"03/Mar/15 01:40;yhuai;To workaround this issue, users need to load the existing data (the one with containsNull=false) and write the data to a new dir.;;;","06/Jul/15 19:06;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/7231;;;","08/Jul/15 22:51;lian cheng;Issue resolved by pull request 7231
[https://github.com/apache/spark/pull/7231];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Python DataFrame type inference for LabeledPoint gets wrong type,SPARK-6121,12778887,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,josephkb,josephkb,02/Mar/15 22:35,03/Mar/15 01:14,14/Jul/23 06:27,03/Mar/15 01:14,1.3.0,,,,,,1.3.0,,,,,,MLlib,PySpark,SQL,,0,,,,,,"In Pyspark, when an RDD of LabeledPoints is converted to a DataFrame using toDF(), the returned DataFrame has type ""null"" instead of VectorUDT.

To reproduce:
{code}
from pyspark.mllib.util import MLUtils
rdd = MLUtils.loadLibSVMFile(sc, ""data/mllib/sample_libsvm_data.txt"")
df = rdd.toDF()
{code}

Examine rdd and df to see:
{code}
>>> df
DataFrame[features: null, label: double]
{code}
",,apachespark,josephkb,mengxr,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 01:14:51 UTC 2015,,,,,,,,,,"0|i269gv:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"02/Mar/15 22:59;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4858;;;","03/Mar/15 01:14;mengxr;Issue resolved by pull request 4858
[https://github.com/apache/spark/pull/4858];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
DecisionTree.save uses too much Java heap space for default spark shell settings,SPARK-6120,12778884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,02/Mar/15 22:27,03/Mar/15 09:07,14/Jul/23 06:27,03/Mar/15 06:35,1.3.0,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"When the Python DecisionTree example in the programming guide is run, it runs out of Java Heap Space:

{code}
scala> model.save(sc, ""myModelPath"")
[Stage 12:>                                                                                                                                        (0 + 8) / 8]15/03/02 14:19:16 ERROR Executor: Exception in task 1.0 in stage 12.0 (TID 22)
java.lang.OutOfMemoryError: Java heap space
	at parquet.bytes.CapacityByteArrayOutputStream.initSlabs(CapacityByteArrayOutputStream.java:65)
	at parquet.bytes.CapacityByteArrayOutputStream.<init>(CapacityByteArrayOutputStream.java:57)
	at parquet.column.values.plain.PlainValuesWriter.<init>(PlainValuesWriter.java:45)
	at parquet.column.values.dictionary.DictionaryValuesWriter.<init>(DictionaryValuesWriter.java:102)
	at parquet.column.values.dictionary.DictionaryValuesWriter$PlainDoubleDictionaryValuesWriter.<init>(DictionaryValuesWriter.java:471)
	at parquet.column.ParquetProperties.getValuesWriter(ParquetProperties.java:111)
	at parquet.column.impl.ColumnWriterImpl.<init>(ColumnWriterImpl.java:74)
	at parquet.column.impl.ColumnWriteStoreImpl.newMemColumn(ColumnWriteStoreImpl.java:68)
	at parquet.column.impl.ColumnWriteStoreImpl.getColumnWriter(ColumnWriteStoreImpl.java:56)
	at parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.<init>(MessageColumnIO.java:178)
	at parquet.io.MessageColumnIO.getRecordWriter(MessageColumnIO.java:369)
	at parquet.hadoop.InternalParquetRecordWriter.initStore(InternalParquetRecordWriter.java:108)
	at parquet.hadoop.InternalParquetRecordWriter.<init>(InternalParquetRecordWriter.java:94)
	at parquet.hadoop.ParquetRecordWriter.<init>(ParquetRecordWriter.java:64)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:282)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:252)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:620)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:641)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:641)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

When saving using JSON format instead of Parquet, this works.  It seems to be caused by Parquet requiring a lot of metadata to describe the schema.

I'm labeling this a bug since it should succeed with the default spark-shell settings.  Potential fixes are:
* increasing spark-shell default heap space settings (This is probably too hard to agree on currently.)
* not using Parquet for storage (This would be good for small examples but probably worse for large models, where Parquet would be more efficient than other formats.)
* compressing the schema (The various values in the DecisionTree model could be flattened into a single Seq of Double.  This may be the best option for now.)

Notes:
* This happens in both pyspark and Scala shells.
* Increasing driver memory to 1g (from the default of 512m) makes this succeed.
* Running other examples such as NaiveBayes with the default of 512m works.
* This is a bit strange in that the actual size of the saved model on disk is small (86K on disk for me).
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3071,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 06:35:37 UTC 2015,,,,,,,,,,"0|i269gf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"03/Mar/15 01:20;josephkb;I checked, and this only happens in save(), not in load().

Increasing a tiny bit to 700m works.;;;","03/Mar/15 01:22;josephkb;This also affects tree ensembles, of course.  (verified)

700m works for ensembles too, but 512m does not.;;;","03/Mar/15 01:26;josephkb;Rather than one of the difficult options above, I'm sending a PR which will simply print a warning if the memory might be too low.;;;","03/Mar/15 02:09;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/4864;;;","03/Mar/15 06:35;mengxr;Issue resolved by pull request 4864
[https://github.com/apache/spark/pull/4864];;;",,,,,,,,,,,,,,,,,,,,,,,,
Explode on nested field fails,SPARK-6114,12778852,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,02/Mar/15 20:58,03/Mar/15 00:12,14/Jul/23 06:27,03/Mar/15 00:12,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 00:12:13 UTC 2015,,,,,,,,,,"0|i269a7:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"02/Mar/15 21:01;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4855;;;","03/Mar/15 00:12;marmbrus;Issue resolved by pull request 4855
[https://github.com/apache/spark/pull/4855];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
event log file ends with .inprogress should be able to display on webUI for standalone mode,SPARK-6107,12778699,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liyezhang556520,liyezhang556520,liyezhang556520,02/Mar/15 07:35,24/Jun/19 09:27,14/Jul/23 06:27,04/Mar/15 12:28,1.2.1,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"when application is finished running abnormally (Ctrl + c for example), the history event log file is still ends with *.inprogress* suffix. And the application state can not be showed on webUI, User can just see ""*Application history not foud xxxx, Application xxx is still in progress*"".  
User should also can see the status of the abnormal finished applications.",,apachespark,eranation,glenn.strycker@gmail.com,krdeepak,leo.zhi,liyezhang556520,octavian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8143,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jun 24 09:21:18 UTC 2019,,,,,,,,,,"0|i268db:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/15 08:01;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/4848;;;","04/Mar/15 12:28;srowen;Issue resolved by pull request 4848
[https://github.com/apache/spark/pull/4848];;;","05/Jun/15 13:52;octavian;I still see this in 1.3.1 . I have spark.eventLog.enabled  true and spark.eventLog.dir set.;;;","06/Jun/15 02:54;liyezhang556520;[~octavian], this is not in 1.3.1, it is fixed in 1.4.0;;;","05/Aug/15 08:26;krdeepak;Is there a plan to fix it in 1.3.1;;;","24/Jun/19 09:21;leo.zhi;in version 2.2.0-cdh6.0.1 and yarn cluster, it still happens.:(;;;",,,,,,,,,,,,,,,,,,,,,,,
UI is malformed when tasks fetch remote results,SPARK-6088,12778535,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,kayousterhout,kayousterhout,01/Mar/15 02:20,24/Mar/15 23:27,14/Jul/23 06:27,24/Mar/15 23:27,1.3.0,,,,,,1.3.1,1.4.0,,,,,Web UI,,,,0,,,,,,"There are three issues when tasks get remote results:

(1) The status never changes from GET_RESULT to SUCCEEDED
(2) The time to get the result is shown as the absolute time (resulting in a non-sensical output that says getting the result took >1 million hours) rather than the elapsed time
(3) The getting result time is included as part of the scheduler delay

cc [~shivaram]",,apachespark,kayousterhout,shivaram,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/15 02:25;shivaram;Screenshot 2015-02-28 18.24.42.png;https://issues.apache.org/jira/secure/attachment/12701659/Screenshot+2015-02-28+18.24.42.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 01 02:28:45 UTC 2015,,,,,,,,,,"0|i267d3:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"01/Mar/15 02:26;shivaram;Also for some reason the get result time is also included in the Scheduler Delay. Screen shot attached shows how the get result took 33 mins and how this shows up in scheduler delay.;;;","01/Mar/15 02:28;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/4839;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQL should fail gracefully when input data format doesn't match expectations,SPARK-6082,12778505,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,kayousterhout,kayousterhout,28/Feb/15 19:13,24/Apr/15 00:41,14/Jul/23 06:27,03/Mar/15 00:18,1.2.1,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"I have a udf that creates a tab-delimited table. If any of the column values contain a tab, SQL fails with an ArrayIndexOutOfBounds exception (pasted below).  It would be great if SQL failed gracefully here, with a helpful exception (something like ""One row contained too many values"").

It looks like this can be done quite easily, by checking here if i > columnBuilders.size and if so, throwing a nicer exception: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/columnar/InMemoryColumnarTableScan.scala#L124.

One thing that makes this problem especially annoying to debug is because if you do ""CREATE table foo as select transform(..."" and then ""CACHE table foo"", it works fine.  It only fails if you do ""CACHE table foo as select transform(..."".  Because of this, it would be great if the problem were more transparent to users.

Stack trace:
java.lang.ArrayIndexOutOfBoundsException: 3
  at org.apache.spark.sql.columnar.InMemoryRelation$anonfun$3$anon$1.next(InMemoryColumnarTableScan.scala:125)
  at org.apache.spark.sql.columnar.InMemoryRelation$anonfun$3$anon$1.next(InMemoryColumnarTableScan.scala:112)
  at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
  at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:163)
  at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:245)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:280)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:247)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
  at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
  at org.apache.spark.scheduler.Task.run(Task.scala:56)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:220)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)",,apachespark,kayousterhout,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 00:18:25 UTC 2015,,,,,,,,,,"0|i2676f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/15 09:38;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4842;;;","03/Mar/15 00:18;marmbrus;Issue resolved by pull request 4842
[https://github.com/apache/spark/pull/4842];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"LogisticRegressionWithLBFGS in PySpark was assigned wrong ""regType"" parameter",SPARK-6080,12778457,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,28/Feb/15 08:34,02/Mar/15 18:17,14/Jul/23 06:27,02/Mar/15 18:17,,,,,,,1.3.0,,,,,,MLlib,PySpark,,,0,,,,,,"Currently LogisticRegressionWithLBFGS in python/pyspark/mllib/classification.py will invoke callMLlibFunc with a wrong ""regType"" parameter.",,apachespark,josephkb,mengxr,tgraves,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 18:17:40 UTC 2015,,,,,,,,,,"0|i266vz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"28/Feb/15 08:35;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4831;;;","28/Feb/15 08:47;yanboliang;This bug is easy to reproduce. In a PySpark environment, when you call
    {code} model = LogisticRegressionWithSGD.train(data, regType=None)  {code} 
it will return a LogisticRegressionModel which was trained with no regularization.

But when you run 
    {code} model = LogisticRegressionWithLBFGS.train(data, regType=None)  {code}
it will throw an exception
    {code} java.lang.IllegalArgumentException: Invalid value for 'regType' parameter. Can only be initialized using the following string values: ['l1', 'l2', None]. {code}

This is due to when invoke callMLlibFunc of LogisticRegressionWithLBFGS at python/pyspark/mllib/classification.py, the parameter was assigned to ""str(regType)"" which translate None(Python) to ""None""(Java/Scala). The right way should be translate None(Python) to null(Java/Scala). We need to do the same thing as LogisticRegressionWithSGD. ;;;","02/Mar/15 18:17;mengxr;Issue resolved by pull request 4831
[https://github.com/apache/spark/pull/4831];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Multiple spark streaming tabs on UI when reuse the same sparkcontext,SPARK-6077,12778447,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhichao-li,zhichao-li,zhichao-li,28/Feb/15 06:51,24/Apr/15 00:42,14/Jul/23 06:27,16/Mar/15 20:12,,,,,,,1.3.1,1.4.0,,,,,DStreams,Web UI,,,0,,,,,,"Currently we would create a new streaming tab for each streamingContext even if there's already one on the same sparkContext which would cause duplicate StreamingTab created and none of them is taking effect. 

snapshot: https://www.dropbox.com/s/t4gd6hqyqo0nivz/bad%20multiple%20streamings.png?dl=0

How to reproduce:
1)
import org.apache.spark.SparkConf
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.storage.StorageLevel

val ssc = new StreamingContext(sc, Seconds(1))
val lines = ssc.socketTextStream(""localhost"", 9999, StorageLevel.MEMORY_AND_DISK_SER)
val words = lines.flatMap(_.split("" ""))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()
.....

2)
ssc.stop(false)
val ssc = new StreamingContext(sc, Seconds(1))
val lines = ssc.socketTextStream(""localhost"", 9999, StorageLevel.MEMORY_AND_DISK_SER)
val words = lines.flatMap(_.split("" ""))
val wordCounts = words.map(x => (x, 1)).reduceByKey(_ + _)
wordCounts.print()
ssc.start()",,apachespark,nchammas,zhichao-li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2463,,,,,,,,,SPARK-2463,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 04:15:18 UTC 2015,,,,,,,,,,"0|i266tr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/15 06:53;apachespark;User 'zhichao-li' has created a pull request for this issue:
https://github.com/apache/spark/pull/4828;;;","28/Feb/15 15:04;srowen;Would this fix SPARK-2463? That one seems like a specific case of this issue, even.;;;","02/Mar/15 01:34;zhichao-li;Yeah. It would fix the SPARK-2463 as well. Almost the same case, although most of the comments on that jira is about stopping concurrently running StreamingContexts in the same JVM;;;","02/Mar/15 04:15;nchammas;Please disregard the comments on SPARK-2463 and focus on the description. The comments veer off into a separate issue from the one put forward in the description.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Fix a potential OOM issue when StorageLevel is MEMORY_AND_DISK_SER,SPARK-6076,12778446,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,28/Feb/15 06:49,17/May/20 18:20,14/Jul/23 06:27,01/May/15 00:38,1.2.1,,,,,,1.4.0,,,,,,Block Manager,Spark Core,,,0,,,,,,"When StorageLevel is MEMORY_AND_DISK_SER, it will copy the content from file into memory, then put it into MemoryStore. See: https://github.com/apache/spark/blob/dcd1e42d6b6ac08d2c0736bf61a15f515a1f222b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L538

However, if the file is bigger than the free memory, OOM will happen. A better approach is testing if there is enough memory. If not, copyForMemory should not be created, since this is an option operation.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 28 06:51:32 UTC 2015,,,,,,,,,,"0|i266tj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/15 06:51;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/4827;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After SPARK-3885, some tasks' accumulator updates may be lost",SPARK-6075,12778444,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,28/Feb/15 05:12,01/Mar/15 06:53,14/Jul/23 06:27,01/Mar/15 06:53,1.4.0,,,,,,1.4.0,,,,,,Spark Core,Tests,,,0,,,,,,"It looks like some of the AccumulatorSuite tests have started failing nondeterministically on Jenkins.  The errors seem to be due to lost / missing accumulator updates, e.g.

{code}
Set(843, 356, 437, [...], 181, 618, 131) did not contain element 901
{code}

This could somehow be related to SPARK-3885 / https://github.com/apache/spark/pull/4021, a patch to garbage-collect accumulators, which was only merged into master.

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-SBT/lastCompletedBuild/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.0,label=centos/testReport/org.apache.spark/AccumulatorSuite/add_value_to_collection_accumulators/

I think I've figured it out: consider the lifecycle of an accumulator in a task, say ShuffleMapTask: on the executor, each task deserializes its own copy of the RDD inside of its runTask method, so the strong reference to the RDD disappears at the end of runTask. In Executor.run(), we call Accumulators.values after runTask has exited, so there's a small window in which the tasks's RDD can be GC'd, causing accumulators to be GC'd as well because there are no longer any strong references to them.

The fix is to keep strong references in localAccums, since we clear this at the end of each task anyways. I'm glad that I was able to figure out precisely why this was necessary and sorry that I missed this during review; I'll submit a fix shortly. In terms of preventative measures, it might be a good idea to write up the lifetime / lifecycle of objects' strong references whenever we're using WeakReferences, since the process of explicitly writing that out would prevent these sorts of mistakes in the future.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 01 06:53:13 UTC 2015,,,,,,,,,,"0|i266t3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/15 20:25;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4835;;;","01/Mar/15 06:53;joshrosen;Issue resolved by pull request 4835
[https://github.com/apache/spark/pull/4835];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Assembly doesn't include pyspark sql files,SPARK-6074,12778434,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,vanzin,vanzin,28/Feb/15 02:45,01/Mar/15 11:11,14/Jul/23 06:27,01/Mar/15 11:05,,,,,,,1.3.0,,,,,,PySpark,SQL,,,0,,,,,,The sql/core/pom.xml file is missing code to package pyspark-related files into the final assembly. This prevents the sql pyspark module from working on Yarn.,,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 01 11:11:52 UTC 2015,,,,,,,,,,"0|i266qv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/15 02:46;vanzin;Testing a fix, will send PR shortly. I'm out during the weekend so please take this over if there are changes needed.;;;","28/Feb/15 02:47;vanzin;[~tgraves] this might be what you guys mentioned on the mailing list.;;;","28/Feb/15 02:56;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4822;;;","01/Mar/15 11:05;srowen;Issue resolved by pull request 4822
[https://github.com/apache/spark/pull/4822];;;","01/Mar/15 11:11;srowen;(These pyspark/sql files didn't exist in 1.2);;;",,,,,,,,,,,,,,,,,,,,,,,,
Need to refresh metastore cache after append data in CreateMetastoreDataSourceAsSelect,SPARK-6073,12778420,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,28/Feb/15 01:42,24/Apr/15 00:42,14/Jul/23 06:27,02/Mar/15 14:45,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"We should drop the metadata cache in CreateMetastoreDataSourceAsSelect after we append data. Otherwise, users have to manually call HiveContext.refreshTable to drop the cached metadata entry from the catalog.",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 28 04:11:00 UTC 2015,,,,,,,,,,"0|i266nr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"28/Feb/15 04:11;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4824;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Yarn Shuffle Service jar packages too many dependencies,SPARK-6070,12778362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,vanzin,vanzin,27/Feb/15 23:05,28/Feb/15 06:44,14/Jul/23 06:27,28/Feb/15 06:44,1.3.0,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"The spark-network-yarn_2.10 assembly is currently packaging too many dependencies. If you look at the generated jar, it contains lots of hadoop classes; if you deploy that jar into an existing Yarn service, and they don't perfectly match, you may end up breaking the NM.

Working on a fix.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 27 23:12:59 UTC 2015,,,,,,,,,,"0|i266bb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"27/Feb/15 23:12;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4820;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metadata in event log makes it very difficult for external libraries to parse event log,SPARK-6066,12778304,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,kayousterhout,kayousterhout,27/Feb/15 20:10,03/Mar/15 00:34,14/Jul/23 06:27,03/Mar/15 00:34,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"The fix for SPARK-2261 added a line at the beginning of the event log that encodes metadata.  This line makes it much more difficult to parse the event logs from external libraries (like https://github.com/kayousterhout/trace-analysis, which is used by folks at Berkeley) because:

(1) The metadata is not written as JSON, unlike the rest of the file
(2) More annoyingly, if the file is compressed, the metadata is not compressed.  This has a few side-effects: first, someone can't just use the command line to uncompress the file and then look at the logs, because the file is in this weird half-compressed format; and second, now external tools that parse these logs also need to deal with this weird format.

We should fix this before the 1.3 release, because otherwise we'll have to add a bunch more backward-compatibility code to handle this weird format!",,apachespark,kayousterhout,pwendell,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 00:34:59 UTC 2015,,,,,,,,,,"0|i265zr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"27/Feb/15 21:55;pwendell;What if as a simple fix we do these things:

1. Put the compression information in the filename. For instance, can we just use an extension indicating the compression format? That's a very common convention.
2. Have the meta-data header use the same compression options as the rest of the file.
3. Have the meta-data header use a single-line JSON dictionary instead of having a second type of format that is not JSON.

I think this would solve the main inter-op problem with other people trying to read these logs. I do think we should try, within reason, to make these easy for third parties to process without a lot of extra effort. It's a big win because it allows researchers, etc to run fairly fine grained analysis of Spark's behavior.

[~vanzin] any thoughts here?;;;","27/Feb/15 22:03;vanzin;I think the correct option if third parties want to read these files is to add a library that allows them to do so. For example, you don't try to read a Snappy file with FileInputStream directly, you use Snappy's library for that, since it handles the header that contains information about how to uncompress the data and the actual decompression.

That is similar to this case. There's a header with the metadata about the file contents, just like before there were a bunch of files in a directory that served as that metadata. The only sub-optimal part is that we don't have a public library to read it.

The extension can help a little bit; it would still require the user to understand the extension-to-codec mapping, and then the contents of the header and what they mean.;;;","27/Feb/15 22:28;vanzin;BTW, quick command line-fu for the interested:

{code}
more +/""=== LOG_HEADER_END ==="" logfile | tail -n +1
{code}
;;;","27/Feb/15 22:38;kayousterhout;[~vanzin] what's wrong with the solution Patrick proposed? I think that makes it easy for external users to parse these files, without really any cost.;;;","27/Feb/15 22:42;vanzin;Nothing wrong, I just don't see how it's much better. The user trying to read it externally still needs to know that if there is a certain extension he needs to use a particular compression codec. And he still needs to understand that the first line, even though it's JSON, is not actually an event, but a header, and needs to understand the contents of that header. (Right now I don't think there's anything particularly interesting there, but at some point there might - e.g. the Spark version might become important to help understand the rest of the file.)

A library would make all that transparent to this user. Basically something like ""java.util.zip.ZipFile"", where instead of bytes you have a collection of ""ZipEntries"" (here you'd have a collection of ""SparkListenerEvent"").

No strong opinion one way or another, I just thing the library is nicer for the end user and more flexible in the long run.;;;","27/Feb/15 23:15;vanzin;BTW said library could also contain logic to handle old, directory-based event logs, so even less things for the users to care about.;;;","27/Feb/15 23:24;pwendell;Hey Marcelo,

I agree having a public library for reading these logs would be nice, especially if we decide to do fancier things with the way the logs are encoded in the future.

As it stands today though, it is nice that logs can be read by third party applications, even those that are not JVM based (Kay's app is actually written in Python), because we are using a very standard serialization format (JSON) and a very widely supported compression library (GZIP, IIRC?). I think we'll get benefit from this as long as we can support such widely used formats. This is especially important since Spark does not expose a library for importing these things today.

So making a minor change that allows these logs to be read by many third party systems, that seems worth it to me.

;;;","27/Feb/15 23:27;vanzin;We only use codecs supported by Spark: snappy, lzo, lzf. Can raw python read those?

Changing the header to be a single-line JSON thing is probably a very small change. ;;;","27/Feb/15 23:39;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4821;;;","01/Mar/15 01:41;pwendell;[~vanzin] - yes you are right (an early scratch version of the feature used a Gzip stream, I think). There are python bindings for all three of those compression codecs. To be fair, I'm not 100% sure the codecs are standardized enough to be compatible across different implementations. Gzip is pretty good in this regard, but not sure about those other three.;;;","03/Mar/15 00:34;pwendell;Thanks Andrew and Marcelo for your work on this patch.;;;",,,,,,,,,,,,,,,,,,
MLlib doesn't pass mvn scalastyle check due to UTF chars in LDAModel.scala,SPARK-6063,12778264,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,michael.griffiths,michael.griffiths,michael.griffiths,27/Feb/15 18:04,25/Mar/15 22:08,14/Jul/23 06:27,28/Feb/15 14:49,1.3.0,,,,,,1.3.1,1.4.0,,,,,Build,Windows,,,0,,,,,,"On Windows 8.1, trying to build Spark from source (latest Github pull) produces a failure during MLlib build. The cause appears similar to SPARK-3372: unicode characters in a Scala file.

*Affected file*
spark/mllib/src/main/scala/org/apache/spark/mllib/clustering
LDAModel.scala:133
{code}
   * This is often called “theta” in the literature.
{code}
Note the two \u201D marks around *theta*.

Replacing the \u201D with a standard single apostrophe (\x27) resolves the issue.",,apachespark,josephkb,michael.griffiths,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6532,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 17:32:03 UTC 2015,,,,,,,,,,"0|i265qv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/15 18:27;apachespark;User 'msjgriffiths' has created a pull request for this issue:
https://github.com/apache/spark/pull/4815;;;","28/Feb/15 14:47;srowen;This seems to only affect Windows, and does not fail scalastyle on OS X / Linux, hence downgrading it significantly.;;;","28/Feb/15 14:49;srowen;Issue resolved by pull request 4815
[https://github.com/apache/spark/pull/4815];;;","25/Mar/15 17:32;srowen;Back ported for 1.3.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add volatile to ApplicationMaster.reporterThread and ApplicationMaster.allocator,SPARK-6059,12778171,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,27/Feb/15 09:53,27/Feb/15 13:34,14/Jul/23 06:27,27/Feb/15 13:33,1.2.1,,,,,,1.4.0,,,,,,YARN,,,,0,,,,,,"ApplicationMaster.reporterThread and ApplicationMaster.allocator are accessed in multiple threads, so they should be marked as volatile.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 27 13:33:54 UTC 2015,,,,,,,,,,"0|i2654f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/15 09:56;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/4814;;;","27/Feb/15 13:33;srowen;Issue resolved by pull request 4814
[https://github.com/apache/spark/pull/4814];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak in pyspark sql due to incorrect equality check,SPARK-6055,12778147,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,27/Feb/15 07:28,28/Feb/15 04:09,14/Jul/23 06:27,28/Feb/15 04:09,1.1.1,1.2.1,1.3.0,,,,1.1.2,1.2.2,1.3.0,,,,PySpark,SQL,,,0,,,,,,"The __eq__ of DataType is not correct, class cache is not used correctly (created class can not be find by dataType), then it will create lots of classes (saved in _cached_cls), never released.

Also, all same DataType have same hash code, there will be many object in a dict with the same hash code, end with hash attach, it's very slow to access this dict (depends on the implementation of CPython).",,apachespark,davies,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 27 07:51:12 UTC 2015,,,,,,,,,,"0|i264z3:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.2,1.3.0,,,,,,,,,,,"27/Feb/15 07:32;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4808;;;","27/Feb/15 07:41;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4809;;;","27/Feb/15 07:51;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4810;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"SQL UDF returning object of case class; regression from 1.2.0",SPARK-6054,12778146,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,spirom,spirom,27/Feb/15 07:28,24/Mar/15 19:29,14/Jul/23 06:27,24/Mar/15 19:29,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"The following code fails with a stack trace beginning with:

{code}
15/02/26 23:21:32 ERROR Executor: Exception in task 2.0 in stage 7.0 (TID 422)
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: makeCopy, tree: scalaUDF(sales#2,discounts#3)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:47)
	at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:309)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:237)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:192)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:207)
{code}

Here is the 1.3.0 version of the code:
{code}
case class SalesDisc(sales: Double, discounts: Double)
def makeStruct(sales: Double, disc:Double) = SalesDisc(sales, disc)
sqlContext.udf.register(""makeStruct"", makeStruct _)
val withStruct =
      sqlContext.sql(""SELECT id, sd.sales FROM (SELECT id, makeStruct(sales, discounts) AS sd FROM customerTable) AS d"")
    withStruct.foreach(println)
{code}

This used to work in 1.2.0. Interestingly, the following simplified version fails similarly, even though it seems to me to be VERY similar to the last test in the UDFSuite:

{code}
SELECT makeStruct(sales, discounts) AS sd FROM customerTable
{code}
The data table is defined thus:
{code}
  val custs = Seq(
      Cust(1, ""Widget Co"", 120000.00, 0.00, ""AZ""),
      Cust(2, ""Acme Widgets"", 410500.00, 500.00, ""CA""),
      Cust(3, ""Widgetry"", 410500.00, 200.00, ""CA""),
      Cust(4, ""Widgets R Us"", 410500.00, 0.0, ""CA""),
      Cust(5, ""Ye Olde Widgete"", 500.00, 0.0, ""MA"")
    )
    val customerTable = sc.parallelize(custs, 4).toDF()

    customerTable.registerTempTable(""customerTable"")
{code}","Windows 8, Scala 2.11.2, Spark 1.3.0 RC1",apachespark,spirom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 24 06:59:20 UTC 2015,,,,,,,,,,"0|i264yv:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"24/Mar/15 06:59;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5157;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In JSON schema inference, we should always set containsNull of an ArrayType to true",SPARK-6052,12778131,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,27/Feb/15 05:20,24/Apr/15 00:42,14/Jul/23 06:27,02/Mar/15 15:19,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,We should not try to figure out if an array contains null or not because we may miss arrays with null if we do sampling or future data may have nulls in the array.,,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 15:19:37 UTC 2015,,,,,,,,,,"0|i264vj:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"27/Feb/15 05:52;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4806;;;","02/Mar/15 15:19;lian cheng;Issue resolved by pull request 4806
[https://github.com/apache/spark/pull/4806];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark on YARN does not work --executor-cores is specified,SPARK-6050,12778121,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,mridulm80,mridulm80,27/Feb/15 03:38,09/Mar/15 18:57,14/Jul/23 06:27,02/Mar/15 22:42,1.3.0,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"
There are multiple issues here (which I will detail as comments), but to reproduce running the following ALWAYS hangs in our cluster with the 1.3 RC

./bin/spark-submit --class org.apache.spark.examples.SparkPi     --master yarn-cluster --executor-cores 8    --num-executors 15     --driver-memory 4g     --executor-memory 2g          --queue webmap     lib/spark-examples*.jar     10","
2.5 based YARN cluster.",apachespark,jongyoul,lianhuiwang,michaelmalak,mridulm80,pwendell,sb58,tgraves,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 27 22:51:23 UTC 2015,,,,,,,,,,"0|i264tb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"27/Feb/15 03:49;mridulm80;Thanks to [~tgraves] for helping investigate this.

There are multiple issues in the codebase - and not all of them have been fully understood.

a) For some reason, either YARN returns incorrect response to an allocate request or we are not setting the right param.
Note the snippet [1] to detail this.
(I cant share the logs unfortunately - but Tom has access to it and should be trivial for others to reproduce the issue).

b) For whatever reason (a) happens, we do not recover from it.
All subsequent requests heartbeat requests DO NOT contain pending allocation requests (and we have rejected/de-allocated whatever yarn just sent us due to (a)).

To elaborate; updateResourceRequests has missing == 0 since it is relying on getNumPendingAllocate() - which DOES NOT do the right thing in our context. Note: the 'ask' list in the super class was cleared as part of the previous allocate() call.



Fixing (a) will mask (b) - but IMO we should address it at the earliest too.




[1] Note the vCore in the response, and the subsequent ignoring of all containers.
15/02/27 01:40:30 INFO YarnAllocator: Will request 1000 executor containers, each with 8 cores and 38912 MB memory including 10240 MB overhead
15/02/27 01:40:30 INFO YarnAllocator: Container request (host: Any, capability: <memory:38912, vCores:8>)
15/02/27 01:40:30 INFO ApplicationMaster: Started progress reporter thread - sleep time : 5000
15/02/27 01:40:30 DEBUG ApplicationMaster: Sending progress
15/02/27 01:40:30 INFO YarnAllocator: missing = 0, targetNumExecutors = 1000, numPendingAllocate = 1000, numExecutorsRunning = 0
15/02/27 01:40:35 DEBUG ApplicationMaster: Sending progress
15/02/27 01:40:35 INFO YarnAllocator: missing = 0, targetNumExecutors = 1000, numPendingAllocate = 1000, numExecutorsRunning = 0
15/02/27 01:40:36 DEBUG YarnAllocator: Allocated containers: 1000. Current executor count: 0. Cluster resources: <memory:43006976, vCores:-1000>.
15/02/27 01:40:36 DEBUG YarnAllocator: Releasing 1000 unneeded containers that were allocated to us
15/02/27 01:40:36 INFO YarnAllocator: Received 1000 containers from YARN, launching executors on 0 of them.;;;","27/Feb/15 06:42;mridulm80;With more verbose debug added, the problem surfaces.
Atleast with hadoop 2.5, the returned response always has vCores == 1 (and at the RM, it is treated as vCores == 1 too ... sigh, unimplemented ?)


So in effect, we must not set executorCores while creating ""resource"" in YarnAllocator.

See below for log snippet :


15/02/27 06:37:33 INFO YarnAllocator: Will request 1 executor containers, each with 2 cores and 32870 MB memory including 2150 MB overhead
15/02/27 06:37:33 DEBUG AMRMClientImpl: Added priority=1
15/02/27 06:37:33 DEBUG AMRMClientImpl: addResourceRequest: applicationId= priority=1 resourceName=* numContainers=1 #asks=1
15/02/27 06:37:33 INFO YarnAllocator: Container request (host: Any, capability: <memory:32870, vCores:2>)
15/02/27 06:37:33 INFO YarnAllocator: missing = 0, targetNumExecutors = 1, numPendingAllocate = 1, numExecutorsRunning = 0
15/02/27 06:37:33 INFO AMRMClientImpl: Received new token for : <host>:8041
15/02/27 06:37:33 DEBUG YarnAllocator: Allocated containers: 1. Current executor count: 0. Cluster resources: <memory:81907200, vCores:-1>.
15/02/27 06:37:33 INFO YarnAllocator: allocatedContainer = Container: [ContainerId: <contained_id>, NodeId: <host>:8041, NodeHttpAddress: <host>:8042, Resource: <memory:33280, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service: <host>:8041 }, ], location = <host>
15/02/27 06:37:33 INFO YarnAllocator: allocatedContainer = Container: [ContainerId: <contained_id>, NodeId: <host>:8041, NodeHttpAddress: <host>:8042, Resource: <memory:33280, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service: <host>:8041 }, ], location = /<IP>
15/02/27 06:37:33 INFO YarnAllocator: allocatedContainer = Container: [ContainerId: <contained_id>, NodeId: <host>:8041, NodeHttpAddress: <host>:8042, Resource: <memory:33280, vCores:1>, Priority: 1, Token: Token { kind: ContainerToken, service: <host>:8041 }, ], location = *
15/02/27 06:37:33 DEBUG YarnAllocator: Releasing 1 unneeded containers that were allocated to us
15/02/27 06:37:33 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 0 of them.
;;;","27/Feb/15 08:03;pwendell;[~mridulm@yahoo-inc.com] thanks for testing and reporting this! Ping to [~vanzin] and [~andrewor14]!;;;","27/Feb/15 14:33;tgraves;Thanks for investigating this more.

This is because on that hadoop cluster cpu scheduling isn't turned on.  When its not turned on,it defaults to 1 core.

@mridulm are you requesting 2 cores in your config?  I didn't see it in your example SparkPi command but going by the log command I assume you are.  

15/02/27 06:37:33 INFO YarnAllocator: Will request 1 executor containers, each with 2 cores and 32870 MB memory including 2150 MB overhead

If that is the case just remove the config or change to 1 core.  Other then that I don't know there is anything Spark can do as it doesn't know how Hadoop is configured.  The change that was made is to now use more of the Hadoop AMClient which adds the matching for the container requests. We just weren't checking that the cores matched before in Spark 1.2.  

We could theorectically look at the hadoop configs but that could get pretty hairy quickly as there are different schedulers and then within the scheduler it handles memory/cores differently.  Many clusters don't have scheduler configs on gateway boxes also. One thing we should do is add better logging as to why they don't match, but the match routine is also in the Hadoop AMClient code so its more us printing exactly what came back with the allocate response.  I can also file Hadoop jira to log information when it doesn't match.  The other option would be to write our own match routine.

Thoughts?




;;;","27/Feb/15 17:08;mridulm80;
[~tgraves] You are right, cpu scheduling is not turned on in our cluster.
In the description, I am specifying number of cores as 8.

For our jobs, I request for the entire memory of the node to use it completely.
We need to specify number of cores in order for spark to launch multiple threads on the executor (else it will be large memory, single thread) : and '--executor-cores' is the means to do so.

So either we will need to check if cpu scheduling is turned on in yarn before specifying cores as a resource (if off, use 1), or assume the user knows best and always ask for '1' core and spin-off multiple threads like in 1.2.;;;","27/Feb/15 17:24;vanzin;Is there any downside to always requesting 1 if cpu scheduling is off? (Other than users using a lot more cores than they would otherwise be allowed, but I guess that's what the Yarn admin wanted by turning that option off.);;;","27/Feb/15 17:35;tgraves;Ah good point.   Note that 1.2 didn't always ask for 1. It asked for whatever the user asked for but it didn't do any validation on the requests coming back.  We should basically do the same thing. If we were to change to only ask for 1 it would negate hadoop cpu scheduling which I don't want to do.;;;","27/Feb/15 17:37;tgraves;Note, the problem is in determining whether cpu scheduling is off or not.  There is no interface exposed to the user. For the Capacity Scheduler you would have to look for specific config:

capacity-scheduler.xml.
    <name>yarn.scheduler.capacity.resource-calculator</name>
    <value>org.apache.hadoop.yarn.util.resource.DominantResourceCalculator</value>


That config file might not even be available on all the gateways as its only needed on RM.;;;","27/Feb/15 19:49;vanzin;Hmm, I'm having trouble reproducing this locally. The correct value to hit this should be {{org.apache.hadoop.yarn.util.resource.DefaultResourseCalculator}}, right? Maybe they fixed this particular behavior in 2.6?;;;","27/Feb/15 19:50;vanzin;Oh, let me try again with {{DefaultResourceCalculator}}. Gotta love typos in documentation.;;;","27/Feb/15 20:05;vanzin;Ok, I think I'm hitting it when using the right name. Will take a look.;;;","27/Feb/15 20:40;tgraves;Sounds like you figured it out already, I cut and paste the wrong one, you need the DominantResourceCalculator with capacity scheduler to get it to do cores.;;;","27/Feb/15 20:57;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4818;;;","27/Feb/15 22:48;tgraves;Note that there is one other problem Mridul reported in this jira and perhaps we should split this into another jira unless [~vanzin] thinks the pr fixes it also:


So the scenario is like this.
Suppose we asked for 10 workers, RM gave us say 12 - but we decided to use only 6 (in this example, we used 0 due to vCores constraints).
Now, we still need 4 workers more - but we will never ask for it again from RM.
;;;","27/Feb/15 22:51;vanzin;That sounds like a potentially different issue. But since the requested resources are static in YarnAllocator, I don't see how the code would only use 6 containers when the RM sends more.

Unless the RM can send back containers that don't match your request (aside from this weird situation for which we're adding the workaround). Otherwise, the code should match all of them and YarnAllocator should use all of them.;;;",,,,,,,,,,,,,,
SparkConf.translateConfKey should not translate on set,SPARK-6048,12778087,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor14,27/Feb/15 00:55,09/Apr/15 21:52,14/Jul/23 06:27,03/Mar/15 00:37,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"There are several issues with translating on set.

(1) The most serious one is that if the user has both the deprecated and the latest version of the same config set, then the value picked up by SparkConf will be arbitrary. Why? Because during initialization of the conf we call `conf.set` on each property in `sys.props` in an order arbitrarily defined by Java. As a result, the value of the more recent config may be overridden by that of the deprecated one. Instead, we should always use the value of the most recent config.

(2) If we translate on set, then we must keep translating everywhere else. In fact, the current code does not translate on remove, which means the following won't work if X is deprecated:
{code}
conf.set(X, Y)
conf.remove(X) // X is not in the conf
{code}
This requires us to also translate in remove and other places, as we already do for contains, leading to more duplicate code.

(3) Since we call `conf.set` on all configs when initializing the conf, we print all deprecation warnings in the beginning. Elsewhere in Spark, however, we warn the user when the deprecated config / option / env var is actually being used.
We should keep this consistent so the user won't expect to find all deprecation messages in the beginning of his logs.",,andrewor14,apachespark,pwendell,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5933,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 27 20:10:22 UTC 2015,,,,,,,,,,"0|i264lr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"27/Feb/15 01:00;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4799;;;","27/Feb/15 01:12;vanzin;I sort of agree with (1). But I think it's both unlikely (users will probably use the old option or the new one, but not both), and probably sort of fixable (but not optimally). Basically, don't override a value that's already set when using the deprecated key.

I disagree with (2). Just fix remove().

I also disagree with (3), and it's not even the correct interpretation of what happens. Warning *only* happen when the configuration keys are set, never when reading. And I think it's actually a good thing that all (or most) of the warnings show up when creating the conf object, which generally happens early in the app's life. It means it's easier to see them.;;;","27/Feb/15 01:31;andrewor14;[~vanzin]

Note that (3) is orthogonal to this change. We can still do all the warnings at the beginning rather than later. However I still don't see why warnings should necessarily be tied to when keys are set, though that is a separate discussion.

For (2), yes we can just fix remove(), but doing so means duplicating the translation and keeping track of one more place where the translation takes place. In the future if we add more methods to SparkConf, we'll have to remember to do the translation otherwise it won't work correctly. I personally find limiting the scope of translation much cleaner.

(1) Maybe it's unlikely, but it breaks existing user behavior in a confounding way nevertheless. When it fails it will be extremely difficult to debug which value is used without doing some querying of the conf itself.;;;","27/Feb/15 02:04;vanzin;What do you mean by ""duplicates the translation"" (regarding 2)? It's just a call to ""translateKey()"".

Regarding 1, that problem exists regardless of my change. You need to specify some precedence order. See the case in FsHistoryProvider, where there are 2 (!) deprecated keys for the same config. What if the user sets those two deprecated keys in the conf? What's the expectation? Perhaps you need to enforce some sort of ordering for the deprecated keys in SparkConf, but I don't see why that means translating on get and not on set.

Note the goal of the deprecated configs was to make the Spark code only have to care about the most recent key name. Your proposal goes against that, and would require the deprecated names to live both in SparkConf and in the code that needs to read them.;;;","27/Feb/15 02:30;andrewor14;bq. What do you mean by ""duplicates the translation"" (regarding 2)? It's just a call to ""translateKey()"".

It's not about the number of lines that are being duplicated. It's about the translation logic. Right now it's not correct to translate in set but not in all interfaces exposed by SparkConf. As we have seen with the case of `remove` it's easy to miss one or two of these interfaces. If we only translate in `get` then we don't have to worry about this.

bq. Regarding 1, that problem exists regardless of my change.

That's actually not true. Before your change, if we specify both the deprecated config and the most recent one, the behavior will be determined by the place where these values are used. Even if we called `set` on the deprecated config over the more recent one, the value of the latter is still preserved because we didn't translate on `set`. To answer your question, the expected behavior is for the value of the more recent config to *always* take precedence.

bq. Note the goal of the deprecated configs was to make the Spark code only have to care about the most recent key name. Your proposal goes against that, and would require the deprecated names to live both in SparkConf and in the code that needs to read them.

Yes, unfortunately, and I agree it's something we need to fix in the future. My eventual goal is to do hide all the deprecation logic throughout the Spark code, and this is why I filed SPARK-5933 before. Currently, however, this is a correctness issue that is blocking the 1.3 release, so my personal opinion is that we should first fix this broken behavior and worry about the code style later.;;;","27/Feb/15 02:33;pwendell;Hey All,

No opinions on which design we chose to implement internally. However, I do feel strongly that the user-facing precedence should not change between versions. It's not reasonable to assume that no user has both the old and new names for a config value. Configuration files can be very long, or there can be multiple sources of configuration, such as a user using both flags and a file. So changing the semantics randomly in a release constitutes a breaking change of behavior.

In terms of the nicest possible way to achieve these semantics, that's up to you guys since you're much more familiar with this code. The current patch seems to just rewind the behavior that was introduced earlier. Marcello, unless you see some correctness problem with that patch, I'd like to merge it to unblock the release. If you guys think it's way better to do translation on writes than reads, it's fine to propose that in a new patch.
;;;","27/Feb/15 17:19;vanzin;But the current patch has a precedence problem and that's what I'm trying to say. Let me illustrate. Let's say some Spark deployment has this spark-defaults.conf:

{code}
spark.files.userClassPathFirst=true
{code}

Some user app is broken and the user does this in his code:

{code}
val conf = new SparkConf().set(""spark.files.userClassPathFirst"", ""false"")
{code}

Now the admin upgrades to Spark 1.3, goes through the docs, and changes spark-defaults.conf to:

{code}
spark.executor.userClassPathFirst=true
{code}

With the change, you've broken that application. SparkConf does't haven enough information to know that the user is trying to override the new config using the old name, because it doesn't keep track of where configs are set from.

Ignoring Andrew's point #1 for a second, doing the translation on set solves that problem.

Now, Andrew has a point about #1, and it's because SparkSubmit mixes things up by using system properties to merge default and user settings (plus Spark allowing the use of system properties for config in general). But that should be easily solvable in SparkSubmit by doing something like:

{code}
Utils.getSystemProperties.foreach { case (k, v) =>
  if (k.startsWith(""spark."")) {
    System.clearProperty(k)
    System.setProperty(SparkConf.translateKey(k), v)
  }
}
{code}

For better or worse, I like to think of config names as a public API. In that light, Andrew's change breaks binary compatibility, because it breaks the application above. If you need further proof, look at the unit test that Andrew's first version of the patch broke. It illustrates exactly the above situation.;;;","27/Feb/15 20:10;pwendell;Okay I just talked to [~vanzin] offline. Basically the crux of this issue is the following precedence question: If we have both the deprecated and new version of a configuration key (let's say c and c') present, how does Spark handle the precedence order with respect to the normal precedence order in Spark for configs?

There are two different possibilities:
1. Using the newer config always gives you highest precedence over any instance of an older config.
2. The newer and older config are treated as identical (basically, aliases) and only the normal precedence order applies.

We never publicly documented either approach, however Spark always has done (1) in the past because we've used simple fallbacks on the read side.

Personally my feeling is that we should clearly document some approach, have it be backwards compatible, and have the same approach for all deprecated configs. In the short term, the only way I see that happening is to roll back the existing translation on ""set"" (this is in some sense lossy, making it hard to support (1)) and stick to translation on ""get"". In the longer term, maybe we can come up with alternative ways to support these semantics.;;;",,,,,,,,,,,,,,,,,,,,,
Provide an easier way for developers to handle deprecated configs,SPARK-6046,12778020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,andrewor14,andrewor14,26/Feb/15 21:26,17/Apr/15 17:09,14/Jul/23 06:27,17/Apr/15 10:06,1.3.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"Right now we have code that looks like this:
https://github.com/apache/spark/blob/8942b522d8a3269a2a357e3a274ed4b3e66ebdde/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L52

where a random class calls `SparkConf.translateConfKey` to warn the user against a deprecated configs. We should refactor this slightly so we can make `translateConfKey` private instead of calling it from everywhere.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 17 17:09:55 UTC 2015,,,,,,,,,,"0|i2647j:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"26/Feb/15 23:43;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4797;;;","14/Apr/15 22:53;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5514;;;","17/Apr/15 10:06;srowen;Issue resolved by pull request 5514
[https://github.com/apache/spark/pull/5514];;;","17/Apr/15 10:07;srowen;I hope it's OK to assign this to Marcelo, as I think his change was the one that addressed the bulk of the broadened scope of this JIRA.;;;","17/Apr/15 17:09;andrewor14;Yes of course, I usually assign it to the author of the PR;;;",,,,,,,,,,,,,,,,,,,,,,,,
RecordWriter should be checked against null in PairRDDFunctions#saveAsNewAPIHadoopDataset,SPARK-6045,12778013,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,tedyu@apache.org,yuzhihong@gmail.com,yuzhihong@gmail.com,26/Feb/15 20:56,26/Feb/15 23:28,14/Jul/23 06:27,26/Feb/15 23:27,,,,,,,1.4.0,,,,,,Input/Output,,,,0,,,,,,"gtinside reported in the thread 'NullPointerException in TaskSetManager' with the following stack trace:
{code}
WARN 2015-02-26 14:21:43,217 [task-result-getter-0] TaskSetManager - Lost
task 14.2 in stage 0.0 (TID 29, devntom003.dev.blackrock.com):
java.lang.NullPointerException

org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopDataset(PairRDDFunctions.scala:1007)
com.bfm.spark.test.CassandraHadoopMigrator$.main(CassandraHadoopMigrator.scala:77)
com.bfm.spark.test.CassandraHadoopMigrator.main(CassandraHadoopMigrator.scala)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:606)
org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:358)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}
Looks like the following call in finally block was the cause:
{code}
    writer.close(hadoopContext)
{code}
We should check writer against null before calling close().",,apachespark,yuzhihong@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 23:27:49 UTC 2015,,,,,,,,,,"0|i2645z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/15 20:58;yuzhihong@gmail.com;https://github.com/apache/spark/pull/4794;;;","26/Feb/15 20:58;apachespark;User 'tedyu' has created a pull request for this issue:
https://github.com/apache/spark/pull/4794;;;","26/Feb/15 21:08;srowen;Per the PR, it's not so much that this isn't checked for {{null}} but that the {{RecordWriter}} is {{null}} in the first place. That can be failed earlier, but it still fails with an exception.

Isn't the problem with {{CassandraHadoopInputOutputWriter}} not returning a {{RecordWriter}}? that seems outside of Spark.;;;","26/Feb/15 21:12;yuzhihong@gmail.com;The logic of CassandraHadoopMigrator.scala is unknown.

With the current PR, user would be able to see exception earlier so that he / she can perform proper analysis.;;;","26/Feb/15 21:28;srowen;Yeah, this replaces an NPE-hiding-an-NPE with a clearer IllegalArgumentException, which is good, but that's all it does. This still dies with an exception, which is I suppose the right-est thing Spark can do since the impl isn't working.;;;","26/Feb/15 23:27;srowen;Issue resolved by pull request 4794
[https://github.com/apache/spark/pull/4794];;;",,,,,,,,,,,,,,,,,,,,,,,
RDD.aggregate() should not use the closure serializer on the zero value,SPARK-6044,12777993,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,mcheah,mcheah,26/Feb/15 19:50,17/Mar/15 06:59,14/Jul/23 06:27,17/Mar/15 06:59,1.3.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"PairRDDFunctions.aggregateByKey() correctly uses SparkEnv.get.serializer.newInstance() to serialize the zero value. It seems this logic is not mirrored in RDD.aggregate(), which computes the aggregation and returns the aggregation directly at the driver. We should change RDD.aggregate() to make this consistent; I ran into some serialization errors because I was expecting RDD.aggregate() to Kryo serialize the zero value.",,apachespark,mcheah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 14 19:01:25 UTC 2015,,,,,,,,,,"0|i2641r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/15 20:17;srowen;Yeah I think that's right, given the recent mailing list discussion. Seems like an easy fix, go for it.;;;","14/Mar/15 19:01;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/5028;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the percent bug in tablesample,SPARK-6040,12777860,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,waterman,waterman,waterman,26/Feb/15 12:15,24/Apr/15 00:43,14/Jul/23 06:27,02/Mar/15 21:16,1.2.1,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"HiveQL expression like ```select count(1) from src tablesample(1 percent);``` means take 1% sample to select. But it means 100% in the current version of the Spark.
",,apachespark,marmbrus,waterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 02 21:16:50 UTC 2015,,,,,,,,,,"0|i263a7:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"26/Feb/15 12:43;apachespark;User 'watermen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4789;;;","02/Mar/15 21:16;marmbrus;Issue resolved by pull request 4789
[https://github.com/apache/spark/pull/4789];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
EventLog process logic has race condition with Akka actor system,SPARK-6036,12777834,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,liyezhang556520,liyezhang556520,liyezhang556520,26/Feb/15 09:29,13/Mar/15 18:07,14/Jul/23 06:27,13/Mar/15 18:06,1.3.0,,,,,,1.3.1,1.4.0,,,,,Spark Core,Web UI,,,0,,,,,,"when application finished, akka actor system will trigger disassociated event, and Master will rebuild SparkUI on web, in which progress will check whether the eventlog files are still in progress. The current logic in SparkContext is first stop the actorsystem, and then stop enentLogListener. This will cause that the enentLogListener has not finished renaming the eventLog dir name (from app-xxxx.inprogress to app-xxxx)  when Spark Master try to access the dir. ",,apachespark,liyezhang556520,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 09:35:02 UTC 2015,,,,,,,,,,"0|i2634f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/15 09:35;apachespark;User 'liyezhang556520' has created a pull request for this issue:
https://github.com/apache/spark/pull/4785;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SizeEstimator gives wrong result for Integer object on 64bit JVM with UseCompressedOops on,SPARK-6030,12777798,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,advancedxy,advancedxy,advancedxy,26/Feb/15 06:36,02/May/15 22:08,14/Jul/23 06:27,02/May/15 22:08,1.0.2,1.1.1,1.2.1,,,,1.4.0,,,,,,Spark Core,,,28/Feb/15 00:00,0,,,,,,"Integer on 64bit JVM with UseCompressedOops on is 16bytes (verified by a related article http://www.javaworld.com/article/2077496/testing-debugging/java-tip-130--do-you-know-your-data-size-.html, I created a gist for that code https://gist.github.com/advancedxy/2ae7c9cc7629f3aeb679), however SizeEstimator give 24bytes for Integer.

SizeEstimator gives the wrong answer because it alignSize on internal shellSize. For Integer, there is a parent class called Number, which has zero fields. Thus the shellSize for Number is 12bytes but was aligned to 16bytes, which resulted the Integer's shellSize to be 20bytes, aligned to 24bytes.
The right path should be 
1. Object-> shellSize: 12bytes, realSize: 16bytes
2. Number -> shellSize: 12bytes + 0, realSize: 16bytes
3. Integer -> shellSize: 12bytes + 4bytes(the int value), realSize: 16bytes

The fix is rather simple, I will submit a pr later.",,advancedxy,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat May 02 22:08:26 UTC 2015,,,,,,,,,,"0|i262uv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/15 06:58;apachespark;User 'advancedxy' has created a pull request for this issue:
https://github.com/apache/spark/pull/4783;;;","02/May/15 22:08;srowen;Issue resolved by pull request 4783
[https://github.com/apache/spark/pull/4783];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Make KafkaUtils work in Python with kafka-assembly provided as --jar or maven package provided as --packages,SPARK-6027,12777776,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,tdas,tdas,26/Feb/15 04:04,26/Feb/15 21:47,14/Jul/23 06:27,26/Feb/15 21:47,1.3.0,,,,,,1.3.0,,,,,,DStreams,PySpark,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5185,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 04:22:49 UTC 2015,,,,,,,,,,"0|i262pz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"26/Feb/15 04:22;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/4779;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When a data source table has too many columns, it's schema cannot be stored in metastore.",SPARK-6024,12777753,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,26/Feb/15 02:24,27/Feb/15 04:46,14/Jul/23 06:27,27/Feb/15 04:46,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Because we are using table properties of a Hive metastore table to store the schema, when a schema is too wide, we cannot persist it in metastore.

{code}
15/02/25 18:13:50 ERROR metastore.RetryingHMSHandler: Retrying HMSHandler after 1000 ms (attempt 1 of 1) with error: javax.jdo.JDODataStoreException: Put request failed : INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) 
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:732)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:719)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy15.createTable(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1261)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1294)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy16.create_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:558)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:547)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy17.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:613)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.createDataSourceTable(HiveMetastoreCatalog.scala:136)
	at org.apache.spark.sql.hive.execution.CreateMetastoreDataSourceAsSelect.run(commands.scala:243)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:55)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:55)
	at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:65)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:1092)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:1092)
	at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:1013)
	at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:963)
	at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:929)
	at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:907)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:32)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40)
	at $line39.$read$$iwC$$iwC$$iwC.<init>(<console>:42)
	at $line39.$read$$iwC$$iwC.<init>(<console>:44)
	at $line39.$read$$iwC.<init>(<console>:46)
	at $line39.$read.<init>(<console>:48)
	at $line39.$read$.<init>(<console>:52)
	at $line39.$read$.<clinit>(<console>)
	at $line39.$eval$.<init>(<console>:7)
	at $line39.$eval$.<clinit>(<console>)
	at $line39.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:856)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:901)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:813)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:656)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:664)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:669)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:996)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:944)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1058)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
NestedThrowablesStackTrace:
org.datanucleus.store.rdbms.exceptions.MappedDatastoreException: INSERT INTO TABLE_PARAMS (PARAM_VALUE,TBL_ID,PARAM_KEY) VALUES (?,?,?) 
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.internalPut(JoinMapStore.java:1078)
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.putAll(JoinMapStore.java:220)
	at org.datanucleus.store.rdbms.mapping.java.MapMapping.postInsert(MapMapping.java:137)
	at org.datanucleus.store.rdbms.request.InsertRequest.execute(InsertRequest.java:519)
	at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.insertTable(RDBMSPersistenceHandler.java:167)
	at org.datanucleus.store.rdbms.RDBMSPersistenceHandler.insertObject(RDBMSPersistenceHandler.java:143)
	at org.datanucleus.state.JDOStateManager.internalMakePersistent(JDOStateManager.java:3784)
	at org.datanucleus.state.JDOStateManager.makePersistent(JDOStateManager.java:3760)
	at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2219)
	at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
	at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
	at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:719)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)
	at com.sun.proxy.$Proxy15.createTable(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1261)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1294)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)
	at com.sun.proxy.$Proxy16.create_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:558)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:547)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:89)
	at com.sun.proxy.$Proxy17.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:613)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.createDataSourceTable(HiveMetastoreCatalog.scala:136)
	at org.apache.spark.sql.hive.execution.CreateMetastoreDataSourceAsSelect.run(commands.scala:243)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:55)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:55)
	at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:65)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:1092)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:1092)
	at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:1013)
	at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:963)
	at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:929)
	at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:907)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:32)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:36)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:38)
	at $line39.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40)
	at $line39.$read$$iwC$$iwC$$iwC.<init>(<console>:42)
	at $line39.$read$$iwC$$iwC.<init>(<console>:44)
	at $line39.$read$$iwC.<init>(<console>:46)
	at $line39.$read.<init>(<console>:48)
	at $line39.$read$.<init>(<console>:52)
	at $line39.$read$.<clinit>(<console>)
	at $line39.$eval$.<init>(<console>:7)
	at $line39.$eval$.<clinit>(<console>)
	at $line39.$eval.$print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:856)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:901)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:813)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:656)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:664)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:669)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:996)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:944)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1058)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
Caused by: java.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR '{""type"":""struct"",""fields"":[{""name"":""contributors"",""type"":""st&' to length 4000.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeLargeUpdate(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedPreparedStatement.executeUpdate(Unknown Source)
	at com.jolbox.bonecp.PreparedStatementHandle.executeUpdate(PreparedStatementHandle.java:205)
	at org.datanucleus.store.rdbms.ParamLoggingPreparedStatement.executeUpdate(ParamLoggingPreparedStatement.java:399)
	at org.datanucleus.store.rdbms.SQLController.executeStatementUpdate(SQLController.java:439)
	at org.datanucleus.store.rdbms.scostore.JoinMapStore.internalPut(JoinMapStore.java:1069)
	... 87 more
Caused by: java.sql.SQLException: A truncation error was encountered trying to shrink VARCHAR '{""type"":""struct"",""fields"":[{""name"":""contributors"",""type"":""st&' to length 4000.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory40.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 101 more
Caused by: ERROR 22001: A truncation error was encountered trying to shrink VARCHAR '{""type"":""struct"",""fields"":[{""name"":""contributors"",""type"":""st&' to length 4000.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.types.SQLChar.hasNonBlankChars(Unknown Source)
	at org.apache.derby.iapi.types.SQLVarchar.normalize(Unknown Source)
	at org.apache.derby.iapi.types.SQLVarchar.normalize(Unknown Source)
	at org.apache.derby.iapi.types.DataTypeDescriptor.normalize(Unknown Source)
	at org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeColumn(Unknown Source)
	at org.apache.derby.impl.sql.execute.NormalizeResultSet.normalizeRow(Unknown Source)
	at org.apache.derby.impl.sql.execute.NormalizeResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.DMLWriteResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.InsertResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 95 more
{code}",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 21:47:52 UTC 2015,,,,,,,,,,"0|i262kv:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"26/Feb/15 02:24;yhuai;Seems we need to split the schema's string representation...;;;","26/Feb/15 21:47;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4795;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetConversions fails to replace the destination MetastoreRelation of an InsertIntoTable node to ParquetRelation2,SPARK-6023,12777746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,26/Feb/15 02:02,24/Apr/15 00:43,14/Jul/23 06:27,26/Feb/15 14:42,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"{code}
import sqlContext._
sql(""drop table if exists test"")

val df1 = sqlContext.jsonRDD(sc.parallelize((1 to 10).map(i => s""""""{""a"":$i}"""""")))
df1.registerTempTable(""jt"")
sql(""create table test (a bigint) stored as parquet "")

sql(""explain insert into table test select a from jt"").collect.foreach(println)
{code}
The plan will be
{code}
[== Physical Plan ==]
[InsertIntoHiveTable (MetastoreRelation default, test, None), Map(), false]
[ PhysicalRDD [a#34L], MapPartitionsRDD[17] at map at JsonRDD.scala:41]
{code}

However, the write path should be converted to our own data source path.",,apachespark,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 14:42:34 UTC 2015,,,,,,,,,,"0|i262jb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"26/Feb/15 02:27;yhuai;The problem is when we do plan.transformUp to replace the relation, we will never visit the destination relation in an InsertIntoTable command.;;;","26/Feb/15 05:54;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4782;;;","26/Feb/15 14:42;lian cheng;Issue resolved by pull request 4782
[https://github.com/apache/spark/pull/4782];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
GraphX `diff` test incorrectly operating on values (not VertexId's),SPARK-6022,12777710,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,boyork,boyork,25/Feb/15 23:52,13/Mar/15 15:59,14/Jul/23 06:27,13/Mar/15 15:59,,,,,,,,,,,,,GraphX,,,,0,,,,,,"The current GraphX {{diff}} test operates on values rather than the VertexId's and, if {{diff}} were working properly (per [SPARK-4600|https://issues.apache.org/jira/browse/SPARK-4600]), it should fail this test. The code to test {{diff}} should look like the below as it correctly generates {{VertexRDD}}'s with different {{VertexId}}'s to {{diff}} against.

{code}
test(""diff functionality with small concrete values"") {
    withSpark { sc =>
      val setA: VertexRDD[Int] = VertexRDD(sc.parallelize(0L until 2L).map(id => (id, id.toInt)))
      // setA := Set((0L, 0), (1L, 1))
      val setB: VertexRDD[Int] = VertexRDD(sc.parallelize(1L until 3L).map(id => (id, id.toInt+2)))
      // setB := Set((1L, 3), (2L, 4))
      val diff = setA.diff(setB)
      assert(diff.collect.toSet == Set((2L, 4)))
    }
  }
{code}",,ankurd,boyork,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 15:59:33 UTC 2015,,,,,,,,,,"0|i262br:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/15 23:54;boyork;FWIW I have this fix put in place under my branch for [SPARK-4600|https://issues.apache.org/jira/browse/SPARK-4600], but wanted to point it out here as I'm going to need to remove the original test. This is all assuming I'm not completely insane and missing something here.

cc [~ankurd];;;","03/Mar/15 07:55;maropu;Is the test correct?
According to the code below, 'diff' is assumed to have same indices in VertexPartitionBase.
That is,  SetA and SetB has the same set of VertexIDs (Im not sure that this behaviour totally correct though).  

https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/impl/VertexPartitionBaseOps.scala#L93;;;","09/Mar/15 18:42;boyork;The test is correct (in what I believe {{diff}} should do). Maybe [~ankurd] can chime in here? And you're also correct in that the code implementing {{diff}} doesn't currently work properly which is why I believe this test should correctly assess whether {{diff}} is operating correctly.;;;","09/Mar/15 21:13;ankurd;[~maropu] is correct: the original intent of diff was to operate on values, not VertexIds. It was really written for internal use in [mapVertices|https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/impl/GraphImpl.scala#L133] and [outerJoinVertices|https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/impl/GraphImpl.scala#L284], which use it to find the set of vertices whose values have changed so they can ship only those to the edge partitions.

Based on your test you're looking for the set difference. Maybe you could introduce a new method called ""minus""?;;;","11/Mar/15 07:55;maropu;Yeah, ISTM it'd be better to add set difference as Graph#minus.;;;","13/Mar/15 15:59;boyork;Awesome, thanks for the clarity guys! I'll close this JIRA and introduce a new one for a Graph#minus that should better reflect what I was going for!;;;",,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.FutureActionSuite - complex sync action,SPARK-6021,12777709,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,andrewor14,andrewor14,25/Feb/15 23:48,30/Dec/15 22:02,14/Jul/23 06:27,30/Dec/15 22:02,1.3.0,,,,,,,,,,,,Spark Core,Tests,,,0,,,,,,"Observed in Spark Master SBT builds:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/1777/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/1773/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/1752/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/1740/

{code}
Stacktrace

sbt.ForkMain$ForkError
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$5.apply(AsyncRDDActions.scala:100)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1$$anonfun$apply$5.apply(AsyncRDDActions.scala:100)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1.apply(AsyncRDDActions.scala:100)
	at org.apache.spark.rdd.AsyncRDDActions$$anonfun$takeAsync$1.apply(AsyncRDDActions.scala:68)
	at org.apache.spark.ComplexFutureAction$$anonfun$run$1.apply(FutureAction.scala:209)
	at org.apache.spark.ComplexFutureAction$$anonfun$run$1.apply(FutureAction.scala:206)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$3.exec(ExecutionContextImpl.scala:107)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}

Not a super helpful stack trace...",,andrewor14,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 30 22:02:43 UTC 2015,,,,,,,,,,"0|i262bj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/15 22:02;joshrosen;Resolving as fixed since this test has not been observed recently in master or 1.6: https://spark-tests.appspot.com/tests/org.apache.spark.FutureActionSuite/complex%20async%20action;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.sql.columnar.PartitionBatchPruningSuite,SPARK-6020,12777707,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,andrewor14,andrewor14,25/Feb/15 23:46,03/Mar/15 06:44,14/Jul/23 06:27,03/Mar/15 06:44,1.3.0,,,,,,,,,,,,SQL,Tests,,,0,,,,,,"Observed in the following builds, only one of which has something to do with SQL:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/27931/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/27930/
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/27929/

org.apache.spark.sql.columnar.PartitionBatchPruningSuite.SELECT key FROM pruningData WHERE NOT (key IN (1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30))

{code}
Error Message

8 did not equal 10 Wrong number of read batches: == Parsed Logical Plan == 'Project ['key]  'Filter NOT 'key IN (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30)   'UnresolvedRelation [pruningData], None  == Analyzed Logical Plan == Project [key#5245]  Filter NOT key#5245 IN (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30)   LogicalRDD [key#5245,value#5246], MapPartitionsRDD[3202] at mapPartitions at ExistingRDD.scala:35  == Optimized Logical Plan == Project [key#5245]  Filter NOT key#5245 INSET (5,10,24,25,14,20,29,1,6,28,21,9,13,2,17,22,27,12,7,3,18,16,11,26,23,8,30,19,4,15)   InMemoryRelation [key#5245,value#5246], true, 10, StorageLevel(true, true, false, true, 1), (PhysicalRDD [key#5245,value#5246], MapPartitionsRDD[3202] at mapPartitions at ExistingRDD.scala:35), Some(pruningData)  == Physical Plan == Filter NOT key#5245 INSET (5,10,24,25,14,20,29,1,6,28,21,9,13,2,17,22,27,12,7,3,18,16,11,26,23,8,30,19,4,15)  InMemoryColumnarTableScan [key#5245], [NOT key#5245 INSET (5,10,24,25,14,20,29,1,6,28,21,9,13,2,17,22,27,12,7,3,18,16,11,26,23,8,30,19,4,15)], (InMemoryRelation [key#5245,value#5246], true, 10, StorageLevel(true, true, false, true, 1), (PhysicalRDD [key#5245,value#5246], MapPartitionsRDD[3202] at mapPartitions at ExistingRDD.scala:35), Some(pruningData))  Code Generation: false == RDD ==
Stacktrace

sbt.ForkMain$ForkError: 8 did not equal 10 Wrong number of read batches: == Parsed Logical Plan ==
'Project ['key]
 'Filter NOT 'key IN (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30)
  'UnresolvedRelation [pruningData], None

== Analyzed Logical Plan ==
Project [key#5245]
 Filter NOT key#5245 IN (1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30)
  LogicalRDD [key#5245,value#5246], MapPartitionsRDD[3202] at mapPartitions at ExistingRDD.scala:35

== Optimized Logical Plan ==
Project [key#5245]
 Filter NOT key#5245 INSET (5,10,24,25,14,20,29,1,6,28,21,9,13,2,17,22,27,12,7,3,18,16,11,26,23,8,30,19,4,15)
  InMemoryRelation [key#5245,value#5246], true, 10, StorageLevel(true, true, false, true, 1), (PhysicalRDD [key#5245,value#5246], MapPartitionsRDD[3202] at mapPartitions at ExistingRDD.scala:35), Some(pruningData)

== Physical Plan ==
Filter NOT key#5245 INSET (5,10,24,25,14,20,29,1,6,28,21,9,13,2,17,22,27,12,7,3,18,16,11,26,23,8,30,19,4,15)
 InMemoryColumnarTableScan [key#5245], [NOT key#5245 INSET (5,10,24,25,14,20,29,1,6,28,21,9,13,2,17,22,27,12,7,3,18,16,11,26,23,8,30,19,4,15)], (InMemoryRelation [key#5245,value#5246], true, 10, StorageLevel(true, true, false, true, 1), (PhysicalRDD [key#5245,value#5246], MapPartitionsRDD[3202] at mapPartitions at ExistingRDD.scala:35), Some(pruningData))

Code Generation: false
== RDD ==
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.sql.columnar.PartitionBatchPruningSuite$$anonfun$checkBatchPruning$1.apply$mcV$sp(PartitionBatchPruningSuite.scala:119)
	at org.apache.spark.sql.columnar.PartitionBatchPruningSuite$$anonfun$checkBatchPruning$1.apply(PartitionBatchPruningSuite.scala:107)
	at org.apache.spark.sql.columnar.PartitionBatchPruningSuite$$anonfun$checkBatchPruning$1.apply(PartitionBatchPruningSuite.scala:107)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.columnar.PartitionBatchPruningSuite.org$scalatest$BeforeAndAfter$$super$runTest(PartitionBatchPruningSuite.scala:26)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.columnar.PartitionBatchPruningSuite.runTest(PartitionBatchPruningSuite.scala:26)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.columnar.PartitionBatchPruningSuite.org$scalatest$BeforeAndAfterAll$$super$run(PartitionBatchPruningSuite.scala:26)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.columnar.PartitionBatchPruningSuite.org$scalatest$BeforeAndAfter$$super$run(PartitionBatchPruningSuite.scala:26)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.columnar.PartitionBatchPruningSuite.run(PartitionBatchPruningSuite.scala:26)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
{code}",,andrewor14,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 06:44:32 UTC 2015,,,,,,,,,,"0|i262b3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/15 06:14;lian cheng;Hey [~andrewor14], I think [PR #4835|https://github.com/apache/spark/pull/4835] has already fixed this PR. {{InMemoryColumnarTableScan}} uses accumulators to generate debugging information for testing purposes. Test failures related to this JIRA ticket showed that accumulator updates got lost nondeterministically, which was also exactly what PR #4835 fixed. Also, according to [your amazing statistics sheets|https://docs.google.com/spreadsheets/d/1VSCTXLBqnglk0XMd0R4IhvUPb2MEDQUaRNwxcHBtSlk/edit#gid=52877182], this test suite hadn't been flaky for a week.;;;","03/Mar/15 06:44;andrewor14;Ok, I will close this as resolved for now. We can always reopen it if it's flaky again. Thanks Josh.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchMethodError in Spark app is swallowed by YARN AM,SPARK-6018,12777696,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,cheolsoo,cheolsoo,cheolsoo,25/Feb/15 22:58,24/Mar/15 10:58,14/Jul/23 06:27,26/Feb/15 21:54,1.2.0,,,,,,1.2.2,1.3.0,,,,,YARN,,,,0,yarn,,,,,"I discovered this bug while testing 1.3 RC with old 1.2 Spark job that I had. Due to changes in DF and SchemaRDD, my app failed with {{java.lang.NoSuchMethodError}}. However, AM was marked as succeeded, and the error was silently swallowed.

The problem is that pattern matching in Spark AM fails to catch NoSuchMethodError-
{code}
15/02/25 20:13:27 INFO cluster.YarnClusterScheduler: YarnClusterScheduler.postStartHook done
Exception in thread ""Driver"" scala.MatchError: java.lang.NoSuchMethodError: org.apache.spark.sql.hive.HiveContext.table(Ljava/lang/String;)Lorg/apache/spark/sql/SchemaRDD; (of class java.lang.NoSuchMethodError)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:485)
{code}",,apachespark,brett_s_r,cheolsoo,tarek_abouzeid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6449,,,,,,,SPARK-6058,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Mar 01 12:06:21 UTC 2015,,,,,,,,,,"0|i2628n:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"25/Feb/15 23:05;apachespark;User 'piaozhexiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/4773;;;","26/Feb/15 15:45;tarek_abouzeid;i am encountering same error while executing job in spark-submit :
Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.spark.sql.hive.HiveContext.sql(Ljava/lang/String;)Lorg/apache/spark/sql/SchemaRDD;

val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
sqlContext.sql(""CREATE TABLE IF NOT EXISTS tarek (key INT, value STRING)"")      <== this line gives the error

although i ran same exact lines in spark-shell and they worked properly 


;;;","01/Mar/15 12:06;tarek_abouzeid;the problem was that i was using spark 1.3.0 and was using spark 1.2.1 to compile the code , problem fixed when i copied all spark 1.3.0 jars and added them into my class path ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot read the parquet table after overwriting the existing table when spark.sql.parquet.cacheMetadata=true,SPARK-6016,12777668,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,25/Feb/15 21:01,24/Apr/15 00:44,14/Jul/23 06:27,26/Feb/15 17:04,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"saveAsTable is fine and seems we have successfully deleted the old data and written the new data. However, when reading the newly created table, an error will be thrown.
{code}
Error in SQL statement: java.lang.RuntimeException: java.lang.RuntimeException: could not merge metadata: key org.apache.spark.sql.parquet.row.metadata has conflicting values: 
at parquet.hadoop.api.InitContext.getMergedKeyValueMetaData(InitContext.java:67)
	at parquet.hadoop.api.ReadSupport.init(ReadSupport.java:84)
	at org.apache.spark.sql.parquet.FilteringParquetRowInputFormat.getSplits(ParquetTableOperations.scala:469)
	at parquet.hadoop.ParquetInputFormat.getSplits(ParquetInputFormat.java:245)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anon$1.getPartitions(newParquet.scala:461)
	...
{code}

If I set spark.sql.parquet.cacheMetadata to false, it's fine to query the data. 

Note: the newly created table needs to have more than one file to trigger the bug (if there is only a single file, we will not need to merge metadata). 

To reproduce it, try...
{code}
import org.apache.spark.sql.SaveMode
import sqlContext._
sql(""drop table if exists test"")

val df1 = sqlContext.jsonRDD(sc.parallelize((1 to 10).map(i => s""""""{""a"":$i}""""""), 2)) // we will save to 2 parquet files.
df1.saveAsTable(""test"", ""parquet"", SaveMode.Overwrite)
sql(""select * from test"").collect.foreach(println) // Warm the FilteringParquetRowInputFormat.footerCache

val df2 = sqlContext.jsonRDD(sc.parallelize((1 to 10).map(i => s""""""{""b"":$i}""""""), 4)) // we will save to 4 parquet files.
df2.saveAsTable(""test"", ""parquet"", SaveMode.Overwrite)
sql(""select * from test"").collect.foreach(println)
{code}
For this example, we have two outdated footers for df1 in footerCache and since we have four parquet files for the new test table, we picked up 2 new footers for df2. Then, we hit the bug.",,apachespark,avignon,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 17:04:51 UTC 2015,,,,,,,,,,"0|i26233:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"25/Feb/15 21:01;yhuai;cc [~lian cheng];;;","25/Feb/15 23:57;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4775;;;","26/Feb/15 17:04;lian cheng;Issue resolved by pull request 4775
[https://github.com/apache/spark/pull/4775];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
java.io.IOException: Filesystem is thrown when ctrl+c or ctrl+d spark-sql on YARN ,SPARK-6014,12777649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,cheolsoo,cheolsoo,25/Feb/15 19:35,22/Aug/18 16:25,14/Jul/23 06:27,22/Apr/15 00:34,1.3.0,,,,,,1.4.0,,,,,,YARN,,,,0,yarn,,,,,"This is a regression of SPARK-2261. In branch-1.3 and master, {{EventLoggingListener}} throws ""{{java.io.IOException: Filesystem closed}}"" when ctrl+c or ctrl+d the spark-sql shell.

The root cause is that DFSClient is already shut down before EventLoggingListener invokes the following HDFS methods, and thus, DFSClient.isClientRunning() check fails-
{code}
Line #135: hadoopDataStream.foreach(hadoopFlushMethod.invoke(_))
Line #187: if (fileSystem.exists(target)) {
{code}
The followings are full stack trace-
{code}
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:135)
	at org.apache.spark.scheduler.EventLoggingListener$$anonfun$logEvent$3.apply(EventLoggingListener.scala:135)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.EventLoggingListener.logEvent(EventLoggingListener.scala:135)
	at org.apache.spark.scheduler.EventLoggingListener.onApplicationEnd(EventLoggingListener.scala:170)
	at org.apache.spark.scheduler.SparkListenerBus$class.onPostEvent(SparkListenerBus.scala:54)
	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
	at org.apache.spark.scheduler.LiveListenerBus.onPostEvent(LiveListenerBus.scala:31)
	at org.apache.spark.util.ListenerBus$class.postToAll(ListenerBus.scala:53)
	at org.apache.spark.util.AsynchronousListenerBus.postToAll(AsynchronousListenerBus.scala:36)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(AsynchronousListenerBus.scala:76)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1$$anonfun$run$1.apply(AsynchronousListenerBus.scala:61)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1613)
	at org.apache.spark.util.AsynchronousListenerBus$$anon$1.run(AsynchronousListenerBus.scala:60)
Caused by: java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)
	at org.apache.hadoop.hdfs.DFSOutputStream.flushOrSync(DFSOutputStream.java:1843)
	at org.apache.hadoop.hdfs.DFSOutputStream.hflush(DFSOutputStream.java:1804)
	at org.apache.hadoop.fs.FSDataOutputStream.hflush(FSDataOutputStream.java:127)
	... 19 more
{code}
{code}
Exception in thread ""Thread-3"" java.io.IOException: Filesystem closed
	at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)
	at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1760)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1124)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1120)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1120)
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1398)
	at org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener.scala:187)
	at org.apache.spark.SparkContext$$anonfun$stop$4.apply(SparkContext.scala:1379)
	at org.apache.spark.SparkContext$$anonfun$stop$4.apply(SparkContext.scala:1379)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1379)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.stop(SparkSQLEnv.scala:66)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$$anon$1.run(SparkSQLCLIDriver.scala:107)
{code}
","Hadoop 2.4, YARN",apachespark,ashwinshankar77,cheolsoo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1304,SPARK-6445,SPARK-7865,SPARK-10358,,,,SPARK-25183,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 24 01:03:09 UTC 2015,,,,,,,,,,"0|i261z3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/15 19:38;apachespark;User 'piaozhexiu' has created a pull request for this issue:
https://github.com/apache/spark/pull/4771;;;","01/Mar/15 08:57;srowen;Although a fix is possible for Hadoop 2.2+, it is not clear there is any way to avoid a race with HDFS's shutdown hook before that. It would be moderately painful to solve this with reflection, and probably not worth it. This can be resolved with the approach in the PR above for 2.2+.;;;","17/Apr/15 18:44;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5560;;;","22/Apr/15 00:34;srowen;Issue resolved by pull request 5560
[https://github.com/apache/spark/pull/5560];;;","24/Apr/15 01:03;apachespark;User 'nishkamravi2' has created a pull request for this issue:
https://github.com/apache/spark/pull/5672;;;",,,,,,,,,,,,,,,,,,,,,,,,
Exception thrown when reading Spark SQL generated Parquet files with different but compatible schemas,SPARK-6010,12777608,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,25/Feb/15 17:19,25/Feb/15 23:21,14/Jul/23 06:27,25/Feb/15 23:21,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"The following test case added in {{ParquetPartitionDiscoverySuite}} can be used to reproduce this issue:
{code}
  test(""read partitioned table - merging compatible schemas"") {
    withTempDir { base =>
      makeParquetFile(
        (1 to 10).map(i => Tuple1(i)).toDF(""intField""),
        makePartitionDir(base, defaultPartitionName, ""pi"" -> 1))

      makeParquetFile(
        (1 to 10).map(i => (i, i.toString)).toDF(""intField"", ""stringField""),
        makePartitionDir(base, defaultPartitionName, ""pi"" -> 2))

      load(base.getCanonicalPath, ""org.apache.spark.sql.parquet"").registerTempTable(""t"")

      withTempTable(""t"") {
        checkAnswer(
          sql(""SELECT * FROM t""),
          (1 to 10).map(i => Row(i, null, 1)) ++ (1 to 10).map(i => Row(i, i.toString, 2)))
      }
    }
  }
{code}
Exception thrown:
{code}
[info]   java.lang.RuntimeException: could not merge metadata: key org.apache.spark.sql.parquet.row.metadata has conflicting values: [{""type"":""struct"",""fields"":[{""name"":""intField"",""type"":""integer"",""nullable"":false,""metadata"":{}},{""name"":""stringField"",""type"":""string"",""nullable"":true,""metadata"":{}}]}, {""type"":""struct"",""fields"":[{""name"":""intField"",""type"":""integer"",""nullable"":false,""metadata"":{}}]}]
[info]          at parquet.hadoop.api.InitContext.getMergedKeyValueMetaData(InitContext.java:67)
[info]          at parquet.hadoop.api.ReadSupport.init(ReadSupport.java:84)
[info]          at org.apache.spark.sql.parquet.FilteringParquetRowInputFormat.getSplits(ParquetTableOperations.scala:484)
[info]          at parquet.hadoop.ParquetInputFormat.getSplits(ParquetInputFormat.java:245)
[info]          at org.apache.spark.sql.parquet.ParquetRelation2$$anon$1.getPartitions(newParquet.scala:461)
[info]          at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
[info]          at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
[info]          at scala.Option.getOrElse(Option.scala:120)
[info]          at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
[info]          at org.apache.spark.rdd.NewHadoopRDD$NewHadoopMapPartitionsWithSplitRDD.getPartitions(NewHadoopRDD.scala:239)
[info]          at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
[info]          at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
[info]          at scala.Option.getOrElse(Option.scala:120)
[info]          at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
[info]          at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
[info]          at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
[info]          at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
[info]          at scala.Option.getOrElse(Option.scala:120)
[info]          at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
[info]          at org.apache.spark.SparkContext.runJob(SparkContext.scala:1518)
[info]          at org.apache.spark.rdd.RDD.collect(RDD.scala:813)
[info]          at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:83)
[info]          at org.apache.spark.sql.DataFrame.collect(DataFrame.scala:790)
[info]          at org.apache.spark.sql.QueryTest$.checkAnswer(QueryTest.scala:115)
[info]          at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:60)
[info]          at org.apache.spark.sql.parquet.ParquetPartitionDiscoverySuite$$anonfun$8$$anonfun$apply$mcV$sp$18$$anonfun$apply$8.apply$mcV$sp(ParquetPartitionDiscoverySuite.scala:337)
[info]          at org.apache.spark.sql.parquet.ParquetTest$class.withTempTable(ParquetTest.scala:112)
[info]          at org.apache.spark.sql.parquet.ParquetPartitionDiscoverySuite.withTempTable(ParquetPartitionDiscoverySuite.scala:35)
[info]          at org.apache.spark.sql.parquet.ParquetPartitionDiscoverySuite$$anonfun$8$$anonfun$apply$mcV$sp$18.apply(ParquetPartitionDiscoverySuite.scala:336)
[info]          at org.apache.spark.sql.parquet.ParquetPartitionDiscoverySuite$$anonfun$8$$anonfun$apply$mcV$sp$18.apply(ParquetPartitionDiscoverySuite.scala:325)
[info]          at org.apache.spark.sql.parquet.ParquetTest$class.withTempDir(ParquetTest.scala:82)
[info]          at org.apache.spark.sql.parquet.ParquetPartitionDiscoverySuite.withTempDir(ParquetPartitionDiscoverySuite.scala:35)
[info]          at org.apache.spark.sql.parquet.ParquetPartitionDiscoverySuite$$anonfun$8.apply$mcV$sp(ParquetPartitionDiscoverySuite.scala:325)
[info]          at org.apache.spark.sql.parquet.ParquetPartitionDiscoverySuite$$anonfun$8.apply(ParquetPartitionDiscoverySuite.scala:325)
[info]          at org.apache.spark.sql.parquet.ParquetPartitionDiscoverySuite$$anonfun$8.apply(ParquetPartitionDiscoverySuite.scala:325)
[info]          at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]          at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]          at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]          at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]          at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]          at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]          at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]          at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]          at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]          at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]          at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]          at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]          at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]          at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]          at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]          at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]          at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]          at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]          at scala.collection.immutable.List.foreach(List.scala:318)
[info]          at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]          at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]          at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]          at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]          at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]          at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]          at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]          at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]          at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]          at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]          at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]          at org.scalatest.FunSuite.run(FunSuite.scala:1555)
[info]          at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]          at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]          at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]          at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]          at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]          at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info]          at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info]          at java.lang.Thread.run(Thread.java:745) (QueryTest.scala:61)
{code}",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 17:27:37 UTC 2015,,,,,,,,,,"0|i261pz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"25/Feb/15 17:27;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4768;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add numRows param in DataFrame.show,SPARK-6007,12777526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jackylk,jackylk,jackylk,25/Feb/15 13:11,26/Feb/15 18:43,14/Jul/23 06:27,26/Feb/15 18:43,1.2.1,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Currently, DataFrame.show only takes 20 rows to show, it will be useful if the user can decide how many rows to show by passing it as a parameter in show()",,apachespark,jackylk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 13:21:02 UTC 2015,,,,,,,,,,"0|i2617r:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"25/Feb/15 13:21;apachespark;User 'jackylk' has created a pull request for this issue:
https://github.com/apache/spark/pull/4767;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.streaming.kafka.DirectKafkaStreamSuite.offset recovery,SPARK-6005,12777489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,dragos,dragos,25/Feb/15 11:00,22/Jun/16 13:11,14/Jul/23 06:27,10/May/16 20:27,,,,,,,1.6.3,2.0.0,,,,,DStreams,,,,0,flaky-test,kafka,streaming,,,"[Link to failing test on Jenkins|https://ci.typesafe.com/view/Spark/job/spark-nightly-build/lastCompletedBuild/testReport/org.apache.spark.streaming.kafka/DirectKafkaStreamSuite/offset_recovery/]

{code}
The code passed to eventually never returned normally. Attempted 208 times over 10.00622791 seconds. Last failure message: strings.forall({   ((elem: Any) => DirectKafkaStreamSuite.collectedData.contains(elem)) }) was false.
{code}

{code:title=Stack trace}
sbt.ForkMain$ForkError: The code passed to eventually never returned normally. Attempted 208 times over 10.00622791 seconds. Last failure message: strings.forall({
  ((elem: Any) => DirectKafkaStreamSuite.collectedData.contains(elem))
}) was false.
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:420)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:438)
	at org.apache.spark.streaming.kafka.KafkaStreamSuiteBase.eventually(KafkaStreamSuite.scala:49)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:307)
	at org.apache.spark.streaming.kafka.KafkaStreamSuiteBase.eventually(KafkaStreamSuite.scala:49)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$5.org$apache$spark$streaming$kafka$DirectKafkaStreamSuite$$anonfun$$sendDataAndWaitForReceive$1(DirectKafkaStreamSuite.scala:225)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$5.apply$mcV$sp(DirectKafkaStreamSuite.scala:287)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$5.apply(DirectKafkaStreamSuite.scala:211)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$5.apply(DirectKafkaStreamSuite.scala:211)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.org$scalatest$BeforeAndAfter$$super$runTest(DirectKafkaStreamSuite.scala:39)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.runTest(DirectKafkaStreamSuite.scala:39)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.org$scalatest$BeforeAndAfter$$super$run(DirectKafkaStreamSuite.scala:39)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.org$scalatest$BeforeAndAfterAll$$super$run(DirectKafkaStreamSuite.scala:39)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.run(DirectKafkaStreamSuite.scala:39)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: sbt.ForkMain$ForkError: strings.forall({
  ((elem: Any) => DirectKafkaStreamSuite.collectedData.contains(elem))
}) was false
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$5$$anonfun$org$apache$spark$streaming$kafka$DirectKafkaStreamSuite$$anonfun$$sendDataAndWaitForReceive$1$1.apply$mcV$sp(DirectKafkaStreamSuite.scala:226)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$5$$anonfun$org$apache$spark$streaming$kafka$DirectKafkaStreamSuite$$anonfun$$sendDataAndWaitForReceive$1$1.apply(DirectKafkaStreamSuite.scala:226)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$5$$anonfun$org$apache$spark$streaming$kafka$DirectKafkaStreamSuite$$anonfun$$sendDataAndWaitForReceive$1$1.apply(DirectKafkaStreamSuite.scala:226)
	at org.scalatest.concurrent.Eventually$class.makeAValiantAttempt$1(Eventually.scala:394)
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:408)
	... 54 more
{code}",,apachespark,dragos,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed May 04 18:38:05 UTC 2016,,,,,,,,,,"0|i260zr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/15 11:02;srowen;Looks virtually identical to SPARK-5731; same issue or just closely related?;;;","25/Feb/15 14:29;dragos;Looks similar, but unless I miss something, that fix didn't fix this one. The failure I report is from a recent build that includes the fixes in 3912d332464dcd124c60b734724c34d9742466a4
;;;","09/Mar/15 23:09;tdas;[~cody@koeninger.org] Can you check this out?;;;","19/Jan/16 14:05;srowen;Given lack of updates, and that I can't recall seeing this fail in recent memory, resolving.;;;","04/May/16 18:38;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/12903;;;",,,,,,,,,,,,,,,,,,,,,,,,
DataFrame.collect() doesn't recognize UDTs,SPARK-5996,12777375,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,mengxr,mengxr,25/Feb/15 01:03,25/Feb/15 18:14,14/Jul/23 06:27,25/Feb/15 18:14,1.3.0,,,,,,1.3.0,,,,,,MLlib,SQL,,,0,,,,,,"{code}
import org.apache.spark.mllib.linalg._
case class Test(data: Vector)
val df = sqlContext.createDataFrame(Seq(Test(Vectors.dense(1.0, 2.0))))
df.collect()[0].getAs[Vector](0)
{code}

throws an exception. `collect()` returns `Row` instead of `Vector`.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 03:49:39 UTC 2015,,,,,,,,,,"0|i260a7:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"25/Feb/15 03:49;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4757;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Published Kafka-assembly JAR was empty in 1.3.0-RC1,SPARK-5993,12777365,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,25/Feb/15 00:44,28/Feb/15 17:18,14/Jul/23 06:27,28/Feb/15 17:18,,,,,,,1.3.0,,,,,,Build,DStreams,,,0,,,,,,"This is because the maven build generated two Jars-
1. an empty JAR file (since kafka-assembly has no code of its own)
2. a assembly JAR file containing everything in a different location as 1

The maven publishing plugin uploaded 1 and not 2. 

Instead if 2 is not configure to generate in a different location, there is only 1 jar containing everything, which gets published.",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 28 17:18:07 UTC 2015,,,,,,,,,,"0|i2607z:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"25/Feb/15 00:45;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/4753;;;","28/Feb/15 17:18;srowen;Looks like this was resolved in https://github.com/apache/spark/pull/4753;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
TimSort broken,SPARK-5984,12777335,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hotou,rxin,rxin,24/Feb/15 22:54,26/Feb/17 09:21,14/Jul/23 06:27,01/Mar/15 02:57,1.1.0,1.1.1,1.2.0,1.2.1,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"See http://envisage-project.eu/proving-android-java-and-python-sorting-algorithm-is-broken-and-how-to-fix-it/

Our TimSort is based on Android's TimSort, which is broken in some corner case. Marking it minor as this problem exists for almost all TimSort implementations out there, including Android, OpenJDK, Python, and it hasn't manifested itself in practice yet.",,apachespark,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 26 09:21:03 UTC 2017,,,,,,,,,,"0|i2601b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/15 04:33;apachespark;User 'hotou' has created a pull request for this issue:
https://github.com/apache/spark/pull/4804;;;","26/Feb/17 09:21;apachespark;User 'xiaoyesoso' has created a pull request for this issue:
https://github.com/apache/spark/pull/17073;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Local Read Time,SPARK-5982,12777320,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kayousterhout,kayousterhout,kayousterhout,24/Feb/15 21:51,25/Feb/15 23:01,14/Jul/23 06:27,25/Feb/15 23:01,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"LocalReadTime was added to TaskMetrics for the 1.3.0 release.  However, this time is actually only a small subset of the local read time, because local shuffle files are memory mapped, so most of the read time occurs later, as data is read from the memory mapped files and the data actually gets read from disk.  We should remove this before the 1.3.0 release, so we never expose this incomplete info.",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 24 22:25:37 UTC 2015,,,,,,,,,,"0|i25zy7:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"24/Feb/15 22:25;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/4749;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
`--packages` should not exclude spark streaming assembly jars for kafka and flume ,SPARK-5979,12777307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,brkyvz,brkyvz,brkyvz,24/Feb/15 21:35,03/Feb/19 20:57,14/Jul/23 06:27,28/Feb/15 07:00,1.3.0,,,,,,1.3.0,,,,,,Deploy,Spark Submit,,,0,,,,,,Currently `--packages` has an exclude rule for all dependencies with the groupId `org.apache.spark` assuming that these are packaged inside the spark-assembly jar. This is not the case and more fine grained filtering is required.,,apachespark,brkyvz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 27 02:28:13 UTC 2015,,,,,,,,,,"0|i25zvb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"25/Feb/15 01:49;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/4754;;;","27/Feb/15 02:28;apachespark;User 'brkyvz' has created a pull request for this issue:
https://github.com/apache/spark/pull/4802;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Factors returned by ALS do not have partitioners associated.,SPARK-5976,12777282,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,24/Feb/15 20:16,26/Feb/15 07:43,14/Jul/23 06:27,26/Feb/15 07:43,1.3.0,,,,,,1.3.0,,,,,,ML,MLlib,,,0,,,,,,"The model trained by ALS requires partitioning information to do quick lookup of a user/item factor for making recommendation on individual requests. In the new implementation, we don't put partitioners with ALS, which would cause performance regression.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 07:43:45 UTC 2015,,,,,,,,,,"0|i25zq7:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"24/Feb/15 21:11;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4748;;;","26/Feb/15 07:43;mengxr;Issue resolved by pull request 4748
[https://github.com/apache/spark/pull/4748];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
zip two rdd with AutoBatchedSerializer will fail,SPARK-5973,12777250,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,24/Feb/15 18:57,25/Feb/15 17:11,14/Jul/23 06:27,24/Feb/15 22:52,1.2.1,1.3.0,,,,,1.2.2,1.3.0,,,,,PySpark,,,,0,,,,,,"zip two rdd with AutoBatchedSerializer will fail, this bug was introduced by SPARK-4841

{code}
>> a.zip(b).count()
15/02/24 12:11:56 ERROR PythonRDD: Python worker exited unexpectedly (crashed)
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
  File ""/Users/davies/work/spark/python/pyspark/worker.py"", line 101, in main
    process()
  File ""/Users/davies/work/spark/python/pyspark/worker.py"", line 96, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/Users/davies/work/spark/python/pyspark/rdd.py"", line 2249, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/Users/davies/work/spark/python/pyspark/rdd.py"", line 2249, in pipeline_func
    return func(split, prev_func(split, iterator))
  File ""/Users/davies/work/spark/python/pyspark/rdd.py"", line 270, in func
    return f(iterator)
  File ""/Users/davies/work/spark/python/pyspark/rdd.py"", line 933, in <lambda>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File ""/Users/davies/work/spark/python/pyspark/rdd.py"", line 933, in <genexpr>
    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()
  File ""/Users/davies/work/spark/python/pyspark/serializers.py"", line 306, in load_stream
    "" in pair: (%d, %d)"" % (len(keys), len(vals)))
ValueError: Can not deserialize RDD with different number of items in pair: (123, 64)
{code}",,apachespark,cchayden,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6008,,,,,SPARK-4841,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 24 21:19:36 UTC 2015,,,,,,,,,,"0|i25zj3:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"24/Feb/15 19:06;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4745;;;","24/Feb/15 21:19;joshrosen;Just for searchability's sake, what error message did this fail with before?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Temporary directories are not removed (but their content is),SPARK-5970,12777140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,straka,straka,straka,24/Feb/15 09:41,08/Jul/15 09:53,14/Jul/23 06:27,25/Feb/15 21:34,1.2.1,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"How to reproduce: 
- extract spark-1.2.1-bin-hadoop2.4.tgz
- without any further configuration, run bin/pyspark
- run sc.stop() and close python shell

Expected results:
- no temporary directories are left in /tmp

Actual results:
- four empty temporary directories are created in /tmp, for example after {{ls -d /tmp/spark*}}:{code}
/tmp/spark-1577b13d-4b9a-4e35-bac2-6e84e5605f53
/tmp/spark-96084e69-77fd-42fb-ab10-e1fc74296fe3
/tmp/spark-ab2ea237-d875-485e-b16c-5b0ac31bd753
/tmp/spark-ddeb0363-4760-48a4-a189-81321898b146
{code}

The issue is caused by changes in {{util/Utils.scala}}. Consider the {{createDirectory}}:
{code}  /**
   * Create a directory inside the given parent directory. The directory is guaranteed to be
   * newly created, and is not marked for automatic deletion.
   */
  def createDirectory(root: String, namePrefix: String = ""spark""): File = ...
{code}

The {{createDirectory}} is used in two places. The first is in {{createTempDir}}, where it is marked for automatic deletion:
{code}
  def createTempDir(
      root: String = System.getProperty(""java.io.tmpdir""),
      namePrefix: String = ""spark""): File = {
    val dir = createDirectory(root, namePrefix)
    registerShutdownDeleteDir(dir)
    dir
  }
{code}

Nevertheless, it is also used in {{getOrCreateLocalDirs}} where it is _not_ marked for automatic deletion:
{code}
  private[spark] def getOrCreateLocalRootDirs(conf: SparkConf): Array[String] = {
    if (isRunningInYarnContainer(conf)) {
      // If we are in yarn mode, systems can have different disk layouts so we must set it
      // to what Yarn on this system said was available. Note this assumes that Yarn has
      // created the directories already, and that they are secured so that only the
      // user has access to them.
      getYarnLocalDirs(conf).split("","")
    } else {
      // In non-Yarn mode (or for the driver in yarn-client mode), we cannot trust the user
      // configuration to point to a secure directory. So create a subdirectory with restricted
      // permissions under each listed directory.
      Option(conf.getenv(""SPARK_LOCAL_DIRS""))
        .getOrElse(conf.get(""spark.local.dir"", System.getProperty(""java.io.tmpdir"")))
        .split("","")
        .flatMap { root =>
          try {
            val rootDir = new File(root)
            if (rootDir.exists || rootDir.mkdirs()) {
              Some(createDirectory(root).getAbsolutePath())
            } else {
              logError(s""Failed to create dir in $root. Ignoring this directory."")
              None
            }
          } catch {
            case e: IOException =>
            logError(s""Failed to create local root dir in $root. Ignoring this directory."")
            None
          }
        }
        .toArray
    }
  }
{code}

Therefore I think the
{code}
Some(createDirectory(root).getAbsolutePath())
{code}
should be replaced by something like (I am not an experienced Scala programmer):
{code}
val dir = createDirectory(root)
registerShutdownDeleteDir(dir)
Some(dir.getAbsolutePath())
{code}","Linux, 64bit
spark-1.2.1-bin-hadoop2.4.tgz",apachespark,michaelmalak,straka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-7917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 21:34:15 UTC 2015,,,,,,,,,,"0|i25yvb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/15 11:08;srowen;I believe that's right. Would you like to open a PR?;;;","25/Feb/15 06:12;straka;I found a _Contributing to Spark_ guide, will do so.;;;","25/Feb/15 06:57;apachespark;User 'foxik' has created a pull request for this issue:
https://github.com/apache/spark/pull/4759;;;","25/Feb/15 21:34;srowen;Issue resolved by pull request 4759
[https://github.com/apache/spark/pull/4759];;;",,,,,,,,,,,,,,,,,,,,,,,,,
The pyspark.rdd.sortByKey always fills only two partitions when ascending=False.,SPARK-5969,12777133,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,straka,straka,straka,24/Feb/15 09:04,10/Apr/15 22:25,14/Jul/23 06:27,10/Apr/15 20:52,1.0.2,1.1.1,1.2.1,1.3.1,,,1.2.3,1.3.2,1.4.0,,,,PySpark,,,,1,,,,,,"The pyspark.rdd.sortByKey always fills only two partitions when ascending=False -- the first one and the last one.

Simple example sorting numbers 0..999 into 10 partitions in descending order:
{code}
sc.parallelize(range(1000)).keyBy(lambda x:x).sortByKey(ascending=False, numPartitions=10).glom().map(len).collect()
{code}
returns the following partition sizes:
{code}
[469, 0, 0, 0, 0, 0, 0, 0, 0, 531]
{code}

When ascending=True, all works as expected:
{code}
sc.parallelize(range(1000)).keyBy(lambda x:x).sortByKey(ascending=True, numPartitions=10).glom().map(len).collect()
Out: [116, 96, 100, 87, 132, 101, 101, 95, 87, 85]
{code}

The problem is caused by the following line 565 in rdd.py:
{code}
        samples = sorted(samples, reverse=(not ascending), key=keyfunc)
{code}
That sorts the samples descending if ascending=False. Nevertheless samples should always be in ascending order, because it is (after subsampling to variable bounds) used in a bisect_left call:
{code}
        def rangePartitioner(k):
            p = bisect.bisect_left(bounds, keyfunc(k))
            if ascending:
                return p
            else:
                return numPartitions - 1 - p
{code}

As you can see, rangePartitioner already handles the ascending=False by itself, so the fix for the whole problem is really trivial: just change line rdd.py:565 to
{{samples = sorted(samples, -reverse=(not ascending),- key=keyfunc)}}
","Linux, 64bit",apachespark,joshrosen,mkman84,straka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 20:52:56 UTC 2015,,,,,,,,,,"0|i25ytr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/15 07:08;apachespark;User 'foxik' has created a pull request for this issue:
https://github.com/apache/spark/pull/4761;;;","10/Apr/15 20:52;joshrosen;Issue resolved by pull request 4761
[https://github.com/apache/spark/pull/4761];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Parquet warning in spark-shell,SPARK-5968,12777074,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,marmbrus,marmbrus,24/Feb/15 07:35,01/Dec/15 21:35,14/Jul/23 06:27,24/Feb/15 18:49,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"This may happen in the case of schema evolving, namely appending new Parquet data with different but compatible schema to existing Parquet files:
{code}
15/02/23 23:29:24 WARN ParquetOutputCommitter: could not write summary file for rankings
parquet.io.ParquetEncodingException: file:/Users/matei/workspace/apache-spark/rankings/part-r-00001.parquet invalid: all the files must be contained in the root rankings
at parquet.hadoop.ParquetFileWriter.mergeFooters(ParquetFileWriter.java:422)
at parquet.hadoop.ParquetFileWriter.writeMetadataFile(ParquetFileWriter.java:398)
at parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:51)
{code}
The reason is that the Spark SQL schemas stored in Parquet key-value metadata differ. Parquet doesn't know how to ""merge"" these opaque user-defined metadata, and just throw an exception and give up writing summary files. Since the Parquet data source in Spark 1.3.0 supports schema merging, it's harmless.  But this is kind of scary for the user.  We should try to suppress this through the logger. ",,apachespark,kotime42@gmail.com,lian cheng,marmbrus,swethakasireddy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Dec 01 21:35:22 UTC 2015,,,,,,,,,,"0|i25ypj:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"24/Feb/15 17:09;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4744;;;","24/Feb/15 18:49;marmbrus;Issue resolved by pull request 4744
[https://github.com/apache/spark/pull/4744];;;","09/Nov/15 07:45;swethakasireddy;[~marmbrus]

How is this issue resolved? I still see the following issue when I try to save my parquet file.

Nov 8, 2015 11:35:39 PM WARNING: parquet.hadoop.ParquetOutputCommitter: could not write summary file for active_sessions_current 
parquet.io.ParquetEncodingException: maprfs:/user/testId/active_sessions_current/part-r-00142.parquet invalid: all the files must be contained in the root active_sessions_current 
        at parquet.hadoop.ParquetFileWriter.mergeFooters(ParquetFileWriter.java:422) 
        at parquet.hadoop.ParquetFileWriter.writeMetadataFile(ParquetFileWriter.java:398) 
        at parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:51) 
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1056) 
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsNewAPIHadoopDataset$1.apply(PairRDDFunctions.scala:998);;;","11/Nov/15 10:32;lian cheng;It had once been fixed via a quite hacky trick. Unfortunately it came back again after upgrading to parquet-mr 1.7.0, and there doesn't seem to be a reliable way to override the log settings because of PARQUET-369, which prevents users or other libraries to redirect Parquet JUL logger via SLF4J. It's fixed in the most recent parquet-format master, but I'm afraid we have to wait for another parquet-format and parquet-mr release to fix this issue completely.;;;","11/Nov/15 23:00;swethakasireddy;[~lian cheng]

Is this just a logger issue or would it have any potential impact on the functionality?;;;","12/Nov/15 09:26;lian cheng;As explained in the JIRA description, this issue shouldn't affect functionality.;;;","01/Dec/15 21:35;swethakasireddy;[~lian cheng]

Following are the dependencies and the versions that I am using. I want to know if using a different version would be of any help to fix this.  I see this error in my Spark Batch Job when I save the Parquet files to hdfs.

        <sparkVersion>1.5.2</sparkVersion>
        <avro.version>1.7.7</avro.version>
        <parquet.version>1.4.3</parquet.version>

        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.10</artifactId>
            <version>${sparkVersion}</version>
            <scope>provided</scope>
        </dependency>


        <dependency>
            <groupId>org.apache.avro</groupId>
            <artifactId>avro</artifactId>
            <version>${avro.version}</version>
        </dependency>

        <dependency>
            <groupId>com.twitter</groupId>
            <artifactId>parquet-avro</artifactId>
            <version>1.6.0rc7</version>
        </dependency>

        <dependency>
            <groupId>com.twitter</groupId>
            <artifactId>parquet-hadoop</artifactId>
            <version>1.6.0rc7</version>
        </dependency>

;;;",,,,,,,,,,,,,,,,,,,,,,
JobProgressListener.stageIdToActiveJobIds never cleared,SPARK-5967,12777067,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,24/Feb/15 06:48,17/Apr/15 18:39,14/Jul/23 06:27,24/Feb/15 19:03,1.2.0,1.2.1,1.3.0,,,,1.2.2,1.3.0,,,,,DStreams,Web UI,,,0,,,,,,The hashmap stageIdToActiveJobIds in JobProgressListener is never cleared. So the hashmap keep increasing in size indefinitely. This is a blocker for 24/7 running applications like Spark Streaming apps.,,apachespark,cdouglas,rssvihla,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 24 12:47:17 UTC 2015,,,,,,,,,,"0|i25ynz:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"24/Feb/15 07:31;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/4741;;;","24/Feb/15 09:56;srowen;This might be a duplicate of this issue: https://issues.apache.org/jira/browse/SPARK-4906
Not sure if it's related, but there's also: https://issues.apache.org/jira/browse/SPARK-3882;;;","24/Feb/15 12:41;tdas;They are similar but not the same. Those were bugs in the old code, and this bug is in the new refactored JobProgressListener. This refactoring and hence the bug was done in Spark 1.2;;;","24/Feb/15 12:47;srowen;[~tdas] can one or both of those be resolved as obsolete then?;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Spark-submit deploy-mode incorrectly affecting submission when master = local[4] ,SPARK-5966,12777047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kevinyu98,tdas,tdas,24/Feb/15 04:32,26/Oct/15 17:28,14/Jul/23 06:27,26/Oct/15 09:36,1.3.0,,,,,,1.5.3,1.6.0,,,,,Spark Submit,,,,0,,,,,,"{code}

[tdas @ Zion spark] bin/spark-submit --master local[10]  --class test.MemoryTest --driver-memory 1G /Users/tdas/Projects/Others/simple-project/target/scala-2.10/simple-app-assembly-0.1-SNAPSHOT.jar
JVM arguments: [-XX:MaxPermSize=128m, -Xms1G, -Xmx1G]
App arguments:
Usage: MemoryTest <allocation size in MB> <interval in seconds>

[tdas @ Zion spark] bin/spark-submit --master local[10] --deploy-mode cluster  --class test.MemoryTest --driver-memory 1G /Users/tdas/Projects/Others/simple-project/target/scala-2.10/simple-app-assembly-0.1-SNAPSHOT.jar
java.lang.ClassNotFoundException:
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:270)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:538)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}",,andrewor14,apachespark,dkbiswal,kevinyu98,rhillegas,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Oct 26 16:34:36 UTC 2015,,,,,,,,,,"0|i25yjj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/15 19:18;andrewor14;You're not supposed to run with cluster deploy mode using a local master;;;","20/May/15 19:51;tdas;Yes, but its super non-intuitive error message when you accidentally do so. There should something like a IllegalArgumentException rather than ClassNotFoundException.;;;","22/Oct/15 07:35;kevinyu98;Hello Tathagata & Andrew:
I have code a possible fix, and the error message will be like this.
$ ./bin/spark-submit --master local[10] --deploy-mode cluster examples/src/main/python/pi.py
Error: Cluster deploy mode is not compatible with master ""local""
Run with --help for usage help or --verbose for debug output

Let me know if you have any comments. Otherwise, I am going to submit a PR shortly. Thanks.

Kevin
;;;","22/Oct/15 07:58;apachespark;User 'kevinyu98' has created a pull request for this issue:
https://github.com/apache/spark/pull/9220;;;","26/Oct/15 09:36;srowen;Issue resolved by pull request 9220
[https://github.com/apache/spark/pull/9220];;;","26/Oct/15 16:34;rhillegas;Should this issue be assigned to Kevin now, since he submitted the pull request? Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,
Spark UI does not show main class when running app in standalone cluster mode,SPARK-5965,12777046,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,tdas,tdas,24/Feb/15 04:29,24/Feb/15 19:08,14/Jul/23 06:27,24/Feb/15 19:08,1.3.0,,,,,,1.3.0,,,,,,Deploy,,,,0,,,,,,"I tried this.

bin/spark-submit --verbose --supervise --master spark://Zion.local:7077 --deploy-mode cluster  --class test.MemoryTest --driver-memory 1G /Users/tdas/Projects/Others/simple-project/target/scala-2.10/simple-app-assembly-0.1-SNAPSHOT.jar 100 0.5

The app got launched correctly but the Spark web UI showed the following screenshot",,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/15 04:30;tdas;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12700335/screenshot-1.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 24 06:40:18 UTC 2015,,,,,,,,,,"0|i25yjb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"24/Feb/15 06:40;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4739;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to lock metastore client in tableExists(),SPARK-5952,12776932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,23/Feb/15 20:15,24/Feb/15 21:40,14/Jul/23 06:27,24/Feb/15 21:40,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 24 21:40:27 UTC 2015,,,,,,,,,,"0|i25xun:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"24/Feb/15 19:47;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4746;;;","24/Feb/15 21:40;marmbrus;Issue resolved by pull request 4746
[https://github.com/apache/spark/pull/4746];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unreachable driver memory properties in yarn client mode (YarnClientSchedulerBackend),SPARK-5951,12776889,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sb58,sb58,sb58,23/Feb/15 17:46,26/Feb/15 22:29,14/Jul/23 06:27,26/Feb/15 22:29,1.3.0,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"In SPARK-4730 warning for deprecated was added
and in SPARK-1953 driver memory configs were removed in yarn client mode

During integration spark.master.memory and SPARK_MASTER_MEMORY were not removed.",yarn,apachespark,sandyr,sb58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 17:49:32 UTC 2015,,,,,,,,,,"0|i25xlb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"23/Feb/15 17:49;apachespark;User 'zuxqoj' has created a pull request for this issue:
https://github.com/apache/spark/pull/4730;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insert array into a metastore table saved as parquet should work when using datasource api,SPARK-5950,12776879,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,viirya,viirya,23/Feb/15 17:05,24/Apr/15 00:45,14/Jul/23 06:27,03/Mar/15 03:34,,,,,,,1.3.0,,,,,,SQL,,,,1,,,,,,,,apachespark,avignon,Ayoub,glenn.strycker@gmail.com,marmbrus,viirya,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5508,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 03:34:05 UTC 2015,,,,,,,,,,"0|i25xjb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"23/Feb/15 17:20;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/4729;;;","26/Feb/15 18:23;yhuai;Seems the problem is that arrays and maps stored with Hive Parquet SerDe may not be able to read by ParquetRelation2 due to issue of the setting of the nullability flag. ;;;","27/Feb/15 01:23;yhuai;OK. Now, I understand what's going on.  For this JIRA, the table is a MetastoreRelation and we are trying to use data source API's write path to insert into it. For a MetastoreRelation, containsNull, valueContainsNull and nullable will always be true. When we try to insert into this table through the data source write path, if any of containsNull/valueContainsNull/nullable is false, InsertIntoTable will not be resolved because of the nullability issue.;;;","27/Feb/15 01:28;viirya;Yes. This is just the part of the problem. containsNull/valueContainsNull is most the problem. nullable should not be a problem.;;;","27/Feb/15 01:31;yhuai;If it is just the part of the problem, mind explain what is the problem?;;;","27/Feb/15 01:36;viirya;Let me explain on github pull.;;;","28/Feb/15 05:53;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4826;;;","03/Mar/15 03:34;marmbrus;Issue resolved by pull request 4826
[https://github.com/apache/spark/pull/4826];;;",,,,,,,,,,,,,,,,,,,,,
Driver program has to register roaring bitmap classes used by spark with Kryo when number of partitions is greater than 2000,SPARK-5949,12776848,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,irashid,ptorok,ptorok,23/Feb/15 14:35,09/Oct/15 03:11,14/Jul/23 06:27,03/Mar/15 23:33,1.2.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,kryo,partitioning,serialization,,,"When more than 2000 partitions are being used with Kryo, the following classes need to be registered by driver program:
- org.apache.spark.scheduler.HighlyCompressedMapStatus
- org.roaringbitmap.RoaringBitmap
- org.roaringbitmap.RoaringArray
- org.roaringbitmap.ArrayContainer
- org.roaringbitmap.RoaringArray$Element
- org.roaringbitmap.RoaringArray$Element[]
- short[]

Our project doesn't have dependency on roaring bitmap and HighlyCompressedMapStatus is intended for internal spark usage. Spark should take care of this registration when Kryo is used.",,apachespark,drcrallen,irashid,lemire,pberline,ptorok,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 09 03:11:24 UTC 2015,,,,,,,,,,"0|i25xcf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/15 16:14;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/4877;;;","03/Mar/15 17:33;ptorok;Actually, there were two more classes discovered to be registered when running massive jobs:
- long[]
- org.roaringbitmap.BitmapContainer

Could you please add these two classes in order to avoid further issues?

Thanks,
;;;","03/Mar/15 20:38;irashid;Thanks [~ptorok].  I've updated the PR.  Can you take a quick look?  I came up with a better test case that covered both types of containers.

Also, would you mind posting a bit of the error you get here as well?  it might help other users discover this issue and see the fix.;;;","04/Mar/15 10:26;ptorok;Hi Imran,

I'm getting the following error if a class is not registered with kryo:

java.lang.IllegalArgumentException: Class is not registered: org.apache.spark.scheduler.HighlyCompressedMapStatus
Note: To register this class use: kryo.register(org.apache.spark.scheduler.HighlyCompressedMapStatus.class);
	at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:442)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:79)
...

In this case, the registrator class needs to be extended by registering the mentioned class. If more classes are involved this need to be done iteratively while getting the exception.

Regarding to the PR, I don't see the Array[Long] is registered. If it's registered elsewhere, it's fine. If not, it should be added.

Thanks,
Peter;;;","04/Mar/15 12:20;irashid;Thanks Peter.  {{Array\[Long\]}} is there, don't worry: https://github.com/apache/spark/pull/4877/files#diff-1f81c62dad0e2dfc387a974bb08c497cR216

:);;;","08/Oct/15 22:26;drcrallen;This breaks when using more recent versions of Roaring where org.roaringbitmap.RoaringArray$Element is no longer present. The following stack trace appears:

{code}
A needed class was not found. This could be due to an error in your runpath. Missing class: org/roaringbitmap/RoaringArray$Element
java.lang.NoClassDefFoundError: org/roaringbitmap/RoaringArray$Element
	at org.apache.spark.serializer.KryoSerializer$.<init>(KryoSerializer.scala:338)
	at org.apache.spark.serializer.KryoSerializer$.<clinit>(KryoSerializer.scala)
	at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:93)
	at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:237)
	at org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:222)
	at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:138)
	at org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:201)
	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:102)
	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:85)
	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:63)
	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1318)
	at org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply(SparkContext.scala:1006)
	at org.apache.spark.SparkContext$$anonfun$hadoopFile$1.apply(SparkContext.scala:1003)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.SparkContext.withScope(SparkContext.scala:700)
	at org.apache.spark.SparkContext.hadoopFile(SparkContext.scala:1003)
	at org.apache.spark.SparkContext$$anonfun$textFile$1.apply(SparkContext.scala:818)
	at org.apache.spark.SparkContext$$anonfun$textFile$1.apply(SparkContext.scala:816)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.SparkContext.withScope(SparkContext.scala:700)
	at org.apache.spark.SparkContext.textFile(SparkContext.scala:816)
	at io.druid.indexer.spark.SparkDruidIndexer$$anonfun$2.apply(SparkDruidIndexer.scala:84)
	at io.druid.indexer.spark.SparkDruidIndexer$$anonfun$2.apply(SparkDruidIndexer.scala:84)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at io.druid.indexer.spark.SparkDruidIndexer$.loadData(SparkDruidIndexer.scala:84)
	at io.druid.indexer.spark.TestSparkDruidIndexer$$anonfun$1.apply$mcV$sp(TestSparkDruidIndexer.scala:131)
	at io.druid.indexer.spark.TestSparkDruidIndexer$$anonfun$1.apply(TestSparkDruidIndexer.scala:40)
	at io.druid.indexer.spark.TestSparkDruidIndexer$$anonfun$1.apply(TestSparkDruidIndexer.scala:40)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FlatSpecLike$$anon$1.apply(FlatSpecLike.scala:1647)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FlatSpec.withFixture(FlatSpec.scala:1683)
	at org.scalatest.FlatSpecLike$class.invokeWithFixture$1(FlatSpecLike.scala:1644)
	at org.scalatest.FlatSpecLike$$anonfun$runTest$1.apply(FlatSpecLike.scala:1656)
	at org.scalatest.FlatSpecLike$$anonfun$runTest$1.apply(FlatSpecLike.scala:1656)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FlatSpecLike$class.runTest(FlatSpecLike.scala:1656)
	at org.scalatest.FlatSpec.runTest(FlatSpec.scala:1683)
	at org.scalatest.FlatSpecLike$$anonfun$runTests$1.apply(FlatSpecLike.scala:1714)
	at org.scalatest.FlatSpecLike$$anonfun$runTests$1.apply(FlatSpecLike.scala:1714)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:390)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:427)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FlatSpecLike$class.runTests(FlatSpecLike.scala:1714)
	at org.scalatest.FlatSpec.runTests(FlatSpec.scala:1683)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FlatSpec.org$scalatest$FlatSpecLike$$super$run(FlatSpec.scala:1683)
	at org.scalatest.FlatSpecLike$$anonfun$run$1.apply(FlatSpecLike.scala:1760)
	at org.scalatest.FlatSpecLike$$anonfun$run$1.apply(FlatSpecLike.scala:1760)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FlatSpecLike$class.run(FlatSpecLike.scala:1760)
	at org.scalatest.FlatSpec.run(FlatSpec.scala:1683)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
	at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
	at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
	at org.scalatest.tools.Runner$.run(Runner.scala:883)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:138)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:28)
Caused by: java.lang.ClassNotFoundException: org.roaringbitmap.RoaringArray$Element
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 84 more
{code};;;","08/Oct/15 22:30;drcrallen;[~lemire] pinging to see if you have any suggestions on how to handle situations like this.;;;","09/Oct/15 03:11;lemire;@drcallen Thanks for pinging me.

We will be glad to help address any issue you have.

The Element class was removed entirely with the release of Roaring version 0.5.0 as a way to save memory (each Element instance used 24 bytes). 

https://github.com/lemire/RoaringBitmap/issues/31

This change did not affect the public API. (Element as ""protected"".) For obvious reasons, I recommend against introducing dependencies against non-public classes or function classes.


The recommended way to persist a RoaringBitmap (or a MutableRoaringBitmap) is in this manner:

        // r is my bitmap
        r.runOptimize(); //if needed, to improve compression (new with version 0.5.x)
        r.serialize(DataOutput ... );

Then one can deserialize it, or just map it (using the ImmutableRoaringBitmap class). We are committed to preserving the data format produced by ""serialize"" across versions. 

;;;",,,,,,,,,,,,,,,,,,,,,
Spark should not retry a stage infinitely on a FetchFailedException,SPARK-5945,12776752,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,ilganeli,irashid,irashid,23/Feb/15 01:48,03/Sep/15 05:08,14/Jul/23 06:27,03/Sep/15 05:08,,,,,,,1.6.0,,,,,,Spark Core,,,,4,,,,,,"While investigating SPARK-5928, I noticed some very strange behavior in the way spark retries stages after a FetchFailedException.  It seems that on a FetchFailedException, instead of simply killing the task and retrying, Spark aborts the stage and retries.  If it just retried the task, the task might fail 4 times and then trigger the usual job killing mechanism.  But by killing the stage instead, the max retry logic is skipped (it looks to me like there is no limit for retries on a stage).

After a bit of discussion with Kay Ousterhout, it seems the idea is that if a fetch fails, we assume that the block manager we are fetching from has failed, and that it will succeed if we retry the stage w/out that block manager.  In that case, it wouldn't make any sense to retry the task, since its doomed to fail every time, so we might as well kill the whole stage.  But this raises two questions:


1) Is it really safe to assume that a FetchFailedException means that the BlockManager has failed, and ti will work if we just try another one?  SPARK-5928 shows that there are at least some cases where that assumption is wrong.  Even if we fix that case, this logic seems brittle to the next case we find.  I guess the idea is that this behavior is what gives us the ""R"" in RDD ... but it seems like its not really that robust and maybe should be reconsidered.

2) Should stages only be retried a limited number of times?  It would be pretty easy to put in a limited number of retries per stage.  Though again, we encounter issues with keeping things resilient.  Theoretically one stage could have many retries, but due to failures in different stages further downstream, so we might need to track the cause of each retry as well to still have the desired behavior.

In general it just seems there is some flakiness in the retry logic.  This is the only reproducible example I have at the moment, but I vaguely recall hitting other cases of strange behavior w/ retries when trying to run long pipelines.  Eg., if one executor is stuck in a GC during a fetch, the fetch fails, but the executor eventually comes back and the stage gets retried again, but the same GC issues happen the second time around, etc.

Copied from SPARK-5928, here's the example program that can regularly produce a loop of stage failures.  Note that it will only fail from a remote fetch, so it can't be run locally -- I ran with {{MASTER=yarn-client spark-shell --num-executors 2 --executor-memory 4000m}}

{code}
    val rdd = sc.parallelize(1 to 1e6.toInt, 1).map{ ignore =>
      val n = 3e3.toInt
      val arr = new Array[Byte](n)
      //need to make sure the array doesn't compress to something small
      scala.util.Random.nextBytes(arr)
      arr
    }
    rdd.map { x => (1, x)}.groupByKey().count()
{code}",,amcelwee,apachespark,cheolsoo,darabos,douglaz,dweeks,dweeks-netflix,glenn.strycker@gmail.com,gregak,ilganeli,irashid,kayousterhout,ltrabuco,nemccarthy,rdub,roczei,rxin,SuYan,tgraves,zheng.tan,,,,,,,,,,,,,,,,,,,,SPARK-6746,SPARK-7308,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Aug 19 18:56:40 UTC 2015,,,,,,,,,,"0|i25wr3:",9223372036854775807,,,,,,,,,,,,,,1.6.0,,,,,,,,,,,,,"28/Feb/15 03:25;SuYan;I encounter stage retry infinitely when a executor lost because a Spark bug which I already fix in SPARK-5259

For solve stage retry, I add a retry limit.

{code}
case FetchFailed{
    ....
    if (disallowStageRetryForTest) {
          abortStage(failedStage, ""Fetch failure will not retry stage due to testing config"")
        } else if (failedStage.attemptId >= maxStageFailures) {
          abortStage(failedStage, s""Fetch failure will not retry stage"" +
            "" due to reach to max Failure times: "" + maxStageFailures)
        } else if (failedStages.isEmpty && eventProcessActor != null) {
          // Don't schedule an event to resubmit failed stages if failed isn't empty, because
   .....
}
{code}

;;;","18/Mar/15 20:59;ilganeli;Hi Imran - I'd be happy to tackle this. Could you please assign it to me? Thank you. ;;;","23/Mar/15 19:28;irashid;Hi [~ilganeli],

sorry for taking a while to respond.  I think the main issue here is not so much just implementing the code (as [~SuYan] already has shown the small required patch).  The big issue is figuring out what the desired semantics are (see the questions I listed above), which means just getting feedback from all the required people on this one.  But if you want to drive that process, that sounds great, it would really be appreciated!;;;","22/Apr/15 18:15;apachespark;User 'ilganeli' has created a pull request for this issue:
https://github.com/apache/spark/pull/5636;;;","22/Apr/15 21:35;kayousterhout;Commenting here rather than on the github for archiving purposes!

I took at look at the proposed pull request, and I'd be in favor of a much simpler approach, where for each stage, we track the number of failures (from any cause), and then fail the job once a stage fails 4 times (4, for consistency with the max task failures).  If the stage succeeds, we can reset the count to 0, to avoid the potential problem [~imranr] mentioned for stages that are re-used by many jobs (so the counter would be numConsecutiveFailures or something like that).  This can just be added to the Stage class, I think.  This is consistent with the approach we use for tasks, where if a task has failed 4 times (for any reason), we abort the stage.

I also would advocate against adding a configuration parameter for this.  I can't imagine a case where someone would want to keep trying after 4 failures, and I think sometimes configuration parameters for things like this lead people to believe they can fix a problem by changing the configuration variable (just up the max number of failures!!) when really there is some bigger underlying issue they should fix.  4 seems to have worked well for tasks, so I'd just use the same default here (and it's always easy to add a configuration variable later on if lots of people say they need it).;;;","22/Apr/15 22:08;ilganeli;[~kayousterhout] - thanks for the review. If I understand correctly, your suggestion would still address [~imranr]'s second comment since the first stage would always (or mostly succeed), e.g. it wouldn't have N consecutive failures so even if subsequent stages fail, those wouldn't count towards the failure count for this particular stage since it would have been reset when it succeeded. 

Do you have any thoughts on the first comment? Specifically, is retrying a stage likely to succeed at all or is it a waste of effort in the first place?
;;;","22/Apr/15 22:11;kayousterhout;When there's a fetch failed exception, what happens is that we mark the corresponding map tasks as failed, and then re-run all of the failed tasks from the previous stage.  When that's done, we re-run the stage with the original fetch failed exception -- so in the normal case, the stage with the fetch failed exception should succeed the second time.;;;","22/Apr/15 22:35;irashid;good point about moving the design discussion to jira, thanks Kay.

Yes, totally agree that we want to retry at least once, its definitely ""normal"" that in a big cluster, a node will go bad from time-to-time, but the good thing is Spark knows how to recover.  I also agree that we shouldn't care about the cause of the failure, just the failure count.

I totally see your points about the different configuration parameters, but let me just play devil's advocate.  yes, configuration parameters are confusing to users, but thats a reason we should have sensible defaults and most users should never need to touch them.  That doesn't mean nobody will want them.  Tasks and stages are in some ways very different things -- tasks are meant to be very small and lightweight, so failing a few extra times is no big deal.  But stages can be really big -- I would imagine in most cases, you actually might want to fail completely if the stage fails even twice, just because you can waste so much time in stage failure.  Then again, there might be the other extreme, with really big clusters and unstable hardware, maybe two failures won't be that big a deal, so some users will want it higher.

I disagree that its easy to add the config later.  Yes, its easy to make the code change.  But its hard to deploy the change in a production environment.  And I can see this as a parameter that devops team needs to play with for their exact system / workload / SLAs etc.  -- not sure at all, but I think we just don't know, and so we should leave the door open.

I also can't see any reason why anyone would want infinite retries -- but I'm hesitant (and asked for the change) just b/c of changing from the old behavior.  I guess int.maxvalue is close enough if somebody needs it?;;;","22/Apr/15 22:55;kayousterhout;I realized there might be a cleaner solution here: I wonder if we should just break the FetchFailedException into two subtypes, one of which is UnrecoverableFetchFailedException.  If a shuffle block is too large, there's no point in retrying the stage; we should just fail it.  I realized maybe this is what you were alluding to in your earlier comment, when you asked whether it's worth it to retry.  It seems like, at the point when we throw the exception, we do know whether it's worth it to retry, and it would be cleaner to just throw an appropriate exception. Thoughts?;;;","23/Apr/15 04:49;irashid;I think we want to do something more than just handling the case of blocks that are too large.  Putting in a special case to avoid retrying at all in that case would be fine, but to me that is a separate issue, the more important thing to do is to put in a general retry limit.  There could be all sorts of other reasons for fetch failures (I just looked into a case where an OOM would kill an executor, which lead to 10 hours of stage retry attempts before somebody manually killed the job).;;;","23/Apr/15 05:09;kayousterhout;Good point -- that makes sense.

I'm still in favor of not adding a config parameter: I think we're in agreement that it's hard to imagine a case where someone wants this to be *more* than 4, so making it 4 seems strictly better than the current approach where it is infinite.  If people complain or want to configure it to be less, we can always change this in a minor release.;;;","24/Apr/15 00:40;ilganeli;So to recap:
a) Move failure count tracking into Stage
b) Reset failure count on Stage success, so even if that stage is re-submitted due to failures downstream, we never hit the cap
c) Remove config parameter. ;;;","29/Apr/15 06:21;irashid;[~kayousterhout] can you please clarify -- did you want to just hardcode to 4, or did you want to reuse {{spark.task.maxFailures}} for stage failures as well?;;;","29/Apr/15 16:20;kayousterhout;I wanted to hardcode to 4 (totally agree with the sentiment you expressed earlier in this thread, that it doesn't make sense / is very confusing to re-use a config parameter for two different things).;;;","26/May/15 22:32;irashid;blocked by SPARK-7308 b/c you need to know which attempt is being failed;;;","02/Jul/15 13:38;darabos;At the moment we have a ton of these infinite retries. A stage is retried a few dozen times, then its parent goes missing and Spark starts retrying the parent until it also goes missing... We are still debugging the cause of our fetch failures, but I just wanted to mention that if there were a {{spark.stage.maxFailures}} option, we would be setting it to 1 at this point.

Thanks for all the work on this bug. Even if it's not fixed yet, it's very informative.;;;","19/Aug/15 18:56;rxin;I have retargeted this and downgraded it from Blocker to Critical since it's been there for a while and not a regression.;;;",,,,,,,,,,,,
Unit Test loads the table `src` twice for leftsemijoin.q,SPARK-5941,12776681,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,22/Feb/15 03:19,24/Apr/15 00:46,14/Jul/23 06:27,13/Apr/15 23:02,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"In leftsemijoin.q, there is a data loading command for table ""sales"" already, but in TestHive, it also creates/loads the table ""sales"", which causes duplicated records inserted into the ""sales"".",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Apr 13 23:02:41 UTC 2015,,,,,,,,,,"0|i25wbj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/15 03:20;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4506;;;","13/Apr/15 23:02;marmbrus;Issue resolved by pull request 4506
[https://github.com/apache/spark/pull/4506];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Make FPGrowth example app take parameters,SPARK-5939,12776637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jackylk,jackylk,jackylk,21/Feb/15 13:56,23/Feb/15 16:48,14/Jul/23 06:27,23/Feb/15 16:47,1.2.1,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,,,apachespark,jackylk,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 16:47:59 UTC 2015,,,,,,,,,,"0|i25w1r:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"21/Feb/15 14:01;apachespark;User 'jackylk' has created a pull request for this issue:
https://github.com/apache/spark/pull/4714;;;","23/Feb/15 16:47;mengxr;Issue resolved by pull request 4714
[https://github.com/apache/spark/pull/4714];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ClientSuite must set SPARK_YARN_MODE to true to ensure correct SparkHadoopUtil implementation is used.,SPARK-5937,12776590,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hshreedharan,hshreedharan,hshreedharan,21/Feb/15 00:09,21/Feb/15 18:02,14/Jul/23 06:27,21/Feb/15 18:02,1.2.0,,,,,,1.3.0,,,,,,Tests,YARN,,,0,,,,,,,,apachespark,hshreedharan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 21 00:13:41 UTC 2015,,,,,,,,,,"0|i25vrb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"21/Feb/15 00:13;apachespark;User 'harishreedharan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4711;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
[SQL] DataFrame.explain() return false result for DDL command,SPARK-5926,12776498,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,20/Feb/15 17:44,25/Apr/15 21:41,14/Jul/23 06:27,25/Feb/15 23:37,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"This bug is easy to reproduce, the following two queries should print out the same explain result, but it's not.

sql(""create table tb as select * from src where key > 490"").explain(true)

sql(""explain extended create table tb as select * from src where key > 490"")",,apachespark,marmbrus,yanboliang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 23:37:40 UTC 2015,,,,,,,,,,"0|i25v6v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/15 17:48;yanboliang;The following is the output of each query
#1:
{panel}
== Parsed Logical Plan ==
LogicalRDD [], ParallelCollectionRDD[7] at parallelize at commands.scala:65

== Analyzed Logical Plan ==
LogicalRDD [], ParallelCollectionRDD[7] at parallelize at commands.scala:65

== Optimized Logical Plan ==
LogicalRDD [], ParallelCollectionRDD[7] at parallelize at commands.scala:65

== Physical Plan ==
PhysicalRDD [], ParallelCollectionRDD[7] at parallelize at commands.scala:65
{panel}
#2:
{panel}
== Parsed Logical Plan ==
'CreateTableAsSelect None, tb, false, Some(TOK_CREATETABLE)
 'Project [*]
  'Filter ('key > 490)
   'UnresolvedRelation [src], None

== Analyzed Logical Plan ==
CreateTableAsSelect [Database:default, TableName: tb, InsertIntoHiveTable]
Project [key#43,value#44]
 Filter (key#43 > 490)
  MetastoreRelation default, src, None


== Optimized Logical Plan ==
CreateTableAsSelect [Database:default, TableName: tb, InsertIntoHiveTable]
Project [key#43,value#44]
 Filter (key#43 > 490)
  MetastoreRelation default, src, None


== Physical Plan ==
ExecutedCommand (CreateTableAsSelect [Database:default, TableName: tb, InsertIntoHiveTable]
Project [key#43,value#44]
 Filter (key#43 > 490)
  MetastoreRelation default, src, None
)
{panel};;;","20/Feb/15 17:58;yanboliang;This is because that for DDL like commands with side effects, DataFrame forces it to execute right away. However if we just want to know the execution plan, we do not need it to execute.
{code:title=DataFrame.scala|borderStyle=solid} 
@transient protected[sql] override lazy val logicalPlan: LogicalPlan = queryExecution.logical match {
    // For various commands (like DDL) and queries with side effects, we force query optimization to
    // happen right away to let these side effects take place eagerly.
    case _: Command |
         _: InsertIntoTable |
         _: CreateTableAsSelect[_] |
         _: CreateTableUsingAsSelect |
         _: WriteToFile =>
      LogicalRDD(queryExecution.analyzed.output, queryExecution.toRdd)(sqlContext)
    case _ =>
      queryExecution.logical
  }
{code} ;;;","20/Feb/15 18:40;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4707;;;","25/Feb/15 23:37;marmbrus;Issue resolved by pull request 4707
[https://github.com/apache/spark/pull/4707];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Very slow query when using Oracle  hive metastore and table has lots of partitions,SPARK-5923,12776426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tbfenet,tbfenet,20/Feb/15 12:33,07/Oct/16 07:54,14/Jul/23 06:27,07/Oct/16 07:54,1.2.0,,,,,,,,,,,,SQL,,,,0,,,,,,"This has two aspects
* The direct sql support for oracle is broken in hive 0.13.1. Fails when partitions get bigger than 1000 due oracle limitation on IN clause. This cause fall back to ORM which is very slow(20 minutes to even start the query)
* Hive it self does not suffer this problem as it passes down to the metadata query, filter terms that restrict the partitions returned. SparkSQL is always asking for all partitions event if they are not all needed. Even when we patched hive it was still taking 2 minutes 
",,smilegator,tbfenet,tgraves,yanakad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 07:54:37 UTC 2016,,,,,,,,,,"0|i25ur3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/15 13:57;yanakad;[~mtaylor] Can you please provide some information on how you debugged this? I just experienced a similar issue -- really poor performance on large metastore even though I'm only touching a few partitions. I'm using a Postgresql metastore . I do not however see ""IN"" queries logged to Postgres, and according to the posgres log no individual query took longer than 50ms. So I'm hoping to get some debugging tips;;;","19/Apr/15 11:03;tbfenet;Postgresql does not have a limitation on IN clause so you would not get an exception. It might be slow anyway but probably not as slow in the oracle case as that reverts to fetching each partition one at a time. I am preparing a CR with a fix for this that passes query parameters on to the metastore query so it does not fetch every partition. ;;;","07/Oct/16 07:54;smilegator;This should have been resolved. Please reopen it if this is still an issue. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Enable spark-submit to run requiring only user permission on windows,SPARK-5914,12776317,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,judynash,judynash,judynash,19/Feb/15 23:54,26/Feb/15 11:16,14/Jul/23 06:27,26/Feb/15 11:16,,,,,,,1.4.0,,,,,,Spark Submit,Windows,,,0,,,,,,"On windows platform only. 

If slave is executed with user permission, spark-submit fails with java.lang.ClassNotFoundException when attempting to read the cached jar from spark_home\work folder. 

This is due to the jars do not have read permission set by default on windows. Fix is to add read permission explicitly for owner of the file. 

Having service account running as admin (equivalent of sudo in Linux) is a major security risk for production clusters. This make it easy for hackers to compromise the cluster by taking over the service account. ",Windows,apachespark,judynash,zzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 11:16:04 UTC 2015,,,,,,,,,,"0|i25u3j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/15 00:09;srowen;Yes of course you are not expected to run as admin. It'd be good to find a way to set the permissions correctly. I don't know how well Java plays with Windows file permissions though?;;;","24/Feb/15 08:35;apachespark;User 'judynash' has created a pull request for this issue:
https://github.com/apache/spark/pull/4742;;;","26/Feb/15 11:16;srowen;Issue resolved by pull request 4742
[https://github.com/apache/spark/pull/4742];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"DataFrame.selectExpr(""col as newName"") does not work",SPARK-5910,12776270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,yhuai,yhuai,19/Feb/15 20:30,24/Feb/15 18:53,14/Jul/23 06:27,24/Feb/15 18:50,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"{code}
val rdd = sc.parallelize((1 to 10).map(i => s""""""{""a"":$i, ""b"":""str${i}""}""""""))
sqlContext.jsonRDD(rdd).selectExpr(""a as newName"")
{code}

{code}
java.lang.RuntimeException: [1.3] failure: ``or'' expected but `as' found

a as newName
  ^
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.catalyst.SqlParser.parseExpression(SqlParser.scala:45)

{code}

For selectExpr, we need to use projection parser instead of expression parser (which cannot parse AS).",,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 24 18:50:20 UTC 2015,,,,,,,,,,"0|i25ttb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"24/Feb/15 01:45;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4736;;;","24/Feb/15 18:50;marmbrus;Issue resolved by pull request 4736
[https://github.com/apache/spark/pull/4736];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive udtf with single alias should be resolved correctly,SPARK-5908,12776174,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,19/Feb/15 14:36,25/Apr/15 21:42,14/Jul/23 06:27,18/Mar/15 01:59,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"ResolveUdtfsAlias in hiveUdfs only considers the HiveGenericUdtf with multiple alias. When only single alias is used with HiveGenericUdtf, the alias is not working.",,apachespark,marmbrus,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 01:59:16 UTC 2015,,,,,,,,,,"0|i25t8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/15 14:37;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/4692;;;","18/Mar/15 01:59;marmbrus;Issue resolved by pull request 4692
[https://github.com/apache/spark/pull/4692];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrap the results returned by PIC and FPGrowth in case classes,SPARK-5900,12776013,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,18/Feb/15 22:43,20/Feb/15 02:06,14/Jul/23 06:27,20/Feb/15 02:06,1.3.0,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,We return tuples in the current version of PIC and FPGrowth. This is not very Java-friendly because the primitive types are translated into Objects.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 20 02:06:38 UTC 2015,,,,,,,,,,"0|i25san:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"19/Feb/15 17:47;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4695;;;","20/Feb/15 02:06;mengxr;Issue resolved by pull request 4695
[https://github.com/apache/spark/pull/4695];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't create DataFrame from Pandas data frame,SPARK-5898,12775994,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,marmbrus,marmbrus,18/Feb/15 21:35,20/Feb/15 23:35,14/Jul/23 06:27,20/Feb/15 23:35,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"{code}
data = sqlContext.table(""sparkCommits"")
p = data.toPandas()
sqlContext.createDataFrame(p)
{code}

{code}
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-12-fb4f1895bd2f> in <module>()
      1 data = sqlContext.table(""sparkCommits"")
      2 p = data.toPandas()
----> 3 sqlContext.createDataFrame(p)

/home/ubuntu/databricks/spark/python/pyspark/sql/context.pyc in createDataFrame(self, data, schema, samplingRatio)
    385             data = self._sc.parallelize(data.to_records(index=False))
    386             if schema is None:
--> 387                 schema = list(data.columns)
    388 
    389         if not isinstance(data, RDD):

AttributeError: 'RDD' object has no attribute 'columns'
",,apachespark,marmbrus,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 20 23:35:29 UTC 2015,,,,,,,,,,"0|i25s6v:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"18/Feb/15 22:34;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4679;;;","20/Feb/15 23:35;marmbrus;Issue resolved by pull request 4679
[https://github.com/apache/spark/pull/4679];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
toDF in python doesn't work with tuple/list w/o names,SPARK-5896,12775991,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,marmbrus,marmbrus,18/Feb/15 21:24,20/Feb/15 23:35,14/Jul/23 06:27,20/Feb/15 23:35,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"{code}
rdd = sc.parallelize(range(10)).map(lambda x: (str(x), x))
kvdf = rdd.toDF()
{code}

{code}
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-18-327cb4e9a02e> in <module>()
      1 rdd = sc.parallelize(range(10)).map(lambda x: (str(x), x))
----> 2 kvdf = rdd.toDF()

/home/ubuntu/databricks/spark/python/pyspark/sql/context.pyc in toDF(self, schema, sampleRatio)
     53         [Row(name=u'Alice', age=1)]
     54         """"""
---> 55         return sqlCtx.createDataFrame(self, schema, sampleRatio)
     56 
     57     RDD.toDF = toDF

/home/ubuntu/databricks/spark/python/pyspark/sql/context.pyc in createDataFrame(self, data, schema, samplingRatio)
    395 
    396         if schema is None:
--> 397             return self.inferSchema(data, samplingRatio)
    398 
    399         if isinstance(schema, (list, tuple)):

/home/ubuntu/databricks/spark/python/pyspark/sql/context.pyc in inferSchema(self, rdd, samplingRatio)
    228             raise TypeError(""Cannot apply schema to DataFrame"")
    229 
--> 230         schema = self._inferSchema(rdd, samplingRatio)
    231         converter = _create_converter(schema)
    232         rdd = rdd.map(converter)

/home/ubuntu/databricks/spark/python/pyspark/sql/context.pyc in _inferSchema(self, rdd, samplingRatio)
    158 
    159         if samplingRatio is None:
--> 160             schema = _infer_schema(first)
    161             if _has_nulltype(schema):
    162                 for row in rdd.take(100)[1:]:

/home/ubuntu/databricks/spark/python/pyspark/sql/types.pyc in _infer_schema(row)
    646             items = row
    647         else:
--> 648             raise ValueError(""Can't infer schema from tuple"")
    649 
    650     elif hasattr(row, ""__dict__""):  # object

ValueError: Can't infer schema from tuple
{code}

Nearly the same code works if you give names (and this works without names in scala and calls the columns _1, _2, ...)",,apachespark,davies,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 20 23:35:31 UTC 2015,,,,,,,,,,"0|i25s67:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/15 21:34;davies;[~marmbrus] There is a mistake in your script, it should be
{code}
data = sc.parallelize([Row(name=""michael"")]).toDF()
{code};;;","18/Feb/15 21:41;marmbrus;Sorry!  I had the wrong example.  Updated.;;;","18/Feb/15 21:44;davies;I think this is a *feature*, not bug.  `_1` is a Scala feature, we can not just copy it into Python.

BTW, we should have better message to tell user that they need to provide a list of names.;;;","18/Feb/15 21:51;marmbrus;Why not auto assign column names by index just like in Scala or in SQL?  Its weird to have python be the only language that doesn't support this.  It can be ""c0"" like in SQL if you are objecting to the underscore.;;;","18/Feb/15 22:34;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4679;;;","20/Feb/15 23:35;marmbrus;Issue resolved by pull request 4679
[https://github.com/apache/spark/pull/4679];;;",,,,,,,,,,,,,,,,,,,,,,,
remove pid file in spark-daemon.sh after killing the process.,SPARK-5889,12775952,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhazhan,zzhan,zzhan,18/Feb/15 19:17,19/Feb/15 23:15,14/Jul/23 06:27,19/Feb/15 23:15,1.2.1,,,,,,1.2.2,1.3.0,,,,,Deploy,,,,0,,,,,,"Currently, if the thriftserver/history server are stopped. The pid file is not deleted. The fix is trial, but it is important for some service checking relying on the file.",,apachespark,zzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4832,SPARK-5825,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 19 23:15:24 UTC 2015,,,,,,,,,,"0|i25rzr:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"18/Feb/15 19:37;zzhan;https://github.com/apache/spark/pull/4676;;;","18/Feb/15 19:38;apachespark;User 'zhzhan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4676;;;","19/Feb/15 10:42;srowen;Yeah I wanted to do this in the original PR, although I think there's a small potential problem: what if {{kill}} fails? then you lose the PID file. In that case, a lot of bets are off anyway and it's not clear that subsequent retries would succeed.

Still, since the script handles old PID files already (er, it's trying to), I wonder if this can be slightly more conservative and only remove if kill succeeds?;;;","19/Feb/15 23:15;srowen;Issue resolved by pull request 4676
[https://github.com/apache/spark/pull/4676];;;",,,,,,,,,,,,,,,,,,,,,,,,,
"RDD remains cached after the table gets overridden by ""CACHE TABLE""",SPARK-5881,12775814,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,yhuai,yhuai,18/Feb/15 05:06,07/Oct/16 22:42,14/Jul/23 06:27,07/Oct/16 22:42,,,,,,,,,,,,,SQL,,,,0,,,,,,"{code}
val rdd = sc.parallelize((1 to 10).map(i => s""""""{""a"":$i, ""b"":""str${i}""}""""""))
sqlContext.jsonRDD(rdd).registerTempTable(""jt"")

sqlContext.sql(""CACHE TABLE foo AS SELECT * FROM jt"")
sqlContext.sql(""CACHE TABLE foo AS SELECT a FROM jt"")
{code}
After the second CACHE TABLE command, the RDD for the first table still remains in the cache.",,apachespark,copris,hongyu.bi,smilegator,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 22:39:26 UTC 2016,,,,,,,,,,"0|i25r5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/15 06:54;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4689;;;","19/Feb/15 16:28;yhuai;As mentioned by [~lian cheng], we should also track the table names in the Cache Manager to correctly handle the following case.
{code}
val df1 = sql(""SELECT * FROM testData LIMIT 10"")
df1.registerTempTable(""t1"")

// Cache t1 explicitly
sql(""CACHE TABLE t1"")

// t1 and t2 share the same query plan
sql(""CACHE TABLE t2 AS SELECT * FROM testData LIMIT 10"")

// Replace t2 with a different query plan
sql(""CACHE TABLE t2 AS SELECT * FROM testData LIMIT 5"")
{code}
;;;","15/Apr/15 04:01;yhuai;[~lian cheng] [~marmbrus] I think we need some major changes with our cache manager to fix it. I am inclined to bump the version.;;;","07/Oct/16 22:39;smilegator;I think this has been resolved. Let me close it. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Python DataFrame.repartition() is broken,SPARK-5878,12775783,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,davies,davies,18/Feb/15 00:47,18/Feb/15 09:01,14/Jul/23 06:27,18/Feb/15 09:01,,,,,,,1.3.0,,,,,,SQL,,,,0,DataFrame,,,,,,,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 18 00:48:40 UTC 2015,,,,,,,,,,"0|i25qzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/15 00:48;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4667;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
logical.Project should not be resolved if it contains aggregates or generators,SPARK-5875,12775758,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,17/Feb/15 22:26,25/Apr/15 21:42,14/Jul/23 06:27,18/Feb/15 01:51,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"To reproduce...
{code}
val rdd = sc.parallelize((1 to 10).map(i => s""""""{""a"":$i, ""b"":""str${i}""}""""""))
sqlContext.jsonRDD(rdd).registerTempTable(""jt"")
sqlContext.sql(""CREATE TABLE gen_tmp (key Int)"")
sqlContext.sql(""INSERT OVERWRITE TABLE gen_tmp SELECT explode(array(1,2,3)) AS val FROM jt LIMIT 1"")
{code}
The exception is
{code}
org.apache.spark.sql.AnalysisException: invalid cast from array<struct<_c0:int>> to int;
    at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$.failAnalysis(Analyzer.scala:85)
    at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$apply$18$$anonfun$apply$2.applyOrElse(Analyzer.scala:98)
    at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$apply$18$$anonfun$apply$2.applyOrElse(Analyzer.scala:92)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:50)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:249)
    at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:263)
{code}

The cause of this exception is that PreInsertionCasts in HiveMetastoreCatalog was triggered on an invalid query plan 
{code}
Project [HiveGenericUdtf#org.apache.hadoop.hive.ql.udf.generic.GenericUDTFExplode(Array(1,2,3)) AS val#19]
  Subquery jt
   LogicalRDD [a#0L,b#1], MapPartitionsRDD[4] at map at JsonRDD.scala:41
{code}
Then, after the transformation of PreInsertionCasts, ImplicitGenerate cannot be applied.",,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 18 01:51:04 UTC 2015,,,,,,,,,,"0|i25qtr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"17/Feb/15 23:33;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4663;;;","18/Feb/15 01:51;marmbrus;Issue resolved by pull request 4663
[https://github.com/apache/spark/pull/4663];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't see partially analyzed plans,SPARK-5873,12775728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,17/Feb/15 20:55,24/Feb/15 01:35,14/Jul/23 06:27,24/Feb/15 01:35,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,Our analysis checks are great for users who make mistakes but make it impossible to see what is going wrong when there is a bug in the analyzer.,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 19 00:23:34 UTC 2015,,,,,,,,,,"0|i25qnb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"19/Feb/15 00:23;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4684;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark shell should start up with SQL/HiveContext,SPARK-5872,12775712,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,marmbrus,marmbrus,17/Feb/15 19:58,17/Feb/15 23:44,14/Jul/23 06:27,17/Feb/15 23:44,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 23:44:56 UTC 2015,,,,,,,,,,"0|i25qjz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"17/Feb/15 20:34;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4659;;;","17/Feb/15 23:44;marmbrus;Issue resolved by pull request 4659
[https://github.com/apache/spark/pull/4659];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Explain in python should output using python,SPARK-5871,12775711,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,marmbrus,marmbrus,17/Feb/15 19:57,17/Feb/15 21:51,14/Jul/23 06:27,17/Feb/15 21:51,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,Instead of relying on the println in scala.,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 21:51:13 UTC 2015,,,,,,,,,,"0|i25qjr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"17/Feb/15 20:25;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4658;;;","17/Feb/15 21:51;marmbrus;Issue resolved by pull request 4658
[https://github.com/apache/spark/pull/4658];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Python UDFs broken by analysis check in HiveContext,SPARK-5868,12775707,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,17/Feb/15 19:46,17/Feb/15 21:24,14/Jul/23 06:27,17/Feb/15 21:24,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Technically they are broken in SQLContext as well, but because of the hacky way we handle python udfs there it doesn't get checked.  Lets fix both things.",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 21:24:04 UTC 2015,,,,,,,,,,"0|i25qiv:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"17/Feb/15 20:00;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4657;;;","17/Feb/15 21:24;marmbrus;Issue resolved by pull request 4657
[https://github.com/apache/spark/pull/4657];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
JdbcRDD: overflow on large range with high number of partitions,SPARK-5860,12775561,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,JeroenWarmerdam,JeroenWarmerdam,JeroenWarmerdam,17/Feb/15 10:38,21/Feb/15 20:43,14/Jul/23 06:27,21/Feb/15 20:40,1.2.1,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"{code}
val jdbcRDD = new JdbcRDD(sc, () =>
      DriverManager.getConnection(url, username, password),
      ""SELECT id FROM documents WHERE ? <= id AND id <= ?"",
      lowerBound = 1131544775L,
      upperBound = 567279358897692673L,
      numPartitions = 20,
      mapRow = r => (r.getLong(""id""))
)
{code}",,apachespark,JeroenWarmerdam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 21 20:40:41 UTC 2015,,,,,,,,,,"0|i25pn3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/15 02:01;apachespark;User 'hotou' has created a pull request for this issue:
https://github.com/apache/spark/pull/4701;;;","21/Feb/15 20:40;srowen;Issue resolved by pull request 4701
[https://github.com/apache/spark/pull/4701];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Using first() to get feature size causes performance regression,SPARK-5858,12775535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,mengxr,mengxr,mengxr,17/Feb/15 08:23,17/Feb/15 18:17,14/Jul/23 06:27,17/Feb/15 18:17,1.3.0,,,,,,1.3.0,,,,,,MLlib,PySpark,,,0,,,,,,We call `.first()` to get the feature size. It causes performance regression because first() still runs on the driver.,,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 18:17:59 UTC 2015,,,,,,,,,,"0|i25phb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"17/Feb/15 09:27;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4647;;;","17/Feb/15 18:17;mengxr;Issue resolved by pull request 4647
[https://github.com/apache/spark/pull/4647];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Fail to convert a newly created empty metastore parquet table to a data source parquet table.,SPARK-5852,12775485,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,17/Feb/15 00:11,25/Apr/15 21:43,14/Jul/23 06:27,17/Feb/15 23:48,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"To reproduce the exception, try
{code}
val rdd = sc.parallelize((1 to 10).map(i => s""""""{""a"":$i, ""b"":""str${i}""}""""""))
sqlContext.jsonRDD(rdd).registerTempTable(""jt"")
sqlContext.sql(""create table test stored as parquet as select * from jt"")
{code}

ParquetConversions tries to convert the write path to the data source API write path. But, the following exception was thrown.
{code}
java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$class.reduceLeft(TraversableOnce.scala:167)
	at scala.collection.mutable.ArrayBuffer.scala$collection$IndexedSeqOptimized$$super$reduceLeft(ArrayBuffer.scala:47)
	at scala.collection.IndexedSeqOptimized$class.reduceLeft(IndexedSeqOptimized.scala:68)
	at scala.collection.mutable.ArrayBuffer.reduceLeft(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.reduce(TraversableOnce.scala:195)
	at scala.collection.AbstractTraversable.reduce(Traversable.scala:105)
	at org.apache.spark.sql.parquet.ParquetRelation2$.readSchema(newParquet.scala:633)
	at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.org$apache$spark$sql$parquet$ParquetRelation2$MetadataCache$$readSchema(newParquet.scala:349)
	at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$refresh$8.apply(newParquet.scala:290)
	at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$refresh$8.apply(newParquet.scala:290)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newParquet.scala:290)
	at org.apache.spark.sql.parquet.ParquetRelation2.<init>(newParquet.scala:354)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$convertToParquetRelation(HiveMetastoreCatalog.scala:218)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog$ParquetConversions$$anonfun$apply$4.apply(HiveMetastoreCatalog.scala:440)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog$ParquetConversions$$anonfun$apply$4.apply(HiveMetastoreCatalog.scala:439)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:47)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog$ParquetConversions$.apply(HiveMetastoreCatalog.scala:439)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog$ParquetConversions$.apply(HiveMetastoreCatalog.scala:416)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:917)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:917)
	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:918)
	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:918)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:919)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:919)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:924)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:922)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:928)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:928)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:931)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:931)
	at org.apache.spark.sql.hive.execution.CreateTableAsSelect.run(CreateTableAsSelect.scala:71)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:55)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:55)
	at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:65)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:931)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:931)
	at org.apache.spark.sql.DataFrameImpl.<init>(DataFrameImpl.scala:75)
	at org.apache.spark.sql.DataFrameImpl.<init>(DataFrameImpl.scala:58)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:35)
	at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:77)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:20)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:28)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:30)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:32)
	at $iwC$$iwC$$iwC.<init>(<console>:34)
	at $iwC$$iwC.<init>(<console>:36)
	at $iwC.<init>(<console>:38)
	at <init>(<console>:40)
	at .<init>(<console>:44)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:856)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:901)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:874)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:901)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:813)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:656)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:664)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:669)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:996)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:944)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:944)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1058)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
{code}",,apachespark,glenn.strycker@gmail.com,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 23:48:35 UTC 2015,,,,,,,,,,"0|i25p67:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"17/Feb/15 00:14;yhuai;The following CTAS statement directly using the data source API is fine.
{code}
sqlContext.sql(""create table test using parquet as select * from jt"")
{code}
;;;","17/Feb/15 03:08;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4562;;;","17/Feb/15 19:27;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4655;;;","17/Feb/15 23:48;marmbrus;Issue resolved by pull request 4655
[https://github.com/apache/spark/pull/4655];;;",,,,,,,,,,,,,,,,,,,,,,,,,
Remove experimental label for Scala 2.11 and FlumePollingStream,SPARK-5850,12775480,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,pwendell,pwendell,16/Feb/15 23:03,18/Feb/15 09:15,14/Jul/23 06:27,18/Feb/15 09:15,,,,,,,1.3.0,,,,,,DStreams,Spark Core,,,0,,,,,,These things have been out for at least one release and can be promoted.,,apachespark,michaelmalak,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 23:05:43 UTC 2015,,,,,,,,,,"0|i25p53:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"16/Feb/15 23:05;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/4638;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle more types of invalid JSON requests in SubmitRestProtocolMessage.parseAction,SPARK-5849,12775478,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,16/Feb/15 22:52,17/Feb/15 02:08,14/Jul/23 06:27,17/Feb/15 02:08,1.3.0,,,,,,1.3.0,,,,,,Spark Submit,,,,0,,,,,,"SubmitRestProtocolMessage.parseAction casts the result of {{parse(json)}} to {{JObject}}, but this cast can fail if invalid requests can still be parsed as other JSON types, such as JSON strings.  We should replace this unchecked cast with pattern-matching in order to handle these types of invalid requests.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 22:57:39 UTC 2015,,,,,,,,,,"0|i25p4n:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"16/Feb/15 22:57;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4637;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConsoleProgressBar timer thread leaks SparkContext,SPARK-5848,12775471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matt.whelan,matt.whelan,matt.whelan,16/Feb/15 21:58,17/Feb/15 01:00,14/Jul/23 06:27,17/Feb/15 01:00,1.2.1,,,,,,1.3.0,,,,,,Spark Shell,,,,0,,,,,,"ConsoleProgressBar starts a timer.  This creates a thread (which is a garbage collection root) with a reference to the ConsoleProgressBar instance, which holds a reference to the SparkContext.  

That timer is never canceled, so SparkContexts are leaked.",,apachespark,matt.whelan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 01:00:15 UTC 2015,,,,,,,,,,"0|i25p33:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/15 22:02;apachespark;User 'MattWhelan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4635;;;","17/Feb/15 01:00;srowen;Issue resolved by pull request 4635
[https://github.com/apache/spark/pull/4635];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark SQL does not correctly set job description and scheduler pool,SPARK-5846,12775466,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kayousterhout,kayousterhout,kayousterhout,16/Feb/15 21:28,19/Feb/15 02:05,14/Jul/23 06:27,19/Feb/15 01:52,1.2.1,1.3.0,,,,,1.2.2,1.3.0,,,,,SQL,,,,0,,,,,,"Spark SQL current sets the scheduler pool and job description AFTER jobs run (see https://github.com/apache/spark/blob/master/sql/hive-thriftserver/v0.13.1/src/main/scala/org/apache/spark/sql/hive/thriftserver/Shim13.scala#L168 -- which happens after calling hiveContext.sql).  As a result, the description for a SQL job ends up being the SQL query corresponding to the previous job.  This should be done before the job is run so the description is correct.
",,apachespark,kayousterhout,lian cheng,sb58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 19 01:52:21 UTC 2015,,,,,,,,,,"0|i25p1z:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"16/Feb/15 21:33;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/4630;;;","16/Feb/15 21:36;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/4631;;;","19/Feb/15 01:52;lian cheng;Issue resolved by pull request 4630
[https://github.com/apache/spark/pull/4630];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Time to cleanup spilled shuffle files not included in shuffle write time,SPARK-5845,12775461,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ilganeli,kayousterhout,kayousterhout,16/Feb/15 21:07,17/May/20 18:30,14/Jul/23 06:27,13/Mar/15 13:21,1.2.1,1.3.0,,,,,1.4.0,,,,,,Shuffle,Spark Core,,,0,,,,,,"When the disk is contended, I've observed cases when it takes as long as 7 seconds to clean up all of the intermediate spill files for a shuffle (when using the sort based shuffle, but bypassing merging because there are <=200 shuffle partitions).  This is even when the shuffle data is non-huge (152MB written from one of the tasks where I observed this).  This is effectively part of the shuffle write time (because it's a necessary side effect of writing data to disk) so should be added to the shuffle write time to facilitate debugging.",,apachespark,ilganeli,kayousterhout,pwendell,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Mar 13 13:21:16 UTC 2015,,,,,,,,,,"0|i25p0v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/15 19:25;ilganeli;Hi Kay - I can knock this one out. Thanks. ;;;","24/Feb/15 23:19;ilganeli;If I understand correctly, the file cleanup happens in IndexShuffleBlockManager:::removeDataByMap(), which is called from either the SortShuffleManager or the SortShuffleWriter. The problem is that these classes do not have any knowledge of the currently collected metrics. Furthermore, the disk cleanup is, unless configured in the SparkConf, triggered asynchronously via the RemoveShuffle message so there doesn't appear to be a straightforward way to provide a set of metrics to be updated. 

Do you have any suggestions for getting around this? Please let me know, thank you. ;;;","24/Feb/15 23:33;pwendell;[~kayousterhout] did you mean the time required to delete spill files used during aggregation? For the shuffle files themselves, they are deleted asynchronously, as [~ilganeli] has mentioned.;;;","24/Feb/15 23:46;kayousterhout;[~pwendell] that's exactly what I meant -- I just updated the JIRA description.;;;","25/Feb/15 00:51;ilganeli;My mistake - missed your comment about the spill files in the detailed description. Given that we're interested in cleaning up the spill files which appear to be cleaned up in ExternalSorter.stop() (please correct me if I'm wrong), I would like to either 

a) Pass the context to the stop() method - this is possible since the SortShuffleWriter has visibility of the TaskContext (which in turn stores the metrics we're interested in).

b) (My preference since it won't break the existing interface) Surround sorter.stop() on line 91 of SortShuffleWriter.scala with a timer. The only downside to this second approach is that it will also include the cleanup of the partition writers. I'm not sure whether that should be included in this time computation. ;;;","25/Feb/15 00:54;kayousterhout;I'd go with (b) -- it's fine (and good, I think!) to include time to cleanup the partition writers, since this also involves interacting with the disk.  Thanks!;;;","27/Feb/15 06:34;ilganeli;I'm code complete on this, will submit a PR shortly.;;;","10/Mar/15 20:30;apachespark;User 'ilganeli' has created a pull request for this issue:
https://github.com/apache/spark/pull/4965;;;","13/Mar/15 13:21;srowen;Issue resolved by pull request 4965
[https://github.com/apache/spark/pull/4965];;;",,,,,,,,,,,,,,,,,,,,
Memory leak in DiskBlockManager,SPARK-5841,12775439,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,matt.whelan,matt.whelan,matt.whelan,16/Feb/15 19:29,17/May/20 18:20,14/Jul/23 06:27,16/Feb/15 22:54,1.2.1,,,,,,1.3.0,,,,,,Block Manager,Spark Core,,,0,,,,,,"DiskBlockManager registers a Runtime shutdown hook, which creates a hard reference to the entire Driver ActorSystem.  If a long-running JVM repeatedly creates and destroys SparkContext instances, it leaks memory.  

I suggest we deregister the shutdown hook if DiskBlockManager.stop is called.  It's redundant at that point.

PR coming.

See also http://mail-archives.apache.org/mod_mbox/spark-user/201501.mbox/%3CCA+kjH+w_DDTEBE9XB6NrPxLTUXD=NC_d-3ogxTUMk_5v-E0bLQ@mail.gmail.com%3E",,apachespark,cqnguyen,matt.whelan,smolav,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5869,SPARK-8024,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jun 02 00:46:50 UTC 2015,,,,,,,,,,"0|i25ovz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/15 19:35;matt.whelan;PR: https://github.com/apache/spark/pull/4627;;;","16/Feb/15 19:36;apachespark;User 'MattWhelan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4627;;;","16/Feb/15 22:54;srowen;Issue resolved by pull request 4627
[https://github.com/apache/spark/pull/4627];;;","17/Feb/15 09:33;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4648;;;","17/Feb/15 19:06;yhuai;[~matt.whelan] I noticed that after SQL unit tests, the following will be logged
{code}
10:59:28.721 ERROR org.apache.spark.util.Utils: Uncaught exception in thread delete Spark local dirs
java.lang.IllegalStateException: Shutdown in progress
	at java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:82)
	at java.lang.Runtime.removeShutdownHook(Runtime.java:239)
	at org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:151)
	at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply$mcV$sp(DiskBlockManager.scala:141)
	at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply(DiskBlockManager.scala:139)
	at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply(DiskBlockManager.scala:139)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1613)
	at org.apache.spark.storage.DiskBlockManager$$anon$1.run(DiskBlockManager.scala:139)
Exception in thread ""delete Spark local dirs"" java.lang.IllegalStateException: Shutdown in progress
	at java.lang.ApplicationShutdownHooks.remove(ApplicationShutdownHooks.java:82)
	at java.lang.Runtime.removeShutdownHook(Runtime.java:239)
	at org.apache.spark.storage.DiskBlockManager.stop(DiskBlockManager.scala:151)
	at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply$mcV$sp(DiskBlockManager.scala:141)
	at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply(DiskBlockManager.scala:139)
	at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply(DiskBlockManager.scala:139)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1613)
	at org.apache.spark.storage.DiskBlockManager$$anon$1.run(DiskBlockManager.scala:139)
{code}
Seems the log is related to this commit. Is it expected? You can try ""test-only org.apache.spark.sql.sources.InsertSuite"" to see it.;;;","17/Feb/15 19:18;srowen;[~yhuai] Fixed (I think) in https://github.com/apache/spark/pull/4648  -- will merge shortly unless anyone objects as I think it's causing test problems.;;;","17/Feb/15 19:24;yhuai;[~sowen] I see. Thank you for the pointer!;;;","19/Feb/15 07:52;apachespark;User 'nishkamravi2' has created a pull request for this issue:
https://github.com/apache/spark/pull/4690;;;","02/Jun/15 00:46;cqnguyen;We are using Luigi with Spark 1.3.1 to manage our jobs

However we run into a unique rare case with the following conditions that trigger this resolved Block Manger Bug:

- Dataset is relatively large ~ 1.5TB
- Spark job is ran with Luigi
- save to local HDFS

The spark job would process data and mappings just fine, until the very end when it proceeds to save the files to local hdfs this is when it triggers this bug. 

However, the job saves and complete data successfully if it was saved to s3:// location.

wondering what might cause this resolved bug to trigger when ran with luigi saving to local hdfs but not trigger when saved to s3 with luigi or ran without luigi?;;;",,,,,,,,,,,,,,,,,,,,
HiveContext cannot be serialized due to tuple extraction,SPARK-5840,12775436,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,16/Feb/15 19:08,25/Apr/15 21:43,14/Jul/23 06:27,18/Feb/15 22:03,1.2.0,1.2.1,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"See the following mailing list question: http://apache-spark-developers-list.1001551.n3.nabble.com/

The use of tuple extraction for (hiveconf, sessionState) creates a non-transient tuple field.",,apachespark,dhruv21,marmbrus,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 18 22:03:03 UTC 2015,,,,,,,,,,"0|i25ovj:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"16/Feb/15 19:43;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4628;;;","18/Feb/15 22:03;marmbrus;Issue resolved by pull request 4628
[https://github.com/apache/spark/pull/4628];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveMetastoreCatalog does not recognize table names and aliases of data source tables.,SPARK-5839,12775425,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yhuai,yhuai,yhuai,16/Feb/15 17:49,17/Feb/15 00:07,14/Jul/23 06:27,17/Feb/15 00:07,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"For example, when we run
{code}
val originalDefaultSource = conf.defaultDataSourceName

val rdd = sparkContext.parallelize((1 to 10).map(i => s""""""{""a"":$i, ""b"":""str${i}""}""""""))
val df = jsonRDD(rdd)

conf.setConf(SQLConf.DEFAULT_DATA_SOURCE_NAME, ""org.apache.spark.sql.json"")
// Save the df as a managed table (by not specifiying the path).
df.saveAsTable(""savedJsonTable"")

checkAnswer(
  sql(""SELECT * FROM savedJsonTable tmp where tmp.a > 5""),
  df.collect())

// Drop table will also delete the data.
sql(""DROP TABLE savedJsonTable"")

conf.setConf(SQLConf.DEFAULT_DATA_SOURCE_NAME, originalDefaultSource)
{code}

We will get
{code}
query with predicates *** FAILED *** (85 milliseconds)
[info]   org.apache.spark.sql.AnalysisException: cannot resolve 'tmp.a' given input columns a, b
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$.failAnalysis(Analyzer.scala:78)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$apply$18$$anonfun$apply$2.applyOrElse(Analyzer.scala:88)
[info]   at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$apply$18$$anonfun$apply$2.applyOrElse(Analyzer.scala:85)
{code}",,apachespark,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 18:04:26 UTC 2015,,,,,,,,,,"0|i25ot3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"16/Feb/15 18:04;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4626;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Adds REFRESH TABLE command to refresh external data sources tables,SPARK-5833,12775318,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,16/Feb/15 09:56,25/Apr/15 21:44,14/Jul/23 06:27,16/Feb/15 20:52,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"This command can be used to refresh (possibly cached) metadata stored in external data source tables. For example, for JSON tables, it forces schema inference; for Parquet tables, it forces schema merging and partition discovery.",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 20:52:36 UTC 2015,,,,,,,,,,"0|i25o5r:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"16/Feb/15 10:01;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4624;;;","16/Feb/15 20:52;marmbrus;Issue resolved by pull request 4624
[https://github.com/apache/spark/pull/4624];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
JavaStreamingContext.fileStream cause Configuration NotSerializableException,SPARK-5826,12775195,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,jerryshao,cnstar9988,cnstar9988,15/Feb/15 07:47,17/Feb/15 10:47,14/Jul/23 06:27,17/Feb/15 10:45,1.2.1,,,,,,1.3.0,,,,,,DStreams,,,,0,,,,,,"org.apache.spark.streaming.api.java.JavaStreamingContext.fileStream(String directory, Class<LongWritable> kClass, Class<Text> vClass, Class<TextInputFormat> fClass, Function<Path, Boolean> filter, boolean newFilesOnly, Configuration conf)

I use JavaStreamingContext.fileStream with 1.3.0/master with Configuration.

but it throw strange exception.

java.io.NotSerializableException: org.apache.hadoop.conf.Configuration
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:440)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply$mcV$sp(DStreamGraph.scala:177)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1075)
	at org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.streaming.CheckpointWriter.write(Checkpoint.scala:184)
	at org.apache.spark.streaming.scheduler.JobGenerator.doCheckpoint(JobGenerator.scala:278)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:169)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$start$1$$anon$1$$anonfun$receive$1.applyOrElse(JobGenerator.scala:78)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$start$1$$anon$1.aroundReceive(JobGenerator.scala:76)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)",,apachespark,cnstar9988,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/15 08:00;cnstar9988;TestStream.java;https://issues.apache.org/jira/secure/attachment/12698959/TestStream.java",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 10:45:46 UTC 2015,,,,,,,,,,"0|i25nfj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/15 07:48;cnstar9988;testcode upload.

It throw Exception every 2 seconds.

15/02/15 15:50:35 ERROR actor.OneForOneStrategy: org.apache.hadoop.conf.Configuration
java.io.NotSerializableException: org.apache.hadoop.conf.Configuration
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1377)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1173)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.defaultWriteObject(ObjectOutputStream.java:440)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$writeObject$1.apply$mcV$sp(DStreamGraph.scala:177)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1075)
	at org.apache.spark.streaming.DStreamGraph.writeObject(DStreamGraph.scala:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.streaming.CheckpointWriter.write(Checkpoint.scala:184)
	at org.apache.spark.streaming.scheduler.JobGenerator.doCheckpoint(JobGenerator.scala:278)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:169)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$start$1$$anon$1$$anonfun$receive$1.applyOrElse(JobGenerator.scala:78)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$start$1$$anon$1.aroundReceive(JobGenerator.scala:76)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107);;;","15/Feb/15 08:02;cnstar9988;I think Checkpoint must not serialize Configuration into disk.;;;","15/Feb/15 08:20;cnstar9988;I put some txt files into /testspark/watchdir, It throws  NullPointerException

15/02/15 16:18:20 WARN dstream.FileInputDStream: Error finding new files
java.lang.NullPointerException
	at org.apache.spark.streaming.api.java.JavaStreamingContext$$anonfun$fn$3$1.apply(JavaStreamingContext.scala:329)
	at org.apache.spark.streaming.api.java.JavaStreamingContext$$anonfun$fn$3$1.apply(JavaStreamingContext.scala:329)
	at org.apache.spark.streaming.dstream.FileInputDStream.org$apache$spark$streaming$dstream$FileInputDStream$$isNewFile(FileInputDStream.scala:215)
	at org.apache.spark.streaming.dstream.FileInputDStream$$anon$3.accept(FileInputDStream.scala:172)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1489)
	at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1523)
	at org.apache.spark.streaming.dstream.FileInputDStream.findNewFiles(FileInputDStream.scala:174)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:132)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:301)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:301)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:288)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:285)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:35)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:301)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:301)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:288)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:285)
	at org.apache.spark.streaming.dstream.ShuffledDStream.compute(ShuffledDStream.scala:41)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:301)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:301)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:300)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:288)
	at scala.Option.orElse(Option.scala:257)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:285)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:38)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:232)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$2.apply(JobGenerator.scala:230)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:230)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:167)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$start$1$$anon$1$$anonfun$receive$1.applyOrElse(JobGenerator.scala:78)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$start$1$$anon$1.aroundReceive(JobGenerator.scala:76)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107);;;","15/Feb/15 09:24;jerryshao;The NPE is caused by setting {{filter}} argument to ""null"" in your code, if you don't need to set filter function, you need to call another fileStream API.

The not serializable exception is actually a bug in FileInputDStream, let me fix it.;;;","15/Feb/15 09:55;jerryshao;Quite curious why DStream#checkpoint() calls {{persist}} internally to cache the RDD, rather than checkpointing RDD, any comments [~tdas]? Thanks a lot.

{code}
  def checkpoint(interval: Duration): DStream[T] = {
    if (isInitialized) {
      throw new UnsupportedOperationException(
        ""Cannot change checkpoint interval of an DStream after streaming context has started"")
    }
    persist()
    checkpointDuration = interval
    this
  }
{code};;;","15/Feb/15 13:16;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/4612;;;","16/Feb/15 01:36;cnstar9988;hi, 
  I see pull/4612, https://github.com/apache/spark/pull/4612/files
  Does serializableConfOpt.map(_.value)  isInstanceOf Configuration, thanks.

;;;","16/Feb/15 02:55;cnstar9988;I merge pull/4612 and rebuild, it works for me, no exception, thanks.

;;;","17/Feb/15 10:45;srowen;Issue resolved by pull request 4612
[https://github.com/apache/spark/pull/4612];;;",,,,,,,,,,,,,,,,,,,,
Failure stopping Services while command line argument is too long,SPARK-5825,12775192,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,chenghao,chenghao,chenghao,15/Feb/15 07:07,17/Apr/15 21:02,14/Jul/23 06:27,19/Feb/15 20:08,1.0.0,,,,,,1.2.2,1.3.0,,,,,Deploy,,,,0,,,,,,"Stopping service in `spark-daemon.sh` will confirm the process id by fuzzy matching the class name, however, it will fail if the java process arguments is very long (greater than 4096).",,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5889,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 18 14:44:57 UTC 2015,,,,,,,,,,"0|i25nev:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"15/Feb/15 07:12;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4611;;;","18/Feb/15 14:44;srowen;Marking as Blocker for the moment since I think we will need to resolve this one way or the other before 1.3.0 is released.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
CTAS should set null format in hive-0.13.1,SPARK-5824,12775178,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,adrian-wang,adrian-wang,adrian-wang,15/Feb/15 04:21,25/Apr/15 21:44,14/Jul/23 06:27,16/Feb/15 20:31,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,adrian-wang,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 20:31:53 UTC 2015,,,,,,,,,,"0|i25nbz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/15 04:25;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4609;;;","16/Feb/15 20:31;marmbrus;Issue resolved by pull request 4609
[https://github.com/apache/spark/pull/4609];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add add-source goal in Scala plugin for Eclipse,SPARK-5822,12775162,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ligangty,ligangty,srowen,14/Feb/15 20:42,14/Feb/15 20:47,14/Jul/23 06:27,14/Feb/15 20:46,1.2.1,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,"I'm opening this just to track https://github.com/apache/spark/pull/4531 :

When import the whole project into eclipse as maven project, found that the
src/main/scala & src/test/scala can not be set as source folder as default
behavior, so add a ""add-source"" goal in scala-maven-plugin to let this work.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 14 20:46:40 UTC 2015,,,,,,,,,,"0|i25n8f:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/15 20:46;srowen;Issue resolved by pull request 4531
[https://github.com/apache/spark/pull/4531];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
JSONRelation and ParquetRelation2 should check if delete is successful for the overwrite operation.,SPARK-5821,12775152,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yanboliang,yanboliang,yanboliang,14/Feb/15 17:29,21/Mar/15 03:22,14/Jul/23 06:27,21/Mar/15 03:17,1.3.0,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"When you run CTAS command such as
{code:sql}
CREATE TEMPORARY TABLE jsonTable
USING org.apache.spark.sql.json.DefaultSource
OPTIONS (
path /a/b/c/d
) AS
SELECT a, b FROM jt
{code}
you will run into failure if you don't have write permission for directory /a/b/c whether d is a directory or file.
{noformat}
Exception in thread ""main"" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/a/b/c/d already exists
        at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1053)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:954)
        at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:863)
        at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1284)
        at org.apache.spark.sql.json.DefaultSource.createRelation(JSONRelation.scala:81)
        at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:300)
        at org.apache.spark.sql.sources.CreateTempTableUsingAsSelect.run(ddl.scala:388)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:55)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:55)
        at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:65)
        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
        at org.apache.spark.sql.DataFrameImpl.<init>(DataFrameImpl.scala:71)
        at org.apache.spark.sql.DataFrameImpl.<init>(DataFrameImpl.scala:58)
        at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:35)
        at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:778)
        at org.apache.spark.sql.Test$.main(Test.scala:149)
        at org.apache.spark.sql.Test.main(Test.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:483)
        at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
{noformat}",,apachespark,lian cheng,yanboliang,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5746,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 21 03:17:09 UTC 2015,,,,,,,,,,"0|i25n67:",9223372036854775807,,,,,,,,,,,,,,1.3.1,1.4.0,,,,,,,,,,,,"14/Feb/15 17:31;yanboliang;The following is one kind of exception log:
Exception in thread ""main"" org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/a/b/c/d already exists
	at org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:132)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1053)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:954)
	at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:863)
	at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1284)
	at org.apache.spark.sql.json.DefaultSource.createRelation(JSONRelation.scala:81)
	at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:300)
	at org.apache.spark.sql.sources.CreateTempTableUsingAsSelect.run(ddl.scala:388)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:55)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:55)
	at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:65)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.DataFrameImpl.<init>(DataFrameImpl.scala:71)
	at org.apache.spark.sql.DataFrameImpl.<init>(DataFrameImpl.scala:58)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:35)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:778)
	at org.apache.spark.sql.Test$.main(Test.scala:149)
	at org.apache.spark.sql.Test.main(Test.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134);;;","14/Feb/15 17:35;yanboliang;This is because that INSERT OVERWRITE in external data sources will first delete the file /a/b/c/d and then save the generated RDD to /a/b/c/d/. However if we don't have write permission for directory /a/b/c, the delete will failure and /a/b/c/d still exists. After that, when call RDD.saveAsTextFile will throw exception.
;;;","14/Feb/15 18:02;apachespark;User 'yanbohappy' has created a pull request for this issue:
https://github.com/apache/spark/pull/4607;;;","15/Feb/15 05:57;apachespark;User 'yanbohappy' has created a pull request for this issue:
https://github.com/apache/spark/pull/4610;;;","15/Feb/15 06:12;yhuai;[~lian cheng] Can you check if we need to address the same issue in parquet relation?;;;","19/Mar/15 13:21;lian cheng;Left comments on GitHub.;;;","20/Mar/15 15:18;apachespark;User 'yanboliang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5107;;;","21/Mar/15 03:17;lian cheng;Issue resolved by pull request 5107
[https://github.com/apache/spark/pull/5107];;;",,,,,,,,,,,,,,,,,,,,,
UDTF column names didn't set properly ,SPARK-5817,12775100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,14/Feb/15 03:47,25/Apr/15 21:44,14/Jul/23 06:27,21/Apr/15 22:11,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"{code}
createQueryTest(""Specify the udtf output"", ""select d from (select explode(array(1,1)) d from src limit 1) t"")
{code}

It throws exception like:
{panel}
org.apache.spark.sql.AnalysisException: cannot resolve 'd' given input columns _c0; line 1 pos 7
	at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:48)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$apply$3$$anonfun$apply$1.applyOrElse(CheckAnalysis.scala:45)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:250)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:50)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:249)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:103)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:117)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:116)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
{panel}
",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 21 22:11:30 UTC 2015,,,,,,,,,,"0|i25muv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/15 03:51;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4602;;;","21/Apr/15 22:11;marmbrus;Issue resolved by pull request 4602
[https://github.com/apache/spark/pull/4602];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add huge backward compatibility warning in DriverWrapper,SPARK-5816,12775090,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,14/Feb/15 01:39,25/Feb/15 01:33,14/Jul/23 06:27,25/Feb/15 01:33,1.3.0,,,,,,1.3.0,,,,,,Deploy,Spark Core,,,0,,,,,,"As of Spark 1.3, we provide backward and forward compatibility in standalone cluster mode through the REST submission gateway. HOWEVER, it nevertheless goes through the legacy o.a.s.deploy.DriverWrapper, and the semantics of the command line arguments there must not change. For instance, this was broken in commit 20a6013106b56a1a1cc3e8cda092330ffbe77cc3.

There is currently no warning against that in the class and so we should add one before it's too late.",,andrewor14,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 19 03:12:16 UTC 2015,,,,,,,,,,"0|i25mrr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"15/Feb/15 13:14;jerryshao;Sorry for wrong link, I just mess the JIRA id. Really sorry about this, seems I cannot delete the comments.;;;","15/Feb/15 18:09;andrewor14;No problem. I removed it.;;;","19/Feb/15 03:12;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4687;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Deprecate SVDPlusPlus APIs that expose DoubleMatrix from JBLAS,SPARK-5815,12775085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,mengxr,mengxr,14/Feb/15 01:06,16/Feb/15 17:05,14/Jul/23 06:27,16/Feb/15 04:41,1.3.0,,,,,,1.3.0,1.4.0,,,,,GraphX,,,,0,,,,,,It is generally bad to expose types defined in a 3rd-party package in Spark public APIs. We should deprecate those methods in SVDPlusPlus and replace them in the next release.,,apachespark,mengxr,michaelmalak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5814,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 17:05:24 UTC 2015,,,,,,,,,,"0|i25mqn:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"15/Feb/15 13:32;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4614;;;","16/Feb/15 04:41;mengxr;Issue resolved by pull request 4614
[https://github.com/apache/spark/pull/4614];;;","16/Feb/15 15:41;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4625;;;","16/Feb/15 17:05;srowen;Also pushed a change to update run() and deprecated runSVDPlusPlus() for 1.4.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Maven Coordinate Inclusion failing in pySpark,SPARK-5810,12775050,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,brkyvz,brkyvz,13/Feb/15 22:11,18/Feb/15 01:48,14/Jul/23 06:27,18/Feb/15 01:48,1.3.0,,,,,,1.3.0,,,,,,Deploy,PySpark,,,0,,,,,,"When including maven coordinates to download dependencies in pyspark, pyspark returns a GatewayError, because it cannot read the proper port to communicate with the JVM. This is because pyspark relies on STDIN to read the port number and in the meantime Ivy prints out a whole lot of logs.",,brkyvz,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2313,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 18 01:48:02 UTC 2015,,,,,,,,,,"0|i25mj3:",9223372036854775807,,,,,joshrosen,,,,,,,,,1.3.0,,,,,,,,,,,,,"17/Feb/15 00:08;joshrosen;I think that this should be fixed now that my patch for SPARK-2313 has been merged.  [~brkyvz], do you think we should add a regression test for this bug?  Do you have tests for Maven coordinate inclusion?;;;","17/Feb/15 00:16;brkyvz;Makes sense to add a regression test. I'll add it with the documentation PR which I'll submit today. I'll ping you on that one so that you can take a look.;;;","18/Feb/15 01:48;brkyvz;Fixed with SPARK-5811 & SPARK-2313;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Assembly generated by sbt does not contain pyspark,SPARK-5808,12775032,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,13/Feb/15 21:22,14/Apr/15 20:41,14/Jul/23 06:27,14/Apr/15 20:41,1.0.0,,,,,,1.4.0,,,,,,Build,,,,0,,,,,,"When you generate the assembly with sbt, the py4j and pyspark files are not added to it. This makes pyspark not work when you run that assembly with yarn, since SPARK_HOME is not propagated in Yarn and thus PythonUtils.scala does not add the needed pyspark paths to PYTHONPATH.

This is minor since all released bits are created by maven, so this should only affect developers who build with sbt and try pyspark on yarn.",,apachespark,lianhuiwang,tgraves,vanzin,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 10 23:26:42 UTC 2015,,,,,,,,,,"0|i25mfb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"10/Apr/15 23:26;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5461;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle creates too many nested directories,SPARK-5801,12774971,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,kayousterhout,kayousterhout,13/Feb/15 17:50,26/Feb/15 17:36,14/Jul/23 06:27,26/Feb/15 17:35,1.2.1,,,,,,1.4.0,,,,,,Shuffle,Spark Core,,,0,,,,,,"When running Spark on EC2, there are 4 nested shuffle directories before the hashed directory names, for example:

/mnt/spark/spark-5824d912-25af-4187-bc6a-29ae42cd78e5/spark-675133f0-b2c8-44a1-8775-5e394674609b/spark-69c1ea15-4e7f-454a-9f57-19763c7bdd17/spark-b036335c-60fa-48ab-a346-f1b420af2027/0c

My understanding is that this should look like:

/mnt/spark/spark-5824d912-25af-4187-bc6a-29ae42cd78e5/0c

This happened when I was using the sort-based shuffle (all default configurations for Spark on EC2).

This is not a correctness problem (the shuffle still works fine).",,apachespark,kayousterhout,Sephiroth-Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5830,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 17:36:15 UTC 2015,,,,,,,,,,"0|i25m1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/15 01:22;Sephiroth-Lin;This is because in standalone, worker will create temp directories for executor like ""/mnt/spark/spark-5824d912-25af-4187-bc6a-29ae42cd78e5/spark-675133f0-b2c8-44a1-8775-5e394674609b"", and on diskblockmanager also will create temp directories like ""/mnt/spark/spark-5824d912-25af-4187-bc6a-29ae42cd78e5/spark-675133f0-b2c8-44a1-8775-5e394674609b/spark-69c1ea15-4e7f-454a-9f57-19763c7bdd17/spark-b036335c-60fa-48ab-a346-f1b420af2027"".;;;","24/Feb/15 19:55;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4747;;;","26/Feb/15 17:35;srowen;Issue resolved by pull request 4747
[https://github.com/apache/spark/pull/4747];;;","26/Feb/15 17:36;srowen;Bumping down priority since it didn't seem to be causing a problem but was not ideal behavior.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
api.java.JavaPairDStream.saveAsNewAPIHadoopFiles may not friendly to java,SPARK-5795,12774837,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,srowen,cnstar9988,cnstar9988,13/Feb/15 09:06,16/Feb/15 19:33,14/Jul/23 06:27,16/Feb/15 19:32,1.2.1,,,,,,1.3.0,,,,,,DStreams,,,,0,,,,,,"import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;

the following code can't compile on java.
JavaPairDStream<Integer, Integer> rs =....
rs.saveAsNewAPIHadoopFiles(""prefix"", ""txt"", Integer.class, Integer.class, TextOutputFormat.class, jobConf);

but similar code in JavaPairRDD works ok.
JavaPairRDD<String, String> counts =.......
counts.saveAsNewAPIHadoopFile(""out"", Text.class, Text.class, TextOutputFormat.class, jobConf);
====================
mybe the 
  def saveAsNewAPIHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ <: NewOutputFormat[_, _]],
      conf: Configuration = new Configuration) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }
=====>
def saveAsNewAPIHadoopFiles[F <: NewOutputFormat[_, _]](
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F],
      conf: Configuration = new Configuration) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }




",,apachespark,cnstar9988,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/15 13:53;cnstar9988;TestStreamCompile.java;https://issues.apache.org/jira/secure/attachment/12698728/TestStreamCompile.java",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 19:32:52 UTC 2015,,,,,,,,,,"0|i25l7z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/15 09:13;cnstar9988;org.apache.spark.api.java.JavaPairRDD<K, V>
{noformat}
/** Output the RDD to any Hadoop-supported file system. */
  def saveAsHadoopFile[F <: OutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F],
      conf: JobConf) {
    rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass, conf)
  }

  /** Output the RDD to any Hadoop-supported file system. */
  def saveAsHadoopFile[F <: OutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F]) {
    rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass)
  }

  /** Output the RDD to any Hadoop-supported file system, compressing with the supplied codec. */
  def saveAsHadoopFile[F <: OutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F],
      codec: Class[_ <: CompressionCodec]) {
    rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass, codec)
  }

  /** Output the RDD to any Hadoop-supported file system. */
  def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F],
      conf: Configuration) {
    rdd.saveAsNewAPIHadoopFile(path, keyClass, valueClass, outputFormatClass, conf)
  }

  /**
   * Output the RDD to any Hadoop-supported storage system, using
   * a Configuration object for that storage system.
   */
  def saveAsNewAPIHadoopDataset(conf: Configuration) {
    rdd.saveAsNewAPIHadoopDataset(conf)
  }

  /** Output the RDD to any Hadoop-supported file system. */
  def saveAsNewAPIHadoopFile[F <: NewOutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F]) {
    rdd.saveAsNewAPIHadoopFile(path, keyClass, valueClass, outputFormatClass)
  }
{noformat}

org.apache.spark.streaming.api.java.JavaPairDStream<K, V>

{noformat}
/**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: ""prefix-TIME_IN_MS.suffix"".
   */
  def saveAsHadoopFiles[F <: OutputFormat[K, V]](prefix: String, suffix: String) {
    dstream.saveAsHadoopFiles(prefix, suffix)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: ""prefix-TIME_IN_MS.suffix"".
   */
  def saveAsHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ <: OutputFormat[_, _]]) {
    dstream.saveAsHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: ""prefix-TIME_IN_MS.suffix"".
   */
  def saveAsHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ <: OutputFormat[_, _]],
      conf: JobConf) {
    dstream.saveAsHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: ""prefix-TIME_IN_MS.suffix"".
   */
  def saveAsNewAPIHadoopFiles[F <: NewOutputFormat[K, V]](prefix: String, suffix: String) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: ""prefix-TIME_IN_MS.suffix"".
   */
  def saveAsNewAPIHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ <: NewOutputFormat[_, _]]) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: ""prefix-TIME_IN_MS.suffix"".
   */
  def saveAsNewAPIHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ <: NewOutputFormat[_, _]],
      conf: Configuration = new Configuration) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }
{noformat};;;","13/Feb/15 10:25;srowen;When you say ""doesn't compile"", you should show the compilation error. Although I think I know what it is. There's a workaround but I agree we can look at fixing it. If it breaks binary compatibility, it would have to wait until later.;;;","13/Feb/15 13:50;cnstar9988;error info...
The method saveAsNewAPIHadoopFiles(String, String, Class<?>, Class<?>, Class<? extends OutputFormat<?,?>>) in the type JavaPairDStream<Integer,Integer> is not applicable for the arguments (String, String, Class<Integer>, Class<Integer>, Class<TextOutputFormat>)


;;;","13/Feb/15 13:53;cnstar9988;my testcase on java 1.7 and spark 1.3 trunk.
Thanks.;;;","13/Feb/15 14:08;cnstar9988;Does it same problem as SPARK-5297, thanks.
;;;","15/Feb/15 00:15;srowen;Yes, I've seen the same problem and been meaning to do something about it. It makes you do this to use {{JavaPairDStream}}:
https://github.com/OryxProject/oryx/blob/master/oryx-lambda/src/main/java/com/cloudera/oryx/lambda/BatchLayer.java#L187

So basically, this is how it's declared now:

{code}
  def saveAsNewAPIHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ <: NewOutputFormat[_, _]],
      conf: Configuration = new Configuration) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }
{code}

but this works, and is how it works in {{JavaPairRDD}}:

{code}
  def saveAsNewAPIHadoopFiles[F <: NewOutputFormat[_, _]](
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F],
      conf: Configuration = new Configuration) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }
{code}

I worry about an API change of course, but, I think the current API isn't directly callable, so seems OK to change.

For a simple demo, try compiling this:

{code}
JavaPairDStream<IntWritable,Text> pds = null; 
pds.saveAsNewAPIHadoopFiles("""", """", IntWritable.class, IntWritable.class, SequenceFileOutputFormat.class);
{code}

The change above makes it work. I'll open a PR. I bumped the priority based on my understanding of the issue.;;;","15/Feb/15 00:17;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4608;;;","15/Feb/15 07:42;cnstar9988;I merge pull/4608 and rebuild, it works for me, thanks.;;;","16/Feb/15 19:32;srowen;Issue resolved by pull request 4608
[https://github.com/apache/spark/pull/4608];;;",,,,,,,,,,,,,,,,,,,,
add jar should return 0,SPARK-5794,12774823,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,adrian-wang,adrian-wang,adrian-wang,13/Feb/15 07:47,25/Apr/15 21:46,14/Jul/23 06:27,14/Apr/15 01:26,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,,,adrian-wang,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 01:26:33 UTC 2015,,,,,,,,,,"0|i25l4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/15 07:48;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4586;;;","14/Apr/15 01:26;marmbrus;Issue resolved by pull request 4586
[https://github.com/apache/spark/pull/4586];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Capture exceptions in Python write thread ,SPARK-5788,12774723,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,12/Feb/15 22:28,17/Feb/15 01:58,14/Jul/23 06:27,17/Feb/15 01:58,1.2.1,1.3.0,,,,,1.2.2,1.3.0,,,,,PySpark,,,,0,,,,,,The exception in Python writer thread will shutdown executor.,,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 01:58:21 UTC 2015,,,,,,,,,,"0|i25kin:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"12/Feb/15 22:31;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4577;;;","17/Feb/15 01:58;joshrosen;Issue resolved by pull request 4577
[https://github.com/apache/spark/pull/4577];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
JIRA version not of form x.y.z breaks merge_spark_pr.py,SPARK-5776,12774653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,12/Feb/15 18:48,12/Feb/15 20:16,14/Jul/23 06:27,12/Feb/15 20:16,,,,,,,1.4.0,,,,,,Project Infra,,,,0,,,,,,"It appears that the version ""2+"" I added to JIRA breaks the merge script since it expects x.y.z only. I will try to patch the logic quickly. Worst case, we can name the version ""2.0.0"" if we have to.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 20:16:02 UTC 2015,,,,,,,,,,"0|i25k3r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/15 19:06;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4570;;;","12/Feb/15 20:16;srowen;Issue resolved by pull request 4570
[https://github.com/apache/spark/pull/4570];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
GenericRow cannot be cast to SpecificMutableRow when nested data and partitioned table,SPARK-5775,12774607,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,Ayoub,Ayoub,12/Feb/15 15:34,06/Oct/15 19:18,14/Jul/23 06:27,28/Feb/15 13:18,1.2.1,,,,,,1.3.0,,,,,,SQL,,,,0,hivecontext,nested,parquet,partition,,"Using the ""LOAD"" sql command in Hive context to load parquet files into a partitioned table causes exceptions during query time. 
The bug requires the table to have a column of *type Array of struct* and to be *partitioned*. 

The example bellow shows how to reproduce the bug and you can see that if the table is not partitioned the query works fine. 

{noformat}
scala> val data1 = """"""{""data_array"":[{""field1"":1,""field2"":2}]}""""""
scala> val data2 = """"""{""data_array"":[{""field1"":3,""field2"":4}]}""""""
scala> val jsonRDD = sc.makeRDD(data1 :: data2 :: Nil)
scala> val schemaRDD = hiveContext.jsonRDD(jsonRDD)
scala> schemaRDD.printSchema
root
 |-- data_array: array (nullable = true)
 |    |-- element: struct (containsNull = false)
 |    |    |-- field1: integer (nullable = true)
 |    |    |-- field2: integer (nullable = true)

scala> hiveContext.sql(""create external table if not exists partitioned_table(data_array ARRAY <STRUCT<field1: INT, field2: INT>>) Partitioned by (date STRING) STORED AS PARQUET Location 'hdfs://****/partitioned_table'"")
scala> hiveContext.sql(""create external table if not exists none_partitioned_table(data_array ARRAY <STRUCT<field1: INT, field2: INT>>) STORED AS PARQUET Location 'hdfs://****/none_partitioned_table'"")

scala> schemaRDD.saveAsParquetFile(""hdfs://****/tmp_data_1"")
scala> schemaRDD.saveAsParquetFile(""hdfs://****/tmp_data_2"")

scala> hiveContext.sql(""LOAD DATA INPATH 'hdfs://qa-hdc001.ffm.nugg.ad:8020/erlogd/tmp_data_1' INTO TABLE partitioned_table PARTITION(date='2015-02-12')"")
scala> hiveContext.sql(""LOAD DATA INPATH 'hdfs://qa-hdc001.ffm.nugg.ad:8020/erlogd/tmp_data_2' INTO TABLE none_partitioned_table"")

scala> hiveContext.sql(""select data.field1 from none_partitioned_table LATERAL VIEW explode(data_array) nestedStuff AS data"").collect
res23: Array[org.apache.spark.sql.Row] = Array([1], [3])

scala> hiveContext.sql(""select data.field1 from partitioned_table LATERAL VIEW explode(data_array) nestedStuff AS data"").collect

15/02/12 16:21:03 INFO ParseDriver: Parsing command: select data.field1 from partitioned_table LATERAL VIEW explode(data_array) nestedStuff AS data
15/02/12 16:21:03 INFO ParseDriver: Parse Completed
15/02/12 16:21:03 INFO MemoryStore: ensureFreeSpace(260661) called with curMem=0, maxMem=280248975
15/02/12 16:21:03 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 254.6 KB, free 267.0 MB)
15/02/12 16:21:03 INFO MemoryStore: ensureFreeSpace(28615) called with curMem=260661, maxMem=280248975
15/02/12 16:21:03 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 27.9 KB, free 267.0 MB)
15/02/12 16:21:03 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on *****:51990 (size: 27.9 KB, free: 267.2 MB)
15/02/12 16:21:03 INFO BlockManagerMaster: Updated info of block broadcast_18_piece0
15/02/12 16:21:03 INFO SparkContext: Created broadcast 18 from NewHadoopRDD at ParquetTableOperations.scala:119
15/02/12 16:21:03 INFO FileInputFormat: Total input paths to process : 3
15/02/12 16:21:03 INFO ParquetInputFormat: Total input paths to process : 3
15/02/12 16:21:03 INFO FilteringParquetRowInputFormat: Using Task Side Metadata Split Strategy
15/02/12 16:21:03 INFO SparkContext: Starting job: collect at SparkPlan.scala:84
15/02/12 16:21:03 INFO DAGScheduler: Got job 12 (collect at SparkPlan.scala:84) with 3 output partitions (allowLocal=false)
15/02/12 16:21:03 INFO DAGScheduler: Final stage: Stage 13(collect at SparkPlan.scala:84)
15/02/12 16:21:03 INFO DAGScheduler: Parents of final stage: List()
15/02/12 16:21:03 INFO DAGScheduler: Missing parents: List()
15/02/12 16:21:03 INFO DAGScheduler: Submitting Stage 13 (MappedRDD[111] at map at SparkPlan.scala:84), which has no missing parents
15/02/12 16:21:03 INFO MemoryStore: ensureFreeSpace(7632) called with curMem=289276, maxMem=280248975
15/02/12 16:21:03 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 7.5 KB, free 267.0 MB)
15/02/12 16:21:03 INFO MemoryStore: ensureFreeSpace(4230) called with curMem=296908, maxMem=280248975
15/02/12 16:21:03 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 4.1 KB, free 267.0 MB)
15/02/12 16:21:03 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on *****:51990 (size: 4.1 KB, free: 267.2 MB)
15/02/12 16:21:03 INFO BlockManagerMaster: Updated info of block broadcast_19_piece0
15/02/12 16:21:03 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:838
15/02/12 16:21:03 INFO DAGScheduler: Submitting 3 missing tasks from Stage 13 (MappedRDD[111] at map at SparkPlan.scala:84)
15/02/12 16:21:03 INFO TaskSchedulerImpl: Adding task set 13.0 with 3 tasks
15/02/12 16:21:03 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 48, *****, NODE_LOCAL, 1640 bytes)
15/02/12 16:21:03 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 49, *****, NODE_LOCAL, 1641 bytes)
15/02/12 16:21:03 INFO TaskSetManager: Starting task 2.0 in stage 13.0 (TID 50, *****, NODE_LOCAL, 1640 bytes)
15/02/12 16:21:03 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on *****:39729 (size: 4.1 KB, free: 133.6 MB)
15/02/12 16:21:03 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on *****:48213 (size: 4.1 KB, free: 133.6 MB)
15/02/12 16:21:04 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on *****:45394 (size: 4.1 KB, free: 133.6 MB)
15/02/12 16:21:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on *****:39729 (size: 27.9 KB, free: 133.6 MB)
15/02/12 16:21:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on *****:48213 (size: 27.9 KB, free: 133.6 MB)
15/02/12 16:21:04 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on *****:45394 (size: 27.9 KB, free: 133.6 MB)
15/02/12 16:21:04 WARN TaskSetManager: Lost task 0.0 in stage 13.0 (TID 48, *****): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to org.apache.spark.sql.catalyst.expressions.SpecificMutableRow
  at org.apache.spark.sql.parquet.ParquetTableScan$$anonfun$execute$4$$anon$1.next(ParquetTableOperations.scala:147)
  at org.apache.spark.sql.parquet.ParquetTableScan$$anonfun$execute$4$$anon$1.next(ParquetTableOperations.scala:144)
  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
  at scala.collection.AbstractIterator.to(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
  at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:797)
  at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:797)
  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
  at org.apache.spark.scheduler.Task.run(Task.scala:56)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:744)

15/02/12 16:21:04 INFO TaskSetManager: Starting task 0.1 in stage 13.0 (TID 51, *****, NODE_LOCAL, 1640 bytes)
15/02/12 16:21:04 INFO TaskSetManager: Lost task 1.0 in stage 13.0 (TID 49) on executor *****: java.lang.ClassCastException (org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to org.apache.spark.sql.catalyst.expressions.SpecificMutableRow) [duplicate 1]
15/02/12 16:21:04 INFO TaskSetManager: Starting task 1.1 in stage 13.0 (TID 52, *****, NODE_LOCAL, 1641 bytes)
15/02/12 16:21:04 INFO TaskSetManager: Lost task 0.1 in stage 13.0 (TID 51) on executor *****: java.lang.ClassCastException (org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to org.apache.spark.sql.catalyst.expressions.SpecificMutableRow) [duplicate 2]
15/02/12 16:21:04 INFO TaskSetManager: Starting task 0.2 in stage 13.0 (TID 53, *****, NODE_LOCAL, 1640 bytes)
15/02/12 16:21:04 INFO TaskSetManager: Finished task 2.0 in stage 13.0 (TID 50) in 405 ms on ***** (1/3)
15/02/12 16:21:04 INFO TaskSetManager: Lost task 1.1 in stage 13.0 (TID 52) on executor *****: java.lang.ClassCastException (org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to org.apache.spark.sql.catalyst.expressions.SpecificMutableRow) [duplicate 3]
15/02/12 16:21:04 INFO TaskSetManager: Starting task 1.2 in stage 13.0 (TID 54, *****, NODE_LOCAL, 1641 bytes)
15/02/12 16:21:04 INFO TaskSetManager: Lost task 0.2 in stage 13.0 (TID 53) on executor *****: java.lang.ClassCastException (org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to org.apache.spark.sql.catalyst.expressions.SpecificMutableRow) [duplicate 4]
15/02/12 16:21:04 INFO TaskSetManager: Starting task 0.3 in stage 13.0 (TID 55, *****, NODE_LOCAL, 1640 bytes)
15/02/12 16:21:04 INFO TaskSetManager: Lost task 1.2 in stage 13.0 (TID 54) on executor *****: java.lang.ClassCastException (org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to org.apache.spark.sql.catalyst.expressions.SpecificMutableRow) [duplicate 5]
15/02/12 16:21:04 INFO TaskSetManager: Starting task 1.3 in stage 13.0 (TID 56, *****, NODE_LOCAL, 1641 bytes)
15/02/12 16:21:04 INFO TaskSetManager: Lost task 0.3 in stage 13.0 (TID 55) on executor *****: java.lang.ClassCastException (org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to org.apache.spark.sql.catalyst.expressions.SpecificMutableRow) [duplicate 6]
15/02/12 16:21:04 ERROR TaskSetManager: Task 0 in stage 13.0 failed 4 times; aborting job
15/02/12 16:21:04 INFO TaskSchedulerImpl: Cancelling stage 13
15/02/12 16:21:04 INFO TaskSchedulerImpl: Stage 13 was cancelled
15/02/12 16:21:04 INFO DAGScheduler: Job 12 failed: collect at SparkPlan.scala:84, took 0.556942 s
15/02/12 16:21:04 WARN TaskSetManager: Lost task 1.3 in stage 13.0 (TID 56, *****): TaskKilled (killed intentionally)
15/02/12 16:21:04 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 4 times, most recent failure: Lost task 0.3 in stage 13.0 (TID 55, *****): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.GenericRow cannot be cast to org.apache.spark.sql.catalyst.expressions.SpecificMutableRow
  at org.apache.spark.sql.parquet.ParquetTableScan$$anonfun$execute$4$$anon$1.next(ParquetTableOperations.scala:147)
  at org.apache.spark.sql.parquet.ParquetTableScan$$anonfun$execute$4$$anon$1.next(ParquetTableOperations.scala:144)
  at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
  at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
  at scala.collection.AbstractIterator.to(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
  at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:797)
  at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:797)
  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
  at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1353)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
  at org.apache.spark.scheduler.Task.run(Task.scala:56)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:200)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:744)

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
  at scala.Option.foreach(Option.scala:236)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
  at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
  at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
  at akka.actor.ActorCell.invoke(ActorCell.scala:487)
  at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
  at akka.dispatch.Mailbox.run(Mailbox.scala:220)
  at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
  at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
  at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
  at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


{noformat}
",,apachespark,avignon,Ayoub,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 06 19:18:04 UTC 2015,,,,,,,,,,"0|i25jtj:",9223372036854775807,,,,,lian cheng,,,,,,,,,1.3.0,,,,,,,,,,,,,"19/Feb/15 18:47;apachespark;User 'anselmevignon' has created a pull request for this issue:
https://github.com/apache/spark/pull/4697;;;","19/Feb/15 18:48;avignon;This bug is due to a problem in the TableScanOperations, involving indeed partition columns and complex type columns.

I made a pull request patching up the issue here :

https://github.com/apache/spark/pull/4697;;;","25/Feb/15 07:36;avignon;[~marmbrus][~lian cheng] Hi,

I'm quite new on the process of debugging spark, but the pull request I updated 5 days ago (referenced above) seems to be solving this issue.

Or did I miss something ? 

Cheers (and thanks for the awesome work),

Anselme;;;","26/Feb/15 19:30;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4792;;;","26/Feb/15 19:42;lian cheng;Hey [~avignon], sorry for the delay. I've left comments on the PR page. Thanks a lot for working on this!;;;","27/Feb/15 00:24;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4798;;;","06/Oct/15 19:18;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/8999;;;",,,,,,,,,,,,,,,,,,,,,,
Number of Cores in Completed Applications of Standalone Master Web Page always be 0 if sc.stop() is called,SPARK-5771,12774559,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,marsishandsome,marsishandsome,marsishandsome,12/Feb/15 13:14,25/Mar/15 20:28,14/Jul/23 06:27,25/Mar/15 20:28,1.2.1,,,,,,1.4.0,,,,,,Web UI,,,,0,,,,,,"In Standalone mode, the number of cores in Completed Applications of the Master Web Page will always be zero, if sc.stop() is called.

But the number will always be right, if sc.stop() is not called.

The reason maybe: 
after sc.stop() is called, the function removeExecutor of class ApplicationInfo will be called, thus reduce the variable coresGranted to zero.  The variable coresGranted is used to display the number of Cores on the Web Page.",,apachespark,marsishandsome,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5076,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 25 00:23:02 UTC 2015,,,,,,,,,,"0|i25jiv:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"12/Feb/15 13:23;apachespark;User 'marsishandsome' has created a pull request for this issue:
https://github.com/apache/spark/pull/4567;;;","25/Feb/15 14:51;srowen;Issue resolved by pull request 4567
[https://github.com/apache/spark/pull/4567];;;","27/Feb/15 02:03;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/4800;;;","01/Mar/15 05:32;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/4841;;;","25/Mar/15 00:23;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/5177;;;",,,,,,,,,,,,,,,,,,,,,,,,
Migrate Parquet data source to the write support of data source API,SPARK-5767,12774524,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,12/Feb/15 10:29,16/Feb/15 09:39,14/Jul/23 06:27,16/Feb/15 09:39,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,Migrate to the newly introduced data source write support API (SPARK-5658). Add support for overwriting and appending to existing tables.,,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 09:39:24 UTC 2015,,,,,,,,,,"0|i25jb3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"12/Feb/15 10:37;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4563;;;","16/Feb/15 09:39;lian cheng;Issue resolved by pull request 4563
[https://github.com/apache/spark/pull/4563];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle write time is incorrect for sort-based shuffle,SPARK-5762,12774481,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kayousterhout,kayousterhout,kayousterhout,12/Feb/15 08:18,13/Feb/15 00:17,14/Jul/23 06:27,12/Feb/15 22:47,1.2.1,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"For the sort-based shuffle, when bypassing merge sort, one file is written for each partition, and then a final file is written that concatenates all of the existing files together. The time to write this final file is not included in the shuffle write time.",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 08:26:28 UTC 2015,,,,,,,,,,"0|i25j1r:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"12/Feb/15 08:26;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/4559;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revamp StandaloneRestProtocolSuite,SPARK-5761,12774474,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,12/Feb/15 07:52,12/Feb/15 22:48,14/Jul/23 06:27,12/Feb/15 22:48,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,It currently runs an end-to-end test which is both slow and reported as flaky here: SPARK-5690. We should make it test the individual components more closely and make it more like unit test suite instead.,,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5690,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 07:56:19 UTC 2015,,,,,,,,,,"0|i25j07:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"12/Feb/15 07:56;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4557;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
StandaloneRestClient/Server error behavior is incorrect,SPARK-5760,12774473,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,12/Feb/15 07:51,12/Feb/15 22:48,14/Jul/23 06:27,12/Feb/15 22:48,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"There are three main known issues:

(1) Server would always send the JSON to the servlet's output stream. However, if the response code is not 200, the client reads from the error stream instead. The server must write to the correct stream depending on the response code.

(2) If the server returns an empty response (no JSON), then both output and error streams are null at the client, leading to NPEs. This happens if the server throws an internal exception that it cannot recover from.

(3) The default error handling servlet did not match the URL cases correctly, because there are empty strings in the list.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 07:56:18 UTC 2015,,,,,,,,,,"0|i25izz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"12/Feb/15 07:56;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4557;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorRunnable should catch YarnException while NMClient start container,SPARK-5759,12774464,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lianhuiwang,lianhuiwang,lianhuiwang,12/Feb/15 06:59,17/Feb/15 18:57,14/Jul/23 06:27,12/Feb/15 22:51,1.2.0,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"some time since some of reasons, it lead to some exception while NMClient start container.example:we do not config spark_shuffle on some machines, so it will throw a exception:
java.lang.Error: org.apache.hadoop.yarn.exceptions.InvalidAuxServiceException: The auxService:spark_shuffle does not exist.
 because YarnAllocator use ThreadPoolExecutor to start Container, so we can not find which container or hostname throw exception. I think we should catch YarnException  in ExecutorRunnable  when start container. if there are some exceptions, we can know the container id or hostname of failed container.",,apachespark,lianhuiwang,mkanchwala,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5721,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 07:16:04 UTC 2015,,,,,,,,,,"0|i25ixz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"12/Feb/15 07:16;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4554;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Analyzer should not throw scala.NotImplementedError for illegitimate sql,SPARK-5756,12774448,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scwf,scwf,scwf,12/Feb/15 05:55,25/Apr/15 21:47,14/Jul/23 06:27,13/Feb/15 13:05,1.2.0,,,,,,,,,,,,SQL,,,,0,,,,,,"```SELECT CAST(x AS STRING) FROM src```  comes a NotImplementedError:
  CliDriver: scala.NotImplementedError: an implementation is missing
        at scala.Predef$.$qmark$qmark$qmark(Predef.scala:252)
        at org.apache.spark.sql.catalyst.expressions.PrettyAttribute.dataType(namedExpressions.scala:221)
        at org.apache.spark.sql.catalyst.expressions.Cast.resolved$lzycompute(Cast.scala:30)
        at org.apache.spark.sql.catalyst.expressions.Cast.resolved(Cast.scala:30)
        at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$childrenResolved$1.apply(Expression.scala:68)
        at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$childrenResolved$1.apply(Expression.scala:68)
        at scala.collection.LinearSeqOptimized$class.exists(LinearSeqOptimized.scala:80)
        at scala.collection.immutable.List.exists(List.scala:84)
        at org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:68)
        at org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:56)
        at org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:56)
        at org.apache.spark.sql.catalyst.expressions.NamedExpression.typeSuffix(namedExpressions.scala:62)
        at org.apache.spark.sql.catalyst.expressions.Alias.toString(namedExpressions.scala:124)
        at org.apache.spark.sql.catalyst.expressions.Expression.prettyString(Expression.scala:78)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$1$$anonfun$7.apply(Analyzer.scala:83)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$1$$anonfun$7.apply(Analyzer.scala:83)
        at scala.collection.immutable.Stream.map(Stream.scala:376)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$1.applyOrElse(Analyzer.scala:83)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$1.applyOrElse(Analyzer.scala:81)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:204)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$.apply(Analyzer.scala:81)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$.apply(Analyzer.scala:79)",,apachespark,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 05:57:14 UTC 2015,,,,,,,,,,"0|i25iuf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"12/Feb/15 05:57;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/4552;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark AM not launching on Windows,SPARK-5754,12774433,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cbvoxel,goiri,goiri,12/Feb/15 04:28,20/Aug/15 13:48,14/Jul/23 06:27,19/Aug/15 17:20,1.1.1,1.2.0,,,,,1.6.0,,,,,,Windows,YARN,,,2,,,,,,"I'm trying to run Spark Pi on a YARN cluster running on Windows and the AM container fails to start. The problem seems to be in the generation of the YARN command which adds single quotes (') surrounding some of the java options. In particular, the part of the code that is adding those is the escapeForShell function in YarnSparkHadoopUtil. Apparently, Windows does not like the quotes for these options. Here is an example of the command that the container tries to execute:

@call %JAVA_HOME%/bin/java -server -Xmx512m -Djava.io.tmpdir=%PWD%/tmp '-Dspark.yarn.secondary.jars=' '-Dspark.app.name=org.apache.spark.examples.SparkPi' '-Dspark.master=yarn-cluster' org.apache.spark.deploy.yarn.ApplicationMaster --class 'org.apache.spark.examples.SparkPi' --jar  'file:/D:/data/spark-1.1.1-bin-hadoop2.4/bin/../lib/spark-examples-1.1.1-hadoop2.4.0.jar'  --executor-memory 1024 --executor-cores 1 --num-executors 2

Once I transform it into:

@call %JAVA_HOME%/bin/java -server -Xmx512m -Djava.io.tmpdir=%PWD%/tmp -Dspark.yarn.secondary.jars= -Dspark.app.name=org.apache.spark.examples.SparkPi -Dspark.master=yarn-cluster org.apache.spark.deploy.yarn.ApplicationMaster --class 'org.apache.spark.examples.SparkPi' --jar  'file:/D:/data/spark-1.1.1-bin-hadoop2.4/bin/../lib/spark-examples-1.1.1-hadoop2.4.0.jar'  --executor-memory 1024 --executor-cores 1 --num-executors 2

Everything seems to start.

How should I deal with this? Creating a separate function like escapeForShell for Windows and call it whenever I detect this is for Windows? Or should I add some sanity check on YARN?

I checked a little and there seems to be people that is able to run Spark on YARN on Windows, so it might be something else. I didn't find anything related on Jira either.","Windows Server 2012, Hadoop 2.4.1.",apachespark,cbvoxel,elgoiri,goiri,imm123,snnn,stevel@apache.org,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2221,SPARK-5033,SPARK-7700,,,,,SPARK-5034,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Aug 08 22:41:04 UTC 2015,,,,,,,,,,"0|i25irj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Feb/15 09:08;srowen;We just resolved http://issues.apache.org/jira/browse/SPARK-4267 which is kind of the flip side to this problem. Now, all arguments are quoted since they may contain spaces, and spaces would break the command.

Some quoting seems to be needed at some level to handle this case. I wonder why single quote is problematic in Windows? does it have to be double quote?

Maybe the logic can be improved to only quote args with a space in them but that still leaves the question of how to correctly quote args with spaces in Windows.;;;","12/Feb/15 18:04;goiri;So, I did some test of what works and what doesn't:
* '-Dspark.driver.port=21390': Error: Could not find or load main class '-Dspark.driver.port=21390'
* -Dspark.driver.port=21390: OK
* ""-Dspark.driver.port=21390"": OK
* -Dspark.driver.port='21390': OK
* -Dspark.driver.port=""21390"": OK

Another one that is problematic is:
* -XX:OnOutOfMemoryError='kill %p': Error: Could not find or load main class ...
* -XX:OnOutOfMemoryError=""kill %p"": Java cannot parse the option
* ""-XX:OnOutOfMemoryError='kill %p'"": Java cannot parse the option



;;;","07/May/15 20:20;stevel@apache.org;I had a look at what we did for slider via our [JavaCommandLineBuilder|https://github.com/apache/incubator-slider/blob/develop/slider-core/src/main/java/org/apache/slider/core/launch/JavaCommandLineBuilder.java]

We just seem to be building up the full command line for %JAVA_HOME%/bin/java without needing spaces between things. (what we do have is SLIDER-87, commas in JVM args on unix...)

Hadoop on windows starts a container process in the native file {{service.c}}, which uses the windows {{CreateProcess(}} call, there's some coverage of its quotation logic in [MSDN|https://msdn.microsoft.com/en-us/library/windows/desktop/ms682425%28v=vs.85%29.aspx]

one possibility here is that java home has a space in and that is causing confusion. [~goiri] -what is your JAVA_HOME value?;;;","07/May/15 20:23;elgoiri;My JAVA_HOME is D:\data\OpenJDK7. Thanks for looking into this.;;;","07/May/15 20:24;elgoiri;By the way, I manually tried to run those commands from cmd.exe and that's when I got:
•'-Dspark.driver.port=21390': Error: Could not find or load main class '-Dspark.driver.port=21390'
•-Dspark.driver.port=21390: OK
•""-Dspark.driver.port=21390"": OK
•-Dspark.driver.port='21390': OK
•-Dspark.driver.port=""21390"": OK
;;;","07/May/15 20:52;stevel@apache.org;I think windows will need its own escaping logic -once we can work out what it is. Something like that should really go into hadoop common (with a test); then it can be ported into spark until the versions are in sync.;;;","19/Jun/15 09:01;snnn;Still exist in spark 1.4.;;;","29/Jul/15 20:43;cbvoxel;And in 1.5.0. Actually I have quickly hacked a couple of lines to fix this issue. However, I also suggest that a command helper should be written, such that we populate a List of CommandEntry-Objects (oho creative naming!!) that each have key, value and a keyValueSeparator. We could assemble the correct command (either as a String or a List of Strings) plattform dependent. That way we could make sure, that commands don't break the container, regardless of the plattform. 

One more thought regarding the above mentioned persisting problem with 

-XX:OnOutOfMemoryError='kill %p'

I have checked and it seems to come from the fact, that cmd expects a environment variable at %p. Consequently every thing breaks. One way to deal with this is using '%%'. Again, this kind of stuff should be in a helper class. I could contribute if that is okay. However, I never have so a bit of help would be appreciated!
;;;","29/Jul/15 21:14;elgoiri;Thank you [~cbvoxel]] for looking into this.
I can send any patches you need, just let me know what you need.;;;","31/Jul/15 10:52;apachespark;User 'dharmeshkakadia' has created a pull request for this issue:
https://github.com/apache/spark/pull/7829;;;","31/Jul/15 15:38;cbvoxel;Thanks, I will put in more time and come up with a suggestion. 
Catch ya later;;;","01/Aug/15 16:40;cbvoxel;Okay so I have thought about this more and kinda have a ""qualified"" opinion now.

I assume that you have fixed this issue for your problem? Did you write a separate escapeForShell for Windows? 

I have and I would like to suggest something like that for a PR. How did you solve this?;;;","01/Aug/15 23:02;elgoiri;I overwrote the existing escapeForShell to use single quotes instead of double and I removed the ""-XX:OnOutOfMemoryError='kill %p'"" part in the command. This is just an internal solution for me but ideally this should check the OS and so on.;;;","02/Aug/15 10:01;cbvoxel;Ah I see! Yes for a single situation such as yours, mine and the others here that need to use spark on hadoop/yarn/windows this will work. I actually did something like that too ^^ 

It seems however to me we should find a good solution to fix this, for good. The discussion on dharmeshkakadia PR is useful.;;;","02/Aug/15 19:10;apachespark;User 'cbvoxel' has created a pull request for this issue:
https://github.com/apache/spark/pull/7872;;;","08/Aug/15 12:40;cbvoxel;Okay, looking over the case and not making the project admin's happy with my solution (and damn rightly so!!) I revisited the problem. 

It occurs to me, that this is *not* really a Windows cmd Problem. It rather seems to be a problem with the jvm in combination with cmd. My findings:

Firstly,
-XX:OnOutOfMemoryError='kill %p'
will never ever work on Windows cmd. So far this is a cmd-specific situation. The reason why this is an issue lies in the fact that only double quotes are respected by cmd for arguments that belong together. (See http://ss64.com/nt/syntax-esc.html) Everything with a space thus must be enclosed by double quotes! That was in essence dharmeshkakadia suggestion as a PR.

Secondly,
-XX:OnOutOfMemoryError='kill %p'
will only work if you got 'kill' installed. As having GNU Tools a must for anyone that needs to build hadoop from scratch on Windows it will not really come up as a problem. Still, should be fixed to 
-XX:OnOutOfMemoryError=""taskkill /F /PID %p""
see e.g. https://www.java.net/node/692850

Thirdly, 
Usually cmd expects, as written earlier, that if a % is read, that a environment variable is about to be parsed. Although I wasn't able to really verify that this is a problem on the command line, it seems to be an issue in .cmd files. The correct way to avoid any hassle is using %%. In fact, please look at 
http://www.oracle.com/technetwork/java/javase/clopts-139448.html#gbmum
it is explained that %p and %%p are treated equally and that they map to the current PID. 

Thus, I suggest the basic escaping logic in spark to distuingish the quoting between unix (') and windows (""). Furthermore, all option variables shall be written with %%, as the JVM interprets them as a single % completely platform independent. The last change suggestion should be in distuingishing which ""kill"" command to use, according to platform.

I will close my PR and open a new one, once I have verified in action all that I have written here. I would be very glad about comments and experience.
;;;","08/Aug/15 13:16;cbvoxel;BTW, the reason why 
'-Dspark.driver.port=21390'
doesn't work is a JVM issue, since it takes the enclosed information ""as is"" if single quotes are used, so it thinks this is a java class which happens to start with a '-'. I am not sure, if this is a Windows JVM specific situation, or if this happens in Linux just the same.;;;","08/Aug/15 22:41;apachespark;User 'cbvoxel' has created a pull request for this issue:
https://github.com/apache/spark/pull/8053;;;",,,,,,,,,,,
Flaky test: o.a.s.sql.hive.thriftserver.HiveThriftServer2Suite sometimes times out,SPARK-5751,12774406,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,lian cheng,lian cheng,12/Feb/15 01:21,25/Feb/15 04:35,14/Jul/23 06:27,25/Feb/15 00:35,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,flaky-test,,,,,"The ""Test JDBC query execution"" test case times out occasionally, all other test cases are just fine. The failure output only contains service startup command line without any log output. Guess somehow the test case misses the log file path.",,apachespark,joshrosen,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 04:35:53 UTC 2015,,,,,,,,,,"0|i25ilr:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"19/Feb/15 05:30;joshrosen;I took a brief look at this today and have a few thoughts on why this might be hanging.  I noticed that we first bind a socket in Java then release it and re-use that port number for the Thriftserver; this probably works most of the time, but it's prone to races.  What happens if the ThriftServer fails to bind to a port or dies during startup?  Another potential cause of flakiness is how we search for the ""Thriftserver started"" message in its log: because we ""tail -f"" the log, it's possible that we might miss the message if a bunch of output is printed after the message we're looking for, causing it to not appear in the initial ""tail"" output.

Maybe we should make several attempts to bind on different ports in case the first attempt fails.  To do this, we probably shouldn't lower the timeout because that might introduce a new source of flakiness if the startup is slow.  Instead, what do you think about using a second future / promise to detect whether the ThriftServer process died then Awaiting on whichever future completes first?  I think we can detect unclean subprocess exit by wrapping a blocking call to Process.exitCode in a future.

I haven't dug in super-deeply, but these were just my initial thoughts after a quick pass through this code today.;;;","19/Feb/15 06:50;lian cheng;Hey [~joshrosen], thanks for the investigation! I came to a similar conclusion about the port contention issue. Also observed port conflicts between core Spark services and HiveThriftServer2. 

The {{tail -f}} part is a great advice, didn't notice that before. I think we can fix this part by using {{tail -n +0 -f}}, which always show all lines of the log file.;;;","22/Feb/15 16:32;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4720;;;","25/Feb/15 00:35;lian cheng;Issue resolved by pull request 4720
[https://github.com/apache/spark/pull/4720];;;","25/Feb/15 04:35;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4758;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix Bash word splitting bugs in compute-classpath.sh,SPARK-5749,12774392,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gvramana,nchammas,nchammas,12/Feb/15 00:00,17/Feb/15 18:23,14/Jul/23 06:27,17/Feb/15 18:08,1.2.1,,,,,,1.2.2,1.3.0,,,,,Build,,,,0,,,,,,"For example [this line|https://github.com/apache/spark/blob/fa6bdc6e819f9338248b952ec578bcd791ddbf6d/bin/compute-classpath.sh#L79] has a word splitting bug.

{code}
for f in ${assembly_folder}/spark-assembly*hadoop*.jar; do
  if [[ ! -e ""$f"" ]]; then
    echo ""Failed to find Spark assembly in $assembly_folder"" 1>&2
    echo ""You need to build Spark before running this program."" 1>&2
    exit 1
  fi
  ASSEMBLY_JAR=""$f""
  num_jars=$((num_jars+1))
done
{code}

If {{assembly_folder}} contains a space, this block of code will fail.",,andrewor14,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5747,,,,SPARK-5765,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 18:23:59 UTC 2015,,,,,,,,,,"0|i25iin:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/15 18:08;nchammas;Fixed by: https://github.com/apache/spark/pull/4561

[~andrewor14] - What are the fix versions for this issue?;;;","17/Feb/15 18:23;andrewor14;I think Sean's assignment is correct.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Check invalid cases for the write path of data source API,SPARK-5746,12774324,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,lian cheng,lian cheng,lian cheng,11/Feb/15 20:08,16/Feb/15 23:52,14/Jul/23 06:27,16/Feb/15 23:52,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Right now, with the newly introduced write support of data source API, {{JSONRelation}} and {{ParquetRelation2}} both delete data first when the save mode is overwrite ([here|https://github.com/apache/spark/blob/1ac099e3e00ddb01af8e6e3a84c70f8363f04b5c/sql/core/src/main/scala/org/apache/spark/sql/json/JSONRelation.scala#L112-L121]) and this behavior introduces issues when the destination table is an input table of the query. For example
{code}
INSERT OVERWRITE t SELECT * FROM t
{code}

We need to add an analysis rule to check cases that are invalid for the write path of data source API.",,apachespark,lian cheng,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5821,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 23:52:19 UTC 2015,,,,,,,,,,"0|i25i3r:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"11/Feb/15 20:08;lian cheng;cc [~yhuai];;;","12/Feb/15 21:32;yhuai;Here are places where we need to take care overwrite, CreateMetastoreDataSourceAsSelect, CreatableRelationProvider.createRelation, and InsertableRelation.insert.;;;","13/Feb/15 18:57;yhuai;For now, we will throw an error when we find this case.;;;","14/Feb/15 18:02;apachespark;User 'yanbohappy' has created a pull request for this issue:
https://github.com/apache/spark/pull/4607;;;","15/Feb/15 05:57;apachespark;User 'yanbohappy' has created a pull request for this issue:
https://github.com/apache/spark/pull/4610;;;","16/Feb/15 00:42;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4617;;;","16/Feb/15 23:52;marmbrus;Issue resolved by pull request 4617
[https://github.com/apache/spark/pull/4617];;;",,,,,,,,,,,,,,,,,,,,,,
RDD.isEmpty / take fails for (empty) RDD of Nothing,SPARK-5744,12774303,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,tbertelsen,tbertelsen,11/Feb/15 18:24,20/Feb/15 10:22,14/Jul/23 06:27,20/Feb/15 10:21,,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"The implementation of {{RDD.isEmpty()}} fails if there is empty partitions. It was introduce by https://github.com/apache/spark/pull/4074

Example:
{code}
sc.parallelize(Seq(), 1).isEmpty()
{code}

The above code throws an exception like this:
{code}
org.apache.spark.SparkDriverExecutionException: Execution error
    at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:977)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1374)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1338)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
Cause: java.lang.ArrayStoreException: [Ljava.lang.Object;
    at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:88)
    at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1466)
    at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1466)
    at org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:973)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1374)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1338)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}",,apachespark,michaelmalak,tbertelsen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,0,,0%,0,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 20 10:22:55 UTC 2015,,,,,,,,,,"0|i25hz3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/15 18:28;apachespark;User 'tbertelsen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4534;;;","13/Feb/15 14:45;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4591;;;","19/Feb/15 21:56;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4698;;;","20/Feb/15 10:21;srowen;Issue resolved by pull request 4698
[https://github.com/apache/spark/pull/4698];;;","20/Feb/15 10:22;srowen;There was ultimately no good fix; partial fixes only raised more issues. The behavior is now documented (and a slightly related minor bug in histogram is resolved);;;",,,,,,,,,,,,,,,,,,,,,,,,
Support the path contains comma in HiveContext,SPARK-5741,12774164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,waterman,waterman,waterman,11/Feb/15 10:47,27/Aug/15 20:22,14/Jul/23 06:27,02/Mar/15 18:13,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"When run ```select * from nzhang_part where hr = 'file,';```, it throws exception ```java.lang.IllegalArgumentException: Can not create a Path from an empty string```. Because the path of hdfs contains comma, and FileInputFormat.setInputPaths will split path by comma.

###############################
SQL
###############################
set hive.merge.mapfiles=true; 
set hive.merge.mapredfiles=true;
set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;
set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;

create table nzhang_part like srcpart;

insert overwrite table nzhang_part partition (ds='2010-08-15', hr) select key, value, hr from srcpart where ds='2008-04-08';

insert overwrite table nzhang_part partition (ds='2010-08-15', hr=11) select key, value from srcpart where ds='2008-04-08';

insert overwrite table nzhang_part partition (ds='2010-08-15', hr)  
select * from (
    select key, value, hr from srcpart where ds='2008-04-08'
    union all
    select '1' as key, '1' as value, 'file,' as hr from src limit 1) s;

select * from nzhang_part where hr = 'file,';

###############################
Error log
###############################
15/02/10 14:33:16 ERROR SparkSQLDriver: Failed in [select * from nzhang_part where hr = 'file,']
java.lang.IllegalArgumentException: Can not create a Path from an empty string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:127)
        at org.apache.hadoop.fs.Path.<init>(Path.java:135)
        at org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:241)
        at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:400)
        at org.apache.spark.sql.hive.HadoopTableReader$.initializeLocalJobConfFunc(TableReader.scala:251)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$11.apply(TableReader.scala:229)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$11.apply(TableReader.scala:229)
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:172)
        at org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$6.apply(HadoopRDD.scala:172)
        at scala.Option.map(Option.scala:145)
        at org.apache.spark.rdd.HadoopRDD.getJobConf(HadoopRDD.scala:172)
        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:196)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)
        at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
        at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)
        at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:223)
        at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:221)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.rdd.RDD.partitions(RDD.scala:221)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1391)
        at org.apache.spark.rdd.RDD.collect(RDD.scala:811)
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:81)
        at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:446)
        at org.apache.spark.sql.hive.thriftserver.AbstractSparkSQLDriver.run(AbstractSparkSQLDriver.scala:58)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:275)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:211)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:403)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:77)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

",,apachespark,glenn.strycker@gmail.com,koert,marmbrus,waterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4967,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 27 20:22:02 UTC 2015,,,,,,,,,,"0|i25h4f:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"11/Feb/15 11:00;apachespark;User 'watermen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4532;;;","02/Mar/15 18:13;marmbrus;Issue resolved by pull request 4532
[https://github.com/apache/spark/pull/4532];;;","27/Aug/15 16:55;koert;i realize i am late to the party but...

by doing this you are losing a very important functionality: passing in multiple input paths comma separated. globs only cover a very limited subset of what you can do with multiple paths. for example selecting partitions (by  day) for the last 30 days cannot be expressed with a glob.

so you are giving up major functionality just to be able to pass in a character that people would generally advice should not be part of a filename anyhow? doesnt sound like a good idea to me.;;;","27/Aug/15 17:51;marmbrus;What format are you trying to read?  There [are still ways to read more than one file|https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrameReader.scala#L258], they just don't rely on brittle string munging anymore.;;;","27/Aug/15 18:27;koert;i am reading avro and csv mostly. but we try to support multiple inputs across a wide range of formats (currently avro, csv, json, and parquet).

i realize parquet supports it, but it does so by explicitly working around the general infrastructure.

i am sympathetic to the idea of no longer doing string munging, but that poses some challenges since the main vehicle to carry this information is a Map[String, String] (DataFrameReader.extraOptions).

if we could come up with a general way to do this that does not involve string munging, i am happy to work on it. the ideal api in my view would be something like:
sqlContext.read.format(...).paths(""a"", ""b"")

alternatively this could be expressed as a union operation of many dataframes, but i do not have the knowledge of the relevant code to understand if that is feasible, scalable and will support predicate pushdown and such. but if that works then i have no need for multiple inputs in DataFrameReader...

from what i know from other projects such as scalding, i think its is a very common request to be able to support multiple paths, and you would exclude a significant userbase by not supporting it. but thats just a guess...;;;","27/Aug/15 18:39;marmbrus;It was originally just parquet that would support more than one file, but now all HadoopFSRelations should. (which covers all but CSV, and we should upgrade that library too)  I would be in favor of generalizing this support for at least these sources given the following constraints:

 - We must keep source/binary compatibility.
 - We should give good errors when the source does not support this feature.
 - For consistency, I'd prefer if we can just add a {{load(path: String*)}} (but I'm not sure if this is possible given the above).
 - {{paths(path: *)}} is okay, but I think I'd prefer if it was not the terminal operator.;;;","27/Aug/15 20:22;koert;given the requirement of source/binary compatibility i do not think it can be done without some kind of string munging.

however the string munging could be restricted to a separate variable, set with paths(path: *) so path does not get polluted. this variable would be exclusively for HadoopFsRelationProvider, and an error thrown in ResolvedDataSource if any other RelationProvider is used with this variable set. also an error would be thrown if path and paths are both set.

does this sound reasonable? if not i will keep looking for other solutions
;;;",,,,,,,,,,,,,,,,,,,,,,
Error Link in Pagination of HistroyPage when showing Incomplete Applications ,SPARK-5733,12774091,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,marsishandsome,marsishandsome,marsishandsome,11/Feb/15 05:14,12/Feb/15 04:31,14/Jul/23 06:27,11/Feb/15 15:56,1.2.1,,,,,,1.3.0,,,,,,Web UI,,,,0,,,,,,"The links in pagination of HistroyPage is wrong when showing Incomplete Applications. 

If ""2"" is click on the following page ""http://history-server:18080/?page=1&showIncomplete=true"", it will go  to  ""http://history-server:18080/?page=2"" instead of ""http://history-server:18080/?page=2&showIncomplete=true"".",,apachespark,marsishandsome,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 11 15:56:28 UTC 2015,,,,,,,,,,"0|i25gon:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/15 05:53;marsishandsome;https://github.com/apache/spark/pull/4523;;;","11/Feb/15 05:54;apachespark;User 'marsishandsome' has created a pull request for this issue:
https://github.com/apache/spark/pull/4523;;;","11/Feb/15 15:56;srowen;Issue resolved by pull request 4523
[https://github.com/apache/spark/pull/4523];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.streaming.kafka.DirectKafkaStreamSuite.basic stream receiving with multiple topics and smallest starting offset,SPARK-5731,12774080,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,pwendell,pwendell,11/Feb/15 03:45,19/Feb/15 02:17,14/Jul/23 06:27,18/Feb/15 06:45,1.3.0,,,,,,1.3.0,,,,,,DStreams,Tests,,,0,flaky-test,,,,,"{code}
sbt.ForkMain$ForkError: The code passed to eventually never returned normally. Attempted 110 times over 20.070287525 seconds. Last failure message: 300 did not equal 48 didn't get all messages.
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:420)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:438)
	at org.apache.spark.streaming.kafka.KafkaStreamSuiteBase.eventually(KafkaStreamSuite.scala:49)
	at org.scalatest.concurrent.Eventually$class.eventually(Eventually.scala:307)
	at org.apache.spark.streaming.kafka.KafkaStreamSuiteBase.eventually(KafkaStreamSuite.scala:49)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$2.apply$mcV$sp(DirectKafkaStreamSuite.scala:110)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$2.apply(DirectKafkaStreamSuite.scala:70)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$2.apply(DirectKafkaStreamSuite.scala:70)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.org$scalatest$BeforeAndAfter$$super$runTest(DirectKafkaStreamSuite.scala:38)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.runTest(DirectKafkaStreamSuite.scala:38)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.org$scalatest$BeforeAndAfter$$super$run(DirectKafkaStreamSuite.scala:38)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.org$scalatest$BeforeAndAfterAll$$super$run(DirectKafkaStreamSuite.scala:38)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite.run(DirectKafkaStreamSuite.scala:38)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: sbt.ForkMain$ForkError: 300 did not equal 48 didn't get all messages
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$2$$anonfun$apply$mcV$sp$1.apply$mcV$sp(DirectKafkaStreamSuite.scala:111)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$2$$anonfun$apply$mcV$sp$1.apply(DirectKafkaStreamSuite.scala:111)
	at org.apache.spark.streaming.kafka.DirectKafkaStreamSuite$$anonfun$2$$anonfun$apply$mcV$sp$1.apply(DirectKafkaStreamSuite.scala:111)
	at org.scalatest.concurrent.Eventually$class.makeAValiantAttempt$1(Eventually.scala:394)
	at org.scalatest.concurrent.Eventually$class.tryTryAgain$1(Eventually.scala:408)
	... 53 more
{code}",,apachespark,pwendell,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 13 21:34:32 UTC 2015,,,,,,,,,,"0|i25gm7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/15 20:49;pwendell;[~cody@koeninger.org] [~tdas] FYI we've disabled this test because it's caused a huge productivity loss to ongoing development with frequent failures. Please try to get this test into good shape ASAP - otherwise this code will be untested.;;;","13/Feb/15 20:54;tdas;Let me take a pass at it.;;;","13/Feb/15 20:58;tdas;This is very weird. the stream is receiving more messages that it is supposed to. Let me try recreating it. ;;;","13/Feb/15 21:34;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/4597;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Potential NPE in StandaloneRestServer if user specifies bad path,SPARK-5729,12774060,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,11/Feb/15 02:10,11/Feb/15 04:19,14/Jul/23 06:27,11/Feb/15 04:19,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"When we delegate something to the default ErrorServlet, the context should be ""/*"", not just ""/"". One line fix.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 11 02:14:00 UTC 2015,,,,,,,,,,"0|i25ghr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"11/Feb/15 02:14;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4518;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
MQTTStreamSuite leaves behind ActiveMQ database files,SPARK-5728,12774056,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,srowen,srowen,srowen,11/Feb/15 01:29,13/Feb/15 10:42,14/Jul/23 06:27,11/Feb/15 08:14,1.2.1,,,,,,1.2.2,1.3.0,,,,,DStreams,Tests,,,0,,,,,,"I've seen this several times and finally wanted to fix it: {{MQTTStreamSuite}} uses a local ActiveMQ broker, that creates a working dir for its database in the {{external/mqtt}} directory called {{activemq}}. This doesn't get cleaned up, at least often it does not for me. It's trivial to set it to use a temp directory which the test harness does clean up.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 11 08:14:20 UTC 2015,,,,,,,,,,"0|i25ggv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/15 01:31;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4517;;;","11/Feb/15 08:14;srowen;Issue resolved by pull request 4517
[https://github.com/apache/spark/pull/4517];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ParquetRelation2.equals throws when compared with non-Parquet relations,SPARK-5725,12774016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,10/Feb/15 23:17,11/Feb/15 01:04,14/Jul/23 06:27,11/Feb/15 01:04,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"it's an apparent mistake, [forgot to return {{false}} in other cases|https://github.com/apache/spark/blob/5820961289eb98e45eb467efa316c7592b8d619c/sql/core/src/main/scala/org/apache/spark/sql/parquet/newParquet.scala#L150-L155].",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 11 01:04:33 UTC 2015,,,,,,,,,,"0|i25g7z:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"10/Feb/15 23:20;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4513;;;","11/Feb/15 01:04;lian cheng;Issue resolved by pull request 4513
[https://github.com/apache/spark/pull/4513];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
misconfiguration in Akka system,SPARK-5724,12774007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,codingcat,codingcat,codingcat,10/Feb/15 22:58,23/Feb/15 11:30,14/Jul/23 06:27,23/Feb/15 11:29,1.1.0,1.2.0,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"In AkkaUtil, we set several failure detector related the parameters as following 

{code:title=AkkaUtil.scala|borderStyle=solid}
al akkaConf = ConfigFactory.parseMap(conf.getAkkaConf.toMap[String, String])
      .withFallback(akkaSslConfig).withFallback(ConfigFactory.parseString(
      s""""""
      |akka.daemonic = on
      |akka.loggers = [""""akka.event.slf4j.Slf4jLogger""""]
      |akka.stdout-loglevel = ""ERROR""
      |akka.jvm-exit-on-fatal-error = off
      |akka.remote.require-cookie = ""$requireCookie""
      |akka.remote.secure-cookie = ""$secureCookie""
      |akka.remote.transport-failure-detector.heartbeat-interval = $akkaHeartBeatInterval s
      |akka.remote.transport-failure-detector.acceptable-heartbeat-pause = $akkaHeartBeatPauses s
      |akka.remote.transport-failure-detector.threshold = $akkaFailureDetector
      |akka.actor.provider = ""akka.remote.RemoteActorRefProvider""
      |akka.remote.netty.tcp.transport-class = ""akka.remote.transport.netty.NettyTransport""
      |akka.remote.netty.tcp.hostname = ""$host""
      |akka.remote.netty.tcp.port = $port
      |akka.remote.netty.tcp.tcp-nodelay = on
      |akka.remote.netty.tcp.connection-timeout = $akkaTimeout s
      |akka.remote.netty.tcp.maximum-frame-size = ${akkaFrameSize}B
      |akka.remote.netty.tcp.execution-pool-size = $akkaThreads
      |akka.actor.default-dispatcher.throughput = $akkaBatchSize
      |akka.log-config-on-start = $logAkkaConfig
      |akka.remote.log-remote-lifecycle-events = $lifecycleEvents
      |akka.log-dead-letters = $lifecycleEvents
      |akka.log-dead-letters-during-shutdown = $lifecycleEvents
      """""".stripMargin))

{code}

Actually, we do not have any parameter naming ""akka.remote.transport-failure-detector.threshold""

see: http://doc.akka.io/docs/akka/2.3.4/general/configuration.html

what we have is ""akka.remote.watch-failure-detector.threshold"" ",,apachespark,codingcat,dougb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 23 11:29:42 UTC 2015,,,,,,,,,,"0|i25g5z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/15 23:01;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/4512;;;","23/Feb/15 11:29;srowen;Issue resolved by pull request 4512
[https://github.com/apache/spark/pull/4512];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Infer_schema_type incorrect for Integers in pyspark,SPARK-5722,12773949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dondrake,dondrake,dondrake,10/Feb/15 20:29,25/Apr/15 21:49,14/Jul/23 06:27,24/Feb/15 01:29,1.2.0,,,,,,1.2.2,,,,,,PySpark,,,,0,,,,,,"The Integers datatype in Python does not match what a Scala/Java integer is defined as.   This causes inference of data types and schemas to fail when data is larger than 2^32 and it is inferred incorrectly as an Integer.

Since the range of valid Python integers is wider than Java Integers, this causes problems when inferring Integer vs. Long datatypes.  This will cause problems when attempting to save SchemaRDD as Parquet or JSON.

Here's an example:
{code}
>>> sqlCtx = SQLContext(sc)
>>> from pyspark.sql import Row
>>> rdd = sc.parallelize([Row(f1='a', f2=100000000000000)])
>>> srdd = sqlCtx.inferSchema(rdd)
>>> srdd.schema()
StructType(List(StructField(f1,StringType,true),StructField(f2,IntegerType,true)))
{code}
That number is a LongType in Java, but an Integer in python.  We need to check the value to see if it should really by a LongType when a IntegerType is initially inferred.

More tests:
{code}
>>> from pyspark.sql import _infer_type
# OK
>>> print _infer_type(1)
IntegerType
# OK
>>> print _infer_type(2**31-1)
IntegerType
#WRONG
>>> print _infer_type(2**31)
#WRONG
IntegerType
>>> print _infer_type(2**61 )
#OK
IntegerType
>>> print _infer_type(2**71 )
LongType
{code}

Java Primitive Types defined:
http://docs.oracle.com/javase/tutorial/java/nutsandbolts/datatypes.html

Python Built-in Types:
https://docs.python.org/2/library/stdtypes.html#typesnumeric
",,apachespark,dondrake,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 24 01:29:47 UTC 2015,,,,,,,,,,"0|i25ftj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/15 04:57;apachespark;User 'dondrake' has created a pull request for this issue:
https://github.com/apache/spark/pull/4521;;;","11/Feb/15 20:38;apachespark;User 'dondrake' has created a pull request for this issue:
https://github.com/apache/spark/pull/4538;;;","11/Feb/15 20:40;dondrake;Hi, I've submitted 2 pull requests for branch-1.2 and branch-1.3.

Please approve.;;;","17/Feb/15 04:49;apachespark;User 'dondrake' has created a pull request for this issue:
https://github.com/apache/spark/pull/4641;;;","18/Feb/15 00:42;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4666;;;","18/Feb/15 23:10;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4681;;;","24/Feb/15 01:29;marmbrus;Issue resolved by pull request 4681
[https://github.com/apache/spark/pull/4681];;;",,,,,,,,,,,,,,,,,,,,,,
Semicolon at end of a comment line,SPARK-5712,12773732,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,adrian-wang,adrian-wang,adrian-wang,10/Feb/15 08:26,25/Apr/15 21:49,14/Jul/23 06:27,17/Mar/15 04:30,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,HIVE-3348,,adrian-wang,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 04:30:06 UTC 2015,,,,,,,,,,"0|i25etj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/15 08:28;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4500;;;","17/Mar/15 04:30;lian cheng;Issue resolved by pull request 4500
[https://github.com/apache/spark/pull/4500];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Enabling spark.sql.codegen throws ClassNotFound exception,SPARK-5707,12773661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,rams,yiyao,yiyao,10/Feb/15 02:21,08/Jul/15 17:43,14/Jul/23 06:27,08/Jul/15 17:43,1.2.0,1.3.1,,,,,1.5.0,,,,,,SQL,,,,2,,,,,,"Exception thrown:
{noformat}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 133.0 failed 4 times, most recent failure: Lost task 13.3 in stage 133.0 (TID 3066, cdh52-node2): java.io.IOException: com.esotericsoftware.kryo.KryoException: Unable to find class: __wrapper$1$81257352e1c844aebf09cb84fe9e7459.__wrapper$1$81257352e1c844aebf09cb84fe9e7459$SpecificRow$1
Serialization trace:
hashTable (org.apache.spark.sql.execution.joins.UniqueKeyHashedRelation)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1011)
        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:164)
        at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
        at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
        at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:87)
        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
        at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$3.apply(BroadcastHashJoin.scala:62)
        at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$3.apply(BroadcastHashJoin.scala:61)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.CartesianRDD.compute(CartesianRDD.scala:75)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.CartesianRDD.compute(CartesianRDD.scala:75)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
SQL:
{code:sql}
INSERT INTO TABLE ${hiveconf:TEMP_TABLE}
SELECT
  s_store_name,
  pr_review_date,
  pr_review_content
FROM (
  --select store_name for stores with flat or declining sales in 3 consecutive months.
  SELECT s_store_name
  FROM store s
  JOIN (
    -- linear regression part
    SELECT
      temp.cat AS cat,
      --SUM(temp.x)as sumX,
      --SUM(temp.y)as sumY,
      --SUM(temp.xy)as sumXY,
      --SUM(temp.xx)as sumXSquared,
      --count(temp.x) as N,
      --N * sumXY - sumX * sumY AS numerator,
      --N * sumXSquared - sumX*sumX AS denom
      --numerator / denom as slope,
      --(sumY - slope * sumX) / N as intercept
      --(count(temp.x) * SUM(temp.xy) - SUM(temp.x) * SUM(temp.y)) AS numerator,
      --(count(temp.x) * SUM(temp.xx) - SUM(temp.x) * SUM(temp.x)) AS denom
      --numerator / denom as slope,
      --(sumY - slope * sumX) / N as intercept
      ((count(temp.x) * SUM(temp.xy) - SUM(temp.x) * SUM(temp.y)) / (count(temp.x) * SUM(temp.xx) - SUM(temp.x) * SUM(temp.x)) ) as slope,
      (SUM(temp.y) - ((count(temp.x) * SUM(temp.xy) - SUM(temp.x) * SUM(temp.y)) / (count(temp.x) * SUM(temp.xx) - SUM(temp.x) * SUM(temp.x)) ) * SUM(temp.x)) / count(temp.x) as intercept
    FROM (
SELECT
        s.ss_store_sk AS cat,
        s.ss_sold_date_sk  AS x,
        SUM(s.ss_net_paid) AS y,
        s.ss_sold_date_sk * SUM(s.ss_net_paid) AS xy,
        s.ss_sold_date_sk*s.ss_sold_date_sk AS xx
      FROM store_sales s
      --select date range
      LEFT SEMI JOIN (
        SELECT d_date_sk
        FROM date_dim d
        WHERE d.d_date >= '${hiveconf:q18_startDate}'
        AND   d.d_date <= '${hiveconf:q18_endDate}'
      ) dd ON ( s.ss_sold_date_sk=dd.d_date_sk )
      WHERE s.ss_store_sk <= 18
      GROUP BY s.ss_store_sk, s.ss_sold_date_sk
    ) temp
    GROUP BY temp.cat
  ) c on s.s_store_sk = c.cat
  WHERE c.slope < 0
) tmp
JOIN  product_reviews pr on (true)
WHERE instr(pr.pr_review_content, tmp.s_store_name) > 0
{code}","yarn-client mode, spark.sql.codegen=true",apachespark,k.shaposhnikov@gmail.com,marmbrus,nadenf,nemccarthy,rxin,smolav,wulei.bj.cn,yhuai,yiyao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3846,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 08 17:43:20 UTC 2015,,,,,,,,,,"0|i25ee7:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"11/May/15 08:09;nemccarthy;+1 seeing this as well. Is there a workaround? ;;;","12/May/15 00:07;nemccarthy;Also throws and error when switching to Java Serialisation 


Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 4 times, most recent failure: Lost task 0.3 in stage 9.0 (TID 515, qtausc-pphd0152.quantium.com.au.local): java.io.IOException: java.lang.ClassNotFoundException: __wrapper$3$5dd511946bf343e58042f4a7fe748b41.__wrapper$3$5dd511946bf343e58042f4a7fe748b41$SpecificRow$1
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1156)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:164)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:87)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$3.apply(BroadcastHashJoin.scala:73)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$3.apply(BroadcastHashJoin.scala:72)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
	at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:634)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: __wrapper$3$5dd511946bf343e58042f4a7fe748b41.__wrapper$3$5dd511946bf343e58042f4a7fe748b41$SpecificRow$1
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:274)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:65)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.HashMap.readObject(HashMap.java:1179)
	at sun.reflect.GeneratedMethodAccessor26.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:68)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:216)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:177)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1153)
	... 24 more

Driver stacktrace:
SQLState:  null
ErrorCode: 0
;;;","26/May/15 07:56;smolav;This is probably a duplicate of SPARK-3846.;;;","03/Jul/15 18:37;marmbrus;This is still broken even with the recent changes to codegen. /cc [~davies]

{code}
sqlContext.range(1000000).repartition(1).write.mode(""ignore"").saveAsTable(""1milints"")
val query = sql(""SELECT * FROM 1milints a JOIN 1milints b ON a.id = b.id"")
query.explain()

== Physical Plan ==
Project [id#3L,id#12L]
 BroadcastHashJoin [id#3L], [id#12L], BuildRight
  PhysicalRDD [id#3L], MapPartitionsRDD[17] at explain at <console>:28
  PhysicalRDD [id#12L], MapPartitionsRDD[21] at explain at <console>:28
query: org.apache.spark.sql.DataFrame = [id: bigint, id: bigint]
{code}

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 10, 10.0.146.98): java.io.IOException: com.esotericsoftware.kryo.KryoException: Unable to find class: SC$SpecificRow
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1264)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$3.apply(BroadcastHashJoin.scala:72)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$3.apply(BroadcastHashJoin.scala:71)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
{code};;;","03/Jul/15 19:06;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/7213;;;","07/Jul/15 20:37;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/7264;;;","07/Jul/15 23:40;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/7272;;;","08/Jul/15 17:43;marmbrus;Issue resolved by pull request 7272
[https://github.com/apache/spark/pull/7272];;;",,,,,,,,,,,,,,,,,,,,,
AllJobsPage throws empty.max error,SPARK-5703,12773649,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,10/Feb/15 01:23,10/Feb/15 05:19,14/Jul/23 06:27,10/Feb/15 05:19,1.2.0,,,,,,1.2.2,1.3.0,,,,,Spark Core,,,,0,,,,,,"In JobProgressListener, if you have a JobEnd that does not have a corresponding JobStart, AND you render the AllJobsPage, then you'll run into the empty.max exception.

I ran into this when trying to replay parts of an event log after trimming a few events. Not a common use case I agree, but I'd argue that it should never fail on empty.max.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 01:31:55 UTC 2015,,,,,,,,,,"0|i25ebr:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"10/Feb/15 01:31;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4490;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Only set ShuffleReadMetrics when task does a shuffle,SPARK-5701,12773642,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,kayousterhout,kayousterhout,10/Feb/15 00:52,10/Feb/15 05:23,14/Jul/23 06:27,10/Feb/15 05:23,1.2.1,1.3.0,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"The updateShuffleReadMetrics method in TaskMetrics (called by the executor heartbeater) will currently always add a ShuffleReadMetrics to TaskMetrics (with values set to 0), even when the task didn't read any shuffle data.  ShuffleReadMetrics should only be added if the task reads shuffle data.",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 01:09:45 UTC 2015,,,,,,,,,,"0|i25eaf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"10/Feb/15 01:09;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/4488;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bump jets3t version from 0.9.2 to 0.9.3 in hadoop-2.3 and hadoop-2.4 profiles,SPARK-5700,12773622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,09/Feb/15 23:52,19/Feb/15 02:16,14/Jul/23 06:27,10/Feb/15 10:29,1.3.0,,,,,,1.3.0,,,,,,SQL,Tests,,,0,,,,,,"This is a follow-up ticket for SPARK-5671 and SPARK-5696.

JetS3t 0.9.2 contains a log4j.properties file inside the artifact and breaks our tests (see SPARK-5696). This is fixed in 0.9.3.",,apachespark,joshrosen,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5696,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 10:29:58 UTC 2015,,,,,,,,,,"0|i25e5z:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"10/Feb/15 00:55;lian cheng;Also, after upgrading JetS3t to 0.9.3, we can revert SPARK-5696 so that users don't need to configure {{hive-log4j.properties}}.;;;","10/Feb/15 07:30;joshrosen;Looks like 0.9.3 is now on Maven Central: http://search.maven.org/#artifactdetails%7Cnet.java.dev.jets3t%7Cjets3t%7C0.9.3%7Cjar;;;","10/Feb/15 08:15;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4499;;;","10/Feb/15 08:16;lian cheng;Thanks, Josh. PR submitted.;;;","10/Feb/15 10:29;lian cheng;Issue resolved by pull request 4499
[https://github.com/apache/spark/pull/4499];;;",,,,,,,,,,,,,,,,,,,,,,,,
Thrift server test suites are not executed even if SQL code is touched,SPARK-5699,12773610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,09/Feb/15 23:15,10/Feb/15 00:52,14/Jul/23 06:27,10/Feb/15 00:52,1.2.0,1.2.1,1.3.0,,,,1.3.0,,,,,,Tests,,,,0,,,,,,"{{hive-thriftserver/test}} is missing [in this line|https://github.com/apache/spark/blob/f48199eb354d6ec8675c2c1f96c3005064058d66/dev/run-tests#L186].",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 00:52:33 UTC 2015,,,,,,,,,,"0|i25e3j:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"09/Feb/15 23:21;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4486;;;","10/Feb/15 00:52;lian cheng;Issue resolved by pull request 4486
[https://github.com/apache/spark/pull/4486];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic allocation: do not allow user to request a negative delta,SPARK-5698,12773593,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,andrewor14,andrewor14,09/Feb/15 22:40,10/Feb/15 01:34,14/Jul/23 06:27,10/Feb/15 01:34,1.2.0,,,,,,1.2.2,1.3.0,,,,,Spark Core,YARN,,,0,,,,,,"If the user calls the following in series:

sc.requestExecutors(-5)
sc.killExecutor(...)

Then we might crash the ApplicationMaster. Why?

Well, if we request a negative number of additional executors, then the YarnAllocator will lower the target number of executors it is trying to achieve by 5. This might shoot the target number past 0. Then, if so, when we try to kill an executor it will fail the assertion that the target number must be >=0.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 09 22:46:21 UTC 2015,,,,,,,,,,"0|i25dzr:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"09/Feb/15 22:46;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4483;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveThriftServer2Suite fails because of extra log4j.properties in the driver classpath,SPARK-5696,12773569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,lian cheng,lian cheng,lian cheng,09/Feb/15 20:49,19/Feb/15 02:18,14/Jul/23 06:27,10/Feb/15 00:51,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,flaky-test,,,,,"PR #2982 added the {{--driver-class-path}} flag to {{HiveThriftServer2Suite}} so that it passes when the {{hadoop-provided}} profile is used. However,  {{lib_managed/jars/jets3s-0.9.2.jar}} in the classpath has a log4j.properties in it, which sets root logger level to ERROR. This makes {{HiveThriftServer2Suite}} fail because it starts new processes and checks for log output.",,apachespark,joshrosen,lian cheng,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5700,,,,,SPARK-5671,SPARK-4048,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 00:51:45 UTC 2015,,,,,,,,,,"0|i25dun:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/15 22:05;lian cheng;Jets3t 0.9.2 breaks {{HiveThriftServer2Suite}} as described in the ticket description.;;;","09/Feb/15 22:21;pwendell;Wow - this must have been a substantial effort to figure out what caused this. Sorry I didn't anticipate this when signing off on that patch. ;;;","09/Feb/15 22:21;joshrosen;It looks like this might be fixed in jets3t 0.0.3:

http://www.jets3t.org/RELEASE_NOTES.html

{quote}
Remove problematic config files that were being included in Maven distribution, especially a default logging config, that could break things for some users (#199)
{quote}

I don't think this has been published to Maven central yet, though, even though it's been released (AFAIK).  Should we contact the jets3t author to see if we can get them to publish 0.9.3?;;;","09/Feb/15 22:30;lian cheng;Yeah, it would be good to upgrade jets3t to 0.9.3. In the meanwhile, I'll prepend {{$SPARK_HOME/conf}} to the driver classpath used in {{HiveThriftServer2Suite}} in order to prevent similar issues in the future.;;;","09/Feb/15 22:31;lian cheng;Never mind, this issue is kinda like a butterfly effect :);;;","09/Feb/15 22:59;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4484;;;","09/Feb/15 23:04;joshrosen;I contacted the JetS3t author and he's publishing 0.9.3 to Maven Central right now, so it should be available in a couple of hours.;;;","09/Feb/15 23:07;lian cheng;Thanks, Josh! Also delivered a fix to comfort Jenkins and prevent similar issues in the future.;;;","10/Feb/15 00:51;lian cheng;Issue resolved by pull request 4484
[https://github.com/apache/spark/pull/4484];;;",,,,,,,,,,,,,,,,,,,,
Preventing duplicate registering of an application has incorrect logic,SPARK-5691,12773536,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mccheah,mcheah,mcheah,09/Feb/15 18:55,09/Feb/15 21:22,14/Jul/23 06:27,09/Feb/15 21:22,1.1.1,1.2.0,,,,,1.0.3,1.1.2,1.2.2,1.3.0,,,Deploy,,,,0,,,,,,"If an application registers twice with the Master, the Master accepts both copies and they both show up in the UI and consume resources. This is incorrect behavior.

This happens inadvertently in regular usage when the Master is under high load, but it boils down to: when an application times out registering with the master and sends a second registration message, but the Master is still alive, it processes the first registration message for the application but also erroneously processes the second registration message instead of discarding it.",,apachespark,mcheah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 09 19:01:12 UTC 2015,,,,,,,,,,"0|i25dnb:",9223372036854775807,,,,,,,,,,,,,,1.0.3,1.1.2,1.2.2,1.3.0,,,,,,,,,,"09/Feb/15 18:57;mcheah;I've determined that this is a pretty simple bug in the Master code.

I'm on commit hash 0793ee1b4dea1f4b0df749e8ad7c1ab70b512faf. In Master.scala, in the registerApplication method, it checks if the application is already registered by checking the addressToWorker data structure. In reality, this is wrong - it should examine the addressToApp data structure.

I'll submit a pull request shortly.;;;","09/Feb/15 19:01;apachespark;User 'mccheah' has created a pull request for this issue:
https://github.com/apache/spark/pull/4477;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.deploy.rest.StandaloneRestSubmitSuite.simple submit until completion,SPARK-5690,12773526,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrewor14,pwendell,pwendell,09/Feb/15 18:27,19/Feb/15 02:17,14/Jul/23 06:27,12/Feb/15 22:48,1.3.0,,,,,,1.3.0,,,,,,Tests,,,,0,flaky-test,,,,,"https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/1647/testReport/junit/org.apache.spark.deploy.rest/StandaloneRestSubmitSuite/simple_submit_until_completion/

{code}
org.apache.spark.deploy.rest.StandaloneRestSubmitSuite.simple submit until completion

Failing for the past 1 build (Since Failed#1647 )
Took 30 sec.
Error Message

Driver driver-20150209035158-0000 did not finish within 30 seconds.
Stacktrace

sbt.ForkMain$ForkError: Driver driver-20150209035158-0000 did not finish within 30 seconds.
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:495)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$class.fail(Assertions.scala:1328)
	at org.scalatest.FunSuite.fail(FunSuite.scala:1555)
	at org.apache.spark.deploy.rest.StandaloneRestSubmitSuite.org$apache$spark$deploy$rest$StandaloneRestSubmitSuite$$waitUntilFinished(StandaloneRestSubmitSuite.scala:152)
	at org.apache.spark.deploy.rest.StandaloneRestSubmitSuite$$anonfun$1.apply$mcV$sp(StandaloneRestSubmitSuite.scala:57)
	at org.apache.spark.deploy.rest.StandaloneRestSubmitSuite$$anonfun$1.apply(StandaloneRestSubmitSuite.scala:52)
	at org.apache.spark.deploy.rest.StandaloneRestSubmitSuite$$anonfun$1.apply(StandaloneRestSubmitSuite.scala:52)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.deploy.rest.StandaloneRestSubmitSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(StandaloneRestSubmitSuite.scala:41)
	at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
	at org.apache.spark.deploy.rest.StandaloneRestSubmitSuite.runTest(StandaloneRestSubmitSuite.scala:41)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.deploy.rest.StandaloneRestSubmitSuite.org$scalatest$BeforeAndAfterAll$$super$run(StandaloneRestSubmitSuite.scala:41)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.deploy.rest.StandaloneRestSubmitSuite.run(StandaloneRestSubmitSuite.scala:41)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,andrewor14,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5761,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 19 02:17:06 UTC 2015,,,,,,,,,,"0|i25dl3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"19/Feb/15 02:17;andrewor14;Fixed in https://github.com/apache/spark/pull/4637;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Key not found exception is thrown in case location of added partition to a parquet table is different than a path containing the partition values,SPARK-5684,12773396,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,saucam,saucam,09/Feb/15 08:36,07/Oct/16 07:38,14/Jul/23 06:27,07/Oct/16 07:38,1.1.0,1.1.1,1.2.0,,,,,,,,,,SQL,,,,0,,,,,,"Create a partitioned parquet table : 

create table test_table (dummy string) partitioned by (timestamp bigint) stored as parquet;

Add a partition to the table and specify a different location:

alter table test_table add partition (timestamp=9) location '/data/pth/different'

Run a simple select  * query 

we get an exception :

15/02/09 08:27:25 ERROR thriftserver.SparkSQLDriver: Failed in [select * from db4_mi2mi_binsrc1_default limit 5]
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21, localhost): java
.util.NoSuchElementException: key not found: timestamp
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at scala.collection.AbstractMap.default(Map.scala:58)
        at scala.collection.MapLike$class.apply(MapLike.scala:141)
        at scala.collection.AbstractMap.apply(Map.scala:58)
        at org.apache.spark.sql.parquet.ParquetTableScan$$anonfun$execute$4$$anonfun$6.apply(ParquetTableOperations.scala:141)
        at org.apache.spark.sql.parquet.ParquetTableScan$$anonfun$execute$4$$anonfun$6.apply(ParquetTableOperations.scala:141)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at org.apache.spark.sql.parquet.ParquetTableScan$$anonfun$execute$4.apply(ParquetTableOperations.scala:141)
        at org.apache.spark.sql.parquet.ParquetTableScan$$anonfun$execute$4.apply(ParquetTableOperations.scala:128)
        at org.apache.spark.rdd.NewHadoopRDD$NewHadoopMapPartitionsWithSplitRDD.compute(NewHadoopRDD.scala:247)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)



This happens because in parquet path it is assumed that (key=value) patterns are present in the partition location, which is not always the case!",,apachespark,saucam,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Oct 07 07:38:46 UTC 2016,,,,,,,,,,"0|i25cs7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/15 09:09;apachespark;User 'saucam' has created a pull request for this issue:
https://github.com/apache/spark/pull/4469;;;","07/Oct/16 07:38;smilegator;This should be fixed after native DDL support. Please try it. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Calling graceful stop() immediately after start() on StreamingContext should not get stuck indefinitely,SPARK-5681,12773386,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,viirya,viirya,09/Feb/15 07:44,17/Jul/15 21:01,14/Jul/23 06:27,17/Jul/15 21:01,,,,,,,1.5.0,,,,,,DStreams,,,,0,,,,,,"Sometimes the receiver will be registered into tracker after ssc.stop is called. Especially when stop() is called immediately after start(). So the receiver doesn't get the StopReceiver message from the tracker. In this case, when you call stop() in graceful mode, stop() would get stuck indefinitely.",,apachespark,juanrh,michaelmalak,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5615,SPARK-7346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jul 15 01:27:05 UTC 2015,,,,,,,,,,"0|i25cpz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/15 07:49;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/4467;;;","20/May/15 20:49;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/6294;;;","15/Jul/15 01:27;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/7276;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"Sum function on all null values, should return zero",SPARK-5680,12773384,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gvramana,gvramana,gvramana,09/Feb/15 07:27,26/Jul/15 04:19,14/Jul/23 06:27,21/Mar/15 20:25,,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"SELECT  sum('a'),  avg('a'),  variance('a'),  std('a') FROM src;
Current output:
NULL	NULL	NULL	NULL
Expected output:
0.0	NULL	NULL	NULL

This fixes hive udaf_number_format.q ",,apachespark,gvramana,holmanl,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8828,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jul 26 04:19:08 UTC 2015,,,,,,,,,,"0|i25cpj:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"09/Feb/15 07:32;apachespark;User 'gvramana' has created a pull request for this issue:
https://github.com/apache/spark/pull/4466;;;","21/Mar/15 20:25;marmbrus;Issue resolved by pull request 4466
[https://github.com/apache/spark/pull/4466];;;","13/Jun/15 00:25;holmanl;Hi Guys,

Just curious about this change to return 0 for the SUM function when all values are NULL as many other data sources (Hive, Impala, SQL Server ...) return NULL in this case. Could you kindly share the motivation behind the change?

Many thanks,
Holman;;;","13/Jun/15 00:48;marmbrus;As stated in the comment, the motivation was to match the result that hive is returning in one of their query tests.  Is this not the case anymore? [~gvramana] did you have some other motivation?;;;","13/Jun/15 05:11;holmanl;Thanks. A closer look at the statement in the description I realized that it's different from our test cases.

Our test cases are:
    select sum(c2) from sum_test
    select c1, sum(c2) from sum_test group by c1

Where c1 is an int column with non-NULL values and c2 is an int column with all NULL values.

Spark 1.3.0 and 1.3.1 return NULL for sum(c2) whereas Spark 1.4.0 returns 0. Hive, Impala and SQL Server returns NULL for the both cases.

For the statement ""select sum('a') from src"" Hive indeed returns 0, my bad. The title of this JIRA caught my attention. Could the change in behavior on sum of all NULL values be related to the changes made for this JIRA?;;;","16/Jun/15 04:52;gvramana;Holman, You are right that column with all NULL values should return NULL.
As my motivation was to fix udaf_number_format.q, ""select sum('a') from src"" returns 0 in hive, mysql.
 and ""select cast('a' as double) from src"" returned NULL in hive.
I assumed or rather wrongly analysed it as ""Sum of ALL NULLs return 0"" and this has introduced the problem.
I apologize for this and will submit the patch to revert that fix. 

""select sum('a') from src"" returning 0 in hive and mysql created this confusion, is still not clear.
;;;","16/Jun/15 15:05;holmanl;Hello Venkata. Thanks very much for looking into this. Could you kindly let us know the JIRA for the patch when you have one created? Thanks.;;;","06/Jul/15 03:39;yhuai;https://issues.apache.org/jira/browse/HIVE-861 is the issue added the test of ""udaf_number_format.q"". I believe that returning ""0.0"" for sum('a') is a bug (the description of the issue indeed mentioned that sum('a') should return null). ;;;","26/Jul/15 04:19;apachespark;User 'yjshen' has created a pull request for this issue:
https://github.com/apache/spark/pull/7667;;;",,,,,,,,,,,,,,,,,,,,
Flaky tests in o.a.s.metrics.InputOutputMetricsSuite: input metrics with interleaved reads and input metrics with mixed read method ,SPARK-5679,12773383,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,pwendell,pwendell,09/Feb/15 07:25,19/Feb/15 02:18,14/Jul/23 06:27,14/Feb/15 01:46,1.3.0,,,,,,1.2.2,1.3.0,,,,,Spark Core,Tests,,,0,flaky-test,,,,,"Please audit these and see if there are any assumptions with respect to File IO that might not hold in all cases. I'm happy to help if you can't find anything.

These both failed in the same run:
https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.3-SBT/38/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.2,label=centos/#showFailuresLink

{code}
org.apache.spark.metrics.InputOutputMetricsSuite.input metrics with mixed read method

Failing for the past 13 builds (Since Failed#26 )
Took 48 sec.
Error Message

20300000 did not equal 6496
Stacktrace

sbt.ForkMain$ForkError: 20300000 did not equal 6496
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.metrics.InputOutputMetricsSuite$$anonfun$9.apply$mcV$sp(InputOutputMetricsSuite.scala:135)
	at org.apache.spark.metrics.InputOutputMetricsSuite$$anonfun$9.apply(InputOutputMetricsSuite.scala:113)
	at org.apache.spark.metrics.InputOutputMetricsSuite$$anonfun$9.apply(InputOutputMetricsSuite.scala:113)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.metrics.InputOutputMetricsSuite.org$scalatest$BeforeAndAfter$$super$runTest(InputOutputMetricsSuite.scala:46)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.metrics.InputOutputMetricsSuite.runTest(InputOutputMetricsSuite.scala:46)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.metrics.InputOutputMetricsSuite.org$scalatest$BeforeAndAfterAll$$super$run(InputOutputMetricsSuite.scala:46)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.metrics.InputOutputMetricsSuite.org$scalatest$BeforeAndAfter$$super$run(InputOutputMetricsSuite.scala:46)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.metrics.InputOutputMetricsSuite.run(InputOutputMetricsSuite.scala:46)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

Second one:

{code}
org.apache.spark.metrics.InputOutputMetricsSuite.input metrics with interleaved reads

Failing for the past 13 builds (Since Failed#26 )
Took 19 sec.
Error Message

2125000 did not equal 2000040
Stacktrace

sbt.ForkMain$ForkError: 2125000 did not equal 2000040
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.metrics.InputOutputMetricsSuite$$anonfun$31.apply$mcV$sp(InputOutputMetricsSuite.scala:289)
	at org.apache.spark.metrics.InputOutputMetricsSuite$$anonfun$31.apply(InputOutputMetricsSuite.scala:254)
	at org.apache.spark.metrics.InputOutputMetricsSuite$$anonfun$31.apply(InputOutputMetricsSuite.scala:254)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.metrics.InputOutputMetricsSuite.org$scalatest$BeforeAndAfter$$super$runTest(InputOutputMetricsSuite.scala:46)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.metrics.InputOutputMetricsSuite.runTest(InputOutputMetricsSuite.scala:46)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.metrics.InputOutputMetricsSuite.org$scalatest$BeforeAndAfterAll$$super$run(InputOutputMetricsSuite.scala:46)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.metrics.InputOutputMetricsSuite.org$scalatest$BeforeAndAfter$$super$run(InputOutputMetricsSuite.scala:46)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.metrics.InputOutputMetricsSuite.run(InputOutputMetricsSuite.scala:46)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,kostas,nchammas,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5227,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 13 22:48:13 UTC 2015,,,,,,,,,,"0|i25cpb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"09/Feb/15 09:39;srowen;Same as SPARK-5227?;;;","09/Feb/15 18:42;kostas;I have tried to repo this in a number of different ways and failed:

1. On mac with sbt and without sbt.
2. On centos 6x using sbt and without sbt
3. On ubuntu using sbt and without sbt

Yet it is reproducible on the build machines but only for hadoop 2.2. As [~joshrosen] pointed out, it might be some shared state that is specific to older versions of hadoop. How do we feel about changing this test suite so that it doesn't use a shared spark context. I know it will slow down the test a bit but might be the easiest way. ;;;","13/Feb/15 22:48;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4599;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Don't return `ERROR 500` when have missing args,SPARK-5672,12773265,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,catap,catap,catap,08/Feb/15 01:52,08/Feb/15 10:32,14/Jul/23 06:27,08/Feb/15 10:32,,,,,,,1.3.0,,,,,,Web UI,,,,0,,,,,,Spark web UI return HTTP ERROR 500 when GET arguments is missing.,,apachespark,catap,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 08 10:32:15 UTC 2015,,,,,,,,,,"0|i25bzb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/15 01:58;apachespark;User 'catap' has created a pull request for this issue:
https://github.com/apache/spark/pull/4239;;;","08/Feb/15 10:32;srowen;Issue resolved by pull request 4239
[https://github.com/apache/spark/pull/4239];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove version from spark-ec2 example.,SPARK-5667,12773239,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,MiguelPeralvo,MiguelPeralvo,MiguelPeralvo,07/Feb/15 18:09,12/Feb/15 04:41,14/Jul/23 06:27,07/Feb/15 18:14,1.2.0,1.2.1,1.2.2,1.3.0,,,1.3.0,,,,,,Documentation,,,,0,documentation,,,,,Remove version from spark-ec2 example for spark-ec2/Launch Cluster.,,apachespark,MiguelPeralvo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 07 18:14:51 UTC 2015,,,,,,,,,,"0|i25btr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"07/Feb/15 18:11;apachespark;User 'MiguelPeralvo' has created a pull request for this issue:
https://github.com/apache/spark/pull/4300;;;","07/Feb/15 18:14;MiguelPeralvo;Fixed in [pull request #4300 - Remove version from spark-ec2 example.] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Restore stty settings when exiting for launching spark-shell from SBT,SPARK-5664,12773216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,07/Feb/15 11:06,12/Feb/15 04:38,14/Jul/23 06:27,09/Feb/15 19:45,,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,,,apachespark,marmbrus,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 09 19:45:29 UTC 2015,,,,,,,,,,"0|i25bon:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/15 11:07;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/4451;;;","09/Feb/15 19:45;marmbrus;Issue resolved by pull request 4451
[https://github.com/apache/spark/pull/4451];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: org.apache.spark.streaming.kafka.KafkaDirectStreamSuite.multi topic stream,SPARK-5662,12773188,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,pwendell,pwendell,07/Feb/15 02:24,25/Feb/15 23:27,14/Jul/23 06:27,25/Feb/15 23:27,1.3.0,,,,,,1.3.0,,,,,,DStreams,,,,0,,,,,,"{code}
sbt.ForkMain$ForkError: java.net.ConnectException: Connection refused
	at org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:319)
	at org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:319)
	at scala.util.Either.fold(Either.scala:97)
	at org.apache.spark.streaming.kafka.KafkaUtils$.createDirectStream(KafkaUtils.scala:318)
	at org.apache.spark.streaming.kafka.KafkaDirectStreamSuite$$anonfun$3.apply$mcV$sp(KafkaDirectStreamSuite.scala:66)
	at org.apache.spark.streaming.kafka.KafkaDirectStreamSuite$$anonfun$3.apply(KafkaDirectStreamSuite.scala:59)
	at org.apache.spark.streaming.kafka.KafkaDirectStreamSuite$$anonfun$3.apply(KafkaDirectStreamSuite.scala:59)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.streaming.kafka.KafkaDirectStreamSuite.org$scalatest$BeforeAndAfter$$super$runTest(KafkaDirectStreamSuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.streaming.kafka.KafkaDirectStreamSuite.runTest(KafkaDirectStreamSuite.scala:32)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.streaming.kafka.KafkaDirectStreamSuite.org$scalatest$BeforeAndAfter$$super$run(KafkaDirectStreamSuite.scala:32)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.streaming.kafka.KafkaDirectStreamSuite.run(KafkaDirectStreamSuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-SBT/1628/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.0,label=centos/testReport/junit/org.apache.spark.streaming.kafka/KafkaDirectStreamSuite/multi_topic_stream/",,andrewor14,boyork,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 23:27:09 UTC 2015,,,,,,,,,,"0|i25bif:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"25/Feb/15 23:27;andrewor14;Resolved in https://github.com/apache/spark/pull/4597;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
function hasShutdownDeleteTachyonDir should use shutdownDeleteTachyonPaths to determine whether contains file,SPARK-5661,12773181,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,xukun,xukun,xukun,07/Feb/15 01:12,17/Feb/15 19:01,14/Jul/23 06:27,17/Feb/15 19:01,1.2.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,function  hasShutdownDeleteTachyonDir should use shutdownDeleteTachyonPaths to determine whether contains file.,,apachespark,xukun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 07 01:14:07 UTC 2015,,,,,,,,,,"0|i25bgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/15 01:14;apachespark;User 'viper-kun' has created a pull request for this issue:
https://github.com/apache/spark/pull/4418;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.streaming.ReceiverSuite.block,SPARK-5659,12773164,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,tdas,pwendell,pwendell,06/Feb/15 23:46,30/Dec/15 21:59,14/Jul/23 06:27,30/Dec/15 21:59,1.3.0,,,,,,,,,,,,DStreams,Tests,,,1,flaky-test,,,,,"{code}
Error Message

recordedBlocks.drop(1).dropRight(1).forall(((block: scala.collection.mutable.ArrayBuffer[Int]) => block.size.>=(minExpectedMessagesPerBlock).&&(block.size.<=(maxExpectedMessagesPerBlock)))) was false # records in received blocks = [11,10,10,10,10,10,10,10,10,10,10,4,16,10,10,10,10,10,10,10], not between 7 and 11
Stacktrace

sbt.ForkMain$ForkError: recordedBlocks.drop(1).dropRight(1).forall(((block: scala.collection.mutable.ArrayBuffer[Int]) => block.size.>=(minExpectedMessagesPerBlock).&&(block.size.<=(maxExpectedMessagesPerBlock)))) was false # records in received blocks = [11,10,10,10,10,10,10,10,10,10,10,4,16,10,10,10,10,10,10,10], not between 7 and 11
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.streaming.ReceiverSuite$$anonfun$3.apply$mcV$sp(ReceiverSuite.scala:200)
	at org.apache.spark.streaming.ReceiverSuite$$anonfun$3.apply(ReceiverSuite.scala:158)
	at org.apache.spark.streaming.ReceiverSuite$$anonfun$3.apply(ReceiverSuite.scala:158)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.streaming.ReceiverSuite.org$scalatest$BeforeAndAfter$$super$runTest(ReceiverSuite.scala:39)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.streaming.ReceiverSuite.runTest(ReceiverSuite.scala:39)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.streaming.ReceiverSuite.org$scalatest$BeforeAndAfter$$super$run(ReceiverSuite.scala:39)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.streaming.ReceiverSuite.run(ReceiverSuite.scala:39)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-SBT/1625/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/testReport/junit/org.apache.spark.streaming/ReceiverSuite/block_generator_throttling/",,joshrosen,pwendell,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Dec 30 21:59:38 UTC 2015,,,,,,,,,,"0|i25bd3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/15 02:00;scwf;my locally test with dev/run-tests also go into this issue.

{code}
org.apache.spark.streaming.ReceiverSuite.block generator throttling

Error Message

126 was greater than or equal to 95.0, but 126 was not less than or equal to 105.0 # records in received blocks = [91,369,294,39,100,100,101,99,100,100,100,100,100,101,100,99,5], not between 95.0 and 105.0, on average
Stacktrace

sbt.ForkMain$ForkError: 126 was greater than or equal to 95.0, but 126 was not less than or equal to 105.0 # records in received blocks = [91,369,294,39,100,100,101,99,100,100,100,100,100,101,100,99,5], not between 95.0 and 105.0, on average
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
	at org.apache.spark.streaming.ReceiverSuite$$anonfun$3.apply$mcV$sp(ReceiverSuite.scala:207)
	at org.apache.spark.streaming.ReceiverSuite$$anonfun$3.apply(ReceiverSuite.scala:158)
	at org.apache.spark.streaming.ReceiverSuite$$anonfun$3.apply(ReceiverSuite.scala:158)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.streaming.ReceiverSuite.org$scalatest$BeforeAndAfter$$super$runTest(ReceiverSuite.scala:39)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.streaming.ReceiverSuite.runTest(ReceiverSuite.scala:39)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.streaming.ReceiverSuite.org$scalatest$BeforeAndAfter$$super$run(ReceiverSuite.scala:39)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.streaming.ReceiverSuite.run(ReceiverSuite.scala:39)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{code}
;;;","30/Dec/15 21:59;joshrosen;Resolving as fixed, since this hasn't ever been observed to fail in master/1.6: https://spark-tests.appspot.com/tests/org.apache.spark.streaming.ReceiverSuite/block%20generator%20throttling;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
NegativeArraySizeException in EigenValueDecomposition.symmetricEigs for large n and/or large k,SPARK-5656,12773085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mbittmann,mbittmann,mbittmann,06/Feb/15 19:03,12/Feb/15 04:39,14/Jul/23 06:27,08/Feb/15 10:13,,,,,,,1.4.0,,,,,,MLlib,,,,0,,,,,,"Large values of n or k in EigenValueDecomposition.symmetricEigs will fail with a NegativeArraySizeException. Specifically, this occurs when 2*n*k > Integer.MAX_VALUE. These values are currently unchecked and allow for the array to be initialized to a value greater than Integer.MAX_VALUE. I have written the below 'require' to fail this condition gracefully. I will submit a pull request. 

require(ncv * n.toLong < Integer.MAX_VALUE, ""Product of 2*k*n must be smaller than "" +
      s""Integer.MAX_VALUE. Found required eigenvalues k = $k and matrix dimension n = $n"")


Here is the exception that occurs from computeSVD with large k and/or n: 

Exception in thread ""main"" java.lang.NegativeArraySizeException
	at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:85)
	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:258)
	at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:190)",,apachespark,mbittmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 08 10:13:42 UTC 2015,,,,,,,,,,"0|i25ax3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/15 19:15;apachespark;User 'mbittmann' has created a pull request for this issue:
https://github.com/apache/spark/pull/4433;;;","08/Feb/15 10:13;srowen;Issue resolved by pull request 4433
[https://github.com/apache/spark/pull/4433];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
YARN Auxiliary Shuffle service can't access shuffle files on Hadoop cluster configured in secure mode,SPARK-5655,12773077,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,andrew.rowson,andrew.rowson,andrew.rowson,06/Feb/15 18:49,12/Feb/15 19:13,14/Jul/23 06:27,12/Feb/15 18:45,1.2.1,,,,,,1.2.2,1.3.0,,,,,YARN,,,,0,hadoop,,,,,"When running a Spark job on a YARN cluster which doesn't run containers under the same user as the nodemanager, and also when using the YARN auxiliary shuffle service, jobs fail with something similar to:
{code:java}
java.io.FileNotFoundException: /data/9/yarn/nm/usercache/username/appcache/application_1423069181231_0032/spark-c434a703-7368-4a05-9e99-41e77e564d1d/3e/shuffle_0_0_0.index (Permission denied)
{code}

The root cause of this here: https://github.com/apache/spark/blob/branch-1.2/core/src/main/scala/org/apache/spark/util/Utils.scala#L287

Spark will attempt to chmod 700 any application directories it creates during the job, which includes files created in the nodemanager's usercache directory. The owner of these files is the container UID, which on a secure cluster is the name of the user creating the job, and on an nonsecure cluster but with the yarn.nodemanager.container-executor.class configured is the value of yarn.nodemanager.linux-container-executor.nonsecure-mode.local-user.

The problem with this is that the auxiliary shuffle manager runs as part of the nodemanager, which is typically running as the user 'yarn'. This can't access these files that are only owner-readable.

YARN already attempts to secure files created under appcache but keep them readable by the nodemanager, by setting the group of the appcache directory to 'yarn' and also setting the setgid flag. This means that files and directories created under this should also have the 'yarn' group. Normally this means that the nodemanager should also be able to read these files, but Spark setting chmod700 wipes this out.

I'm not sure what the right approach is here. Commenting out the chmod700 functionality makes this work on YARN, and still makes the application files only readable by the owner and the group:

{code}
/data/1/yarn/nm/usercache/username/appcache/application_1423247249655_0001/spark-c7a6fc0f-e5df-49cf-a8f5-e51a1ca087df/0c # ls -lah
total 206M
drwxr-s---  2 nobody yarn 4.0K Feb  6 18:30 .
drwxr-s--- 12 nobody yarn 4.0K Feb  6 18:30 ..
-rw-r-----  1 nobody yarn 206M Feb  6 18:30 shuffle_0_0_0.data
{code}
But this may not be the right approach on non-YARN. Perhaps an additional step to see if this chmod700 step is necessary (ie non-YARN) is required. Sadly, I don't have a non-YARN environment to test, otherwise I'd be able to suggest a patch.

I believe this is a related issue in the MapReduce framwork: https://issues.apache.org/jira/browse/MAPREDUCE-3728","Both CDH5.3.0 and CDH5.1.3, latest build on branch-1.2",514793425@qq.com,andrew.rowson,apachespark,dougb,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 16:53:53 UTC 2015,,,,,,,,,,"0|i25avb:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"10/Feb/15 16:26;apachespark;User 'growse' has created a pull request for this issue:
https://github.com/apache/spark/pull/4507;;;","10/Feb/15 16:53;apachespark;User 'growse' has created a pull request for this issue:
https://github.com/apache/spark/pull/4509;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
in ApplicationMaster rename isDriver to isClusterMode,SPARK-5653,12773013,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lianhuiwang,lianhuiwang,lianhuiwang,06/Feb/15 14:17,06/Feb/15 18:49,14/Jul/23 06:27,06/Feb/15 18:48,1.0.0,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"in ApplicationMaster rename isDriver to isClusterMode,because in Client it uses isClusterMode,ApplicationMaster should keep consistent with it and uses isClusterMode.isClusterMode is easier to understand.",,apachespark,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 14:20:30 UTC 2015,,,,,,,,,,"0|i25ahb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"06/Feb/15 14:20;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4430;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use broadcasted weights in LogisticRegressionModel and fix compilation error,SPARK-5652,12772984,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,06/Feb/15 11:58,06/Feb/15 19:22,14/Jul/23 06:27,06/Feb/15 19:22,,,,,,,1.3.0,,,,,,,,,,0,,,,,,LogisticRegressionModel's predictPoint should directly use broadcasted weights. This pr also fixes the compilation errors of two unit test suite.,,apachespark,mengxr,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 19:22:28 UTC 2015,,,,,,,,,,"0|i25aav:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/15 11:59;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/4429;;;","06/Feb/15 19:22;mengxr;Issue resolved by pull request 4429
[https://github.com/apache/spark/pull/4429];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support 'create db.table' in HiveContext,SPARK-5651,12772958,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,waterman,waterman,waterman,06/Feb/15 09:42,25/Apr/15 21:50,14/Jul/23 06:27,18/Mar/15 02:47,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Now spark version is only support ```create table table_in_database_creation.test1 as select * from src limit 1;``` in HiveContext.
This patch is used to support ```create table `table_in_database_creation.test2` as select * from src limit 1;``` in HiveContext.",,apachespark,marmbrus,waterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 02:47:05 UTC 2015,,,,,,,,,,"0|i25a53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/15 09:46;apachespark;User 'watermen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4427;;;","09/Feb/15 15:19;apachespark;User 'OopsOutOfMemory' has created a pull request for this issue:
https://github.com/apache/spark/pull/4473;;;","18/Mar/15 02:47;marmbrus;Issue resolved by pull request 4427
[https://github.com/apache/spark/pull/4427];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Optional 'FROM' clause in HiveQl,SPARK-5650,12772957,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,viirya,viirya,viirya,06/Feb/15 09:41,12/Feb/15 04:42,14/Jul/23 06:27,06/Feb/15 20:14,,,,,,,1.3.0,,,,,,,,,,0,,,,,,"In Hive, 'FROM' clause should be optional.",,apachespark,marmbrus,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 20:14:00 UTC 2015,,,,,,,,,,"0|i25a4v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/15 09:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/4426;;;","06/Feb/15 20:14;marmbrus;Issue resolved by pull request 4426
[https://github.com/apache/spark/pull/4426];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
org.apache.spark.sql.catalyst.ScalaReflection is not thread safe,SPARK-5640,12772888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tobias.schlatter,tobias.schlatter,tobias.schlatter,06/Feb/15 01:43,12/Feb/15 04:42,14/Jul/23 06:27,06/Feb/15 20:15,,,,,,,1.3.0,,,,,,,,,,0,,,,,,ScalaReflection uses the Scala reflection API but does not synchronize (for example in the {{schemaFor}} method). This leads to concurrency bugs.,,apachespark,marmbrus,tobias.schlatter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 20:15:44 UTC 2015,,,,,,,,,,"0|i259pr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/15 17:27;apachespark;User 'gzm0' has created a pull request for this issue:
https://github.com/apache/spark/pull/4431;;;","06/Feb/15 20:15;marmbrus;Issue resolved by pull request 4431
[https://github.com/apache/spark/pull/4431];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Lower dynamic allocation add interval,SPARK-5636,12772870,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,05/Feb/15 23:48,06/Feb/15 18:54,14/Jul/23 06:27,06/Feb/15 18:54,1.2.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,The current default of 1 min is a little long especially since a recent patch causes the number of executors to start at 0 by default. We should ramp up much more quickly in the beginning.,,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 23:53:55 UTC 2015,,,,,,,,,,"0|i259lz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"05/Feb/15 23:53;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4409;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
History server shows misleading message when there are no incomplete apps,SPARK-5634,12772844,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,05/Feb/15 22:07,15/Apr/15 21:02,14/Jul/23 06:27,15/Apr/15 01:52,1.2.0,,,,,,1.3.2,1.4.0,,,,,Spark Core,,,,1,,,,,,"If you go to the history server, and click on ""Show incomplete applications"", but there are no incomplete applications, you get a misleading message:

{noformat}
No completed applications found!

Did you specify the correct logging directory? (etc etc)
{noformat}

That's the same message used when no complete applications are found; it should probably be tweaked for the incomplete apps case.",,apachespark,rdub,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 14 23:43:28 UTC 2015,,,,,,,,,,"0|i259gf:",9223372036854775807,,,,,,,,,,,,,,1.3.2,1.4.0,,,,,,,,,,,,"14/Apr/15 23:43;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/5515;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spurious test failures due to NullPointerException in EasyMock test code,SPARK-5626,12772793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,joshrosen,joshrosen,joshrosen,05/Feb/15 20:00,25/Apr/15 21:51,14/Jul/23 06:27,13/Feb/15 18:28,1.3.0,,,,,,,,,,,,Tests,,,,0,flaky-test,,,,,"I've seen a few cases where a test failure will trigger a cascade of spurious failures when instantiating test suites that use EasyMock.  Here's a sample symptom:

{code}
[info] CacheManagerSuite:
[info] Exception encountered when attempting to run a suite with class name: org.apache.spark.CacheManagerSuite *** ABORTED *** (137 milliseconds)
[info]   java.lang.NullPointerException:
[info]   at org.objenesis.strategy.StdInstantiatorStrategy.newInstantiatorOf(StdInstantiatorStrategy.java:52)
[info]   at org.objenesis.ObjenesisBase.getInstantiatorOf(ObjenesisBase.java:90)
[info]   at org.objenesis.ObjenesisBase.newInstance(ObjenesisBase.java:73)
[info]   at org.objenesis.ObjenesisHelper.newInstance(ObjenesisHelper.java:43)
[info]   at org.easymock.internal.ObjenesisClassInstantiator.newInstance(ObjenesisClassInstantiator.java:26)
[info]   at org.easymock.internal.ClassProxyFactory.createProxy(ClassProxyFactory.java:219)
[info]   at org.easymock.internal.MocksControl.createMock(MocksControl.java:59)
[info]   at org.easymock.EasyMock.createMock(EasyMock.java:103)
[info]   at org.scalatest.mock.EasyMockSugar$class.mock(EasyMockSugar.scala:267)
[info]   at org.apache.spark.CacheManagerSuite.mock(CacheManagerSuite.scala:28)
[info]   at org.apache.spark.CacheManagerSuite$$anonfun$1.apply$mcV$sp(CacheManagerSuite.scala:40)
[info]   at org.apache.spark.CacheManagerSuite$$anonfun$1.apply(CacheManagerSuite.scala:38)
[info]   at org.apache.spark.CacheManagerSuite$$anonfun$1.apply(CacheManagerSuite.scala:38)
[info]   at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:195)
[info]   at org.apache.spark.CacheManagerSuite.runTest(CacheManagerSuite.scala:28)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.apache.spark.CacheManagerSuite.org$scalatest$BeforeAndAfter$$super$run(CacheManagerSuite.scala:28)
[info]   at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
[info]   at org.apache.spark.CacheManagerSuite.run(CacheManagerSuite.scala:28)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}

This is from https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/26852/consoleFull.",,joshrosen,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5735,,,,,,,,,,,,,,,"05/Feb/15 20:01;joshrosen;consoleText.txt;https://issues.apache.org/jira/secure/attachment/12696844/consoleText.txt",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 13 18:28:32 UTC 2015,,,,,,,,,,"0|i2595b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/15 20:01;joshrosen;I've uploaded a copy of the full Jenkins log so that we don't lose it after the test information is garbage-collected.;;;","06/Feb/15 02:27;pwendell;[~joshrosen] I may have caused this by merging SPARK-5607;;;","07/Feb/15 00:17;joshrosen;It's probably easiest to just replace our use of EasyMock with Mockito, since I think it's only used in one or two suites and there's not a good reason for us to be using two different mocking frameworks.;;;","13/Feb/15 18:28;joshrosen;This should hopefully be fixed now that I've merged SPARK-5735 to remove EasyMock.  I'm going to resolve this issue for now, but let's re-open it if we observe this flakiness again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Can't find new column ,SPARK-5624,12772746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,alexliu68,alexliu68,05/Feb/15 17:31,15/Sep/15 22:51,14/Jul/23 06:27,15/Sep/15 22:51,1.1.1,,,,,,,,,,,,SQL,,,,0,,,,,,"The following test fails

{code}
0: jdbc:hive2://localhost:10000> DROP TABLE IF EXISTS alter_test_table;
+---------+
| Result  |
+---------+
+---------+
No rows selected (0.175 seconds)
0: jdbc:hive2://localhost:10000> DROP TABLE IF EXISTS alter_test_table_ctas;
+---------+
| Result  |
+---------+
+---------+
No rows selected (0.155 seconds)
0: jdbc:hive2://localhost:10000> DROP TABLE IF EXISTS alter_test_table_renamed;
+---------+
| Result  |
+---------+
+---------+
No rows selected (0.162 seconds)
0: jdbc:hive2://localhost:10000> CREATE TABLE alter_test_table (foo INT, bar STRING) COMMENT 'table to test DDL ops' PARTITIONED BY (ds STRING) STORED AS TEXTFILE;
+---------+
| result  |
+---------+
+---------+
No rows selected (0.247 seconds)
0: jdbc:hive2://localhost:10000> LOAD DATA LOCAL INPATH '/Users/alex/project/automaton/resources/tests/data/files/kv1.txt' OVERWRITE INTO TABLE alter_test_table PARTITION (ds='2008-08-08');  
+---------+
| result  |
+---------+
+---------+
No rows selected (0.367 seconds)
0: jdbc:hive2://localhost:10000> CREATE TABLE alter_test_table_ctas as SELECT * FROM alter_test_table;
+------+------+-----+
| foo  | bar  | ds  |
+------+------+-----+
+------+------+-----+
No rows selected (0.641 seconds)
0: jdbc:hive2://localhost:10000> ALTER TABLE alter_test_table ADD COLUMNS (new_col1 INT);
+---------+
| result  |
+---------+
+---------+
No rows selected (0.226 seconds)
0: jdbc:hive2://localhost:10000> INSERT OVERWRITE TABLE alter_test_table PARTITION (ds='2008-08-15') SELECT foo, bar, 333333333 FROM alter_test_table_ctas WHERE ds='2008-08-08';
+------+------+------+
| foo  | bar  | c_2  |
+------+------+------+
+------+------+------+
No rows selected (0.522 seconds)
0: jdbc:hive2://localhost:10000> select * from alter_test_table ;
Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 35.0 failed 4 times, most recent failure: Lost task 0.3 in stage 35.0 (TID 66, 127.0.0.1): java.lang.RuntimeException: cannot find field new_col1 from [0:foo, 1:bar]
        org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:367)
        org.apache.hadoop.hive.serde2.lazy.objectinspector.LazySimpleStructObjectInspector.getStructFieldRef(LazySimpleStructObjectInspector.java:168)
        org.apache.spark.sql.hive.HadoopTableReader$$anonfun$9.apply(TableReader.scala:275)
        org.apache.spark.sql.hive.HadoopTableReader$$anonfun$9.apply(TableReader.scala:275)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        scala.collection.AbstractTraversable.map(Traversable.scala:105)
        org.apache.spark.sql.hive.HadoopTableReader$.fillObject(TableReader.scala:275)
        org.apache.spark.sql.hive.HadoopTableReader$$anonfun$3$$anonfun$apply$1.apply(TableReader.scala:193)
        org.apache.spark.sql.hive.HadoopTableReader$$anonfun$3$$anonfun$apply$1.apply(TableReader.scala:187)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
        org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)
        org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)
        org.apache.spark.scheduler.Task.run(Task.scala:54)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace: (state=,code=0)
{code}",,alexliu68,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 22:51:12 UTC 2015,,,,,,,,,,"0|i258uv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/15 23:40;alexliu68;Test it on the latest master branch it doesn't have this issue.;;;","15/Sep/15 22:51;joshrosen;Resolving as ""Fixed"" per claim that this works in a newer release.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Support 'show roles' in HiveContext,SPARK-5619,12772675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,waterman,waterman,waterman,05/Feb/15 11:51,12/Feb/15 04:42,14/Jul/23 06:27,06/Feb/15 20:29,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,apachespark,marmbrus,waterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 20:29:45 UTC 2015,,,,,,,,,,"0|i258f3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/15 11:55;apachespark;User 'watermen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4397;;;","06/Feb/15 20:29;marmbrus;Issue resolved by pull request 4397
[https://github.com/apache/spark/pull/4397];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
test failure of SQLQuerySuite,SPARK-5617,12772669,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,scwf,scwf,scwf,05/Feb/15 11:36,06/Feb/15 18:36,14/Jul/23 06:27,06/Feb/15 18:36,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"SQLQuerySuite test failure: 
{code}
[info] - simple select (22 milliseconds)
[info] - sorting (722 milliseconds)
[info] - external sorting (728 milliseconds)
[info] - limit (95 milliseconds)
[info] - date row *** FAILED *** (35 milliseconds)
[info]   Results do not match for query:
[info]   'Limit 1
[info]    'Project [CAST(2015-01-28, DateType) AS c0#3630]
[info]     'UnresolvedRelation [testData], None
[info]   
[info]   == Analyzed Plan ==
[info]   Limit 1
[info]    Project [CAST(2015-01-28, DateType) AS c0#3630]
[info]     LogicalRDD [key#0,value#1], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:35
[info]   
[info]   == Physical Plan ==
[info]   Limit 1
[info]    Project [16463 AS c0#3630]
[info]     PhysicalRDD [key#0,value#1], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:35
[info]   
[info]   == Results ==
[info]   !== Correct Answer - 1 ==   == Spark Answer - 1 ==
[info]   ![2015-01-28]               [2015-01-27] (QueryTest.scala:77)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:495)
[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
[info]   at org.scalatest.Assertions$class.fail(Assertions.scala:1328)
[info]   at org.scalatest.FunSuite.fail(FunSuite.scala:1555)
[info]   at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:77)
[info]   at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:95)
[info]   at org.apache.spark.sql.SQLQuerySuite$$anonfun$23.apply$mcV$sp(SQLQuerySuite.scala:300)
[info]   at org.apache.spark.sql.SQLQuerySuite$$anonfun$23.apply(SQLQuerySuite.scala:300)
[info]   at org.apache.spark.sql.SQLQuerySuite$$anonfun$23.apply(SQLQuerySuite.scala:300)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNode
{code}",,apachespark,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 11:43:01 UTC 2015,,,,,,,,,,"0|i258dr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"05/Feb/15 11:43;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/4395;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
YarnClientSchedulerBackend fails to get application report when yarn restarts,SPARK-5613,12772599,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kasjain,kasjain,kasjain,05/Feb/15 05:56,11/Feb/15 04:02,14/Jul/23 06:27,10/Feb/15 21:04,1.2.0,,,,,,1.2.2,1.3.0,,,,,YARN,,,,0,,,,,,"Steps to Reproduce
1) Run any spark job
2) Stop yarn while the spark job is running (an application id has been generated by now)
3) Restart yarn now
4) AsyncMonitorApplication thread fails due to ApplicationNotFoundException exception. This leads to termination of thread. 







Here is the StackTrace

15/02/05 05:22:37 INFO Client: Retrying connect to server: nn1/192.168.173.176:8032. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
15/02/05 05:22:38 INFO Client: Retrying connect to server: nn1/192.168.173.176:8032. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
15/02/05 05:22:39 INFO Client: Retrying connect to server: nn1/192.168.173.176:8032. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
15/02/05 05:22:40 INFO Client: Retrying connect to server: nn1/192.168.173.176:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)
5/02/05 05:22:40 INFO Client: Retrying connect to server: nn1/192.168.173.176:8032. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)

Exception in thread ""Yarn application state monitor"" org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1423113179043_0003' doesn't exist in RM.
	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:284)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:145)
	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:321)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Unknown Source)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
	at java.lang.reflect.Constructor.newInstance(Unknown Source)
	at org.apache.hadoop.yarn.ipc.RPCUtil.instantiateException(RPCUtil.java:53)
	at org.apache.hadoop.yarn.ipc.RPCUtil.unwrapAndThrowException(RPCUtil.java:101)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplicationReport(ApplicationClientProtocolPBClientImpl.java:166)
	at sun.reflect.GeneratedMethodAccessor18.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy12.getApplicationReport(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplicationReport(YarnClientImpl.java:291)
	at org.apache.spark.deploy.yarn.Client.getApplicationReport(Client.scala:116)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$$anon$1.run(YarnClientSchedulerBackend.scala:120)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException): Application with id 'application_1423113179043_0003' doesn't exist in RM.
	at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:284)
	at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:145)
	at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:321)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Unknown Source)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy11.getApplicationReport(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplicationReport(ApplicationClientProtocolPBClientImpl.java:163)
	... 9 more
",,andrewor14,apachespark,kasjain,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 11 04:02:54 UTC 2015,,,,,,,,,,"0|i257yn:",9223372036854775807,,,,,kasjain,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"05/Feb/15 09:24;apachespark;User 'kasjain' has created a pull request for this issue:
https://github.com/apache/spark/pull/4392;;;","09/Feb/15 06:50;kasjain;The fix has been cherry-picked to 1.2 and master branch but not 1.3.  ;;;","10/Feb/15 20:13;pwendell;I have cherry picked it into the 1.3 branch.;;;","10/Feb/15 21:04;andrewor14;Thanks Patrick. I just verified that it was merged into all of 1.2, 1.3 and Master. Closing this again.;;;","11/Feb/15 04:02;kasjain;Thanks Patrick and Andrew;;;",,,,,,,,,,,,,,,,,,,,,,,,
PythonMLlibAPI trainGaussianMixture seed should use Java type,SPARK-5609,12772572,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,mengxr,josephkb,josephkb,05/Feb/15 03:10,05/Feb/15 05:43,14/Jul/23 06:27,05/Feb/15 05:43,1.3.0,,,,,,1.3.0,,,,,,MLlib,PySpark,,,0,,,,,,"trainGaussianMixture takes parameter seed of type scala.Long but should take java.lang.Long.
Otherwise, the test for whether seed is null (None in Python) will be ineffective.  See compilation warning:
{code}
[warn] /Users/josephkb/spark/mllib/src/main/scala/org/apache/spark/mllib/api/python/PythonMLLibAPI.scala:304: comparing values of types Long and Null using `!=' will always yield true
[warn]     if (seed != null) gmmAlg.setSeed(seed)
[warn]              ^
{code}
",,josephkb,MeethuMathew,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 05:43:24 UTC 2015,,,,,,,,,,"0|i257sn:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"05/Feb/15 03:12;josephkb;Ping [~MeethuMathew] [~mengxr];;;","05/Feb/15 04:02;MeethuMathew;Please assign the ticket to me. [~josephkb]
;;;","05/Feb/15 04:36;MeethuMathew;I think this is solved with this https://github.com/apache/spark/commit/679228b7f4c865147ac65099ffd6f5aa45e7126d;;;","05/Feb/15 05:37;josephkb;Oh, thanks!  I must have an outdated master...;;;","05/Feb/15 05:43;josephkb;Fixed by [https://github.com/apache/spark/commit/679228b7f4c865147ac65099ffd6f5aa45e7126d];;;",,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in objenesis,SPARK-5607,12772567,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,rxin,rxin,05/Feb/15 02:42,17/Feb/15 22:16,14/Jul/23 06:27,17/Feb/15 22:16,,,,,,,1.3.0,,,,,,Tests,,,,0,,,,,,"Tests are sometimes failing with the following exception.

The problem might be that Kryo is using a different version of objenesis from Mockito.

{code}
[info] - Process succeeds instantly *** FAILED *** (107 milliseconds)
[info]   java.lang.NullPointerException:
[info]   at org.objenesis.strategy.StdInstantiatorStrategy.newInstantiatorOf(StdInstantiatorStrategy.java:52)
[info]   at org.objenesis.ObjenesisBase.getInstantiatorOf(ObjenesisBase.java:90)
[info]   at org.objenesis.ObjenesisBase.newInstance(ObjenesisBase.java:73)
[info]   at org.mockito.internal.creation.jmock.ClassImposterizer.createProxy(ClassImposterizer.java:111)
[info]   at org.mockito.internal.creation.jmock.ClassImposterizer.imposterise(ClassImposterizer.java:51)
[info]   at org.mockito.internal.util.MockUtil.createMock(MockUtil.java:52)
[info]   at org.mockito.internal.MockitoCore.mock(MockitoCore.java:41)
[info]   at org.mockito.Mockito.mock(Mockito.java:1014)
[info]   at org.mockito.Mockito.mock(Mockito.java:909)
[info]   at org.apache.spark.deploy.worker.DriverRunnerTest$$anonfun$1.apply$mcV$sp(DriverRunnerTest.scala:50)
[info]   at org.apache.spark.deploy.worker.DriverRunnerTest$$anonfun$1.apply(DriverRunnerTest.scala:47)
[info]   at org.apache.spark.deploy.worker.DriverRunnerTest$$anonfun$1.apply(DriverRunnerTest.scala:47)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuite.run(FunSuite.scala:1555)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}


More information:

Kryo depends on objenesis 1.2, and line 52 is getting some VM_VERSION, which is a static field from the super class.

Mockito depends on objenesis 2.1.
",,apachespark,joshrosen,pwendell,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 22:16:00 UTC 2015,,,,,,,,,,"0|i257rj:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"05/Feb/15 03:31;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/4383;;;","05/Feb/15 06:40;pwendell;I've merged a patch attempting to fix this. Let's re-open this if we see it again;;;","06/Feb/15 01:37;pwendell;This may have actually caused more of an issue than it solved :(. Lots of cascading failures in Spark SQL recently;;;","06/Feb/15 02:38;pwendell;I've reverted my patch since it may have caused more harm than good.;;;","17/Feb/15 22:16;joshrosen;I think that this has been fixed by my patch that removes EasyMock, so I'm going to mark this as Resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support plus sign in HiveContext,SPARK-5606,12772555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,waterman,waterman,waterman,05/Feb/15 01:29,12/Feb/15 04:44,14/Jul/23 06:27,05/Feb/15 07:16,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Now spark version is only support ```SELECT -key FROM DECIMAL_UDF;``` in HiveContext.
This patch is used to support ```SELECT +key FROM DECIMAL_UDF;``` in HiveContext.",,apachespark,marmbrus,waterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 07:16:50 UTC 2015,,,,,,,,,,"0|i257p3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/15 01:41;apachespark;User 'watermen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4378;;;","05/Feb/15 07:16;marmbrus;Issue resolved by pull request 4378
[https://github.com/apache/spark/pull/4378];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Sort order of unfinished apps can be wrong in History Server,SPARK-5600,12772454,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,04/Feb/15 18:39,06/Feb/15 22:23,14/Jul/23 06:27,06/Feb/15 22:23,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"The code that merges new logs with old logs sorts applications by their end time only. Unfinished apps all have the same end time (-1), so the sort order ends up being undefined.

This was uncovered by the attempt to fix SPARK-5345 (https://github.com/apache/spark/pull/4133).",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 19:13:20 UTC 2015,,,,,,,,,,"0|i2572v:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"04/Feb/15 19:13;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4370;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace BlockManager listener with Executor listener in ExecutorAllocationListener,SPARK-5593,12772356,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lianhuiwang,lianhuiwang,lianhuiwang,04/Feb/15 12:56,06/Feb/15 18:50,14/Jul/23 06:27,06/Feb/15 18:50,1.2.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"More strictly, in ExecutorAllocationListener, we need to replace onBlockManagerAdded, onBlockManagerRemoved with onExecutorAdded,onExecutorRemoved. because at some time, onExecutorAdded and onExecutorRemoved are more accurate to express these meanings. example at SPARK-5529, BlockManager has been removed,but executor is existed.
[~andrewor14] [~sandyr] ",,apachespark,lianhuiwang,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 16:04:39 UTC 2015,,,,,,,,,,"0|i256h3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"04/Feb/15 16:04;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4369;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: Python regression,SPARK-5585,12772270,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,pwendell,pwendell,04/Feb/15 06:23,04/Feb/15 16:54,14/Jul/23 06:27,04/Feb/15 16:54,1.3.0,,,,,,1.3.0,,,,,,MLlib,,,,0,flaky-test,,,,,"Hey [~davies] any chance you can take a look at this? The master build is having random python failures fairly often. Not quite sure what is going on:

{code}
0inputs+128outputs (0major+13320minor)pagefaults 0swaps
Run mllib tests ...
Running test: pyspark/mllib/classification.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.43user 0.12system 0:14.85elapsed 3%CPU (0avgtext+0avgdata 94272maxresident)k
0inputs+280outputs (0major+12627minor)pagefaults 0swaps
Running test: pyspark/mllib/clustering.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.35user 0.11system 0:12.63elapsed 3%CPU (0avgtext+0avgdata 93568maxresident)k
0inputs+88outputs (0major+12532minor)pagefaults 0swaps
Running test: pyspark/mllib/feature.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.28user 0.08system 0:05.73elapsed 6%CPU (0avgtext+0avgdata 93424maxresident)k
0inputs+32outputs (0major+12548minor)pagefaults 0swaps
Running test: pyspark/mllib/linalg.py
0.16user 0.05system 0:00.22elapsed 98%CPU (0avgtext+0avgdata 89888maxresident)k
0inputs+0outputs (0major+8099minor)pagefaults 0swaps
Running test: pyspark/mllib/rand.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.25user 0.08system 0:05.42elapsed 6%CPU (0avgtext+0avgdata 87872maxresident)k
0inputs+0outputs (0major+11849minor)pagefaults 0swaps
Running test: pyspark/mllib/recommendation.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.32user 0.09system 0:11.42elapsed 3%CPU (0avgtext+0avgdata 94256maxresident)k
0inputs+32outputs (0major+11797minor)pagefaults 0swaps
Running test: pyspark/mllib/regression.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.53user 0.17system 0:23.53elapsed 3%CPU (0avgtext+0avgdata 99600maxresident)k
0inputs+48outputs (0major+12402minor)pagefaults 0swaps
Running test: pyspark/mllib/stat/_statistics.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.29user 0.09system 0:08.03elapsed 4%CPU (0avgtext+0avgdata 92656maxresident)k
0inputs+48outputs (0major+12508minor)pagefaults 0swaps
Running test: pyspark/mllib/tree.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.57user 0.16system 0:25.30elapsed 2%CPU (0avgtext+0avgdata 94400maxresident)k
0inputs+144outputs (0major+12600minor)pagefaults 0swaps
Running test: pyspark/mllib/util.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
0.20user 0.06system 0:08.08elapsed 3%CPU (0avgtext+0avgdata 92768maxresident)k
0inputs+56outputs (0major+12474minor)pagefaults 0swaps
Running test: pyspark/mllib/tests.py
tput: No value for $TERM and no -T specified
Spark assembly has been built with Hive, including Datanucleus jars on classpath
.........F/usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
./usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
/usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
/usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
/usr/lib64/python2.6/site-packages/numpy/core/fromnumeric.py:2499: VisibleDeprecationWarning: `rank` is deprecated; use the `ndim` attribute or function instead. To find the rank of a matrix see `numpy.linalg.matrix_rank`.
  VisibleDeprecationWarning)
./usr/lib64/python2.6/site-packages/numpy/lib/utils.py:95: DeprecationWarning: `dot` is deprecated!
  warnings.warn(depdoc, DeprecationWarning)
.............
======================================================================
FAIL: test_regression (__main__.ListTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""pyspark/mllib/tests.py"", line 289, in test_regression
    self.assertTrue(rf_model.predict(features[0]) <= 0)
AssertionError: False is not true

----------------------------------------------------------------------
Ran 25 tests in 53.798s
{code}

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=centos/1496/console",,apachespark,davies,mengxr,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 16:54:38 UTC 2015,,,,,,,,,,"0|i255xz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/15 07:06;davies;[~pwendell] I can not reproduce it locally, will add a seed for it, test it several times in jenkins.;;;","04/Feb/15 07:13;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4358;;;","04/Feb/15 16:54;mengxr;Issue resolved by pull request 4358
[https://github.com/apache/spark/pull/4358];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
History server does not list anything if log root contains an empty directory,SPARK-5582,12772240,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vanzin,vanzin,vanzin,04/Feb/15 03:18,06/Feb/15 18:43,14/Jul/23 06:27,06/Feb/15 10:07,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"As summary says. Exception from logs:

{noformat}
15/02/03 17:35:10.292 pool-1-thread-1-ScalaTest-running-FsHistoryProviderSuite ERROR FsHistoryProvider: Exception in checking for event log updates
java.lang.UnsupportedOperationException: empty.max
        at scala.collection.TraversableOnce$class.max(TraversableOnce.scala:216)
        at scala.collection.AbstractTraversable.max(Traversable.scala:105)
        at org.apache.spark.deploy.history.FsHistoryProvider.org$apache$spark$deploy$history$FsHistoryProvider$$getModificationTime(FsHistoryProvider.scala:315)
{noformat}

Patch coming up.",,apachespark,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 10:07:51 UTC 2015,,,,,,,,,,"0|i255rj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/15 03:36;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4352;;;","06/Feb/15 10:07;srowen;Issue resolved by pull request 4352
[https://github.com/apache/spark/pull/4352];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Grep bug in compute-classpath.sh,SPARK-5580,12772230,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,waterman,waterman,04/Feb/15 02:30,04/Feb/15 02:50,14/Jul/23 06:27,04/Feb/15 02:50,1.2.0,,,,,,1.3.0,,,,,,,,,,0,,,,,,"When I test spark, I often need to change assembly jar to test different version.
So I will move spark-assembly.*hadoop.*.jar to spark-assembly.*hadoop.*.jar.bak.
But I will get the error info ""Found multiple Spark assembly jars in $assembly_folder:"". I think it just need to compare jar, so the grep expression need to begin with ""^"" and end with ""$"".",,waterman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-02-04 02:30:31.0,,,,,,,,,,"0|i255pb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Utils.createDirectory ignores namePrefix,SPARK-5574,12772174,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,imranr,irashid,irashid,03/Feb/15 22:37,04/Feb/15 09:02,14/Jul/23 06:27,04/Feb/15 09:02,,,,,,,1.3.0,,,,,,,,,,0,,,,,,"this is really minor, I just noticed it as I was trying to find the ""blockmgr"" dir during some debugging, and then realized that the {{namePrefix}} is ignored in  {{Utils.createDirectory}}.  Also via {{Utils.createTempDir}} this effects these dirs:

* httpd
* userFiles
* broadcast

I'll submit a PR.",,apachespark,irashid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 22:48:49 UTC 2015,,,,,,,,,,"0|i255dr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/15 22:48;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/4344;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"No docs stating that `new SparkConf().set(""spark.driver.memory"", ...) will not work",SPARK-5570,12772115,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ilganeli,tdas,tdas,03/Feb/15 19:26,27/Feb/15 13:09,14/Jul/23 06:27,27/Feb/15 13:09,1.2.0,,,,,,1.3.0,,,,,,Documentation,,,,0,,,,,,,,apachespark,michaelmalak,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 23:53:52 UTC 2015,,,,,,,,,,"0|i25513:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/15 16:27;ilganeli;I would be happy to fix this. Thank you. ;;;","17/Feb/15 23:53;apachespark;User 'ilganeli' has created a pull request for this issue:
https://github.com/apache/spark/pull/4665;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
pySpark zip function unexpected errors,SPARK-5558,12772023,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cchayden,cchayden,03/Feb/15 13:41,10/Feb/15 09:17,14/Jul/23 06:27,10/Feb/15 09:17,1.2.0,,,,,,1.3.0,,,,,,PySpark,,,,0,pyspark,,,,,"Example:
{quote}
x = sc.parallelize(range(0,5))
y = x.map(lambda x: x+1000, preservesPartitioning=True)
y.take(10)
x.zip\(y).collect()
{quote}

Fails in the JVM: py4J: org.apache.spark.SparkException: 
Can only zip RDDs with same number of elements in each partition

If the range is changed to range(0,1000) it fails in pySpark code:
ValueError: Can not deserialize RDD with different number of items in pair: (100, 1) 

It also fails if y.take(10) is replaced with y.toDebugString()
It even fails if we print y._jrdd",,cchayden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 09:17:29 UTC 2015,,,,,,,,,,"0|i254h3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/15 02:59;cchayden;This seems to be working as expected in 1.3 branch and in master.;;;","10/Feb/15 09:17;srowen;Could be related to how https://issues.apache.org/jira/browse/SPARK-5351 was fixed. OK, let's close for now if it seems to be verified as fixed for 1.3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Servlet API classes now missing after jetty shading,SPARK-5557,12772001,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,pwendell,gq,gq,03/Feb/15 11:10,06/Feb/15 02:15,14/Jul/23 06:27,06/Feb/15 02:15,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,5,,,,,,"the log:
{noformat}
5/02/03 19:06:39 INFO spark.HttpServer: Starting HTTP Server
Exception in thread ""main"" java.lang.NoClassDefFoundError: javax/servlet/http/HttpServletResponse
	at org.apache.spark.HttpServer.org$apache$spark$HttpServer$$doStart(HttpServer.scala:75)
	at org.apache.spark.HttpServer$$anonfun$1.apply(HttpServer.scala:62)
	at org.apache.spark.HttpServer$$anonfun$1.apply(HttpServer.scala:62)
	at org.apache.spark.util.Utils$$anonfun$startServiceOnPort$1.apply$mcVI$sp(Utils.scala:1774)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.apache.spark.util.Utils$.startServiceOnPort(Utils.scala:1765)
	at org.apache.spark.HttpServer.start(HttpServer.scala:62)
	at org.apache.spark.repl.SparkIMain.<init>(SparkIMain.scala:130)
	at org.apache.spark.repl.SparkILoop$SparkILoopInterpreter.<init>(SparkILoop.scala:185)
	at org.apache.spark.repl.SparkILoop.createInterpreter(SparkILoop.scala:214)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:946)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:942)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:942)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:942)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1039)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:403)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:77)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: javax.servlet.http.HttpServletResponse
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	... 25 more

{noformat}",,apachespark,bmabey,gq,irashid,kostas,medale,meiyoula,nchammas,neelesh77,nemccarthy,pwendell,rajao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 01:07:07 UTC 2015,,,,,,,,,,"0|i254c7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/15 14:24;nemccarthy;I am seeing similar when trying to launch spark standalone master via start-master.sh.

Built via;
./make-distribution.sh --name mapr3 --skip-java-test --tgz -Pmapr3 -Phive -Phive-thriftserver -Phive-0.12.0 

Stack trace:


15/02/04 08:53:56 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/02/04 08:53:56 INFO Remoting: Starting remoting
15/02/04 08:53:56 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkMaster@hadoop-009:7077]
15/02/04 08:53:56 INFO Remoting: Remoting now listens on addresses: [akka.tcp://sparkMaster@hadoop-009:7077]
...skipping...
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at akka.util.Reflect$.instantiate(Reflect.scala:66)
        at akka.actor.ArgsReflectConstructor.produce(Props.scala:352)
        at akka.actor.Props.newActor(Props.scala:252)
        at akka.actor.ActorCell.newActor(ActorCell.scala:552)
        at akka.actor.ActorCell.create(ActorCell.scala:578)
        ... 9 more
Caused by: java.lang.NoClassDefFoundError: javax/servlet/FilterRegistration
        at org.spark-project.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:136)
        at org.spark-project.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:129)
        at org.spark-project.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:98)
        at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:96)
        at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:87)
        at org.apache.spark.ui.WebUI.attachPage(WebUI.scala:67)
        at org.apache.spark.deploy.master.ui.MasterWebUI.initialize(MasterWebUI.scala:40)
        at org.apache.spark.deploy.master.ui.MasterWebUI.<init>(MasterWebUI.scala:36)
        at org.apache.spark.deploy.master.Master.<init>(Master.scala:95)
        ... 18 more
Caused by: java.lang.ClassNotFoundException: javax.servlet.FilterRegistration
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 27 more

;;;","03/Feb/15 22:58;bmabey;I ran a git bisect and this is the first bad commit:

https://github.com/apache/spark/commit/7930d2bef0e2c7f62456e013124455061dfe6dc8

The commit adds a Jetty dep so that seems like the culprit.


;;;","05/Feb/15 06:11;nemccarthy;Any ideas on what needs to be done to fix this? ;;;","06/Feb/15 00:36;pwendell;I can send a fix for this shortly. It also works fine if you build with Hadoop 2 support.;;;","06/Feb/15 00:56;kostas;[~pwendell] recommended this which did the trick:
{code}
diff --git a/core/pom.xml b/core/pom.xml
index 2dc5f74..f03ec47 100644
--- a/core/pom.xml
+++ b/core/pom.xml
@@ -132,6 +132,11 @@
       <artifactId>jetty-servlet</artifactId>
       <scope>compile</scope>
     </dependency>
+    <dependency>
+      <groupId>org.eclipse.jetty.orbit</groupId>
+      <artifactId>javax.servlet</artifactId>
+      <version>3.0.0.v201112011016</version>
+    </dependency>
 
     <dependency>
       <groupId>org.apache.commons</groupId>
{code};;;","06/Feb/15 00:58;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/4411;;;","06/Feb/15 01:07;pwendell;I'm sorry that this affected so many people for so long. It is not acceptable to have the master build not working for this many hours. Unfortunately our tests do not catch this for some reason.;;;",,,,,,,,,,,,,,,,,,,,,,
Enable UISeleniumSuite tests,SPARK-5555,12771981,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,joshrosen,joshrosen,03/Feb/15 09:26,06/Feb/15 19:15,14/Jul/23 06:27,06/Feb/15 19:15,1.2.0,1.3.0,,,,,1.3.0,,,,,,Web UI,,,,0,,,,,,"SPARK-3616 added a suite of Selenium tests for the Spark Web UI and subsequent patches added regression tests to it.  These tests are currently disabled by default because they're a bit slow to run, but I think we should re-enable them to prevent regressions in the web UI.",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 19:15:21 UTC 2015,,,,,,,,,,"0|i2547r:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"03/Feb/15 09:54;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4334;;;","06/Feb/15 19:15;joshrosen;Issue resolved by pull request 4334
[https://github.com/apache/spark/pull/4334];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Custom UDF is case sensitive for HiveContext,SPARK-5550,12771953,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,chenghao,chenghao,03/Feb/15 06:47,03/Feb/15 20:13,14/Jul/23 06:27,03/Feb/15 20:13,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"SQL in HiveContext, should be case insensitive, however, the following query will fail.

```
udf.register(""random0"", ()  => { Math.random()})
assert(sql(""SELECT RANDOM0() FROM src LIMIT 1"").head().getDouble(0) >= 0.0)
```",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 20:13:14 UTC 2015,,,,,,,,,,"0|i2541j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/15 06:49;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4326;;;","03/Feb/15 20:13;marmbrus;Issue resolved by pull request 4326
[https://github.com/apache/spark/pull/4326];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.util.AkkaUtilsSuite.remote fetch ssl on - untrusted server,SPARK-5548,12771928,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,jlewandowski,pwendell,pwendell,03/Feb/15 03:50,19/Feb/15 17:55,14/Jul/23 06:27,19/Feb/15 17:55,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,flaky-test,,,,,"{code}
sbt.ForkMain$ForkError: Expected exception java.util.concurrent.TimeoutException to be thrown, but akka.actor.ActorNotFound was thrown.
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:496)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$class.intercept(Assertions.scala:1004)
	at org.scalatest.FunSuite.intercept(FunSuite.scala:1555)
	at org.apache.spark.util.AkkaUtilsSuite$$anonfun$8.apply$mcV$sp(AkkaUtilsSuite.scala:373)
	at org.apache.spark.util.AkkaUtilsSuite$$anonfun$8.apply(AkkaUtilsSuite.scala:349)
	at org.apache.spark.util.AkkaUtilsSuite$$anonfun$8.apply(AkkaUtilsSuite.scala:349)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
	at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.util.AkkaUtilsSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(AkkaUtilsSuite.scala:37)
	at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
	at org.apache.spark.util.AkkaUtilsSuite.runTest(AkkaUtilsSuite.scala:37)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.util.AkkaUtilsSuite.org$scalatest$BeforeAndAfterAll$$super$run(AkkaUtilsSuite.scala:37)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.util.AkkaUtilsSuite.run(AkkaUtilsSuite.scala:37)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: sbt.ForkMain$ForkError: Actor not found for: ActorSelection[Anchor(akka.ssl.tcp://spark@localhost:41417/), Path(/user/MapOutputTracker)]
	at akka.actor.ActorSelection$$anonfun$resolveOne$1.apply(ActorSelection.scala:65)
	at akka.actor.ActorSelection$$anonfun$resolveOne$1.apply(ActorSelection.scala:63)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:67)
	at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:82)
	at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)
	at akka.dispatch.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:59)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at akka.dispatch.BatchingExecutor$Batch.run(BatchingExecutor.scala:58)
	at akka.dispatch.ExecutionContexts$sameThreadExecutionContext$.unbatchedExecute(Future.scala:74)
	at akka.dispatch.BatchingExecutor$class.execute(BatchingExecutor.scala:110)
	at akka.dispatch.ExecutionContexts$sameThreadExecutionContext$.execute(Future.scala:73)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at akka.pattern.PromiseActorRef.$bang(AskSupport.scala:267)
	at akka.actor.EmptyLocalActorRef.specialHandle(ActorRef.scala:508)
	at akka.actor.DeadLetterActorRef.specialHandle(ActorRef.scala:541)
	at akka.actor.DeadLetterActorRef.$bang(ActorRef.scala:531)
	at akka.remote.RemoteActorRefProvider$RemoteDeadLetterActorRef.$bang(RemoteActorRefProvider.scala:87)
	at akka.remote.EndpointWriter.postStop(Endpoint.scala:561)
	at akka.actor.Actor$class.aroundPostStop(Actor.scala:475)
	at akka.remote.EndpointActor.aroundPostStop(Endpoint.scala:415)
	at akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:210)
	at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:172)
	at akka.actor.ActorCell.terminate(ActorCell.scala:369)
	at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:462)
	at akka.actor.ActorCell.systemInvoke(ActorCell.scala:478)
	at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:263)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}",,andrewor14,apachespark,jlewandowski,joshrosen,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 19 17:55:42 UTC 2015,,,,,,,,,,"0|i253vz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"03/Feb/15 07:39;jlewandowski;This test passes on my laptop (OSX). Will try to run the tests on some other machine. Is there some specification or docker environment for running tests?
;;;","03/Feb/15 13:30;jlewandowski;I tried also on ubuntu with Java 7 - the tests passes too... Now, I'm trying on Ubuntu + Java 6...;;;","03/Feb/15 17:32;jlewandowski;Java 6 test passed... except kinesis ;;;","03/Feb/15 20:26;joshrosen;Maybe this fails non-deterministically?  Or maybe there are multiple failure modes, e.g. it sometimes fails with ActorNotFound and other times fails with TimeoutException, with the choice of exception being timing-dependent?

It looks like one of the tests actually expects to intercept ActorNotFound while the other intercepts TimeoutException:

- https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/util/AkkaUtilsSuite.scala#L340
- https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/util/AkkaUtilsSuite.scala#L373

Should these tests expect either TimeoutException or ActorNotFound?, not just one of them?;;;","03/Feb/15 20:31;jlewandowski;I'll definitely check it. However I didn't have that problem before. 

Thanks [~joshrosen];;;","03/Feb/15 20:48;jlewandowski;You are absolutely right [~joshrosen]. This is due to the inconsistent behaviour of {{Await.result}} and {{resolveOne}} methods. The first one fails with {{TimeoutException}} while the second (in case of timeout) fails with {{ActorNotFoundException}}. I'll fix it right away. 
;;;","03/Feb/15 21:08;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4343;;;","16/Feb/15 17:03;joshrosen;I'm re-opening this issue since we've continued to see this flakiness on AMPLab Jenkins (even after the latest patch), and Typesafe has also run into it in their tests: https://ci.typesafe.com/view/Spark/job/spark-nightly-build/36/testReport/;;;","16/Feb/15 21:50;jlewandowski;:(;;;","19/Feb/15 17:55;andrewor14;Closing again https://github.com/apache/spark/pull/4653. Let's hope we won't have to reopen this again.;;;",,,,,,,,,,,,,,,,,,,
Improve path to Kafka assembly when trying Kafka Python API,SPARK-5546,12771923,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,tdas,tdas,tdas,03/Feb/15 03:19,26/Feb/15 21:58,14/Jul/23 06:27,26/Feb/15 21:53,1.3.0,,,,,,1.3.0,,,,,,DStreams,,,,0,,,,,,,,apachespark,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 26 04:22:50 UTC 2015,,,,,,,,,,"0|i253uv:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"26/Feb/15 04:22;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/4779;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove unused import JsonUtil from from org.apache.spark.util.JsonProtocol.scala which fails builds with older versions of hadoop-core,SPARK-5543,12771905,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nemccarthy,nemccarthy,nemccarthy,03/Feb/15 01:30,03/Feb/15 04:04,14/Jul/23 06:27,03/Feb/15 04:03,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,easyfix,,,,,"There is an unused import in org.apache.spark.util.JsonProtocol.scala 

`import org.apache.hadoop.hdfs.web.JsonUtil`

This which fails builds with older versions of hadoop-core. In particular building mapr3 causes a compile error;

[ERROR] /var/lib/jenkins/workspace/cse-Apache-Spark/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala:35: object web is not a member of package org.apache.hadoop.hdfs
[ERROR] import org.apache.hadoop.hdfs.web.JsonUtil

This import is unused. It was introduced in PR #4029 https://github.com/apache/spark/pull/4029 as a part of JIRA SPARK-5231 
",,apachespark,joshrosen,nemccarthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5231,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 04:03:35 UTC 2015,,,,,,,,,,"0|i253r3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/15 01:42;apachespark;User 'nemccarthy' has created a pull request for this issue:
https://github.com/apache/spark/pull/4320;;;","03/Feb/15 04:03;joshrosen;Issue resolved by pull request 4320
[https://github.com/apache/spark/pull/4320];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"Decouple publishing, packaging, and tagging in release script",SPARK-5542,12771891,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pwendell,pwendell,pwendell,03/Feb/15 00:41,03/Feb/15 05:00,14/Jul/23 06:27,03/Feb/15 05:00,,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,Our release script should make it easy to do these separately. I.e. it should be possible to publish a release from a tag that we already cut. This would help with things such as publishing nightly releases (SPARK-1517).,,apachespark,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 00:45:02 UTC 2015,,,,,,,,,,"0|i253nz:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"03/Feb/15 00:45;apachespark;User 'pwendell' has created a pull request for this issue:
https://github.com/apache/spark/pull/4319;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CachedTableSuite failure due to unpersisting RDDs in a non-blocking way,SPARK-5538,12771875,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,rxin,lian cheng,lian cheng,02/Feb/15 23:29,05/Feb/15 03:53,14/Jul/23 06:27,05/Feb/15 03:52,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"[PR #4173|https://github.com/apache/spark/pull/4173/files#diff-726d84ece1e6f6197b98a5868c881ac7R164] introduced this, and introduced a race condition in {{CachedTableSuite}}.",,apachespark,lian cheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 03:53:13 UTC 2015,,,,,,,,,,"0|i253k7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/15 03:53;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4379;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"EdgeRDD, VertexRDD getStorageLevel return bad values",SPARK-5534,12771857,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,josephkb,josephkb,josephkb,02/Feb/15 22:16,03/Feb/15 01:02,14/Jul/23 06:27,03/Feb/15 01:02,1.3.0,,,,,,1.3.0,,,,,,GraphX,,,,0,,,,,,"After caching a graph, its edge and vertex RDDs still return StorageLevel.None.

Reproduce error:
{code}
import org.apache.spark.graphx.{Edge, Graph}
val edges = Seq(
  Edge[Double](0, 1, 0),
  Edge[Double](1, 2, 0),
  Edge[Double](2, 3, 0),
  Edge[Double](3, 4, 0))
val g = Graph.fromEdges[Double,Double](sc.parallelize(edges), 0)
g.vertices.getStorageLevel  // returns value for StorageLevel.None
g.edges.getStorageLevel  // returns value for StorageLevel.None
g.cache()
g.vertices.count()
g.edges.count()
g.vertices.getStorageLevel  // returns value for StorageLevel.None
g.edges.getStorageLevel  // returns value for StorageLevel.None
{code}
",,apachespark,josephkb,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 01:02:47 UTC 2015,,,,,,,,,,"0|i253g7:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"02/Feb/15 22:17;josephkb;Note: This is needed for [https://github.com/apache/spark/pull/4047], which is a PR for this JIRA: [https://issues.apache.org/jira/browse/SPARK-1405];;;","02/Feb/15 23:06;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/4317;;;","03/Feb/15 01:02;mengxr;Issue resolved by pull request 4317
[https://github.com/apache/spark/pull/4317];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Repartitioning DataFrame causes saveAsParquetFile to fail with VectorUDT,SPARK-5532,12771822,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,josephkb,josephkb,02/Feb/15 20:17,31/Mar/15 14:52,14/Jul/23 06:27,24/Feb/15 18:52,1.3.0,,,,,,1.3.0,,,,,,MLlib,SQL,,,0,,,,,,"Deterministic failure:
{code}
import org.apache.spark.mllib.linalg._
import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)
import sqlContext._
val data = sc.parallelize(Seq((1.0, Vectors.dense(1,2,3)))).toDataFrame(""label"", ""features"")
data.repartition(1).saveAsParquetFile(""blah"")
{code}
If you remove the repartition, then this succeeds.

Here's the stack trace:
{code}
15/02/02 12:10:53 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 4, 192.168.1.230): java.lang.ClassCastException: org.apache.spark.mllib.linalg.DenseVector cannot be cast to org.apache.spark.sql.Row
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:186)
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:177)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:166)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:129)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:315)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:332)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:332)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

15/02/02 12:10:54 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 7, 192.168.1.230): java.lang.ClassCastException: org.apache.spark.mllib.linalg.DenseVector cannot be cast to org.apache.spark.sql.Row
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:186)
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:177)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:166)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:129)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:315)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:332)
	at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:332)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1366)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1327)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}
",,apachespark,josephkb,marmbrus,rajao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 14:52:34 UTC 2015,,,,,,,,,,"0|i2539r:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"24/Feb/15 02:15;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4738;;;","24/Feb/15 18:52;marmbrus;Issue resolved by pull request 4738
[https://github.com/apache/spark/pull/4738];;;","31/Mar/15 14:52;rajao;I get the same problem with a DataFrame created with sqlContext.createDataFrame. Is this a related issue ? For example with the following code :

object TestDataFrame {

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName(""RankingEval"").setMaster(""local[4]"")


    val sc = new SparkContext(conf)
    val sqlContext = new SQLContext(sc)

    import sqlContext.implicits._

    val data = sc.parallelize(Seq(LabeledPoint(1, Vectors.zeros(10))))
    val dataDF = data.toDF

    dataDF.printSchema()
    //dataDF.save(""test1.parquet"")

    val dataDF2 = sqlContext.createDataFrame(dataDF.rdd, dataDF.schema)

    dataDF2.printSchema()

    dataDF2.saveAsParquetFile(""test3.parquet"")
  }
};;;",,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationMaster can't kill executor when using dynamicAllocation,SPARK-5530,12771718,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,meiyoula,meiyoula,meiyoula,02/Feb/15 12:40,02/Feb/15 20:38,14/Jul/23 06:27,02/Feb/15 20:38,1.3.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"Yarn allocator logs ""Attempted to kill unknown executor 3"", and executor can't be killed.",,apachespark,meiyoula,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 02 12:44:57 UTC 2015,,,,,,,,,,"0|i252nj:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"02/Feb/15 12:44;apachespark;User 'XuTingjun' has created a pull request for this issue:
https://github.com/apache/spark/pull/4309;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlockManager heartbeat expiration does not kill executor,SPARK-5529,12771717,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shenhong,shenhong,shenhong,02/Feb/15 12:37,30/Apr/15 16:06,14/Jul/23 06:27,27/Feb/15 02:43,1.2.0,,,,,,1.3.2,1.4.0,,,,,Spark Core,YARN,,,0,,,,,,"When I run a spark job, one executor is hold, after 120s, blockManager is removed by driver, but after half an hour before the executor is remove by  driver. Here is the log:
{code}
15/02/02 14:58:43 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(1, 10.215.143.14, 47234) with no recent heart beats: 147198ms exceeds 120000ms
....
15/02/02 15:26:55 ERROR YarnClientClusterScheduler: Lost executor 1 on 10.215.143.14: remote Akka client disassociated
15/02/02 15:26:55 WARN ReliableDeliverySupervisor: Association with remote system [akka.tcp://sparkExecutor@10.215.143.14:46182] has failed, address is now gated for [5000] ms. Reason is: [Disassociated].
15/02/02 15:26:55 INFO TaskSetManager: Re-queueing tasks for 1 from TaskSet 0.0
15/02/02 15:26:55 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3, 10.215.143.14): ExecutorLostFailure (executor 1 lost)
15/02/02 15:26:55 ERROR YarnClientSchedulerBackend: Asked to remove non-existent executor 1
15/02/02 15:26:55 INFO DAGScheduler: Executor lost: 1 (epoch 0)
15/02/02 15:26:55 INFO BlockManagerMasterActor: Trying to remove executor 1 from BlockManagerMaster.
15/02/02 15:26:55 INFO BlockManagerMaster: Removed 1 successfully in removeExecutor
{code}",,apachespark,arov,jongyoul,lianhuiwang,rxin,sandyr,sb58,shenhong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Feb/15 09:09;shenhong;SPARK-5529.patch;https://issues.apache.org/jira/secure/attachment/12696416/SPARK-5529.patch",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Apr 30 16:06:14 UTC 2015,,,,,,,,,,"0|i252nb:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"02/Feb/15 12:41;shenhong;If a blockManager has not send heartBeat  more than 120s, BlockManagerMasterActor will remove it. But coarseGrainedSchedulerBackend can only remove executor after an DisassociatedEvent. Mybe the executor should remove after removing blockManager.;;;","02/Feb/15 13:00;lianhuiwang;i think when akka timeout time > blockManagerSlaveTimeoutMs, that is happened. so i think you can set blockManagerSlaveTimeoutMs > akka timeout. because if executor is removed according to timeout, blockmanager on this executor can be removed by driver. ;;;","03/Feb/15 02:15;shenhong;But in fact, I have set spark.akka.timeout  = 100;;;","03/Feb/15 04:56;lianhuiwang;if set spark.akka.timeout=100, why executor is lost after half a hour. i think we need to find the reason.;;;","04/Feb/15 01:11;shenhong;Executor will lost when a akka throw a disassociatedEvent.;;;","04/Feb/15 02:20;lianhuiwang;the phenomenon is:
blockManagerSlave is timeout  and BlockManagerMasterActor will remove this blockManager, but executor on this blockManager is not timeout because akka's heartbeat is normal.Because blockManager is in executor, if blockManager is removed, executor on this blockManager should be removed too.
Especially when dynamicAllocation is enabled, allocationManager listen onBlockManagerRemoved and remove this executor. but actually in CoarseGrainedSchedulerBackend it is still in executorDataMap.
[~rxin] [~andrewor14]  [~sandyr] when BlockManagerMasterActor remove blockmanager due to timeout of BlockManager, we need to check whether executor on this blockmanager has been removed. if its executor has not been removed, we should firstly remove this executor. how about this way to solve this problem? 
;;;","04/Feb/15 08:55;rxin;[~lianhuiwang] yes - I think they should fate share. Are you interested in working on this? ;;;","04/Feb/15 09:05;lianhuiwang;OK, I will address a PR later and then please help to review code.;;;","04/Feb/15 09:15;shenhong;I had changed it in our own branch.;;;","04/Feb/15 09:47;apachespark;User 'shenh062326' has created a pull request for this issue:
https://github.com/apache/spark/pull/4363;;;","04/Feb/15 12:36;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4367;;;","04/Feb/15 18:46;sandyr;[~shenhong] [~lianhuiwang] both of these patches look good.  I'm going to review [~shenhong]'s because it came first.;;;","05/Feb/15 01:22;lianhuiwang;OK, [~sandyr] thanks.;;;","24/Apr/15 17:26;arov;We are facing this issue on cdh5.3.2 spark 1.2.0-SNAPSHOT

Is there any workaround except upgrading to 1.4 version of spark?;;;","25/Apr/15 04:29;shenhong;1.4.0 version would be release in june.;;;","28/Apr/15 15:51;arov;CDH is usually somewhat slow on picking up the latest changes though. Would it be possible to backport this fix into 1.3?;;;","28/Apr/15 16:02;srowen;[~arov] CDH always has the latest upstream minor release in minor releases, and back-ports maintenance release fixes into maintenance releases. This is on about the same 3-4 month cycle as Spark, so it's about as fast one could expect; CDH 5.4 = 1.3.x already. This change isn't even in a Spark release yet, so yes you want it to be back-ported to 1.3, probably. That has to precede ending up in CDH though.;;;","28/Apr/15 16:02;arov;Applied patch to 1.3: https://github.com/apache/spark/pull/5745;;;","28/Apr/15 16:02;apachespark;User 'alexrovner' has created a pull request for this issue:
https://github.com/apache/spark/pull/5745;;;","28/Apr/15 16:08;arov;Sorry to quickly pulled the trigger... Need to resolve some compilation errors ;;;","28/Apr/15 17:35;apachespark;User 'alexrovner' has created a pull request for this issue:
https://github.com/apache/spark/pull/5747;;;","28/Apr/15 17:35;arov;Sorry about all the pull requests. Here is one rebased against the right branch and without any compilation issues: https://github.com/apache/spark/pull/5747;;;","29/Apr/15 22:51;apachespark;User 'alexrovner' has created a pull request for this issue:
https://github.com/apache/spark/pull/5793;;;","30/Apr/15 16:06;srowen;Backported to 1.3 in https://github.com/apache/spark/pull/5793;;;",,,,,
expression [date '2011-01-01' = cast(timestamp('2011-01-01 23:24:25') as date)] return false,SPARK-5526,12771683,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,xukun,xukun,02/Feb/15 09:41,04/Feb/15 02:13,14/Jul/23 06:27,04/Feb/15 02:13,1.2.0,,,,,,,,,,,,SQL,,,,0,,,,,,"previous work for test case，
create table date_1(d date); 
insert overwrite table date_1 select cast('2011-01-01' as date) from src tablesample (1 rows); 

In Hive，execute sql  {select date '2011-01-01' = cast(timestamp('2011-01-01 23:24:25') as date) from date_1 limit 1; } return true,but in spark SQL, return false",,apachespark,xukun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 02:13:53 UTC 2015,,,,,,,,,,"0|i252gf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"02/Feb/15 09:45;apachespark;User 'viper-kun' has created a pull request for this issue:
https://github.com/apache/spark/pull/4307;;;","04/Feb/15 02:13;xukun;the issue is fixed by #4325;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskMetrics and TaskInfo have innumerable copies of the hostname string,SPARK-5523,12771659,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,tdas,tdas,02/Feb/15 06:18,15/Jul/15 02:54,14/Jul/23 06:27,15/Jul/15 02:54,,,,,,,1.5.0,,,,,,DStreams,Spark Core,,,0,,,,,," TaskMetrics and TaskInfo objects have the hostname associated with the task. As these are created (directly or through deserialization of RPC messages), each of them have a separate String object for the hostname even though most of them have the same string data in them. This results in thousands of string objects, increasing memory requirement of the driver. 
This can be easily deduped when deserializing a TaskMetrics object, or when creating a TaskInfo object.

This affects streaming particularly bad due to the rate of job/stage/task generation. 

For solution, see how this dedup is done for StorageLevel. 
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala#L226 ",,apachespark,jerryshao,jlewandowski,lianhuiwang,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 17 07:34:53 UTC 2015,,,,,,,,,,"0|i252b3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/15 09:40;srowen;How about `String.intern()` here, as a native implementation the flyweight pattern? you pay some extra overhead of consulting the interned string pool every time, but potentially save memory.;;;","02/Feb/15 09:52;tdas;That is a little risky. In Java 6 internalized strings go to Perm gen
space, which is already oversubscribed thanks to he huge set of
dependencies of Spark - Hadoop, Hive, etc etc etc. It might be better to do
a very specific solution that we can bound the performance penalty of
(bounded HashMap for hostname cache).

;;;","09/Mar/15 23:14;tdas;[~jerryshao] You can take a crack at this if you want. 

;;;","10/Mar/15 01:40;jerryshao;Hi [~tdas], I will take a look at this issue and try to find a solution, looks like a basic solution is to use HashMap to cache the data like {{StorageLevel}}.;;;","16/Mar/15 03:04;jerryshao;Hi [~tdas], will this large number of string brings heavy overhead to GC or something others? Assuming these objects are out-of-date very soon, they will be in the young gen and GCed fastly, I'm not sure how large overhead will this bring in?;;;","16/Mar/15 07:33;jerryshao;After investigating a little on the implementation details of host in {{TaskMetrics}} and {{TaskInfo}}. 

I think for {{TaskInfo}}, Spark already uses {{hostPortParseResults}} in Utils.scala to cache the host name, so we don't need to do this again. Also {{TaskInfo}} will only reside in driver, so RPC related object recreation is not existed.

For {{TaskMetrics}}, the problem lies in deserialization, mainly we could rewrite {{readObject()}} function to use a cached host name like this way:

{code}
  @throws(classOf[IOException])
  private def readObject(in: ObjectInputStream): Unit = Utils.tryOrIOException {
    in.defaultReadObject()
    _hostname = getHostFromPool(_hostname)
  }
{code}

The question is that we still create a {{_hostname}} String object, though very short-life. I'm not sure is there a way we could reuse the cached hostname without even creating a very short-life one. Any suggestions? Thanks a lot.;;;","17/Mar/15 00:23;tdas;As long as the hostname object is short-lived its cool. That's the same
strategy used for StorageLevel. So it is fine.

On Mon, Mar 16, 2015 at 12:36 AM, Saisai Shao (JIRA) <jira@apache.org>

;;;","17/Mar/15 07:34;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/5064;;;",,,,,,,,,,,,,,,,,,,,,
Build fails with spark-ganglia-lgpl profile,SPARK-5515,12771640,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,sarutak,sarutak,02/Feb/15 01:39,02/Feb/15 05:43,14/Jul/23 06:27,02/Feb/15 05:43,1.3.0,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,Build fails with spark-ganglia-lgpl profile at the moment. This is because pom.xml for spark-ganglia-lgpl is not updated.,,andrewor14,apachespark,pwendell,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 02 05:43:30 UTC 2015,,,,,,,,,,"0|i2526v:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"02/Feb/15 01:56;andrewor14;https://github.com/apache/spark/pull/4303;;;","02/Feb/15 01:57;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/4303;;;","02/Feb/15 05:41;pwendell;[~andrewor14] is this fixed now?;;;","02/Feb/15 05:43;andrewor14;yes I just closed this;;;",,,,,,,,,,,,,,,,,,,,,,,,,
EqualTo operator doesn't handle binary type properly,SPARK-5509,12771565,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lian cheng,lian cheng,lian cheng,31/Jan/15 22:46,12/Feb/15 04:43,14/Jul/23 06:27,05/Feb/15 23:30,1.1.0,1.1.1,1.2.0,1.2.1,1.3.0,,1.3.0,,,,,,SQL,,,,0,,,,,,"Binary type is mapped to {{Array\[Byte\]}}, which can't be compared with {{==}} directly. However, {{EqualTo.eval()}} uses plain {{==}} to compare values. Run the following {{spark-shell}} snippet with Spark 1.2.0 to reproduce this issue: 
{code}
import org.apache.spark.sql.SQLContext
import sc._

val sqlContext = new SQLContext(sc)
import sqlContext._

case class KV(key: Int, value: Array[Byte])

def toBinary(s: String): Array[Byte] = s.toString.getBytes(""UTF-8"")
registerFunction(""toBinary"", toBinary _)

parallelize(1 to 1024).map(i => KV(i, toBinary(i.toString))).registerTempTable(""bin"")

// OK
sql(""select * from bin where value < toBinary('100')"").collect()

// Oops, returns nothing
sql(""select * from bin where value = toBinary('100')"").collect()
{code}",,apachespark,lian cheng,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 05 23:30:49 UTC 2015,,,,,,,,,,"0|i251qf:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"02/Feb/15 19:06;apachespark;User 'liancheng' has created a pull request for this issue:
https://github.com/apache/spark/pull/4308;;;","05/Feb/15 23:30;marmbrus;Issue resolved by pull request 4308
[https://github.com/apache/spark/pull/4308];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
ScalaReflection.convertToCatalyst should support nested arrays,SPARK-5504,12771437,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,josephkb,josephkb,josephkb,30/Jan/15 21:50,30/Jan/15 23:40,14/Jul/23 06:27,30/Jan/15 23:40,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"After the recent refactoring, convertToCatalyst in ScalaReflection does not recurse on Arrays.  It should.",,apachespark,josephkb,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 30 23:40:40 UTC 2015,,,,,,,,,,"0|i250z3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"30/Jan/15 22:17;apachespark;User 'jkbradley' has created a pull request for this issue:
https://github.com/apache/spark/pull/4295;;;","30/Jan/15 23:40;marmbrus;Issue resolved by pull request 4295
[https://github.com/apache/spark/pull/4295];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"[SPARK-SQL]when the partition schema does not match table schema,it throws java.lang.ClassCastException and so on",SPARK-5498,12771323,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jeanlyn,jeanlyn,jeanlyn,30/Jan/15 12:26,17/Mar/18 01:04,14/Jul/23 06:27,17/Mar/18 01:04,1.2.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"when the partition schema does not match table schema,it will thows exception when the task is running.For example,we modify the type of column from int to bigint by the sql *ALTER TABLE table_with_partition CHANGE COLUMN key key BIGINT* ,then we query the patition data which was stored before the changing,we would get the exception:
{noformat}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 4 times, most recent failure: Lost task 0.3 in stage 27.0 (TID 30, BJHC-HADOOP-HERA-16950.jeanlyn.local): java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.MutableLong cannot be cast to org.apache.spark.sql.catalyst.expressions.MutableInt
        at org.apache.spark.sql.catalyst.expressions.SpecificMutableRow.setInt(SpecificMutableRow.scala:241)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$13$$anonfun$apply$4.apply(TableReader.scala:286)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$13$$anonfun$apply$4.apply(TableReader.scala:286)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$1.apply(TableReader.scala:322)
        at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$1.apply(TableReader.scala:314)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:312)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.execution.Limit$$anonfun$4.apply(basicOperators.scala:141)
        at org.apache.spark.sql.execution.Limit$$anonfun$4.apply(basicOperators.scala:141)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
        at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1314)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
        at akka.dispatch.Mailbox.run(Mailbox.scala:220)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{noformat}
we can reproduce the bug as follow:
add the code to the unit test *sql/hive/src/test/scala/org/apache/spark/sql/hive/InsertIntoHiveTableSuite.scala*
{noformat}
test(""partition schema does not match table schema""){
    val testData = TestHive.sparkContext.parallelize(
      (1 to 10).map(i => TestData(i, i.toString)))

    testData.registerTempTable(""testData"")
    val tmpDir = Files.createTempDir()

    sql(s""CREATE TABLE table_with_partition(key int,value string) PARTITIONED by (ds string) location '${tmpDir.toURI.toString}' "")

    sql(""INSERT OVERWRITE TABLE table_with_partition  partition (ds='1') SELECT key,value FROM testData"")

    sql(""ALTER TABLE table_with_partition CHANGE COLUMN key key BIGINT"")
    checkAnswer(sql(""select key,value from table_with_partition where ds='1' ""),
      testData.toSchemaRDD.collect.toSeq
    )

    sql(""DROP TABLE table_with_partition"")
    
  }
{noformat}
run the test 
{noformat}
mvn -Dhadoop.version=... - DwildcardSuites=org.apache.spark.sql.hive.InsertIntoHiveTableSuite test
{noformat}",,apachespark,dongjoon,jeanlyn,liutang123,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6644,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Mar 17 01:03:37 UTC 2018,,,,,,,,,,"0|i25093:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/15 14:12;apachespark;User 'jeanlyn' has created a pull request for this issue:
https://github.com/apache/spark/pull/4289;;;","26/Mar/15 00:48;marmbrus;Issue resolved by pull request 4289
[https://github.com/apache/spark/pull/4289];;;","16/Mar/18 10:00;liutang123;In our cluster, we use hive 1.2, spark 2.2, hadoop 2.7. When we read hive table use spark, we will get the error below:

 
{code:java}
ERROR SparkSQLDriver: Failed in [select * from test_par1 where b='2']
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 3, localhost, executor driver): java.lang.ClassCastException: org.apache.hadoop.io.Text cannot be cast to org.apache.hadoop.io.LongWritable
at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableLongObjectInspector.get(WritableLongObjectInspector.java:36)
at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$13$$anonfun$apply$6.apply(TableReader.scala:398)
at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$13$$anonfun$apply$6.apply(TableReader.scala:398)
at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:439)
at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:430)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:235)
at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:228)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
at org.apache.spark.scheduler.Task.run(Task.scala:108)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
{code}
repreduce this issue:
{code:java}
create table test_par(a string)
PARTITIONED BY (`b` bigint)
ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'
STORED AS
INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';
ALTER TABLE test_par CHANGE a a bigint restrict;  -- in hive
select * from test_par;
{code}
 ;;;","16/Mar/18 10:29;apachespark;User 'liutang123' has created a pull request for this issue:
https://github.com/apache/spark/pull/20846;;;","17/Mar/18 01:01;dongjoon;[~liutang123]. Spark should not do this kind of risky thing. Hive 2.3.2 also disallows incompatible schema changes like the following. 

{code}
hive> CREATE TABLE test_par(a string) PARTITIONED BY (b bigint) ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat';
OK
Time taken: 0.262 seconds

hive> ALTER TABLE test_par CHANGE a a bigint RESTRICT;
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. The following columns have types incompatible with the existing columns in their respective positions :
a

hive> SELECT VERSION();
OK
2.3.2 r857a9fd8ad725a53bd95c1b2d6612f9b1155f44d
Time taken: 0.711 seconds, Fetched: 1 row(s)
{code};;;","17/Mar/18 01:03;dongjoon;And, SPARK-5498 has resolved 3 years ago. Please open a new issue if you need.;;;",,,,,,,,,,,,,,,,,,,,,,,
"Allow both ""classification"" and ""Classification"" in Algo for trees",SPARK-5496,12771248,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mengxr,mengxr,mengxr,30/Jan/15 08:41,30/Jan/15 18:08,14/Jul/23 06:27,30/Jan/15 18:08,,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"We use ""classification"" in tree but ""Classification"" in boosting. We switched to ""classification"" in both cases, but still need to accept ""Classification"" to be backward compatible.",,apachespark,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 30 18:08:22 UTC 2015,,,,,,,,,,"0|i24ztb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"30/Jan/15 08:55;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4287;;;","30/Jan/15 18:08;mengxr;Issue resolved by pull request 4287
[https://github.com/apache/spark/pull/4287];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSqlSerializer Ignores KryoRegistrators,SPARK-5494,12771233,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,hkothari,hkothari,30/Jan/15 07:00,22/May/15 00:11,14/Jul/23 06:27,22/May/15 00:11,1.2.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,We should make SparkSqlSerializer call {{super.newKryo}} before doing any of it's custom stuff in order to make sure it picks up on custom KryoRegistrators.,,aash,apachespark,hkothari,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6465,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 19 15:51:40 UTC 2015,,,,,,,,,,"0|i24zpz:",9223372036854775807,,,,,marmbrus,,,,,,,,,1.4.0,,,,,,,,,,,,,"19/Feb/15 15:51;apachespark;User 'hkothari' has created a pull request for this issue:
https://github.com/apache/spark/pull/4693;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread statistics can break with older Hadoop versions,SPARK-5492,12771201,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sandyr,pwendell,pwendell,30/Jan/15 01:52,02/Feb/15 08:54,14/Jul/23 06:27,02/Feb/15 08:54,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"{code}
 java.lang.ClassNotFoundException: org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:191)
at org.apache.spark.deploy.SparkHadoopUtil.getFileSystemThreadStatisticsMethod(SparkHadoopUtil.scala:180)
at org.apache.spark.deploy.SparkHadoopUtil.getFSBytesReadOnThreadCallback(SparkHadoopUtil.scala:139)
at org.apache.spark.rdd.NewHadoopRDD$$anon$1$$anonfun$2.apply(NewHadoopRDD.scala:120)
at org.apache.spark.rdd.NewHadoopRDD$$anon$1$$anonfun$2.apply(NewHadoopRDD.scala:118)
at scala.Option.orElse(Option.scala:257)
{code}

I think the issue is we need to catch ClassNotFoundException here:
https://github.com/apache/spark/blob/b1b35ca2e440df40b253bf967bb93705d355c1c0/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala#L144

However, I'm really confused how this didn't fail our unit tests, since we explicitly tried to test this.",,apachespark,marmbrus,pwendell,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5199,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 02 08:54:16 UTC 2015,,,,,,,,,,"0|i24ziv:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"30/Jan/15 01:53;pwendell;/cc [~sandyr];;;","30/Jan/15 02:54;sandyr;Very weird.  I'll look into it.  Did that come up during a test?;;;","30/Jan/15 04:29;sandyr;Are you able to provide any more detail on the environment this occurred in?

I think all versions of Hadoop that don't expose StatisticsData are also missing a getThreadStatistics method, so they should run into a NoSuchMethodException at https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala#L160 and not make it down to the ClassNotFoundException.

It's probably good to guard against the ClassNotFoundException anyway, but not sure how this would come up.;;;","02/Feb/15 01:16;marmbrus;We hit this bug using the default hadoop version for the spark-ec2 scripts (1.0.4).;;;","02/Feb/15 01:44;sandyr;After seeing this I tried with 1.0.4 and didn't hit anything. I guess the ec2 setup is different in some way - I'll post a patch tonight.;;;","02/Feb/15 05:52;pwendell;[~sandyr] I share your confusion, Sandy. Nonetheless, catching ClassNotFoundException does seem reasonable. If anything it's just a better hedge against random Hadoop versions that might have intermediate sets of functionality. What do you think?;;;","02/Feb/15 07:30;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/4305;;;","02/Feb/15 08:54;pwendell;Thanks, Sandy.;;;",,,,,,,,,,,,,,,,,,,,,
Pregel should checkpoint periodically to avoid StackOverflowError,SPARK-5484,12771092,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dingding,ankurd,ankurd,29/Jan/15 19:22,31/Oct/17 04:49,14/Jul/23 06:27,25/Apr/17 18:23,,,,,,,2.2.0,2.3.0,,,,,GraphX,,,,4,,,,,,"Pregel-based iterative algorithms with more than ~50 iterations begin to slow down and eventually fail with a StackOverflowError due to Spark's lack of support for long lineage chains. Instead, Pregel should checkpoint the graph periodically.",,adeandrade,ankurd,apachespark,ding,lamerman,maropu,michael,praetp,prateekrungta,robineast,rohit13k,shreyagarwal@outlook.com,wesolows,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5561,,SPARK-15739,SPARK-12431,,,,,SPARK-15042,SPARK-10335,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 31 04:49:04 UTC 2017,,,,,,,,,,"0|i24yvb:",9223372036854775807,,,,,,,,,,,,,,2.2.0,2.3.0,,,,,,,,,,,,"29/Jan/15 19:27;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/4273;;;","02/Jun/16 03:30;adeandrade;How is this not a priority? If we are not fixing Pregel then let's expose {{aggregateMessagesWithActiveSet}} to users so they can create their own version.;;;","16/Jul/16 09:32;wesolows;[~ankurd] do you plan to prepare another solution? I could see your pull request was closed, but did solve problem and was best thing at a time. The only thing I could see lacking was no ckeckpoint directory cleaning, and I guess I would change checkpoint iterations to 35 since it worked better during my tests.
More general solution proposed within SPARK-5561 needs a change within PeriodicRDDCheckpointer and PeriodicGraphCheckpointer - either to move it from mllib or change access modifier. ;;;","16/Sep/16 02:11;ding;I will work on the issue if nobody took it.;;;","16/Sep/16 10:37;maropu;This component seems very inactive, so I think there is a little chance to review your code if you take on this.;;;","16/Sep/16 17:18;ding;Thank you for your kindly reminder. However as the code is almost ready, I will still send PR in case someone has interest to review it.;;;","17/Sep/16 02:52;apachespark;User 'dding3' has created a pull request for this issue:
https://github.com/apache/spark/pull/15125;;;","03/Oct/16 17:38;shreyagarwal@outlook.com;Hi,

I am running a Spark 2.0 cluster and want to check if there is a way I can deploy this fix onto that. Also, it is kind of urgent :)

Regards,
Shreya;;;","18/Jan/17 21:03;michael;Hi Guys,

@ding has rebased his PR, and it LGTM. Can a committer review it please? It's quite a helpful patch.;;;","31/Oct/17 04:49;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/19618;;;",,,,,,,,,,,,,,,,,,,
PySpark on yarn mode need to support non-local python files,SPARK-5479,12770959,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vanzin,lianhuiwang,lianhuiwang,29/Jan/15 09:53,03/Aug/15 20:24,14/Jul/23 06:27,10/Jun/15 20:28,1.4.0,,,,,,1.5.0,,,,,,PySpark,YARN,,,1,,,,,," In SPARK-5162 [~vgrigor] reports this:
Now following code cannot work:
aws emr add-steps --cluster-id ""j-XYWIXMD234"" \
--steps Name=SparkPi,Jar=s3://eu-west-1.elasticmapreduce/libs/script-runner/script-runner.jar,Args=[/home/hadoop/spark/bin/spark-submit,--deploy-mode,cluster,--master,yarn-cluster,--py-files,s3://mybucketat.amazonaws.com/tasks/main.py,main.py,param1],ActionOnFailure=CONTINUE

so we need to support non-local python files on yarn client and cluster mode.
before submitting application to Yarn, we need to download non-local files to local or hdfs path.
or spark.yarn.dist.files need to support other non-local files.",,apachespark,diederik,lianhuiwang,qwertymaniac,tgraves,vgrigor,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-8017,SPARK-7725,,,,,,,,SPARK-5162,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 22 18:45:04 UTC 2015,,,,,,,,,,"0|i24y1z:",9223372036854775807,,,,,,,,,,,,,,1.5.0,,,,,,,,,,,,,"30/Jan/15 09:14;vgrigor;https://github.com/apache/spark/pull/3976 potentially closes this issue;;;","24/Mar/15 13:46;tgraves;Was this fixed by https://github.com/apache/spark/pull/3976 ?;;;","22/May/15 18:45;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/6360;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Add miss right parenthesis in Stage page Pending stages label,SPARK-5478,12770958,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jerryshao,jerryshao,jerryshao,29/Jan/15 09:47,02/Feb/15 07:56,14/Jul/23 06:27,02/Feb/15 07:56,1.3.0,,,,,,1.3.0,,,,,,,,,,0,,,,,,"right parenthesis is missing in one label, minor problem in UI.",,apachespark,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 29 09:53:53 UTC 2015,,,,,,,,,,"0|i24y1r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/15 09:53;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/4267;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
use defaultClassLoader of Serializer to load classes of classesToRegister in KryoSerializer,SPARK-5470,12770884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lianhuiwang,lianhuiwang,lianhuiwang,29/Jan/15 01:59,06/Feb/15 18:50,14/Jul/23 06:27,06/Feb/15 11:00,,,,,,,1.3.0,1.4.0,,,,,Spark Core,,,,0,,,,,,"Now KryoSerializer load classes of classesToRegister at the time of its initialization. when we set spark.kryo.classesToRegister=class1, it will throw  SparkException(""Failed to load class to register with Kryo"".
because in KryoSerializer's initialization, classLoader cannot include class of user's jars.
we need to use defaultClassLoader of Serializer in newKryo(), because executor will reset defaultClassLoader of Serializer after Serializer's initialization.",,apachespark,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 11:00:51 UTC 2015,,,,,,,,,,"0|i24xlr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/15 02:30;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4258;;;","06/Feb/15 11:00;srowen;Issue resolved by pull request 4258
[https://github.com/apache/spark/pull/4258];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Build Error caused by Guava shading in Spark,SPARK-5466,12770845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,vanzin,janzhou,janzhou,28/Jan/15 23:06,29/Jan/15 21:01,14/Jul/23 06:27,29/Jan/15 21:01,1.3.0,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,"Guava is shaded inside spark-core itself.

https://github.com/apache/spark/commit/37a5e272f898e946c09c2e7de5d1bda6f27a8f39

This causes build error in multiple components, including Graph/MLLib/SQL, when package com.google.common on the classpath incompatible with the version used when compiling Utils.class

[error] bad symbolic reference. A signature in Utils.class refers to term util
[error] in package com.google.common which is not available.
[error] It may be completely missing from the current classpath, or the version on
[error] the classpath might be incompatible with the version used when compiling Utils.class.
[error] 
[error]      while compiling: /spark/graphx/src/main/scala/org/apache/spark/graphx/util/BytecodeUtils.scala
[error]         during phase: erasure
[error]      library version: version 2.10.4
[error]     compiler version: version 2.10.4
",,apachespark,Bilna,janzhou,pwendell,vanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4809,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 29 21:01:20 UTC 2015,,,,,,,,,,"0|i24xd3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"28/Jan/15 23:18;srowen;I see this too from a completely clean build.;;;","29/Jan/15 07:01;pwendell;I sent [~vanzin] and e-mail today about this. Guess I'm not the only one seeing it. I was using zinc on OSX... are you guys using that too? I set up a zinc maven build on Jenkins and it worked just fine.;;;","29/Jan/15 07:02;pwendell;Also - [~srowen] can you reproduce this if you do not use Zinc?;;;","29/Jan/15 08:31;pwendell;Okay Maven is reproducing this now even without zinc: https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/1393/hadoop.version=1.0.4,label=centos/console

It appears that it might only be happening with certain parameterizations of the build:

{code}
[ERROR] bad symbolic reference. A signature in Utils.class refers to term util
in package com.google.common which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling Utils.class.
[ERROR] 
     while compiling: /home/jenkins/workspace/Spark-Master-Maven-pre-YARN/hadoop.version/1.0.4/label/centos/graphx/src/main/scala/org/apache/spark/graphx/util/BytecodeUtils.scala
{code};;;","29/Jan/15 09:22;srowen;I saw the same error without zinc. I was doing a plain build with {{mvn -DskipTests clean package}}. I tried {{mvn -DskipTests clean install}} for kicks and got the same problem.;;;","29/Jan/15 10:32;Bilna;I too had the same exception while doing build with {noformat} mvn -DskipTests clean package {noformat};;;","29/Jan/15 17:36;vanzin;Let me try that and see if I figure what's going on.;;;","29/Jan/15 17:42;vanzin;Ok, I can reproduce it locally with the default parameters. I have a pretty good idea of what's going on, so stay tuned for a patch.;;;","29/Jan/15 18:20;apachespark;User 'vanzin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4272;;;","29/Jan/15 21:01;pwendell;Thanks [~vanzin] for quickly fixing this!;;;",,,,,,,,,,,,,,,,,,,
"Calling help() on a Python DataFrame fails with ""cannot resolve column name __name__"" error",SPARK-5464,12770804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,28/Jan/15 21:35,30/Jan/15 00:23,14/Jul/23 06:27,30/Jan/15 00:23,1.3.0,,,,,,1.3.0,,,,,,PySpark,SQL,,,0,,,,,,"Trying to call {{help()}} on a Python DataFrame fails with an exception:

{code}
>>> help(df)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/joshrosen/anaconda/lib/python2.7/site.py"", line 464, in __call__
    return pydoc.help(*args, **kwds)
  File ""/Users/joshrosen/anaconda/lib/python2.7/pydoc.py"", line 1787, in __call__
    self.help(request)
  File ""/Users/joshrosen/anaconda/lib/python2.7/pydoc.py"", line 1834, in help
    else: doc(request, 'Help on %s:')
  File ""/Users/joshrosen/anaconda/lib/python2.7/pydoc.py"", line 1571, in doc
    pager(render_doc(thing, title, forceload))
  File ""/Users/joshrosen/anaconda/lib/python2.7/pydoc.py"", line 1545, in render_doc
    object, name = resolve(thing, forceload)
  File ""/Users/joshrosen/anaconda/lib/python2.7/pydoc.py"", line 1540, in resolve
    name = getattr(thing, '__name__', None)
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/sql.py"", line 2154, in __getattr__
    return Column(self._jdf.apply(name))
  File ""/Users/joshrosen/Documents/Spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/Users/joshrosen/Documents/Spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o31.apply.
: java.lang.RuntimeException: Cannot resolve column name ""__name__""
	at org.apache.spark.sql.DataFrame$$anonfun$resolve$1.apply(DataFrame.scala:123)
	at org.apache.spark.sql.DataFrame$$anonfun$resolve$1.apply(DataFrame.scala:123)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.DataFrame.resolve(DataFrame.scala:122)
	at org.apache.spark.sql.DataFrame.apply(DataFrame.scala:237)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
{code}

Here's a reproduction:

{code}
>>> from pyspark.sql import SQLContext, Row
>>> sqlContext = SQLContext(sc)
>>> rdd = sc.parallelize(['{""foo"":""bar""}', '{""foo"":""baz""}'])
>>> df = sqlContext.jsonRDD(rdd)
>>> help(df)
{code}

I think the problem here is that we don't throw the expected exception from our overloaded {{getattr}} if a column can't be found.

We should be able to fix this by only attempting to call {{apply}} after checking that the column name is valid (e.g. check against {{columns}}).",,apachespark,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 29 22:49:13 UTC 2015,,,,,,,,,,"0|i24x5b:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"29/Jan/15 22:49;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4278;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Catalyst UnresolvedException ""Invalid call to qualifiers on unresolved object"" error when accessing fields in DataFrames returned from sqlCtx.sql()",SPARK-5462,12770802,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,28/Jan/15 21:31,30/Jan/15 02:23,14/Jul/23 06:27,30/Jan/15 02:23,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"When trying to access fields on a Python DataFrame created via inferSchema, I ran into a confusing Catalyst Py4J error.  Here's a reproduction:

{code}
from pyspark import SparkContext
from pyspark.sql import SQLContext, Row

sc = SparkContext(""local"", ""test"")
sqlContext = SQLContext(sc)

# Load a text file and convert each line to a Row.
lines = sc.textFile(""examples/src/main/resources/people.txt"")
parts = lines.map(lambda l: l.split("",""))
people = parts.map(lambda p: Row(name=p[0], age=int(p[1])))

# Infer the schema, and register the SchemaRDD as a table.
schemaPeople = sqlContext.inferSchema(people)
schemaPeople.registerTempTable(""people"")

# SQL can be run over SchemaRDDs that have been registered as a table.
teenagers = sqlContext.sql(""SELECT name FROM people WHERE age >= 13 AND age <= 19"")

print teenagers.name
{code}

This fails with the following error:

{code}
Traceback (most recent call last):
  File ""/Users/joshrosen/Documents/spark/sqltest.py"", line 19, in <module>
    print teenagers.name
  File ""/Users/joshrosen/Documents/Spark/python/pyspark/sql.py"", line 2154, in __getattr__
    return Column(self._jdf.apply(name))
  File ""/Users/joshrosen/Documents/Spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py"", line 538, in __call__
  File ""/Users/joshrosen/Documents/Spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py"", line 300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o66.apply.
: org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to qualifiers on unresolved object, tree: 'name
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.qualifiers(unresolved.scala:50)
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.qualifiers(unresolved.scala:46)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:143)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:140)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:140)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:126)
	at org.apache.spark.sql.DataFrame.resolve(DataFrame.scala:122)
	at org.apache.spark.sql.DataFrame.apply(DataFrame.scala:237)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
{code}

This is distinct from the helpful error message that I get when trying to access a non-existent column.  This error didn't occur when I tried the same thing with a DataFrame created via jsonRDD.",,apachespark,joshrosen,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 30 00:30:12 UTC 2015,,,,,,,,,,"0|i24x4v:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"29/Jan/15 23:43;joshrosen;I'm working on a patch for this now.  It looks like the problem crops up when trying to select columns from DataFrames that are returned by SQL queries, as opposed to ones created by applying or inferring a schema.  Here's a regression test demonstrating this:

{code}

    def test_column_selection_on_dataframes_created_by_queries(self):
        # Regression test for SPARK-5462
        df = self.df
        df.registerTempTable(""test"")
        df_from_query = self.sqlCtx.sql(""select key, values from test"")
        df_from_query.key  # Throws exception
        df_from_query.value
{code};;;","30/Jan/15 00:03;joshrosen;Actually, this issue isn't Python-specific: it also occurs when running the ""people / teenagers"" example from the SQL Programming Guide in the regular Spark Shell:

{code}
scala> teenagers(""name"")
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to qualifiers on unresolved object, tree: 'name
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.qualifiers(unresolved.scala:50)
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.qualifiers(unresolved.scala:46)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:143)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$2.apply(LogicalPlan.scala:140)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:140)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:126)
	at org.apache.spark.sql.DataFrame.resolve(DataFrame.scala:120)
	at org.apache.spark.sql.DataFrame.apply(DataFrame.scala:258)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:20)
	at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:27)
	at $iwC$$iwC$$iwC.<init>(<console>:29)
	at $iwC$$iwC.<init>(<console>:31)
	at $iwC.<init>(<console>:33)
	at <init>(<console>:35)
	at .<init>(<console>:39)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:854)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:899)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:811)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:654)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:662)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:667)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:994)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:942)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:942)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:942)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1039)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:366)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code};;;","30/Jan/15 00:03;joshrosen;[~liancheng] [~marmbrus] Is this possibly related to SPARK-2063?;;;","30/Jan/15 00:30;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4282;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Refer to aggregateByKey instead of combineByKey in docs,SPARK-5458,12770734,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,sandyr,sandyr,sandyr,28/Jan/15 17:16,28/Jan/15 20:42,14/Jul/23 06:27,28/Jan/15 20:42,,,,,,,1.3.0,,,,,,Documentation,,,,0,,,,,,,,apachespark,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 28 17:20:06 UTC 2015,,,,,,,,,,"0|i24wpr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/15 17:20;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/4251;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add missing DSL for ApproxCountDistinct.,SPARK-5457,12770721,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,28/Jan/15 16:33,30/Jan/15 09:22,14/Jul/23 06:27,30/Jan/15 09:22,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,,,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 28 16:36:29 UTC 2015,,,,,,,,,,"0|i24wmv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/15 16:36;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4250;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Decimal Type comparison issue,SPARK-5456,12770699,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,adrian-wang,kul,kul,28/Jan/15 15:55,18/Aug/16 12:27,14/Jul/23 06:27,06/May/15 17:05,1.2.0,1.3.0,,,,,1.3.2,1.4.0,,,,,SQL,,,,1,,,,,,"Not quite able to figure this out but here is a junit test to reproduce this, in JavaAPISuite.java

{code:title=DecimalBug.java}
  @Test
  public void decimalQueryTest() {
    List<Row> decimalTable = new ArrayList<Row>();
    decimalTable.add(RowFactory.create(new BigDecimal(""1""), new BigDecimal(""2"")));
    decimalTable.add(RowFactory.create(new BigDecimal(""3""), new BigDecimal(""4"")));
    JavaRDD<Row> rows = sc.parallelize(decimalTable);
    List<StructField> fields = new ArrayList<StructField>(7);
    fields.add(DataTypes.createStructField(""a"", DataTypes.createDecimalType(), true));
    fields.add(DataTypes.createStructField(""b"", DataTypes.createDecimalType(), true));
    sqlContext.applySchema(rows.rdd(), DataTypes.createStructType(fields)).registerTempTable(""foo"");
    Assert.assertEquals(sqlContext.sql(""select * from foo where a > 0"").collectAsList(), decimalTable);

  }
{code}

Fails with
java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal",,AbdealiJK,adrian-wang,apachespark,blbradley,hotou,karthikg01,kul,rxin,yhuai,yuzhihong@gmail.com,zhangyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6784,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Aug 18 12:27:30 UTC 2016,,,,,,,,,,"0|i24whz:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"27/Mar/15 06:15;karthikg01;This is a blocker when using Spark with Databases which have Decimal / Big Decimal columns. Is there a workaround?;;;","30/Mar/15 12:31;karthikg01;One workaround we followed is to convert all Decimal Datatype columns to integer (truncated).
But, this is not possible when the application needs to connect to third-party databases were the datatype is obviously not under our control. 

So, this is a serious bug, IMHO, which prohibits using Spark in cases, unless there is a workaround?;;;","31/Mar/15 03:29;kul;[~karthikg01]

1) Switch to hive context, I am not trying to deride the plain sql context, but the hive context is just better tested and has a well defined syntax borrowed from hive.
2) Even in hive context i have faced problems with bigdecimals, so like your workaround i also convert bigdecimals to a double (not int). And for all practical purposes it is more than enough. I have not seem many datasources with those types. rdbms maps `NUMERIC` type to bigdecimal in jdbc but you can always workaround this by have a simple map transformation before you register it to sql context.

2 cents.;;;","28/Apr/15 02:12;zhangyi;I got same issue. When I access Postgresql based on Spark SQL, and the code is as below:
dataFrame.registerTempTable(""Employees"")
val emps = sqlContext.sql(""select name, sum(salary) from Employees group by name"")
monitor {
emps.take(10)
.map(row => (row.getString(0), row.getDecimal(1)))
.foreach(println)
}
The type of salary column in data table of Postgresql is numeric(10, 2).

Running it, then it throws the following exception:
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal;;;","30/Apr/15 05:59;adrian-wang;Hi, I just tried the code in the description of this issue with latest master, and got not errors.
Can you guys confirm that?;;;","30/Apr/15 08:08;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/5803;;;","30/Apr/15 08:16;adrian-wang;I created a pull request for [~zhangyi];;;","07/May/15 17:22;yhuai;Seems it is not in 1.2 branch. So I am removing the fix version ""1.2.3"". ;;;","25/Aug/15 18:02;blbradley;I'm still experiencing this in 1.4.0 and 1.4.1. I believe the fix for it should be in 1.4.1.;;;","18/Aug/16 12:27;AbdealiJK;For the record, I get this error in 1.4.1 too.;;;",,,,,,,,,,,,,,,,,,,
[SQL] Self join with ArrayType columns problems,SPARK-5454,12770645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,pborck,pborck,28/Jan/15 10:42,12/Feb/15 04:37,14/Jul/23 06:27,11/Feb/15 20:33,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,3,,,,,,"Weird behaviour when performing self join on a table with some ArrayType field.  (potential bug ?) 

I have set up a minimal non working example here: 
https://gist.github.com/pierre-borckmans/4853cd6d0b2f2388bf4f

In a nutshell, if the ArrayType column used for the pivot is created manually in the StructType definition, everything works as expected. 
However, if the ArrayType pivot column is obtained by a sql query (be it by using a ""array"" wrapper, or using a collect_list operator for instance), then results are completely off. 
",,apachespark,marmbrus,pborck,svend,ThorGutierrez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 11 20:33:31 UTC 2015,,,,,,,,,,"0|i24w5z:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"30/Jan/15 01:33;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4284;;;","11/Feb/15 03:32;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4520;;;","11/Feb/15 20:33;marmbrus;Issue resolved by pull request 4520
[https://github.com/apache/spark/pull/4520];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
'spark.blockManager.port' conflict in netty service,SPARK-5444,12770599,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,carlmartin,carlmartin,carlmartin,28/Jan/15 06:48,17/May/20 18:20,14/Jul/23 06:27,06/Feb/15 22:37,1.2.0,,,,,,1.3.0,,,,,,Block Manager,Spark Core,,,0,,,,,,"If set the 'spark.blockManager.port` = 4040 in spark-default.conf, it will throw the conflict port exception and exit directly.",,apachespark,carlmartin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 28 07:48:10 UTC 2015,,,,,,,,,,"0|i24vvr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"28/Jan/15 07:48;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/4240;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SerDeUtil Pair RDD to python conversion doesn't accept empty RDDs,SPARK-5441,12770534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mnazario,mnazario,mnazario,28/Jan/15 00:07,17/Feb/15 00:40,14/Jul/23 06:27,17/Feb/15 00:40,1.1.1,1.2.0,,,,,1.2.2,1.3.0,,,,,PySpark,,,,0,backport-needed,,,,,SerDeUtil.pairRDDToPython and SerDeUtil.pythonToPairRDD rely on rdd.first() which throws an exception if the RDD is empty. We should be able to handle the empty RDD case because this doesn't prevent a valid RDD from being created.,,apachespark,joshrosen,mnazario,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 00:40:45 UTC 2015,,,,,,,,,,"0|i24vhj:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.2,1.3.0,,,,,,,,,,,"28/Jan/15 00:20;apachespark;User 'mnazario' has created a pull request for this issue:
https://github.com/apache/spark/pull/4236;;;","28/Jan/15 22:04;joshrosen;I've merged https://github.com/apache/spark/pull/4236, which fixes this, in to 1.3.0 and I'll come back later to backport this to 1.2.2 and 1.1.2.;;;","17/Feb/15 00:40;joshrosen;I've cherry-picked this to `branch-1.2` (1.2.2), so I'm going to mark this as fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
DriverSuite and SparkSubmitSuite incorrect timeout behavior,SPARK-5437,12770492,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,andrewor14,andrewor14,andrewor14,27/Jan/15 22:11,02/Feb/15 21:17,14/Jul/23 06:27,02/Feb/15 21:17,1.0.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"In DriverSuite, we currently set a timeout of 60 seconds. If after this time the process has not terminated, we leak the process because we never destroy it.

In SparkSubmitSuite, we currently do not have a timeout so the test can hang indefinitely.",,andrewor14,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 27 22:13:51 UTC 2015,,,,,,,,,,"0|i24v7z:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"27/Jan/15 22:13;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4230;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Preserve spaces in path to spark-ec2,SPARK-5434,12770457,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nchammas,nchammas,nchammas,27/Jan/15 20:35,27/Feb/15 13:49,14/Jul/23 06:27,27/Feb/15 13:49,1.2.0,,,,,,1.2.2,1.3.0,,,,,EC2,,,,0,,,,,,"If the path to {{spark-ec2}} contains spaces, the script won't run.",,apachespark,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 27 13:49:17 UTC 2015,,,,,,,,,,"0|i24uzj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/15 20:37;apachespark;User 'nchammas' has created a pull request for this issue:
https://github.com/apache/spark/pull/4224;;;","27/Feb/15 13:49;srowen;I backported to 1.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't generate Hive golden answer on Hive 0.13.1,SPARK-5429,12770401,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,27/Jan/15 17:07,29/Jan/15 23:29,14/Jul/23 06:27,29/Jan/15 23:29,,,,,,,1.3.0,,,,,,,,,,0,,,,,,"I found that running HiveComparisonTest.createQueryTest to generate Hive golden answer files on Hive 0.13.1 would throw KryoException. Since Hive 0.13.0, Kryo plan serialization is introduced alongside javaXML one. This is a quick fix to set hive configuration to use javaXML serialization.",,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 27 17:11:31 UTC 2015,,,,,,,,,,"0|i24unr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/15 17:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/4223;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Java API helper methods,SPARK-5426,12770341,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,kul,kul,27/Jan/15 12:36,05/Feb/15 03:56,14/Jul/23 06:27,04/Feb/15 23:09,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,DataFrame previously SchemaRDD is not directly java compatible. But this does seems a bit odd as `SQLContext` is now to be used with Java API and any operation will yield a SchemaRDD which the uesr will have to covert to JavaRDD.,,apachespark,kul,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 23:09:12 UTC 2015,,,,,,,,,,"0|i24uaf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"27/Jan/15 12:38;kul;This looks related to
https://issues.apache.org/jira/browse/SPARK-5193;;;","28/Jan/15 06:19;kul;`toJavaRDD` seems to be the trick.  But this does seems a bit odd imo as `SQLContext` is now to be used with Java API and any operation will yield a SchemaRDD which the uer will have to covert to JavaRDD.;;;","28/Jan/15 08:59;apachespark;User 'kul' has created a pull request for this issue:
https://github.com/apache/spark/pull/4243;;;","04/Feb/15 23:09;marmbrus;Issue resolved by pull request 4243
[https://github.com/apache/spark/pull/4243];;;",,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during SparkConf creation,SPARK-5425,12770307,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jlewandowski,jlewandowski,jlewandowski,27/Jan/15 09:58,08/Feb/15 03:17,14/Jul/23 06:27,08/Feb/15 03:17,1.1.1,1.2.0,,,,,1.1.2,1.2.2,1.3.0,,,,Spark Core,,,,0,,,,,,"This fragment of code:

{code}
  if (loadDefaults) {
    // Load any spark.* system properties
    for ((k, v) <- System.getProperties.asScala if k.startsWith(""spark."")) {
      settings(k) = v
    }
  }
{code}

causes 

{noformat}
ERROR 09:43:15  SparkMaster service caused error in state STARTINGjava.util.ConcurrentModificationException: null
	at java.util.Hashtable$Enumerator.next(Hashtable.java:1167) ~[na:1.7.0_60]
	at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$3.next(Wrappers.scala:458) ~[scala-library-2.10.4.jar:na]
	at scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$3.next(Wrappers.scala:454) ~[scala-library-2.10.4.jar:na]
	at scala.collection.Iterator$class.foreach(Iterator.scala:727) ~[scala-library-2.10.4.jar:na]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) ~[scala-library-2.10.4.jar:na]
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) ~[scala-library-2.10.4.jar:na]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.10.4.jar:na]
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) ~[scala-library-2.10.4.jar:na]
	at org.apache.spark.SparkConf.<init>(SparkConf.scala:53) ~[spark-core_2.10-1.2.1_dse-20150121.075638-2.jar:1.2.1_dse-SNAPSHOT]
	at org.apache.spark.SparkConf.<init>(SparkConf.scala:47) ~[spark-core_2.10-1.2.1_dse-20150121.075638-2.jar:1.2.1_dse-SNAPSHOT]
{noformat}

when there is another thread which modifies system properties at the same time. 

This bug https://issues.scala-lang.org/browse/SI-7775 is somehow related to the issue and shows that the problem has been also found elsewhere. 
",,apachespark,jlewandowski,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1010,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Feb 08 03:17:40 UTC 2015,,,,,,,,,,"0|i24u2v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/15 12:16;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4220;;;","27/Jan/15 12:17;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4221;;;","27/Jan/15 12:18;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4222;;;","27/Jan/15 12:19;jlewandowski;[~joshrosen] can you take a look?;;;","02/Feb/15 22:09;joshrosen;I've merged [~jlewandowski]'s patch (https://github.com/apache/spark/pull/4222) to fix this in `master` (1.3.0) and `branch-1.1` (1.1.2), and I've added the {{backport-needed}} tag so we remember to merge it into 1.2.2.;;;","08/Feb/15 03:17;joshrosen;I've merged this into `branch-1.2` (1.2.2), completing the backports.;;;",,,,,,,,,,,,,,,,,,,,,,,
SparkSql throw OOM at shuffle,SPARK-5421,12770256,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,shenhong,shenhong,27/Jan/15 02:45,16/Sep/15 08:06,14/Jul/23 06:27,15/Sep/15 21:36,1.2.0,,,,,,1.5.0,,,,,,SQL,,,,1,,,,,,"ExternalAppendOnlyMap if only for the spark job that aggregator isDefined,  but sparkSQL's shuffledRDD haven't define aggregator, so sparkSQL won't spill at shuffle, it's very easy to throw OOM at shuffle.  I think sparkSQL also need spill at shuffle.
One of the executor's log, here is  stderr:
15/01/27 07:02:19 INFO spark.MapOutputTrackerWorker: Don't have map outputs for shuffle 1, fetching them
15/01/27 07:02:19 INFO spark.MapOutputTrackerWorker: Doing the fetch; tracker actor = Actor[akka.tcp://sparkDriver@10.196.128.140:40952/user/MapOutputTracker#1435377484]
15/01/27 07:02:19 INFO spark.MapOutputTrackerWorker: Got the output locations
15/01/27 07:02:19 INFO storage.ShuffleBlockFetcherIterator: Getting 143 non-empty blocks out of 143 blocks
15/01/27 07:02:19 INFO storage.ShuffleBlockFetcherIterator: Started 4 remote fetches in 72 ms
15/01/27 07:47:29 ERROR executor.CoarseGrainedExecutorBackend: RECEIVED SIGNAL 15: SIGTERM

here is  stdout:
2015-01-27T07:44:43.487+0800: [Full GC 3961343K->3959868K(3961344K), 29.8959290 secs]
2015-01-27T07:45:13.460+0800: [Full GC 3961343K->3959992K(3961344K), 27.9218150 secs]
2015-01-27T07:45:41.407+0800: [GC 3960347K(3961344K), 3.0457450 secs]
2015-01-27T07:45:52.950+0800: [Full GC 3961343K->3960113K(3961344K), 29.3894670 secs]
2015-01-27T07:46:22.393+0800: [Full GC 3961118K->3960240K(3961344K), 28.9879600 secs]
2015-01-27T07:46:51.393+0800: [Full GC 3960240K->3960213K(3961344K), 34.1530900 secs]
#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError=""kill %p""
#   Executing /bin/sh -c ""kill 9050""...
2015-01-27T07:47:25.921+0800: [GC 3960214K(3961344K), 3.3959300 secs]
",,marmbrus,romi-totango,shenhong,smolav,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 21:36:27 UTC 2015,,,,,,,,,,"0|i24trr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/15 10:12;romi-totango;does this still happen on the latest version?
I got some OOM with Spark 1.4.0;;;","15/Sep/15 21:36;marmbrus;Spark 1.5 should have taken care of this.  Please reopen if you can still reproduce.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Initialize Executor.threadPool before ExecutorSource,SPARK-5416,12770243,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,rdub,rdub,rdub,27/Jan/15 01:11,06/Feb/15 18:53,14/Jul/23 06:27,06/Feb/15 12:22,1.2.0,,,,,,1.3.0,1.4.0,,,,,Spark Core,,,,0,,,,,,"I recently saw some NPEs from [{{ExecutorSource:44}}|https://github.com/apache/spark/blob/0497ea51ac345f8057d222a18dbbf8eae78f5b92/core/src/main/scala/org/apache/spark/executor/ExecutorSource.scala#L44] in the first couple seconds of my executors' being initialized.

I think that {{ExecutorSource}} was trying to report these metrics before its threadpool was initialized; there are a few LoC between the source being registered ([Executor.scala:82|https://github.com/apache/spark/blob/0497ea51ac345f8057d222a18dbbf8eae78f5b92/core/src/main/scala/org/apache/spark/executor/Executor.scala#L82]) and the threadpool being initialized ([Executor.scala:106|https://github.com/apache/spark/blob/0497ea51ac345f8057d222a18dbbf8eae78f5b92/core/src/main/scala/org/apache/spark/executor/Executor.scala#L106]).

We should initialize the threapool before the ExecutorSource is registered.",,apachespark,hammer,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 12:22:42 UTC 2015,,,,,,,,,,"0|i24tov:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/15 01:18;apachespark;User 'ryan-williams' has created a pull request for this issue:
https://github.com/apache/spark/pull/4212;;;","06/Feb/15 12:22;srowen;Issue resolved by pull request 4212
[https://github.com/apache/spark/pull/4212];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot bind Master to a specific hostname as per the documentation,SPARK-5412,12770208,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,aseigneurin,aseigneurin,26/Jan/15 22:21,15/May/15 18:32,14/Jul/23 06:27,15/May/15 18:32,1.2.0,,,,,,1.2.3,1.3.2,1.4.0,,,,Deploy,,,,0,,,,,,"Documentation on http://spark.apache.org/docs/latest/spark-standalone.html indicates:

{quote}
You can start a standalone master server by executing:
./sbin/start-master.sh
...
the following configuration options can be passed to the master and worker:
...
-h HOST, --host HOST	Hostname to listen on
{quote}

The ""\-h"" or ""--host"" parameter actually doesn't work with the start-master.sh script. Instead, one has to set the ""SPARK_MASTER_IP"" variable prior to executing the script.

Either the script or the documentation should be updated.",,apachespark,aseigneurin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 12:49:06 UTC 2015,,,,,,,,,,"0|i24thr:",9223372036854775807,,,,,,,,,,,,,,1.2.3,1.3.2,1.4.0,,,,,,,,,,,"15/May/15 11:49;srowen;A-ha. I think the issue is that additional args to {{start-master.sh}} aren't passed through to {{Master}} with ""$@"". I think they are intended to be, as the same thing is done in {{start-slave.sh}} for example. Let me look a little more and open a PR if it seems like the right thing to do.;;;","15/May/15 12:49;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/6185;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Error parsing scientific notation in a select statement,SPARK-5410,12770122,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,hmf,hmf,26/Jan/15 16:37,15/Sep/15 21:37,14/Jul/23 06:27,15/Sep/15 21:37,1.2.0,,,,,,,,,,,,SQL,,,,0,numeric,parsing,sql,,,"I am using the Cassandra DB and am attempting a select through the Spark SQL interface.

SELECT * from key_value WHERE f2 < 2.2E10

And get the following error (no error if I remove the E10):

[info] - should be able to select a subset of applicable features *** FAILED ***
[info]   java.lang.RuntimeException: [1.39] failure: ``UNION'' expected but identifier E10 found
[info] 
[info] SELECT * from key_value WHERE f2 < 2.2E10
[info]                                       ^
[info]   at scala.sys.package$.error(package.scala:27)
[info]   at org.apache.spark.sql.catalyst.AbstractSparkSQLParser.apply(SparkSQLParser.scala:33)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:79)
[info]   at org.apache.spark.sql.SQLContext$$anonfun$1.apply(SQLContext.scala:79)
[info]   at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:174)
[info]   at org.apache.spark.sql.catalyst.SparkSQLParser$$anonfun$org$apache$spark$sql$catalyst$SparkSQLParser$$others$1.apply(SparkSQLParser.scala:173)
[info]   at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
[info]   at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
[info]   at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
[info]   ...
",,glenn.strycker@gmail.com,hmf,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 21:37:37 UTC 2015,,,,,,,,,,"0|i24szr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/15 21:37;marmbrus;I tested and in Spark 1.5 the HiveContext can parse {{2.2E10}}.  HiveContext is recommended for all users even if you aren't using Hive support as the parser is much better.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
MaxPermSize is ignored by ExecutorRunner and DriverRunner,SPARK-5408,12770034,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,jlewandowski,jlewandowski,jlewandowski,26/Jan/15 08:31,07/Feb/15 15:59,14/Jul/23 06:27,07/Feb/15 15:58,1.2.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"ExecutorRunner and DriverRunner uses CommandUtils to build the command which runs executor or driver. The problem is that it has hardcoded {{-XX:MaxPermSize=128m}} and uses it regardless it is specified in extraJavaOpts or not. 
",,apachespark,bcantoni,donnchadh,jlewandowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 07 15:58:25 UTC 2015,,,,,,,,,,"0|i24sgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/15 08:48;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4202;;;","26/Jan/15 08:49;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4203;;;","26/Jan/15 08:51;jlewandowski;Can anybody take a look?;;;","07/Feb/15 15:58;srowen;Issue resolved by pull request 4203
[https://github.com/apache/spark/pull/4203];;;",,,,,,,,,,,,,,,,,,,,,,,,,
LocalLAPACK mode in RowMatrix.computeSVD should have much smaller upper bound,SPARK-5406,12770019,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,26/Jan/15 07:25,02/Feb/15 04:14,14/Jul/23 06:27,02/Feb/15 03:43,1.2.0,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"In RowMatrix.computeSVD, under LocalLAPACK mode, the code would invoke brzSvd. Yet breeze svd for dense matrix has latent constraint. In it's implementation
( https://github.com/scalanlp/breeze/blob/master/math/src/main/scala/breeze/linalg/functions/svd.scala   ):

      val workSize = ( 3
        * scala.math.min(m, n)
        * scala.math.min(m, n)
        + scala.math.max(scala.math.max(m, n), 4 * scala.math.min(m, n)
          * scala.math.min(m, n) + 4 * scala.math.min(m, n))
      )
      val work = new Array[Double](workSize)

as a result, column num must satisfy 7 * n * n + 4 * n < Int.MaxValue
thus, n < 17515.

This jira is only the first step. If possbile, I hope spark can handle matrix computation up to 80K * 80K.
","centos, others should be similar",apachespark,mengxr,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 02 04:14:57 UTC 2015,,,,,,,,,,"0|i24sdj:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"26/Jan/15 07:36;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4200;;;","02/Feb/15 03:43;mengxr;fixed by https://github.com/apache/spark/pull/4200;;;","02/Feb/15 04:14;yuhaoyan;fix and merged. Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Statistic of Logical Plan is too aggresive,SPARK-5404,12770003,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,26/Jan/15 05:43,25/Apr/15 21:57,14/Jul/23 06:27,18/Mar/15 02:33,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"The statistic number of a logical plan is quite helpful while do optimization like join reordering, however, the default algorithm is too aggressive, which probably lead to a totally wrong join order.",,apachespark,chenghao,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 18 02:33:09 UTC 2015,,,,,,,,,,"0|i24s9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/15 06:02;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4199;;;","05/Mar/15 14:31;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4914;;;","18/Mar/15 02:33;marmbrus;Issue resolved by pull request 4914
[https://github.com/apache/spark/pull/4914];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Executor ID should be set before MetricsSystem is created,SPARK-5401,12769923,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rdub,rdub,rdub,25/Jan/15 07:00,25/Jan/15 22:18,14/Jul/23 06:27,25/Jan/15 22:18,1.2.0,,,,,,1.2.1,1.3.0,,,,,Spark Core,,,,0,,,,,,"MetricsSystem construction [attempts to namespace metrics from each executor using that executor's ID|https://github.com/apache/spark/blob/0d1e67ee9b29b51bccfc8a319afe9f9b4581afc8/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala#L131].

The ID is [currently set at Executor construction time|https://github.com/apache/spark/blob/0d1e67ee9b29b51bccfc8a319afe9f9b4581afc8/core/src/main/scala/org/apache/spark/executor/Executor.scala#L76-L79] (uncoincidentally, just before the {{ExecutorSource}} is registered), but this is after the {{MetricsSystem}} has been initialized (which [happens during {{SparkEnv}} construction|https://github.com/apache/spark/blob/0d1e67ee9b29b51bccfc8a319afe9f9b4581afc8/core/src/main/scala/org/apache/spark/SparkEnv.scala#L323-L332], which itself happens during {{ExecutorBackend}} construction, *before* {{Executor}} construction).

I noticed this problem because I wasn't seeing any JVM metrics from my executors in a Graphite dashboard I've set up; turns out all the executors (and the driver) were namespacing their metrics under ""<driver>"", and Graphite responds to such a situation by only taking the last value it receives for each ""metric"" within a configurable time window (e.g. 10s). I was seeing per-executor metrics, properly namespaced with each executor's ID, from {{ExecutorSource}}, which as I mentioned above is registered after the executor ID is set.

I have a one-line fix for this that I will submit shortly.",,apachespark,hammer,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 25 07:04:54 UTC 2015,,,,,,,,,,"0|i24rs7:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,,,"25/Jan/15 07:04;apachespark;User 'ryan-williams' has created a pull request for this issue:
https://github.com/apache/spark/pull/4194;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assigning aliases to several return values of an UDF,SPARK-5397,12769860,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,mucks17,mucks17,24/Jan/15 13:08,16/Sep/15 08:06,14/Jul/23 06:27,15/Sep/15 21:38,,,,,,,1.5.0,,,,,,SQL,,,,0,,,,,,"The query with following syntax is no valid SQL in Spark due to the assigment of multiple aliases.
So it seems not possible for me to port former HiveQL queries with UDFs returning multiple values to Spark SQL.

Query 
-------- 
SELECT my_function(param_one, param_two) AS (return_one, return_two,
return_three) 
FROM my_table; 

Error 
-------- 
Unsupported language features in query: SELECT my_function(param_one,
param_two) AS (return_one, return_two, return_three) 
FROM my_table; 

TOK_QUERY 
  TOK_FROM 
    TOK_TABREF 
      TOK_TABNAME 
        my_table 
    TOK_SELECT 
      TOK_SELEXPR 
        TOK_FUNCTION 
          my_function 
          TOK_TABLE_OR_COL 
            param_one 
          TOK_TABLE_OR_COL 
            param_two 
        return_one 
        return_two 
        return_three 
",,dreamquster,marmbrus,mucks17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 21:38:10 UTC 2015,,,,,,,,,,"0|i24ref:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/15 21:38;marmbrus;Tested in 1.5 and this query now parses correctly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Syntax error in spark scripts on windows.,SPARK-5396,12769838,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tsudukim,protsenkovi,protsenkovi,24/Jan/15 10:43,05/Mar/15 18:27,14/Jul/23 06:27,06/Feb/15 18:59,1.3.0,,,,,,1.3.0,,,,,,Spark Shell,,,,0,,,,,,"I made the following steps: 

1. downloaded and installed Scala 2.11.5 
2. downloaded spark 1.2.0 by git clone git://github.com/apache/spark.git 
3. run dev/change-version-to-2.11.sh and mvn -Dscala-2.11 -DskipTests clean package (in git bash) 

After installation tried to run spark-shell.cmd in cmd shell and it says there is a syntax error in file. The same with spark-shell2.cmd, spark-submit.cmd and  spark-submit2.cmd.

!windows7.png!",Window 7 and Window 8.1.,apachespark,nchammas,protsenkovi,tsudukim,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/15 10:45;protsenkovi;windows7.png;https://issues.apache.org/jira/secure/attachment/12694344/windows7.png","24/Jan/15 10:45;protsenkovi;windows8.1.png;https://issues.apache.org/jira/secure/attachment/12694345/windows8.1.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 05 18:27:41 UTC 2015,,,,,,,,,,"0|i24r9j:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"06/Feb/15 10:31;apachespark;User 'tsudukim' has created a pull request for this issue:
https://github.com/apache/spark/pull/4428;;;","06/Feb/15 10:44;tsudukim;This seems to be caused by a simple syntax error which is introduced last month by [SPARK-4990].;;;","28/Feb/15 21:18;nchammas;What does that error message say in English? So we can pattern match to similar reports elsewhere.;;;","05/Mar/15 18:27;protsenkovi;It says ""Syntax error in command."".;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Large number of Python workers causing resource depletion,SPARK-5395,12769827,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,skrasser,skrasser,24/Jan/15 08:19,17/Feb/15 04:36,14/Jul/23 06:27,17/Feb/15 04:35,1.2.0,1.3.0,,,,,1.2.2,1.3.0,,,,,PySpark,,,,3,,,,,,"During job execution a large number of Python worker accumulates eventually causing YARN to kill containers for being over their memory allocation (in the case below that is about 8G for executors plus 6G for overhead per container). 

In this instance, at the time of killing the container 97 pyspark.daemon processes had accumulated.

{noformat}
2015-01-23 15:36:53,654 INFO [Reporter] yarn.YarnAllocationHandler (Logging.scala:logInfo(59)) - Container marked as failed: container_1421692415636_0052_01_000030. Exit status: 143. Diagnostics: Container [pid=35211,containerID=container_1421692415636_0052_01_000030] is running beyond physical memory limits. Current usage: 14.9 GB of 14.5 GB physical memory used; 41.3 GB of 72.5 GB virtual memory used. Killing container.
Dump of the process-tree for container_1421692415636_0052_01_000030 :
|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
|- 54101 36625 36625 35211 (python) 78 1 332730368 16834 python -m pyspark.daemon
|- 52140 36625 36625 35211 (python) 58 1 332730368 16837 python -m pyspark.daemon
|- 36625 35228 36625 35211 (python) 65 604 331685888 17694 python -m pyspark.daemon
	[...]
{noformat}

The configuration used uses 64 containers with 2 cores each.

Full output here: https://gist.github.com/skrasser/e3e2ee8dede5ef6b082c

Mailinglist discussion: https://www.mail-archive.com/user@spark.apache.org/msg20102.html",AWS ElasticMapReduce,apachespark,cbmeyer,davies,joshrosen,marmbrus,mkman84,skrasser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 04:35:59 UTC 2015,,,,,,,,,,"0|i24r73:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"26/Jan/15 17:21;mkman84;Having the same issue in standalone deployment mode. A single spark-submitted job is spawning a ton of pyspark.daemon instances and depleting the cluster memory even though the appropriate environment variables have been set.;;;","26/Jan/15 23:36;skrasser;[~mkman84], do you also see this for both spark.python.worker.reuse false/true? (FWIW, the run I pasted above had reuse disabled.)

Also, do you happen to have a small job that can be used as a repro ([~davies] was asking for one, so far I only managed to trigger this condition using production data).;;;","26/Jan/15 23:56;mkman84;[~skrasser], I actually only managed to have this reproduced using production data as well (so far). I'll try to write a simple version tomorrow but it seems that it's a mix of both python worker processes not being killed after it's no longer running (causing build up), as well as the python worker exceeding the allocated memory limit. 

I think it *may* be related to a couple of specific actions such as groupByKey/cogroup, though I'll still need to do some tests to be sure what's causing this.

I should also add that we haven't modified the default for the python.worker.reuse variable, so in our case it should be using the default of True.;;;","27/Jan/15 01:04;mkman84;This may prove to be useful...

I'm watching a presently running spark-submitted job, while watching the pyspark.daemon processes. The framework is permitted to only use 8 cores on each node with the default python worker memory of 512mb per node (not the executor memory which is set to higher than this).

Ignoring the exact RDD actions for a moment, it looks like while it transitions from Stage 1 -> Stage 2, it spawned up 8-10 additional pyspark.daemon processes making the box use more cores than it was even allowed to... A few seconds after that, the other 8 processes entered a sleeping state while still holding onto the physical memory it ate up in Stage 1. As soon as Stage 2 finished, practically all of the pyspark.daemons vanished and freed up the memory usage. I was keeping an eye on 2 random nodes and the exact same thing occurred on both. It was also the only currently executing job at the time so there was really no other interference/contention for resources.

I will try to provide a bit more detail on the exact transformations/actions occurring between the 2 stages, although I know a PartionBy and cogroup are occurring at the very least without inspecting the spark-submitted code directly.;;;","27/Jan/15 03:14;skrasser;Some additional findings from my side: I've managed to trigger the problem using a simpler job on production data that basically does a reduceByKey followed by a count action. I get >20 workers (2 cores per executor) before any tasks in the first stage (reduceByKey) complete (i.e. different from the stage transition behavior you noticed). However, this doesn't occur if I run over a smaller data set, i.e. fewer production data files.

Before calling reduceByKey I have a coalesce call. Without that the error does not occur (at least in this smaller script). This at first glance looked potentially spilling related (more data per task), but attempting to force spills by setting the worker memory very low did not help with my attempts to get a repro on test data.;;;","27/Jan/15 15:05;mkman84;Actually I think I know why this happens... I'm thinking the problem really occurs due to the way auto-persistence of specific actions occur. 

ReduceByKey, GroupByKey, cogroup, etc, are typically heavy actions that get auto-persisted for the reason that the resulting RDD's will most likely be used for something right after. 

The interesting thing is that this memory is outside of the executor memory for the framework (it's what goes into these pyspark daemons that get spawned up temporarily). The other interesting fact is that let's say we leave the default python worker memory set to 512MB, and you have a framework that uses 8 cores on each executor, it spawns up 8 * 512MB (4GB) of python workers while the stage is running. 

[~skrasser] In your case, if you chain a bunch of auto-persisting actions (which I believe coalesce is a part of, since instead of dealing with a shuffle read, it instead builds a potentially large array of partitions on the executor), it will spawn an additional 2 python workers per executor for that separate task, while the previous tasks' python workers are left in a sleeping state, waiting for the results of the subsequent task to complete... 

If that's the case, then it should actually be a bit easier showing how a single framework can nuke a single host by creating a crazy chain of coalescing/reduceByKey/GroupByKey/cogrouping actions (which I'm off to try out now haha)

EDIT: I'm almost positive this is what's causing this to occur now. Unfortunately there is no easy way to prevent a single framework from wiping out all of the memory on a single box if it does a huge amount of shuffle writing, with the combination of auto-persisting and chained RDD actions which depend on previous RDD computations... You COULD break up the chain by forcing an intermediate step to DISK using (saveAsPickleFile/saveAsTextFile perhaps), and then in the next step reading it back in. At least that would force the previous python worker daemons to be cleaned up before potentially spawning new ones...

Ideally there should be an environment variable for the max number of python workers allowed to be spawned per executor, because it looks like that doesn't exist as of yet! ;;;","27/Jan/15 17:31;skrasser;I've definitely seen this behavior when adding additional reduceByKey operations that could execute in parallel (around 2 workers per such operation), but in my small test script it's a single reduceByKey operation followed by a single coalesce statement. I'm running it right now, and it already spiked up to over 30 Python workers per executor, so there must be something else going on on top of this.;;;","28/Jan/15 01:43;skrasser;Some new findings: I can trigger the problem now just using the {{coalesce}} call. My job now looks like this:
{code}sc.newAPIHadoopFile().map().map().coalesce().count(){code}

In the 64 executor case, this occurs when processing 1TB in 1500 files. If I go down to 2 executors, 200GB in 305 files make the worker count go up to 9 (higher as I add more files). With less data, things appear normal.

That raises the question about what {{coalesce()}} is doing that causes new workers to spawn.;;;","28/Jan/15 07:26;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4238;;;","28/Jan/15 17:15;skrasser;Thanks Davies!;;;","28/Jan/15 17:20;mkman84;Thanks! Hopefully that tackles the same problem I was seeing!;;;","30/Jan/15 01:30;joshrosen;I've committed Davies' patch (https://github.com/apache/spark/pull/4238) to {{master}} for inclusion in Spark 1.3.0 and tagged it for later backport to Spark 1.2.2. (I'll cherry-pick the commit after we close the 1.2.1 vote).;;;","17/Feb/15 04:35;joshrosen;I've merged this into `branch-1.2` (1.2.2), so I'm marking this as fixed.;;;",,,,,,,,,,,,,,,,
Flood of util.RackResolver log messages after SPARK-1714,SPARK-5393,12769806,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sandyr,sandyr,sandyr,24/Jan/15 01:55,25/Oct/19 20:25,14/Jul/23 06:27,30/Jan/15 17:32,1.3.0,,,,,,1.3.0,,,,,,,,,,0,,,,,,"I thought I fixed this while working on the patch, but [~laserson] seems to have encountered it when running on master.",,apachespark,junping_du,sandyr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-1714,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 16 16:37:08 UTC 2015,,,,,,,,,,"0|i24r2f:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"24/Jan/15 22:26;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/4192;;;","16/Mar/15 14:24;junping_du;A similar JIRA just get filed in YARN-3350 to fix in YARN side. [~sandyr], did we have a special reason to not fixing it in YARN side before?;;;","16/Mar/15 16:20;sandyr;Hi [~junping_du], there's no special reason we didn't fix YARN.  The Spark-side fix was just higher priority because we wanted Spark to work with all versions of YARN (not just versions with a fix).;;;","16/Mar/15 16:37;junping_du;Thanks [~sandyr] for confirmation on this! I will go ahead to commit that patch in YARN (after getting updated with my comments).;;;",,,,,,,,,,,,,,,,,,,,,,,,,
SparkSQL fails to create tables with custom JSON SerDe,SPARK-5391,12769777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,dyross,dyross,24/Jan/15 00:10,15/Oct/15 22:36,14/Jul/23 06:27,13/Oct/15 18:33,,,,,,,1.6.0,,,,,,SQL,,,,4,,,,,,"- Using Spark built from trunk on this commit: https://github.com/apache/spark/commit/bc20a52b34e826895d0dcc1d783c021ebd456ebd
- Build for Hive13
- Using this JSON serde: https://github.com/rcongiu/Hive-JSON-Serde

First download jar locally:
{code}
$ curl http://www.congiu.net/hive-json-serde/1.3/cdh5/json-serde-1.3-jar-with-dependencies.jar > /tmp/json-serde-1.3-jar-with-dependencies.jar
{code}

Then add it in SparkSQL session:
{code}
add jar /tmp/json-serde-1.3-jar-with-dependencies.jar
{code}

Finally create table:
{code}
create table test_json (c1 boolean) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe';
{code}

Logs for add jar:
{code}
15/01/23 23:48:33 INFO thriftserver.SparkExecuteStatementOperation: Running query 'add jar /tmp/json-serde-1.3-jar-with-dependencies.jar'
15/01/23 23:48:34 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/01/23 23:48:34 INFO SessionState: Added /tmp/json-serde-1.3-jar-with-dependencies.jar to class path
15/01/23 23:48:34 INFO SessionState: Added resource: /tmp/json-serde-1.3-jar-with-dependencies.jar
15/01/23 23:48:34 INFO spark.SparkContext: Added JAR /tmp/json-serde-1.3-jar-with-dependencies.jar at http://192.168.99.9:51312/jars/json-serde-1.3-jar-with-dependencies.jar with timestamp 1422056914776
15/01/23 23:48:34 INFO thriftserver.SparkExecuteStatementOperation: Result Schema: List()
15/01/23 23:48:34 INFO thriftserver.SparkExecuteStatementOperation: Result Schema: List()
{code}

Logs (with error) for create table:
{code}
15/01/23 23:49:00 INFO thriftserver.SparkExecuteStatementOperation: Running query 'create table test_json (c1 boolean) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe''
15/01/23 23:49:00 INFO parse.ParseDriver: Parsing command: create table test_json (c1 boolean) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
15/01/23 23:49:01 INFO parse.ParseDriver: Parse Completed
15/01/23 23:49:01 INFO session.SessionState: No Tez session required at this point. hive.execution.engine=mr.
15/01/23 23:49:01 INFO log.PerfLogger: <PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO log.PerfLogger: <PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO ql.Driver: Concurrency mode is disabled, not creating a lock manager
15/01/23 23:49:01 INFO log.PerfLogger: <PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO log.PerfLogger: <PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO parse.ParseDriver: Parsing command: create table test_json (c1 boolean) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
15/01/23 23:49:01 INFO parse.ParseDriver: Parse Completed
15/01/23 23:49:01 INFO log.PerfLogger: </PERFLOG method=parse start=1422056941103 end=1422056941104 duration=1 from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO log.PerfLogger: <PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO parse.SemanticAnalyzer: Starting Semantic Analysis
15/01/23 23:49:01 INFO parse.SemanticAnalyzer: Creating table test_json position=13
15/01/23 23:49:01 INFO ql.Driver: Semantic Analysis Completed
15/01/23 23:49:01 INFO log.PerfLogger: </PERFLOG method=semanticAnalyze start=1422056941104 end=1422056941240 duration=136 from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO ql.Driver: Returning Hive schema: Schema(fieldSchemas:null, properties:null)
15/01/23 23:49:01 INFO log.PerfLogger: </PERFLOG method=compile start=1422056941071 end=1422056941252 duration=181 from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO log.PerfLogger: <PERFLOG method=Driver.execute from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO ql.Driver: Starting command: create table test_json (c1 boolean) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
15/01/23 23:49:01 INFO log.PerfLogger: </PERFLOG method=TimeToSubmit start=1422056941067 end=1422056941258 duration=191 from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO log.PerfLogger: <PERFLOG method=runTasks from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO log.PerfLogger: <PERFLOG method=task.DDL.Stage-0 from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 WARN security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user anonymous
org.apache.hadoop.util.Shell$ExitCodeException: id: anonymous: No such user

  at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)
  at org.apache.hadoop.util.Shell.run(Shell.java:418)
  at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
  at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)
  at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)
  at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
  at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
  at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
  at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)
  at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1409)
  at org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setConf(HadoopDefaultAuthenticator.java:63)
  at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
  at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
  at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthenticator(HiveUtils.java:424)
  at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:377)
  at org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:867)
  at org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:589)
  at org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:174)
  at org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:116)
  at org.apache.hadoop.hive.ql.metadata.Hive.newTable(Hive.java:2566)
  at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4046)
  at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:281)
  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)
  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
  at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:292)
  at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:264)
  at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
  at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:61)
  at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:474)
  at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:474)
  at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
  at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:107)
  at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:73)
  at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:160)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:483)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
  at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
  at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
  at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
  at com.sun.proxy.$Proxy18.executeStatement(Unknown Source)
  at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:220)
  at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
  at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
15/01/23 23:49:01 WARN security.UserGroupInformation: No groups available for user anonymous
15/01/23 23:49:01 WARN security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user anonymous
org.apache.hadoop.util.Shell$ExitCodeException: id: anonymous: No such user

  at org.apache.hadoop.util.Shell.runCommand(Shell.java:505)
  at org.apache.hadoop.util.Shell.run(Shell.java:418)
  at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:650)
  at org.apache.hadoop.util.Shell.execCommand(Shell.java:739)
  at org.apache.hadoop.util.Shell.execCommand(Shell.java:722)
  at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:83)
  at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:52)
  at org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.getGroups(JniBasedUnixGroupsMappingWithFallback.java:50)
  at org.apache.hadoop.security.Groups.getGroups(Groups.java:139)
  at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1409)
  at org.apache.hadoop.hive.ql.security.HadoopDefaultAuthenticator.setConf(HadoopDefaultAuthenticator.java:64)
  at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:73)
  at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)
  at org.apache.hadoop.hive.ql.metadata.HiveUtils.getAuthenticator(HiveUtils.java:424)
  at org.apache.hadoop.hive.ql.session.SessionState.setupAuth(SessionState.java:377)
  at org.apache.hadoop.hive.ql.session.SessionState.getAuthenticator(SessionState.java:867)
  at org.apache.hadoop.hive.ql.session.SessionState.getUserFromAuthenticator(SessionState.java:589)
  at org.apache.hadoop.hive.ql.metadata.Table.getEmptyTable(Table.java:174)
  at org.apache.hadoop.hive.ql.metadata.Table.<init>(Table.java:116)
  at org.apache.hadoop.hive.ql.metadata.Hive.newTable(Hive.java:2566)
  at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4046)
  at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:281)
  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)
  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
  at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:292)
  at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:264)
  at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
  at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:61)
  at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:474)
  at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:474)
  at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
  at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:107)
  at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:73)
  at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:160)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:483)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
  at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
  at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
  at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
  at com.sun.proxy.$Proxy18.executeStatement(Unknown Source)
  at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:220)
  at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
  at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
15/01/23 23:49:01 WARN security.UserGroupInformation: No groups available for user anonymous
15/01/23 23:49:01 ERROR exec.DDLTask: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot validate serde: org.openx.data.jsonserde.JsonSerDe
  at org.apache.hadoop.hive.ql.exec.DDLTask.validateSerDe(DDLTask.java:3952)
  at org.apache.hadoop.hive.ql.exec.DDLTask.createTable(DDLTask.java:4084)
  at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:281)
  at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)
  at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)
  at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1503)
  at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1270)
  at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1088)
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:911)
  at org.apache.hadoop.hive.ql.Driver.run(Driver.java:901)
  at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:292)
  at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:264)
  at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
  at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:61)
  at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:474)
  at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:474)
  at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
  at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:107)
  at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:73)
  at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:160)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:483)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
  at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
  at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
  at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
  at com.sun.proxy.$Proxy18.executeStatement(Unknown Source)
  at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:220)
  at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
  at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: Class org.openx.data.jsonserde.JsonSerDe not found
  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:1801)
  at org.apache.hadoop.hive.ql.exec.DDLTask.validateSerDe(DDLTask.java:3946)
  ... 47 more

15/01/23 23:49:01 ERROR ql.Driver: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Cannot validate serde: org.openx.data.jsonserde.JsonSerDe
15/01/23 23:49:01 INFO log.PerfLogger: </PERFLOG method=Driver.execute start=1422056941252 end=1422056941379 duration=127 from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO log.PerfLogger: <PERFLOG method=releaseLocks from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 INFO log.PerfLogger: </PERFLOG method=releaseLocks start=1422056941379 end=1422056941379 duration=0 from=org.apache.hadoop.hive.ql.Driver>
15/01/23 23:49:01 ERROR hive.HiveContext:
======================
HIVE FAILURE OUTPUT
======================
SET spark.sql.codegen=false
SET spark.sql.parquet.binaryAsString=true
SET spark.sql.parquet.cacheMetadata=true
SET spark.sql.hive.version=0.13.1
SET spark.sql.autoBroadcastJoinThreshold=500000
SET spark.sql.shuffle.partitions=10
ADD JAR /tmp/json-serde-1.3-jar-with-dependencies.jar
Added /tmp/json-serde-1.3-jar-with-dependencies.jar to class path
Added resource: /tmp/json-serde-1.3-jar-with-dependencies.jar
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Cannot validate serde: org.openx.data.jsonserde.JsonSerDe

======================
END HIVE FAILURE OUTPUT
======================

15/01/23 23:49:01 ERROR thriftserver.SparkExecuteStatementOperation: Error executing query:
org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Cannot validate serde: org.openx.data.jsonserde.JsonSerDe
  at org.apache.spark.sql.hive.HiveContext.runHive(HiveContext.scala:296)
  at org.apache.spark.sql.hive.HiveContext.runSqlHive(HiveContext.scala:264)
  at org.apache.spark.sql.hive.execution.HiveNativeCommand.run(HiveNativeCommand.scala:37)
  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
  at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
  at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:61)
  at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:474)
  at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:474)
  at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
  at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:107)
  at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:73)
  at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:160)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:483)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
  at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
  at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
  at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
  at com.sun.proxy.$Proxy18.executeStatement(Unknown Source)
  at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:220)
  at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
  at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
15/01/23 23:49:01 WARN thrift.ThriftCLIService: Error executing statement:
org.apache.hive.service.cli.HiveSQLException: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Cannot validate serde: org.openx.data.jsonserde.JsonSerDe
  at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:189)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
  at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:483)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
  at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
  at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
  at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
  at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
  at com.sun.proxy.$Proxy18.executeStatement(Unknown Source)
  at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:220)
  at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
  at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
  at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
  at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
  at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
  at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)
{code}",,davies,dyross,marmbrus,muthu,PKUKILLA,yashwanth.rao11@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Oct 13 18:33:41 UTC 2015,,,,,,,,,,"0|i24qvz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/15 05:13;muthu;Same error occurred when a table is created with json serde in hive table and queried from SparkQL.;;;","15/Sep/15 21:39;marmbrus;Is this still a problem?  Is there a reason you aren't using the native JSON support?;;;","15/Sep/15 21:47;dyross;Haven't tried native JSON but looks promising, so this ticket is probably lower priority.;;;","13/Oct/15 18:33;davies;I tried this, it worked in master, will close this as resolved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Provide a stable application submission gateway in standalone cluster mode,SPARK-5388,12769723,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,andrewor14,andrewor14,andrewor14,23/Jan/15 19:16,14/Jul/16 08:18,14/Jul/23 06:27,06/Feb/15 23:57,1.2.0,,,,,,1.3.0,,,,,,Spark Core,,,,1,,,,,,"The existing submission gateway in standalone mode is not compatible across Spark versions. If you have a newer version of Spark submitting to an older version of the standalone Master, it is currently not guaranteed to work. The goal is to provide a stable REST interface to replace this channel.

For more detail, please see the most recent design doc attached.",,andrewor14,apachespark,cfregly,ckadner,codingcat,dragos,jeromatron,joshrosen,jshook,kzhang,lucas.partridge,pwendell,qwertymaniac,rvernica,sandyr,tigerquoll,vanzin,zxzxy1988,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-16532,,,,,,,,,,,,,,,"05/Feb/15 02:34;andrewor14;stable-spark-submit-in-standalone-mode-2-4-15.pdf;https://issues.apache.org/jira/secure/attachment/12696651/stable-spark-submit-in-standalone-mode-2-4-15.pdf",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Apr 28 18:07:00 UTC 2015,,,,,,,,,,"0|i24ql3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"24/Jan/15 22:06;tigerquoll;Hi Andrew,
I think the idea is well worth considering. 

I have a question if there is an intention for other entities (such as job servers) to communicate with the master at all? If so then the proposed gateway is semantically defined at a fairly low level (just RPC over JSON/HTTP). This is fine if the interface is not going to be exposed to anybody who is not a spark developer with detailed knowledge of spark internals. Did you use the term “REST” to simply mean RPC over JSON/HTTP?

Creating a REST interface is more then a HTTP RPC gateway. If the interface is going to be exposed to 3rd parties (such as developers of Job servers and web notebooks etc) then there is a benefit to simplifying some of the exposed application semantics, and exposing an API that is more integrated with HTTP’s protocol semantics which most people are already familiar with - this is what a true REST interface does and if you are defining an endpoint for others to use it is a very powerful concept that allows other people to quickly grasp how to properly use the exposed interface.

A rough sketch of a more “REST”ed version of the API would be:

*Submit_driver_request*
HTTP POST JSON body of request http://host:port/SparkMaster?SubmitDriver
Responds with standard HTTP Response including allocated DRIVER_ID if driver submission ok, http error codes with spark specific error if not.

*Get status of DRIVER*
HTTP GET http://host:port/SparkMaster/Drivers/<DRIVER_ID>
Responds with JSON body containing information on driver execution.  If no record of driver_id, then http error code 404 (Not found) returned.

*Kill Driver request*
HTTP DELETE http://host:port/SparkMaster/Drivers/<DRIVER_ID>
Responds with JSON body containing information on driver kill request, or http error code if an error occurs.

I would be happy to prototype something like this up to test the concept out for you if you are looking for something more than just RPC over JSON/HTTP.

;;;","27/Jan/15 02:26;apachespark;User 'andrewor14' has created a pull request for this issue:
https://github.com/apache/spark/pull/4216;;;","27/Jan/15 02:33;andrewor14;Hi Dale, thank you for your comments. Yes, in the design doc I used REST roughly interchangeably with HTTP/JSON. But the goal is not to provide a mechanism for other entities to communicate with the Master as you suggested; it is simply to provide a stable mechanism for Spark to work across multiple versions. For instance, you might have a long-running Master that outlives multiple Spark versions, in which case we want to guarantee that newer versions of Spark will still be able to submit to the long-running Master.

I think your proposal to make this more REST-like is potentially a great idea. However, I find the alternative of simply putting the action in the JSON itself easier to reason about. This also allows us to add other messages in the future that are not strictly limited to the semantics of GET, POST, and DELETE. That said, my proposal is also not set in stone yet so if there is a reason compelling enough to change it then I will do so.

Also, a first-cut implementation of my design is now posted at: https://github.com/apache/spark/pull/4216. Please take a look if you feel inclined.;;;","28/Jan/15 23:42;vanzin;Hi [~andrewor14],

I read through the spec and the protocol specification seems to be lacking some details. The mains things that bother me are:

- It's not really a REST API. There's a single endpoint to which you POST different messages. This sort of forces your hand to use a custom implementation, instead of being able to use a much nicer framework for this purpose such as JAX-RS. Using a framework like that can later benefit other parts of Spark too, such as providing a REST API for application data through the web ui / history server. And as I mentioned in the PR, it allows you to define the endpoints using classes or interfaces, which serves two purposes: it allows you to do backwards compatibility checks with tools like MIMA, and it allows you to use the client functionality of JAX-RS for client requests too (and similar tools for other languages for those who, sort of feeding back into Dale's comment). Plus, you can use things like Jackson and not care about how to parse or generate JSON.

- It's unclear how the protocol will be allowed to evolve. What happens when you add a new field or message in a later version, and that version tries to submit to Spark 1.3? Is there a version negotiation up front, so that the new client knows to use the old protocol if possible, or does the client just send the new message and the server will complain if it contains things it doesn't understand?

The latter kinda feeds into the first comment. With a proper REST-based API, you'd put the first version of the protocol under ""/v1"", for example. Later versions are added under ""/v2"" and can add new things. Client and server can then negotiate up front (e.g, client needs at least version ""x"" for the current app, asks the server for its supported versions, and complains if ""x"" is not there).

Also, it could be more specific about how errors are reported. Do you get specific HTTP error codes for different things? Is there an ""Error"" type that is sent back to the client in JSON, and if so, what fields does it have?
;;;","02/Feb/15 21:57;pwendell;Hey [~vanzin] and [~tigerquoll], the original goal of this was just to provide a stable API between the spark-submit and the standalone master, similar to what YARN has. And a simple JSON/REST API was something that is easy to support in the long term.

In terms of using more of the standard web services conventions for the HTTP method (i.e. using GET, POST, and DELETE), I agree that would be nicer if it's not too hard to do. It be mostly for clarity though rather than correctness, this isn't something where we expect things like caching of responses, etc, which depend on strict interpretation of the HTTP method.

In terms of versioning, the original proposal has the client and server communicate versions, so the idea is that compatibility could be handled on that basis, and that all older versions would be supported (hence this being a stable API). For web services API's they sometimes have ""v1"", ""v2"" labels at the URL path. However, this type of versioning implies breakage of older versions, which is exactly what we want to avoid, right? For instance if github adds new fields to a return type (in a way that is backward compatible), I'm assuming they don't increment the version identifier, right? I think the idea here is similar to our handling of the event logs, we want to make sure we support older versions by only increasing what is communicated over time, and having the server gracefully handle older clients. So I'm not totally sure what the ""v2"" style labeling would help with unless we actually think we are going to break this API in a way that's not backwards compatible.

All that said, maybe we could add ""v1"" here just as a hedge in cases this is ever changed (e.g. in Spark 2.0) and we do end up having third party integrations that write against this API. Just wanted to be clear on what the intention was in terms of when we'd change it.;;;","03/Feb/15 18:22;vanzin;HI [~pwendell],

Let me try to write a point-by-point feedback for the current spec.

h4. Public protocol or not?

If this is not supposed to be public (i.e., we don't expect someone to try to directly try to talk to the Spark master, it's always going to happen through the Spark libraries), then the underlying protocol is less important, since we only care about different versions being compatible in some way.

Assuming a non-public protocol, my question would be: why implement your own RPC framework? Why not reuse something that's already there? For example, Avro has a stable serialization infrastructure that defines semantics for versioned data, and works well on top of HTTP. If handles serialization and dispatching - which would remove a lot of code from the current patch, and probably has other features that the current, ""cluster-mode"" only protocol doesn't need but other future uses might.

h4. Non-submission uses

Similarly, in the non-public protocol scenario, a proper REST-based API would look like overkill. But a proper REST infrastructure provides interesting room for growth of the master's public-facing API. For example, you could easily expose an endpoint for listing the current applications being tracked by the master, or an endpoint to kill an application. The former could benefit, also, the history server, which could expose the same API to list the applications it has found.

h4. Evolvability and Versioning

The current spec does not specify the behavior of the cluster nor the client with regards to different versions of the protocol. It has a table that basically says ""future versions need to be able to submit standalone cluster applications to a 1.3 master"", but it doesn't explain what that means or how that happens.

Does it mean that, after 1.3, you can't ever change any of the messages used to launch a standalone cluster app, nor can you add new messages? Or, if that's allowed, what happens on the server side if it sees a field it doesn't understand? Does it ignore it, which could potentially break the application being submitted? Does it throw an error, in which case the client should make sure to submit an older version of the data structures if that's compatible with the app being submitted? If the latter, how does it know which version to use?

As an example of how you could do this ""negotiation"": the client checks what features the app being submitted needs, and chooses the oldest supported api version based on that. It then can submit the request to, e.g., ""/v2"" and, if submitting to a 1.3 cluster, it will fail, because it doesn't support the features needed by that app.

Also, thinking about the framework, what if later you need different features than the ones provided now? What if you need to use query params, path params, or non-json request bodies (e.g. for uploading files)? Are you gonna extend the current framework to the point where it starts looking like other existing ones?

Of, if HTTP is being used mostly as a dumb pipe, what are the semantics for when something goes wrong? Should clients only bother about a response if the status is ""200 OK"", or should they try to interpret the body of a ""500 Internal Server Error"" message or ""401 Bad Request""? Those things need to be specified.

h4. Others

If the suggestions above don't sound particularly interesting for this use case, I'd strongly suggest, in the very least, removing any mention of REST from the spec and the code, because this is not a REST protocol in any way.

Also, a question: if it's an HTTP protocol, why not expose it through the existing http port?


To reply to the questions about my suggestions for how to use REST:

* when you add a new version, you don't remove old ones. Spark v1.4 could add ""/v2"", but it must still support ""/v1"" in the way that it was specified.
* as for new fields / types, that really depends on how you specify things. Personally, I like to declare a released API ""frozen"": you can't add new types, fields, or anything that the old release doesn't know about. Any new thing requires a new protocol version. But you could take a different approach, by adding optional fields that don't cause breakages when submitted to the old server that doesn't know about them. Again, these choices need to be specified up front, otherwise the implementation of v1 becomes the spec, since where the spec is not clear, the choices made by the implementation will become a de facto specification.

(BTW, especially with a v1, the implementation will invariably become a ""de facto"" specification, that's unavoidable. But it helps to have the spec clearly cover all areas, so that hopefully you don't need to reverse-engineer the implementation code to figure out how things work.)

Anyway, hope this is useful and clarifies some of the concerns I have about the current spec.;;;","03/Feb/15 21:17;pwendell;The intention for this is really just to take single RPC that was using Akka and add a stable version of it that we are okay supporting long term. It doesn't preclude moving to avro or some other RPC as a general thing we use across all of Spark. However, that design choice was intentionally excluded from this decision given all the complexities you bring up. Doing some basic message dispatching on our own - there is only a small and very straightforward code related to this. Adopting Avro would be overkill for this.

In the current implementation the client and server exchange Spark versions, so this is the basis of reasoning about version changes - maybe it wasn't in the design doc. In terms of evolvability, the way you do this is that you only add new functionality over time, and you never remove fields from messages. This is similar to the API contract of the history logs with the history server. So the idea is that newer clients would implement a super set of messages and fields as older ones.

Adding v1 seems like a good idea in case this evolves into something public or more well specified over time. It would just be good to define precisely what it means to advance that version identifier. That all matters a lot more if we want it to be something others interact with.;;;","03/Feb/15 21:36;vanzin;Hi Patrick,

Most of my questions are related to the protocol specification attached to this bug. So when I ask about something, I generally mean that the specification is vague about that. If the implementation made a choice about that thing, it just means that the implementation should be the specification, and everybody should just ignore the document attached to this bug. And we can then move the discussion to the PR itself.

bq.  The intention for this is really just to take single RPC that was using Akka and add a stable version of it that we are okay supporting long term. 

That's fine, but I'd really like the spec to actually be very clear about what this means. For example, the very last sentence:

bq. n. This set of fields must remain compatible across Spark version

See my previous comment, where I asked the same question: what does that mean? Does that mean that you can never add any fields to existing messages? You mention the code does some version negotiation, but the spec doesn't mention that. So maybe that negotiation is the answer to my question?

Anyway, I'm just a little concerned that there's still some vagueness in the spec, for a protocol that is supposed to be stable from the get go.

;;;","05/Feb/15 03:01;andrewor14;Hi [~vanzin], thank you for all of your comments. I agree with many of the points that you raised and have addressed them in the PR. I'd just like to clarify, in case there is any confusion, that this is intended to be an internal submission gateway only for standalone cluster mode. This is in no way expected to interface with third party applications or intended to be a stable alternative to Spark submit across all cluster managers and deploy modes

For this reason, the protocol has very few core requirements, the most important of which, backward and forward compatibility, is sufficiently handled by using any stable RPC framework. It's just so that we chose a HTTP/JSON-based design because it is the simplest one that fulfills this requirement. The initial design did not adhere closely to conventions commonly used by other REST protocols because this was not strictly necessary to achieve the compatibility guarantees we wanted to provide. Not to mention there is not really a formal W3C-style REST specification, and many existing REST protocols out there (e.g. twitter, flickr, github) actually diverge widely in terms of the extent to which they adherence to these vaguely-defined conventions.

That said, I agree completely that the initial design doc was vague. I have updated it by expanding in much greater detail on things like how the server responds to different error conditions and how versioning is handled. Please have a look and let me know if you have any other concerns.;;;","05/Feb/15 09:24;tigerquoll;Heh Andrew, definitely starting to look a bit more Rest-like in the protocol!

Http Delete should be used for your kill request - it is considered best practice

The primary resource you are dealing with is a submission - this should form the base of your url structure.
For a rest protocol, actions/verbs are used to affect these resources - so they are mapped to to the HTTP operations of GET/POST/DELETE/HEAD/OPTIONS etc, against the resources defined by the full url.

Full URLs serve to identify the resources that these actions are performed on. GET/DELETE are used where the full identity of the resource is known at the time of generating the request, POST is used when you may not know the address of the resource at the time of generating the request (eg When submitting a program to run, you will not know submission id because it is returned by the request)

So, taking this into account:
RequestSubmitDriver → POST /submission/create
RequestKillDriver → DELETE /submission/[submissionId]
RequestDriverStatus → GET /submission/[submissionId]/status  - The resource is the submission, so the current status of the submission in a sub-resource of the submission, other sub entries such as 
/submission/[submissionId]/performanceCounters 
could be added in the future without affecting existing clients.


;;;","05/Feb/15 23:40;vanzin;Hi [~andrewor14],

Thanks for updating the spec! This one looks much, much better. I think most of my concerns have been addressed. Adherence to ""RESTfulness"" is not super important since this is an internal API, although I really would suggest picking a better name for the Scala package (e.g. ""org.apache.spark.deploy.proto"" or something, instead of ""rest"").

A few questions:

- is the ""action"" field required? Since you have different URIs handling different messages, it seems redundant now. And responses having an ""action"" is kinda weird.
- what is the ""protocolVersion"" field in ErrorResponse? From the spec, it sounds like the maximum protocol version supported by the server. If that's the case, can the property be renamed to ""maxProtocolVersion""?
- the message definitions use strings for all data, is that intentional? It would feel more natural to have proper types, e.g.: ""jars"" : [ ""one.jar"", ""two.jar"" ], driverCores: 8, superviseDriver: false.
- The spec says the server should report unknown fields back to the client. There's nothing in the response type that supports that; is the server expected to embed that information in the ""message"" field? Feels like it would be better to have an explicit field for that.
- Is the ""kill"" endpoint protected in any way? Right now it seems like anyone can post to that and kill a driver, if they know (or guess) the submission ID. If there's no special protection, I'd say in the spec that the submission ID should be, at least, cryptographically secure. At that point, as long as the server has SSL enabled, it should be hard enough to kill a random driver.
;;;","05/Feb/15 23:42;vanzin;Also, a fun fact about the Jersey dependency. Here's an excerpt of the output of ""mvn dependency:tree"" for the yarn module:

{noformat}
[INFO] +- org.apache.hadoop:hadoop-yarn-common:jar:2.4.0:compile
[INFO] |  +- javax.xml.bind:jaxb-api:jar:2.2.2:compile
[INFO] |  |  +- javax.xml.stream:stax-api:jar:1.0-2:compile
[INFO] |  |  \- javax.activation:activation:jar:1.1:compile
[INFO] |  +- org.apache.commons:commons-compress:jar:1.4.1:compile
[INFO] |  |  \- org.tukaani:xz:jar:1.0:compile
[INFO] |  +- commons-codec:commons-codec:jar:1.5:compile
[INFO] |  +- com.sun.jersey:jersey-core:jar:1.9:compile
{noformat}
;;;","06/Feb/15 04:43;andrewor14;[~tigerquoll] I still don't think we should use DELETE for kill for the following reason. In normal REST servers that host static resources, if you GET after a DELETE, you run into a 404. Here, our resources are by no means static, and if you GET after a DELETE you actually get a different status (that your driver is now KILLED instead of RUNNING) instead. Because of these side-effects I think it is safest to use POST.

[~vanzin]
- The action field is actually required especially since many of the responses look quite alike. We need to know how to deserialize the messages safely in case the response we get from the server is not the type that we expect it to be (e.g. ErrorResponse).
- Yes, I could rename the protocolVersion field.
- The issue with having non-String types is that you will need to deal with numeric and boolean values specially. For instance, if the user does not explicitly set the field there is no easy way to not include it in the JSON without doing some Option hack. I went down that route and opted out for simpler code.
- The unknown fields reporting is added in the PR but is missing in the spec. In the PR it is reported in its own explicit field.
- Even in the existing interface you can use o.a.s.deploy.Client to kill an application, and the security guarantees there are the same. I agree that it is something we need to address that at some point, but I prefer to keep that outside the scope of this patch.;;;","06/Feb/15 04:47;andrewor14;By the way for the more specific comments it would be good if you can leave them on the PR itself: https://github.com/apache/spark/pull/4216. The specs and the actual code will diverge after some review so the most up-to-date version will likely be there.;;;","06/Feb/15 05:18;pwendell;I think it's reasonable to use DELETE per [~tigerquoll]'s suggestion. It's not a perfect match with DELETE semantics, but I think it's fine to use it if it's not too much work. I also think calling it maxProtocolVersion is a good idea if those are indeed the semantics. For security, yeah the killing is the same as it is in the current mode, which is that there is no security. One thing we could do if there is user demand is add a flag that globally disables killing, but let's see if users request this first.;;;","06/Feb/15 18:42;vanzin;I kinda agree with Andrew regarding delete. You're not deleting the submission, you're killing it. It will still exist afterwards, in a different state.

bq. The action field is actually required especially since many of the responses look quite alike. We need to know how to deserialize the messages

I thought each endpoint could only return a single type. So the handler for a particular endpoint knows what the types of the request / response are without having to include that information in the JSON payload itself.

bq. The issue with having non-String types is that you will need to deal with numeric and boolean values specially.

{{java.lang.Boolean}}, {{java.lang.Long}} and friends are nullable. Another reason why I like to use POJOs (instead of Scala classes) when using Jackson.;;;","06/Feb/15 20:06;pwendell;On DELETE, I'll defer to you guys, have zero strong feelings either way.;;;","06/Feb/15 20:23;pwendell;One the boolean and numeric values. I don't mind one way or the other how they are handled programmatically (since we are not exposing this). However, it does seem weird that in the wire protocol defines these as string types. I looked at a few other API's, Github, Twitter, etc and they all use proper boolean types. So I'd definitely recommend setting them as proper types in the JSON, and if that's easier to do by making them nullable Boolean and Long values, seems like a good approach.;;;","06/Feb/15 22:40;andrewor14;Great nullable booleans and numbers are all I'm looking for. Thanks.;;;","28/Apr/15 18:07;jeromatron;Would people be amenable to having additional features for this such as a shared context and others described in SPARK-818 done by [~velvia]?;;;",,,,,,,,,
"Calling textFile, parallelize, zip, then partitions causes failure on some local[*]",SPARK-5385,12769671,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,pedrorodriguez,pedrorodriguez,23/Jan/15 16:41,23/Jan/15 18:04,14/Jul/23 06:27,23/Jan/15 18:02,,,,,,,,,,,,,,,,,0,,,,,,"There is a bug in Spark core which produces the exception: ""Can't zip RDDs with unequal numbers of partitions""

General Steps to reproduce:
1. Run sc.textFiles
2. Run sc.parallelize
3. Zip results of top two
4. Call partitions on result of zip
5. Run for local, local[2], local[3],...
6. My machine (macbook air) fails on local[3].

Github repository with code example: https://github.com/EntilZha/spark-zip-bug
Steps to run: execute ""sbt run"", wait for failure

Stack trace:
java.lang.IllegalArgumentException: Can't zip RDDs with unequal numbers of partitions
	at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
	at App$.main(App.scala:33)
	at App.main(App.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)

I am looking into the relevant classes, but insight would be appreciated. This ticket may also be related to https://issues.apache.org/jira/browse/SPARK-2823 and https://issues.apache.org/jira/browse/SPARK-5351",,pedrorodriguez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 23 18:04:02 UTC 2015,,,,,,,,,,"0|i24q9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/15 17:46;srowen;Based on this description, this does not sound like a bug. The error means what it says: you can't zip RDDs unless each partition has an equal number of elements. I don't see a reason to expect that an arbitrary call to textFile and parallelize meets this criterion. It may happen to work, depending on what your data and program does, but you haven't specified it.;;;","23/Jan/15 17:57;pedrorodriguez;Perhaps its not a bug then, if so, then what would be the best way to insure that they have the same number of partitions?;;;","23/Jan/15 17:59;srowen;The number of partitions is not the problem, although several methods (including coalesce and repartition) let you change the number of partitions. The issue is that the partitions do not all have the same number of elements as each other.;;;","23/Jan/15 18:02;pedrorodriguez;Indeed, not a bug, fixed by calling textFiles, then passing partitions.size as parameter to parallelize for number of slices.;;;","23/Jan/15 18:04;pedrorodriguez;On your prior comment, I know (deterministically) that the elements of the RDD are of the same size, they were just being partitioned differently so partitions would have non-equal sizes/different number of partitions.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Vectors.sqdist return inconsistent result for sparse/dense vectors when the vectors have different lengths,SPARK-5384,12769669,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,23/Jan/15 16:30,27/Jan/15 00:05,14/Jul/23 06:27,26/Jan/15 06:18,1.3.0,,,,,,1.3.0,,,,,,MLlib,,,,0,,,,,,"For two vectors of different lengths, Vectors.sqdist would return different result when the vectors are represented as sparse and dense respectively. Sample:   
    val s1 = new SparseVector(4, Array(0,1,2,3), Array(1.0, 2.0, 3.0, 4.0))
    val s2 = new SparseVector(1, Array(0), Array(9.0))
    val d1 = new DenseVector(Array(1.0, 2.0, 3.0, 4.0))
    val d2 = new DenseVector(Array(9.0))
    println(s1 == d1 && s2 == d2)
    println(Vectors.sqdist(s1, s2))
    println(Vectors.sqdist(d1, d2))
result:
     true
     93.0
     64.0

More precisely, for the extra part, Vectors.sqdist would include it for sparse vectors and exclude it for dense vectors. I'll send a PR and we can have more detailed discussion there.
","centos, others should be similar",apachespark,mengxr,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 26 06:21:42 UTC 2015,,,,,,,,,,"0|i24q9j:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"23/Jan/15 16:32;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4183;;;","26/Jan/15 06:18;mengxr;Issue resolved by pull request 4183
[https://github.com/apache/spark/pull/4183];;;","26/Jan/15 06:21;yuhaoyan;fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Scripts do not use SPARK_CONF_DIR where they should,SPARK-5382,12769610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,jlewandowski,jlewandowski,23/Jan/15 10:20,28/Jan/15 06:16,14/Jul/23 06:27,26/Jan/15 07:41,1.2.0,1.3.0,,,,,1.2.1,1.3.0,,,,,Spark Core,,,,0,,,,,,"h5.spark-class:
{code}
# Load extra JAVA_OPTS from conf/java-opts, if it exists
if [ -e ""$FWDIR/conf/java-opts"" ] ; then
  JAVA_OPTS=""$JAVA_OPTS `cat ""$FWDIR""/conf/java-opts`""
fi
{code}

h5.spark-submit:
{code}
DEFAULT_PROPERTIES_FILE=""$SPARK_HOME/conf/spark-defaults.conf""
{code}

",,apachespark,bcantoni,jiml,jlewandowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,SPARK-4616,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 23 10:40:38 UTC 2015,,,,,,,,,,"0|i24pwn:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,,,"23/Jan/15 10:31;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4177;;;","23/Jan/15 10:38;apachespark;User 'jacek-lewandowski' has created a pull request for this issue:
https://github.com/apache/spark/pull/4179;;;","23/Jan/15 10:40;jlewandowski;[~joshrosen] is it something which you could merge before releasing 1.2.1? The patch SPARK-4616 was incomplete and didn't address branch 1.2. 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
There will be an ArrayIndexOutOfBoundsException if the format of the source file is wrong,SPARK-5380,12769584,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LiuHao,LiuHao,LiuHao,23/Jan/15 08:32,12/Feb/15 04:43,14/Jul/23 06:27,06/Feb/15 09:05,1.2.0,,,,,,1.3.0,1.4.0,,,,,GraphX,,,,0,,,,,,"When I build a graph with a file format error, there will be an ArrayIndexOutOfBoundsException",,apachespark,LiuHao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 09:05:48 UTC 2015,,,,,,,,,,"0|i24prb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/15 09:34;apachespark;User 'Leolh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4176;;;","06/Feb/15 09:05;srowen;I'm manually marking this fixed since the merge script wasn't able to do it for me, as I didn't have the right Python lib installed. Should work automatically next time.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
 literal in agg grouping expressioons leads to incorrect result,SPARK-5373,12769523,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,23/Jan/15 02:52,24/Jul/15 04:23,14/Jul/23 06:27,29/Jan/15 23:47,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"select key, count( * ) from src group by key, 1 will get the wrong answer!",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jul 24 04:23:09 UTC 2015,,,,,,,,,,"0|i24pdr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"23/Jan/15 03:04;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/4169;;;","29/Jan/15 23:47;marmbrus;Issue resolved by pull request 4169
[https://github.com/apache/spark/pull/4169];;;","24/Jul/15 04:23;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/7583;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Failure to analyze query with UNION ALL and double aggregation,SPARK-5371,12769489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,marmbrus,dyross,dyross,23/Jan/15 00:26,31/Mar/15 18:44,14/Jul/23 06:27,31/Mar/15 18:44,1.2.0,1.3.0,,,,,1.3.1,1.4.0,,,,,SQL,,,,2,,,,,,"This SQL session:

{code}
DROP TABLE
    test1;
DROP TABLE
    test2;
CREATE TABLE
    test1
    (
        c11 INT,
        c12 INT,
        c13 INT,
        c14 INT
    );
CREATE TABLE
    test2
    (
        c21 INT,
        c22 INT,
        c23 INT,
        c24 INT
    );
SELECT
    MIN(t3.c_1),
    MIN(t3.c_2),
    MIN(t3.c_3),
    MIN(t3.c_4)
FROM
    (
        SELECT
            SUM(t1.c11) c_1,
            NULL        c_2,
            NULL        c_3,
            NULL        c_4
        FROM
            test1 t1
        UNION ALL
        SELECT
            NULL        c_1,
            SUM(t2.c22) c_2,
            SUM(t2.c23) c_3,
            SUM(t2.c24) c_4
        FROM
            test2 t2 ) t3; 
{code}

Produces this error:

{code}
15/01/23 00:25:21 INFO thriftserver.SparkExecuteStatementOperation: Running query 'SELECT
    MIN(t3.c_1),
    MIN(t3.c_2),
    MIN(t3.c_3),
    MIN(t3.c_4)
FROM
    (
        SELECT
            SUM(t1.c11) c_1,
            NULL        c_2,
            NULL        c_3,
            NULL        c_4
        FROM
            test1 t1
        UNION ALL
        SELECT
            NULL        c_1,
            SUM(t2.c22) c_2,
            SUM(t2.c23) c_3,
            SUM(t2.c24) c_4
        FROM
            test2 t2 ) t3'
15/01/23 00:25:21 INFO parse.ParseDriver: Parsing command: SELECT
    MIN(t3.c_1),
    MIN(t3.c_2),
    MIN(t3.c_3),
    MIN(t3.c_4)
FROM
    (
        SELECT
            SUM(t1.c11) c_1,
            NULL        c_2,
            NULL        c_3,
            NULL        c_4
        FROM
            test1 t1
        UNION ALL
        SELECT
            NULL        c_1,
            SUM(t2.c22) c_2,
            SUM(t2.c23) c_3,
            SUM(t2.c24) c_4
        FROM
            test2 t2 ) t3
15/01/23 00:25:21 INFO parse.ParseDriver: Parse Completed
15/01/23 00:25:21 ERROR thriftserver.SparkExecuteStatementOperation: Error executing query:
java.util.NoSuchElementException: key not found: c_2#23488
	at scala.collection.MapLike$class.default(MapLike.scala:228)
	at org.apache.spark.sql.catalyst.expressions.AttributeMap.default(AttributeMap.scala:29)
	at scala.collection.MapLike$class.apply(MapLike.scala:141)
	at org.apache.spark.sql.catalyst.expressions.AttributeMap.apply(AttributeMap.scala:29)
	at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$1.applyOrElse(Optimizer.scala:77)
	at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$1.applyOrElse(Optimizer.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:135)
	at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$.pushToRight(Optimizer.scala:76)
	at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$apply$1$$anonfun$applyOrElse$6.apply(Optimizer.scala:98)
	at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$apply$1$$anonfun$applyOrElse$6.apply(Optimizer.scala:98)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$apply$1.applyOrElse(Optimizer.scala:98)
	at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$apply$1.applyOrElse(Optimizer.scala:85)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:162)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:191)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:147)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:135)
	at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$.apply(Optimizer.scala:85)
	at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$.apply(Optimizer.scala:59)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:34)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:462)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:462)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:467)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:465)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:471)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:471)
	at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:463)
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:178)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
	at sun.reflect.GeneratedMethodAccessor61.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatement(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:220)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/01/23 00:25:22 WARN thrift.ThriftCLIService: Error executing statement:
org.apache.hive.service.cli.HiveSQLException: java.util.NoSuchElementException: key not found: c_2#23488
	at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.run(Shim13.scala:189)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:231)
	at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:212)
	at sun.reflect.GeneratedMethodAccessor61.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)
	at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:37)
	at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:64)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
	at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:493)
	at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:60)
	at com.sun.proxy.$Proxy18.executeStatement(Unknown Source)
	at org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:220)
	at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:344)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)
	at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:55)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
{code}

Some similar queries work. For example:

{code}
SELECT
    MIN(t3.c_1),
    MIN(t3.c_2),
    MIN(t3.c_3),
    MIN(t3.c_4)
FROM
    (
        SELECT
            SUM(t1.c11) c_1,
            SUM(t1.c12) c_2,
            SUM(t1.c13) c_3,
            SUM(t1.c14) c_4
        FROM
            test1 t1
        UNION ALL
        SELECT
            SUM(t2.c21) c_1,
            SUM(t2.c22) c_2,
            SUM(t2.c23) c_3,
            SUM(t2.c24) c_4
        FROM
            test2 t2 ) t3; 
{code}

Works fine. Notice the only difference is the {{null}}.",,apachespark,dyross,glenn.strycker@gmail.com,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 31 18:44:07 UTC 2015,,,,,,,,,,"0|i24p67:",9223372036854775807,,,,,,,,,,,,,,1.3.1,,,,,,,,,,,,,"31/Mar/15 02:30;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/5278;;;","31/Mar/15 18:44;marmbrus;Issue resolved by pull request 5278
[https://github.com/apache/spark/pull/5278];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove some unnecessary synchronization in YarnAllocator,SPARK-5370,12769366,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sandyr,sandyr,sandyr,22/Jan/15 17:01,17/Feb/15 20:56,14/Jul/23 06:27,22/Jan/15 19:50,1.3.0,,,,,,1.3.0,,,,,,,,,,0,,,,,,,,apachespark,sandyr,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5369,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 22 17:05:59 UTC 2015,,,,,,,,,,"0|i24ofz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/15 17:05;apachespark;User 'sryza' has created a pull request for this issue:
https://github.com/apache/spark/pull/4164;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
support star expression in udf,SPARK-5367,12769327,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,scwf,scwf,22/Jan/15 15:23,04/Feb/15 03:46,14/Jul/23 06:27,29/Jan/15 23:45,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"now spark sql does not support star expression in udf, the following sql will get error
```
select concat( * ) from src
```
",,apachespark,marmbrus,scwf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 03:46:57 UTC 2015,,,,,,,,,,"0|i24o7b:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"22/Jan/15 15:34;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/4163;;;","29/Jan/15 23:45;marmbrus;Issue resolved by pull request 4163
[https://github.com/apache/spark/pull/4163];;;","04/Feb/15 03:46;apachespark;User 'scwf' has created a pull request for this issue:
https://github.com/apache/spark/pull/4353;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Spark 1.2 freeze without error notification,SPARK-5363,12769186,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,TJKlein,TJKlein,22/Jan/15 02:49,25/May/15 15:52,14/Jul/23 06:27,26/Feb/15 19:59,1.2.0,1.2.1,1.3.0,,,,1.2.2,1.3.0,1.4.0,,,,PySpark,,,,0,,,,,,"After a number of calls to a map().collect() statement Spark freezes without reporting any error.  Within the map a large broadcast variable is used.

The freezing can be avoided by setting 'spark.python.worker.reuse = false' (Spark 1.2) or using an earlier version, however, at the prize of low speed. ",,apachespark,cretz,davies,donnchadh,idanzalz,joshrosen,nchammas,sb58,TJKlein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3030,SPARK-3993,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon May 25 15:52:39 UTC 2015,,,,,,,,,,"0|i24nd3:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"06/Feb/15 17:38;davies;I tried it with hundreds of iterations (big broadcast and collect()), can not reprodduce it, will try it again in the QA of 1.3.;;;","08/Feb/15 01:16;nchammas;[~TJKlein] - Can you provide more information about the environment in which you see this error? Can you also come up with a simple repro script?;;;","08/Feb/15 02:04;TJKlein;Unfortunately the script I have at hand is a bit complicated and there is no simple way to make it simpler. There is a bunch of other people in the nabble forum having the same issue. Maybe they have a small script to try it?;;;","13/Feb/15 23:41;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4601;;;","13/Feb/15 23:42;davies;[~TJKlein] Could you try the patch in https://github.com/apache/spark/pull/4601 whether it fix your problem?;;;","16/Feb/15 18:24;TJKlein;Sure, I will it a try shortly. I will let you know.;;;","17/Feb/15 03:54;TJKlein;I tried the patch. Unfortunately, it still freezes.;;;","17/Feb/15 04:38;joshrosen;I've merged Davies' patch for 1.3.0 and 1.2.2; let's hope that this resolves the issue.;;;","17/Feb/15 08:09;TJKlein;I am a bit surprised that you guys set it to 'resolved' as it is not.;;;","17/Feb/15 08:26;davies;not fixed;;;","17/Feb/15 08:26;davies;It's marked as `solved` before saw your comment, it's a mistake.;;;","26/Feb/15 00:52;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4776;;;","26/Feb/15 00:54;davies;[~TJKlein] Could you try to new patch? Hopefully it will be fixed.;;;","26/Feb/15 19:59;joshrosen;I've merged https://github.com/apache/spark/pull/4776, which fixes one of our reproductions of this issue (our job added and removed broadcast variables in a way that might trigger this bug fixed by that patch).  Therefore, I'm going to mark this issue as ""Resolved"", but please comment here if you still observe the issue after this latest patch.;;;","26/Feb/15 21:38;TJKlein;Sure, I will have a look and try it out.;;;","25/May/15 15:52;idanzalz;Can't prove it's related to the same issue but we have been experiencing hangs with BroadcastHashJoin, even though we use the scala API.

I was unable to create a simple repro, but in a complicated sql statement that contains multiple tables, joined with BroadcastHashJoin. 
Calling ""collect"" on the RDD, causes the spark context to hang.;;;",,,,,,,,,,,,,
Multiple Java RDD <-> Python RDD conversions not working correctly,SPARK-5361,12769139,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wingchen,wingchen,wingchen,21/Jan/15 22:38,30/Apr/15 13:34,14/Jul/23 06:27,17/Feb/15 00:52,1.2.0,,,,,,1.2.2,1.3.0,,,,,PySpark,,,,0,,,,,,"This is found through reading RDD from `sc.newAPIHadoopRDD` and writing it back using `rdd.saveAsNewAPIHadoopFile` in pyspark.

It turns out that whenever there are multiple RDD conversions from JavaRDD to PythonRDD then back to JavaRDD, the exception below happens:

{noformat}
15/01/16 10:28:31 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 7)
java.lang.ClassCastException: [Ljava.lang.Object; cannot be cast to java.util.ArrayList
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:157)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:153)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
{noformat}

The test case code below reproduces it:

{noformat}
from pyspark.rdd import RDD

dl = [
    (u'2', {u'director': u'David Lean'}), 
    (u'7', {u'director': u'Andrew Dominik'})
]

dl_rdd = sc.parallelize(dl)
tmp = dl_rdd._to_java_object_rdd()
tmp2 = sc._jvm.SerDe.javaToPython(tmp)
t = RDD(tmp2, sc)
t.count()

tmp = t._to_java_object_rdd()
tmp2 = sc._jvm.SerDe.javaToPython(tmp)
t = RDD(tmp2, sc)
t.count() # it blows up here during the 2nd time of conversion
{noformat}",,apachespark,joshrosen,wingchen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 00:52:17 UTC 2015,,,,,,,,,,"0|i24n3b:",9223372036854775807,,,,,,,,,,,,,,1.2.2,,,,,,,,,,,,,"21/Jan/15 22:43;apachespark;User 'wingchen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4146;;;","22/Jan/15 22:22;wingchen;Found a good way to reproduce it:

{noformat}
from pyspark.rdd import RDD

dl = [
    (u'2', {u'director': u'David Lean'}), 
    (u'7', {u'director': u'Andrew Dominik'})
]

dl_rdd = sc.parallelize(dl)
tmp = dl_rdd._to_java_object_rdd()
tmp2 = sc._jvm.SerDe.javaToPython(tmp)
t = RDD(tmp2, sc)
t.count()

tmp = t._to_java_object_rdd()
tmp2 = sc._jvm.SerDe.javaToPython(tmp)
t = RDD(tmp2, sc)
t.count() # it blows up here during the 2nd time of conversion
{noformat}

I am going to make a test case from this example.;;;","28/Jan/15 19:10;joshrosen;This has been fixed by https://github.com/apache/spark/pull/4146 in 1.3.0.  I'd also like to backport this to {{branch-1.2}}, but I'm not doing that right away since we're voting on a 1.2.1 RC right now.  I've added the {{backport-needed}} label and I'll merge this to {{branch-1.2}} as soon as 1.2.1 is released.;;;","17/Feb/15 00:52;joshrosen;I've cherry-picked the fix into `branch-1.2` (1.2.2), so I'm marking this as Fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"For CoGroupedRDD, rdds for narrow dependencies and shuffle handles are included twice in serialized task",SPARK-5360,12769128,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kayousterhout,kayousterhout,kayousterhout,21/Jan/15 21:52,21/Apr/15 18:02,14/Jul/23 06:27,21/Apr/15 18:02,1.2.0,,,,,,1.4.0,,,,,,Spark Core,,,,0,,,,,,"CoGroupPartition, part of CoGroupedRDD, includes references to each RDD that the CoGroupedRDD narrowly depends on, and a reference to the ShuffleHandle.  The partition is serialized separately from the RDD, so when the RDD and partition arrive on the worker, the references in the partition and in the RDD no longer point to the same object.

This is a relatively minor performance issue (the closure can be 2x larger than it needs to be because the rdds and partitions are serialized twice; see numbers below) but is more annoying as a developer issue (this is where I ran into): if any state is stored in the RDD or ShuffleHandle on the worker side, subtle bugs can appear due to the fact that the references to the RDD / ShuffleHandle in the RDD and in the partition point to separate objects.  I'm not sure if this is enough of a potential future problem to fix this old and central part of the code, so hoping to get input from others here.

I did some simple experiments to see how much this effects closure size.  For this example: 
$ val a = sc.parallelize(1 to 10).map((_, 1))
$ val b = sc.parallelize(1 to 2).map(x => (x, 2*x))
$ a.cogroup(b).collect()

the closure was 1902 bytes with current Spark, and 1129 bytes after my change.  The difference comes from eliminating duplicate serialization of the shuffle handle.

For this example:
$ val sortedA = a.sortByKey()
$ val sortedB = b.sortByKey()
$ sortedA.cogroup(sortedB).collect()

the closure was 3491 bytes with current Spark, and 1333 bytes after my change. Here, the difference comes from eliminating duplicate serialization of the two RDDs for the narrow dependencies.

The ShuffleHandle includes the ShuffleDependency, so this difference will get larger if a ShuffleDependency includes a serializer, a key ordering, or an aggregator (all set to None by default).  However, the difference is not affected by the size of the function the user specifies, which (based on my understanding) is typically the source of large task closures.",,apachespark,kayousterhout,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 21 22:04:21 UTC 2015,,,,,,,,,,"0|i24n0v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/15 22:04;apachespark;User 'kayousterhout' has created a pull request for this issue:
https://github.com/apache/spark/pull/4145;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade from commons-codec 1.5,SPARK-5357,12769104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,matt.whelan,MattWhelan,MattWhelan,21/Jan/15 20:54,16/Feb/15 23:06,14/Jul/23 06:27,16/Feb/15 23:06,1.1.0,1.2.0,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"Spark uses commons-codec 1.5, which has a race condition in Base64.  That race was introduced in commons-codec 1.4 and resolved in 1.7.  The current version of commons-codec is 1.10.

Code that runs in Workers and assumes that Base64 is thread-safe will break because spark is using a non-thread-safe version.  See CODEC-96

In addition, the spark.files.userClassPathFirst mechanism is currently broken, (bug to come), so there isn't a viable work around for this issue.",,apachespark,MattWhelan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 16 23:06:02 UTC 2015,,,,,,,,,,"0|i24mvj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jan/15 01:23;apachespark;User 'MattWhelan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4153;;;","16/Feb/15 23:06;srowen;Issue resolved by pull request 4153
[https://github.com/apache/spark/pull/4153];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkConf is not thread-safe,SPARK-5355,12769086,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,21/Jan/15 20:03,09/Feb/15 05:58,14/Jul/23 06:27,22/Jan/15 00:52,1.2.0,1.3.0,,,,,1.2.1,1.3.0,,,,,Spark Core,,,,0,,,,,,"The SparkConf is not thread-safe, but is accessed by many threads. The getAll() could return parts of the configs if another thread is access it.",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 26 16:42:54 UTC 2015,,,,,,,,,,"0|i24mrj:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,,,"21/Jan/15 20:06;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4143;;;","22/Jan/15 00:52;joshrosen;Issue resolved by pull request 4143
[https://github.com/apache/spark/pull/4143];;;","26/Jan/15 16:42;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4208;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Can't zip RDDs with unequal numbers of partitions in ReplicatedVertexView.upgrade(),SPARK-5351,12769004,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,maropu,maropu,21/Jan/15 15:44,13/Feb/15 00:37,14/Jul/23 06:27,24/Jan/15 03:36,,,,,,,1.2.1,1.3.0,,,,,GraphX,,,,0,,,,,,"If the value of 'spark.default.parallelism' does not match the number of partitoins in EdgePartition(EdgeRDDImpl), 
the following error occurs in ReplicatedVertexView.scala:72;

object GraphTest extends Logging {
def run[VD: ClassTag, ED: ClassTag](graph: Graph[VD, ED]): VertexRDD[Int] = {
graph.aggregateMessages[Int](
ctx => {
ctx.sendToSrc(1)
ctx.sendToDst(2)
},
_ + _)
}
}

val g = GraphLoader.edgeListFile(sc, ""graph.txt"")
val rdd = GraphTest.run(g)

java.lang.IllegalArgumentException: Can't zip RDDs with unequal numbers of partitions
	at org.apache.spark.rdd.ZippedPartitionsBaseRDD.getPartitions(ZippedPartitionsRDD.scala:57)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:206)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:204)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:206)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:204)
	at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:82)
	at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:80)
	at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:193)
	at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:191)
    ...
",,ankurd,apachespark,maropu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2823,,,SPARK-5790,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 24 03:36:34 UTC 2015,,,,,,,,,,"0|i24m9z:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/15 16:19;apachespark;User 'maropu' has created a pull request for this issue:
https://github.com/apache/spark/pull/4136;;;","24/Jan/15 03:36;ankurd;Issue resolved by pull request 4136
https://github.com/apache/spark/pull/4136;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.deploy.history.FsHistoryProviderSuite,SPARK-5345,12768903,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sarutak,sarutak,sarutak,21/Jan/15 07:30,06/Mar/15 08:00,14/Jul/23 06:27,13/Feb/15 18:31,1.3.0,,,,,,1.3.0,,,,,,Deploy,Web UI,,,0,flaky-test,,,,,"In FsHistoryProviderSuite, a test ""Parse new and old application logs"" sometimes fail and sometimes succeed. It's unstable.",,andrewor14,apachespark,joshrosen,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 19 02:22:16 UTC 2015,,,,,,,,,,"0|i24lnr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"21/Jan/15 07:37;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/4133;;;","13/Feb/15 18:31;joshrosen;It looks like this has been fixed by SPARK-5600, so I'm going to resolve this for now.  Let's re-open if the test becomes flaky again.;;;","19/Feb/15 02:22;andrewor14;The real fix should be here: https://github.com/apache/spark/pull/4222;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
HistoryServer cannot recognize that inprogress file was renamed to completed file,SPARK-5344,12768892,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,21/Jan/15 06:46,25/Jan/15 23:40,14/Jul/23 06:27,25/Jan/15 23:40,1.3.0,,,,,,1.3.0,,,,,,Web UI,,,,0,,,,,,"FsHistoryProvider tries to update application status but if checkForLogs is called before .inprogress file is renamed to completed file, the file is not recognized as completed.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 21 06:49:02 UTC 2015,,,,,,,,,,"0|i24llb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"21/Jan/15 06:49;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/4132;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
ShortestPaths traverses backwards,SPARK-5343,12768872,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,boyork,michaelmalak,michaelmalak,21/Jan/15 04:20,12/Feb/15 04:33,14/Jul/23 06:27,10/Feb/15 23:02,1.2.0,,,,,,1.3.0,,,,,,GraphX,,,,0,,,,,,"GraphX ShortestPaths seems to be following edges backwards instead of forwards:

import org.apache.spark.graphx._
val g = Graph(sc.makeRDD(Array((1L,""""), (2L,""""), (3L,""""))), sc.makeRDD(Array(Edge(1L,2L,""""), Edge(2L,3L,""""))))

lib.ShortestPaths.run(g,Array(3)).vertices.collect
res1: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map()), (3,Map(3 -> 0)), (2,Map()))

lib.ShortestPaths.run(g,Array(1)).vertices.collect

res2: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map(1 -> 0)), (3,Map(1 -> 2)), (2,Map(1 -> 1)))

The following changes may be what will make it run ""forward"":

Change one occurrence of ""src"" to ""dst"" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L64

Change three occurrences of ""dst"" to ""src"" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L65
",,ankurd,apachespark,boyork,michaelmalak,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 23:02:40 UTC 2015,,,,,,,,,,"0|i24lgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/15 18:37;boyork;I'll take this issue, thanks.;;;","09/Feb/15 19:04;apachespark;User 'brennonyork' has created a pull request for this issue:
https://github.com/apache/spark/pull/4478;;;","10/Feb/15 23:02;ankurd;Issue resolved by pull request 4478
https://github.com/apache/spark/pull/4478;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
build/mvn doesn't work because of invalid URL for maven's tgz.,SPARK-5339,12768767,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,sarutak,sarutak,sarutak,20/Jan/15 19:49,26/Jan/15 21:08,14/Jul/23 06:27,26/Jan/15 21:08,1.3.0,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,"build/mvn will automatically download tarball of maven. But currently, the URL is invalid. ",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 20 19:57:47 UTC 2015,,,,,,,,,,"0|i24kt3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"20/Jan/15 19:54;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/4124;;;","20/Jan/15 19:57;srowen;To clarify, there are two issues: the current download link points to a mirror, not an Apache source. Mirrors don't maintain all versions, just recent ones, and the linked-to 3.2.3 has been replaced by 3.2.5 in mirrors. Apache sources will maintain versions for longer. Although it's not ideal to point at a non-mirror, I don't see a single URL one can access that automatically redirects?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
spark.executor.cores must not be less than spark.task.cpus,SPARK-5336,12768714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,20/Jan/15 17:05,21/Jan/15 15:44,14/Jul/23 06:27,21/Jan/15 15:44,,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"If user set spark.executor.cores to be less than spark.task.cpus, task scheduler will fall in infinite loop, we should throw an exception.in that case.

In standalone and mesos mode, we should respect spark.task.cpus too, and I will file another JIRA to solve that.",,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5337,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 20 17:28:22 UTC 2015,,,,,,,,,,"0|i24khj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/15 17:28;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/4123;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Destroying cluster in VPC with ""--delete-groups"" fails to remove security groups",SPARK-5335,12768698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vgrigor,vgrigor,vgrigor,20/Jan/15 15:16,12/Feb/15 23:28,14/Jul/23 06:27,12/Feb/15 23:27,,,,,,,1.2.2,1.3.0,,,,,EC2,,,,0,,,,,,"When I try to remove security groups using option of the script, it fails because in VPC one should remove security groups by id, not name as it is now.


{code}
$ ./spark-ec2 -k key20141114 -i ~/key.pem --region=eu-west-1 --delete-groups destroy SparkByScript
Are you sure you want to destroy the cluster SparkByScript?
The following instances will be terminated:
Searching for existing cluster SparkByScript...
ALL DATA ON ALL NODES WILL BE LOST!!
Destroy cluster SparkByScript (y/N): y
Terminating master...
Terminating slaves...
Deleting security groups (this will take some time)...
Waiting for cluster to enter 'terminated' state.
Cluster is now in 'terminated' state. Waited 0 seconds.
Attempt 1
Deleting rules in security group SparkByScript-slaves
Deleting rules in security group SparkByScript-master
ERROR:boto:400 Bad Request
ERROR:boto:<?xml version=""1.0"" encoding=""UTF-8""?>
<Response><Errors><Error><Code>InvalidParameterValue</Code><Message>Invalid value 'SparkByScript-slaves' for groupName. You may not reference Amazon VPC security groups by name. Please use the corresponding id for this operation.</Message></Error></Errors><RequestID>60313fac-5d47-48dd-a8bf-e9832948c0a6</RequestID></Response>
Failed to delete security group SparkByScript-slaves
ERROR:boto:400 Bad Request
ERROR:boto:<?xml version=""1.0"" encoding=""UTF-8""?>
<Response><Errors><Error><Code>InvalidParameterValue</Code><Message>Invalid value 'SparkByScript-master' for groupName. You may not reference Amazon VPC security groups by name. Please use the corresponding id for this operation.</Message></Error></Errors><RequestID>74ff8431-c0c1-4052-9ecb-c0adfa7eeeac</RequestID></Response>
Failed to delete security group SparkByScript-master
Attempt 2
....
{code}",,apachespark,nchammas,vgrigor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5246,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Feb 12 23:27:04 UTC 2015,,,,,,,,,,"0|i24kdz:",9223372036854775807,,,,,shivaram,,,,,,,,,,,,,,,,,,,,,,"20/Jan/15 15:36;apachespark;User 'voukka' has created a pull request for this issue:
https://github.com/apache/spark/pull/4122;;;","05/Feb/15 22:28;nchammas;For the record: [AWS says|https://forums.aws.amazon.com/thread.jspa?messageID=572559] you must use the group ID (as opposed to the name) when deleting groups within a VPC. That appears to be the root cause of the issue reported here.;;;","12/Feb/15 23:27;srowen;Issue resolved by pull request 4122
[https://github.com/apache/spark/pull/4122];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when getting files from S3 (hadoop 2.3+),SPARK-5334,12768652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,swkimme,swkimme,20/Jan/15 11:32,03/Mar/15 09:28,14/Jul/23 06:27,03/Mar/15 09:28,1.2.0,,,,,,1.3.0,,,,,,Input/Output,,,,0,,,,,,"In Spark 1.2 built with Hadoop 2.3+, 
unable to get files from AWS S3. 
Same codes works well with same setup in Spark built with Hadoop 2.2-.
I saw that jets3t version changed in profile with Hadoop 2.3+, I guess there might be an issue with it.

===

scala> sc.textFile(""s3n://logs/log.2014-12-05.gz"").count
15/01/20 11:22:40 INFO MemoryStore: ensureFreeSpace(104533) called with curMem=0, maxMem=27783541555
15/01/20 11:22:40 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 102.1 KB, free 25.9 GB)
java.lang.NullPointerException
	at org.apache.hadoop.fs.s3native.NativeS3FileSystem.getFileStatus(NativeS3FileSystem.java:433)
	at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)
	at org.apache.hadoop.fs.Globber.glob(Globber.java:248)
	at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1642)
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:257)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:304)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
	at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:202)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1157)
	at org.apache.spark.rdd.RDD.count(RDD.scala:904)
	at $iwC$$iwC$$iwC$$iwC.<init>(<console>:13)
	at $iwC$$iwC$$iwC.<init>(<console>:18)
	at $iwC$$iwC.<init>(<console>:20)
	at $iwC.<init>(<console>:22)
	at <init>(<console>:24)
	at .<init>(<console>:28)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:789)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1062)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:615)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:646)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:610)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:823)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:868)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:780)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:625)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:633)
	at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:638)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:963)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:911)
	at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:911)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:911)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1006)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:329)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)",Spark 1.2 built with Hadoop 2.3+,swkimme,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Mar 03 09:28:46 UTC 2015,,,,,,,,,,"0|i24k3r:",9223372036854775807,,,,,,,,,,,,,,1.3.0,1.4.0,,,,,,,,,,,,"08/Feb/15 13:14;srowen;Related to, or resolved by, SPARK-5671?;;;","08/Feb/15 13:43;swkimme;[~srowen] Oh thanks! I'll test it.;;;","03/Mar/15 09:28;swkimme;I tested this issue is resolved by SPARK-5671.;;;","03/Mar/15 09:28;swkimme;related to SPARK-5671;;;",,,,,,,,,,,,,,,,,,,,,,,,,
[Mesos] MesosTaskLaunchData occurs BufferUnderflowException,SPARK-5333,12768637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,jongyoul,jongyoul,jongyoul,20/Jan/15 10:13,20/Jan/15 18:21,14/Jul/23 06:27,20/Jan/15 18:21,,,,,,,1.3.0,,,,,,Mesos,,,,0,,,,,,"MesosTaskLaunchData occurs exception when MesosExecutorBackend launches task because serializedTask.remaining is 0.

{code}
Exception in thread ""Thread-6"" java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:498)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355)
	at org.apache.spark.scheduler.cluster.mesos.MesosTaskLaunchData$.fromByteString(MesosTaskLaunchData.scala:46)
	at org.apache.spark.executor.MesosExecutorBackend.launchTask(MesosExecutorBackend.scala:81)
{code}

I've checked this bug with fine-grained mode. This is because MesosTaskLaunchData.toByteString doesn't rewind byteBuffer after they put data.",,apachespark,jongyoul,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-4014,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 20 18:21:12 UTC 2015,,,,,,,,,,"0|i24k0n:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"20/Jan/15 10:19;apachespark;User 'jongyoul' has created a pull request for this issue:
https://github.com/apache/spark/pull/4119;;;","20/Jan/15 18:12;joshrosen;Good catch.  I've created a link to the SPARK-4014 JIRA so that we don't forget to backport this patch, too, when porting that patch to earlier branches.;;;","20/Jan/15 18:21;joshrosen;Fixed by https://github.com/apache/spark/pull/4119;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Spark workers can't find tachyon master as spark-ec2 doesn't set spark.tachyonStore.url,SPARK-5331,12768610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shivaram,florianverhein,florianverhein,20/Jan/15 07:02,07/Feb/16 08:55,14/Jul/23 06:27,30/Jan/16 00:47,,,,,,,1.4.0,,,,,,EC2,,,,0,,,,,,"ps -ef | grep Tachyon 
shows Tachyon running on the master (and the slave) node with correct setting:
-Dtachyon.master.hostname=ec2-54-252-156-187.ap-southeast-2.compute.amazonaws.com

However from stderr log on worker running the SparkTachyonPi example:

15/01/20 06:00:56 INFO CacheManager: Partition rdd_0_0 not found, computing it
15/01/20 06:00:56 INFO : Trying to connect master @ localhost/127.0.0.1:19998
15/01/20 06:00:56 ERROR : Failed to connect (1) to master localhost/127.0.0.1:19998 : java.net.ConnectException: Connection refused
15/01/20 06:00:57 ERROR : Failed to connect (2) to master localhost/127.0.0.1:19998 : java.net.ConnectException: Connection refused
15/01/20 06:00:58 ERROR : Failed to connect (3) to master localhost/127.0.0.1:19998 : java.net.ConnectException: Connection refused
15/01/20 06:00:59 ERROR : Failed to connect (4) to master localhost/127.0.0.1:19998 : java.net.ConnectException: Connection refused
15/01/20 06:01:00 ERROR : Failed to connect (5) to master localhost/127.0.0.1:19998 : java.net.ConnectException: Connection refused
15/01/20 06:01:01 WARN TachyonBlockManager: Attempt 1 to create tachyon dir null failed
java.io.IOException: Failed to connect to master localhost/127.0.0.1:19998 after 5 attempts
	at tachyon.client.TachyonFS.connect(TachyonFS.java:293)
	at tachyon.client.TachyonFS.getFileId(TachyonFS.java:1011)
	at tachyon.client.TachyonFS.exist(TachyonFS.java:633)
	at org.apache.spark.storage.TachyonBlockManager$$anonfun$createTachyonDirs$2.apply(TachyonBlockManager.scala:117)
	at org.apache.spark.storage.TachyonBlockManager$$anonfun$createTachyonDirs$2.apply(TachyonBlockManager.scala:106)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.storage.TachyonBlockManager.createTachyonDirs(TachyonBlockManager.scala:106)
	at org.apache.spark.storage.TachyonBlockManager.<init>(TachyonBlockManager.scala:57)
	at org.apache.spark.storage.BlockManager.tachyonStore$lzycompute(BlockManager.scala:94)
	at org.apache.spark.storage.BlockManager.tachyonStore(BlockManager.scala:88)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:773)
	at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:638)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:145)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:228)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: tachyon.org.apache.thrift.TException: Failed to connect to master localhost/127.0.0.1:19998 after 5 attempts
	at tachyon.master.MasterClient.connect(MasterClient.java:178)
	at tachyon.client.TachyonFS.connect(TachyonFS.java:290)
	... 28 more
Caused by: tachyon.org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused
	at tachyon.org.apache.thrift.transport.TSocket.open(TSocket.java:185)
	at tachyon.org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
	at tachyon.master.MasterClient.connect(MasterClient.java:156)
	... 29 more
Caused by: java.net.ConnectException: Connection refused
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at tachyon.org.apache.thrift.transport.TSocket.open(TSocket.java:180)
	... 31 more
15/01/20 06:01:01 ERROR TachyonBlockManager: Failed 10 attempts to create tachyon dir in /tmp_spark_tachyon/spark-f1c7257e-b79e-4fa2-955a-e3734d80dbc6/1","Running on EC2 via modified spark-ec2 scripts (to get dependencies right so tachyon starts)
Using tachyon 0.5.0 built against hadoop 2.4.1
Spark 1.2.0 built against tachyon 0.5.0 and hadoop 0.4.1
Tachyon configured using the template in 0.5.0 but updated with slave list and master variables etc..
",florianverhein,nchammas,shivaram,uronce,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 30 00:47:19 UTC 2016,,,,,,,,,,"0|i24juv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/15 02:55;florianverhein;The problem was actually that the spark-ec2 scripts don't set up spark.tachyonStore.url in spark-defaults.conf. 

To fix, add the following to spark-defaults.conf
spark.tachyonStore.url  tachyon://{{active_master}}:19998

It had nothing to do with tachyon workers (thanks for spotting this [~haoyuan]!). Updated issue to reflect that.
;;;","15/May/15 12:52;srowen;[~florianverhein] is this an issue then or just a matter of setting the config correctly?;;;","28/Jun/15 01:06;uronce;fixed in PR: https://github.com/mesos/spark-ec2/pull/125;;;","30/Jan/16 00:47;shivaram;Fixed by https://github.com/mesos/spark-ec2/pull/125;;;",,,,,,,,,,,,,,,,,,,,,,,,,
UIWorkloadGenerator should stop SparkContext.,SPARK-5329,12768590,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,20/Jan/15 03:51,20/Jan/15 20:41,14/Jul/23 06:27,20/Jan/15 20:41,1.3.0,,,,,,1.3.0,,,,,,Web UI,,,,0,,,,,,"UIWorkloadGenerator don't stop SparkContext. I ran UIWorkloadGenerator and try to watch the result at WebUI but Jobs are marked as finished.
It's because SparkContext is not stopped.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 20 03:52:53 UTC 2015,,,,,,,,,,"0|i24jqv:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"20/Jan/15 03:52;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/4112;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Results of describe can't be queried,SPARK-5324,12768536,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,OopsOutOfMemory,marmbrus,marmbrus,19/Jan/15 21:08,12/Feb/15 04:42,14/Jul/23 06:27,06/Feb/15 20:33,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"{code}
sql(""DESCRIBE TABLE test"").registerTempTable(""describeTest"")
sql(""SELECT * FROM describeTest"").collect()
{code}",,apachespark,marmbrus,OopsOutOfMemory,x1,yanboliang,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 20:33:36 UTC 2015,,,,,,,,,,"0|i24jf3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"26/Jan/15 16:01;yanboliang;[~marmbrus]
I have pull a request for this issue which will implement ""DESCRIBE [FORMATTED] [db_name.]table_name"" command for SQLContext.
Meanwhile, it need to make the least effect on the corresponding command output of HiveContext.
And I think other metadata operation command like ""show databases/tables, analyze, explain"" can also leverage this scenario.
Can you assign this to me?;;;","28/Jan/15 02:54;OopsOutOfMemory;hi, [~yanboliang] Thanks for working on this. But by the way, this JIRA(SPARK-5324) Results of describe can't be queried is mainly focus on make describe command can be query like a table but not Add a describe command in sqlContext, shall we make this PR focus on it's own JIRA issue? And you have no more test suites to demonstrate bug fixing. Would you mind close this PR, and if you have some good advices for add describe table you can refer to SPARK-5135 #4227 , and comment on my PR. I'd be very pleasure : );;;","28/Jan/15 08:02;yanboliang;[~OopsOutOfMemory] Thanks for your comments. I agree that this JIRA will focus on it's own issue. So we will wait until SPARK-5135 had been closed and check this issue whether still exist.;;;","28/Jan/15 12:09;apachespark;User 'OopsOutOfMemory' has created a pull request for this issue:
https://github.com/apache/spark/pull/4249;;;","06/Feb/15 20:33;marmbrus;Issue resolved by pull request 4249
[https://github.com/apache/spark/pull/4249];;;",,,,,,,,,,,,,,,,,,,,,,,,
Joins on simple table created using select gives error,SPARK-5320,12768404,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,x1,kul,kul,19/Jan/15 10:10,23/Mar/15 11:13,14/Jul/23 06:27,21/Mar/15 20:23,1.1.1,,,,,,1.3.1,1.4.0,,,,,SQL,,,,0,,,,,,"Register ""select 0 as a, 1 as b"" as table zeroone
Register ""select 0 as x, 1 as y"" as table zeroone2

The following sql 
""select * from zeroone ta join zeroone2 tb on ta.a = tb.x""

gives error 
java.lang.UnsupportedOperationException: LeafNode NoRelation$ must implement statistics.
",,apachespark,kul,marmbrus,x1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 23 11:13:00 UTC 2015,,,,,,,,,,"0|i24in3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/15 09:59;apachespark;User 'x1-' has created a pull request for this issue:
https://github.com/apache/spark/pull/5105;;;","21/Mar/15 20:23;marmbrus;Issue resolved by pull request 5105
[https://github.com/apache/spark/pull/5105];;;","22/Mar/15 08:01;x1;Thank you very much Michael Armbrust.
Could you change assignee noassign to me(x1 - Yuri Saito)?;;;","23/Mar/15 11:13;x1;Thank you very much!
I'm very grad!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
reduceByWindow returns Scala DStream not JavaDStream,SPARK-5315,12768382,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jerryshao,jerryshao,19/Jan/15 08:35,23/Jan/15 06:09,14/Jul/23 06:27,23/Jan/15 06:09,1.2.0,,,,,,1.3.0,,,,,,DStreams,,,,0,,,,,,"{code}

  def reduceByWindow(
      reduceFunc: (T, T) => T,
      windowDuration: Duration,
      slideDuration: Duration
    ): DStream[T] = {
    dstream.reduceByWindow(reduceFunc, windowDuration, slideDuration)
  }
{code}

It should return JavaDStream not DStream for java code.",,apachespark,jerryshao,jnadler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 19 17:01:54 UTC 2015,,,,,,,,,,"0|i24ii7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/15 09:15;srowen;Ah, yeah that looks like a typo for sure. The other overload of this method in {{JavaDStreamLike}} returns {{JavaDStream}}. Hm, that would change the API unfortunately, but I wonder if anyone is relying on it? Of course, callers can just wrap it in {{JavaDStream}} as an easy workaround.;;;","19/Jan/15 09:23;jerryshao;Yes, user can wrap with JavaDStream, but I think it is quite tricky and un-straightforward for users. Also from my understanding, change this API's signature is meaningful because the previous one is actually not correct.;;;","19/Jan/15 09:44;srowen;Agree, but someone may be calling this method and wrapping the result. This change breaks such a program. It may well be worth it but not obvious. There is not a good answer. Could also just deprecate it to mark the problem and workaround.;;;","19/Jan/15 09:51;jerryshao;Another point need to be noted that the {{reduceFunc}} in reduceByWindow's parameter is scala's Function2, not the Java Function2 , even with JavaDStream wrapped cannot solve the problem, actually the code cannot be compiled.;;;","19/Jan/15 09:54;srowen;Oh, that's a good point and good news then. We can add the right method then, and it will simply be an overload. The existing method can be left and deprecated.;;;","19/Jan/15 10:27;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/4104;;;","19/Jan/15 17:01;jnadler;Thanks Jerry!   I was just about to file this and you're already done.  ;;;",,,,,,,,,,,,,,,,,,,,,,
java.lang.OutOfMemoryError in SparkSQL with GROUP BY,SPARK-5314,12768361,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,alexbaretta,alexbaretta,19/Jan/15 05:01,16/Sep/15 08:06,14/Jul/23 06:27,15/Sep/15 21:39,,,,,,,1.5.0,,,,,,SQL,,,,1,,,,,,"I am running a SparkSQL GROUP BY query on a largish Parquet table (a few hundred million rows), weighing it at about 50GB. My cluster has 1.7 TB of RAM, so it should have more than plenty resources to cope with this query.

WARN TaskSetManager: Lost task 279.0 in stage 22.0 (TID 1229, ds-model-w-21.c.eastern-gravity-771.internal): java.lang.OutOfMemoryError: GC overhead limit exceeded
        at scala.collection.SeqLike$class.distinct(SeqLike.scala:493)
        at scala.collection.AbstractSeq.distinct(Seq.scala:40)
        at org.apache.spark.sql.catalyst.expressions.Coalesce.resolved$lzycompute(nullFunctions.scala:33)
        at org.apache.spark.sql.catalyst.expressions.Coalesce.resolved(nullFunctions.scala:33)
        at org.apache.spark.sql.catalyst.expressions.Coalesce.dataType(nullFunctions.scala:37)
        at org.apache.spark.sql.catalyst.expressions.Expression.n2(Expression.scala:100)
        at org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:101)
        at org.apache.spark.sql.catalyst.expressions.Coalesce.eval(nullFunctions.scala:50)
        at org.apache.spark.sql.catalyst.expressions.MutableLiteral.update(literals.scala:81)
        at org.apache.spark.sql.catalyst.expressions.SumFunction.update(aggregates.scala:571)
        at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:167)
        at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:151)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:615)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:615)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:231)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:264)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:231)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
",,alexbaretta,lianhuiwang,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Sep 15 21:39:45 UTC 2015,,,,,,,,,,"0|i24idj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/15 07:08;alexbaretta;Per Akhil's comment on the dev list, ""SET spark.sql.shuffle.partitions=1024"" resolves the OOM issue. I wonder if a more robust solution could be found.;;;","15/Sep/15 21:39;marmbrus;1.5 now uses sort based aggregation with spilling by default.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
MD5 / SHA1 hash format doesn't match standard Maven output,SPARK-5308,12768301,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,kul,kul,18/Jan/15 13:43,30/May/15 23:07,14/Jul/23 06:27,27/Jan/15 18:28,1.2.0,,,,,,1.2.1,1.3.0,,,,,Build,,,,0,,,,,,"https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.10/1.2.0/spark-core_2.10-1.2.0.pom.md5

The above does not look like a proper md5 which is causing failure in some build tools like leiningen.
https://github.com/technomancy/leiningen/issues/1802

Compare this with 1.1.0 release
https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.10/1.1.0/spark-core_2.10-1.1.0.pom.md5",,apachespark,kul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CALCITE-747,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 28 03:13:46 UTC 2015,,,,,,,,,,"0|i24i1b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/15 18:35;srowen;I see that the {{create-release.sh}} script makes an MD5 hash with {{gpg --print-md MD5 ...}} which gives this other format. 

How about {{md5 -q ...}} ? that will just print the hash. However, I think this is not necessarily a standard utility installed on Linux distros. It's on Macs. Would it be fair to assume the releases will be made on a Mac or something with {{md5}} installed?

The standard Maven plugins can also make these hash files, and they generate files with just the hash, yes. I think I ran into something like this a while ago when I was verifying the SHA1 hashes. 

Yeah seems nicer to standardize it. Anybody see a problem with using {{md5}} or have a better command that will make the same output?;;;","18/Jan/15 20:46;srowen;Hm, well {{md5sum ... | cut -f1 -d' '}} looks like it gets the job done on Linux. But it's not on Macs. Nothing an if statement can't solve.;;;","22/Jan/15 14:27;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4161;;;","28/Jan/15 03:13;kul;Thanks [~srowen]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Add utility to help with NotSerializableException debugging,SPARK-5307,12768280,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,rxin,rxin,rxin,18/Jan/15 07:14,31/Jan/15 06:42,14/Jul/23 06:27,31/Jan/15 06:42,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"Scala closures can easily capture objects unintentionally, especially with implicit arguments. I think we can do more than just relying on the users being smart about using sun.io.serialization.extendedDebugInfo to find more debug information.",,apachespark,huasanyelao,rxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3694,,,,,,,,,SPARK-3694,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 31 06:41:53 UTC 2015,,,,,,,,,,"0|i24hx3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"18/Jan/15 07:20;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4093;;;","19/Jan/15 05:58;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4098;;;","31/Jan/15 06:41;apachespark;User 'rxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/4297;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
JavaStreamingContext.fileStream won't work because type info isn't propagated,SPARK-5297,12768224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jerryshao,fegaras,fegaras,17/Jan/15 15:35,27/Feb/15 15:27,14/Jul/23 06:27,27/Feb/15 15:26,1.2.0,,,,,,1.3.0,,,,,,DStreams,,,,0,,,,,,"The following code:
{code}
stream_context.<K,V,SequenceFileInputFormat<K,V>>fileStream(directory)
.foreachRDD(new Function<JavaPairRDD<K,V>,Void>() {
     public Void call ( JavaPairRDD<K,V> rdd ) throws Exception {
         for ( Tuple2<K,V> x: rdd.collect() )
             System.out.println(""# ""+x._1+"" ""+x._2);
         return null;
     }
  });
stream_context.start();
stream_context.awaitTermination();
{code}
for custom (serializable) classes K and V compiles fine but gives an error
when I drop a new hadoop sequence file in the directory:
{quote}
15/01/17 09:13:59 ERROR scheduler.JobScheduler: Error generating jobs for time 1421507639000 ms
java.lang.ClassCastException: java.lang.Object cannot be cast to org.apache.hadoop.mapreduce.InputFormat
	at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:91)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
	at org.apache.spark.streaming.dstream.FileInputDStream$$anonfun$3.apply(FileInputDStream.scala:236)
	at org.apache.spark.streaming.dstream.FileInputDStream$$anonfun$3.apply(FileInputDStream.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.streaming.dstream.FileInputDStream.org$apache$spark$streaming$dstream$FileInputDStream$$filesToRDD(FileInputDStream.scala:234)
	at org.apache.spark.streaming.dstream.FileInputDStream.compute(FileInputDStream.scala:128)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:296)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:288)
	at scala.Option.orElse(Option.scala:257)
{quote}
The same classes K and V work fine for non-streaming Spark:
{code}
spark_context.newAPIHadoopFile(path,F.class,K.class,SequenceFileInputFormat.class,conf)
{code}
also streaming works fine for TextFileInputFormat.

The issue is that class manifests are erased to object in the Java file stream constructor, but those are relied on downstream when creating the Hadoop RDD that backs each batch of the file stream.

https://github.com/apache/spark/blob/v1.2.0/streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaStreamingContext.scala#L263
https://github.com/apache/spark/blob/v1.2.0/core/src/main/scala/org/apache/spark/SparkContext.scala#L753
",,apachespark,cnstar9988,fegaras,jerryshao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-3754,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 27 15:27:10 UTC 2015,,,,,,,,,,"0|i24hmn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/15 18:58;srowen;I don't see that this has anything to do with custom key / values? 

What it suggests is that type information got lost along the way from Java to Scala. This fails: {{val inputFormat = inputFormatClass.newInstance}}.

I won't claim to understand this enough to be sure, but I don't see how the type info gets into the underlying call to {{StreamingContext.fileStream}}. {{JavaStreamingContext}} says:

{code}
  def fileStream[K, V, F <: NewInputFormat[K, V]](
      directory: String): JavaPairInputDStream[K, V] = {
    implicit val cmk: ClassTag[K] =
      implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[K]]
    implicit val cmv: ClassTag[V] =
      implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[V]]
    implicit val cmf: ClassTag[F] =
      implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[F]]
    ssc.fileStream[K, V, F](directory)
  }
{code}

Obviously in {{newAPIHadoopFile}} the type information is carried explicitly there. 

Is this something you can weigh in on [~tdas]?;;;","19/Jan/15 01:39;jerryshao;Hi [~sowen], I think this issue is the same as previously fixed SPARK-2103 issue, the problem is that using 
{code}
implicit val cmk: ClassTag[K] = 
    implicitly[ClassTag[AnyRef]].asInstanceOf[ClassTag[K]]
{code}
 to get the implicit class tag actually cannot the the class of right object for Java, I will fix this.;;;","19/Jan/15 07:18;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/4101;;;","22/Jan/15 01:45;apachespark;User 'jerryshao' has created a pull request for this issue:
https://github.com/apache/spark/pull/4154;;;","27/Feb/15 15:27;srowen;Decided not to back port to 1.2 per tdas;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fail to drop an invalid table when using the data source API,SPARK-5286,12768129,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,yhuai,yhuai,16/Jan/15 21:05,25/Feb/15 03:51,14/Jul/23 06:27,25/Feb/15 03:51,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Example
{code}
CREATE TABLE jsonTable
USING org.apache.spark.sql.json.DefaultSource
OPTIONS (
  path 'it is not a path at all!'
)

DROP TABLE jsonTable
{code}

We will get 
{code}
[info]   com.google.common.util.concurrent.UncheckedExecutionException: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/yhuai/Projects/Spark/spark/sql/hive/it is not a path at all!
[info]   at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4882)
[info]   at com.google.common.cache.LocalCache$LocalLoadingCache.apply(LocalCache.java:4898)
[info]   at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:147)
[info]   at org.apache.spark.sql.hive.HiveContext$$anon$2.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$$super$lookupRelation(HiveContext.scala:241)
[info]   at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:137)
[info]   at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:137)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:137)
[info]   at org.apache.spark.sql.hive.HiveContext$$anon$2.lookupRelation(HiveContext.scala:241)
[info]   at org.apache.spark.sql.SQLContext.table(SQLContext.scala:332)
[info]   at org.apache.spark.sql.hive.execution.DropTable.run(commands.scala:57)
[info]   at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
[info]   at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
[info]   at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:61)
[info]   at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:472)
[info]   at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:472)
[info]   at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
[info]   at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
[info]   at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:73)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite$$anonfun$9.apply$mcV$sp(MetastoreDataSourcesSuite.scala:258)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite$$anonfun$9.apply(MetastoreDataSourcesSuite.scala:246)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite$$anonfun$9.apply(MetastoreDataSourcesSuite.scala:246)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(MetastoreDataSourcesSuite.scala:36)
[info]   at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite.runTest(MetastoreDataSourcesSuite.scala:36)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuite.run(FunSuite.scala:1555)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
[info]   Cause: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/yhuai/Projects/Spark/spark/sql/hive/it is not a path at all!
[info]   at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)
[info]   at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)
[info]   at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:201)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:206)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.rdd.RDD.partitions(RDD.scala:204)
[info]   at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:206)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.rdd.RDD.partitions(RDD.scala:204)
[info]   at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:206)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.rdd.RDD.partitions(RDD.scala:204)
[info]   at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:206)
[info]   at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.rdd.RDD.partitions(RDD.scala:204)
[info]   at org.apache.spark.SparkContext.runJob(SparkContext.scala:1367)
[info]   at org.apache.spark.rdd.RDD.reduce(RDD.scala:881)
[info]   at org.apache.spark.sql.json.JsonRDD$.inferSchema(JsonRDD.scala:54)
[info]   at org.apache.spark.sql.json.JSONRelation$$anonfun$schema$1.apply(JSONRelation.scala:60)
[info]   at org.apache.spark.sql.json.JSONRelation$$anonfun$schema$1.apply(JSONRelation.scala:59)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.sql.json.JSONRelation.schema$lzycompute(JSONRelation.scala:58)
[info]   at org.apache.spark.sql.json.JSONRelation.schema(JSONRelation.scala:58)
[info]   at org.apache.spark.sql.sources.LogicalRelation.<init>(LogicalRelation.scala:30)
[info]   at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anon$2.load(HiveMetastoreCatalog.scala:85)
[info]   at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anon$2.load(HiveMetastoreCatalog.scala:63)
[info]   at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
[info]   at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
[info]   at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
[info]   at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
[info]   at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
[info]   at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
[info]   at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
[info]   at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4880)
[info]   at com.google.common.cache.LocalCache$LocalLoadingCache.apply(LocalCache.java:4898)
[info]   at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:147)
[info]   at org.apache.spark.sql.hive.HiveContext$$anon$2.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$$super$lookupRelation(HiveContext.scala:241)
[info]   at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:137)
[info]   at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:137)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:137)
[info]   at org.apache.spark.sql.hive.HiveContext$$anon$2.lookupRelation(HiveContext.scala:241)
[info]   at org.apache.spark.sql.SQLContext.table(SQLContext.scala:332)
[info]   at org.apache.spark.sql.hive.execution.DropTable.run(commands.scala:57)
[info]   at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:53)
[info]   at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:53)
[info]   at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:61)
[info]   at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:472)
[info]   at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:472)
[info]   at org.apache.spark.sql.SchemaRDDLike$class.$init$(SchemaRDDLike.scala:58)
[info]   at org.apache.spark.sql.SchemaRDD.<init>(SchemaRDD.scala:108)
[info]   at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:73)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite$$anonfun$9.apply$mcV$sp(MetastoreDataSourcesSuite.scala:258)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite$$anonfun$9.apply(MetastoreDataSourcesSuite.scala:246)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite$$anonfun$9.apply(MetastoreDataSourcesSuite.scala:246)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(MetastoreDataSourcesSuite.scala:36)
[info]   at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
[info]   at org.apache.spark.sql.hive.MetastoreDataSourcesSuite.runTest(MetastoreDataSourcesSuite.scala:36)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuite.run(FunSuite.scala:1555)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}",,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 25 03:51:58 UTC 2015,,,,,,,,,,"0|i24h4n:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"16/Jan/15 21:10;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4076;;;","19/Jan/15 18:45;marmbrus;Issue resolved by pull request 4076
[https://github.com/apache/spark/pull/4076];;;","25/Feb/15 01:51;yhuai;I am reopen this issue because we need to catch all Throwables instead of just Exceptions.;;;","25/Feb/15 01:55;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4755;;;","25/Feb/15 03:51;marmbrus;Issue resolved by pull request 4755
[https://github.com/apache/spark/pull/4755];;;",,,,,,,,,,,,,,,,,,,,,,,,
Insert into Hive throws NPE when a inner complex type field has a null value,SPARK-5284,12768112,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,yhuai,yhuai,16/Jan/15 20:03,19/Jan/15 18:44,14/Jul/23 06:27,19/Jan/15 18:44,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"For  a table like the following one, 
{code}
CREATE TABLE nullValuesInInnerComplexTypes
  (s struct<innerStruct: struct<s1:string>,
            innerArray:array<int>,
            innerMap: map<string, int>>)
{code}

When we want to insert a row like this 
{code}
Row(Row(null, null, null))
{code}

Will get a NPE
{code}
[info]   org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 1 times, most recent failure: Lost task 1.0 in stage 0.0 (TID 1, localhost): java.lang.NullPointerException
[info] 	at scala.runtime.Tuple3Zipped$.foreach$extension(Tuple3Zipped.scala:105)
[info] 	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$3.apply(HiveInspectors.scala:351)
[info] 	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$3$$anonfun$apply$4.apply(HiveInspectors.scala:351)
[info] 	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$3$$anonfun$apply$4.apply(HiveInspectors.scala:351)
[info] 	at scala.runtime.Tuple3Zipped$$anonfun$foreach$extension$1.apply(Tuple3Zipped.scala:109)
[info] 	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
[info] 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
[info] 	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
[info] 	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
[info] 	at scala.runtime.Tuple3Zipped$.foreach$extension(Tuple3Zipped.scala:107)
[info] 	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$3.apply(HiveInspectors.scala:351)
[info] 	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:108)
[info] 	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:105)
[info] 	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
[info] 	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
[info] 	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:105)
[info] 	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:87)
[info] 	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:87)
[info] 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
[info] 	at org.apache.spark.scheduler.Task.run(Task.scala:64)
[info] 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:192)
[info] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info] 	at java.lang.Thread.run(Thread.java:745)
[info] 
[info] Driver stacktrace:
[info]   at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1199)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1188)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1187)
[info]   at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
[info]   at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
[info]   at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1187)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
[info]   at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
[info]   at scala.Option.foreach(Option.scala:236)
[info]   at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1399)
[info]   at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
[info]   at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1360)
[info]   at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
[info]   at akka.actor.ActorCell.invoke(ActorCell.scala:487)
[info]   at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
[info]   at akka.dispatch.Mailbox.run(Mailbox.scala:220)
[info]   at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
[info]   at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
[info]   at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
[info]   at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
[info]   at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}
",,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 19 18:44:40 UTC 2015,,,,,,,,,,"0|i24h13:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"16/Jan/15 21:17;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4077;;;","19/Jan/15 18:44;marmbrus;Issue resolved by pull request 4077
[https://github.com/apache/spark/pull/4077];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
RowMatrix easily gets int overflow in the memory size warning,SPARK-5282,12768000,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,yuhaoyan,yuhaoyan,yuhaoyan,16/Jan/15 12:22,20/Jan/15 01:29,14/Jul/23 06:27,19/Jan/15 18:10,1.2.0,,,,,,1.2.1,1.3.0,,,,,MLlib,,,,0,,,,,,"The warning in the RowMatrix will easily get int overflow when the cols is larger than 16385.

minor issue.","centos, others should be similar",apachespark,mengxr,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 20 01:29:01 UTC 2015,,,,,,,,,,"0|i24gdj:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"16/Jan/15 12:25;yuhaoyan;typical wrong message: Row matrix: 17000 cloumns will require at least -1982967296 bytes of memory!

PR on the way.;;;","16/Jan/15 12:35;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/4069;;;","19/Jan/15 18:10;mengxr;Issue resolved by pull request 4069
[https://github.com/apache/spark/pull/4069];;;","20/Jan/15 01:29;yuhaoyan;fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Registering table on RDD is giving MissingRequirementError,SPARK-5281,12767994,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dragos,sarsol,sarsol,16/Jan/15 11:40,10/Dec/15 17:21,14/Jul/23 06:27,07/May/15 23:25,1.2.0,1.3.1,,,,,1.4.0,,,,,,SQL,,,,17,,,,,,"Application crashes on this line  {{rdd.registerTempTable(""temp"")}}  in 1.2 version when using sbt or Eclipse SCALA IDE

Stacktrace:

{code}
Exception in thread ""main"" scala.reflect.internal.MissingRequirementError: class org.apache.spark.sql.catalyst.ScalaReflection in JavaMirror with primordial classloader with boot classpath [C:\sar\scala\scala-ide\eclipse\plugins\org.scala-ide.scala210.jars_4.0.0.201407240952\target\jars\scala-library.jar;C:\sar\scala\scala-ide\eclipse\plugins\org.scala-ide.scala210.jars_4.0.0.201407240952\target\jars\scala-reflect.jar;C:\sar\scala\scala-ide\eclipse\plugins\org.scala-ide.scala210.jars_4.0.0.201407240952\target\jars\scala-actor.jar;C:\sar\scala\scala-ide\eclipse\plugins\org.scala-ide.scala210.jars_4.0.0.201407240952\target\jars\scala-swing.jar;C:\sar\scala\scala-ide\eclipse\plugins\org.scala-ide.scala210.jars_4.0.0.201407240952\target\jars\scala-compiler.jar;C:\Program Files\Java\jre7\lib\resources.jar;C:\Program Files\Java\jre7\lib\rt.jar;C:\Program Files\Java\jre7\lib\sunrsasign.jar;C:\Program Files\Java\jre7\lib\jsse.jar;C:\Program Files\Java\jre7\lib\jce.jar;C:\Program Files\Java\jre7\lib\charsets.jar;C:\Program Files\Java\jre7\lib\jfr.jar;C:\Program Files\Java\jre7\classes] not found.
	at scala.reflect.internal.MissingRequirementError$.signal(MissingRequirementError.scala:16)
	at scala.reflect.internal.MissingRequirementError$.notFound(MissingRequirementError.scala:17)
	at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:48)
	at scala.reflect.internal.Mirrors$RootsBase.getModuleOrClass(Mirrors.scala:61)
	at scala.reflect.internal.Mirrors$RootsBase.staticModuleOrClass(Mirrors.scala:72)
	at scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:119)
	at scala.reflect.internal.Mirrors$RootsBase.staticClass(Mirrors.scala:21)
	at org.apache.spark.sql.catalyst.ScalaReflection$$typecreator1$1.apply(ScalaReflection.scala:115)
	at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe$lzycompute(TypeTags.scala:231)
	at scala.reflect.api.TypeTags$WeakTypeTagImpl.tpe(TypeTags.scala:231)
	at scala.reflect.api.TypeTags$class.typeOf(TypeTags.scala:335)
	at scala.reflect.api.Universe.typeOf(Universe.scala:59)
	at org.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:115)
	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:33)
	at org.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaReflection.scala:100)
	at org.apache.spark.sql.catalyst.ScalaReflection$.schemaFor(ScalaReflection.scala:33)
	at org.apache.spark.sql.catalyst.ScalaReflection$class.attributesFor(ScalaReflection.scala:94)
	at org.apache.spark.sql.catalyst.ScalaReflection$.attributesFor(ScalaReflection.scala:33)
	at org.apache.spark.sql.SQLContext.createSchemaRDD(SQLContext.scala:111)
	at com.sar.spark.dq.poc.SparkPOC$delayedInit$body.apply(SparkPOC.scala:43)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:40)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.App$$anonfun$main$1.apply(App.scala:71)
	at scala.App$$anonfun$main$1.apply(App.scala:71)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:32)
	at scala.App$class.main(App.scala:71)
{code}",,alewando,apachespark,ardlema,clive,cnstar9988,dongtalk@gmail.com,dragos,dreamquster,glenn.strycker@gmail.com,Haopu Wang,irene,Joshua Zhang,k.shaposhnikov@gmail.com,krisden,marmbrus,mbonaci,nburoojy,pprett,prudenko,sangkyoonnam,sarsol,skoppar,syepes,willbenton,yraimond,zmre,,,,,,,,,,,,,,,,,,,,,,SPARK-5290,,,,,,,,,,,,,,SPARK-8470,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Dec 10 17:21:34 UTC 2015,,,,,,,,,,"0|i24gc7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/15 15:22;pprett;I'm seeing the same problem in a slightly different context (Jobserver). I get it even without registering temp tables.;;;","23/Jan/15 23:30;pprett;my problem was resolved when not running Jobserver via ``sbt reStart`` but via ``java -jar job-server-assembly.jar``;;;","09/Feb/15 09:14;irene;same issue here, since last week, no news on this?;;;","20/Feb/15 21:54;syepes;Also having this issue with 1.2.1 the standard context (sc);;;","24/Feb/15 14:44;ardlema;Having this problem as well trying to migrate from 1.1.1 to 1.2.1. Any updates?;;;","27/Feb/15 06:13;sangkyoonnam;I have same problem.
In my case, I used CDH 5.3.x;;;","14/Mar/15 17:36;clive;I have this isssue with Eclipse Luna and spark 1.3.0;;;","07/Apr/15 00:03;zmre;I also have this issue with spark 1.3.0.  Even example snippets where case classes are used in the rrd's trigger the problem.  For me, this happens from eclipse and from sbt.;;;","07/Apr/15 00:34;willbenton;As [~marmbrus] recently pointed out on the user list, this happens when you don't have all of the dependencies for Scala reflection loaded by the primordial classloader.  For running apps from sbt, setting {{fork := true}} should do the trick.  For running a REPL from sbt, try [this workaround|http://chapeau.freevariable.com/2015/04/spark-sql-repl.html].  (Sorry to not have a solution for Eclipse.);;;","07/Apr/15 00:49;marmbrus;I'll add that this is the trick we use if you run {{build/sbt sparkShell}} from the spark distribution.;;;","13/Apr/15 10:27;dongtalk@gmail.com;I also have this isssue with Eclipse Luna and spark 1.3.0, any idea ?;;;","27/Apr/15 19:15;skoppar;I have the same issue. Posted an issue for the same here - https://www.mail-archive.com/user@spark.apache.org/msg27114.html. I am hitting it in Eclipse as well. Setting fork := true in build.sbt has no effect. Not sure if I am doing it right. ;;;","06/May/15 09:25;dragos;Here's my workaround from [this stack overflow quesiton|https://stackoverflow.com/questions/29796928/whats-the-most-efficient-way-to-filter-a-dataframe]

- find your launch configuration and go to ""Classpath""
- remove Scala Library and Scala Compiler from the ""Bootstrap"" entries
- add (as external jars) scala-reflect, scala-library and scala-compiler to user entries

Make sure to add the right version (2.10.4 at this point).

;;;","06/May/15 18:12;yraimond;+1 - this workaround does the trick with Eclipse Luna. Thanks!;;;","07/May/15 13:56;apachespark;User 'dragos' has created a pull request for this issue:
https://github.com/apache/spark/pull/5981;;;","07/May/15 23:25;marmbrus;Issue resolved by pull request 5981
[https://github.com/apache/spark/pull/5981];;;","28/Jun/15 10:52;cnstar9988;Does this patch merge to spark 1.3 branch, thanks.
;;;","08/Jul/15 22:03;nburoojy;This patch is not in 1.3. [~dragos], can this be merged into 1.3?;;;","08/Jul/15 22:13;marmbrus;This is too big of a change (and it actually caused several regressions that needed to be fixed) to backport into a maintenance branch.;;;","09/Jul/15 08:59;dragos;[~marmbrus] what were those regressions, out of curiosity? I haven't followed Jira closely, but nothing is linked to this ticket.;;;","09/Jul/15 18:56;marmbrus;They were minor and pretty easy to fix.  I believe that there was https://issues.apache.org/jira/browse/SPARK-8470.  There was also a problem since the mirror was originally a {{val}}, and sometimes we change the classloader when jars are added.;;;","10/Jul/15 15:51;dragos;Thanks for pointing them out. Glad it wasn't too bad :);;;","10/Dec/15 17:21;skoppar;We tried this workaround as well and the application works perfectly, however, we are not able to use the ScalaTest and write unit tests which can invoke the application code itself. The test with below error:

Exception in thread ""main"" java.lang.NoClassDefFoundError: scala/Function1
	at scala.tools.eclipse.scalatest.launching.ScalaTestLauncher.main(ScalaTestLauncher.scala)
Caused by: java.lang.ClassNotFoundException: scala.Function1
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)

Which essentially means it cannot find the library;;;",,,,,,
check ambiguous reference to fields in Spark SQL is incompleted,SPARK-5278,12767936,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,16/Jan/15 06:05,14/Aug/16 11:23,14/Jul/23 06:27,06/Feb/15 21:11,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"at hive context

for json string like
{code}{""a"": {""b"": 1, ""B"": 2}}{code}
The SQL `SELECT a.b from t` will report error for ambiguous reference to fields.
But for json string like
{code}{""a"": [{""b"": 1, ""B"": 2}]}{code}
The SQL `SELECT a[0].b from t` will pass and pick the first `b`",,apachespark,cloud_fan,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 06 21:11:01 UTC 2015,,,,,,,,,,"0|i24g0n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/15 06:54;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/4068;;;","06/Feb/15 21:11;marmbrus;Issue resolved by pull request 4068
[https://github.com/apache/spark/pull/4068];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkSqlSerializer does not register user specified KryoRegistrators ,SPARK-5277,12767897,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mhseiden,mhseiden,mhseiden,16/Jan/15 01:18,25/Apr/15 21:57,14/Jul/23 06:27,15/Apr/15 23:15,1.2.1,1.3.0,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Although the SparkSqlSerializer class extends the KryoSerializer in core, it's overridden newKryo() does not call super.newKryo(). This results in inconsistent serializer behaviors depending on whether a KryoSerializer instance or a SparkSqlSerializer instance is used. This may also be related to the TODO in KryoResourcePool, which uses KryoSerializer instead of SparkSqlSerializer due to yet-to-be-investigated test failures.

An example of the divergence in behavior: The Exchange operator creates a new SparkSqlSerializer instance (with an empty conf; another issue) when it is constructed, whereas the GENERIC ColumnType pulls a KryoSerializer out of the resource pool (see above). The result is that the serialized in-memory columns are created using the user provided serializers / registrators, while serialization during exchange does not.",,apachespark,marmbrus,mhseiden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 15 23:15:24 UTC 2015,,,,,,,,,,"0|i24fs7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/15 07:05;apachespark;User 'mhseiden' has created a pull request for this issue:
https://github.com/apache/spark/pull/5237;;;","15/Apr/15 23:15;marmbrus;Issue resolved by pull request 5237
[https://github.com/apache/spark/pull/5237];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
pyspark.streaming is not included in assembly jar,SPARK-5275,12767884,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,16/Jan/15 00:00,21/Jan/15 20:45,14/Jul/23 06:27,21/Jan/15 06:46,1.2.0,1.3.0,,,,,1.2.1,1.3.0,,,,,PySpark,,,,0,,,,,,The pyspark.streaming is not included in assembly jar of spark.,,apachespark,davies,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 21 00:47:27 UTC 2015,,,,,,,,,,"0|i24fpj:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,,,"21/Jan/15 00:47;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4128;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
CoarseGrainedExecutorBackend exits for irrelevant DisassociatedEvent,SPARK-5268,12767748,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,codingcat,codingcat,codingcat,15/Jan/15 15:16,26/Jan/15 03:29,14/Jul/23 06:27,26/Jan/15 03:29,1.2.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"In CoarseGrainedExecutorBackend, we subscribe DisassociatedEvent in executor backend actor and exit the program upon receive such event...

let's consider the following case

The user may develop an Akka-based program which starts the actor with Spark's actor system and communicate with an external actor system (e.g. an Akka-based receiver in spark streaming which communicates with an external system)  If the external actor system fails or disassociates with the actor within spark's system with purpose, we may receive DisassociatedEvent and the executor is restarted.

This is not the expected behavior.....",,apachespark,codingcat,sb58,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 15 15:20:32 UTC 2015,,,,,,,,,,"0|i24ewf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"15/Jan/15 15:20;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/4063;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
numExecutorsFailed should exclude number of killExecutor in yarn mode,SPARK-5266,12767724,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,lianhuiwang,lianhuiwang,15/Jan/15 13:42,15/Jan/15 14:02,14/Jul/23 06:27,15/Jan/15 14:02,,,,,,,,,,,,,YARN,,,,0,,,,,,"when driver request killExecutor, am will kill container and numExecutorsFailed will increment. when numExecutorsFailed> maxNumExecutorFailures in AM, AM will exit with EXIT_MAX_EXECUTOR_FAILURES reason. so numExecutorsFailed should exclude the killExecutor from driver.",,apachespark,lianhuiwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 15 13:53:24 UTC 2015,,,,,,,,,,"0|i24er3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/15 13:53;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4061;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
widen types for parameters of coalesce(),SPARK-5262,12767678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,adrian-wang,adrian-wang,15/Jan/15 08:19,02/Feb/15 02:51,14/Jul/23 06:27,02/Feb/15 02:51,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"Currently Coalesce(null, 1, null) would throw exceptions.",,adrian-wang,apachespark,marmbrus,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 02 02:51:52 UTC 2015,,,,,,,,,,"0|i24eh3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/15 08:22;apachespark;User 'adrian-wang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4057;;;","21/Jan/15 05:12;yhuai;[~adrian-wang] I tried Coalesce(null, 1, null) in HiveContext. It works. Actually, I got a NPE when I tried Coalesce(null, null, null). ;;;","21/Jan/15 05:16;adrian-wang;Currently if you try coalesce in hivecontext, it will use hive udf instead of scala build-in method.;;;","21/Jan/15 05:25;yhuai;OK i see. In HiveContext, we are still using Hive's UDF. Actually, it will be good to do the work of this JIRA and SPARK-5244 together. ;;;","21/Jan/15 05:30;adrian-wang;Yeah, so I think we'd better merge this first and add a test case in SPARK-5244 . They focus differently, one on a bug while the other on a new feature.;;;","21/Jan/15 06:00;yhuai;I am not sure they are actually focusing on different things. But if you want to do these separately, we should make sure that the behavior of coalesce is correct and the PR has unit tests.;;;","02/Feb/15 02:51;marmbrus;Issue resolved by pull request 4057
[https://github.com/apache/spark/pull/4057];;;",,,,,,,,,,,,,,,,,,,,,,
Do not submit stage until its dependencies map outputs are registered,SPARK-5259,12767653,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,SuYan,SuYan,SuYan,15/Jan/15 04:06,09/Dec/16 12:06,14/Jul/23 06:27,21/Sep/15 19:26,1.1.1,1.2.0,,,,,1.6.0,,,,,,Spark Core,,,,1,,,,,,"We should track pending tasks by partition ID instead of Task objects.

Before this, failure & retry could result in a case where a stage got submitted before the map output from its dependencies get registered. This was due to an error in the condition for registering map outputs.

More complete explanation of the original problem:

1. while shuffle stage was retry, there may have 2 taskSet running. 

we call the 2 taskSet:taskSet0.0, taskSet0.1, and we know, taskSet0.1 will re-run taskSet0.0's un-complete task

if taskSet0.0 was run all the task that the taskSet0.1 not complete yet but covered the partitions.

then stage is Available is true.
{code}
  def isAvailable: Boolean = {
    if (!isShuffleMap) {
      true
    } else {
      numAvailableOutputs == numPartitions
    }
  } 
{code}

but stage.pending task is not empty, to protect register mapStatus in mapOutputTracker.

because if task is complete success, pendingTasks is minus Task in reference-level because the task is not override hashcode() and equals()
pendingTask -= task

but numAvailableOutputs is according to partitionID.

here is the testcase to prove:

{code}
  test(""Make sure mapStage.pendingtasks is set() "" +
    ""while MapStage.isAvailable is true while stage was retry "") {
    val firstRDD = new MyRDD(sc, 6, Nil)
    val firstShuffleDep = new ShuffleDependency(firstRDD, null)
    val firstShuyffleId = firstShuffleDep.shuffleId
    val shuffleMapRdd = new MyRDD(sc, 6, List(firstShuffleDep))
    val shuffleDep = new ShuffleDependency(shuffleMapRdd, null)
    val shuffleId = shuffleDep.shuffleId
    val reduceRdd = new MyRDD(sc, 2, List(shuffleDep))
    submit(reduceRdd, Array(0, 1))
    complete(taskSets(0), Seq(
      (Success, makeMapStatus(""hostB"", 1)),
      (Success, makeMapStatus(""hostB"", 2)),
      (Success, makeMapStatus(""hostC"", 3)),
      (Success, makeMapStatus(""hostB"", 4)),
      (Success, makeMapStatus(""hostB"", 5)),
      (Success, makeMapStatus(""hostC"", 6))
    ))
    complete(taskSets(1), Seq(
      (Success, makeMapStatus(""hostA"", 1)),
      (Success, makeMapStatus(""hostB"", 2)),
      (Success, makeMapStatus(""hostA"", 1)),
      (Success, makeMapStatus(""hostB"", 2)),
      (Success, makeMapStatus(""hostA"", 1))
    ))
    runEvent(ExecutorLost(""exec-hostA""))
    runEvent(CompletionEvent(taskSets(1).tasks(0), Resubmitted, null, null, null, null))
    runEvent(CompletionEvent(taskSets(1).tasks(2), Resubmitted, null, null, null, null))
    runEvent(CompletionEvent(taskSets(1).tasks(0),
      FetchFailed(null, firstShuyffleId, -1, 0, ""Fetch Mata data failed""),
      null, null, null, null))
    scheduler.resubmitFailedStages()
    runEvent(CompletionEvent(taskSets(1).tasks(0), Success,
      makeMapStatus(""hostC"", 1), null, null, null))
    runEvent(CompletionEvent(taskSets(1).tasks(2), Success,
      makeMapStatus(""hostC"", 1), null, null, null))
    runEvent(CompletionEvent(taskSets(1).tasks(4), Success,
      makeMapStatus(""hostC"", 1), null, null, null))
    runEvent(CompletionEvent(taskSets(1).tasks(5), Success,
      makeMapStatus(""hostB"", 2), null, null, null))
    val stage = scheduler.stageIdToStage(taskSets(1).stageId)
    assert(stage.attemptId == 2)
    assert(stage.isAvailable)
    assert(stage.pendingTasks.size == 0)
  }


{code}
",,apachespark,asukhenko,irashid,rdub,rxin,stevel@apache.org,SuYan,xukun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-10370,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Dec 09 12:06:06 UTC 2016,,,,,,,,,,"0|i24ebr:",9223372036854775807,,,,,,,,,,,,,,1.6.0,,,,,,,,,,,,,"15/Jan/15 04:08;apachespark;User 'suyanNone' has created a pull request for this issue:
https://github.com/apache/spark/pull/4055;;;","27/Jul/15 16:05;apachespark;User 'squito' has created a pull request for this issue:
https://github.com/apache/spark/pull/7699;;;","19/Aug/15 18:56;rxin;I have retargeted this and downgraded it from Blocker to Critical since it's been there for a while and not a regression.;;;","21/Sep/15 19:26;irashid;Issue resolved by pull request 7699
[https://github.com/apache/spark/pull/7699];;;","09/Dec/16 12:06;xukun;[~squito] [~SuYan] Would it be possible to backport this to branch 1.5?;;;",,,,,,,,,,,,,,,,,,,,,,,,
Streaming StatefulNetworkWordCount example hangs,SPARK-5252,12767476,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,davies,LutzBuech,LutzBuech,14/Jan/15 17:09,23/Apr/15 19:37,14/Jul/23 06:27,23/Apr/15 19:37,1.2.0,,,,,,1.3.0,,,,,,DStreams,Examples,PySpark,,0,,,,,,"Running the stateful network word count example in Python (on one local node):
https://github.com/apache/spark/blob/master/examples/src/main/python/streaming/stateful_network_wordcount.py

At the beginning, when no data is streamed, empty status outputs are generated, only decorated by the current Time, e.g.:
-------------------------------------------
Time: 2015-01-14 17:58:20
-------------------------------------------

-------------------------------------------
Time: 2015-01-14 17:58:21
-------------------------------------------

As soon as I stream some data via netcat, no new status updates will show. Instead, one line saying

[Stage <number>:====================================================>                          (2 + 0) / 3]

where <number> is some integer number, e.g. 132. There is no further output on stdout.",Ubuntu Linux,LutzBuech,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/15 17:19;LutzBuech;debug.txt;https://issues.apache.org/jira/secure/attachment/12692267/debug.txt",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 09 23:20:14 UTC 2015,,,,,,,,,,"0|i24d93:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/15 17:19;LutzBuech;log at DEBUG level;;;","13/Feb/15 14:52;srowen;Looks like you have an environment problem:

{code}
java.io.IOException: HADOOP_HOME or hadoop.home.dir are not set.
{code}

Can you resolve this and then see if you have this problem?;;;","09/Mar/15 23:20;tdas;[~LutzBuech] Can you try out the latest master and see if the problem still persists? This should have been fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
"""ec2/spark_ec2.py lauch"" does not work with VPC if no public DNS or IP is available",SPARK-5242,12767367,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mdagost,vgrigor,vgrigor,14/Jan/15 06:47,08/Apr/15 20:49,14/Jul/23 06:27,08/Apr/15 20:49,,,,,,,1.4.0,,,,,,EC2,,,,1,easyfix,,,,,"How to reproduce: user starting cluster in VPC needs to wait forever:
{code}
./spark-ec2 -k key20141114 -i ~/aws/key.pem -s 1 --region=eu-west-1 --spark-version=1.2.0 --instance-type=m1.large --vpc-id=vpc-2e71dd46 --subnet-id=subnet-2571dd4d --zone=eu-west-1a  launch SparkByScript
Setting up security groups...
Searching for existing cluster SparkByScript...
Spark AMI: ami-1ae0166d
Launching instances...
Launched 1 slaves in eu-west-1a, regid = r-e70c5502
Launched master in eu-west-1a, regid = r-bf0f565a
Waiting for cluster to enter 'ssh-ready' state..........{forever}
{code}

Problem is that current code makes wrong assumption that VPC instance has public_dns_name or public ip_address. Actually more common is that VPC instance has only private_ip_address.


The bug is already fixed in my fork, I am going to submit pull request",,apachespark,nchammas,vgrigor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6588,,,,,,,SPARK-5246,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Apr 08 20:49:08 UTC 2015,,,,,,,,,,"0|i24cl3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/15 07:20;apachespark;User 'voukka' has created a pull request for this issue:
https://github.com/apache/spark/pull/4038;;;","14/Jan/15 07:21;vgrigor;This bug is fixed in https://github.com/apache/spark/pull/4038;;;","04/Apr/15 05:36;vgrigor;updated PR from upstream/master;;;","07/Apr/15 13:45;apachespark;User 'mdagost' has created a pull request for this issue:
https://github.com/apache/spark/pull/5244;;;","08/Apr/15 20:49;srowen;Issue resolved by pull request 5244
[https://github.com/apache/spark/pull/5244];;;",,,,,,,,,,,,,,,,,,,,,,,,
"JdbcRDD throws ""java.lang.AbstractMethodError: oracle.jdbc.driver.xxxxxx.isClosed()Z""",SPARK-5239,12767362,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,luogankun,luogankun,luogankun,14/Jan/15 06:00,10/Feb/15 09:19,14/Jul/23 06:27,10/Feb/15 09:19,1.1.1,1.2.0,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"I try use JdbcRDD to operate the table of Oracle database, but failed. My test code as follows:

{code}
import java.sql.DriverManager
import org.apache.spark.SparkContext
import org.apache.spark.rdd.JdbcRDD
import org.apache.spark.SparkConf

object JdbcRDD4Oracle {
  def main(args: Array[String]) {
    val sc = new SparkContext(new SparkConf().setAppName(""JdbcRDD4Oracle"").setMaster(""local[2]""))
    val rdd = new JdbcRDD(sc,
      () => getConnection, getSQL, 12987, 13055, 3,
      r => {
        (r.getObject(""HISTORY_ID""), r.getObject(""APPROVE_OPINION""))
      })
    println(rdd.collect.toList)
    
    sc.stop()
  }

  def getConnection() = {
    Class.forName(""oracle.jdbc.driver.OracleDriver"").newInstance()
    DriverManager.getConnection(""jdbc:oracle:thin:@hadoop000:1521/ORCL"", ""scott"", ""tiger"")
  }
  
  def getSQL() = {
	""select HISTORY_ID,APPROVE_OPINION from CI_APPROVE_HISTORY WHERE HISTORY_ID >=? AND HISTORY_ID <=?""  	
  }
}
{code}

Run the example, I get the following exception:
{code}
09:56:48,302 [Executor task launch worker-0] ERROR Logging$class : Error in TaskCompletionListener
java.lang.AbstractMethodError: oracle.jdbc.driver.OracleResultSetImpl.isClosed()Z
	at org.apache.spark.rdd.JdbcRDD$$anon$1.close(JdbcRDD.scala:99)
	at org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:63)
	at org.apache.spark.rdd.JdbcRDD$$anon$1$$anonfun$1.apply(JdbcRDD.scala:71)
	at org.apache.spark.rdd.JdbcRDD$$anon$1$$anonfun$1.apply(JdbcRDD.scala:71)
	at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:85)
	at org.apache.spark.TaskContext$$anonfun$markTaskCompleted$1.apply(TaskContext.scala:110)
	at org.apache.spark.TaskContext$$anonfun$markTaskCompleted$1.apply(TaskContext.scala:108)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.TaskContext.markTaskCompleted(TaskContext.scala:108)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:64)
	at org.apache.spark.scheduler.Task.run(Task.scala:54)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
09:56:48,302 [Executor task launch worker-1] ERROR Logging$class : Error in TaskCompletionListener
{code}",centos6.4 + ojdbc14,apachespark,luogankun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5481,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 10 09:19:28 UTC 2015,,,,,,,,,,"0|i24cjz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/15 06:13;apachespark;User 'luogankun' has created a pull request for this issue:
https://github.com/apache/spark/pull/4033;;;","14/Jan/15 09:51;srowen;`isClosed()` is part of the standard `ResultSet` API in Java 6, and Spark requires Java 6: http://docs.oracle.com/javase/6/docs/api/java/sql/ResultSet.html#isClosed()

Therefore I think you need a newer version of the driver to work with Java 6 and therefore Spark. This is not a Spark problem.;;;","09/Feb/15 09:51;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4470;;;","10/Feb/15 09:19;srowen;Issue resolved by pull request 4470
[https://github.com/apache/spark/pull/4470];;;",,,,,,,,,,,,,,,,,,,,,,,,,
History Server shows wrong job submission time.,SPARK-5231,12767315,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,14/Jan/15 00:11,03/Feb/15 04:02,14/Jul/23 06:27,16/Jan/15 18:06,1.3.0,,,,,,1.3.0,,,,,,Web UI,,,,0,backport-needed,,,,,"History Server doesn't show collect job submission time.
It's because JobProgressListener updates job submission time every time onJobStart method is invoked from ReplayListenerBus.",,apachespark,joshrosen,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5543,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 04:02:31 UTC 2015,,,,,,,,,,"0|i24c9j:",9223372036854775807,,,,,,,,,,,,,,1.2.2,1.3.0,,,,,,,,,,,,"14/Jan/15 00:14;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/4029;;;","03/Feb/15 04:02;joshrosen;I'm linking this JIRA to SPARK-5543, which fixes a bug introduced in this PR via the addition of an unnecessary import.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"InputOutputMetricsSuite ""input metrics when reading text file with multiple splits"" test fails in branch-1.2 SBT Jenkins build w/hadoop1.0 and hadoop2.0 profiles",SPARK-5227,12767284,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,joshrosen,joshrosen,joshrosen,13/Jan/15 21:57,14/Feb/15 01:46,14/Jul/23 06:27,14/Feb/15 01:46,1.2.1,,,,,,1.2.2,1.3.0,,,,,Spark Core,,,,0,flaky-test,,,,,"The InputOutputMetricsSuite "" input metrics when reading text file with multiple splits"" test has been failing consistently in our new {{branch-1.2}} Jenkins SBT build: https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.2-SBT/14/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.0,label=centos/testReport/junit/org.apache.spark.metrics/InputOutputMetricsSuite/input_metrics_when_reading_text_file_with_multiple_splits/

Here's the error message

{code}
ArrayBuffer(32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 16) had length 3438 instead of expected length 2
{code}

I can't reproduce this issue by running the test in isolation and it also seems to be passing in the pull request builder.  I _was_ able to reproduce the error by modifying that test to add a call to

{code}
sc.hadoopConfiguration.setLong(""fs.local.block.size"", 32)
{code}

It turns out that there's a call to this in WholeTextFileRecordReaderSuite and the execution of that suite happens before the failing InputOutputMetricsSuite test.  I've looked through the code and it doesn't look like we have any shared state between those test suites, so I suspect that there could be some Hadoop bug in older versions that's impacting us here (such as global shared state in a Hadoop class that's configured from the first test run and not cleared for the second test).

I don't have the cycles to fix this right now, but it would be great if a Hadoop expert could take a look and see if they can come up with a workaround (or spot a bug in our code). ",,apachespark,joshrosen,lianhuiwang,nchammas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5679,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Feb 13 22:48:11 UTC 2015,,,,,,,,,,"0|i24c3j:",9223372036854775807,,,,,,,,,,,,,,1.2.1,,,,,,,,,,,,,"22/Jan/15 00:56;joshrosen;I've bumped this up to a 1.2.1 blocker to see if we can find a fix, since this is preventing the unit tests from running in SBT under certain Hadoop configurations.;;;","09/Feb/15 12:16;lianhuiwang;function of computing split size in hadoop's  FileInputFormat is:
protected long computeSplitSize(long goalSize, long minSize,
                                       long blockSize) {
    return Math.max(minSize, Math.min(goalSize, blockSize));
  }
long goalSize = totalSize / (numSplits == 0 ? 1 : numSplits);
and minSize = 1
blockSize = getConf().getLong(""fs.local.block.size"", 32 * 1024 * 1024);
so when numSplits=2 and totalSize is very large, then goalSize > blockSize. if it is true, actual splitSize is blockSize, not goalSize.
In this situation, the error in this PR will be happened.;;;","13/Feb/15 19:50;joshrosen;I think this might be caused by HADOOP-8490: the test code might be getting a cached FileSystem instance that was created by an earlier test run, causing the configuration from the earlier test to be re-used here.  We could try to completely disable this caching, but this could have a large negative performance impact on Hadoop library code which assumes that FileSystem creation is cheap.  I wonder if there's a way that we can clear this cache in between our test runs, which would at least address the test-flakiness issues.;;;","13/Feb/15 22:48;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4599;;;",,,,,,,,,,,,,,,,,,,,,,,,,
parallelize list/ndarray is really slow,SPARK-5224,12767216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,davies,davies,davies,13/Jan/15 18:53,15/Jan/15 19:41,14/Jul/23 06:27,15/Jan/15 19:41,1.2.0,,,,,,1.2.1,1.3.0,,,,,PySpark,,,,0,,,,,,"After the default batchSize changed to 0 (batched based on the size of object), but parallelize() still use BatchedSerializer with batchSize=1.

Also, BatchedSerializer did not work well with list and numpy.ndarray",,apachespark,davies,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Jan 15 19:41:21 UTC 2015,,,,,,,,,,"0|i24bon:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,,,"13/Jan/15 19:01;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4024;;;","15/Jan/15 19:41;joshrosen;Issue resolved by pull request 4024
[https://github.com/apache/spark/pull/4024];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Use pickle instead of MapConvert and ListConvert in MLlib Python API,SPARK-5223,12767205,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,davies,davies,davies,13/Jan/15 18:21,13/Jan/15 20:51,14/Jul/23 06:27,13/Jan/15 20:50,,,,,,,1.2.1,1.3.0,,,,,MLlib,PySpark,,,0,,,,,,"It will introduce problems if the object in dict/list/tuple can not support by py4j, such as Vector.

Also, pickle may have better performance for larger object (less RPC).

In some cases that the object in dict/list can not be pickled (such as JavaObject), we should still use MapConvert/ListConvert.

discussion: http://apache-spark-developers-list.1001551.n3.nabble.com/Python-to-Java-object-conversion-of-numpy-array-td10065.html",,apachespark,davies,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 13 20:50:50 UTC 2015,,,,,,,,,,"0|i24bkv:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,,,"13/Jan/15 18:33;apachespark;User 'davies' has created a pull request for this issue:
https://github.com/apache/spark/pull/4023;;;","13/Jan/15 20:50;mengxr;Issue resolved by pull request 4023
[https://github.com/apache/spark/pull/4023];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
"keepPushingBlocks in BlockGenerator terminated when an exception occurs, which causes the block pushing thread to terminate and blocks receiver  ",SPARK-5220,12767147,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hshreedharan,superxma,superxma,13/Jan/15 15:08,20/May/15 08:19,14/Jul/23 06:27,20/May/15 08:19,1.2.0,,,,,,1.3.2,1.4.0,,,,,DStreams,,,,2,,,,,,"I am running a Spark streaming application with ReliableKafkaReceiver. It uses BlockGenerator to push blocks to BlockManager. However, writing WALs to HDFS may time out that causes keepPushingBlocks in BlockGenerator to terminate.

15/01/12 19:07:06 ERROR receiver.BlockGenerator: Error in block pushing thread
java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:107)
        at org.apache.spark.streaming.receiver.WriteAheadLogBasedBlockHandler.storeBlock(ReceivedBlockHandler.scala:176)
        at org.apache.spark.streaming.receiver.ReceiverSupervisorImpl.pushAndReportBlock(ReceiverSupervisorImpl.scala:160)
        at org.apache.spark.streaming.receiver.ReceiverSupervisorImpl.pushArrayBuffer(ReceiverSupervisorImpl.scala:126)
        at org.apache.spark.streaming.receiver.Receiver.store(Receiver.scala:124)
        at org.apache.spark.streaming.kafka.ReliableKafkaReceiver.org$apache$spark$streaming$kafka$ReliableKafkaReceiver$$storeBlockAndCommitOffset(ReliableKafkaReceiver.scala:207)
        at org.apache.spark.streaming.kafka.ReliableKafkaReceiver$GeneratedBlockHandler.onPushBlock(ReliableKafkaReceiver.scala:275)
        at org.apache.spark.streaming.receiver.BlockGenerator.pushBlock(BlockGenerator.scala:181)
        at org.apache.spark.streaming.receiver.BlockGenerator.org$apache$spark$streaming$receiver$BlockGenerator$$keepPushingBlocks(BlockGenerator.scala:154)
        at org.apache.spark.streaming.receiver.BlockGenerator$$anon$1.run(BlockGenerator.scala:86)

Then the block pushing thread is done and no subsequent blocks can be pushed into blockManager. In turn this blocks receiver from receiving new data.

So when running my app and the TimeoutException happens, the ReliableKafkaReceiver stays in ACTIVE status but doesn't do anything at all. The application rogues.",,jerryshao,jnadler,superxma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri May 15 13:08:25 UTC 2015,,,,,,,,,,"0|i24b87:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/15 01:39;jerryshao;Hi Max, as I said in the mail, this is an expected behavior of receiver and block generator because of locking mechanism of BlockGenerator. The receiver will block on the locks for adding data into BlockGenerator, and the BlockGenerator is waiting for pushing thread to put data into HDFS and BM. Because of unmatched speed, it is expected from my understanding.;;;","14/Jan/15 13:35;superxma;Hi Jerry, my point is that keepPushingBlocks in BlockGenerator should not terminate because of any exception thrown by the receiver. With current implementation, if an exception is thrown, even when it is not stopped, keepPushingBlocks will terminate, so does the block pushing thread. It is not a matter of speed, it finished. No subsequent blocks can be pushed to BM because there is not block pushing thread any more after the exception. This behavior contradict its name.;;;","14/Jan/15 13:42;jerryshao;Aha, got it, seems this is make sense for normal receivers, but for reliable receivers, I'm not sure is it still make sense to push the following blocks if the previous block is failed.;;;","14/Jan/15 13:54;superxma;Current situation: 
1. The TimeoutException is thrown by ReliableKafkaReceiver, receiver logs the error and do nothing else;
2. Block pushing thread in BlockGenerator is terminated due to the exception;
3. ReliableKafkaReceiver is ACTIVE but rogues afterward because blocksForPushing queue is full and no block pushing thread available to clean the queue.

So when the exception occurs, what user will see is the streaming application keeps running but sits there doing nothing (We are using some supervisor to restart the failed app. But if the app keeps running and doing nothing, we have to have some curator to monitor the data flow and kill the app in such situation). Can ReliableKafkaReceiver handle TimeoutException? For example retry pushing block when a TimeoutException is thrown? Or restart itself? Or at least fail the streaming application?;;;","14/Jan/15 14:10;jerryshao;Yeah, I totally understand what is your requirement, maybe we should figure out a more elegant way to deal with such situation. Currently there's a solution trying to partially fix the problem you mentioned, you can refer to (https://github.com/apache/spark/pull/3655). But I think it is not a thorough way because the push thread will finally exited and block the whole system.;;;","14/Jan/15 14:42;superxma;I believe https://github.com/apache/spark/pull/3655 fixes the problem. The block pushing thread will not exit when an exception happens because the exception is handled inside storeBlockAndCommitOffset. The change stops the receiver after three retries, which will also stop the BlockGenerator inside ReliableKafkaReceiver. What would happen when the receiver is stopped? Does the application fail?;;;","15/May/15 13:08;srowen;[~superxma] is this resolved then?;;;",,,,,,,,,,,,,,,,,,,,,,
Race condition in TaskSchedulerImpl and TaskSetManager,SPARK-5219,12767136,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zsxwing,zsxwing,zsxwing,13/Jan/15 14:43,03/Feb/15 05:42,14/Jul/23 06:27,03/Feb/15 05:42,1.2.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"TaskSchedulerImpl.handleTaskGettingResult, TaskSetManager.canFetchMoreResults and TaskSetManager.abort will access variables which are used in multiple threads, but they don't use a correct lock.",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 13 14:47:44 UTC 2015,,,,,,,,,,"0|i24b5r:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/15 14:47;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/4019;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restore HiveMetastoreTypes.toDataType,SPARK-5211,12767009,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,yhuai,yhuai,yhuai,13/Jan/15 01:25,16/Jan/15 07:25,14/Jul/23 06:27,16/Jan/15 07:25,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"It was a public API. Since developers are using it, we need to get it back.",,apachespark,rxin,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 14 17:40:13 UTC 2015,,,,,,,,,,"0|i24adr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"13/Jan/15 20:09;apachespark;User 'yhuai' has created a pull request for this issue:
https://github.com/apache/spark/pull/4026;;;","14/Jan/15 17:39;rxin;BTW who are the developers using it?;;;","14/Jan/15 17:40;rxin;I'm under the impression that everything in the Hive package that is not HiveContext should not have been a public API.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
union with different decimal type report error,SPARK-5203,12766802,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,guowei2,guowei2,guowei2,12/Jan/15 08:30,25/Apr/15 21:57,14/Jul/23 06:27,03/Apr/15 18:03,,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"Test case like this:
{code:sql}
create table test (a decimal(10,1));
select a from test union all select a*2 from test;
{code}
Exception thown:
{noformat}
15/01/12 16:28:54 ERROR SparkSQLDriver: Failed in [select a from test union all select a*2 from test]
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Unresolved attributes: *, tree:
'Project [*]
 'Subquery _u1
  'Union 
   Project [a#1]
    MetastoreRelation default, test, None
   Project [CAST((CAST(a#2, DecimalType()) * CAST(CAST(2, DecimalType(10,0)), DecimalType())), DecimalType(21,1)) AS _c0#0]
    MetastoreRelation default, test, None

	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$1.applyOrElse(Analyzer.scala:85)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$$anonfun$1.applyOrElse(Analyzer.scala:83)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:144)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:135)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$.apply(Analyzer.scala:83)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$CheckResolution$.apply(Analyzer.scala:81)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:34)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:410)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:410)
	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData$lzycompute(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.withCachedData(SQLContext.scala:411)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan$lzycompute(SQLContext.scala:412)
	at org.apache.spark.sql.SQLContext$QueryExecution.optimizedPlan(SQLContext.scala:412)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:417)
	at org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:415)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:421)
	at org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:421)
	at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:369)
	at org.apache.spark.sql.hive.thriftserver.AbstractSparkSQLDriver.run(AbstractSparkSQLDriver.scala:58)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:275)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:211)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
{noformat}",,apachespark,glenn.strycker@gmail.com,guowei2,lian cheng,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Apr 03 18:03:21 UTC 2015,,,,,,,,,,"0|i2494v:",9223372036854775807,,,,,,,,,,,,,,1.4.0,,,,,,,,,,,,,"12/Jan/15 08:39;apachespark;User 'guowei2' has created a pull request for this issue:
https://github.com/apache/spark/pull/4004;;;","17/Feb/15 17:32;yhuai;Just a note, seems we also need to change the merge function in StructType. There is a rule to merge two DecimalTypes with fixed precision. I think we should apply the same rule as union.;;;","21/Mar/15 15:36;lian cheng;Good point! [This line|https://github.com/apache/spark/blob/e5d2c37c68ac00a57c2542e62d1c5b4ca267c89e/sql/catalyst/src/main/scala/org/apache/spark/sql/types/dataTypes.scala#L926] is really a bug. Should apply the same rule as [here|https://github.com/apache/spark/pull/4004/files#diff-d33f6b266aab79a1708e888dc1a1caf3R339].;;;","03/Apr/15 18:03;lian cheng;Issue resolved by pull request 4004
[https://github.com/apache/spark/pull/4004];;;",,,,,,,,,,,,,,,,,,,,,,,,,
HiveContext doesn't support the Variables Substitution,SPARK-5202,12766793,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chenghao,chenghao,chenghao,12/Jan/15 07:47,22/Jan/15 01:34,14/Jul/23 06:27,22/Jan/15 01:34,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+VariableSubstitution

This is a block issue for the CLI user, it impacts the existed hql scripts from Hive.",,apachespark,chenghao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 12 07:50:44 UTC 2015,,,,,,,,,,"0|i2492v:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/15 07:50;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4003;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ParallelCollectionRDD.slice(seq, numSlices) has int overflow when dealing with inclusive range",SPARK-5201,12766784,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,advancedxy,advancedxy,advancedxy,12/Jan/15 07:22,16/Jan/15 17:31,14/Jul/23 06:27,16/Jan/15 17:31,1.0.0,,,,,,1.2.1,1.3.0,,,,,Spark Core,,,,0,rdd,,,,,"{code}
 sc.makeRDD(1 to (Int.MaxValue)).count       // result = 0
 sc.makeRDD(1 to (Int.MaxValue - 1)).count   // result = 2147483646 = Int.MaxValue - 1
 sc.makeRDD(1 until (Int.MaxValue)).count    // result = 2147483646 = Int.MaxValue - 1
{code}
More details on the discussion https://github.com/apache/spark/pull/2874",,advancedxy,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 12 07:49:44 UTC 2015,,,,,,,,,,"0|i2490v:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,,,"12/Jan/15 07:22;advancedxy;I will send a pr for this.;;;","12/Jan/15 07:49;apachespark;User 'advancedxy' has created a pull request for this issue:
https://github.com/apache/spark/pull/4002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Add comment field in Create Table Field DDL,SPARK-5196,12766713,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,OopsOutOfMemory,OopsOutOfMemory,11/Jan/15 15:35,02/Feb/15 02:42,14/Jul/23 06:27,02/Feb/15 02:42,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,Support `comment` in Create Table Field DDL,,apachespark,marmbrus,OopsOutOfMemory,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 02 02:42:13 UTC 2015,,,,,,,,,,"0|i248l3:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"11/Jan/15 15:43;apachespark;User 'OopsOutOfMemory' has created a pull request for this issue:
https://github.com/apache/spark/pull/3991;;;","12/Jan/15 06:29;apachespark;User 'OopsOutOfMemory' has created a pull request for this issue:
https://github.com/apache/spark/pull/3999;;;","02/Feb/15 02:42;marmbrus;Issue resolved by pull request 3999
[https://github.com/apache/spark/pull/3999];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
when hive table is query with alias  the cache data  lose effectiveness.,SPARK-5195,12766712,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yixiaohua,yixiaohuamaxfly,yixiaohuamaxfly,11/Jan/15 15:22,07/May/20 03:24,14/Jul/23 06:27,03/Feb/15 00:14,1.2.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"override the MetastoreRelation's sameresult method only compare databasename and table name

because in previous :
cache table t1;
select count() from t1;
it will read data from memory but the sql below will not,instead it read from hdfs:
select count() from t1 t;

because cache data is keyed by logical plan and compare with sameResult ,so when table with alias the same table 's logicalplan is not the same logical plan with out alias so modify the sameresult method only compare databasename and table name",,apachespark,marmbrus,pwendell,yixiaohua,yixiaohuamaxfly,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 03 00:34:27 UTC 2015,,,,,,,,,,"0|i248kv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/15 15:31;apachespark;User 'seayi' has created a pull request for this issue:
https://github.com/apache/spark/pull/3898;;;","03/Feb/15 00:14;marmbrus;Issue resolved by pull request 3898
[https://github.com/apache/spark/pull/3898];;;","03/Feb/15 00:34;pwendell;I reverted this from Spark 1.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
CACHE TABLE AS SELECT fails with Hive UDFs,SPARK-5187,12766567,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,marmbrus,marmbrus,marmbrus,09/Jan/15 23:24,10/Jan/15 22:35,14/Jul/23 06:27,10/Jan/15 22:35,1.2.0,,,,,,1.2.1,1.3.0,,,,,SQL,,,,0,,,,,,"{code}
  test(""CACHE TABLE with Hive UDF"") {
    sql(""CACHE TABLE udfTest AS SELECT * FROM src WHERE floor(key) = 1"")
    assertCached(table(""udfTest""))
    uncacheTable(""udfTest"")
  }
{code}",,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Jan 10 22:35:26 UTC 2015,,,,,,,,,,"0|i247tb:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"09/Jan/15 23:56;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3987;;;","10/Jan/15 22:35;marmbrus;Issue resolved by pull request 3987
[https://github.com/apache/spark/pull/3987];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Vector.equals  and Vector.hashCode are very inefficient and fail on SparseVectors with large size,SPARK-5186,12766555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yuhaoyan,derrickburns,derrickburns,09/Jan/15 22:25,12/Mar/15 08:40,14/Jul/23 06:27,12/Mar/15 08:40,1.1.1,1.2.0,1.2.1,,,,1.2.2,,,,,,MLlib,,,,0,,,,,,"The implementation of Vector.equals and Vector.hashCode are correct but slow for SparseVectors that are truly sparse.
",,apachespark,derrickburns,mengxr,yuhaoyan,,,,,,,,,,,,,,,,,,,,,,,,,,,,900,900,,0%,900,900,,,,,,,,,,,,,,,,,,,,SPARK-6288,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Thu Mar 12 08:40:53 UTC 2015,,,,,,,,,,"0|i247s7:",9223372036854775807,,,,,,,,,,,,,,1.1.2,1.2.2,,,,,,,,,,,,"09/Jan/15 22:29;srowen;It looks like equals and hashCode are based on the result of toArray in all cases. The indices matter of course to the result of toArray. What's an example of two SparseVectors that aren't correct? ;;;","09/Jan/15 22:36;derrickburns;My mistake!  I mis-read the implementation of toArray!

However, this means that the implementation is NOT incorrect, just poor.
Converting sparse vectors to dense vectors to compare them can be extremely
costly.


;;;","09/Jan/15 22:42;srowen;Agree, this could easily be specialized for much better performance. The indices are sorted anyway, so should just be a matter of comparing size, and the two arrays.;;;","12/Jan/15 05:23;apachespark;User 'hhbyyh' has created a pull request for this issue:
https://github.com/apache/spark/pull/3997;;;","15/Jan/15 07:17;derrickburns;The aforementioned pull request does fix part of the problem, however, hashCode is broken, not just inefficient. If one calls hashCode on a SparseVector with large size, one will get an insufficient memory error.  
;;;","16/Jan/15 09:30;yuhaoyan;I just updated the PR with a hashCode fix. Please help review at will.;;;","20/Jan/15 23:20;mengxr;Issue resolved by pull request 3997
[https://github.com/apache/spark/pull/3997];;;","11/Mar/15 17:47;apachespark;User 'mengxr' has created a pull request for this issue:
https://github.com/apache/spark/pull/4985;;;","11/Mar/15 17:49;mengxr;Re-opened this issue for branch-1.2.;;;","12/Mar/15 08:40;mengxr;Issue resolved by pull request 4985
[https://github.com/apache/spark/pull/4985];;;",,,,,,,,,,,,,,,,,,,
pyspark --jars does not add classes to driver class path,SPARK-5185,12766552,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,joshrosen,laserson,laserson,09/Jan/15 22:13,14/Mar/16 19:22,14/Jul/23 06:27,14/Mar/16 19:22,1.2.0,,,,,,2.0.0,,,,,,PySpark,,,,4,,,,,,"I have some random class I want access to from an Spark shell, say {{com.cloudera.science.throwaway.ThrowAway}}.  You can find the specific example I used here:

https://gist.github.com/laserson/e9e3bd265e1c7a896652

I packaged it as {{throwaway.jar}}.

If I then run {{bin/spark-shell}} like so:

{code}
bin/spark-shell --master local[1] --jars throwaway.jar
{code}

I can execute

{code}
val a = new com.cloudera.science.throwaway.ThrowAway()
{code}

Successfully.

I now run PySpark like so:
{code}
PYSPARK_DRIVER_PYTHON=ipython bin/pyspark --master local[1] --jars throwaway.jar
{code}

which gives me an error when I try to instantiate the class through Py4J:
{code}
In [1]: sc._jvm.com.cloudera.science.throwaway.ThrowAway()
---------------------------------------------------------------------------
Py4JError                                 Traceback (most recent call last)
<ipython-input-1-4eedbe023c29> in <module>()
----> 1 sc._jvm.com.cloudera.science.throwaway.ThrowAway()

/Users/laserson/repos/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py in __getattr__(self, name)
    724     def __getattr__(self, name):
    725         if name == '__call__':
--> 726             raise Py4JError('Trying to call a package.')
    727         new_fqn = self._fqn + '.' + name
    728         command = REFLECTION_COMMAND_NAME +\

Py4JError: Trying to call a package.
{code}

However, if I explicitly add the {{--driver-class-path}} to add the same jar
{code}
PYSPARK_DRIVER_PYTHON=ipython bin/pyspark --master local[1] --jars throwaway.jar --driver-class-path throwaway.jar
{code}

it works

{code}
In [1]: sc._jvm.com.cloudera.science.throwaway.ThrowAway()
Out[1]: JavaObject id=o18
{code}

However, the docs state that {{--jars}} should also set the driver class path.",,apachespark,CK49,ckadner,copris,EarthsonLu,freiss,holden,joshrosen,Judpom,karenyng,laserson,ltrabuco,mjhb,mnazario,neelesh77,qzhzm173227,steamer25,stevenshe,tdas,twietsma,vanzin,zjffdu,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-6027,SPARK-5975,SPARK-6301,SPARK-6047,,SPARK-6047,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Mar 14 19:22:35 UTC 2016,,,,,,,,,,"0|i247rj:",9223372036854775807,,,,,,,,,,,,,,2.0.0,,,,,,,,,,,,,"15/Jan/15 18:12;vanzin;BTW I talked to Uri offline about this. The cause is that {{sc._jvm.blah}} seems to use the system class loader to load ""blah"", and {{--jars}} adds things to the application class loader instantiated by SparkSubmit. e.g., this works:

{code}
sc._jvm.java.lang.Thread.currentThread().getContextClassLoader().loadClass(""com.cloudera.science.throwaway.ThrowAway"").newInstance()
{code}

That being said, I'm not sure what's the expectation here. {{_jvm}}, starting with an underscore, gives me the impression that it's not really supposed to be a public API.;;;","30/Jan/15 12:35;copris;I have a similar possible issue. I need to modify the pyspark/iphyton notebook *driver* classpath at runtime.

While it's possible to modify the application classpath with addJars() it's not possible to modify the driver's classpath from within pyspark/notebook.

The main use case for this is to allow users to share an ipython server process and set the classpath dynamically from within running notebooks.

A possible solution is to load the py4j.GatewayServer into a dynamic classloader who's classpath can be modified at runtime.

Clojure shell uses a solution like this, please see https://github.com/clojure/clojure/blob/master/src/jvm/clojure/lang/DynamicClassLoader.java
;;;","26/Feb/15 04:08;tdas;I also encountered this for KafkaUtils in Python. I am doing the said workaround. But we should fix this for the general case. 

;;;","20/Apr/15 18:04;steamer25;Another work-around (that requires less reflection?) is to use the SPARK_SUBMIT_CLASSPATH environment variable so that the jars get added to the system classpath of the Py4JGatewayServer.;;;","13/May/15 09:29;ltrabuco;We've been setting {{SPARK_SUBMIT_CLASSPATH}} as a workaround to this issue, but as of https://github.com/apache/spark/commit/517975d89d40a77c7186f488547eed11f79c1e97 this variable no longer exists. We're now setting {{SPARK_CLASSPATH}} as a workaround.;;;","17/Nov/15 09:26;zjffdu;I think pyspark --jars do put classes to driver class path. But the issue is sc._jvm use the system class loader rather than spark class loader as [~vanzin] mentioned. And I don't think it is good practise to call java code directly in python by using sc._jvm, instead it would be better to implement corresponding python class as a bridge between python and spark. In this case, we wouldn't get ClassNotFoundException. 

;;;","13/Mar/16 21:49;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/11687;;;","14/Mar/16 19:22;joshrosen;Issue resolved by pull request 11687
[https://github.com/apache/spark/pull/11687];;;",,,,,,,,,,,,,,,,,,,,,
inaccurate log when WAL is disabled,SPARK-5181,12766534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,codingcat,codingcat,09/Jan/15 21:17,10/Jan/15 23:38,14/Jul/23 06:27,10/Jan/15 23:38,1.2.0,,,,,,1.2.1,1.3.0,,,,,DStreams,,,,0,,,,,,"Currently, even the logManager is not created, we still see the log entry 

s""Writing to log $record""

because of the following lines

{code:title=ReceivedBlockTracker.scala|borderStyle=solid}
/** Write an update to the tracker to the write ahead log */
  private def writeToLog(record: ReceivedBlockTrackerLogEvent) {
    logDebug(s""Writing to log $record"")
    logManagerOption.foreach { logManager =>
        logManager.writeToLog(ByteBuffer.wrap(Utils.serialize(record)))
    }
  }
{code}",,apachespark,codingcat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 09 21:21:34 UTC 2015,,,,,,,,,,"0|i247nr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/15 21:21;apachespark;User 'CodingCat' has created a pull request for this issue:
https://github.com/apache/spark/pull/3985;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spark UI history job duration is wrong,SPARK-5179,12766529,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,oliviertoupin,oliviertoupin,09/Jan/15 21:06,05/Jan/16 21:51,14/Jul/23 06:27,05/Jan/16 21:51,1.2.0,,,,,,,,,,,,Web UI,,,,0,,,,,,"In the Web UI, the jobs duration times are wrong when using reviewing the job with the history. The stages duration times are ok.

Jobs are shown with milliseconds duration, which is wrong. However, it's only an history issue, while the job is running, it works.

More details in that discussion on the mailing list:

http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-UI-history-job-duration-is-wrong-tc10010.html
",,donnchadh,joshrosen,oliviertoupin,rdub,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 05 21:51:11 UTC 2016,,,,,,,,,,"0|i247mv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/16 21:51;joshrosen;I believe that a duplicate of this issue was fixed for Spark 1.2.1 or 1.3 or something like that. Please re-open if this is still an issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift server fails with confusing error message when deploy-mode is cluster,SPARK-5176,12766047,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tpanning,tpanning,tpanning,09/Jan/15 14:54,02/Feb/15 01:58,14/Jul/23 06:27,02/Feb/15 01:58,1.1.0,1.2.0,,,,,1.3.0,,,,,,SQL,,,,0,starter,,,,,"With Spark 1.2.0, when I try to run
{noformat}
$SPARK_HOME/sbin/start-thriftserver.sh --deploy-mode cluster --master spark://xd-spark.xdata.data-tactics-corp.com:7077
{noformat}
The log output is
{noformat}
Spark assembly has been built with Hive, including Datanucleus jars on classpath
Spark Command: /usr/java/latest/bin/java -cp ::/home/tpanning/Projects/spark/spark-1.2.0-bin-hadoop2.4/sbin/../conf:/home/tpanning/Projects/spark/spark-1.2.0-bin-hadoop2.4/lib/spark-assembly-1.2.0-hadoop2.4.0.jar:/home/tpanning/Projects/spark/spark-1.2.0-bin-hadoop2.4/lib/datanucleus-core-3.2.10.jar:/home/tpanning/Projects/spark/spark-1.2.0-bin-hadoop2.4/lib/datanucleus-rdbms-3.2.9.jar:/home/tpanning/Projects/spark/spark-1.2.0-bin-hadoop2.4/lib/datanucleus-api-jdo-3.2.6.jar -XX:MaxPermSize=128m -Xms512m -Xmx512m org.apache.spark.deploy.SparkSubmit --class org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 --deploy-mode cluster --master spark://xd-spark.xdata.data-tactics-corp.com:7077 spark-internal
========================================

Jar url 'spark-internal' is not in valid format.
Must be a jar file path in URL format (e.g. hdfs://host:port/XX.jar, file:///XX.jar)

Usage: DriverClient [options] launch <active-master> <jar-url> <main-class> [driver options]
Usage: DriverClient kill <active-master> <driver-id>

Options:
   -c CORES, --cores CORES        Number of cores to request (default: 1)
   -m MEMORY, --memory MEMORY     Megabytes of memory to request (default: 512)
   -s, --supervise                Whether to restart the driver on failure
   -v, --verbose                  Print more debugging output
     
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
{noformat}

I do not get this error if deploy-mode is set to client. The --deploy-mode option is described by the --help output, so I expected it to work. I checked, and this behavior seems to be present in Spark 1.1.0 as well.",,AdamGS,apachespark,pwendell,tpanning,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 21 16:33:19 UTC 2015,,,,,,,,,,"0|i244lr:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"16/Jan/15 06:27;pwendell;Yes, we should add a check here similar to the existing ones for the thriftserver class:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L143

[~tpanning] are you interested in contributing this? If not, someone else will pick it up.;;;","19/Jan/15 11:14;AdamGS;Should it be fixed in 1.2.0/1.3.0 or straight into the master branch for future releases?;;;","19/Jan/15 20:04;tpanning;[~pwendell] I can submit the patch for this.;;;","21/Jan/15 16:33;apachespark;User 'tpanningnextcen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4137;;;",,,,,,,,,,,,,,,,,,,,,,,,,
spark-examples-***.jar shades a wrong Hadoop distribution,SPARK-5172,12765988,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,zsxwing,zsxwing,09/Jan/15 10:21,12/Jan/15 20:26,14/Jul/23 06:27,12/Jan/15 20:26,,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,"Steps to check it:

1. Download  ""spark-1.2.0-bin-hadoop2.4.tgz"" from http://www.apache.org/dyn/closer.cgi/spark/spark-1.2.0/spark-1.2.0-bin-hadoop2.4.tgz
2. unzip `spark-examples-1.2.0-hadoop2.4.0.jar`.
3. There is a file called `org/apache/hadoop/package-info.class` in the jar. It doesn't exist in hadoop 2.4. 
4. Run ""javap -classpath . -private -c -v  org.apache.hadoop.package-info""
{code}
Compiled from ""package-info.java""
interface org.apache.hadoop.package-info
  SourceFile: ""package-info.java""
  RuntimeVisibleAnnotations: length = 0x24
   00 01 00 06 00 06 00 07 73 00 08 00 09 73 00 0A
   00 0B 73 00 0C 00 0D 73 00 0E 00 0F 73 00 10 00
   11 73 00 12 
  minor version: 0
  major version: 50
  Constant pool:
const #1 = Asciz	org/apache/hadoop/package-info;
const #2 = class	#1;	//  ""org/apache/hadoop/package-info""
const #3 = Asciz	java/lang/Object;
const #4 = class	#3;	//  java/lang/Object
const #5 = Asciz	package-info.java;
const #6 = Asciz	Lorg/apache/hadoop/HadoopVersionAnnotation;;
const #7 = Asciz	version;
const #8 = Asciz	1.2.1;
const #9 = Asciz	revision;
const #10 = Asciz	1503152;
const #11 = Asciz	user;
const #12 = Asciz	mattf;
const #13 = Asciz	date;
const #14 = Asciz	Wed Jul 24 13:39:35 PDT 2013;
const #15 = Asciz	url;
const #16 = Asciz	https://svn.apache.org/repos/asf/hadoop/common/branches/branch-1.2;
const #17 = Asciz	srcChecksum;
const #18 = Asciz	6923c86528809c4e7e6f493b6b413a9a;
const #19 = Asciz	SourceFile;
const #20 = Asciz	RuntimeVisibleAnnotations;

{
}
{code}
The version is {{1.2.1}}

It comes because a wrong hbase version settings in examples project. Here is a part of the dependencly tree when runnning ""mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -pl examples dependency:tree""
{noformat}
[INFO] +- org.apache.hbase:hbase-testing-util:jar:0.98.7-hadoop1:compile
[INFO] |  +- org.apache.hbase:hbase-common:test-jar:tests:0.98.7-hadoop1:compile
[INFO] |  +- org.apache.hbase:hbase-server:test-jar:tests:0.98.7-hadoop1:compile
[INFO] |  |  +- com.sun.jersey:jersey-core:jar:1.8:compile
[INFO] |  |  +- com.sun.jersey:jersey-json:jar:1.8:compile
[INFO] |  |  |  +- org.codehaus.jettison:jettison:jar:1.1:compile
[INFO] |  |  |  +- com.sun.xml.bind:jaxb-impl:jar:2.2.3-1:compile
[INFO] |  |  |  \- org.codehaus.jackson:jackson-xc:jar:1.7.1:compile
[INFO] |  |  \- com.sun.jersey:jersey-server:jar:1.8:compile
[INFO] |  |     \- asm:asm:jar:3.3.1:test
[INFO] |  +- org.apache.hbase:hbase-hadoop1-compat:jar:0.98.7-hadoop1:compile
[INFO] |  +- org.apache.hbase:hbase-hadoop1-compat:test-jar:tests:0.98.7-hadoop1:compile
[INFO] |  +- org.apache.hadoop:hadoop-core:jar:1.2.1:compile
[INFO] |  |  +- xmlenc:xmlenc:jar:0.52:compile
[INFO] |  |  +- commons-configuration:commons-configuration:jar:1.6:compile
[INFO] |  |  |  +- commons-digester:commons-digester:jar:1.8:compile
[INFO] |  |  |  |  \- commons-beanutils:commons-beanutils:jar:1.7.0:compile
[INFO] |  |  |  \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
[INFO] |  |  \- commons-el:commons-el:jar:1.0:compile
[INFO] |  +- org.apache.hadoop:hadoop-test:jar:1.2.1:compile
[INFO] |  |  +- org.apache.ftpserver:ftplet-api:jar:1.0.0:compile
[INFO] |  |  +- org.apache.mina:mina-core:jar:2.0.0-M5:compile
[INFO] |  |  +- org.apache.ftpserver:ftpserver-core:jar:1.0.0:compile
[INFO] |  |  \- org.apache.ftpserver:ftpserver-deprecated:jar:1.0.0-M2:compile
[INFO] |  +- com.github.stephenc.findbugs:findbugs-annotations:jar:1.3.9-1:compile
[INFO] |  \- junit:junit:jar:4.10:test
[INFO] |     \- org.hamcrest:hamcrest-core:jar:1.1:test
{noformat}

If I ran `mvn -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -pl examples -am dependency:tree -Dhbase.profile=hadoop2`, the dependency tree is right.

",,apachespark,yunsh,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 11 20:12:25 UTC 2015,,,,,,,,,,"0|i2448n:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/15 10:34;zsxwing;If someone puts the spark-examples.jar in the classpath, it may use a wrong hadoop client.;;;","09/Jan/15 12:21;srowen;It's not quite that -- it's that the HBase examples are being compiled with the hadoop1 compat libraries. 

You have to set {{-Phbase-hadoop2}} when building for Hadoop 2, or set {{-Dhbase.profile=hadoop2}}. 

This is sort of what I was worried about when these two new profiles were introduced with https://issues.apache.org/jira/browse/SPARK-1297 , that nobody is going to remember to set this correctly, and indeed nothing in the Spark build sets these.

Solutions:

- Update build scripts and docs to set this profile for Hadoop 2 builds
- Push this logic into the existing {{hadoop-2.x}} profiles in the parent
- Make Hadoop 2 the default anyway to make this less likely to come up anyway (cf https://issues.apache.org/jira/browse/SPARK-5134 )
;;;","11/Jan/15 20:12;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3992;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
fetch the correct max attempts,SPARK-5169,12765965,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,09/Jan/15 08:11,09/Jan/15 14:11,14/Jul/23 06:27,09/Jan/15 14:10,,,,,,,1.3.0,,,,,,YARN,,,,0,,,,,,"If we set an spark.yarn.maxAppAttempts which is larger than yarn.resourcemanager.am.max-attempts in yarn side, it will be overrided. So we should use both spark conf and yarn conf to get the correct retry number which is used to judge if it is the last attempt.",,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2165,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 09 08:15:40 UTC 2015,,,,,,,,,,"0|i2443j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/15 08:15;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/3942;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.streaming.kafka.ReliableKafkaStreamSuite,SPARK-5153,12765843,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,tdas,codingcat,codingcat,08/Jan/15 19:56,30/Apr/15 15:58,14/Jul/23 06:27,03/Feb/15 21:47,1.2.0,,,,,,1.2.2,1.3.0,,,,,DStreams,,,,0,flaky-test,,,,,"I have seen several irrelevant PR failed on this test

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/25254/consoleFull

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/25248/

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/25251/console",,apachespark,codingcat,jerryshao,tdas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 01:03:13 UTC 2015,,,,,,,,,,"0|i243cf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/15 06:51;jerryshao;Hi [~CodingCat], thanks a lot for your reporting, mainly the problem is caused in here:

{code}
    eventually(timeout(1000 milliseconds), interval(100 milliseconds)) {
      assert(
        server.apis.leaderCache.keySet.contains(TopicAndPartition(topic, partition)),
        s""Partition [$topic, $partition] metadata not propagated after timeout""
      )
    }
{code}

which trying to verify if topic is created and propagated accordingly, currently the longest timeout is 1000 ms, which might be fine for local test, but probably it will be timeout if the machine is in high load. Actually I've tested in my local machine for several rounds, this issue is hard to reproduce.

I think a possible way is to increase the timeout (1000 ms) to a large value.

;;;","09/Jan/15 12:43;codingcat;[~saisai_shao], yeah, I got up early to check whether the test case can pass in a lightly loaded jenkins, 

Same code, just rebased

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/25323/consoleFull

It passed...

So I agree with you that it's just caused by the overloaded server

but I'm not sure prolonging the timeout duration is the right way to do...as it's still possible to have a ""super overloaded"" server to make that timeout again...
;;;","09/Jan/15 14:16;jerryshao;Yes, I agree with you that increasing the timeout may not be an elegant solution, always have chance to meet the timeout. Actually this code is referred to Kafka's unit test, I will try to dig into the Kafka code to see if I can find a better solution.;;;","03/Feb/15 19:14;tdas;[~jerryshao] Any elegant solution for this? Or should we ust increase the timeout?;;;","03/Feb/15 19:22;apachespark;User 'tdas' has created a pull request for this issue:
https://github.com/apache/spark/pull/4342;;;","03/Feb/15 19:32;tdas;I increased timeout to reduce flakiness. If you can think of a better solution, please send a PR;;;","04/Feb/15 01:03;jerryshao;Hi TD, thanks a lot for your PR, currently I've no better solution instead of increasing the timeout threshold, as I remembered in the Kafka unit test, it also deal with this way, I will check again to see if we can solve this with elegant way.;;;",,,,,,,,,,,,,,,,,,,,,,
spark-network-yarn 2.11 depends on spark-network-shuffle 2.10,SPARK-5143,12765722,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,srowen,aniket,aniket,08/Jan/15 10:16,05/Mar/15 05:01,14/Jul/23 06:27,05/Mar/15 05:01,1.2.1,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,"It seems that spark-network-yarn compiled for scala 2.11 depends on spark-network-shuffle compiled for scala 2.10. This causes builds with SBT 0.13.7 to fail with the error ""Conflicting cross-version suffixes"".

Screenshot of dependency: http://www.uploady.com/#!/download/6Yn95UZA0DR/3taAJFjCJjrsSXOR",,aniket,apachespark,pwendell,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Mar 04 21:03:51 UTC 2015,,,,,,,,,,"0|i242mf:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"03/Mar/15 14:34;srowen;Yes, the same thing appears for 1.2.1:
http://mvnrepository.com/artifact/org.apache.spark/spark-network-yarn_2.11/1.2.1

I tried building for Scala 2.11 and got the correct dependencies:

{code}
[INFO] org.apache.spark:spark-network-yarn_2.11:jar:1.3.0-SNAPSHOT
[INFO] +- org.apache.spark:spark-network-shuffle_2.11:jar:1.3.0-SNAPSHOT:compile
[INFO] |  \- org.apache.spark:spark-network-common_2.11:jar:1.3.0-SNAPSHOT:compile
[INFO] |     +- io.netty:netty-all:jar:4.0.23.Final:compile
[INFO] |     \- com.google.guava:guava:jar:14.0.1:provided
{code}

But I added a dep to my project:

{code}
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-network-yarn_2.11</artifactId>
      <version>1.2.1</version>
    </dependency>
{code}

and do indeed see

{code}
[INFO] \- org.apache.spark:spark-network-yarn_2.11:jar:1.2.1:compile
[INFO]    +- (org.apache.spark:spark-network-shuffle_2.10:jar:1.2.1:compile)
[INFO]    \- (org.spark-project.spark:unused:jar:1.0.0:compile)
{code}

I think the problem here is that the ""2.11"" must be hard-coded elsewhere into the POMs for this to work. In particular, scala.binary.version has to be set to 2.11 in the POM in order for the artifact names to be correct, since the dependency is expressed as {{spark-network-shuffle_${scala.binary.version}}}

Let me try a PR
;;;","03/Mar/15 15:42;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/4876;;;","04/Mar/15 10:54;srowen;Pardon me marking this Blocker, but I think we have to resolve this before the final release as the published artifacts are wrong otherwise. I think we almost have a fix here in the PR.;;;","04/Mar/15 21:03;pwendell;Yes - good catch Sean. Curious that this was not noticed in the past... I guess it's either because people aren't using the 2.11 build yet, or because people are just marking Spark as provided and it somehow worked.

I believe in the original patch, we expected that effective pom support would interpolate the active value for this property in the published pom. Clearly that either isn't a feature, or it's just not working correctly.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
"CaseInsensitiveMap throws ""java.io.NotSerializableException""",SPARK-5141,12765564,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,luogankun,luogankun,luogankun,08/Jan/15 07:28,10/Jan/15 04:40,14/Jul/23 06:27,10/Jan/15 04:39,,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,"The following code throws a serialization.[https://github.com/luogankun/spark-jdbc|https://github.com/luogankun/spark-jdbc]

{code}
CREATE TEMPORARY TABLE jdbc_table
USING com.luogankun.spark.jdbc
OPTIONS (
sparksql_table_schema  '(TBL_ID int, TBL_NAME string, TBL_TYPE string)',
jdbc_table_name    'TBLS',
jdbc_table_schema '(TBL_ID , TBL_NAME , TBL_TYPE)',
url    'jdbc:mysql://hadoop000:3306/hive',
user    'root',
password    'root'
);

select TBL_ID,TBL_ID,TBL_TYPE from jdbc_table;
{code}

I get the following stack trace:

{code}
org.apache.spark.SparkException: Task not serializable
        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166)
        at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158)
        at org.apache.spark.SparkContext.clean(SparkContext.scala:1448)
        at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:616)
        at org.apache.spark.sql.execution.Project.execute(basicOperators.scala:43)
        at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:81)
        at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:386)
        at org.apache.spark.sql.hive.thriftserver.AbstractSparkSQLDriver.run(AbstractSparkSQLDriver.scala:57)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:275)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:211)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:365)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:75)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.io.NotSerializableException: org.apache.spark.sql.sources.CaseInsensitiveMap
        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1183)
        at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
		......
		at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:42)
        at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:73)
        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:164)
{code}",,apachespark,luogankun,yhuai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Jan 09 06:37:33 UTC 2015,,,,,,,,,,"0|i242dj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/15 08:19;apachespark;User 'luogankun' has created a pull request for this issue:
https://github.com/apache/spark/pull/3944;;;","08/Jan/15 17:04;yhuai;I took a look at your repo. Seems you want to make [jdbcProps at this line|https://github.com/luogankun/spark-jdbc/blob/master/src/main/scala/com/luogankun/spark/jdbc/JDBCRelation.scala#L16] transient. Can you try it and see if the problem can be resolved? As [~scwf] mentioned in your PR, the map of parameters passed into createRelation is intended to be used to build your relation at the driver side.;;;","08/Jan/15 17:13;yhuai;Or, you can pull out needed parameters in your relation provider and pass these to your relation instead of extracting needed options in the relation class. I feel it may be better way.;;;","09/Jan/15 06:37;luogankun;Resolved;;;","09/Jan/15 06:37;luogankun;Resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,
pyspark unable to infer schema of namedtuple,SPARK-5138,12765444,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,,gmulley,gmulley,07/Jan/15 19:29,13/Jan/15 05:44,14/Jul/23 06:27,13/Jan/15 05:44,1.2.0,,,,,,1.3.0,,,,,,PySpark,SQL,,,0,,,,,,"When attempting to infer the schema of an RDD that contains namedtuples, pyspark fails to identify the records as namedtuples, resulting in it raising an error.

Example:
{noformat}
from pyspark import SparkContext
from pyspark.sql import SQLContext
from collections import namedtuple
import os

sc = SparkContext()
rdd = sc.textFile(os.path.join(os.getenv('SPARK_HOME'), 'README.md'))
TextLine = namedtuple('TextLine', 'line length')
tuple_rdd = rdd.map(lambda l: TextLine(line=l, length=len(l)))
tuple_rdd.take(5)  # This works

sqlc = SQLContext(sc)

# The following line raises an error
schema_rdd = sqlc.inferSchema(tuple_rdd)
{noformat}

The error raised is:
{noformat}
  File ""/opt/spark-1.2.0-bin-hadoop2.4/python/pyspark/worker.py"", line 107, in main
    process()
  File ""/opt/spark-1.2.0-bin-hadoop2.4/python/pyspark/worker.py"", line 98, in process
    serializer.dump_stream(func(split_index, iterator), outfile)
  File ""/opt/spark-1.2.0-bin-hadoop2.4/python/pyspark/serializers.py"", line 227, in dump_stream
    vs = list(itertools.islice(iterator, batch))
  File ""/opt/spark-1.2.0-bin-hadoop2.4/python/pyspark/rdd.py"", line 1107, in takeUpToNumLeft
    yield next(iterator)
  File ""/opt/spark-1.2.0-bin-hadoop2.4/python/pyspark/sql.py"", line 816, in convert_struct
    raise ValueError(""unexpected tuple: %s"" % obj)
TypeError: not all arguments converted during string formatting
{noformat}

",,apachespark,gmulley,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 13 05:44:59 UTC 2015,,,,,,,,,,"0|i241nb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/15 14:06;apachespark;User 'mulby' has created a pull request for this issue:
https://github.com/apache/spark/pull/3978;;;","13/Jan/15 05:44;marmbrus;Issue resolved by pull request 3978
[https://github.com/apache/spark/pull/3978];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
The name for get stage info atempt ID from Json was wrong,SPARK-5132,12765335,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,SuYan,SuYan,SuYan,07/Jan/15 09:31,07/Jan/15 20:10,14/Jul/23 06:27,07/Jan/15 20:10,1.2.0,,,,,,1.1.2,1.2.1,1.3.0,,,,Spark Core,Web UI,,,0,,,,,,"stageInfoToJson: Stage Attempt Id
stageInfoFromJson: Attempt Id",,apachespark,joshrosen,SuYan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 07 20:10:26 UTC 2015,,,,,,,,,,"0|i240zb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/15 09:34;apachespark;User 'suyanNone' has created a pull request for this issue:
https://github.com/apache/spark/pull/3932;;;","07/Jan/15 20:10;joshrosen;Issue resolved by pull request 3932
[https://github.com/apache/spark/pull/3932];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
A typo in configuration doc,SPARK-5131,12765331,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,uncleGen,uncleGen,uncleGen,07/Jan/15 09:02,02/Feb/15 23:46,14/Jul/23 06:27,24/Jan/15 23:35,,,,,,,1.2.1,1.3.0,,,,,,,,,0,,,,,,,,apachespark,uncleGen,wypoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5485,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Feb 02 23:46:59 UTC 2015,,,,,,,,,,"0|i240yf:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/15 09:08;apachespark;User 'uncleGen' has created a pull request for this issue:
https://github.com/apache/spark/pull/3930;;;","24/Jan/15 23:35;srowen;The PR was resolved for master and 1.2: https://github.com/apache/spark/commit/39e333ec4350ddafe29ee0958c37eec07bec85df;;;","02/Feb/15 22:05;wypoon;If this is fixed, why is the website not updated? See https://spark.apache.org/docs/latest/configuration.html. Do we really need to wait for a release before correcting the website?
;;;","02/Feb/15 23:46;srowen;Sometimes site changes reflect changes not in the current stable release, so in general it is updated with a release. Typos could be fixed directly in the interim. In this case the site will be updated very shortly for 1.2.1 anyway. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,
yarn-cluster mode should not be considered as client mode in spark-submit,SPARK-5130,12765324,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WangTaoTheTonic,WangTaoTheTonic,WangTaoTheTonic,07/Jan/15 08:25,08/Jan/15 19:47,14/Jul/23 06:27,08/Jan/15 19:47,1.2.0,,,,,,1.2.1,1.3.0,,,,,Deploy,,,,0,,,,,,"spark-submit will choose SparkSubmitDriverBootstrapper or SparkSubmit to launch according to --deploy-mode.
When submitting application using yarn-cluster we do not need to specify --deploy-mode so spark-submit will launch SparkSubmitDriverBootstrapper, and it is not proper to do this.",,apachespark,WangTaoTheTonic,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 07 08:39:39 UTC 2015,,,,,,,,,,"0|i240wv:",9223372036854775807,,,,,,,,,,,,,,1.2.1,1.3.0,,,,,,,,,,,,"07/Jan/15 08:39;apachespark;User 'WangTaoTheTonic' has created a pull request for this issue:
https://github.com/apache/spark/pull/3929;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stored as parquet doens't support the CTAS,SPARK-5121,12765287,Bug,Closed,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,wangxj8,wangxj8,07/Jan/15 03:50,07/Jan/15 06:30,14/Jul/23 06:27,07/Jan/15 06:30,1.2.0,,,,,,1.2.0,,,,,,SQL,,,07/Jan/15 00:00,0,,,,,,"In the CTAS, stored as parquet  is an unsupported Hive feature",hive-0.13.1,wangxj8,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,2015-01-07 03:50:27.0,,,,,,,,,,"0|i240on:",9223372036854775807,,,,,,,,,,,,,,1.2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException on trying to train decision tree model,SPARK-5119,12765285,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lewuathe,vvkulkarni,vvkulkarni,07/Jan/15 03:47,27/Jan/15 19:38,14/Jul/23 06:27,27/Jan/15 02:03,1.1.0,1.2.0,,,,,1.3.0,,,,,,ML,MLlib,,,0,,,,,,"First I tried to see if there was a bug raised before with similar trace. I found https://www.mail-archive.com/user@spark.apache.org/msg13708.html but the suggestion to upgarde to latest code bae ( I cloned from master branch) does not fix this issue.

Issue: try to train a decision tree classifier on some data.After training and when it begins colllect, it crashes:

15/01/06 22:28:15 INFO BlockManagerMaster: Updated info of block rdd_52_1
15/01/06 22:28:15 ERROR Executor: Exception in task 1.0 in stage 31.0 (TID 1895)
java.lang.ArrayIndexOutOfBoundsException: -1
        at org.apache.spark.mllib.tree.impurity.GiniAggregator.update(Gini.scala:93)
        at org.apache.spark.mllib.tree.impl.DTStatsAggregator.update(DTStatsAggregator.scala:100)
        at org.apache.spark.mllib.tree.DecisionTree$.orderedBinSeqOp(DecisionTree.scala:419)
        at org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$nodeBinSeqOp$1(DecisionTree.scala:511)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:536
)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1$1.apply(DecisionTree.scala:533
)
        at scala.collection.immutable.Map$Map1.foreach(Map.scala:109)
        at org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$binSeqOp$1(DecisionTree.scala:533)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:628)
        at org.apache.spark.mllib.tree.DecisionTree$$anonfun$6$$anonfun$apply$8.apply(DecisionTree.scala:628)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)

Minimal code:
 data = MLUtils.loadLibSVMFile(sc, '/scratch1/vivek/datasets/private/a1a').cache()

model = DecisionTree.trainClassifier(data, numClasses=2, categoricalFeaturesInfo={}, maxDepth=5, maxBins=100)

Just download the data from: http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/a1a
",Linux ubuntu 14.04,apachespark,lewuathe,mengxr,ngarneau,vvkulkarni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 27 19:38:50 UTC 2015,,,,,,,,,,"0|i240o7:",9223372036854775807,,,,,,,,,,,,,,1.3.0,,,,,,,,,,,,,"09/Jan/15 00:58;lewuathe;I think impurity implemented MLlib cannot keep negative labels. In this case it is -1.
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/tree/impurity/Gini.scala#L93
Should impurity support negative label?;;;","09/Jan/15 10:00;srowen;Yes, the input must contain categories that are nonnegative integers. I think this is a reasonable restriction. Although MLUtils.loadLibSVMFile will convert the 1-based feature numbers to 0-based, it leaves the -1 / 1 target untouched. Simply map your -1 labels to 0.;;;","09/Jan/15 12:40;apachespark;User 'Lewuathe' has created a pull request for this issue:
https://github.com/apache/spark/pull/3975;;;","27/Jan/15 02:03;mengxr;Issue resolved by pull request 3975
[https://github.com/apache/spark/pull/3975];;;","27/Jan/15 19:38;ngarneau;Hey guys, I am wondering what you think about letting the user control if its feature vectors are 0-based or 1-based. I used to have 0-based vectors for my datasets (worked a lot with scikit-learn) and I saw in the loadLibSVMFile function that you are ""converting"" any vectors to a 0-based...
Thought it would be cool to add a optional parameters or something...
Thanks guys, I'd be glad to give you some help :);;;",,,,,,,,,,,,,,,,,,,,,,,,
"""Create table test stored as parquet as select ..."" report error",SPARK-5118,12765282,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,guowei2,guowei2,07/Jan/15 03:10,04/Feb/15 23:26,14/Jul/23 06:27,04/Feb/15 23:26,1.3.0,,,,,,1.3.0,,,,,,SQL,,,,0,,,,,,Caused by: java.lang.RuntimeException: Unhandled clauses: TOK_TBLPARQUETFILE,,apachespark,guowei2,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Feb 04 23:26:56 UTC 2015,,,,,,,,,,"0|i240nj:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/15 03:14;apachespark;User 'guowei2' has created a pull request for this issue:
https://github.com/apache/spark/pull/3921;;;","04/Feb/15 23:26;marmbrus;Issue resolved by pull request 3921
[https://github.com/apache/spark/pull/3921];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to make jackson dependency version consistent with hadoop-2.6.0.,SPARK-5108,12765164,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhazhan,zzhan,zzhan,06/Jan/15 17:59,06/Mar/15 01:54,14/Jul/23 06:27,07/Feb/15 19:42,,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,,,apachespark,brett_s_r,zzhan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BIGTOP-1728,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sat Feb 07 19:42:14 UTC 2015,,,,,,,,,,"0|i23zxb:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/15 18:16;srowen;Let me see if I can guess the detail: are you referring to org.codehaus.jackson:jackson-jaxrs / org.codehaus.jackson:jackson-xc ?
It looks like Hadoop 2.6 now depends on them, version 1.9.13, but other Jackson deps are managed down to 1.8.8 for consistency with other deps.;;;","06/Jan/15 18:36;zzhan; [~sowen] You are right. ;;;","06/Jan/15 18:37;zzhan;java.lang.NoSuchMethodError: org.codehaus.jackson.map.ObjectMapper.setSerializationInclusion(Lorg/codehaus/jackson/map/annotate/JsonSerialize$Inclusion;)Lorg/codehaus/jackson/map/ObjectMapper;
	at org.apache.hadoop.yarn.webapp.YarnJacksonJaxbJsonProvider.configObjectMapper(YarnJacksonJaxbJsonProvider.java:59)
	at org.apache.hadoop.yarn.util.timeline.TimelineUtils.<clinit>(TimelineUtils.java:47)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceInit(YarnClientImpl.java:166)
	at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:65)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:140)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:343)
	at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:986)
	at $iwC$$iwC.<init>(<console>:9)
	at $iwC.<init>(<console>:18)
	at <init>(<console>:20)
	at .<init>(<console>:24)
	at .<clinit>(<console>)
	at .<init>(<console>:7)
	at .<clinit>(<console>)
	at $print(<console>);;;","06/Jan/15 20:47;apachespark;User 'zhzhan' has created a pull request for this issue:
https://github.com/apache/spark/pull/3914;;;","07/Jan/15 21:18;apachespark;User 'zhzhan' has created a pull request for this issue:
https://github.com/apache/spark/pull/3937;;;","07/Jan/15 22:01;apachespark;User 'zhzhan' has created a pull request for this issue:
https://github.com/apache/spark/pull/3938;;;","07/Feb/15 19:42;srowen;Issue resolved by pull request 3938
[https://github.com/apache/spark/pull/3938];;;",,,,,,,,,,,,,,,,,,,,,,
CompressedMapStatus needs to be registered with Kryo,SPARK-5102,12765097,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,lianhuiwang,darabos,darabos,06/Jan/15 11:33,12/Jan/15 18:57,14/Jul/23 06:27,12/Jan/15 18:57,1.2.0,,,,,,1.2.1,1.3.0,,,,,,,,,0,,,,,,"After upgrading from Spark 1.1.0 to 1.2.0 I got this exception:

{code}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.IllegalArgumentException: Class is not registered: org.apache.spark.scheduler.CompressedMapStatus
Note: To register this class use: kryo.register(org.apache.spark.scheduler.CompressedMapStatus.class);
	at com.esotericsoftware.kryo.Kryo.getRegistration(Kryo.java:442)
	at com.esotericsoftware.kryo.util.DefaultClassResolver.writeClass(DefaultClassResolver.java:79)
	at com.esotericsoftware.kryo.Kryo.writeClass(Kryo.java:472)
	at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:565)
	at org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:165)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:206)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

I had to register {{org.apache.spark.scheduler.CompressedMapStatus}} with Kryo. I think this should be done in {{spark/serializer/KryoSerializer.scala}}, unless instances of this class are not expected to be sent over the wire. (Maybe I'm doing something wrong?)",,apachespark,chrilisf,darabos,pwendell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 12 18:57:46 UTC 2015,,,,,,,,,,"0|i23zin:",9223372036854775807,,,,,,,,,,,,,,1.2.1,,,,,,,,,,,,,"12/Jan/15 12:47;apachespark;User 'lianhuiwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/4007;;;","12/Jan/15 18:57;pwendell;Fixed by: https://github.com/apache/spark/pull/4007;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
SparkBuild.scala assumes you are at the spark root dir,SPARK-5096,12765047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,marmbrus,marmbrus,marmbrus,06/Jan/15 03:58,18/Jan/15 01:03,14/Jul/23 06:27,18/Jan/15 01:03,,,,,,,1.3.0,,,,,,Build,,,,0,,,,,,This is bad because it breaks compiling spark as an external project ref and is generally bad SBT practice.,,apachespark,marmbrus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Jan 06 04:00:18 UTC 2015,,,,,,,,,,"0|i23z7j:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/15 04:00;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3905;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
Vector conversion broken for non-float64 arrays,SPARK-5089,12764938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,freeman-lab,freeman-lab,freeman-lab,05/Jan/15 17:00,05/Jan/15 21:12,14/Jul/23 06:27,05/Jan/15 21:12,1.2.0,,,,,,1.2.1,1.3.0,,,,,MLlib,PySpark,,,0,,,,,,"Prior to performing many MLlib operations in PySpark (e.g. KMeans), data are automatically converted to {{DenseVectors}}. If the data are numpy arrays with dtype {{float64}} this works. If data are numpy arrays with lower precision (e.g. {{float16}} or {{float32}}), they should be upcast to {{float64}}, but due to a small bug in this line this currently doesn't happen (casting is not inplace). 

{code:none}
if ar.dtype != np.float64:
    ar.astype(np.float64)
{code}
 
Non-float64 values are in turn mangled during SerDe. This can have significant consequences. For example, the following yields confusing and erroneous results:

{code:none}
from numpy import random
from pyspark.mllib.clustering import KMeans
data = sc.parallelize(random.randn(100,10).astype('float32'))
model = KMeans.train(data, k=3)
len(model.centers[0])
>> 5 # should be 10!
{code}

But this works fine:

{code:none}
data = sc.parallelize(random.randn(100,10).astype('float64'))
model = KMeans.train(data, k=3)
len(model.centers[0])
>> 10 # this is correct
{code}

The fix is trivial, I'll submit a PR shortly.",,apachespark,freeman-lab,mengxr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 05 21:12:04 UTC 2015,,,,,,,,,,"0|i23ykn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/15 20:07;apachespark;User 'freeman-lab' has created a pull request for this issue:
https://github.com/apache/spark/pull/3902;;;","05/Jan/15 21:12;mengxr;Issue resolved by pull request 3902
[https://github.com/apache/spark/pull/3902];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky test: o.a.s.scheduler.DAGSchedulerSuite,SPARK-5074,12764781,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,zsxwing,zsxwing,zsxwing,04/Jan/15 11:55,09/May/15 09:12,14/Jul/23 06:27,05/May/15 14:06,,,,,,,1.3.2,1.4.0,,,,,Spark Core,,,,0,flaky-test,,,,,"fix the following non-deterministic test in org.apache.spark.scheduler.DAGSchedulerSuite

{noformat}
[info] DAGSchedulerSuite:
[info] - [SPARK-3353] parent stage should have lower stage id *** FAILED *** (27 milliseconds)
[info]   1 did not equal 2 (DAGSchedulerSuite.scala:242)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
[info]   at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$2.apply$mcV$sp(DAGSchedulerSuite.scala:242)
[info]   at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$2.apply(DAGSchedulerSuite.scala:239)
[info]   at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$2.apply(DAGSchedulerSuite.scala:239)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.apache.spark.scheduler.DAGSchedulerSuite.org$scalatest$BeforeAndAfter$$super$runTest(DAGSchedulerSuite.scala:60)
{noformat}",,andrewor14,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue May 05 14:06:37 UTC 2015,,,,,,,,,,"0|i23xlz:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/15 11:58;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3889;;;","24/Jan/15 23:36;srowen;This was merged in https://github.com/apache/spark/commit/5c506cecb933b156b2f06a688ee08c4347bf0d47;;;","25/Feb/15 23:22;andrewor14;Reopening because this is still flaky:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.0,label=centos/1771/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.0,label=centos/1770/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.0,label=centos/1761/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB_JENKINS_BUILD_PROFILE=hadoop2.0,label=centos/1754/

{code}
""...e to stage failure: [Task serialization failed: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:101) org.apache.spark.SparkContext.broadcast(SparkContext.scala:1037) org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:839) org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:778) org.apache.spark.scheduler.DAGScheduler$$anonfun$resubmitFailedStages$3.apply(DAGScheduler.scala:590) org.apache.spark.scheduler.DAGScheduler$$anonfun$resubmitFailedStages$3.apply(DAGScheduler.scala:589) scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) org.apache.spark.scheduler.DAGScheduler.resubmitFailedStages(DAGScheduler.scala:589) org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1396) org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354) org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) ]"" did not equal ""...e to stage failure: [Exception failure in map stage]""
{code};;;","05/May/15 04:58;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/5903;;;","05/May/15 14:06;srowen;Issue resolved by pull request 5903
[https://github.com/apache/spark/pull/5903];;;",,,,,,,,,,,,,,,,,,,,,,,,
"""spark.storage.memoryMapThreshold"" has two default values",SPARK-5073,12764773,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,,zuiwanyuan,zuiwanyuan,04/Jan/15 09:55,18/Aug/17 03:32,14/Jul/23 06:27,11/Jan/15 22:04,1.2.0,,,,,,,,,,,,Spark Core,,,,0,,,,,,"In org.apache.spark.storage.DiskStore:
     val minMemoryMapBytes = blockManager.conf.getLong(""spark.storage.memoryMapThreshold"", 2 * 4096L)

In org.apache.spark.network.util.TransportConf:
     public int memoryMapBytes() {
         return conf.getInt(""spark.storage.memoryMapThreshold"", 2 * 1024 * 1024);
     }
",,apachespark,lewuathe,zuiwanyuan,扎啤,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Fri Aug 18 03:32:30 UTC 2017,,,,,,,,,,"0|i23xk7:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/15 12:08;srowen;The documentation suggests that 8192 is the intended default, and that's consistent with the javadoc that says this should be a limit near the OS page size, which is indeed I think 4KB on modern OSes (?). So open a PR to fix the default in {{TransportConf}}?;;;","05/Jan/15 12:51;apachespark;User 'Lewuathe' has created a pull request for this issue:
https://github.com/apache/spark/pull/3900;;;","05/Jan/15 12:57;lewuathe;I did not notice above comment. Sorry, I've just created PR for this issue.;;;","18/Aug/17 03:32;扎啤;I wonder what this parameter is for?Also want to know if this parameter is related to the memory-mapped file?I hope You can tell you the trouble.;;;",,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in TaskSchedulerImpl.dagScheduler,SPARK-5069,12764758,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zsxwing,zsxwing,zsxwing,04/Jan/15 08:30,05/Jan/15 05:06,14/Jul/23 06:27,05/Jan/15 05:06,,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"`TaskSchedulerImpl.dagScheduler` is set in DAGSchedulerEventProcessActor.preStart. But Akka doesn't guarantee which thread `Actor.preStart` will run in. Usually it will run in a different thread, without proper protection, `TaskSchedulerImpl.dagScheduler` may be null when it's used. The following test failure demonstrates it.

{noformat}
[info] - Scheduler does not always schedule tasks on the same workers *** FAILED *** (37 milliseconds)
[info]   java.lang.NullPointerException:
[info]   at org.apache.spark.scheduler.TaskSchedulerImpl.executorAdded(TaskSchedulerImpl.scala:459)
[info]   at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$1.apply(TaskSchedulerImpl.scala:226)
[info]   at org.apache.spark.scheduler.TaskSchedulerImpl$$anonfun$resourceOffers$1.apply(TaskSchedulerImpl.scala:221)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.apache.spark.scheduler.TaskSchedulerImpl.resourceOffers(TaskSchedulerImpl.scala:221)
[info]   at org.apache.spark.scheduler.TaskSchedulerImplSuite$$anonfun$4$$anonfun$6.apply(TaskSchedulerImplSuite.scala:287)
[info]   at org.apache.spark.scheduler.TaskSchedulerImplSuite$$anonfun$4$$anonfun$6.apply(TaskSchedulerImplSuite.scala:284)
[info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   at scala.collection.immutable.Range.foreach(Range.scala:141)
[info]   at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
{noformat}
",,apachespark,zsxwing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5072,SPARK-5070,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 04 08:46:36 UTC 2015,,,,,,,,,,"0|i23xgv:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/15 08:46;apachespark;User 'zsxwing' has created a pull request for this issue:
https://github.com/apache/spark/pull/3887;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When the path not found in the hdfs,we can't get the result",SPARK-5068,12764750,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongxu,jeanlyn,jeanlyn,04/Jan/15 06:05,04/Nov/15 20:34,14/Jul/23 06:27,12/Apr/15 01:33,1.2.0,,,,,,1.4.0,,,,,,SQL,,,,0,,,,,,"when the partion path was found in the metastore but not found in the hdfs,it will casue some problems as follow:
{noformat}
hive> show partitions partition_test;
OK
dt=1
dt=2
dt=3
dt=4
Time taken: 0.168 seconds, Fetched: 4 row(s)
{noformat}

{noformat}
hive> dfs -ls /user/jeanlyn/warehouse/partition_test;
Found 3 items
drwxr-xr-x   - jeanlyn supergroup          0 2014-12-02 16:29 /user/jeanlyn/warehouse/partition_test/dt=1
drwxr-xr-x   - jeanlyn supergroup          0 2014-12-02 16:29 /user/jeanlyn/warehouse/partition_test/dt=3
drwxr-xr-x   - jeanlyn supergroup          0 2014-12-02 17:42 /user/jeanlyn/warehouse/partition_test/dt=4
{noformat}
when i run the sql 
{noformat}
select * from partition_test limit 10
{noformat} in  *hive*,i got no problem,but when i run in *spark-sql* i get the error as follow:

{noformat}
Exception in thread ""main"" org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://jeanlyn:9000/user/jeanlyn/warehouse/partition_test/dt=2
    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:251)
    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:270)
    at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:201)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
    at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
    at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
    at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
    at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
    at scala.collection.AbstractTraversable.map(Traversable.scala:105)
    at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
    at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:205)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:203)
    at org.apache.spark.SparkContext.runJob(SparkContext.scala:1328)
    at org.apache.spark.rdd.RDD.collect(RDD.scala:780)
    at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:84)
    at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:444)
    at org.apache.spark.sql.hive.testpartition$.main(test.scala:23)
    at org.apache.spark.sql.hive.testpartition.main(test.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
{noformat}
",,apachespark,jeanlyn,njonkers,smilegator,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Nov 04 20:33:38 UTC 2015,,,,,,,,,,"0|i23xf3:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/15 16:49;apachespark;User 'jeanlyn' has created a pull request for this issue:
https://github.com/apache/spark/pull/3891;;;","07/Jan/15 10:16;apachespark;User 'jeanlyn' has created a pull request for this issue:
https://github.com/apache/spark/pull/3907;;;","04/Feb/15 06:44;apachespark;User 'chenghao-intel' has created a pull request for this issue:
https://github.com/apache/spark/pull/4356;;;","17/Mar/15 03:28;apachespark;User 'lazyman500' has created a pull request for this issue:
https://github.com/apache/spark/pull/5059;;;","04/Nov/15 17:49;njonkers;We stil see this issue on Spark 1.5:
hive can handle the missing partition but not spark-sql:

HDFS:

$ hdfs dfs -lsr /data
lsr: DEPRECATED: Please use 'ls -R' instead.
drwxr-xr-x   - hadoop hadoop          0 2015-11-04 17:40 /data/year=2015
drwxr-xr-x   - hadoop hadoop          0 2015-11-04 17:35 /data/year=2015/month=10
-rw-r--r--   1 hadoop hadoop         20 2015-11-04 17:35 /data/year=2015/month=10/names

Spark:

15/11/04 17:47:46 INFO ParseDriver: Parsing command: select * from th
15/11/04 17:47:46 INFO ParseDriver: Parse Completed
15/11/04 17:47:46 INFO MemoryStore: ensureFreeSpace(481656) called with curMem=9466, maxMem=560993402
15/11/04 17:47:46 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 470.4 KB, free 534.5 MB)
15/11/04 17:47:46 INFO MemoryStore: ensureFreeSpace(45219) called with curMem=491122, maxMem=560993402
15/11/04 17:47:46 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 44.2 KB, free 534.5 MB)
15/11/04 17:47:46 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.43.193.77:57944 (size: 44.2 KB, free: 535.0 MB)
15/11/04 17:47:46 INFO SparkContext: Created broadcast 3 from processCmd at CliDriver.java:376
15/11/04 17:47:46 INFO FileInputFormat: Total input paths to process : 1
15/11/04 17:47:46 ERROR SparkSQLDriver: Failed in [select * from th]
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://ip-10-43-193-77.ec2.internal:8020/data/year=2015/month=11
	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:251)
	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:200)
	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:279)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:207)
;;;","04/Nov/15 20:33;smilegator;Now, the default value of this feature is off. spark.sql.hive.verifyPartitionPath 

You can turn it on and do a retry. 

For more details, you can read the following JIRA:
https://issues.apache.org/jira/browse/SPARK-10198;;;",,,,,,,,,,,,,,,,,,,,,,,
BroadCast can still work after sc had been stopped.,SPARK-5065,12764744,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,carlmartin,carlmartin,04/Jan/15 02:48,17/Feb/15 22:21,14/Jul/23 06:27,17/Feb/15 22:21,1.2.1,,,,,,,,,,,,Spark Core,,,,0,,,,,,"Code as follow:
{code:borderStyle=solid}
val sc1 = new SparkContext
val sc2 = new SparkContext
sc1.stop
sc1.broadcast(1)
{code}
It can work well, because sc1.broadcast will reuse the BlockManager in sc2.
To fix it, throw a sparkException when broadCastManager had stopped.",,apachespark,carlmartin,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-5063,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Tue Feb 17 22:21:11 UTC 2015,,,,,,,,,,"0|i23xdr:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/15 02:54;apachespark;User 'SaintBacchus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3885;;;","17/Feb/15 22:21;joshrosen;This was fixed as part of my PR for SPARK-5063;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,
GraphX rmatGraph hangs,SPARK-5064,12764739,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,michaelmalak,michaelmalak,04/Jan/15 01:36,21/Jan/15 20:38,14/Jul/23 06:27,21/Jan/15 20:38,1.2.0,,,,,,1.2.1,1.3.0,,,,,GraphX,,,,0,,,,,,"org.apache.spark.graphx.util.GraphGenerators.rmatGraph(sc, 4, 8)

It just outputs ""0 edges"" and then locks up.

A spark-user message reports similar behavior:
http://mail-archives.apache.org/mod_mbox/spark-user/201408.mbox/%3C1408617621830-12570.post@n3.nabble.com%3E
",CentOS 7 REPL (no HDFS). Also tried Cloudera 5.2.0 QuickStart standalone compiled Scala with spark-submit.,ankurd,apachespark,kj-ki,michaelmalak,xhudik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Wed Jan 21 20:38:41 UTC 2015,,,,,,,,,,"0|i23xcn:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/15 13:40;xhudik;the same behavior can be seen in version 1.3.0-snapshot as well. It get stuck and eat all resources;;;","08/Jan/15 13:32;apachespark;User 'kj-ki' has created a pull request for this issue:
https://github.com/apache/spark/pull/3950;;;","21/Jan/15 20:38;ankurd;Issue resolved by pull request 3950
[https://github.com/apache/spark/pull/3950];;;",,,,,,,,,,,,,,,,,,,,,,,,,,
com.google.common.base.Optional binary has a wrong method signatures,SPARK-5052,12764455,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,elmer.garduno,elmer.garduno,elmer.garduno,01/Jan/15 17:41,27/Jan/15 01:41,14/Jul/23 06:27,27/Jan/15 01:41,1.2.0,,,,,,1.3.0,,,,,,Spark Core,,,,0,,,,,,"PR https://github.com/apache/spark/pull/1813 shaded Guava jar file and moved Guava classes to package org.spark-project.guava when Spark is built by Maven.

When a user jar uses the actual com.google.common.base.Optional transform(com.google.common.base.Function); method from Guava,  a java.lang.NoSuchMethodError: com.google.common.base.Optional.transform(Lcom/google/common/base/Function;)Lcom/google/common/base/Optional; is thrown.

The reason seems to be that the Optional class included on spark-assembly-1.2.0-hadoop1.0.4.jar has an incorrect method signature that includes the shaded class as an argument:

Expected:
javap -classpath target/scala-2.10/googlegenomics-spark-examples-assembly-1.0.jar com.google.common.base.Optional
  public abstract <V extends java/lang/Object> com.google.common.base.Optional<V> transform(com.google.common.base.Function<? super T, V>);

Found:
javap -classpath lib/spark-assembly-1.2.0-hadoop1.0.4.jar com.google.common.base.Optional
  public abstract <V extends java/lang/Object> com.google.common.base.Optional<V> transform(org.spark-project.guava.common.base.Function<? super T, V>);

",,apachespark,elmer.garduno,mfawzymkh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-2848,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Sun Jan 18 21:21:40 UTC 2015,,,,,,,,,,"0|i23w53:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jan/15 19:36;srowen;Guava is shaded, except for this one class, because it was sort of inadvertently used in the public API. It should be a shading of Guava 14. You are perhaps trying to use a different version? And that results in this ugly little conflict. Best thing is to use Guava 14 or at least ensure a different copy does not shadow this Optional at runtime. I am not sure there is otherwise a change in Spark here. ;;;","01/Jan/15 22:02;elmer.garduno;Reverting to Guava 14 still generates the same error. But including com.google.common.base.Function on the [pom|https://github.com/apache/spark/blob/master/core/pom.xml#L394] seems to generate an assembly jar with the correct method signature:

public abstract <V extends java/lang/Object> com.google.common.base.Optional<V> transform(com.google.common.base.Function<? super T, V>);

I'm now creating the distribution to test on the cluster.;;;","02/Jan/15 03:31;elmer.garduno;As mentioned in the previous comment the method signatures get fixed when including the conflicting shaded classes in core/pom.xml and excluding them (in assembly/pom.xml). Here are the signatures for the classes generated on Spark 1.2.0 and the signatures after fixing the dependencies, note how the {{or(org.spark-project.guava.common.base.Supplier)}} and {{transform(org.spark-project.guava.common.base.Function)}} have incorrect signatures. 

The problem is that If user code attempts to use this methods a {{NoSuchMethodError}} will be thrown as the classes on runtime have the incorrect signatures:

{code}
javap -classpath ../spark-1.2.0-bin-hadoop1/lib/spark-assembly-1.2.0-hadoop1.0.4.jar com.google.common.base.Optional
Compiled from ""Optional.java""
public abstract class com.google.common.base.Optional<T> implements java.io.Serializable {
  ...
  public abstract T or(org.spark-project.guava.common.base.Supplier<? extends T>);
  ...
  public abstract <V extends java/lang/Object> com.google.common.base.Optional<V> transform(org.spark-project.guava.common.base.Function<? super T, V>);
  ...
}
{code}

After adding the required classes to the pom files, the methods have the correct signatures:

{code}
javap -classpath assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop1.2.1.jar com.google.common.base.Optional
Compiled from ""Optional.java""
public abstract class com.google.common.base.Optional<T> implements java.io.Serializable {
  ...
  public abstract com.google.common.base.Optional<T> or(com.google.common.base.Optional<? extends T>);
  ...
  public abstract <V extends java/lang/Object> com.google.common.base.Optional<V> transform(com.google.common.base.Function<? super T, V>);
  ...
}
{code}

Now the tricky part, adding only the two affected classes (Function, Supplier) generates runtime problems as they have downstream dependencies:

{noformat}
Exception in thread ""main"" java.lang.VerifyError: Bad return type
Exception Details:
  Location:
    org/spark-project/guava/common/base/Equivalence.onResultOf(Lcom/google/common/base/Function;)Lorg/spark-project/guava/common/base/Equivalence; @9: areturn
  Reason:
    Type 'com/google/common/base/FunctionalEquivalence' (current frame, stack[0]) is not assignable to 'org/spark-project/guava/common/base/Equivalence' (from method signature)
  Current Frame:
    bci: @9
    flags: { }
    locals: { 'org/spark-project/guava/common/base/Equivalence', 'com/google/common/base/Function' }
    stack: { 'com/google/common/base/FunctionalEquivalence' }
  Bytecode:
    0000000: bb00 3059 2b2a b700 33b0

	at org.spark-project.guava.common.collect.MapMakerInternalMap$Strength$1.defaultEquivalence(MapMakerInternalMap.java:304)
	at org.spark-project.guava.common.collect.MapMaker.getKeyEquivalence(MapMaker.java:158)
	at org.spark-project.guava.common.collect.MapMakerInternalMap.<init>(MapMakerInternalMap.java:201)
	at org.spark-project.guava.common.collect.MapMaker.makeMap(MapMaker.java:506)
	at org.apache.spark.SparkEnv.<init>(SparkEnv.scala:77)
	at org.apache.spark.SparkEnv$.create(SparkEnv.scala:337)
	at org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:159)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:232)
	at com.google.cloud.genomics.spark.examples.GenomicsConf.newSparkContext(GenomicsConf.scala:57)
	at com.google.cloud.genomics.spark.examples.VariantsPcaDriver.<init>(VariantsPca.scala:59)
	at com.google.cloud.genomics.spark.examples.VariantsPcaDriver$.apply(VariantsPca.scala:53)
	at com.google.cloud.genomics.spark.examples.VariantsPcaDriver$.main(VariantsPca.scala:44)
	at com.google.cloud.genomics.spark.examples.VariantsPcaDriver.main(VariantsPca.scala)
{noformat}

The fix to the problem is to include all of {{com/google/common/base/*}} (or narrow it down manually to all the required deps), but that seems to defeat the purpose of these exclusions? Thoughts?

;;;","02/Jan/15 03:49;apachespark;User 'elmer-garduno' has created a pull request for this issue:
https://github.com/apache/spark/pull/3874;;;","03/Jan/15 20:15;elmer.garduno;The problem seem to be fixed when using spark-submit instead of spark-class.;;;","04/Jan/15 01:29;elmer.garduno;False alarm, still getting the same errors when running on the standalone cluster.;;;","18/Jan/15 08:27;mfawzymkh;Is there any progress on this issue.
I am trying to use facebook-swift  in a user jar and hitting this issue with Guava.

facebook swift-service 14.0 uses Guava 16 
;;;","18/Jan/15 21:21;elmer.garduno;The pull request is waiting for an admin to look at it.;;;",,,,,,,,,,,,,,,,,,,,,
ParquetTableScan always prepends the values of partition columns in output rows irrespective of the order of the partition columns in the original SELECT query,SPARK-5049,12764447,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,rahul.aggarwal,rahul.aggarwal,01/Jan/15 12:10,12/Jan/15 23:19,14/Jul/23 06:27,12/Jan/15 23:19,1.1.0,1.2.0,,,,,1.2.1,1.3.0,,,,,SQL,,,,0,,,,,,"This happens when ParquetTableScan is being used by turning on spark.sql.hive.convertMetastoreParquet

For example:

spark-sql> set spark.sql.hive.convertMetastoreParquet=true;

spark-sql> create table table1(a int , b int) partitioned by (p1 string, p2 int) ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe' STORED AS  INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat' OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat';

spark-sql> insert into table table1 partition(p1='January',p2=1) select key, 10  from src;    

spark-sql> select a, b, p1, p2 from table1 limit 10;

January	1	484	10
January	1	484	10
January	1	484	10
January	1	484	10
January	1	484	10
January	1	484	10
January	1	484	10
January	1	484	10
January	1	484	10
January	1	484	10

The correct output should be 

484	10	January	1
484	10	January	1
484	10	January	1
484	10	January	1
484	10	January	1
484	10	January	1
484	10	January	1
484	10	January	1
484	10	January	1
484	10	January	1


This also leads to schema mismatch if the query is run using HiveContext and the result is a SchemaRDD.
For example :

scala> import org.apache.spark.sql.hive._
scala> val hc = new HiveContext(sc)
scala> hc.setConf(""spark.sql.hive.convertMetastoreParquet"", ""true"")
scala> val res = hc.sql(""select a, b, p1, p2 from table1 limit 10"")
scala> res.collect
res2: Array[org.apache.spark.sql.Row] = Array([January,1,238,10], [January,1,86,10], [January,1,311,10], [January,1,27,10], [January,1,165,10], [January,1,409,10], [January,1,255,10], [January,1,278,10], [January,1,98,10], [January,1,484,10])

scala> res.schema
res5: org.apache.spark.sql.StructType = StructType(ArrayBuffer(StructField(a,IntegerType,true), StructField(b,IntegerType,true), StructField(p1,StringType,true), StructField(p2,IntegerType,true)))


",,apachespark,marmbrus,rahul.aggarwal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Mon Jan 12 23:19:45 UTC 2015,,,,,,,,,,"0|i23w3b:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jan/15 13:09;apachespark;User 'rahulaggarwalguavus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3870;;;","01/Jan/15 13:09;rahul.aggarwal;https://github.com/apache/spark/pull/3870;;;","11/Jan/15 01:00;apachespark;User 'marmbrus' has created a pull request for this issue:
https://github.com/apache/spark/pull/3990;;;","12/Jan/15 23:19;marmbrus;Issue resolved by pull request 3990
[https://github.com/apache/spark/pull/3990];;;",,,,,,,,,,,,,,,,,,,,,,,,,
