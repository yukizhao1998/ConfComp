Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Outward issue link (Regression),Outward issue link (Required),Inward issue link (Supercedes),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Authors),Custom field (Authors),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Impacts),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Platform),Custom field (Platform),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Reviewers),Custom field (Reviewers),Custom field (Reviewers),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Since Version),Custom field (Since Version),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Fix in-jvm dtest java 11 compatibility,CASSANDRA-15463,13275606,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,19/Dec/19 22:16,27/Aug/20 15:07,13/Jul/23 08:37,28/Jan/20 00:00,4.0,4.0-alpha3,,,,Test/dtest/java,,,,0,,,,The url classloader used by the in jvm dtests is not accessible by default in java 11.,,bdeggleston,dcapwell,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15524,,,,,,,,,,,,,,,,,CASSANDRA-15502,CASSANDRA-15508,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Jan 28 00:00:28 UTC 2020,,,,,,,All,,,,,"0|z09v20:",9223372036854775807,,,,,,,dcapwell,ifesdjeen,,,Low,,4.0,,,https://github.com/apache/cassandra/commit/1f7e3c2835c79363025a01a8470ee85d17457cf8,,,,,,,,,circle,,,,,"20/Dec/19 22:53;bdeggleston;[4.0|https://github.com/bdeggleston/cassandra/tree/15463-4.0]
[circle|https://circleci.com/workflow-run/f1845b60-c8be-41d1-8cea-b03f8a8bc064]\

Added a flag to ant, and reworked a test. The failing repair test is also failing on trunk.;;;","06/Jan/20 23:19;dcapwell;Started testing this now.

 

org.apache.cassandra.distributed.test.MessageForwardingTest#mutationsForwardedToAllReplicasTest is failing since the logic in the stream doesn't seem to be executing anymore.  In speaking with [~bdeggleston] seems that streams is able to avoid calling the map logic to compute count, so map gets skipped (which causes the test to fail).

 

I have yet to replicate the issue that the build change is trying to solve;;;","07/Jan/20 00:52;dcapwell;LGTM.  You could allocate less by calling \{code}.collect(Collectors.counting());\{code} but since this is a test the current version is fine.

 

+1;;;","07/Jan/20 11:42;ifesdjeen;Thank you for the patch! LGTM, +1.;;;","28/Jan/20 00:00;bdeggleston;Comitted to trunk as [1f7e3c2835c79363025a01a8470ee85d17457cf8|https://github.com/apache/cassandra/commit/1f7e3c2835c79363025a01a8470ee85d17457cf8]. FailingRepairTest is still failing, but should be addressed in CASSANDRA-15508.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Purgable tombstones can cause false positives in repaired data tracking,CASSANDRA-15462,13275498,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,19/Dec/19 13:09,21/Dec/20 08:08,13/Jul/23 08:38,23/Jan/20 10:20,4.0,4.0-alpha3,,,,Observability/Metrics,,,,0,,,,"Calculating the repaired data digest on the read path (for the purposes of detecting mismatches in the repaired data across replicas) is done before purging any tombstones due to gcgs or last repaired time. This causes false positives when repaired sstables include GC-able tombstones on some replicas but not others. 

Validation compactions do purge tombstones so it's perfectly possible for sstables to mismatch in this way without causing any streaming during repair.
",,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15461,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Correctness,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Jan 23 10:20:49 UTC 2020,,,,,,,All,,,,,"0|z09ue0:",9223372036854775807,,,,,,,marcuse,,,,Normal,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/9a280516ca8b9e730ae0648e5e29ee6280605132,,,,,,,,,Updated unit and in-jvm dtests,,,,,"19/Dec/19 13:20;samt;The linked branch is based on the fix for CASSANDRA-15461. We can't actually purge the eligible tombstones when generating the repaired data digest, as this happens before the repaired and unrepaired data is merged. Instead, we supply a {{PurgeFunction}} to the digest generating function, which uses it to test which tombstones are going to be purged from the final result, excluding them from the digest if necessary. The main complication is that we don't know whether a partition will be completely purged until after it has been processed. o work around this, we create an overall digest for the entire resultset and a temporary digest of the current partition, only adding the temporary digest to the overall if the partition is not fully purged.

||branch||CI||

|[15462-4.0|https://github.com/beobal/cassandra/tree/15462-4.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/15462-4.0]|;;;","22/Jan/20 07:45;marcuse;+1, nits;

* the comment in the {{RepairedDataPurger}} constructor should be removed ({{isForThrift}} is gone in 4.0)
* {{WithTracking}} - keep the {{protected DecoratedKey applyToPartitionKey(DecoratedKey key)}} method and call {{repairedDataInfo.onNewPartition(iterator);}} there?;;;","23/Jan/20 10:20;samt;Thanks, committed with nits addressed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Legacy counter shards can cause false positives in repaired data tracking,CASSANDRA-15461,13275496,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,19/Dec/19 13:07,21/Dec/20 08:08,13/Jul/23 08:38,23/Jan/20 10:21,4.0,4.0-alpha3,,,,Observability/Metrics,,,,0,,,,"It is expected that the serialization of legacy (pre-2.1) counter cells may differ across replicas due to the remote vs local designation of the shards. This will cause the repaired data digests calculated at read time to differ where certain legacy shards are encountered. This does not, however, indicate corruption of the repaired dataset and there isn't any action that operators can take in this scenario. Excluding counter cells which contain legacy shards from the digest calculation will avoid false positives.
",,jeromatron,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15462,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Correctness,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Jan 23 10:21:34 UTC 2020,,,,,,,All,,,,,"0|z09udk:",9223372036854775807,,,,,,,marcuse,,,,Normal,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/39eb7db65fd45653fdece1087ba75c3356a10c97,,,,,,,,,Updated unit tests,,,,,"19/Dec/19 13:13;samt;The patch abstracts the digest generation, wrapping the underlying {{Hasher}} with a new {{Digest}} class. Specializations of {{Digest}} can incorporate logic depending on their specific usage; e.g. when generating a digest for repaired data tracking, don't include legacy counter shards. This also enables a lot of {{ByteBuffer}} duplication to be removed.

||branch||CI||
|[15461-4.0|https://github.com/beobal/cassandra/tree/15461-4.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/15461-4.0]|
;;;","16/Jan/20 15:42;marcuse;+1

few extremely minor nits that you can ignore or fix on commit:
 In {{Digest}}:
* maybe use {{ThreadLocal.withInitial}} for creating {{localBuffer}}
* {{public Digest update(ByteBuffer input, int pos, int len)}} can be private
* update javadoc comment on {{public Digest updateWithCounterContext(ByteBuffer context)}} (it is not ""{{updateDigest}}"" anymore)

In {{UnfilteredPartitionIterators}}:
* double indentation on {{UnfilteredRowIterators.digest(partition, digest, version);}} in {{public static void digest(...)}};;;","23/Jan/20 10:21;samt;Thanks, committed with nits addressed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix missing call to enable RPC after native transport is started in in-jvm dtests,CASSANDRA-15460,13275484,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,drohrer,drohrer,drohrer,19/Dec/19 12:08,16/Mar/22 12:53,13/Jul/23 08:38,29/Jan/20 18:27,2.2.16,3.0.20,3.11.6,4.0,4.0-alpha3,Test/dtest/java,,,,0,pull-request-available,,,"When starting the native transport, the original patch missed the step of calling {{StorageService.instance.setRpcReady(true);}}. This appears to only be required for counter columns, but without it you can't update a counter value.

We should add this call after starting up the native transport, and set it to {{false}} during the shutdown sequence to mimic the production code.",,dcapwell,drohrer,ifesdjeen,yifanc,,,,,,,,,,,,,,,,,,,,,,,,"JeetKunDoug commented on pull request #399: CASSANDRA-15460 - Fix missing call to enable RPC after native transport is started in in-jvm dtests
URL: https://github.com/apache/cassandra/pull/399
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Dec/19 12:17;githubbot;600","smiklosovic closed pull request #399:
URL: https://github.com/apache/cassandra/pull/399


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:53;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,drohrer,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Low Hanging Fruit,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Jan 29 18:26:07 UTC 2020,,,,,,,All,,,,,"0|z09uaw:",9223372036854775807,,,,,,,dcapwell,ifesdjeen,yifanc,,Normal,,2.2.15,,,https://github.com/apache/cassandra/commit/f7ee96c74f783b42e520d26d278eafaca2a59678,,,,,,,,,Patch in Github PR #399 includes new unit test for counter updates.,,,,,"20/Dec/19 02:57;yifanc;Thanks [~drohrer]. The patch LGTM. +1;;;","03/Jan/20 19:24;dcapwell;LGTM, +1

 

Minor nit (aka feel free to ignore).  If you modify startNativeTransport and destroyNativeTransport to set this flag, then no change is needed in dtest.;;;","29/Jan/20 18:26;ifesdjeen;Thank you for the patch.

+1 from my side, too.

Committed to 2.2 with [f7ee96c74f783b42e520d26d278eafaca2a59678|https://github.com/apache/cassandra/commit/f7ee96c74f783b42e520d26d278eafaca2a59678] and merged up to [3.0|https://github.com/apache/cassandra/commit/f1d07ac198b61b6b989a4744263d98b73007c340], [3.11|https://github.com/apache/cassandra/commit/02e551aa6a062cf72e712a092166792e0e9442e9], and [trunk|https://github.com/apache/cassandra/commit/609bac6bbd424d0fe5fe149ab367eaf54110c185].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Short read protection doesn't work on group-by queries,CASSANDRA-15459,13275270,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,adelapena,jasonstack,jasonstack,18/Dec/19 14:13,27/May/21 15:53,13/Jul/23 08:38,06/Aug/20 12:30,3.11.8,4.0,4.0-beta2,,,Legacy/Coordination,,,,0,correctness,,,"[DTest to reproduce|https://github.com/apache/cassandra-dtest/compare/master...jasonstack:srp_group_by_trunk?expand=1]: it affects all versions..

{code}
In a two-node cluster with RF = 2

Execute only on Node1:
* Insert pk=1 and ck=1 with timestamp 9
* Delete pk=0 and ck=0 with timestamp 10
* Insert pk=2 and ck=2 with timestamp 9

Execute only on Node2:
* Delete pk=1 and ck=1 with timestamp 10
* Insert pk=0 and ck=0 with timestamp 9
* Delete pk=2 and ck=2 with timestamp 10

Query: ""SELECT pk, c FROM %s GROUP BY pk LIMIT 1""
* Expect no live data, but got [0, 0]
{code}


Note: for group-by queries, SRP should use ""group counted"" to calculate limits used for SRP query, rather than ""row counted"".",,abdulazizali,adelapena,aholmber,jasonstack,jeromatron,maedhroz,,,,,,,,,,,,,,,,,,,,,,"adelapena opened a new pull request #77:
URL: https://github.com/apache/cassandra-dtest/pull/77


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jun/20 16:16;githubbot;600","adelapena opened a new pull request #635:
URL: https://github.com/apache/cassandra/pull/635


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jun/20 16:24;githubbot;600","adelapena opened a new pull request #636:
URL: https://github.com/apache/cassandra/pull/636


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jun/20 16:24;githubbot;600","maedhroz commented on a change in pull request #635:
URL: https://github.com/apache/cassandra/pull/635#discussion_r465217710



##########
File path: src/java/org/apache/cassandra/service/DataResolver.java
##########
@@ -759,9 +759,10 @@ public UnfilteredPartitionIterator moreContents()
             return executeReadCommand(cmd);
         }
 
-        // Counts the number of rows for regular queries and the number of groups for GROUP BY queries
+        /** Returns the number of results counted by the counter */
         private int counted(Counter counter)
         {
+            // We are interested by the number of rows but for GROUP BY queries 'counted' returns the number of groups.
             return command.limits().isGroupByLimit()
                  ? counter.rowCounted()
                  : counter.counted();

Review comment:
       @adelapena Even if we can only do this in trunk (because of the Thrift counter in 3.11), why don't we just get rid of the `counted()` method altogether and call `counter.rowCounted()` (which I would rename to `rowsCounted()`, btw) in all cases? `rowCounted()` and `counted()` return the same thing in the non-grouping case anyway :/




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Aug/20 17:37;githubbot;600","maedhroz commented on a change in pull request #635:
URL: https://github.com/apache/cassandra/pull/635#discussion_r465218055



##########
File path: src/java/org/apache/cassandra/service/DataResolver.java
##########
@@ -904,9 +905,11 @@ public UnfilteredRowIterator moreContents()
                 return UnfilteredPartitionIterators.getOnlyElement(executeReadCommand(cmd), cmd);
             }
 
-            // Counts the number of rows for regular queries and the number of groups for GROUP BY queries
+            /** Returns the number of results counted in the partition by the counter */
             private int countedInCurrentPartition(Counter counter)
             {
+                // We are interested by the number of rows but for GROUP BY queries 'countedInCurrentPartition' returns
+                // the number of groups in the current partition.
                 return command.limits().isGroupByLimit()
                      ? counter.rowCountedInCurrentPartition()
                      : counter.countedInCurrentPartition();

Review comment:
       @adelapena Similar comment as above. `counter.rowCountedInCurrentPartition()` returns the same thing as `counter.countedInCurrentPartition()` in the non-grouping case.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Aug/20 17:37;githubbot;600","maedhroz commented on a change in pull request #635:
URL: https://github.com/apache/cassandra/pull/635#discussion_r465242986



##########
File path: src/java/org/apache/cassandra/db/filter/DataLimits.java
##########
@@ -934,7 +934,7 @@ protected Row applyToStatic(Row row)
                 // It's possible that we're ""done"" if the partition we just started bumped the number of groups (in
                 // applyToPartition() above), in which case Transformation will still call this method. In that case, we
                 // want to ignore the static row, it should (and will) be returned with the next page/group if needs be.
-                if (isDone())
+                if (enforceLimits && isDone())

Review comment:
       @adelapena It doesn't look like anything in `TestConsistency::test_group_by_srp` fails if we remove this. (i.e. It looks like the new logic is correct, just doesn't have any coverage.)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Aug/20 18:22;githubbot;600","maedhroz commented on a change in pull request #77:
URL: https://github.com/apache/cassandra-dtest/pull/77#discussion_r465248098



##########
File path: consistency_test.py
##########
@@ -1571,6 +1571,78 @@ def test_quorum_available_during_failure(self):
         for n in range(100):
             query_c1c2(session, n, cl)
 
+    @since('3.11')
+    def test_group_by_srp(self):
+        """"""
+        Test GROUP BY with short read protection, particularly when there is a limit.
+        @jira_ticket CASSANDRA-15459
+        """"""
+        cluster = self.cluster
+
+        # disable hinted handoff and set batch commit log so this doesn't interfere with the test
+        cluster.set_configuration_options(values={'hinted_handoff_enabled': False})
+        cluster.set_batch_commitlog(enabled=True)
+
+        # We want to disable read repair so it doesn't interfere with the test. On 4.0 we can just disable it on table
+        # creation, but if we are before 4.0 we'll have to use byteman to disable it.
+        use_byteman = self.dtest_config.cassandra_version_from_build < '4.0'
+
+        cluster.populate(2, install_byteman=use_byteman).start(wait_other_notice=True)
+        node1, node2 = cluster.nodelist()
+
+        session = self.patient_cql_connection(node1)
+
+        query = ""CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor':2}""
+        session.execute(query)
+
+        query = ""CREATE TABLE test.test (pk int, ck int, PRIMARY KEY (pk, ck))""
+        if not use_byteman:
+            query += "" WITH READ_REPAIR='NONE'""
+        session.execute(query)
+
+        # with node2 down, populate data on node1
+        # node1:
+        #   key 1 : 1
+        #   key 0 : x
+        #   key 2 : 2
+
+        node2.stop()

Review comment:
       @adelapena This is the sort of thing where we might be able to get real performance advantages in an in-JVM dtest, where we can just call `executeInternal()` rather than stopping a node.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Aug/20 18:30;githubbot;600","adelapena commented on a change in pull request #77:
URL: https://github.com/apache/cassandra-dtest/pull/77#discussion_r465748489



##########
File path: consistency_test.py
##########
@@ -1571,6 +1571,78 @@ def test_quorum_available_during_failure(self):
         for n in range(100):
             query_c1c2(session, n, cl)
 
+    @since('3.11')
+    def test_group_by_srp(self):
+        """"""
+        Test GROUP BY with short read protection, particularly when there is a limit.
+        @jira_ticket CASSANDRA-15459
+        """"""
+        cluster = self.cluster
+
+        # disable hinted handoff and set batch commit log so this doesn't interfere with the test
+        cluster.set_configuration_options(values={'hinted_handoff_enabled': False})
+        cluster.set_batch_commitlog(enabled=True)
+
+        # We want to disable read repair so it doesn't interfere with the test. On 4.0 we can just disable it on table
+        # creation, but if we are before 4.0 we'll have to use byteman to disable it.
+        use_byteman = self.dtest_config.cassandra_version_from_build < '4.0'
+
+        cluster.populate(2, install_byteman=use_byteman).start(wait_other_notice=True)
+        node1, node2 = cluster.nodelist()
+
+        session = self.patient_cql_connection(node1)
+
+        query = ""CREATE KEYSPACE test WITH replication = {'class':'SimpleStrategy', 'replication_factor':2}""
+        session.execute(query)
+
+        query = ""CREATE TABLE test.test (pk int, ck int, PRIMARY KEY (pk, ck))""
+        if not use_byteman:
+            query += "" WITH READ_REPAIR='NONE'""
+        session.execute(query)
+
+        # with node2 down, populate data on node1
+        # node1:
+        #   key 1 : 1
+        #   key 0 : x
+        #   key 2 : 2
+
+        node2.stop()

Review comment:
       Right, I have added an in-JVM dtest. We can take back this PR.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 14:01;githubbot;600","adelapena commented on a change in pull request #635:
URL: https://github.com/apache/cassandra/pull/635#discussion_r465795775



##########
File path: src/java/org/apache/cassandra/db/filter/DataLimits.java
##########
@@ -934,7 +934,7 @@ protected Row applyToStatic(Row row)
                 // It's possible that we're ""done"" if the partition we just started bumped the number of groups (in
                 // applyToPartition() above), in which case Transformation will still call this method. In that case, we
                 // want to ignore the static row, it should (and will) be returned with the next page/group if needs be.
-                if (isDone())
+                if (enforceLimits && isDone())

Review comment:
       Right, just added an in-JVM dtests for each condition.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 15:05;githubbot;600","adelapena commented on a change in pull request #635:
URL: https://github.com/apache/cassandra/pull/635#discussion_r465795775



##########
File path: src/java/org/apache/cassandra/db/filter/DataLimits.java
##########
@@ -934,7 +934,7 @@ protected Row applyToStatic(Row row)
                 // It's possible that we're ""done"" if the partition we just started bumped the number of groups (in
                 // applyToPartition() above), in which case Transformation will still call this method. In that case, we
                 // want to ignore the static row, it should (and will) be returned with the next page/group if needs be.
-                if (isDone())
+                if (enforceLimits && isDone())

Review comment:
       Right, just added an in-JVM dtest for each condition.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 15:05;githubbot;600","adelapena commented on a change in pull request #635:
URL: https://github.com/apache/cassandra/pull/635#discussion_r465835874



##########
File path: src/java/org/apache/cassandra/service/DataResolver.java
##########
@@ -759,9 +759,10 @@ public UnfilteredPartitionIterator moreContents()
             return executeReadCommand(cmd);
         }
 
-        // Counts the number of rows for regular queries and the number of groups for GROUP BY queries
+        /** Returns the number of results counted by the counter */
         private int counted(Counter counter)
         {
+            // We are interested by the number of rows but for GROUP BY queries 'counted' returns the number of groups.
             return command.limits().isGroupByLimit()
                  ? counter.rowCounted()
                  : counter.counted();

Review comment:
       Makes sense, done for trunk.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;05/Aug/20 16:03;githubbot;600","adelapena closed pull request #77:
URL: https://github.com/apache/cassandra-dtest/pull/77


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/20 11:46;githubbot;600","adelapena commented on pull request #77:
URL: https://github.com/apache/cassandra-dtest/pull/77#issuecomment-669879349


   Closing without merging because the tests have been done in-JVM


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Aug/20 11:47;githubbot;600","adelapena closed pull request #635:
URL: https://github.com/apache/cassandra/pull/635


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/May/21 15:53;githubbot;600","adelapena closed pull request #636:
URL: https://github.com/apache/cassandra/pull/636


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;27/May/21 15:53;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,0,9000,,,0,9000,,,,,,,,,,,,,,,,,,,,,CASSANDRA-16307,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,adelapena,,,,,,,,,,,,Correctness -> Consistency,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Aug 06 12:29:21 UTC 2020,,,,,,,All,,,,,"0|z09szc:",9223372036854775807,,,,,,,maedhroz,,,,Normal,,3.10,,,https://github.com/apache/cassandra/commit/86a9261fd94707283e4ce149f88756099e22fcb6,,,,,,,,,"New in-JVM dtests added [here|https://github.com/apache/cassandra/blob/86a9261fd94707283e4ce149f88756099e22fcb6/test/distributed/org/apache/cassandra/distributed/test/ShortReadProtectionTest.java].",,,,,"17/Jun/20 11:31;adelapena;I think that the way ""row counted"" is used to calculate limits is not the cause of the problem here. The comments about this usage in [{{ShortReadPartitionsProtection#counted}}|https://github.com/apache/cassandra/blob/4f50a6712ada5c4298ec860836015ea15049cbda/src/java/org/apache/cassandra/service/DataResolver.java#L762-L768] and [{{ShortReadPartitionsProtection.ShortReadRowsProtection#countedInCurrentPartition}}|https://github.com/apache/cassandra/blob/4f50a6712ada5c4298ec860836015ea15049cbda/src/java/org/apache/cassandra/service/DataResolver.java#L907-L913] do not seem to describe what the method is actually doing. These comments where added during CASSANDRA-10707, replacing the original comments added during CASSANDRA-13794 ([here|https://github.com/apache/cassandra/commit/2bae4ca907ac4d2ab53c899e5cf5c9e4de631f52]). I'm restoring the original description of those methods.

It seems to me that the cause of the error is [here|https://github.com/apache/cassandra/blob/4f50a6712ada5c4298ec860836015ea15049cbda/src/java/org/apache/cassandra/db/filter/DataLimits.java#L966]. Implementations of {{DataLimit.Counter}} are meant to both count results and also to limit them, being a {{StoppingTransformation}}. The method {{DataLimit.Counter#onlyCount}} allows to disable their result-limiting behaviour, so they only count results without transforming them. The counter [{{singleResultCounter}}|https://github.com/apache/cassandra/blob/4f50a6712ada5c4298ec860836015ea15049cbda/src/java/org/apache/cassandra/service/DataResolver.java#L630-L631] in short read protection uses this read-only behaviour, so it counts the replica results without truncating them, in case more replica results are needed after reconciliation. However, the method {{GroupByAwareCounter#applyToRow}} unconditionally returns a {{null}} row in case the read partition has more rows than the specified by the limit, which can violate the count-only behaviour. Something similar happens in {{GroupByAwareCounter#applyToStatic}}. The proposed PR simply takes into account the {{Counter.enforceLimits}} to prevent this filtering.

The dtest PR just adds the excellent test provided by [~jasonstack] in the description, with a minimal change to disable read-repair in 3.11 with Byteman, because we don't have the {{NONE}} repair strategy available in that version. I'm also excluding 3.0 because {{GROUP BY}} was added in 3.10.

CI results:
||branch||ci-cassandra utest||ci-cassandra dtest||CircleCI j8||CircleCI j11||
|[3.11|https://github.com/apache/cassandra/pull/635]|[126|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-test/126/]|[163|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-dtest/163/]|[link|https://app.circleci.com/pipelines/github/adelapena/cassandra/53/workflows/ce4d2cad-a811-43af-a215-b4ea71260d0e]||
|[trunk|https://github.com/apache/cassandra/pull/636]|[127|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-test/127/]|[164|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-dtest/163/]|[link|https://app.circleci.com/pipelines/github/adelapena/cassandra/55/workflows/9f73dadc-a963-43b6-8792-ea5e0c0e17c8]|[link|https://app.circleci.com/pipelines/github/adelapena/cassandra/55/workflows/56a025b0-4c18-4eae-9f77-2c261b1d2cc5]|

CC [~blerer]
;;;","04/Aug/20 17:27;maedhroz;[~adelapena] What's your opinion on whether the existing Python dtest would be better as an in-JVM dtest? One thing that might be nice in this case is that we could leave the trunk/4.0 version of the test in a simpler state, due to not needing the Byteman injection to avoid blocking read-repair there. Also, I think in-jvm tests would be able to run considerably faster, given they don't need to stop and start nodes. (i.e. They can write only to specific nodes w/ {{executeInternal()}}.)

UPDATE: After reviewing, there might be a couple tests to add anyway, so I suppose that adds to the argument for an in-JVM dtest.;;;","05/Aug/20 17:02;maedhroz;+1 on both 3.11 and trunk PRs (and the decision to discard the Python dtest PR)

Especially w/ the trunk patch, there are some nice simplifications/readability improvements.

Might want to run the unit tests one more time to account for CASSANDRA-15872 and CASSANDRA-15677.;;;","05/Aug/20 19:34;adelapena;New CI round:

||PR||utest||dtest||CircleCI j8||CircleCI j11||
|[3.11 |https://github.com/apache/cassandra/pull/635]|[206|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-test/206/]|[2|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-dtest/2/]|[link|https://app.circleci.com/pipelines/github/adelapena/cassandra/77/workflows/e0030e7d-bdc3-4584-b82d-4eea55bbc599]|-|
|[trunk|https://github.com/apache/cassandra/pull/636]|[207|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-test/207/]|[3|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-dtest/3/]|[link|https://app.circleci.com/pipelines/github/adelapena/cassandra/79/workflows/1fa1cbcf-820d-438f-97bd-91e20a50baba]|[link|https://app.circleci.com/pipelines/github/adelapena/cassandra/79/workflows/a841da8f-d565-4608-85c6-1432733faeb3]|
 ;;;","05/Aug/20 20:11;maedhroz;I haven't seen the trunk failure of [TestBootstrap::test_node_cannot_join_as_hibernating_node_without_replace_address|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-dtest/3/label=cassandra,split=16/testReport/junit/dtest.bootstrap_test/TestBootstrap/test_node_cannot_join_as_hibernating_node_without_replace_address/] before, but it may just be flaky test that hasn't been reported yet. Otherwise things look clean.;;;","05/Aug/20 20:17;maedhroz;Also linked the run with what look like unrelated failures in {{TestRepair}} to CASSANDRA-15986.;;;","05/Aug/20 20:23;adelapena;It seems that {{test_node_cannot_join_as_hibernating_node_without_replace_address}} has failed in the two last CI rounds for trunk ([259|https://ci-cassandra.apache.org/job/Cassandra-trunk/259/testReport/dtest-novnode.bootstrap_test/TestBootstrap/test_node_cannot_join_as_hibernating_node_without_replace_address/] and [258|https://ci-cassandra.apache.org/job/Cassandra-trunk/258/testReport/dtest-novnode.bootstrap_test/TestBootstrap/test_node_cannot_join_as_hibernating_node_without_replace_address/]), and doesn't seem related with the changes.;;;","05/Aug/20 21:22;maedhroz;[~adelapena] I've created CASSANDRA-16030 to follow what happens to {{test_node_cannot_join_as_hibernating_node_without_replace_address}}.;;;","06/Aug/20 12:29;adelapena;Thanks for the review.

Committed to 3.11 as [86a9261fd94707283e4ce149f88756099e22fcb6|https://github.com/apache/cassandra/commit/86a9261fd94707283e4ce149f88756099e22fcb6] and merged up to [trunk|https://github.com/apache/cassandra/commit/ae51f0fd12820707927803fdbe63581f33111d4b].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove bad assert when getting active compactions for an sstable,CASSANDRA-15457,13274942,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,17/Dec/19 09:12,03/Jan/21 17:04,13/Jul/23 08:38,25/Sep/20 05:51,4.0,4.0-beta3,,,,Local/Compaction,,,,0,,,,"CASSANDRA-14935 added a check that an sstable can only be in a single 'compaction', this is wrong. An sstable can be in a validation and a normal compaction at the same time for example.",,bdeggleston,e.dimitrova,jeromatron,marcuse,yifanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-16110,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Degradation -> Other Exception,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Sep 25 05:51:00 UTC 2020,,,,,,,All,,,,,"0|z09qyg:",9223372036854775807,,,,,,,bdeggleston,yifanc,,,Low,,4.0-alpha1,,,https://github.com/apache/cassandra/commit/8b25cd58bfa646db9e6c24c51896950da02945db,,,,,,,,,circleci run,,,,,"17/Dec/19 11:19;marcuse;[patch|https://github.com/krummas/cassandra/commits/marcuse/15457] [tests|https://circleci.com/workflow-run/f55a8412-44ec-4884-93c0-5fadc3e8cb81];;;","07/Sep/20 09:31;marcuse;ping [~bdeggleston]

new cci run: https://app.circleci.com/pipelines/github/krummas/cassandra/494/workflows/e33cf891-a041-4bc8-8858-b9089771a157

;;;","24/Sep/20 04:42;yifanc;Need a second eye?;;;","24/Sep/20 16:39;bdeggleston;+1;;;","24/Sep/20 18:17;yifanc;nit: {{!cis.isEmpty()}} is always true if {{cis}} is not null, according to the current implementation. But I think keeping the emptiness check is probably more future-proof (in case that the method can return an empty collection)

Since we are using {{StringBuilder}} already, we might just compose the entire error message using the builder, and probably also address the {{todo}}.;;;","25/Sep/20 05:51;marcuse;Committed with the string builder change

The todo is still valid - we would need to start tracking which repair session that started the anticompaction causing the conflict, which we don't do now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update generations and update comments for altered distributed system keyspaces,CASSANDRA-15454,13274734,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,16/Dec/19 11:07,21/Dec/20 08:07,13/Jul/23 08:38,16/Dec/19 17:35,4.0,4.0-alpha3,,,,Cluster/Schema,,,,0,,,,"In 4.0, we set {{memtable_flush_period_in_ms}} to 0 (from 1 hour previously), and changed default chunk length from 64KiB to 16KiB. On startup we would try to override existing schema definitions for {{system_traces}}, {{system_distributed}}, and {{system_auth}} to reflect these changes, but because these are changes to smaller values, while timestamps remain the same, these will never override existing, pre-upgrade values from 3.0 or 3.11. As a result, a schema migration request will be pushed by a node on every bounce to every replica, causing unnecessary and ultimately fruitless work.

Bumping the generations fixes the issue, and documenting the changes doesn't hurt either.",,aleksey,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Degradation,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Dec 16 17:35:27 UTC 2019,,,,,,,All,,,,,"0|z09po8:",9223372036854775807,,,,,,,samt,,,,Low,,4.0-alpha,4.0-alpha1,,"[8202845facd741f01fbfbbec93d6a3c8e6078644|https://github.com/apache/cassandra/commit/8202845facd741f01fbfbbec93d6a3c8e6078644]",,,,,,,,,N/A,,,,,"16/Dec/19 11:10;aleksey;Code: https://github.com/iamaleksey/cassandra/commits/15454-4.0;;;","16/Dec/19 13:47;samt;+1;;;","16/Dec/19 17:35;aleksey;Cheers, committed to 4.0 as [8202845facd741f01fbfbbec93d6a3c8e6078644|https://github.com/apache/cassandra/commit/8202845facd741f01fbfbbec93d6a3c8e6078644];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"in-jvm dtest cluster uncaughtExceptions propagation of exception goes to the wrong instance, it uses cluster generation when it should be using the instance id",CASSANDRA-15450,13274240,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,dcapwell,dcapwell,dcapwell,12/Dec/19 21:09,27/Aug/20 15:08,13/Jul/23 08:38,17/Jan/20 13:34,4.0,4.0-alpha3,,,,Test/dtest/java,,,,0,pull-request-available,,,"In AbstractCluster.uncaughtExceptions, we attempt to get the instance from the class loader and used the “generation”. This value is actually the cluster id, so causes tests to fail when multiple tests share the same JVM; it should be using the “id” field which represents the instance id relative to the cluster.",,dcapwell,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,"dcapwell commented on pull request #397: [CASSANDRA-15450] in-jvm dtest cluster uncaughtExceptions propagation of exception goes to the wrong instance, it uses cluster generation when it should be using the instance id
URL: https://github.com/apache/cassandra/pull/397
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 21:11;githubbot;600","JeetKunDoug commented on issue #397: [CASSANDRA-15450] in-jvm dtest cluster uncaughtExceptions propagation of exception goes to the wrong instance, it uses cluster generation when it should be using the instance id
URL: https://github.com/apache/cassandra/pull/397#issuecomment-565443767
 
 
   👍 - only question is if we really need to change the `getGeneration` method name... it ends up it's no longer used internally, and I doubt there are many (any) external users of it, but it's changing a public API which is always a bit of a concern to me.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 13:37;githubbot;600","dcapwell commented on issue #397: [CASSANDRA-15450] in-jvm dtest cluster uncaughtExceptions propagation of exception goes to the wrong instance, it uses cluster generation when it should be using the instance id
URL: https://github.com/apache/cassandra/pull/397#issuecomment-565496695
 
 
   Yeah, I get that.  This is something I talked with Alex a little about so thought good time to rename.
   
   Since it's dead code also glad to delete
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Dec/19 16:00;githubbot;600","dcapwell commented on pull request #397: [CASSANDRA-15450] in-jvm dtest cluster uncaughtExceptions propagation of exception goes to the wrong instance, it uses cluster generation when it should be using the instance id
URL: https://github.com/apache/cassandra/pull/397
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jan/20 00:48;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15456,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,dcapwell,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Jan 17 13:32:48 UTC 2020,,,,,,,All,,,,,"0|z09mmg:",9223372036854775807,,,,,,,ifesdjeen,,,,Normal,,4.0,,,https://github.com/apache/cassandra/commit/8f355ca2c25836784085f55eb464ad12ffaa1716,,,,,,,,,"Patch located here: https://github.com/apache/cassandra/pull/397

Replicated the issue in IntelliJ by selecting {code}GossipSettlesTest{code} and {code}FailingRepairTest{code} and calling run.  Before this patch, FailingRepairTest hangs until timeout, it never sees the jvm kill attempt (went to the wrong host); after this patch the correct host gets killed",,,,,"12/Dec/19 21:18;dcapwell;[~drohrer] could you review as you were the one who reported the issue?
[~ifesdjeen] could you also review?;;;","16/Dec/19 14:33;ifesdjeen;+1, LGTM. I suppose repro should be rather simple; should we add a test?

I'm also OK to remove cluster generation alltogether; just keep in mind we should do it in all branches (this patch has to be committed to all branches, too).;;;","16/Dec/19 18:09;dcapwell;{quote}should we add a test?{quote}

 

The issue happens when the second cluster created in the JVM attempts to monitor the kill request; to replicate this in a single DC we would need to create a cluster, then create a second cluster, attempt to cause the first node to be killed (disk failure policy), then wait for the failure to propagate.  

 

I don't think that test would add a lot of value, and the current test infrastructure is able to detect this and cause periodic failures; making the test flaky at the moment.  I am in favor of not adding a new test for this, but relying on the fact that the test stop being flaky.  [~ifesdjeen] sound good to you?

 

This test was added by CASSANDRA-15332 which was not back ported to the 3.x branches.  In order to get this patch to apply in 3.x we would first need to back port CASSANDRA-15332.  Speaking to [~ifesdjeen] it sounds like we want 3.x and 4.x dtests to share the same API, so we should first backport CASSANDRA-15332 before applying this patch to 3.x.;;;","16/Dec/19 20:50;dcapwell;Will file a Jira to backport;;;","08/Jan/20 15:28;ifesdjeen;+1 for not adding test. Patch LGTM as-is.

Thank you for filing a jira for back port, too!;;;","17/Jan/20 13:32;ifesdjeen;Committed only to trunk, under assumption that other branches are going to be fixed with a correct back ported version: [8f355ca2c25836784085f55eb464ad12ffaa1716|https://github.com/apache/cassandra/commit/8f355ca2c25836784085f55eb464ad12ffaa1716];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
in-jvm dtest support for subnets doesn't change seed provider subnet,CASSANDRA-15447,13273970,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,drohrer,drohrer,drohrer,11/Dec/19 21:48,16/Mar/22 12:51,13/Jul/23 08:38,18/Dec/19 17:16,2.2.16,3.0.20,3.11.8,4.0,4.0-alpha3,Test/dtest/java,,,,0,pull-request-available,,,"When using the `withSubnet` function on AbstractCluster.Builder, the newly-selected subnet is never used when setting up the SeedProvider in the constructor of InstanceConfig, which is hard-coded to 127.0.0.1. Because of this, clusters with any subnet other than 0, and gossip enabled, cannot start up as they have no seed provider in their subnet and what should be the seed (instance 1) doesn't think it is the seed.
",,dcapwell,drohrer,ifesdjeen,yifanc,,,,,,,,,,,,,,,,,,,,,,,,"JeetKunDoug commented on pull request #393: CASSANDRA-15447 - Pass correct seed node through to InstanceConfig so it can set it properly when starting clusters on non-0 subnets.
URL: https://github.com/apache/cassandra/pull/393
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 02:43;githubbot;600","JeetKunDoug commented on pull request #394: CASSANDRA-15447 - Pass correct seed node through to InstanceConfig so it can set it properly when starting clusters on non-0 subnets.
URL: https://github.com/apache/cassandra/pull/394
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 02:59;githubbot;600","JeetKunDoug commented on pull request #395: CASSANDRA-15447 - Pass correct seed node through to InstanceConfig so it can set it properly when starting clusters on non-0 subnets.
URL: https://github.com/apache/cassandra/pull/395
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 13:40;githubbot;600","JeetKunDoug commented on pull request #396: CASSANDRA-15447 - Pass correct seed node through to InstanceConfig so it can set it properly when starting clusters on non-0 subnets.
URL: https://github.com/apache/cassandra/pull/396
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 15:15;githubbot;600","yifan-c commented on pull request #396: CASSANDRA-15447 - Pass correct seed node through to InstanceConfig so it can set it properly when starting clusters on non-0 subnets.
URL: https://github.com/apache/cassandra/pull/396#discussion_r357283052
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/InstanceConfig.java
 ##########
 @@ -239,20 +240,21 @@ public String getString(String name)
         return (String)params.get(name);
     }
 
-    public static InstanceConfig generate(int nodeNum, String ipAddress, NetworkTopology networkTopology, File root, String token)
+    public static InstanceConfig generate(int nodeNum, String ipAddress, NetworkTopology networkTopology, File root, String token, String seedIp)
     {
         return new InstanceConfig(nodeNum,
                                   networkTopology,
                                   ipAddress,
                                   ipAddress,
                                   ipAddress,
                                   ipAddress,
-                                  String.format(""%s/node%d/saved_caches"", root, nodeNum),
+                seedIp, String.format(""%s/node%d/saved_caches"", root, nodeNum),
 
 Review comment:
   nit: one line per parameter
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 17:47;githubbot;600","JeetKunDoug commented on pull request #396: CASSANDRA-15447 - Pass correct seed node through to InstanceConfig so it can set it properly when starting clusters on non-0 subnets.
URL: https://github.com/apache/cassandra/pull/396#discussion_r357287625
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/InstanceConfig.java
 ##########
 @@ -239,20 +240,21 @@ public String getString(String name)
         return (String)params.get(name);
     }
 
-    public static InstanceConfig generate(int nodeNum, String ipAddress, NetworkTopology networkTopology, File root, String token)
+    public static InstanceConfig generate(int nodeNum, String ipAddress, NetworkTopology networkTopology, File root, String token, String seedIp)
     {
         return new InstanceConfig(nodeNum,
                                   networkTopology,
                                   ipAddress,
                                   ipAddress,
                                   ipAddress,
                                   ipAddress,
-                                  String.format(""%s/node%d/saved_caches"", root, nodeNum),
+                seedIp, String.format(""%s/node%d/saved_caches"", root, nodeNum),
 
 Review comment:
   Also weird indentation - will fix up.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 17:58;githubbot;600","dcapwell commented on pull request #396: CASSANDRA-15447 - Pass correct seed node through to InstanceConfig so it can set it properly when starting clusters on non-0 subnets.
URL: https://github.com/apache/cassandra/pull/396#discussion_r357295911
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/InstanceConfig.java
 ##########
 @@ -239,20 +240,22 @@ public String getString(String name)
         return (String)params.get(name);
     }
 
-    public static InstanceConfig generate(int nodeNum, String ipAddress, NetworkTopology networkTopology, File root, String token)
+    public static InstanceConfig generate(int nodeNum, String ipAddress, NetworkTopology networkTopology, File root, String token, String seedIp)
     {
         return new InstanceConfig(nodeNum,
                                   networkTopology,
                                   ipAddress,
                                   ipAddress,
                                   ipAddress,
                                   ipAddress,
+                                  seedIp,
                                   String.format(""%s/node%d/saved_caches"", root, nodeNum),
                                   new String[] { String.format(""%s/node%d/data"", root, nodeNum) },
                                   String.format(""%s/node%d/commitlog"", root, nodeNum),
                                   String.format(""%s/node%d/hints"", root, nodeNum),
                                   String.format(""%s/node%d/cdc"", root, nodeNum),
-                                  token);
+                                  token
 
 Review comment:
   nit: moving it back would shrink the diff (up to you)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 18:16;githubbot;600","JeetKunDoug commented on pull request #396: CASSANDRA-15447 - Pass correct seed node through to InstanceConfig so it can set it properly when starting clusters on non-0 subnets.
URL: https://github.com/apache/cassandra/pull/396#discussion_r357297536
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/InstanceConfig.java
 ##########
 @@ -239,20 +240,22 @@ public String getString(String name)
         return (String)params.get(name);
     }
 
-    public static InstanceConfig generate(int nodeNum, String ipAddress, NetworkTopology networkTopology, File root, String token)
+    public static InstanceConfig generate(int nodeNum, String ipAddress, NetworkTopology networkTopology, File root, String token, String seedIp)
     {
         return new InstanceConfig(nodeNum,
                                   networkTopology,
                                   ipAddress,
                                   ipAddress,
                                   ipAddress,
                                   ipAddress,
+                                  seedIp,
                                   String.format(""%s/node%d/saved_caches"", root, nodeNum),
                                   new String[] { String.format(""%s/node%d/data"", root, nodeNum) },
                                   String.format(""%s/node%d/commitlog"", root, nodeNum),
                                   String.format(""%s/node%d/hints"", root, nodeNum),
                                   String.format(""%s/node%d/cdc"", root, nodeNum),
-                                  token);
+                                  token
 
 Review comment:
   fixed
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Dec/19 18:19;githubbot;600","smiklosovic closed pull request #394:
URL: https://github.com/apache/cassandra/pull/394


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:48;githubbot;600","smiklosovic closed pull request #395:
URL: https://github.com/apache/cassandra/pull/395


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:51;githubbot;600","smiklosovic closed pull request #393:
URL: https://github.com/apache/cassandra/pull/393


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:51;githubbot;600","smiklosovic closed pull request #396:
URL: https://github.com/apache/cassandra/pull/396


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:51;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,0,7200,,,0,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,drohrer,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Low Hanging Fruit,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Dec 18 17:16:40 UTC 2019,,,,,,,All,,,,,"0|z09kyg:",9223372036854775807,,,,,,,dcapwell,yifanc,,,Normal,,2.2.15,,,https://github.com/apache/cassandra/commit/fdc055da124efc92a73bd208b4463ee5528c85e0,,,,,,,,,"To test, back-ported the GossipSettlesTest and made it use a non-zero subnet, which will fail if the patch is not applied. See GitHub PRs for patch details.",,,,,"12/Dec/19 03:11;dcapwell;[~drohrer] can you click “Open” and fill out the details.  If the code is ready for review can you also press “Parch available “ (visible after you open)

Thanks;;;","12/Dec/19 03:20;drohrer;Getting late here - will submit PRs for 3.11 and trunk in the morning, but other than IpAddress vs. IpAddressAndPort there's no difference really.;;;","12/Dec/19 15:36;drohrer;PRs for all four active branches are now available. The patch is essentially identical for all four, and require very few changes.
Separately, it appears there's something causing FailingRepairTest to fail when run with other dtests - this is true on trunk w/o my changes but I'll take a look and see if I can figure out what's bleeding over from some previous test that's causing it. ;;;","12/Dec/19 17:54;yifanc;Thanks [~drohrer]. The PRs LGTM. 

One nit: the PR to trunk has 2 parameters at the same line for the multiple-line statement. According to the [code style|http://cassandra.apache.org/doc/latest/development/code_style.html], it should be 1 pre line.;;;","12/Dec/19 18:16;dcapwell;I modified org.apache.cassandra.distributed.test.DistributedReadWritePathTest#pagingTests (it used subnet) without your patch, and it failed matching the jira description; I then partially applied your patch and it worked.

PR on trunk LGTM; +1;;;","18/Dec/19 17:16;ifesdjeen;Committed to 2.2 as [563592801aad00e002f0a162d5e2625a4b0e8723|https://github.com/apache/cassandra/commit/563592801aad00e002f0a162d5e2625a4b0e8723] and merged to [3.0|https://github.com/apache/cassandra/commit/49918efa803d6a420a281f24f9b12ca9208ba68f], [3.11|https://github.com/apache/cassandra/commit/78143bc66fbb0050fcbdd1b31ba08c5aed466ad8] and [trunk|https://github.com/apache/cassandra/commit/fdc055da124efc92a73bd208b4463ee5528c85e0].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair implicitly increases read timeout value,CASSANDRA-15442,13272022,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yifanc,yifanc,yifanc,03/Dec/19 17:58,21/Dec/20 08:07,13/Jul/23 08:38,16/Dec/19 23:48,4.0,4.0-alpha3,,,,Legacy/Core,,,,0,pull-request-available,,,"When read repair occurs during a read, internally, it starts several _blocking_ operations in sequence. See {{org.apache.cassandra.service.StorageProxy#fetchRows}}. 
 The timeline of the blocking operations
 # Regular read, wait for full data/digest read response to complete. {{reads[*].awaitResponses();}}
 # Read repair read, wait for full data read response to complete. {{reads[*].awaitReadRepair();}}
 # Read repair write, wait for write response to complete. {{concatAndBlockOnRepair(results, repairs);}}

Step 1 and 2 share the same timeout, and wait for the duration of read timeout, say 5 s.
Step 3 waits for the duration of write timeout, say 2 s.
In the worse case, the actual time taken for a read could accumulate to ~7 s, if each individual step does not exceed the timeout value.
From the client perspective, it may not expect a request taken higher than the database configured timeout value. 
Such scenario is especially bad for the clients that have set up client-side timeout monitoring close to the configured one. The clients think the operations timed out and abort, but they are in fact still running on server.",,bdeggleston,cnlwsu,jeromatron,jwest,weideng,yifanc,,,,,,,,,,,,,,,,,,,,,,"yifan-c commented on pull request #391: CASSANDRA-15442: Read repair implicitly increases read timeout value
URL: https://github.com/apache/cassandra/pull/391
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Dec/19 04:35;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,CASSANDRA-2494,CASSANDRA-14635,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,yifanc,,,,,,,,,,,,Availability -> Response Crash,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Dec 16 23:48:42 UTC 2019,,,,,,,All,,,,,"0|z098xk:",9223372036854775807,,,,,,,bdeggleston,jwest,,,Low,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/9abe2127dde7ea317928b37b8b5c662e787b2192,,,,,,,,,circle,,,,,"04/Dec/19 19:38;yifanc;The read repair needs to be blocking to guarantee monotonic read, per CASSANDRA-2494.

According to the discussion at CASSANDRA-14635, making the repair (write) async is not a considered use case.
----
h4. Proposed Fix

To respect the timeout that client is expecting, in each step, the blocking operation and the internode messagings should only use the *remaining* timeout. The write part at repair is still part of the read, so it should share the same timeout. 

The step 2 in the timeline already adjusts the internode requests timeout to the remaining. 

The proposed fix argues the step 3 should also use the remaining timeout, instead of using a separate {{WriteRPCTimeout.}}
h4. The Impact

- The read timeout (due to blocking read repair) may occur more frequently if using the existing {{ReadRPCTimeout}}. The read timeout may need to be configured higher to allow the blocking read repair to complete. In fact, the timeout is increased to reflect the actual time taken. (The time for write is just not counted in read as of now)
- Increasing the read timeout allows the genuine slow read queries (but no read repair) to stay longer and negatively impact throughput. 
 ;;;","05/Dec/19 17:22;bdeggleston;nice find [~yifanc], I've taken a cursory look at the relevant code and it does look like we're not doing the right thing here. Agreed we should not exceed the timeout for the original read. I'll review your patch when it's ready;;;","05/Dec/19 23:10;yifanc;[~bdeggleston], thanks! 
 The patch is ready.
||Code||PR||Test||
|[Code|https://github.com/yifan-c/cassandra/tree/CASSANDRA-15442-read-repair-timeout-fix]|[PR|https://github.com/apache/cassandra/pull/391]|[Test|https://app.circleci.com/github/yifan-c/cassandra/pipelines/e7a6e071-1ef5-4976-a45b-c15d70440199/workflows/c5c2f735-8f06-45c7-8304-c1bf24a49704]|

_* 12/05/19, updated the test result link to the one that unit test and jvm dtest passed_

Briefly, the patch does
 # In the BlockingReadRepair, repair process wait based on the read timeout value.
 # Added awaitRepairsUntil to accept a future time to timeout.
 # Added timeout test in dtest.;;;","09/Dec/19 23:16;bdeggleston;Thanks Yifan, feedback below:
 * The configured write timeout no longer factors in to the timeout calculation. I'd expect repair writes to timeout on the write timeout, or whatever is remaining on the read timeout. Whichever is less. WDYT?
 * {{awaitRepairsUntil doesn't follow the }}convention of taking a {{TimeUnit}} argument 
 * DistributedReadWritePathTest
 ** {{Assert.fail(""Read timeout expected but it did not occur”);}} can go right after the statement you're expecting to throw ({{cluster.coordinator(1).ex...), instead of setting and checking a boolean.}}
 ** You could get a further confirmation things are working are expected by setting the write timeout to a value 2-3x above the read timeout.
 ** Since you're only blocking the write responses. you could confirm the timeout came from the write by checking the mutations repaired the recipients.;;;","10/Dec/19 01:25;yifanc;Thanks you for the review, [~bdeggleston]

 
{quote} I'd expect repair writes to timeout on the write timeout, or whatever is remaining on the read timeout. Whichever is less.
{quote}
I thought of it.

Since the mutation is part of the read, it should take whatever the remaining timeout is. If adjusting the timeout of mutation to {{Min(WriteTimeout, RemainingTimeout)}}, it is possible for client to see a request error out before reaching to the read timeout, which is unexpected. It also means server could allow some slow mutations to stay longer than the globally configured write timeout, which may impact throughput. But from the client perspective, if the read is permitted with this amount of time, the server should allow it. 

I am all ears if server should stop a read request early. 

Updated the PR to address the comments. Since I did not change the write timeout to {{Min(WriteTimeout, RemainingTimeout)}}, I did not add the ""setting the write timeout to a value 2-3x above the read timeout."";;;","10/Dec/19 20:54;jwest;Thanks for the patch [~yifanc]. I've actually wondered about it for a while but never thought of it as a bug (assumed it was by design). Glad to see it addressed. A few minor nits, all optional:

 
 * I agree that we should only use the repair timeout for the reason you mentioned. I find it confusing that the write timeout is involved since the client has no idea if read repair will occur when submitting the read. 
 * Since `awaitRepairs` is only called from `awaitRepairsUntil` consider merging the two functions. It might make the code more clear and require less jumping around.
 * I agree with [~bdeggleston] that setting the write timeout very high in your test can show how the change ensures independence from it – regardless of any other code changes. It may also give you a more flexible way to write the assertion at the end of `readRepairTimeoutTest()`.
 * Additionally, re: the two `Assert.assertTrue`s at the end of `readRepairTimeoutTest`: consider adding failure messages to make them more descriptive, as well as collapsing it in to one expression (e.g `actualTimeTaken < reducedReadTimeout + magicDelayAmount && actualTimeTaken > reducedReadTimeout`). ;;;","10/Dec/19 23:26;bdeggleston;No argument that using the read timeout is more understandable from the client perspective, my concern is unintended consequences of silently increasing the write timeout. Granted I can't think of any off the top of my head. [~clohfink], [~jjirsa], can you think of anything bad that might happen if we did that?;;;","12/Dec/19 00:46;yifanc;Putting some formula here to analyze the impact of allowing longer mutation time for read repair prudently. 

For simplicity, using a simple and stable queue to model the request handling. We have,

_M / Lm = Rm_

where _M_ is the average amount of mutations at a given time window, _Lm_ is the average latency and _Rm_ is the rate. 

If some of the mutation can have a longer timeout, the new average latency is 

_Lm’ = (Lm * M1 + L * M2) / M_

where _M1_ is the regular mutation, and _M2_ is the amount of the super slow mutation (without increasing the timeout, they will timeout) from read repair, and _L_ is the average latency those slow mutations take. _M1_ and _M2_ satisfies,
 * _M1 + M2 = M_
 * _M2 = M * Prr * Pmto_, _Prr_ is the observed percentage of read repair mutation vs. total, and _Pmto_ is the observed percentage of the timeouted mutation vs. total.

The range of _L_ is 

_Lm < L <= R_, _R_ is the configured read timeout

Therefore, we have

_Lm' <= Lm + (R - Lm) * Prr * Pmto_

_Rm' >= M / (Lm + (R - Lm) * Prr * Pmto)_

Let _K = (R - Lm) / Lm_, since they are constants. The ratio between the prior rate and the new rate with the change is,

_1 < Rm / Rm' <= 1 + K * Prr * Pmto_ 

Based on the equation, the percentage of read repair and mutation timeouts are the factors to the throughput. 

In a health cluster, i.e. low mutation timeout rate, the ratio should be close to 1, meaning the impact is small. ;;;","13/Dec/19 23:13;cnlwsu;I cant think of anything any worse than if you make a bunch of the same writes repeatedly in same period of time. I think patch looks good with a small doc nit:

{code}If the {@param timeoutAtNanos} is a past time, the method returns immediately with the repair result.{code}  should be updated as timeoutAt as timeoutAtNanos is computed in the method, not a param.;;;","13/Dec/19 23:49;yifanc;Thank you [~cnlwsu] and [~jrwest].

The PR was updated with the changes suggested and rebased onto the latest in trunk.

Changes made:
 * Removed {{awaitRepairs}}
 * Updated the doc for {{awaitRepairsUntil}}

\* 12/16 update: [CI|https://app.circleci.com/github/yifan-c/cassandra/pipelines/fb87f418-2cc7-4283-8800-99cd1f62d977/workflows/1a53780a-fec2-40dd-9140-38ae7b51f85a] passed.;;;","16/Dec/19 23:48;bdeggleston;+1. Again, good find. Committed to trunk as [9abe2127dde7ea317928b37b8b5c662e787b2192 |https://github.com/apache/cassandra/commit/9abe2127dde7ea317928b37b8b5c662e787b2192]. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Bump generations and document changes to system_distributed and system_traces in 3.0, 3.11",CASSANDRA-15441,13271710,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,02/Dec/19 14:02,17/Dec/19 17:42,13/Jul/23 08:38,04/Dec/19 12:23,3.0.20,3.11.6,,,,Cluster/Schema,,,,0,,,,We should document all the changes to distributed system keyspaces and assign unique generations to them. In 3.0 and 3.11 this is just a documentation issue.,,aleksey,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15385,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Dec 04 12:23:30 UTC 2019,,,,,,,All,,,,,"0|z0970g:",9223372036854775807,,,,,,,samt,,,,Low,,3.0.0,,,"[3e30f9c1ba59fffa485da1f21b5ea58d6c115c57|https://github.com/apache/cassandra/commit/3e30f9c1ba59fffa485da1f21b5ea58d6c115c57]",,,,,,,,,Documentation,,,,,"02/Dec/19 16:30;aleksey;3.0: https://github.com/apache/cassandra/commits/15441-3.0
3.11: https://github.com/apache/cassandra/commits/15441-3.11;;;","03/Dec/19 17:01;samt;+1 LGTM;;;","04/Dec/19 12:23;aleksey;Thanks, committed to 3.0 as [3e30f9c1ba59fffa485da1f21b5ea58d6c115c57|https://github.com/apache/cassandra/commit/3e30f9c1ba59fffa485da1f21b5ea58d6c115c57] and merged up;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SELECT JSON does not return the correct value for empty blobs,CASSANDRA-15435,13269826,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,blerer,blerer,21/Nov/19 12:24,15/May/20 08:39,13/Jul/23 08:38,09/Dec/19 10:08,2.2.16,3.0.20,3.11.6,4.0,4.0-alpha3,CQL/Interpreter,,,,0,,,,"In an attempt to fix the side effect of a problem CASSANDRA-13868 was committed in 2.2.11, 3.0.15, 3.11.1 and trunk.
This patch introduced an issue on how empty values were rendered by {{SELECT JSON}} queries.
Instead of returning the correct value for the type a null value was now returned.
A user detected that problem for text column and opened CASSANDRA-14245 to request a fix for that problem. Unfortunately, I misunderstood the problem and the fix did not solve the real problem. It only made the code return 'an empty string instead of null values.

The proper fix is to rollback the changes made for CASSANDRA-13868 and CASSANDRA-14245.
Some unit tests also need to be added to test the behavior.
",,blerer,e.dimitrova,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/19 15:29;e.dimitrova;0001-Patch-applied-successfully.patch;https://issues.apache.org/jira/secure/attachment/12986965/0001-Patch-applied-successfully.patch","27/Nov/19 15:28;e.dimitrova;Screen Shot 2019-11-27 at 9.24.23 AM.png;https://issues.apache.org/jira/secure/attachment/12986971/Screen+Shot+2019-11-27+at+9.24.23+AM.png","27/Nov/19 15:28;e.dimitrova;Screen Shot 2019-11-27 at 9.25.09 AM.png;https://issues.apache.org/jira/secure/attachment/12986970/Screen+Shot+2019-11-27+at+9.25.09+AM.png","27/Nov/19 15:28;e.dimitrova;Screen Shot 2019-11-27 at 9.25.37 AM.png;https://issues.apache.org/jira/secure/attachment/12986969/Screen+Shot+2019-11-27+at+9.25.37+AM.png","27/Nov/19 15:28;e.dimitrova;Screen Shot 2019-11-27 at 9.26.09 AM.png;https://issues.apache.org/jira/secure/attachment/12986968/Screen+Shot+2019-11-27+at+9.26.09+AM.png","27/Nov/19 15:28;e.dimitrova;Screen Shot 2019-11-27 at 9.26.20 AM.png;https://issues.apache.org/jira/secure/attachment/12986967/Screen+Shot+2019-11-27+at+9.26.20+AM.png","27/Nov/19 15:28;e.dimitrova;Screen Shot 2019-11-27 at 9.26.33 AM.png;https://issues.apache.org/jira/secure/attachment/12986966/Screen+Shot+2019-11-27+at+9.26.33+AM.png",,,,,,,,,,,,,,,,,,,,,,,7.0,e.dimitrova,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Dec 09 10:07:10 UTC 2019,,,,,,,All,,,,,"0|z08vds:",9223372036854775807,,,,,,,blerer,,,,Low,,2.2.11,,,https://github.com/apache/cassandra/commit/c3dc6eb57ef94ca1ff19ab766fc3cc5179242cac,,,,,,,,,Unit tests,,,,,"22/Nov/19 17:05;e.dimitrova;Patch applied to v2.2. Unit test was successful. Currently running Jenkins tests before submission for review and merge to the newer versions. ;;;","27/Nov/19 15:30;e.dimitrova;Jenkins results added. The failed jobs seems not to be related to the patch. Patch added for 2.2 [~blerer] please review and let me know to merge with the next versions. Thank you;;;","09/Dec/19 10:07;blerer;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pending ranges are not recalculated on keyspace creation,CASSANDRA-15433,13269217,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sumanth.pasupuleti,josnyder,josnyder,19/Nov/19 01:55,27/May/22 19:25,13/Jul/23 08:38,15/Oct/21 09:30,3.0.26,3.11.12,4.0.2,4.1,4.1-alpha1,Cluster/Membership,,,,0,,,,"When a node begins bootstrapping, Cassandra recalculates pending tokens for each keyspace that exists when the state change is observed (in StorageService:handleState*). When new keyspaces are created, we do not recalculate pending ranges (around Schema:merge). As a result, writes for new keyspaces are not received by nodes in BOOT or BOOT_REPLACE modes. When bootstrapping finishes, the node which just bootstrapped will not have data for the newly created keyspace.

Consider a ring with bootstrapped nodes A, B, and C. Node D is pending, and when it finishes bootstrapping, C will cede ownership of some ranges to D. A quorum write is acknowledged by C and A. B missed the write, and the coordinator didn't send it to D at all. When D finishes bootstrapping, the quorum B+D will not contain the mutation.

Steps to reproduce:
# Join a node in BOOT mode
# Create a keyspace
# Send writes to that keyspace
# On the joining node, observe that {{nodetool cfstats}} records zero writes to the new keyspace

I have observed this directly in Cassandra 3.0, and based on my reading the code, I believe it affects up through trunk.",,abdulazizali,ifesdjeen,jeromatron,josnyder,sumanth.pasupuleti,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15497,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,sumanth.pasupuleti,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Oct 15 09:30:02 UTC 2021,,,,,,,All,,,,,"0|z08rmg:",9223372036854775807,,,,,,,ifesdjeen,,,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/11af037fd99bfb4a942f7d7dd55c177a37d29f63,,,,,,,,,"in-jvm dtest

 ",,,,,"23/Dec/19 07:37;sumanth.pasupuleti;A trivial approach would be to recalculate pending ranges whenever keyspaces change.
https://github.com/apache/cassandra/compare/cassandra-3.0...sumanth-pasupuleti:30_15433?expand=1

Added a blocking wait to ensure keyspace change isn't complete unless pending changes are recalculated to avoid any potential miss of mutations on the bootstrapping nodes.

Passing Tests - https://circleci.com/workflow-run/e7f2416c-c882-41ea-a9e4-dc7763f96b58;;;","14/Jan/20 15:37;ifesdjeen;Thank you for the patch.

Following up an offline discussion, I've submitted a patch that you can use to create a test for this patch (see dependent jira) that could also help to simplify further development of features related to bootstrap and ring movements.

While what you are suggesting seems to mitigate the problem I'm not fully certain that this is what we need to do. First, pending range tasks will be triggered for _all_ keyspaces, which is definitely not what we want to do. From looking at the code, it seems like for newly created keyspaces, we do not calculate pending ranges at all, so I'd recompute them for one of the keyspaces (namely, for freshly created one). 

Moreover, keyspaces diff is more than just created keyspaces. It also contains created and altered ones, so triggering recompute without a regard to what exactly happened to which keyspace is also not what we'd like.

To summarise, I'd say we need to add a test for bootstrap + keyspace creation; technically it'll also cover moving and leaving nodes since we also use pending ranges there. And we need to make sure we recompute pending ranges only for the keyspaces that were created. ;;;","17/Feb/20 04:06;sumanth.pasupuleti;+1 on invoking pendingrange calculation only for newly created keyspaces. I have updated 3.0 and trunk changes accordingly.
[3.0 changes|https://github.com/sumanth-pasupuleti/cassandra/commit/d2841ed2075db9e7241d428a054cf1d7d2936f9e]
[Trunk changes |https://github.com/sumanth-pasupuleti/cassandra/commit/dac07262b23e96607fb12a869f84b55a4081ef36]

I am still working on in-jvm test. As I was mentioning offline, trying to figure out how to make a node to be in ""bootstrapping mode"" in order to then trigger a keyspace creation while the node is in bootstrapping mode.;;;","23/Aug/21 09:26;sumanth.pasupuleti;[~ifesdjeen] I was finally able to get back to making progress on this ticket. Please find the updated patches below that include in-jvm tests for this scenario.
[Trunk changes|https://github.com/apache/cassandra/compare/trunk...sumanth-pasupuleti:bugfix/trunk_15433?expand=1]
[3.0 changes|https://github.com/apache/cassandra/compare/cassandra-3.0...sumanth-pasupuleti:bugfix/30_15433?expand=1];;;","20/Sep/21 10:48;ifesdjeen;+1, the patch and the test looks good to me!
;;;","12/Oct/21 23:38;sumanth.pasupuleti;[~ifesdjeen] curious if you will be able to commit this patch;;;","15/Oct/21 09:30;ifesdjeen;Committed to 3.0 with [11af037fd99bfb4a942f7d7dd55c177a37d29f63|https://github.com/apache/cassandra/commit/11af037fd99bfb4a942f7d7dd55c177a37d29f63] and merged up to [3.11|https://github.com/apache/cassandra/commit/610d5d4eb5205cd4ca2f573237da14db055757c1], [4.0|https://github.com/apache/cassandra/commit/25f67a75e22fdd9bdabd4b17dd0e11973ce59c2d] and [trunk|https://github.com/apache/cassandra/commit/d9ca61404334f3bd94c08cf66ccd15e8c5287f52].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The ""read defragmentation"" optimization does not work",CASSANDRA-15432,13269076,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,18/Nov/19 13:35,03/Jan/21 17:04,13/Jul/23 08:38,17/Aug/20 09:47,3.0.22,3.11.8,4.0,4.0-beta2,,Legacy/Local Write-Read Paths,,,,0,,,,"The so-called ""read defragmentation"" that has been added way back with CASSANDRA-2503 actually does not work, and never has. That is, the defragmentation writes do happen, but they only additional load on the nodes without helping anything, and are thus a clear negative.

The ""read defragmentation"" (which only impact so-called ""names queries"") kicks in when a read hits ""too many"" sstables (> 4 by default), and when it does, it writes down the result of that read. The assumption being that the next read for that data would only read the newly written data, which if not still in memtable would at least be in a single sstable, thus speeding that next read.

Unfortunately, this is not how this work. When we defrag and write the result of our original read, we do so with the timestamp of the data read (as we should, changing the timestamp would be plain wrong). And as a result, following reads will read that data first, but will have no way to tell that no more sstables should be read. Technically, the [{{reduceFilter}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/SinglePartitionReadCommand.java#L830] call will not return {{null}} because the {{currentMaxTs}} will be higher than at least some of the data in the result, and this until we've read from as many sstables than in the original read.

I see no easy way to fix this. It might be possible to make it work with additional per-sstable metadata, but nothing sufficiently simple and cheap to be worth it comes to mind. And I thus suggest simply removing that code.

For the record, I'll note that there is actually a 2nd problem with that code: currently, we ""defrag"" a read even if we didn't got data for everything that the query requests. This also is ""wrong"" even if we ignore the first issue: a following read that would read the defragmented data would also have no way to know to not read more sstables to try to get the missing parts. This problem would be fixeable, but is obviously overshadowed by the previous one anyway.

Anyway, as mentioned, I suggest to just remove the ""optimization"" (which again, never optimized anything) altogether, and happy to provide the simple patch.

The only question might be in which versions? This impact all versions, but this isn't a correction bug either, ""just"" a performance one. So do we want 4.0 only or is there appetite for earlier?
",,aleksey,benedict,e.dimitrova,jeromatron,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Aug 17 09:47:29 UTC 2020,,,,,,,All,,,,,"0|z08qrk:",9223372036854775807,,,,,,,aleksey,,,,Normal,,1.1.0,,,"3.0:https://github.com/apache/cassandra/commit/e2ecdf268a82fa3ac0f4c9fe77ab35bca33cc72a, 3.11:https://github.com/apache/cassandra/commit/ecd23f1da5894511cccac6c8445f962f3b73f733, trunk:https://github.com/apache/cassandra/commit/efce6b39fb557314fad0cb56b0",,,,,,,,,"No impact on testing as this is removing code and no test existed for the removed optimization. Afaict, the optimization was not documented, so no impact on documentation.",,,,,"19/Nov/19 15:30;aleksey;Never liked that piece of code, and IIRC [~jbellis] suggested removing it at least once, too.

bq. The only question might be in which versions? This impact all versions, but this isn't a correction bug either, ""just"" a performance one. So do we want 4.0 only or is there appetite for earlier?

It's borderline, if not clearly, a bug. I would go for 3.0, 3.11, and 4.0.;;;","20/Nov/19 09:53;benedict;bq. Never liked that piece of code

+1.  I squinted at it distrustfully recently, but never invested the time to decide it was definitely bad.  Will be glad to see it go.;;;","13/Aug/20 12:51;slebresne;Back on this later than I meant, but attaching fairly trivial patches to remove said optimization on 3.0, 3.11 and trunk/4.0.
||patch||CI||
|[3.0|https://github.com/pcmanus/cassandra/commits/C-15432-3.0]|[#239|https://ci-cassandra.apache.org/job/Cassandra-devbranch/239/]|
|[3.11|https://github.com/pcmanus/cassandra/commits/C-15432-3.11]|[#240|https://ci-cassandra.apache.org/job/Cassandra-devbranch/240/]|
|[trunk|https://github.com/pcmanus/cassandra/commits/C-15432-trunk]|[#241|https://ci-cassandra.apache.org/job/Cassandra-devbranch/241/]|

[~aleksey] or [~benedict]: would one of you have cycles to review by any chance (pretty simple diff, removing the {{if}} triggering the defrag as well as tiny bits of incidental code that is now dead).

;;;","13/Aug/20 14:00;aleksey;+1. Good riddance.;;;","17/Aug/20 09:47;slebresne;Thanks for the review. CI doesn't seem to show anything new broken so committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.2 eclipse-warnings,CASSANDRA-15427,13268335,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mck,mck,mck,14/Nov/19 14:40,15/Nov/19 06:40,13/Jul/23 08:38,15/Nov/19 06:40,2.2.16,,,,,Build,,,,0,,,,"Cassandra-2.2 artifact builds are failing from eclipse-warnings.

{noformat}
# 11/11/19 2:58:41 PM UTC
# Eclipse Compiler for Java(TM) v20150120-1634, 3.10.2, Copyright IBM Corp 2000, 2013. All rights reserved.
incorrect classpath: /home/jenkins/jenkins-slave/workspace/Cassandra-2.2-artifacts/build/cobertura/classes
----------
1. ERROR in /home/jenkins/jenkins-slave/workspace/Cassandra-2.2-artifacts/src/java/org/apache/cassandra/db/compaction/CompactionManager.java (at line 880)
	ISSTableScanner scanner = cleanupStrategy.getScanner(sstable, getRateLimiter());
	                ^^^^^^^
Resource 'scanner' should be managed by try-with-resource
----------
----------
2. ERROR in /home/jenkins/jenkins-slave/workspace/Cassandra-2.2-artifacts/src/java/org/apache/cassandra/db/compaction/LeveledCompactionStrategy.java (at line 257)
	scanners.add(new LeveledScanner(intersecting, range));
	             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
Potential resource leak: '<unassigned Closeable value>' may not be closed
----------
----------
3. ERROR in /home/jenkins/jenkins-slave/workspace/Cassandra-2.2-artifacts/src/java/org/apache/cassandra/tools/SSTableExport.java (at line 315)
	ISSTableScanner scanner = reader.getScanner();
	                ^^^^^^^
Resource 'scanner' should be managed by try-with-resource
----------
3 problems (3 errors)
{noformat}
ref: https://builds.apache.org/job/Cassandra-2.2-artifacts/180/artifact/build/ecj/eclipse_compiler_checks.txt",,benedict,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mck,,,,,,,,,,,,Code,,,,,,,,Low Hanging Fruit,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Nov 15 06:40:39 UTC 2019,,,,,,,All,,,,,"0|z08m6w:",9223372036854775807,,,,,,,benedict,,,,Low,,2.2.0 beta 1,,,https://github.com/apache/cassandra/commit/f05bf64fc1606beeba5b26ffdb63207fec549238,,,,,,,,,asf jenkins,,,,,"14/Nov/19 14:51;mck;
||branch||circleci||asf jenkins tests||asf jenkins dtests||
|[cassandra-2.2_15427|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/cassandra-2.2_15427]|[circleci|https://circleci.com/workflow-run/4fa881cc-e842-45d2-8b4f-bffea7601152]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/28//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/28/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/702//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/702]|;;;","14/Nov/19 14:57;benedict;+1;;;","15/Nov/19 06:40;mck;Committed as f05bf64fc1606beeba5b26ffdb63207fec549238;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra 3.11.5 fails to start on Windows ,CASSANDRA-15426,13268261,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,ashish_singh2@bmc.com,ashish_singh2@bmc.com,14/Nov/19 09:47,15/May/20 08:39,13/Jul/23 08:38,29/Jan/20 02:39,3.0.20,3.11.6,4.0,4.0-alpha3,,Consistency/Hints,,,,5,,,,"Cassandra 3.11.5 fails to start on Windows server 2012 R2. with following error trace.

Cassandra 3.11.4 doesn't fail on Windows 2012 R2. 

   

org.apache.cassandra.io.FSReadError: java.io.IOException: Invalid folder descriptor trying to create log replica C:\Users\Administrator\Downloads\apache-cassandra-3.11.5-bin.tar\apache-cassandra-3.11.5-bin\apache-cassandra-3.11.5\data\data\system\local-7ad54392bcdd35a684174e047860b377
 at org.apache.cassandra.db.lifecycle.LogReplica.create(LogReplica.java:58) ~[apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LogReplicaSet.maybeCreateReplica(LogReplicaSet.java:86) ~[apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LogFile.makeRecord(LogFile.java:311) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LogFile.add(LogFile.java:283) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LogTransaction.trackNew(LogTransaction.java:139) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.lifecycle.LifecycleTransaction.trackNew(LifecycleTransaction.java:528) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.format.big.BigTableWriter.<init>(BigTableWriter.java:81) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.format.big.BigFormat$WriterFactory.open(BigFormat.java:92) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.format.SSTableWriter.create(SSTableWriter.java:102) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.io.sstable.SimpleSSTableMultiWriter.create(SimpleSSTableMultiWriter.java:119) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.createSSTableMultiWriter(AbstractCompactionStrategy.java:588) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.compaction.CompactionStrategyManager.createSSTableMultiWriter(CompactionStrategyManager.java:1027) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.ColumnFamilyStore.createSSTableMultiWriter(ColumnFamilyStore.java:532) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable$FlushRunnable.createFlushWriter(Memtable.java:504) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable$FlushRunnable.<init>(Memtable.java:443) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable$FlushRunnable.<init>(Memtable.java:420) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable.createFlushRunnables(Memtable.java:307) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.Memtable.flushRunnables(Memtable.java:298) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.ColumnFamilyStore$Flush.flushMemtable(ColumnFamilyStore.java:1153) [apache-cassandra-3.11.5.jar:3.11.5]
 at org.apache.cassandra.db.ColumnFamilyStore$Flush.run(ColumnFamilyStore.java:1118) [apache-cassandra-3.11.5.jar:3.11.5]
 at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [na:1.8.0_161]
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_161]
 at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:84) [apache-cassandra-3.11.5.jar:3.11.5]
 at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_161]
Caused by: java.io.IOException: Invalid folder descriptor trying to",,abdulazizali,Andrew.Kostousov@gmail.com,ashish_singh2@bmc.com,djoshi,hirik,jeromatron,jhonglei,tsteinmaurer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15053,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Feb 12 19:44:01 UTC 2020,,,,,,,Windows,,,,,"0|z08lqg:",9223372036854775807,,,,,,,djoshi,,,,Normal,,3.0.19,,,https://github.com/apache/cassandra/commit/4e6c0fadfe69419e0f6f7c3fa5960bdcd90d510a,,,,,,,,,CircleCI for Linux based hosts and manual testing for Windows.,,,,,"14/Nov/19 10:41;Andrew.Kostousov@gmail.com;Seems like the problem was introduced with CASSANDRA-15053 patch. Note that {{NativeLibrary.tryOpenDirectory()}} is not implemented on Windows and always returns -1.;;;","28/Jan/20 19:46;djoshi;Jeff supplied a patch that I imported [here|https://github.com/dineshjoshi/cassandra/tree/15426-3.0] for Cassandra 3.0. Circle is running [here|https://circleci.com/workflow-run/19a0c18c-ff2a-4131-8d44-63dd21bb84fd]. Once I've confirmed that this fixes the issue in 3.0 and doesn't cause regressions, I will forward port it to 3.11 and trunk.

[~ashish_singh2@bmc.com] [~Andrew.Kostousov@gmail.com] any chance I could get both of you to try this out in your respective environments. I don't have Windows Server installation at hand.;;;","29/Jan/20 02:37;djoshi;I verified that the patch works on 3.0, 3.11 and trunk. +1.;;;","29/Jan/20 02:39;djoshi;Committed as 4e6c0fadfe69419e0f6f7c3fa5960bdcd90d510a and merged up to 3.11 and trunk.;;;","12/Feb/20 05:15;ashish_singh2@bmc.com;[~djoshi]: HI Dinesh , Where can i get the patch to try out the fix, Also, Please share steps to apply the patch to existing installation.;;;","12/Feb/20 19:44;djoshi;[~ashish_singh2@bmc.com], this fix is in 3.11.6 which will be released in a few days. If you cannot wait that long, you can apply my patch from the [3.11 branch|https://github.com/apache/cassandra/commit/f2a36e17cd7977044e461e2ff134e2a703f6843d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sonatype-2013-0069 (The setuptools package is vulnerable to Directory Traversal),CASSANDRA-15425,13267973,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,abhishek.scs,abhishek.scs,13/Nov/19 09:43,17/Jan/22 12:49,13/Jul/23 08:38,17/Jan/22 12:49,,,,,,,,,,0,,,,"*Description :**Description :* *Severity :* Sonatype CVSS 3.0: 7.5
 
 *Weakness :* Sonatype CWE: 22
 
 *Source :* Sonatype Data Research
 
 *Categories :* Data 
 *Explanation :* The setuptools package is vulnerable to Directory Traversal. The _install() function and _build_egg() function in the ez_setup.py file creates setuptools as a .tar.gz file for distribution and allows files to be extracted to arbitrary locations. An attacker can exploit this vulnerability by uploading a tar archive that contains filenames starting with directory traversal characters such as (../../../../../etc/passwd) or symbolic links which, when untarred, will overwrite arbitrary files. 
 *Detection :* The application is vulnerable by using this component. 
 *Recommendation :* We recommend upgrading to a version of this component that is not vulnerable to this specific issue. 
 *Root Cause :* Cassandra-2.2.5.nupkgez_setup.py : [0.7.3, 3.0b1)
 
 *Advisories :* Project: https://github.com/pypa/setuptools/issues/7
 
 *CVSS Details :* Sonatype CVSS 3.0: 7.5
*Occurences (Paths) :* ["" apache-cassandra.zip/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/bin/cqlsh.bat"" ; "" apache-cassandra.zip/bin/debug-cql.bat"" ; "" apache-cassandra.zip/bin/source-conf.ps1"" ; "" apache-cassandra.zip/bin/sstableloader.bat"" ; "" apache-cassandra.zip/bin/sstablescrub.bat"" ; "" apache-cassandra.zip/bin/sstableupgrade.bat"" ; "" apache-cassandra.zip/bin/sstableverify.bat"" ; "" apache-cassandra.zip/bin/stop-server"" ; "" apache-cassandra.zip/bin/stop-server.bat"" ; "" apache-cassandra.zip/bin/stop-server.ps1"" ; "" apache-cassandra.zip/conf/README.txt"" ; "" apache-cassandra.zip/conf/cassandra-rackdc.properties"" ; "" apache-cassandra.zip/conf/cassandra-topology.properties"" ; "" apache-cassandra.zip/conf/commitlog_archiving.properties"" ; "" apache-cassandra.zip/conf/triggers/README.txt"" ; "" apache-cassandra.zip/lib/ST4-4.0.8.jar"" ; "" apache-cassandra.zip/lib/airline-0.6.jar"" ; "" apache-cassandra.zip/lib/antlr-runtime-3.5.2.jar"" ; "" apache-cassandra.zip/lib/commons-cli-1.1.jar"" ; "" apache-cassandra.zip/lib/commons-lang3-3.1.jar"" ; "" apache-cassandra.zip/lib/commons-math3-3.2.jar"" ; "" apache-cassandra.zip/lib/compress-lzf-0.8.4.jar"" ; "" apache-cassandra.zip/lib/concurrentlinkedhashmap-lru-1.4.jar"" ; "" apache-cassandra.zip/lib/disruptor-3.0.1.jar"" ; "" apache-cassandra.zip/lib/ecj-4.4.2.jar"" ; "" apache-cassandra.zip/lib/futures-2.1.6-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/high-scale-lib-1.0.6.jar"" ; "" apache-cassandra.zip/lib/jamm-0.3.0.jar"" ; "" apache-cassandra.zip/lib/javax.inject.jar"" ; "" apache-cassandra.zip/lib/jbcrypt-0.3m.jar"" ; "" apache-cassandra.zip/lib/jcl-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/joda-time-2.4.jar"" ; "" apache-cassandra.zip/lib/json-simple-1.1.jar"" ; "" apache-cassandra.zip/lib/libthrift-0.9.2.jar"" ; "" apache-cassandra.zip/lib/licenses/ST4-4.0.8.txt"" ; "" apache-cassandra.zip/lib/licenses/antlr-runtime-3.5.2.txt"" ; "" apache-cassandra.zip/lib/licenses/compress-lzf-0.8.4.txt"" ; "" apache-cassandra.zip/lib/licenses/concurrent-trees-2.4.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ecj-4.4.2.txt"" ; "" apache-cassandra.zip/lib/licenses/futures-2.1.6.txt"" ; "" apache-cassandra.zip/lib/licenses/high-scale-lib-1.0.6.txt"" ; "" apache-cassandra.zip/lib/licenses/jbcrypt-0.3m.txt"" ; "" apache-cassandra.zip/lib/licenses/jcl-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/jna-4.2.2.txt"" ; "" apache-cassandra.zip/lib/licenses/jstackjunit-0.0.1.txt"" ; "" apache-cassandra.zip/lib/licenses/log4j-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-classic-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-core-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/lz4-1.3.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-core-3.1.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-jvm-3.1.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ohc-0.4.4.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config-base-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config3-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/sigar-1.6.4.txt"" ; "" apache-cassandra.zip/lib/licenses/six-1.7.3.txt"" ; "" apache-cassandra.zip/lib/licenses/slf4j-api-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/stream-2.5.2.txt"" ; "" apache-cassandra.zip/lib/log4j-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/logback-classic-1.1.3.jar"" ; "" apache-cassandra.zip/lib/logback-core-1.1.3.jar"" ; "" apache-cassandra.zip/lib/lz4-1.3.0.jar"" ; "" apache-cassandra.zip/lib/metrics-core-3.1.0.jar"" ; "" apache-cassandra.zip/lib/metrics-logback-3.1.0.jar"" ; "" apache-cassandra.zip/lib/sigar-1.6.4.jar"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-pa-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-s390x-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal64-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-amd64-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.lib"" ; "" apache-cassandra.zip/lib/six-1.7.3-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/slf4j-api-1.7.7.jar"" ; "" apache-cassandra.zip/lib/snakeyaml-1.11.jar"" ; "" apache-cassandra.zip/lib/snappy-java-1.1.1.7.jar"" ; "" apache-cassandra.zip/lib/stream-2.5.2.jar"" ; "" apache-cassandra.zip/lib/thrift-server-0.3.7.jar"" ; "" apache-cassandra.zip/pylib/cqlshlib/__init__.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/saferscanner.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/sslhandling.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/ansi_colors.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/basecase.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cql_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_commands.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_invocation.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/winpty.py"" ; "" apache-cassandra.zip/tools/bin/cassandra-stress.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/tools/bin/sstableexpiredblockers.bat"" ; "" apache-cassandra.zip/tools/bin/sstablelevelreset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablemetadata.bat"" ; "" apache-cassandra.zip/tools/bin/sstableofflinerelevel.bat"" ; "" apache-cassandra.zip/tools/bin/sstablerepairedset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablesplit.bat""]
*CVE :* sonatype-2013-0069
*URL :* No URL Present.",,abhishek.scs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,2019-11-13 09:43:53.0,,,,,,,All,,,,,"0|z08jyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE-2015-2156 (Netty is vulnerable to Information Disclosure) ,CASSANDRA-15423,13267965,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,abhishek.scs,abhishek.scs,13/Nov/19 09:35,17/Jan/22 12:50,13/Jul/23 08:38,17/Jan/22 12:50,,,,,,,,,,0,,,,"*Description :**Description :* *Severity :* CVE CVSS 3.0: 7.5Sonatype CVSS 3.0: 7.5
 
 *Weakness :* CVE CWE: 20
 
 *Source :* National Vulnerability Database
 
 *Categories :* Data 
 *Description from CVE :* Netty before 3.9.8.Final, 3.10.x before 3.10.3.Final, 4.0.x before 4.0.28.Final, and 4.1.x before 4.1.0.Beta5 and Play Framework 2.x before 2.3.9 might allow remote attackers to bypass the httpOnly flag on cookies and obtain sensitive information by leveraging improper validation of cookie name and value characters.
 
 *Explanation :* Netty is vulnerable to Information Disclosure.Multiple methods in multiple files improperly validate cookie names and values. This allows the presence of single-quote and double-quote characters to break tokenization.A remote attacker can exploit this vulnerability by inducing a victim to send a crafted request containing quote characters in any parameter value that sets a cookie.If that tainted cookie gets reflected in the response, the attacker can then use Cross-Site Scripting (XSS) to potentially retrieve the entire cookie header, despite the presence of an HttpOnly flag.
The Sonatype security research team discovered that the vulnerability is present in all versions prior to 3.9.7.Final and 3.10.x before 3.10.2.Final, and not in all the versions before 3.9.8.Final and 3.10.x before 3.10.3.Final as the advisory states. 
 *Detection :* The application is vulnerable by using this component if it reflects any cookie information in a HTML page, and that page is also prone to Cross-Site Scripting (XSS) attacks. 
 *Recommendation :* We recommend upgrading to a version of this component that is not vulnerable to this specific issue. 
 *Root Cause :* Cassandra-2.2.5.nupkgCookieDecoder.class : [5.0.0.Alpha1, 5.0.0.Alpha2)
 
 *Advisories :* Project: https://engineering.linkedin.com/security/look-netty_s-recen...
 
 *CVSS Details :* CVE CVSS 3.0: 7.5
*Occurences (Paths) :* ["" apache-cassandra.zip/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/bin/cqlsh.bat"" ; "" apache-cassandra.zip/bin/debug-cql.bat"" ; "" apache-cassandra.zip/bin/source-conf.ps1"" ; "" apache-cassandra.zip/bin/sstableloader.bat"" ; "" apache-cassandra.zip/bin/sstablescrub.bat"" ; "" apache-cassandra.zip/bin/sstableupgrade.bat"" ; "" apache-cassandra.zip/bin/sstableverify.bat"" ; "" apache-cassandra.zip/bin/stop-server"" ; "" apache-cassandra.zip/bin/stop-server.bat"" ; "" apache-cassandra.zip/bin/stop-server.ps1"" ; "" apache-cassandra.zip/conf/README.txt"" ; "" apache-cassandra.zip/conf/cassandra-rackdc.properties"" ; "" apache-cassandra.zip/conf/cassandra-topology.properties"" ; "" apache-cassandra.zip/conf/commitlog_archiving.properties"" ; "" apache-cassandra.zip/conf/triggers/README.txt"" ; "" apache-cassandra.zip/lib/ST4-4.0.8.jar"" ; "" apache-cassandra.zip/lib/airline-0.6.jar"" ; "" apache-cassandra.zip/lib/antlr-runtime-3.5.2.jar"" ; "" apache-cassandra.zip/lib/commons-cli-1.1.jar"" ; "" apache-cassandra.zip/lib/commons-lang3-3.1.jar"" ; "" apache-cassandra.zip/lib/commons-math3-3.2.jar"" ; "" apache-cassandra.zip/lib/compress-lzf-0.8.4.jar"" ; "" apache-cassandra.zip/lib/concurrentlinkedhashmap-lru-1.4.jar"" ; "" apache-cassandra.zip/lib/disruptor-3.0.1.jar"" ; "" apache-cassandra.zip/lib/ecj-4.4.2.jar"" ; "" apache-cassandra.zip/lib/futures-2.1.6-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/high-scale-lib-1.0.6.jar"" ; "" apache-cassandra.zip/lib/jamm-0.3.0.jar"" ; "" apache-cassandra.zip/lib/javax.inject.jar"" ; "" apache-cassandra.zip/lib/jbcrypt-0.3m.jar"" ; "" apache-cassandra.zip/lib/jcl-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/joda-time-2.4.jar"" ; "" apache-cassandra.zip/lib/json-simple-1.1.jar"" ; "" apache-cassandra.zip/lib/libthrift-0.9.2.jar"" ; "" apache-cassandra.zip/lib/licenses/ST4-4.0.8.txt"" ; "" apache-cassandra.zip/lib/licenses/antlr-runtime-3.5.2.txt"" ; "" apache-cassandra.zip/lib/licenses/compress-lzf-0.8.4.txt"" ; "" apache-cassandra.zip/lib/licenses/concurrent-trees-2.4.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ecj-4.4.2.txt"" ; "" apache-cassandra.zip/lib/licenses/futures-2.1.6.txt"" ; "" apache-cassandra.zip/lib/licenses/high-scale-lib-1.0.6.txt"" ; "" apache-cassandra.zip/lib/licenses/jbcrypt-0.3m.txt"" ; "" apache-cassandra.zip/lib/licenses/jcl-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/jna-4.2.2.txt"" ; "" apache-cassandra.zip/lib/licenses/jstackjunit-0.0.1.txt"" ; "" apache-cassandra.zip/lib/licenses/log4j-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-classic-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-core-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/lz4-1.3.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-core-3.1.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-jvm-3.1.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ohc-0.4.4.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config-base-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config3-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/sigar-1.6.4.txt"" ; "" apache-cassandra.zip/lib/licenses/six-1.7.3.txt"" ; "" apache-cassandra.zip/lib/licenses/slf4j-api-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/stream-2.5.2.txt"" ; "" apache-cassandra.zip/lib/log4j-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/logback-classic-1.1.3.jar"" ; "" apache-cassandra.zip/lib/logback-core-1.1.3.jar"" ; "" apache-cassandra.zip/lib/lz4-1.3.0.jar"" ; "" apache-cassandra.zip/lib/metrics-core-3.1.0.jar"" ; "" apache-cassandra.zip/lib/metrics-logback-3.1.0.jar"" ; "" apache-cassandra.zip/lib/sigar-1.6.4.jar"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-pa-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-s390x-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal64-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-amd64-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.lib"" ; "" apache-cassandra.zip/lib/six-1.7.3-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/slf4j-api-1.7.7.jar"" ; "" apache-cassandra.zip/lib/snakeyaml-1.11.jar"" ; "" apache-cassandra.zip/lib/snappy-java-1.1.1.7.jar"" ; "" apache-cassandra.zip/lib/stream-2.5.2.jar"" ; "" apache-cassandra.zip/lib/thrift-server-0.3.7.jar"" ; "" apache-cassandra.zip/pylib/cqlshlib/__init__.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/saferscanner.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/sslhandling.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/ansi_colors.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/basecase.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cql_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_commands.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_invocation.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/winpty.py"" ; "" apache-cassandra.zip/tools/bin/cassandra-stress.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/tools/bin/sstableexpiredblockers.bat"" ; "" apache-cassandra.zip/tools/bin/sstablelevelreset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablemetadata.bat"" ; "" apache-cassandra.zip/tools/bin/sstableofflinerelevel.bat"" ; "" apache-cassandra.zip/tools/bin/sstablerepairedset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablesplit.bat""]
*CVE :* CVE-2015-2156
*URL :* http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2015-2156",,abhishek.scs,djoshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Nov 15 04:41:04 UTC 2019,,,,,,,All,,,,,"0|z08jwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/19 21:54;djoshi;Thanks for reporting this. While the version of Netty is vulnerable, I don't think Cassandra uses Netty's HTTP classes at all so its unlikely we're vulnerable to the said attack.;;;","15/Nov/19 04:41;abhishek.scs;Thanks Dinesh. I took a note of it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE-2019-16869(Netty is vulnerable to HTTP Request Smuggling) of severity 7.5 for Cassendra 2.2.5,CASSANDRA-15418,13267915,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,abhishek.scs,abhishek.scs,13/Nov/19 05:43,17/Jan/22 12:51,13/Jul/23 08:38,17/Jan/22 12:51,,,,,,,,,,0,,,,"*Description :**Description :* *Severity :* CVE CVSS 3: 7.5Sonatype CVSS 3: 7.5
 
 *Weakness :* CVE CWE: 444
 
 *Source :* National Vulnerability Database
 
 *Categories :* Data 
 *Description from CVE :* Netty before 4.1.42.Final mishandles whitespace before the colon in HTTP headers , which leads to HTTP request smuggling.
 
 *Explanation :* Netty is vulnerable to HTTP Request Smuggling. The splitHeader method in HttpObjectDecoder.class does not properly handle HTTP headers containing whitespace between the header field-name and colon. An attacker can exploit this by sending such a header containing this white space and have the header end up being parsed by one endpoint and not another, due to inconsistencies in how the whitespace in the header is handled. 
 *Detection :* The application is vulnerable by using this component. 
 *Recommendation :* We recommend upgrading to a version of this component that is not vulnerable to this specific issue. 
 *Root Cause :* Cassandra-2.2.5.nupkgio/netty/handler/codec/http/HttpObjectDecoder.class : [4.0.0.Beta1, 4.1.42.Final]
 
 *Advisories :* Project: https://github.com/netty/netty/issues/9571
 
 *CVSS Details :* CVE CVSS 3: 7.5CVSS Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:H/A:N
*Occurences (Paths) :* ["" apache-cassandra.zip/bin/cassandra.bat"" ; "" apache-cassandra.zip/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/bin/cqlsh.bat"" ; "" apache-cassandra.zip/bin/debug-cql.bat"" ; "" apache-cassandra.zip/bin/source-conf.ps1"" ; "" apache-cassandra.zip/bin/sstableloader.bat"" ; "" apache-cassandra.zip/bin/sstablescrub.bat"" ; "" apache-cassandra.zip/bin/sstableupgrade.bat"" ; "" apache-cassandra.zip/bin/sstableverify.bat"" ; "" apache-cassandra.zip/bin/stop-server"" ; "" apache-cassandra.zip/bin/stop-server.ps1"" ; "" apache-cassandra.zip/conf/README.txt"" ; "" apache-cassandra.zip/conf/cassandra-rackdc.properties"" ; "" apache-cassandra.zip/conf/cassandra-topology.properties"" ; "" apache-cassandra.zip/conf/commitlog_archiving.properties"" ; "" apache-cassandra.zip/conf/triggers/README.txt"" ; "" apache-cassandra.zip/lib/ST4-4.0.8.jar"" ; "" apache-cassandra.zip/lib/airline-0.6.jar"" ; "" apache-cassandra.zip/lib/antlr-runtime-3.5.2.jar"" ; "" apache-cassandra.zip/lib/commons-cli-1.1.jar"" ; "" apache-cassandra.zip/lib/commons-lang3-3.1.jar"" ; "" apache-cassandra.zip/lib/commons-math3-3.2.jar"" ; "" apache-cassandra.zip/lib/compress-lzf-0.8.4.jar"" ; "" apache-cassandra.zip/lib/concurrentlinkedhashmap-lru-1.4.jar"" ; "" apache-cassandra.zip/lib/disruptor-3.0.1.jar"" ; "" apache-cassandra.zip/lib/futures-2.1.6-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/high-scale-lib-1.0.6.jar"" ; "" apache-cassandra.zip/lib/jamm-0.3.0.jar"" ; "" apache-cassandra.zip/lib/javax.inject.jar"" ; "" apache-cassandra.zip/lib/jbcrypt-0.3m.jar"" ; "" apache-cassandra.zip/lib/jcl-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/joda-time-2.4.jar"" ; "" apache-cassandra.zip/lib/json-simple-1.1.jar"" ; "" apache-cassandra.zip/lib/libthrift-0.9.2.jar"" ; "" apache-cassandra.zip/lib/licenses/ST4-4.0.8.txt"" ; "" apache-cassandra.zip/lib/licenses/antlr-runtime-3.5.2.txt"" ; "" apache-cassandra.zip/lib/licenses/compress-lzf-0.8.4.txt"" ; "" apache-cassandra.zip/lib/licenses/concurrent-trees-2.4.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ecj-4.4.2.txt"" ; "" apache-cassandra.zip/lib/licenses/futures-2.1.6.txt"" ; "" apache-cassandra.zip/lib/licenses/high-scale-lib-1.0.6.txt"" ; "" apache-cassandra.zip/lib/licenses/jbcrypt-0.3m.txt"" ; "" apache-cassandra.zip/lib/licenses/jcl-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/jna-4.2.2.txt"" ; "" apache-cassandra.zip/lib/licenses/jstackjunit-0.0.1.txt"" ; "" apache-cassandra.zip/lib/licenses/log4j-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-classic-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-core-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/lz4-1.3.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-core-3.1.5.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-jvm-3.1.5.txt"" ; "" apache-cassandra.zip/lib/licenses/ohc-0.4.4.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config-base-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config3-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/sigar-1.6.4.txt"" ; "" apache-cassandra.zip/lib/licenses/six-1.7.3.txt"" ; "" apache-cassandra.zip/lib/licenses/slf4j-api-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/stream-2.5.2.txt"" ; "" apache-cassandra.zip/lib/log4j-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/logback-classic-1.1.3.jar"" ; "" apache-cassandra.zip/lib/logback-core-1.1.3.jar"" ; "" apache-cassandra.zip/lib/lz4-1.3.0.jar"" ; "" apache-cassandra.zip/lib/sigar-1.6.4.jar"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-pa-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-s390x-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal64-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-amd64-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.lib"" ; "" apache-cassandra.zip/lib/six-1.7.3-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/slf4j-api-1.7.7.jar"" ; "" apache-cassandra.zip/lib/snakeyaml-1.11.jar"" ; "" apache-cassandra.zip/lib/snappy-java-1.1.1.7.jar"" ; "" apache-cassandra.zip/lib/stream-2.5.2.jar"" ; "" apache-cassandra.zip/lib/thrift-server-0.3.7.jar"" ; "" apache-cassandra.zip/pylib/cqlshlib/__init__.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/saferscanner.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/sslhandling.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/ansi_colors.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/basecase.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cql_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_commands.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_invocation.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/winpty.py"" ; "" apache-cassandra.zip/tools/bin/cassandra-stress.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/tools/bin/sstableexpiredblockers.bat"" ; "" apache-cassandra.zip/tools/bin/sstablelevelreset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablemetadata.bat"" ; "" apache-cassandra.zip/tools/bin/sstableofflinerelevel.bat"" ; "" apache-cassandra.zip/tools/bin/sstablerepairedset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablesplit.bat""]*CVE :* CVE-2019-16869
*URL :* http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-16869
*Remediation :* This component does not have any non-vulnerable Version. Please contact the vendor to get this vulnerability fixed.",,abdulazizali,abhishek.scs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,2019-11-13 05:43:22.0,,,,,,,All,,,,,"0|z08jlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CVE-2019-0205 (Apache Thrift all versions up to and including 0.12.0 vulnerable) of severity 7.5,CASSANDRA-15415,13267909,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,abhishek.scs,abhishek.scs,13/Nov/19 04:59,17/Jan/22 12:50,13/Jul/23 08:38,17/Jan/22 12:50,,,,,,,,,,0,,,,"*Description :**Description :* *Severity :* CVE CVSS 3: 7.5Sonatype CVSS 3: 7.5
 
 *Weakness :* CVE CWE: 835
 
 *Source :* National Vulnerability Database
 
 *Categories :* Data 
 *Description from CVE :* In Apache Thrift all versions up to and including 0.12.0, a server or client may run into an endless loop when feed with specific input data. Because the issue had already been partially fixed in version 0.11.0, depending on the installed version it affects only certain language bindings.
 
 *Explanation :* This issue has undergone the Sonatype Fast-Track process. For more information, please see the Sonatype Knowledge Base Guide. 
 *Detection :* The application is vulnerable by using this component. 
 *Recommendation :* We recommend upgrading to a version of this component that is not vulnerable to this specific issue.Note: If this component is included as a bundled/transitive dependency of another component, there may not be an upgrade path. In this instance, we recommend contacting the maintainers who included the vulnerable package. Alternatively, we recommend investigating alternative components or a potential mitigating control. 
 *Advisories :* Project: http://mail-archives.apache.org/mod_mbox/thrift-dev/201910.mâ€¦
 
 *CVSS Details :* CVE CVSS 3: 7.5CVSS Vector: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:H
*Occurences (Paths) :* [""TSO/windows_bao_devstudio_installer_8.2.01.zip/files/5d8b80e7a292.zip/plugins/com.bmc.ao.ui.studio.plugin_1.0.0.jar/lib/ecj-4.4.2.jar"" ; "" apache-cassandra.zip/bin/cassandra.bat"" ; "" apache-cassandra.zip/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/bin/cqlsh.bat"" ; "" apache-cassandra.zip/bin/debug-cql.bat"" ; "" apache-cassandra.zip/bin/source-conf.ps1"" ; "" apache-cassandra.zip/bin/sstableloader.bat"" ; "" apache-cassandra.zip/bin/sstablescrub.bat"" ; "" apache-cassandra.zip/bin/sstableupgrade.bat"" ; "" apache-cassandra.zip/bin/sstableverify.bat"" ; "" apache-cassandra.zip/bin/stop-server"" ; "" apache-cassandra.zip/bin/stop-server.ps1"" ; "" apache-cassandra.zip/conf/README.txt"" ; "" apache-cassandra.zip/conf/cassandra-rackdc.properties"" ; "" apache-cassandra.zip/conf/cassandra-topology.properties"" ; "" apache-cassandra.zip/conf/commitlog_archiving.properties"" ; "" apache-cassandra.zip/conf/triggers/README.txt"" ; "" apache-cassandra.zip/lib/ST4-4.0.8.jar"" ; "" apache-cassandra.zip/lib/airline-0.6.jar"" ; "" apache-cassandra.zip/lib/antlr-runtime-3.5.2.jar"" ; "" apache-cassandra.zip/lib/commons-cli-1.1.jar"" ; "" apache-cassandra.zip/lib/commons-lang3-3.1.jar"" ; "" apache-cassandra.zip/lib/commons-math3-3.2.jar"" ; "" apache-cassandra.zip/lib/compress-lzf-0.8.4.jar"" ; "" apache-cassandra.zip/lib/concurrentlinkedhashmap-lru-1.4.jar"" ; "" apache-cassandra.zip/lib/disruptor-3.0.1.jar"" ; "" apache-cassandra.zip/lib/futures-2.1.6-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/high-scale-lib-1.0.6.jar"" ; "" apache-cassandra.zip/lib/jamm-0.3.0.jar"" ; "" apache-cassandra.zip/lib/javax.inject.jar"" ; "" apache-cassandra.zip/lib/jbcrypt-0.3m.jar"" ; "" apache-cassandra.zip/lib/jcl-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/joda-time-2.4.jar"" ; "" apache-cassandra.zip/lib/json-simple-1.1.jar"" ; "" apache-cassandra.zip/lib/libthrift-0.9.2.jar"" ; "" apache-cassandra.zip/lib/licenses/ST4-4.0.8.txt"" ; "" apache-cassandra.zip/lib/licenses/antlr-runtime-3.5.2.txt"" ; "" apache-cassandra.zip/lib/licenses/compress-lzf-0.8.4.txt"" ; "" apache-cassandra.zip/lib/licenses/concurrent-trees-2.4.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ecj-4.4.2.txt"" ; "" apache-cassandra.zip/lib/licenses/futures-2.1.6.txt"" ; "" apache-cassandra.zip/lib/licenses/high-scale-lib-1.0.6.txt"" ; "" apache-cassandra.zip/lib/licenses/jbcrypt-0.3m.txt"" ; "" apache-cassandra.zip/lib/licenses/jcl-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/jna-4.2.2.txt"" ; "" apache-cassandra.zip/lib/licenses/jstackjunit-0.0.1.txt"" ; "" apache-cassandra.zip/lib/licenses/log4j-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-classic-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-core-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/lz4-1.3.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-core-3.1.5.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-jvm-3.1.5.txt"" ; "" apache-cassandra.zip/lib/licenses/ohc-0.4.4.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config-base-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config3-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/sigar-1.6.4.txt"" ; "" apache-cassandra.zip/lib/licenses/six-1.7.3.txt"" ; "" apache-cassandra.zip/lib/licenses/slf4j-api-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/stream-2.5.2.txt"" ; "" apache-cassandra.zip/lib/log4j-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/logback-classic-1.1.3.jar"" ; "" apache-cassandra.zip/lib/logback-core-1.1.3.jar"" ; "" apache-cassandra.zip/lib/lz4-1.3.0.jar"" ; "" apache-cassandra.zip/lib/sigar-1.6.4.jar"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-pa-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-s390x-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal64-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-amd64-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.lib"" ; "" apache-cassandra.zip/lib/six-1.7.3-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/slf4j-api-1.7.7.jar"" ; "" apache-cassandra.zip/lib/snakeyaml-1.11.jar"" ; "" apache-cassandra.zip/lib/snappy-java-1.1.1.7.jar"" ; "" apache-cassandra.zip/lib/stream-2.5.2.jar"" ; "" apache-cassandra.zip/lib/thrift-server-0.3.7.jar"" ; "" apache-cassandra.zip/pylib/cqlshlib/__init__.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/saferscanner.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/sslhandling.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/ansi_colors.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/basecase.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cql_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_commands.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_invocation.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/winpty.py"" ; "" apache-cassandra.zip/tools/bin/cassandra-stress.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/tools/bin/sstableexpiredblockers.bat"" ; "" apache-cassandra.zip/tools/bin/sstablelevelreset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablemetadata.bat"" ; "" apache-cassandra.zip/tools/bin/sstableofflinerelevel.bat"" ; "" apache-cassandra.zip/tools/bin/sstablerepairedset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablesplit.bat""]
*CVE :* CVE-2019-0205
*URL :* http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2019-0205
*Remediation :* This component does not have any non-vulnerable Version. Please contact the vendor to get this vulnerability fixed.",,abhishek.scs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,2019-11-13 04:59:49.0,,,,,,,All,,,,,"0|z08jk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sonatype-2018-0119 (Netty is vulnerable to a Denial of Service (DoS) attack),CASSANDRA-15414,13267905,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,abhishek.scs,abhishek.scs,13/Nov/19 04:43,17/Jan/22 12:51,13/Jul/23 08:38,17/Jan/22 12:51,,,,,,,,,,0,,,,"*Description :*
*Severity :* Sonatype CVSS 3.0: 7.5

*Weakness :* Sonatype CWE: 400

*Source :* Sonatype Data Research

*Categories :* Data

*Explanation :* Netty is vulnerable to a Denial of Service (DoS) attack.The OpenSslEngine class does not have a mechanism to reject remotely initiated SSL renegotiation requests.An attacker can exploit this vulnerability by sending a large number of SSL renegotiation requests, causing the application to attempt to process all of them and tying up CPU and memory resources until the application becomes unresponsive or crashed, resulting in a Denial of Service.

*Detection :* The application is vulnerable by using this component.

*Recommendation :* We recommend upgrading to a version of this component that is not vulnerable to this specific issue.

*Root Cause :* Cassandra-2.2.5.nupkgOpenSslServerContext.class : [4.0.20.Final, 4.0.25.Final)

*Advisories :* Project: [https://github.com/netty/netty/pull/3750]

*CVSS Details :* Sonatype CVSS 3.0: 7.5

Occurences (Paths) : ["" apache-cassandra.zip/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/bin/cassandra.in.sh"" ;"" apache-cassandra.zip/bin/cqlsh.bat"" ; "" apache-cassandra.zip/bin/debug-cql.bat"" ; "" apache-cassandra.zip/bin/source-conf.ps1"" ; "" apache-cassandra.zip/bin/sstableloader.bat"" ; "" apache-cassandra.zip/bin/sstablescrub.bat"" ; "" apache-cassandra.zip/bin/sstableupgrade.bat"" ; "" apache-cassandra.zip/bin/sstableverify.bat"" ; "" apache-cassandra.zip/bin/stop-server"" ; "" apache-cassandra.zip/bin/stop-server.bat"" ; "" apache-cassandra.zip/bin/stop-server.ps1"" ; "" apache-cassandra.zip/conf/README.txt"" ; "" apache-cassandra.zip/conf/cassandra-rackdc.properties"" ; "" apache-cassandra.zip/conf/cassandra-topology.properties"" ; "" apache-cassandra.zip/conf/commitlog_archiving.properties"" ; "" apache-cassandra.zip/conf/triggers/README.txt"" ; "" apache-cassandra.zip/lib/ST4-4.0.8.jar"" ; "" apache-cassandra.zip/lib/airline-0.6.jar"" ; "" apache-cassandra.zip/lib/antlr-runtime-3.5.2.jar"" ; "" apache-cassandra.zip/lib/commons-cli-1.1.jar"" ; "" apache-cassandra.zip/lib/commons-lang3-3.1.jar"" ; "" apache-cassandra.zip/lib/commons-math3-3.2.jar"" ; "" apache-cassandra.zip/lib/compress-lzf-0.8.4.jar"" ; "" apache-cassandra.zip/lib/concurrentlinkedhashmap-lru-1.4.jar"" ; "" apache-cassandra.zip/lib/disruptor-3.0.1.jar"" ; "" apache-cassandra.zip/lib/ecj-4.4.2.jar"" ; "" apache-cassandra.zip/lib/futures-2.1.6-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/high-scale-lib-1.0.6.jar"" ; "" apache-cassandra.zip/lib/jamm-0.3.0.jar"" ; "" apache-cassandra.zip/lib/javax.inject.jar"" ; "" apache-cassandra.zip/lib/jbcrypt-0.3m.jar"" ; "" apache-cassandra.zip/lib/jcl-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/joda-time-2.4.jar"" ; "" apache-cassandra.zip/lib/json-simple-1.1.jar"" ; "" apache-cassandra.zip/lib/libthrift-0.9.2.jar"" ; "" apache-cassandra.zip/lib/licenses/ST4-4.0.8.txt"" ; "" apache-cassandra.zip/lib/licenses/antlr-runtime-3.5.2.txt"" ; "" apache-cassandra.zip/lib/licenses/compress-lzf-0.8.4.txt"" ; "" apache-cassandra.zip/lib/licenses/concurrent-trees-2.4.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ecj-4.4.2.txt"" ; "" apache-cassandra.zip/lib/licenses/futures-2.1.6.txt"" ; "" apache-cassandra.zip/lib/licenses/high-scale-lib-1.0.6.txt"" ; "" apache-cassandra.zip/lib/licenses/jbcrypt-0.3m.txt"" ; "" apache-cassandra.zip/lib/licenses/jcl-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/jna-4.2.2.txt"" ; "" apache-cassandra.zip/lib/licenses/jstackjunit-0.0.1.txt"" ; "" apache-cassandra.zip/lib/licenses/log4j-over-slf4j-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-classic-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/logback-core-1.1.3.txt"" ; "" apache-cassandra.zip/lib/licenses/lz4-1.3.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-core-3.1.0.txt"" ; "" apache-cassandra.zip/lib/licenses/metrics-jvm-3.1.0.txt"" ; "" apache-cassandra.zip/lib/licenses/ohc-0.4.4.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config-base-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/reporter-config3-3.0.3.txt"" ; "" apache-cassandra.zip/lib/licenses/sigar-1.6.4.txt"" ; "" apache-cassandra.zip/lib/licenses/six-1.7.3.txt"" ; "" apache-cassandra.zip/lib/licenses/slf4j-api-1.7.7.txt"" ; "" apache-cassandra.zip/lib/licenses/stream-2.5.2.txt"" ; "" apache-cassandra.zip/lib/log4j-over-slf4j-1.7.7.jar"" ; "" apache-cassandra.zip/lib/logback-classic-1.1.3.jar"" ; "" apache-cassandra.zip/lib/logback-core-1.1.3.jar"" ; "" apache-cassandra.zip/lib/lz4-1.3.0.jar"" ; "" apache-cassandra.zip/lib/metrics-core-3.1.0.jar"" ; "" apache-cassandra.zip/lib/metrics-logback-3.1.0.jar"" ; "" apache-cassandra.zip/lib/sigar-1.6.4.jar"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-amd64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ia64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-pa-hpux-11.sl"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-aix-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-ppc64-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-s390x-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-sparc64-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-universal64-macosx.dylib"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-5.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-freebsd-6.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-linux.so"" ; "" apache-cassandra.zip/lib/sigar-bin/libsigar-x86-solaris.so"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-amd64-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.dll"" ; "" apache-cassandra.zip/lib/sigar-bin/sigar-x86-winnt.lib"" ; "" apache-cassandra.zip/lib/six-1.7.3-py2.py3-none-any.zip"" ; "" apache-cassandra.zip/lib/slf4j-api-1.7.7.jar"" ; "" apache-cassandra.zip/lib/snakeyaml-1.11.jar"" ; "" apache-cassandra.zip/lib/snappy-java-1.1.1.7.jar"" ; "" apache-cassandra.zip/lib/stream-2.5.2.jar"" ; "" apache-cassandra.zip/lib/thrift-server-0.3.7.jar"" ; "" apache-cassandra.zip/pylib/cqlshlib/__init__.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/saferscanner.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/sslhandling.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/ansi_colors.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/basecase.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cql_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_commands.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_invocation.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/test_cqlsh_parsing.py"" ; "" apache-cassandra.zip/pylib/cqlshlib/test/winpty.py"" ; "" apache-cassandra.zip/tools/bin/cassandra-stress.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.bat"" ; "" apache-cassandra.zip/tools/bin/cassandra.in.sh"" ; "" apache-cassandra.zip/tools/bin/sstableexpiredblockers.bat"" ; "" apache-cassandra.zip/tools/bin/sstablelevelreset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablemetadata.bat"" ; "" apache-cassandra.zip/tools/bin/sstableofflinerelevel.bat"" ; "" apache-cassandra.zip/tools/bin/sstablerepairedset.bat"" ; "" apache-cassandra.zip/tools/bin/sstablesplit.bat""]

*CVE :* sonatype-2018-0119",,abhishek.scs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14612,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,2019-11-13 04:43:38.0,,,,,,,All,,,,,"0|z08jjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool compactionstats showing extra pending task for TWCS,CASSANDRA-15409,13267505,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,e.dimitrova,e.dimitrova,11/Nov/19 16:08,15/May/20 08:39,13/Jul/23 08:38,11/Dec/19 16:07,3.11.6,4.0,4.0-alpha3,,,Tool/nodetool,,,,0,,,,"Summary: nodetool compactionstats showing extra pending task for TWCS
-----------------
The output of {{nodetool compactionstats}}can show ""pending tasks: 1"" when there are actually none. This seems to be a consistent problem in testing C* trunk. In my testing, it looks like the {{nodetool compactionstats}} counter output is consistently off by 1 as compared to the table output of the tasks

 

testing with {{concurrent_compactors: 8}}

In 12 hours it never ended, always showing 1 pending job

 ",,e.dimitrova,jeromatron,stefania,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,e.dimitrova,,,,,,,,,,,,Correctness -> Consistency,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Dec 11 16:05:06 UTC 2019,,,,,,,All,,,,,"0|z08h2g:",9223372036854775807,,,,,,,stefania,,,,Low,,3.0.8,,,"https://gitbox.apache.org/repos/asf?p=cassandra.git;a=commitdiff;h=122cf57f1134704769cce9daddf882c3ea578905",,,,,,,,,"The issues was successfully reproduced and then tested again after the patch, to confirm it is solved

DTests passed too",,,,,"11/Nov/19 20:59;e.dimitrova;Fallout test successfully ran to reproduce the error.

Patch implemented for version 3.11 and test restarted;;;","13/Nov/19 18:22;e.dimitrova;Patch implemented for both v.3.11 and v.4

[Repository|https://github.com/ekaterinadimitrova2/cassandra]

branch 3.11 - CASSANDRA-15409-3.11

branch 4 - CASSANDRA-15409-trunk;;;","15/Nov/19 15:25;stefania;Patch LGTM

Moving forward, it's perhaps easier to post the direct link to the branches:

[CASSANDRA-15409-3.11|https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15409-3.11]
 [CASSANDRA-15409-trunk|https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15409-trunk]

Also, before merging, we need to post the CI results.;;;","11/Dec/19 16:05;stefania;CI looked good on our infra, so committed to 3.11 as [122cf57f1134704769cce9daddf882c3ea578905|https://gitbox.apache.org/repos/asf?p=cassandra.git;a=commitdiff;h=122cf57f1134704769cce9daddf882c3ea578905] and merged into trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra throws SyntaxException for obsolete keywords that Thrift API permits,CASSANDRA-15408,13267180,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,leonz,leonz,08/Nov/19 19:41,13/Nov/19 16:53,13/Jul/23 08:38,13/Nov/19 16:53,3.0.20,3.11.6,,,,Documentation/NEWS.txt,,,,0,,,,"In [this refactor|https://github.com/apache/cassandra/commit/b31845c4a7982358a7c5bfd9bcf572fda6c1bfa9#diff-826a67bf1ae2e45372a35a6a2a6f3f3cL74] of CFPropDefs to TableAttributes for CASSANDRA-9712, three obsolete keywords were removed:
{code:java}
        obsoleteKeywords.add(""index_interval"");
        obsoleteKeywords.add(""replicate_on_write"");
        obsoleteKeywords.add(""populate_io_cache_on_flush"");
{code}
 
The Thrift API continues to reference these keywords as deprecated, so it's not clear that they are actually unsupported.

Could we either add them back as obsoleteKeywords, or add a change log that statements with these properties will fail (There is already a changelog about ""index_interval"" but not the other two)?  I understand that the Thrift API is totally deprecated so I don't feel strongly about cleaning it up.",,aleksey,leonz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/19 00:53;leonz;CASSANDRA-15408.patch;https://issues.apache.org/jira/secure/attachment/12985568/CASSANDRA-15408.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,Docs,,Wed Nov 13 16:52:50 UTC 2019,,,,,,,All,,,,,"0|z08f28:",9223372036854775807,,,,,,,aleksey,,,,Low,,3.0.0,,,https://github.com/apache/cassandra/commit/01b52de41e742d4c9a27d9d907839f1082af95a2,,,,,,,,,"None; just a docs PR",,,,,"11/Nov/19 14:17;aleksey;They have no effect anymore since 2.1, and were deprecated there. And, per our policies, removed in the next major release after deprecation.

Not to mention Thrift is completely gone in 4.0 now. I would prefer to let these go personally.;;;","11/Nov/19 15:29;leonz;Hey Aleksey - What are your thoughts on just adding a retroactive change log signifying this break?  While these keywords are no longer used, this is still an API break in 3.x which we had to deal with during a migration.;;;","11/Nov/19 15:42;aleksey;Hi Leon; I'm inclined to agree that the omission of {{replicate_on_write}} and {{populate_io_cache_on_flush}} in 3.0 (and perhaps 3.11) is a documentation bug. {{index_interval}}, luckily, is mentioned.

So if you cook up a tiny patch to update NEWS.txt, upgrading section, to mention them, I'll be happy to commit.;;;","12/Nov/19 00:54;leonz;Added a quick patch for the docs change!;;;","13/Nov/19 16:52;aleksey;Thanks! Committed to 3.0 and merged upwards.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Hint-dispatcher file-channel not closed, if ""open()"" fails with OOM",CASSANDRA-15407,13267176,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,e.dimitrova,e.dimitrova,08/Nov/19 19:35,21/Dec/20 08:07,13/Jul/23 08:38,21/Nov/19 11:09,4.0,4.0-alpha3,,,,Consistency/Hints,,,,0,,,,Some places in the code base do not to close the file (some channel-proxy) in case of errors. We should close the channel-proxy in those cases - at least to not make the situation (due to that OOM) even worse.,,abdulazizali,aleksey,e.dimitrova,ifesdjeen,jeromatron,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,e.dimitrova,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Nov 21 11:09:46 UTC 2019,,,,,,,All,,,,,"0|z08f1c:",9223372036854775807,,,,,,,ifesdjeen,snazy,,,Low,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/c2e11bd4224b2110abe6aa84c8882e85980e3491,,,,,,,,,"DTests completed successfully.  

Patch for version 4 available here at branch CASSANDRA-15407-trunk",,,,,"11/Nov/19 15:44;e.dimitrova;Patch implemented:

[https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15407-trunk]

The build was tested locally. Currently running the Jenkins tests before submission for review.;;;","14/Nov/19 19:03;e.dimitrova;Tests completed successfully. Patch ready for review/commit;;;","20/Nov/19 15:00;snazy;+1

Committed as [c2e11bd4224b2110abe6aa84c8882e85980e3491|https://github.com/apache/cassandra/commit/c2e11bd4224b2110abe6aa84c8882e85980e3491] to [trunk|https://github.com/apache/cassandra/tree/trunk]
;;;","21/Nov/19 11:09;ifesdjeen;Closing since it is committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Mixed mode reads on compact storage tables can return incomplete results,CASSANDRA-15405,13266879,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,07/Nov/19 17:37,11/Nov/19 10:01,13/Jul/23 08:38,11/Nov/19 10:01,3.0.20,3.11.6,,,,Legacy/Core,,,,0,,,,"In mixed mode (2.1/3.0), when coordinating a read on a 2.1 node, reading data from 3.0 nodes, we [incorrectly trim|https://github.com/apache/cassandra/blob/53f604dc1789a800dbcbc3c8aee77f8f36b8b5db/src/java/org/apache/cassandra/db/LegacyLayout.java#L529] the result (if it has tombstones) when preparing it for the 2.1 node, this is then [interpreted by the 2.1 node|https://github.com/apache/cassandra/blob/cassandra-2.1/src/java/org/apache/cassandra/service/pager/AbstractQueryPager.java#L110] as the pager has been exhausted.",,aleksey,jeromatron,marcuse,samt,sumanth.pasupuleti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Nov 11 10:01:22 UTC 2019,,,,,,,All,,,,,"0|z08d7c:",9223372036854775807,,,,,,,aleksey,samt,,,Critical,,3.0.0,,,https://github.com/apache/cassandra/commit/f0aa60bec728fbdb9ee7455d2d6d2f6feb183330,,,,,,,,,"new upgrade test, circle ci runs",,,,,"07/Nov/19 17:43;marcuse;Patch makes sure we don't blindly trim the results to limit, instead it makes sure the number of live cells returned is correct.

[patch|https://github.com/krummas/cassandra/commits/marcuse/15405], [circle|https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15405];;;","07/Nov/19 18:05;samt;Mostly LGTM. 
You could short circuit {{maybeTrimLiveCells}} where {{maxLiveCells > cells.size()}}, which should be the more common case I would think. 
Also, there are a handful of test failures due to {{command}} being null. Mostly these are down to the test itself, but there is a legit case where a null {{ReadCommand}} is passed, in {{LegacyBatchlogMigrator}}. 

I've pushed a patch which fixes these [here|https://github.com/beobal/cassandra/commit/18f8fe1f2b8a993efbc5fdb6bf99c2e90e7348be];;;","07/Nov/19 18:30;marcuse;thanks, cherry-picked and rerunning tests;;;","08/Nov/19 11:58;aleksey;LGTM as is to me. Minor inefficiency, I think: we are potentially oversending some tombstones if they follow the last live cell that's still within limit. I think it's safe to not do that.;;;","11/Nov/19 10:01;marcuse;Thanks, committed with the small change to not oversend tombstones;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra 3.0.18 went OOM several hours after joining a cluster,CASSANDRA-15400,13266623,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,tsteinmaurer,tsteinmaurer,06/Nov/19 16:22,15/May/20 08:39,13/Jul/23 08:38,11/Nov/19 23:36,3.0.20,3.11.6,4.0,4.0-alpha3,,Local/SSTable,,,,0,,,,"We have been moving from Cassandra 2.1.18 to Cassandra 3.0.18 and have been facing an OOM two times with 3.0.18 on newly added nodes joining an existing cluster after several hours being successfully bootstrapped.

Running in AWS:
* m5.2xlarge, EBS SSD (gp2)
* Xms/Xmx12G, Xmn3G, CMS GC, OpenJDK8u222
* 4 compaction threads, throttling set to 32 MB/s

What we see is a steady increase in the OLD gen over many hours.
!cassandra_jvm_metrics.png!

* The node started to join / auto-bootstrap the cluster on Oct 30 ~ 12:00
* It basically finished joining the cluster (UJ => UN) ~ 19hrs later on Oct 31 ~ 07:00 also starting to be a member of serving client read requests
!cassandra_operationcount.png!

Memory-wise (on-heap) it didn't look that bad at that time, but old gen usage constantly increased.

We see a correlation in increased number of SSTables and pending compactions.
!cassandra_sstables_pending_compactions.png!

Until we reached the OOM somewhere in Nov 1 in the night. After a Cassandra startup (metric gap in the chart above), number of SSTables + pending compactions is still high, but without facing memory troubles since then.

This correlation is confirmed by the auto-generated heap dump with e.g. ~ 5K BigTableReader instances with ~ 8.7GByte retained heap in total.
!cassandra_hprof_dominator_classes.png!

Having a closer look on a single object instance, seems like each instance is ~ 2MByte in size.
!cassandra_hprof_bigtablereader_statsmetadata.png!
With 2 pre-allocated byte buffers (highlighted in the screen above) at 1 MByte each

We have been running with 2.1.18 for > 3 years and I can't remember dealing with such OOM in the context of extending a cluster.

While the MAT screens above are from our production cluster, we partly can reproduce this behavior in our loadtest environment (although not going full OOM there), thus I might be able to share a hprof from this non-prod environment if needed.

Thanks a lot.



",,abdulazizali,bdeggleston,ifesdjeen,jeromatron,jjirsa,marcuse,tsteinmaurer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/19 16:18;tsteinmaurer;cassandra_hprof_bigtablereader_statsmetadata.png;https://issues.apache.org/jira/secure/attachment/12985084/cassandra_hprof_bigtablereader_statsmetadata.png","06/Nov/19 16:16;tsteinmaurer;cassandra_hprof_dominator_classes.png;https://issues.apache.org/jira/secure/attachment/12985085/cassandra_hprof_dominator_classes.png","06/Nov/19 22:05;tsteinmaurer;cassandra_hprof_statsmetadata.png;https://issues.apache.org/jira/secure/attachment/12985121/cassandra_hprof_statsmetadata.png","06/Nov/19 16:07;tsteinmaurer;cassandra_jvm_metrics.png;https://issues.apache.org/jira/secure/attachment/12985088/cassandra_jvm_metrics.png","06/Nov/19 16:10;tsteinmaurer;cassandra_operationcount.png;https://issues.apache.org/jira/secure/attachment/12985087/cassandra_operationcount.png","06/Nov/19 16:13;tsteinmaurer;cassandra_sstables_pending_compactions.png;https://issues.apache.org/jira/secure/attachment/12985086/cassandra_sstables_pending_compactions.png","10/Nov/19 11:30;ifesdjeen;image.png;https://issues.apache.org/jira/secure/attachment/12985454/image.png","13/Nov/19 08:10;tsteinmaurer;oldgen_increase_nov12.jpg;https://issues.apache.org/jira/secure/attachment/12985701/oldgen_increase_nov12.jpg",,,,,,,,,,,,,,,,,,,,,,8.0,bdeggleston,,,,,,,,,,,,Degradation -> Resource Management,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Nov 13 08:10:59 UTC 2019,,,,,,,All,,,,,"0|z08bmg:",9223372036854775807,,,,,,,bdeggleston,marcuse,,,Normal,,3.0.17,,,https://github.com/apache/cassandra/commit/9382186f70cf0560c5308dc324411bef52cf461f,,,,,,,,,circle,,,,,"06/Nov/19 17:53;marcuse;Could you post your schema?;;;","06/Nov/19 22:13;tsteinmaurer;[~marcuse], the data model has evolved starting with Astyanax/Thrift moved over to pure CQL3 access (without real data migration), but still with our own application-side serializer framework, working with byte buffers, thus BLOBs on the data model side.

Our high volume (usually > 1TByte per node, RF=3) CF/table looks like that, where we also see the majority of increasing number of pending compaction tasks, according to a per-CF JMX based self-monitoring:
{noformat}
CREATE TABLE ks.cf1 (
    k blob,
    n blob,
    v blob,
    PRIMARY KEY (k, n)
) WITH COMPACT STORAGE
...
;
{noformat}
Although we tend to also have single partitions in the area of > 100MByte, e.g. visible due to according compaction logs in the Cassandra log, all not being a real problem in practice with a heap of Xms/Xmx12G resp Xmn3G and Cas 2.1.

A few additional thoughts:
 * Likely the Cassandra node is utilizing most of the compaction threads (4 in this scenario with the m5.2xlarge instance type) with larger compactions on streamed data, giving less room for compactions of live data / actual writes while being in UJ, resulting in accessing much more smaller SSTables (looks like we have/had plenty in the area of 10-50MByte) then in UN starting to serve read requests
 * Is there anything known in Cas 3.0, which might result in streaming more data from other nodes compared to 2.1 resulting in increased compaction work to be done for newly joined nodes compared to 2.1
 * Is there anything known in Cas 3.0, which results in more frequent memtable flushes compared to 2.1, again resulting in increased compaction work
 * Talking about a single {{BigTableReader}} instance again, did anything change in regard to byte buffer pre-allocation at 1MByte in {{StatsMetadata}} per data member {{minClusteringValues}} and {{maxClusteringValues}} as shown in the hprof? Looks to me we potentially waste quite some on-heap memory here 
 !cassandra_hprof_statsmetadata.png|width=800!
* Is {{StatsMetadata}} purely on-heap? Or is it somehow pulled from off-heap first resulting in the 1MByte allocation, reminding me a bit on the NIO cache buffer bug (https://support.datastax.com/hc/en-us/articles/360000863663-JVM-OOM-direct-buffer-errors-affected-by-unlimited-java-nio-cache), with a recommendation setting it to exactly the number (-Djdk.nio.maxCachedBufferSize=1048576) we see in the hprof for the on-heap byte buffer

Number of compaction threads, compaction throttling is unchanged during the upgrade from 2.1 to 3.0 and if memory serves me well, we should see improved compaction throughput in 3.0 with the same throttling settings anyway.;;;","07/Nov/19 19:31;bdeggleston;Thanks for the excellent bug report [~tsteinmaurer], patches below

| [3.0|https://github.com/bdeggleston/cassandra/tree/15400-3.0] | [circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15400-3.0] |
| [3.11|https://github.com/bdeggleston/cassandra/tree/15400-3.11] | [circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15400-3.11] |
| [4.0|https://github.com/bdeggleston/cassandra/tree/15400-trunk] | [circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15400-trunk] |;;;","08/Nov/19 12:30;marcuse;+1, just the javadoc on {{minimize()}} in ClusteringPrefix.java: {{otherwise it returns a copy of itself.}} - it does not copy in this case

pushed a unit test for this here: https://github.com/krummas/cassandra/commits/blake/15400-3.0;;;","08/Nov/19 13:10;tsteinmaurer;[~bdeggleston], from ticket creation to a patch in ~ 24h. This is awesome! Many thanks.

* Out of curiosity, haven't looked too deep. I guess the patch does not make the content of the byte array smaller, but the capacity of the byte array basically in-sync with that and not 1MByte in general?
* Secondly, as 3.0.19 was released just recently, any ETA when a 3.0.20 public release might be available?

Again, many thanks.;;;","11/Nov/19 23:36;bdeggleston;committed as [9382186f70cf0560c5308dc324411bef52cf461f|https://github.com/apache/cassandra/commit/9382186f70cf0560c5308dc324411bef52cf461f], thanks!;;;","11/Nov/19 23:52;bdeggleston;[~tsteinmaurer], it doesn't reduce the size of the clustering values, just trims off the rest of the byte array the byte buffer is pointing to. So {{HeapByteBuffer[offset=100, limit=108, capacity=65536]}} becomes {{HeapByteBuffer[offset=0, limit=8, capacity=8]}}, which I think it what you meant. Regarding 3.0.20, I don't think we have immediate plans to cut a new release, but I'd encourage you to poke the dev list about cutting one with this patch if it's causing you pain in prod.;;;","13/Nov/19 08:10;tsteinmaurer; [~bdeggleston], thanks for the follow-up. Yes, causing quite some pain in prod in the moment, e.g. yesterday evening, close to running OOM again.
!oldgen_increase_nov12.jpg!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix system_traces creation timestamp; optimise system keyspace upgrades",CASSANDRA-15398,13266352,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,05/Nov/19 14:24,15/May/20 08:38,13/Jul/23 08:38,02/Dec/19 13:03,3.0.20,3.11.6,4.0,4.0-alpha3,,Cluster/Schema,,,,0,,,,"We have introduced changes to system_traces tables in 3.0 (removal of default_time_to_live, lowering of bloom_filter_fp_chance). We did not, however, bump the timestamp with which we add the tables to schema, still defaulting to 0. As a result, for clusters that upgraded from 2.1/2.2, on bounce we would always detect a mismatch between actual and desired table definitions, always try to reconcile it by issuing migration tasks, but have them never override the existing definitions in place.

Additionally, prior to 2.0.2 (CASSANDRA-6016) we’d use a ‘real’ timestamp, so for clusters that started on even earlier versions of C* (say, 1.2), a bump to the timestamp by 1 would be insufficient, and a larger generation is necessary (I picked Jan 1 2020 as cut-off date).

The patch also optimises the process of upgrading replicated system tables. Instead of issuing a migration task for every table that changed for every node, we batch all changes into a single schema migration task.",,aleksey,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Code,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Dec 02 13:02:49 UTC 2019,,,,,,,All,,,,,"0|z089y8:",9223372036854775807,,,,,,,samt,,,,Low,,3.0.0,,,"[7e878c1eb61b180227d6f1b70c4223e3ee71a754|https://github.com/apache/cassandra/commit/7e878c1eb61b180227d6f1b70c4223e3ee71a754]",,,,,,,,,Unit tests added,,,,,"07/Nov/19 18:11;aleksey;3.0: [code|https://github.com/iamaleksey/cassandra/commits/15398-3.0], [CI|https://circleci.com/workflow-run/67c87af7-3421-4e0e-8104-58bbb2bdfa67]
3.11: [code|https://github.com/iamaleksey/cassandra/commits/15398-3.11], [CI|https://circleci.com/workflow-run/8035e900-0589-46ad-88cf-cd64c6d2d2af]
4.0: [code|https://github.com/iamaleksey/cassandra/commits/15398-4.0], [CI|https://circleci.com/workflow-run/66c01c80-e97b-420a-b94b-598f8702d318];;;","21/Nov/19 13:53;samt;+1 LGTM;;;","02/Dec/19 13:02;aleksey;Cheers, committed to 3.0 as [7e878c1eb61b180227d6f1b70c4223e3ee71a754|https://github.com/apache/cassandra/commit/7e878c1eb61b180227d6f1b70c4223e3ee71a754] and merged up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure that tracing doesn't break connections in 3.x/4.0 mixed mode by default,CASSANDRA-15385,13265319,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,30/Oct/19 14:51,21/Dec/20 08:08,13/Jul/23 08:38,04/Nov/19 18:05,3.0.20,3.11.6,,,,Observability/Tracing,,,,0,,,,,,aleksey,mck,samt,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15441,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Availability,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Nov 04 18:03:13 UTC 2019,,,,,,,All,,,,,"0|z083ko:",9223372036854775807,,,,,,,samt,,,,Normal,,4.0-alpha,4.0-alpha1,,"[9b1f3796a65db46f15f2f2ad8af4180f71e3f53f|https://github.com/apache/cassandra/commit/9b1f3796a65db46f15f2f2ad8af4180f71e3f53f]",,,,,,,,,"Use existing upgrade test that covers it, remove whitelisted error logs.",,,,,"31/Oct/19 13:30;aleksey;CASSANDRA-7544 allowed storage port to be configurable per node, and among its changes introduced several new columns to some of the distributed system tables:

1. {{coordinator_port}} and {{participants_v2}} to {{system_distributed.repair_history}}
2. {{coordinator_port}} to {{system_traces.sessions}}
3. {{source_port}} to {{system_traces.events}}

(1) is not a huge deal, since we don't support repair in mixed mode clusters; (2) and (3), however, are. And while CASSANDRA-14841 modifies tracing logic to not write those new added columns while still in mixed mode, this is still a problem for reads - which in case of tracing will be issued automatically by the drivers.

CASSANDRA-14897 gives advice to add some of these columns manually (though not tracing) while still on 3.x, but there is a reason why such alters are explicitly forbidden and require a workaround: distributed pseudo-system tables are evolved programmatically, by bumping internal generation timestamp. Mixing manual alters and real-world timestamps and those surrogate timestamps simply prevents us from deterministically evolving those schemas in future versions.

The patches to be linked pre-add the columns on 3.0/3.11 side automatically, preserving the correct timestamps, so that tracing can go on uninhibited, without crashing the internode connections and losing the enqueued messages in the process.;;;","01/Nov/19 17:21;aleksey;3.0: [code|https://github.com/iamaleksey/cassandra/commits/15385-3.0], [CI|https://circleci.com/workflow-run/e202c2ef-6bba-453b-ac05-c8125b643683]
3.11: [code|https://github.com/iamaleksey/cassandra/commits/15385-3.11], [CI|https://circleci.com/workflow-run/a0ce5ec1-b3c7-417e-97c9-7387a30d0ab5]
4.0: [code|https://github.com/iamaleksey/cassandra/commits/15385-4.0] (only updates NEWS.txt)

dtest changes: [here|https://github.com/iamaleksey/cassandra-dtest/commits/15385]

We do have a dtest that covers the issue - it's just that at the moment it white-lists these exceptions and ignores them in the logs. The commit to dtest repo forces the test to fail without the suggested changes to C*, but the test passes cleanly with them, without any disconnects, message drops, and errors in the logs. Running the test is a little involved, however, since running upgrade tests is currently broken on Circle. I did run them locally however.;;;","04/Nov/19 12:11;samt;+1 Both the C* and dtest changes LGTM ;;;","04/Nov/19 18:03;aleksey;Cheers - committed to 3.0 as [9b1f3796a65db46f15f2f2ad8af4180f71e3f53f|https://github.com/apache/cassandra/commit/9b1f3796a65db46f15f2f2ad8af4180f71e3f53f] and merged upwards.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to init Verb in jvm-dtest,CASSANDRA-15383,13264968,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,yifanc,yifanc,29/Oct/19 06:53,21/Dec/20 08:08,13/Jul/23 08:38,29/Oct/19 11:11,4.0,4.0-alpha3,,,,Test/dtest/java,,,,0,,,,"Verb cannot be initialized in dtest. The test cases that has references to Verb get the following error
{code:java}
java.lang.ExceptionInInitializerError
	at org.apache.cassandra.net.Verb.<clinit>(Verb.java:87)
	at org.apache.cassandra.distributed.test.DistributedReadWritePathTest.failingReadRepairTest(DistributedReadWritePathTest.java:134)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.getConcurrentReaders(DatabaseDescriptor.java:1611)
	at org.apache.cassandra.concurrent.Stage.<clinit>(Stage.java:48)
{code}
It is caused by the DatabaseDescriptor being uninitialized and NPE was thrown. The dtest initializes the DatabaseDescriptor at instance-level with the isolated InstanceClassLoader. Since test code is not loaded by the InstanceClassLoader, it does not know the already initialized DatabaseDescriptor.",,benedict,yifanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Oct 29 16:08:27 UTC 2019,,,,,,,All,,,,,"0|z081eo:",9223372036854775807,,,,,,,benedict,,,,,,4.0-alpha,4.0-alpha1,,"[f05cf4f21a938bffea27b9d31e1fb79f37c44e81|https://github.com/apache/cassandra/commit/f05cf4f21a938bffea27b9d31e1fb79f37c44e81]",,,,,,,,,n/a: test fix,,,,,"29/Oct/19 11:11;benedict;Thanks, ninja fixed as super trivial issue.;;;","29/Oct/19 15:42;yifanc;Thanks. Since other dtest cases could also reference Verb, it probably make sense to move the initialization hook to the test base class.;;;","29/Oct/19 16:08;benedict;Maybe, though there might be other tests in future that might require a different initialisation sequence - it feels a little too heavy handed to impose it on them all, as I'm not sure it's easy to override the behaviour.  I agree it's a bit suboptimal as stands though.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing test - testDatabaseDescriptorRef::org.apache.cassandra.config.DatabaseDescriptorRefTest,CASSANDRA-15381,13264737,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,vinaykumarcse,vinaykumarcse,28/Oct/19 05:24,28/Oct/19 18:21,13/Jul/23 08:38,28/Oct/19 18:21,,,,,,Test/unit,,,,0,,,,"As part of Apache Cassandra 4.0-alpha2 voting, the following test is failing across different test suites and runs. 

CircleCI Run: [https://circleci.com/gh/vinaykumarchella/cassandra/487#tests/containers/37]

*testDatabaseDescriptorRef-compression - org.apache.cassandra.config.DatabaseDescriptorRefTest*
{code:java}
junit.framework.AssertionFailedError
	at org.apache.cassandra.config.DatabaseDescriptorRefTest.checkViolations(DatabaseDescriptorRefTest.java:293)
	at org.apache.cassandra.config.DatabaseDescriptorRefTest.testDatabaseDescriptorRef(DatabaseDescriptorRefTest.java:277){code}",,jmeredithco,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15371,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Oct 28 15:26:18 UTC 2019,,,,,,,All,,,,,"0|z07zzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/19 15:26;jmeredithco;Fix included as part of CASSANDRA-15371;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool command help is incorrect,CASSANDRA-15380,13264686,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,barakm,barakm,barakm,27/Oct/19 13:02,30/Apr/21 06:41,13/Jul/23 08:38,21/Mar/21 21:11,4.0,4.0-rc1,,,,Tool/nodetool,,,,0,,,,"The CLI help currently says:

""If no snapshotName is specified we will remove all snapshots""

 

Following CASSANDRA-13391, this is incorrect.

 ",,barakm,paulo,,,,,,,,,,,,,,,,,,,,,,,,,,"asfgit closed pull request #370:
URL: https://github.com/apache/cassandra/pull/370


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Mar/21 21:07;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,CASSANDRA-13391,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,barakm,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Sun Mar 21 21:10:16 UTC 2021,,,,,,,All,,,,,"0|z07zo0:",9223372036854775807,,,,,,,paulo,,,,Low,,4.0-alpha1,,,https://github.com/apache/cassandra/pull/370,,,,,,,,,"This is a doc change, so no test plan is required.",,,,,"27/Oct/19 13:10;barakm;A Pull Request to fix this issue is available at: [https://github.com/apache/cassandra/pull/370]

 

It was previously linked to CASSANDRA-13391;;;","21/Mar/21 21:10;paulo;Thanks for the patch. Committed as f7591080e936319fa44159c2a3591bbf822dc3f7.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove BackPressureStrategy,CASSANDRA-15375,13264494,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,benedict,rustyrazorblade,rustyrazorblade,25/Oct/19 15:10,17/Jul/20 21:19,13/Jul/23 08:38,17/Jul/20 15:17,4.0,4.0-beta1,,,,Messaging/Client,Observability/Logging,,,0,,,,"This is odd:

{{INFO [main] 2019-10-25 10:33:07,985 DatabaseDescriptor.java:803 - Back-pressure is disabled with strategy org.apache.cassandra.net.RateBasedBackPressure\{high_ratio=0.9, factor=5, flow=FAST}.}}

When I saw that, I wasn't sure if back pressure was actually disabled, or if I was really using {{RateBasedBackPressure.}}

This should change to output either:

{{Back-pressure is disabled}}

{{or}}

{{Back-pressure is enabled with strategy org.apache.cassandra.net.RateBasedBackPressure\{high_ratio=0.9, factor=5, flow=FAST}.}}{{}}

 ",,benedict,dcapwell,jwest,rustyrazorblade,sbtourist,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Jul 17 15:18:01 UTC 2020,,,,,,,All,,,,,"0|z07yhc:",9223372036854775807,,,,,,,benedict,sbtourist,snazy,,Low,,4.0-alpha4,,,"[d8993934e976d8edb94cbfe2974688dac63c5db5|https://github.com/apache/cassandra/commit/d8993934e976d8edb94cbfe2974688dac63c5db5]",,,,,,,,,n/a,,,,,"29/Oct/19 11:23;benedict;Which version of Cassandra are we discussing here?

My preference is to remove the back pressure feature entirely, since it's never really been tested at scale that I know of.  In 4.0 we have the capability of pushing the actual internode backpressure back to clients, in a similar manner to [~sumanth.pasupuleti] achieved recently in CASSANDRA-15013

But if that's too contentious, sure, let's clean up our logging :);;;","29/Oct/19 19:38;rustyrazorblade;I found this in trunk, but assume it's elsewhere.

I don't have any test data showing it's affect on performance either way, I've never enabled it.;;;","22/Feb/20 01:41;rustyrazorblade;[~benedict] I think we might be a bit late to remove the back pressure mechanism from 4.0.  I intend to fix the logging, and maybe we evaluate it for removal in the next release?;;;","22/Feb/20 11:14;benedict;It's never too late: it's about 5m of work to remove, it has precisely two insertion points in the code, and it's never actually used.  If we are interested at all in doing it, I can do it right now.;;;","22/Feb/20 18:36;dcapwell;bq. My preference is to remove the back pressure feature entirely, since it's never really been tested at scale that I know of.

A large chunk of the work for 4.0 is testing, so removing something because its not tested isn't really fair since this wouldn't be unique. Also, based off this JIRA, you can disable it; so don't see a reason to remove.

bq. In 4.0 we have the capability of pushing the actual internode backpressure back to clients

Is there a jira for actually testing the different back pressures we have before 4.0 releases?  If not can we?

I would personally love to see ""this is how 4.0 doesn't crash under load"" and tests to back it up; if there is a test for this already do let me know =);;;","22/Feb/20 22:36;benedict;bq. since it's never really been tested at scale that I know of

^ This was a euphemism for “this feature has never been used, and is probably bad”.  It was implemented some time ago by DataStax, never advertised in any way by OSS, and has never ben updated (making it either the first perfect feature, or broken).  It has perhaps been used by DataStax in their own offerings, but never by OSS.  It is unlikely (m?)any even know it exists.

Given the 4.0 networking changes, this feature no longer provides any utility for stability.  We now limit the amount of data inbound from any specific (and all) coordinators so that we cannot be overwhelmed, and vice-versa, and this happens instantly i.e. responsively*.

This feature, however, makes some basic implementation errors, and appears to have several problematic semantics, particularly with vnodes, responsiveness and choppiness (imposing three arbitrary rates of LOW, HIGH, INFINITE for all unique combination of message recipient (probably really problematic with vnodes, and high RF), updated once every WriteRpcTimeout - assuming the system clock doesn’t get updated by e.g. NTP).

The only behaviour missing from internode is the ability to notify clients of back pressure, either by propagating to the client connection or by throwing overloaded exceptions.  However this is also implemented poorly here, “applying backpressure” by consuming a {{RequestPoolExecutor}} thread until permitted to proceed.  Thanks to CASSANDRA-15013 this will only be suboptimal, but prior to 4.0 this would have lead to really problematic cluster behaviours.

It’s worth noting that the above was all perhaps a reasonable set of trade-offs when first implemented, though the original ticket lead to a great deal of debate about the reasonableness of the approach (CASSANDRA-9318).  However it also suggests to me we are better removing this unused, unmaintained feature that is no longer particularly needed, and if we have time implementing the version that makes sense in the current context.

(*That all said, 4.0 stability at scale is part of the 4.0 testing plan, and determining reasonable numbers for the limits is a remaining exercise - they are almost certainly too high today to guarantee stability.)
;;;","24/Feb/20 10:00;benedict;For sake of discussion, [here|http://github.com/belliottsmith/cassandra/tree/15375] is a quick 10m patch (I guess I was a slight optimist) to remove this from the codebase;;;","24/Feb/20 20:40;dcapwell;Thanks for the detailed write up!

bq. The only behaviour missing from internode is the ability to notify clients of back pressure

I didn't get that statement since I thought we had it, but Jordan than informed me that they are related just not connected (NodeA -> NodeB has support, coordinator  -> client has support; NodeA -> coordinator -> client does not have support).;;;","24/Feb/20 22:29;benedict;bq. NodeA -> coordinator -> client 

Right, so fortunately we're talking about (I expect) fairly limited scaffolding to link the two systems together, when we get around to it.;;;","03/Mar/20 18:40;rustyrazorblade;I've worked on a few hundred clusters (from maybe 75 teams / companies) and I have never seen this feature used.  I have no objections to removing it.;;;","03/Mar/20 19:01;sbtourist;As the original author of that implementation, I regard it as experimental and never proven to be widely used, so I agree with removing it. Dead code is bad code (regardless of its actual quality or past merits).;;;","03/Mar/20 19:47;benedict;bq. regardless of its actual quality or past merits

FWIW, I'd like to disclaim any desire to discredit the original work - it was both a very different time, and nothing is ever perfect first time.  With maintenance it could no doubt have rapidly become a useful feature, but today it does not make sense.;;;","04/Mar/20 10:41;sbtourist;bq. I'd like to disclaim any desire to discredit the original work

Of course, and as I said it was experimental and meant to evolve once widely used, which never happened. Do you want me to review your patch?;;;","04/Mar/20 10:45;benedict;That would be great, thanks!;;;","27/Mar/20 12:01;sbtourist;[~benedict], apologies for this late review. I've added some comments to your commit here: https://github.com/belliottsmith/cassandra/commit/3584b4305ab87f836fdc94d4d5b28bd102642ccb;;;","12/May/20 18:49;jwest;Sorry to come in late on this when there is already a patch but shouldn't we deprecate {{RateBasedBackPressure}} before removing it? I understand folks here haven't seen it in the wild but that doesn't mean someone isn't using it. ;;;","12/May/20 21:45;benedict;Not everything has to go through a deprecation process, no.  Since this feature was never advertised by any means, it cannot be taken to be supported.  Since we also don't think it's _useful_, ignoring the property will leave the database approximately as functional as it was before.  Such a user, if they exist, will still have a working database after upgrade.

;;;","12/May/20 22:06;jwest;While I don't totally agree with your bar for what constitutes an official public API, my biggest concern was that since we weren't deprecating it, if there was a user of it, there would be an exception when their configuration was read on startup. Looking closer at your patch, it does look like this has been accounted for by indeed marking the fields {{@Deprecated}} in {{Config}} (as an aside I also agree with the comment that we should note them as such in cassandra.yaml). So while they will take no effect, they won't harm anyone who had it set in their config either. ;;;","12/May/20 22:13;benedict;FWIW, we haven't actually always been great about deprecating old parameters in config files.  I'm pretty sure every version has had some config parameter vanish without deprecation (which is not an endorsement of that, of course)

Separately, I hadn't actually realised it was in the yaml, I had been searching for the wrong text string.  That could certainly be taken to be an advertisement of the feature.  I still don't think deprecation is warranted, given the earlier discussions about the feature itself, but my prior position was taken on faulty information.  I'd be happy to defer to somebody with a stronger opinion.;;;","12/May/20 22:27;jwest;I'd be +1 on this if we update cassandra.yaml with a deprecation notice -- continuing with code removal as it is in the patch now. ;;;","17/Jul/20 09:13;benedict;Sorry for dropping the ball on this, the ticket doesn't show up in my normal Jira query (will figure out why later).  I've pushed an update to the branch addressing the review feedback; I think it's ready to commit with a quick +1 (CI is running [here|https://app.circleci.com/pipelines/github/belliottsmith/cassandra?branch=15375]);;;","17/Jul/20 11:32;snazy;+1

super nit: maybe [change the wording here|https://github.com/belliottsmith/cassandra/commit/2ba8b4d162c20142c3d4c7a225432337b7bdbc36#diff-4805e34bd9553ede03778be66ddc06c7R262] to ""removed"" instead of ""deprecated""?;;;","17/Jul/20 15:18;benedict;Thanks, committed with your suggestion;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
validate value sizes in LegacyLayout,CASSANDRA-15373,13264131,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,23/Oct/19 22:38,15/May/20 08:38,13/Jul/23 08:38,06/Nov/19 20:14,3.0.20,3.11.6,4.0,4.0-alpha3,,Legacy/Local Write-Read Paths,,,,0,,,,"In 2.1, all values are serialized as variable length blobs, with a length prefix, followed by the actual value, even with fixed width types like int32. The 3.0 storage engine, on the other hand, omits the length prefix for fixed width types. Since the length of fixed width types are not validated on the 3.0 write path, writing data for a fixed width type from an incorrectly sized byte buffer will over or underflow the space allocated for it, corrupting the remainder of that partition or indexed region from being read. This is not discovered until we attempt to read the corrupted value. This patch updates LegacyLayout to throw a marshal exception if it encounters an unexpected value size for fixed size columns.",,bdeggleston,benedict,ifesdjeen,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15778,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Correctness -> Unrecoverable Corruption / Loss,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Nov 11 14:00:44 UTC 2019,,,,,,,All,,,,,"0|z07w8o:",9223372036854775807,,,,,,,benedict,samt,,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/53f604dc1789a800dbcbc3c8aee77f8f36b8b5db,,,,,,,,,circleci,,,,,"23/Oct/19 22:43;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/15373-3.0]|[tests|https://circleci.com/gh/bdeggleston/cassandra/tree/15373-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/15373-3.11]|[tests|https://circleci.com/gh/bdeggleston/cassandra/tree/15373-3.11]|;;;","24/Oct/19 10:32;benedict;Patch looks good, only suggestion is that we should consider also performing this check for RTs in {{decodeBound}}, and statics in {{extractStaticColumns}};;;","24/Oct/19 16:22;bdeggleston;thanks, added checks to both methods;;;","25/Oct/19 14:36;samt;LGTM, with 2 minor comments:

* In {{decodeBound}}, shouldn't we also check in the non-compound case? 
* {{LegacyCellName(Clustering clustering, ColumnDefinition column, ByteBuffer collectionElement)}} is only called from {{LegacyCellName create(Clustering clustering, ColumnDefinition column)}} so you could just construct the {{LegacyCellName}} directly there.

;;;","25/Oct/19 17:29;bdeggleston;thanks, pushed up updated branches;;;","28/Oct/19 13:50;benedict;I think there's one remaining missing check: in {{decodeCellName}} we need to verify that the {{collectionElement}} is valid, as it can be a fixed width type.;;;","28/Oct/19 15:58;bdeggleston;good catch, fixed;;;","28/Oct/19 16:00;benedict;+1 from me;;;","28/Oct/19 16:47;samt;+1 from me too - you just need to fix the newest tests on the 3.11 branch (e.g. {{s/new Clustering/Clustering.make/}});;;","28/Oct/19 18:12;bdeggleston;fixed, also made a small adjustment to unbreak a test for CASSANDRA-14912 (https://circleci.com/gh/bdeggleston/cassandra/2537#tests/containers/94);;;","06/Nov/19 20:14;bdeggleston;thanks, committed as [53f604dc1789a800dbcbc3c8aee77f8f36b8b5db|https://github.com/apache/cassandra/commit/53f604dc1789a800dbcbc3c8aee77f8f36b8b5db];;;","11/Nov/19 08:14;ifesdjeen;(Big thanks to [~marcuse] for noticing I initially commented on the wrong jira)


I've noticed that the patch uses {{validateIfFixedSize}}. Wanted to let you know that {{validateIfFixedSize}} is not implemented for {{ByteType}} and {{ShortType}} even though they're fixed size. 
;;;","11/Nov/19 14:00;benedict;So, while it might make sense to {{validateIfFixedSize}} on these fields, in fact they are not ""fixed size"" according to the storage layer.  They clearly _are_ fixed size, but somebody forgot to mark them as such, so that for purposes of this patch the validation isn't important.

But you're right that we might as well validate them too, and we could follow up with a simple modification.

_If we do_ then, while we're there, I also notice we have {{valueLengthIfFixed}} and {{validateIfFixedSize}} and we should probably standardise on either ""fixed size"" or ""fixed length"" rather than mixing the two (sorry, should have spotted that during review first time around);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect messaging service version breaks in-JVM upgrade tests on trunk,CASSANDRA-15371,13263839,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jmeredithco,jmeredithco,jmeredithco,22/Oct/19 16:55,27/Aug/20 15:09,13/Jul/23 08:38,28/Oct/19 18:20,2.2.16,3.0.20,3.11.6,4.0,4.0-alpha3,Test/dtest/java,,,,0,,,,"The in-JVM upgrade tests on trunk currently fail because the messaging
 version for internode messaging is selected as {{MessagingService.current_version}},
 a regression from the implementation in CASSANDRA-15078.",,djoshi,headrush,jmeredithco,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15381,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jmeredithco,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Oct 28 18:20:38 UTC 2019,,,,,,,All,,,,,"0|z07ufs:",9223372036854775807,,,,,,,djoshi,,,,Low,,2.2.x,,,https://github.com/apache/cassandra/commit/c009a7b9d4e3c32e88238a7ba8f67b14d287c5d3,,,,,,,,,This is a fix for a test failure,,,,,"22/Oct/19 17:34;jmeredithco;This fixes upgrade test for me when run on 3.0, 3.11 and trunk.
 * [2.2 changes|https://github.com/jonmeredith/cassandra/tree/CASSANDRA-15371-2.2] [CircleCI|https://circleci.com/workflow-run/d7a1f47d-a411-4245-9924-422e8ac6f8c7]
 * [3.0 changes|https://github.com/jonmeredith/cassandra/tree/CASSANDRA-15371-3.0] [CircleCI|https://circleci.com/workflow-run/54d2ae1f-3599-426b-b0ab-6ef54edbddab]
 * [3.11 changes|https://github.com/jonmeredith/cassandra/tree/CASSANDRA-15371-3.11] [CircleCI|https://circleci.com/workflow-run/25df7f33-58a6-40cd-b64c-235515b8ad2d]
 * [trunk changes|https://github.com/jonmeredith/cassandra/tree/CASSANDRA-15371-trunk] [CircleCI|https://circleci.com/workflow-run/577036b9-772b-4f06-8db1-3af8e0c0b049]

Currently the upgrade tests need to be run manually.
 * Checkout 2.2, 3.0, 3.11 and trunk and run `ant dtest-jar` to create the dtest jars in the build directory.
 * Copy/symlink all previous version dtest jars into the build directory ({{cd build; ln -s ../../path_to_3.11>/build/dtest-3.11.5.jar}})
 * Run the upgrade test (either by hand, or from IntelliJ)
 {{ant test-jvm-dtest-some -Dtest.name=org.apache.cassandra.distributed.upgrade.UpgradeTest -Dtest.methods=upgradeTest}};;;","28/Oct/19 18:19;djoshi;+1;;;","28/Oct/19 18:20;djoshi;Thanks for the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fake row deletions and range tombstones, causing digest mismatch and sstable growth",CASSANDRA-15369,13263784,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,benedict,benedict,22/Oct/19 11:37,07/Dec/20 18:53,13/Jul/23 08:38,02/Nov/20 18:29,4.0,4.0-beta4,,,,Consistency/Coordination,Local/Memtable,Local/SSTable,,0,,,,"As assessed in CASSANDRA-15363, we generate fake row deletions and fake tombstone markers under various circumstances:
 * If we perform a clustering key query (or select a compact column):
 * Serving from a {{Memtable}}, we will generate fake row deletions
 * Serving from an sstable, we will generate fake row tombstone markers


 * If we perform a slice query, we will generate only fake row tombstone markers for any range tombstone that begins or ends outside of the limit of the requested slice
 * If we perform a multi-slice or IN query, this will occur for each slice/clustering

Unfortunately, these different behaviours can lead to very different data stored in sstables until a full repair is run.  When we read-repair, we only send these fake deletions or range tombstones.  A fake row deletion, clustering RT and slice RT, each produces a different digest.  So for each single point lookup we can produce a digest mismatch twice, and until a full repair is run we can encounter an unlimited number of digest mismatches across different overlapping queries.

Relatedly, this seems a more problematic variant of our atomicity failures caused by our monotonic reads, since RTs can have an atomic effect across (up to) the entire partition, whereas the propagation may happen on an arbitrarily small portion.  If the RT exists on only one node, this could plausibly lead to fairly problematic scenario if that node fails before the range can be repaired. 

At the very least, this behaviour can lead to an almost unlimited amount of extraneous data being stored until the range is repaired and compaction happens to overwrite the sub-range RTs and row deletions.",,aleksey,benedict,jasonstack,jeromatron,maedhroz,marcuse,pauloricardomg,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15640,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,Correctness -> Consistency,,,,,,,,Challenging,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Nov 02 18:29:23 UTC 2020,,,,,,,All,,,,,"0|z07u3k:",9223372036854775807,,,,,,,adelapena,marcuse,,,Normal,,3.0.0,,,trunk: https://github.com/apache/cassandra/pull/473,,,,,,,,,"Added unit tests to read RT with named queries and slice queries.

[CIrcle CI|https://app.circleci.com/pipelines/github/jasonstack/cassandra/328/workflows/5bdb12cd-6c92-4d06-aed2-445f75e2b8f5] [asf-ci|https://ci-cassandra.apache.org/view/all/job/Cassandra-devbranch-dtest/141/]running",,,,,"24/Oct/19 15:20;benedict;Relatedly, CASSANDRA-11072 (which has a very stale patch from nearly 4 years ago), which looks to remove some of the special casing around {{ClusteringIndexNamesFilter}} and {{searchIterator}}, alongside other improvements.  Doubt we'll want to reuse it, but worth mentioning here.;;;","24/Oct/19 15:21;benedict;I think this problem will need to be addressed incrementally, initially addressing only the differing ways we create fake deletions, followed by efforts to minimise the slicing of RTs.  This latter change will be nigh impossible with the current storage format, so might need to be revisited later alongside improvements there.;;;","09/Mar/20 16:09;jasonstack;bq.  initially addressing only the differing ways we create fake deletions

do you mean by unifying the tombstone creation from memtable/ sstable/slice-query to only range tombstone markers?;;;","09/Mar/20 16:20;benedict;I think _probably_ it is preferable to generate fake row deletions where possible, since their semantics are much better than range tombstones.  If the user is lucky, they might never see a range tombstone.

Since it's anyway impossible today to deal with range tombstones, we need a separate effort there, and so it's probably reasonable to leave unsolved for now the cases that _require_ fake RTs.  We will either need to guarantee RTs are replicated as inserted (without any subdivisions we currently produce) or that they are only accounted for in digest via non-RT data (since otherwise there seems no possible way to ensure a consistent digest for a response).  Either way, it's probably better to do our best to avoid the scenario altogether, and use row deletions wherever possible.;;;","16/Mar/20 12:15;jasonstack;Changes in [trunk patch|https://github.com/apache/cassandra/pull/473/files]:

1. Return row range tombstone (aka, covering 1 row) instead row deletion when doing single partition named query(aka. with full clustering keys) on a range tombstone in memtable.
 * The reason I chose row RT instead row deletion is that I am trying to make named query consistent with slice query where fake RT are created for slice clustering bound.
 * For example, in a table with pk, ck1, ck2, ck3, there is existing deletion pk=1 & ck=1 with timestamp 10. After the patch:
 ** query with pk=1 should return the RT as it is, as query covers origin RT.
 ** query with pk=1 & ck1=1 should return the RT as it is, as query covers origin RT.
 ** query with pk=1 & ck1=1 & ck2=1 should return fake RT with ClusteringBound ck1=1 & ck2=1.
 ** query with pk=1 & ck1=1 & ck2=1 & ck3=1 should return fake RT with ClusteringBound ck1=1 & ck2=1 & ck3=1. 

2. When partition deletion timestamp ties with range tombstone or row deletion, it will remove RT or row deletion when creating responses.
 * Prior to the patch, partition deletion will only remove row deletion with same timestamp during compaction, via {{Row.Merger.merge()}}

During testing, I found another issues related to SPRC skipping older sstables causing digest mismatch: CASSANDRA-15640

 

[~benedict] WDYT?;;;","30/May/20 09:25;jasonstack;I have addressed reviewed and rebased to trunk.;;;","08/Jun/20 07:09;jasonstack;[~marcuse] do you mind having a look? thanks;;;","03/Oct/20 03:58;jasonstack;[~marcuse] do you still plan to review this ticket? should I find another reviewer? thanks..;;;","05/Oct/20 10:52;marcuse;sorry, I'll try to get to this soon;;;","14/Oct/20 07:20;marcuse;looks good in general, two concerns;
* performance of {{SinglePartitionReadCommand#reduceFilter}} is much worse now (a silly laptop local benchmark shows queries being 15% slower) - the reason seems to be that we use {{try (UnfilteredRowIterator iterator = result.unfilteredIterator(columnFilter(), filter.getSlices(metadata()), false))}} - I think we can just replace that with {{try (UnfilteredRowIterator iterator = result.unfilteredIterator(columnFilter(), clusterings, false))}}?
* {{AbstractBTreePartition#getRow}} - this looks like it is missing the fix from CASSANDRA-15363 - the {{row == null}} case should probably be
{code}
                    // this means our partition level deletion superseedes all other deletions and we don't have to keep the row deletions
                    if (activeDeletion == partitionDeletion)
                        return null;
                    // no need to check activeDeletion.isLive here - if anything superseedes the partitionDeletion
                    // it must be non-live
                    return BTreeRow.emptyDeletedRow(clustering, Row.Deletion.regular(activeDeletion));
{code};;;","18/Oct/20 16:42;jasonstack;[~marcuse]thanks for the feedback, I have updated the branch;;;","19/Oct/20 11:36;marcuse;Just one more question;

Before, in [searchIterator()|https://github.com/apache/cassandra/blob/ccab496d2d37c86341d364dea6c27513fda27331/src/java/org/apache/cassandra/db/partitions/AbstractBTreePartition.java#L139] we returned EMPTY_STATIC_ROW, [now|https://github.com/apache/cassandra/pull/473/files#diff-6e27ca8fc225036969f774910f2142568fcf85ab588a100c5d3484ac412048f3R121] we return null, why is that? Don't think it makes a difference but would probably be good with a comment why we return null.;;;","19/Oct/20 11:39;marcuse;btw, are we targetting 3.0+ here? I'd probably prefer this just going to 4.0 - otherwise I'd like to see patches for 3.0/3.11 as well;;;","25/Oct/20 10:39;jasonstack;bq. Before, in searchIterator() we returned EMPTY_STATIC_ROW

right. but in {{getRow()}}, it return {{null}} if it's empty clustering row. I have added the comments back.

bq. btw, are we targetting 3.0+ here? I'd probably prefer this just going to 4.0

+1 to 4.0 only.;;;","29/Oct/20 10:21;marcuse;+1;;;","02/Nov/20 18:29;jasonstack;thanks for the review. committted to trunk: [commit|https://github.com/apache/cassandra/commit/56e697dc124e9d94581052cca1ba97ad9b1044c5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memtable memory allocations may deadlock,CASSANDRA-15367,13263782,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,22/Oct/19 11:32,15/May/20 08:41,13/Jul/23 08:38,08/Apr/20 00:03,3.0.21,3.11.7,4.0,4.0-alpha4,,Local/Commit Log,Local/Memtable,,,0,,,,"* Under heavy contention, we guard modifications to a partition with a mutex, for the lifetime of the memtable.
* Memtables block for the completion of all {{OpOrder.Group}} started before their flush began
* Memtables permit operations from this cohort to fall-through to the following Memtable, in order to guarantee a precise commitLogUpperBound
* Memtable memory limits may be lifted for operations in the first cohort, since they block flush (and hence block future memory allocation)

With very unfortunate scheduling
* A contended partition may rapidly escalate to a mutex
* The system may reach memory limits that prevent allocations for the new Memtable’s cohort (C2) 
* An operation from C2 may hold the mutex when this occurs
* Operations from a prior Memtable’s cohort (C1), for a contended partition, may fall-through to the next Memtable
* The operations from C1 may execute after the above is encountered by those from C2
",,baylanger,bdeggleston,benedict,jasonstack,onmstester,rtib,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Availability -> Process Crash,,,,,,,,Challenging,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Mar 04 11:45:04 UTC 2020,,,,,,,All,,,,,"0|z07u34:",9223372036854775807,,,,,,,bdeggleston,benedict,,,Critical,,2.1 beta1,,,"[e3f54d4a0c3403141db24f86714c3900eb9f212e|https://github.com/apache/cassandra/commit/e3f54d4a0c3403141db24f86714c3900eb9f212e]",,,,,,,,,unit test included,,,,,"08/Nov/19 14:48;benedict;patches available:
||Branch||Tests||
|[3.0|https://github.com/belliottsmith/cassandra/tree/15367-3.0]|[circleci|https://circleci.com/workflow-run/7bab1194-44d7-49e4-bb04-6fe809657065]|
|[3.11|https://github.com/belliottsmith/cassandra/tree/15367-3.11]|[circleci|https://circleci.com/workflow-run/85e153ff-fda0-4d12-929a-b56ffa67ba16]|
|[4.0|https://github.com/belliottsmith/cassandra/tree/15367-4.0]|[circleci|https://circleci.com/workflow-run/3bda14eb-76f0-45c3-99c4-4abe2ea9c34a]|
;;;","15/Jan/20 23:14;bdeggleston;I've been trying to work out exactly how this deadlock can occur, based on your description. Could the deadlock be restated like this?

 
 For a given partition key:
 * a write is part of an OpGroup before a barrier set on Memtable1 (M1), but with a replay position after the final replay position set on M1 before it flushes.
 * So it’s forwarded to M2, while still blocking flushes on M1
 * M2 has another in flight write for this partition, it’s contended, so it’s holding the lock
 ** It can’t progress because it can’t allocate memory (in part because M1 can’t flush)
 ** It doesn’t degrade to allocating on heap it’s oporder isn’t blocking anything.
 * The write stage becomes saturated with deadlocked writes like these, no more writes

 

 ;;;","15/Jan/20 23:30;benedict;Correct, except perhaps the last part.  There's no need to collect more than one of these deadlocks to bring down the node.  If there are no memtable flushes already in progress, then no more flushes will ever occur, because they must wait for all earlier operations to complete, including the deadlock.  So from this point on no Memtable memory will ever be released.;;;","16/Jan/20 19:25;bdeggleston;What if AtomicBTreePartition never attempted to acquire a lock if was applying an update that had fallen through from a previous memtable? That would prevent the deadlock, and wouldn’t alter memtable behavior in the common case. 

I suppose this could create a problem if a bunch of large contending writes overflowed. I’m not sure if this would actually be an issue in practice, but I suppose you could synchronize on something other than the partition, like the allocator or something, to prevent that.

I have an rough sketch of how this might work [here|https://github.com/bdeggleston/cassandra/commit/da6e709d6c5792de71e4686c6578110f48e6dd06];;;","16/Jan/20 19:36;benedict;You're right.  It's structurally ugly, but it would do the job.

I'm not particularly keen on this as a long term solution, as it's an extra layer of obfuscation around behaviour, and I'm not sure how to better materialise to a reader that this property is being set, or why.  But it's a lot simpler than all of the approaches I've taken so far, and probably has a minimal impact on the performance characteristics of the system.;;;","16/Jan/20 19:59;benedict;For comparison, [this patch|https://github.com/belliottsmith/cassandra/tree/15367-a2] addresses this ticket by ensuring allocations only happen whilst the lock is not held.  It aims to reduce the necessity of locking, not just for this use case, without removing it altogether (but laying the groundwork for removing it). 

* So that the fast path is unaffected, we perform our first attempt to insert as normal
Unlike before, we disable {{abortEarly}} for this first attempt, so that we always construct a complete new tree
** If we fail, we walk this new tree, looking for any remnants of the insert
** These remnants are collected into a new insert containing only the parts that were retained after resolving
** This new insert contains only Memtable-allocated data, so we do not need to copy anything next attempt
* Future attempts to insert operate on this minimal copied version of the data, this preventing the worst case scenario the lock was introduced for, namely Memtable exhaustion
* However, to minimise any performance regression, we retain the lock and continue to perform the same waste tracking as before
* If locking has been enabled for the partition, step 1 is skipped, and we immediately copy the entire insert into the Memtable before obtaining the lock

The performance impact of this patch is still being comprehensively validated, and the results will be posted in a few days. It is reasonable to expect that there will be some slight performance penalty in some cases, and some improvements in others.
;;;","18/Jan/20 00:09;benedict;[~bdeggleston] do you want to polish your patch at least for 3.0, 3.x, and I guess 4.0 for now?  It's definitely the least invasive approach.  I'll file a separate ticket for moving towards removing the lock entirely, and we can consider if/when we want to address it.  I would prefer 4.0 had a ""better"" (conceptually) solution to the problem, but it's very much up for debate (one that can be had on another ticket);;;","24/Jan/20 12:34;benedict;So, I decided to start writing a version of your approach with slightly more explicit control flow.  However, I realised that this bug is not fixed by this approach, or my original approach.

The issue is that we have all been assuming there is only one table on the system.  In fact, the flushing {{Memtable}} that's waiting for the operation to complete may be in an altogether different table.  It might be that the operation holding the lock and the operation that needs to obtain the lock are both members are the same logical cohort for this {{Memtable}}. 

We _could_ try to introduce a separate {{OpOrder}} per table, but this causes its own issues, since we can have multiple tables in a single operation, each one with its own different blocking behaviour.  I don't want to think about what bugs we might introduce there.

We could explicitly order operations by their {{OpOrder.Group}} when acquiring a lock - if pessimistic locking is required, we wait for all earlier operations to complete before we acquire the lock.  I'm not sure what impact this might have on the system, as this might introduce delays for these operations.

Alternatively, we really do need the follow-up work I've done recently to remove the lock entirely.  This is a significant amount of work, but has no real caveats.;;;","24/Jan/20 13:13;benedict;I have force-pushed a version that takes this simple approach.

As stated, I'm nervous about the impact this might have, though in practice it should _hopefully_ be limited.  I don't see another realistic option besides eliminating the lock entirely, as I had previously hoped to.  It does increase my desire to land that change in 4.0 though.;;;","24/Jan/20 17:33;bdeggleston;Just so I understand, the scenario you’re describing could be described like this?
 * there are 2 tables, T1 & T2, writing against OpGroup 1 (OP[1])
 * a write comes in for T1 and is assigned to op group 1 (W1[1])
 * T2 starts to flush, OpGroup is bumped to OP[2]. T2 is now waiting on W1[1]
 * a write comes in for T1 and is assigned to op group 2 (W2[2])
 * W2[2] acquires a lock, but is unable to allocate memory until T2 flushes
 * W1[1] is blocking the T2 flush, and is unable to acquire the lock
 * deadlock;;;","24/Jan/20 17:34;bdeggleston;also, is [this|https://github.com/belliottsmith/cassandra/tree/15367-a2] the branch you pushed to? ;;;","24/Jan/20 22:26;benedict;Sorry, I meant [this branch|https://github.com/belliottsmith/cassandra/tree/15367].

I think you're right, except with the following modification (presumably a typo):
* W1[*1*] is blocking the T2 flush, and is unable to acquire the lock
;;;","25/Jan/20 00:41;bdeggleston;Yep, I think that would fix the problem. Another approach that wouldn’t have the potential to introduce delays would be to skip locking if we have (or are about to) set the final replay position on a memtable waiting on an op group. Like setting blocking, but it won’t bypass the allocator in case the flush queue is long. That would fix the deadlock without delaying later writes, although it could increase contention.

Rough example with lazy naming [here|https://github.com/bdeggleston/cassandra/tree/15367-alternative-2]

It would be nice if a write waiting for a lock could unblock itself as soon as it's op group becomes blocking

Random thoughts about longer term fixes:

I didn’t have a chance to get my head around how you’d intended to remove the lock completely, but I don’t understand how that could be done without reintroducing the contention gc problem.

It seems to me that the root cause of all this is that we have 2 mechanisms for ordering events (OpOrder and ReplayPosition) which are mostly independent, but have to interact in non-deterministic ways during memtable flush, which creates these edge cases. I think the right fix (or one of them) is to either merge these two classes, or make one control the other.;;;","25/Jan/20 00:58;benedict;bq. I don’t understand how that could be done without reintroducing the contention gc problem.

Simply by making it fast enough.  CASSANDRA-15511 manages to get the costs significantly lower than today, even with 16-threads actively spinning on a dual-socket 24-core machine.  It manages to compete fairly well even against itself with locking enabled (although the lock variant does manage lower garbage, it isn't dramatic overall).  

The spreadsheet I posted compares a number of possible approaches, settings and workloads.  

Of course, there would still be the potential for some wasted work that could be usefully spent elsewhere, but I think the gains are minimal.  There are also other options available to us for minimising contention besides a lock, that I've broached before (e.g. on failure to update, tag the write onto a linked-list and merge lazily on read, potentially attempting to apply the merge to the tree each time to reduce duplicated work).

bq. either merge these two classes, or make one control the other

It might well be possible to merge the management of these things, it's an interesting idea and something to consider.

bq. Rough example with lazy naming here

Thanks.  I'll have to dig into that next week, failing to reckon with it right now.;;;","25/Jan/20 01:04;benedict;bq. Rough example with lazy naming here

I don't think that patch works, actually, as {{orderOnOrBefore}} is null until {{issue()}} is invoked;;;","25/Jan/20 01:08;bdeggleston;Sorry, it's really just intended as an illustration of the idea in the preceding paragraph.;;;","25/Jan/20 01:13;benedict;Right, but ordering here is pretty essential, as the {{issue}} has to happen first (so that we know what to mark), but as soon as it happens these operations can begin to route to the new {{Memtable}} and bypass our don't-lock check.  But I won't try to make any stronger claims about what can or cannot happen until next week.  I suppose it might be possible to {{markSomething}} in the {{issue}} critical section, but before making {{orderOrOnBefore}} visible.;;;","25/Jan/20 11:06;benedict;So, I think anyway it's unclear to me what the best approach is between these two, as one permits competition amongst operations permitted to grow memtable memory usage unboundedly, and the other potentially reduces parallelism briefly.  The number of parallelism reductions is limited to the number of memtable flushes on the system, which can actually be quite frequent, but probably not frequent enough to matter.  However the number of competing operations to create memory pressure on memtables is also relatively few.  But neither are desirable eventualities, and we cannot predict their true incidence.

I wonder if a mixture of approach would be a good idea:  try to introduce your suggestion of ignoring the lock for operations that could deadlock (by potentially marking during the critical section, which is likely an imperceptible cost given the rate of barrier issue), so that we do not harm parallelism.  But also prevent operations from re-allocating memory into memtable space, as this memory cannot be reclaimed until a (slow) flush occurs, potentially harming node stability when we're past our memtable limit.;;;","29/Jan/20 12:59;benedict;I've pushed a branch that implements your suggested approach.  I wanted to get your feedback on whether or not you think we should try to mitigate the issue of competition in these overflow scenario.

One approach would be to ignore most of the work I did initially wrt {{extractUnshadowed}}, and _only in the event we will refuse to lock_:
# Do not {{abortEarly}} for our first update attempt
# For remaining attempts, apply the whole new partition result to the current head, until we succeed
 
This may mean increased heap allocations for each failed attempt, and each individual update may be slightly slower, but there will be:
# No extra heap allocations for successful attempts
# We can avoid re-cloning values into the memtable space (which is not reclaimable until flush)
# Majority of operations are entirely unaffected, so we don't have to worry much about performance impact 

WDYT?  I'm _relatively_ comfortable doing nothing to mitigate this scenario, too.  But it _might_ be preferable to do this.;;;","30/Jan/20 18:53;bdeggleston;Nice. I’m also fairly comfortable not addressing competition in this scenario. Ideally, we would, but I’m not sure it would be worth adding a secondary synchronization path. Although I guess we could synchronize on the writeOp.

There still is (technically) a brief window for deadlock between {{setCommitLogUpperBound}} and {{writeBarrier.issue()}} in {{org.apache.cassandra.db.ColumnFamilyStore.Flush#Flush}}, but I’m not sure if it’s worth addressing, since we’d need to immediately waste 10MB on a partition as soon as a memtable is created, and it’s not exacerbated by flush queue length. Anyway, I think this qualifies as good enough. I’d also prefer it over waiting on the previous op group because it limits the window of potential bad behavior to a narrower set of circumstances. What do you think?

About removing the lock, I’m sure 15511 will help with contention, and we should commit it, however I think there will still be pathological cases where faster updates won’t be enough. For instance, if there were 20 small updates and one much larger one contending with each other, I can imagine the large one would have a tough time making progress and end up wasting a lot of memory.

<random-idea>
 This might be better illustrated with code, and would be a trunk-only follow on ticket, but instead of synchronizing writes on the partition object whenever there’s contention, what if we queued up contended writes on the partition? If a write comes in and there’s no longer contention, or the size of queued writes is too high, it could merge the updates and synchronize on applying them. By merging the updates, I think we’d end up allocating less memory in the contended case than the uncontended case.
 </random-idea>;;;","30/Jan/20 23:51;benedict;bq.  but I’m not sure if it’s worth addressing

I don't think any deadlock is acceptable to ignore.  Hmm.  If we don't go with one of the other approaches I've suggested, I'll have to find some time in a week to see if there's a variant of this suggested approach that works in this respect.

bq. <random-idea>

I think this is something I have proposed before, but it's not trivial.  I had planned to implement something like this as part of my work addressing this problem, but decided not to given the complexity.  The idea would be to introduce a linked-list of deferred updates, and merge them either on future reads or writes, but ensuring everyone sees a consistent view with this approach, while minimising duplicated work and ensuring progress, is less trivial than I imagined when I proposed it a while ago.

bq. About removing the lock, I’m sure 15511 will help with contention, and we should commit it, however I think there will still be pathological cases where faster updates won’t be enough

We can benchmark this specific scenario, but all we really care about is if the aggregate behaviour for all 21 operations is good enough to warrant removal of the lock, and the commensurate reduction in complexity when reasoning about the system (that has been _amply_ demonstrated by this ticket).  IMO, the performance numbers from 15511 more than cross this threshold, but we can certainly explore further verification work to be certain.;;;","31/Jan/20 16:41;bdeggleston;bq. I don't think any deadlock is acceptable to ignore.

I was assuming this couldn’t be modified to eliminate that window for deadlock. It would be great if it could be. If not, I agree this needs to be fixed eventually, but given the extent that this mitigates the issue, I do think it should be committed, at least as a stop-gap for trunk, but possibly as the fix for 3.x.

bq. lock stuff

maybe we should pick this conversation up when we get back to removing the lock. I’m pretty unfamiliar with the specifics, and it’s probably not very useful for me to be arguing about stuff based on a bunch of assumptions.;;;","13/Feb/20 13:50;benedict;So, I'm looking at this more closely now I have some time, and I wonder if you could outline how you think the deadlock occurs between {{setCommitLogUpperBound}} and {{writeBarrier.issue()}}?  Because the deadlock requires a new cohort to exist, that does not get instantiated until {{writeBarrier.issue()}} so the deadlock cannot occur until then?

However there _is_ a window _after_ {{!writeOp.isBehindBarrier()}}, which cannot be avoided because there are no timed wait mechanisms for obtaining a monitor, and {{tryMonitorEnter}} anyway isn't possible in later versions of Java.

So, I propose a variant of my earlier approach that definitely worked, that waited for all earlier operations to complete, to instead essentially invert the behaviour of your suggestion: if there are any running older operations, refuse to lock until they all complete (and invoke {{Thread.yield()}} once to give them an opportunity with the CPU).  So locking is essentially disabled for all newer operations until the older ones expire, and we try to give them dibs on the CPU if the scheduler lets us, so that this window is as narrow as possible.;;;","13/Feb/20 14:14;benedict;I've pushed an update that does this, but I need to look at it again tomorrow to be 100% certain everything is fine.;;;","18/Feb/20 21:15;bdeggleston;{quote}how you think the deadlock occurs
{quote}
It may be the same one you're referring to. Basically if a write blocks trying to acquire the lock on the new memtable after the final commit log position is set and before the write barrier is issued, there's a risk of deadlock.

Given op groups O1 & O2, replay positions R1 & R2, and memtables M1 & M2. M1 is flushing with a barrier on O1, and final commit log upper bound R1 is set on it. A new write (W1) is assigned to O1, and writes to the commit log at position R2. It will overflow to M2, the new memtable. Before the barrier is set on it, it blocks on a locked partition. Write W2 against O2 then acquires the lock ahead of W1, blocks on the allocator, and we deadlock.
{quote}So, I propose a variant of my earlier approach that definitely worked
{quote}
Great, as far as I can tell, this fixes the deadlock 100% and with minimal risk of adding new issues or disrupting system behavior. +1;;;","04/Mar/20 11:45;benedict;Sorry for the delay on this, I've been trying to figure out what's wrong with CircleCI for 3.0 and 3.11, but have given up.  It appears the issues occur on HEAD for both as well.

patch:[3.0|https://github.com/belliottsmith/cassandra/tree/15367-3.0] ci:[3.0|https://circleci.com/workflow-run/b6396d79-d2bb-42a1-a76f-805715e896de] head ci:[3.0|https://circleci.com/workflow-run/759d8331-7e2f-431f-8161-847fe46af4d3]
patch:[3.11|https://github.com/belliottsmith/cassandra/tree/15367-311] ci:[3.11|https://circleci.com/workflow-run/7c74fea5-3730-44bf-bd62-5d112cf974d4] head ci:[3.11|https://circleci.com/workflow-run/72a173e0-713e-479d-b5db-9fd7e94eb3d6]
patch:[trunk|https://github.com/belliottsmith/cassandra/tree/15367-trunk] ci:[trunk|https://circleci.com/workflow-run/b2b24c56-d191-4923-9f66-b05036d62ea1]

I will commit tomorrow after double checking the test failures are consistent and assuming you don't say otherwise.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add primary key liveness info when skipping illegal cells,CASSANDRA-15365,13263092,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,18/Oct/19 12:18,30/Oct/19 14:18,13/Jul/23 08:38,30/Oct/19 14:18,3.0.20,3.11.6,,,,Local/SSTable,,,,0,,,,"In CASSANDRA-15086/CASSANDRA-15178 we started skipping the illegal legacy cells, problem is that if the row only contains illegal cells, we return a totally empty row which breaks stats collection: https://github.com/apache/cassandra/blob/93815db9853cb592edf13d82e91dc2e9d172f01f/src/java/org/apache/cassandra/db/rows/Rows.java#L70

If the row only has these invalid cells, we should add a primary key liveness info to it to match the 2.1 behaviour.",,benedict,jeromatron,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,samt,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Normal,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Oct 30 14:18:43 UTC 2019,,,,,,,All,,,,,"0|z07qaw:",9223372036854775807,,,,,,,benedict,samt,,,Low,,3.0 alpha 1,,,https://github.com/apache/cassandra/commit/767a68cd00050298abf7bbfd8b322e5663439c23,,,,,,,,,new LegacySSTableTest added,,,,,"18/Oct/19 12:22;marcuse;[patch|https://github.com/krummas/cassandra/commits/marcuse/15365] which keeps track of whether the row only contains invalid cells, if so, we add the primary key liveness.

[circle|https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15365];;;","22/Oct/19 10:25;samt;LGTM (just needs the JIRA # adding to the comment at {{LegacyLayout#1294}});;;","24/Oct/19 14:21;benedict;I've pushed a branch [here|https://github.com/belliottsmith/cassandra/tree/15365-suggest] with a suggested comment.

Patch LGTM, however there's a remaining edge case that I don't know if we care to address or not.  Specifically, the case where we _currently_ have data in the row but _will not_ in future, but where the invalid data would have remained at that future point in time.

For instance, either TTL'd data or tombstones that have not yet been dropped.  If the invalid data is not TTL'd, or is TTL'd for a later date, then we would later stop returning the empty row in our CQL results.

I think we have three options:
# Carry on without worrying about this - it's already unclear what the correct response is to this invalid data after all
# Always update the row liveness info with the invalid cell liveness we collect, if it supersedes any other liveness info
# Ignore tombstone cells (and perhaps TTL'd cells) when deciding if the row is empty before adding our invalid cell liveness info, if it supersedes

I think any of these options is acceptable in this weird edge case, where we are presumably repairing data that has been erroneously written by the user via external tooling.;;;","28/Oct/19 15:54;marcuse;so, in 2.1 these invalid cells act as a kind of permanent row markers, for example:

* {{create table %s (pk int, c1 text, c2 text, v1 text, primary key (pk, c1, c2))}}
* insert an invalid cell, select * now returns: {{3, a, aa, null}}
* do an insert with a ttl for the whole row: {{insert into %s (pk, c1, c2, v1) values (3, 'a', 'aa', 'vaaalue') using ttl 2}}
* select * before ttl: {{3, a, aa, vaaalue}}
* select * after ttl expires: {{3, a, aa, null}}

If the invalid cell didn't exist, the select would have returned nothing

Translating this to 3.0 including this patch, we would translate the whole-row-insert row marker to the PKLI and ignore the invalid cell, and after ttl expires we would return nothing.

We can't really fully translate the 2.1 behaviour to 3.0 using only the PKLI - for example if we in the example above set the PKLI to the timestamp of the invalid cell and later overwrote that row with a ttl row, we would fully purge it after ttl expires, while in the 2.1 case we would still keep the invalid cell

I vote we keep the current patch behaviour, wdyt [~samt] & [~benedict]?
;;;","28/Oct/19 15:58;benedict;WFM;;;","28/Oct/19 16:54;samt;+1 ;;;","30/Oct/19 14:18;marcuse;and committed to 3.0 and merged to 3.11, thanks

test runs: [3.0|https://circleci.com/workflow-run/0c910638-db5f-494b-ba47-cc728a4d2eb6] [3.11|https://circleci.com/workflow-run/da2cd137-ae9e-45bf-9c4d-1b98a006e08a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid over scanning data directories in LogFile.verify(),CASSANDRA-15364,13263081,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,18/Oct/19 11:30,15/May/20 08:39,13/Jul/23 08:38,11/Dec/19 08:11,3.0.20,3.11.6,4.0,4.0-alpha3,,Local/Compaction,,,,0,,,,"We currently list the data directory for every {{REMOVE}} record in the file in {{LogFile.verify()}} - this can get very expensive during startup when we call {{LogTransaction.removeUnfinishedLeftovers()}}. In {{LogRecord.getExistingFiles(Set<String> absoluteFilePaths)}} we also fully parse the file name of the sstables found, here we only need to prefix match.",,cscotta,dcapwell,jwest,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Low Hanging Fruit,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Dec 11 08:11:51 UTC 2019,,,,,,,All,,,,,"0|z07q8g:",9223372036854775807,,,,,,,dcapwell,jwest,,,Low,,3.0 alpha 1,,,https://github.com/apache/cassandra/commit/af963e3a7318aad177bc1b985478d4a598b51d9c,,,,,,,,,circle ci run,,,,,"18/Oct/19 11:53;marcuse;[patch|https://github.com/krummas/cassandra/commits/marcuse/15364] - this lists the files only once and improves performance of getExistingFiles by using a TreeSet containing the file name prefixes. In my silly laptop local benchmark (verifying a LogFile with 2000 remove records in a directory with 2000 sstables), unpatched takes 35s and patched about 150ms.
[circle|https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15364];;;","21/Oct/19 20:35;dcapwell;* As a general comment, almost every case I see where absolutePath.ifPresent is called is known to be a ADD or REMOVE so the lambda version could be replaced with .get() (style comment, feel free to ignore)

 * [https://github.com/apache/cassandra/compare/trunk...krummas:marcuse/15364#diff-d8721a4dd04f4f35b65e7edeb6c883f6R357]
 The makeRecord call above will do a listing (in make(Type, SSTable) method), so this change looks to be because deleteRecordFiles(LogRecord) no longer exists and was replaced by deleteRecordFiles(List<File>); LGTM

 * [https://github.com/apache/cassandra/compare/trunk...krummas:marcuse/15364#diff-d8721a4dd04f4f35b65e7edeb6c883f6R386]
 Is this to handle failure in the middle of the log (while deleting on commit we hard-stop in the middle, so recovery may see missing files)? Seems fine, was just curious if there was a different reason (there was a comment in the code about people copying tx files to different nodes, hence why I wanted to call out).

 * [https://github.com/apache/cassandra/compare/trunk...krummas:marcuse/15364#diff-2e4c884e6aace427bdb6b0a122164ce4R308-R328]
 Took a while to parse this change, so I am curious what was the motivation? Was it just to avoid File::getCanonicalPath? As far as I can tell Descriptor::fromFilenameWithComponent is only string parsing with a call to File::getCanonicalFile.
 While reading this change, what bothered me was the question ""why is the file name a prefix"" and I missed the comment in the javadoc which expresses that expectation (""absoluteFilePaths contains full file parts up to the component name"", ""up to"" being the important part); the code before relied on Descriptor:fromFilename so didn't really care.
The new version looks fine, I just wanted to better understand the motive

Overall, looks good to me; I want to do one more pass and test this, ETA EOD.;;;","22/Oct/19 01:27;dcapwell;FYI I created a quick JMH to show the difference between the TreeSet version and the canonical file version; here are the results (too lazy to run long enough, stoped after iterations became stable... only ran on Mac...)

 

Number of Components: 5

 
{code:java}
# 1 SSTable
* Before : ~30031.749 ops/s
* After  : ~42116.323 ops/s
# 20000
* Before : ~0.328 ops/s
* After  : ~4.523 ops/s
* Before w/abspath : ~3.133 ops/s
{code}
 

 

The last one is when I switched from FileUtils.getCanonicalPath to new File(baseFilename).getAbsolutePath() (though, descriptor still calls getCanonicalFile).

 

LGTM.;;;","22/Oct/19 01:45;dcapwell;Ran locally and ran with a debugger to monitor file listings; this lowers the listing on startup to a handful per table.

+1;;;","22/Oct/19 13:35;marcuse;bq. As a general comment, almost every case I see where absolutePath.ifPresent is called is known to be a ADD or REMOVE
guess I could add asserts that it exists instead, would probably be clearer

bq. Is this to handle failure in the middle of the log (while deleting on commit we hard-stop in the middle, so recovery may see missing files)?
the null check is unnecessary, I'll remove it

bq. Took a while to parse this change, so I am curious what was the motivation?
I was mostly worried that this patch would slow things down in the ""normal case"" (a tiny number of remove records in a directory with many files) by parsing the filename and creating a Descriptor for every file in the directory when we really don't have to.;;;","24/Oct/19 20:54;jwest;I agree with [~dcapwell]'s evaluation that this reduces file listing calls considerably. Some code comments below. None are serious or required, so +1:

In {{LogFile#deleleFilesForRecordOfType}}:
- Unless its truly exceptional and requires throwing, I prefer the filtering approach to getting rid of non-present absolute paths (like the code below). If it is something we should throw for, consider adding a description to the assert or a more descriptive exception.
{code}
records.stream()
       .filter(r -> type.matches(r) && r.absolutePath.isPresent())
       .forEach(r -> absolutePaths.add(r.absolutePath.get()));
{code}
- Regarding the second for loop in this method, could it instead iterate over absolutePaths, which is already filtered (regardless of the suggestion above) for paths matched by type and having the path present in the record? This would save a few calls/branches, and reduce the need for the repeated assert.

In LogFile#verify, these are truly minor nits and personal style, it could be argued they are less efficient but I prefer the more java 8 like approach to:
  - #L160-1: {{records.forEach(r -> r.absolutePath.ifPresent(absolutePaths::add))}}
  - #L164-173: 
{code}
for (LogRecord record : records)
{
    List<File> existingFiles = record.absolutePath.flatMap(k -> Optional.ofNullable(recordFiles.get(k)))
                                                  .orElse(Collections.emptyList());
    LogFile.verifyRecord(record, existingFiles);
}
{code};;;","28/Oct/19 09:25;marcuse;pushed a commit which adds an explanation to the assert and changes the second loop to iterate over the values of the map instead;;;","28/Oct/19 15:57;dcapwell;Assert msg looks good to me; loop assumes the above asserts ran so can safely loop over the files.

+1;;;","11/Dec/19 08:11;marcuse;And committed, thanks

tests: [trunk|https://circleci.com/workflow-run/96079445-bfe1-4a54-a7e3-94ed4d93d72a] [3.11|https://circleci.com/workflow-run/2cebe9b9-9547-4336-b78f-08b71ff2e63d] [3.0|https://circleci.com/workflow-run/3f11f211-6b4d-44f7-bf42-f9da96512853];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read repair in mixed mode between 2.1 and 3.0 on COMPACT STORAGE tables causes unreadable sstables after upgrade,CASSANDRA-15363,13263049,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,18/Oct/19 09:01,15/May/20 08:36,13/Jul/23 08:38,24/Oct/19 09:27,3.0.19,3.11.5,4.0,4.0-alpha2,,Consistency/Coordination,,,,0,,,,"if we have a table like this:

{{CREATE TABLE tbl (pk ascii, b boolean, v blob, PRIMARY KEY (pk)) WITH COMPACT STORAGE}}

with a cluster where node1 is 2.1 and node2 is 3.0 (during upgrade):

* node2 coordinates a delete {{DELETE FROM tbl WHERE pk = 'something'}} which node1 does not get
* node1 coordinates a quorum read {{SELECT * FROM tbl WHERE id = 'something'}} which causes a read repair
* this makes node1 flush an sstable like this:
{code}
[
{""key"": ""something"",
 ""metadata"": {""deletionInfo"": {""markedForDeleteAt"":1571388944364000,""localDeletionTime"":1571388944}},
 ""cells"": [[""b"",""b"",1571388944364000,""t"",1571388944],
           [""v"",""v"",1571388944364000,""t"",1571388944]]}
]
{code}
(It has range tombstones which are covered by the partition deletion)

Then, when we upgrade this node to 3.0 and try to read or run upgradesstables, we get this:
{code}
ERROR [node1_CompactionExecutor:1] node1 2019-10-18 10:44:11,325 DebuggableThreadPoolExecutor.java:242 - Error in ThreadPoolExecutor
java.lang.UnsupportedOperationException: null
	at org.apache.cassandra.db.LegacyLayout.extractStaticColumns(LegacyLayout.java:779) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.io.sstable.SSTableSimpleIterator$OldFormatIterator.readStaticRow(SSTableSimpleIterator.java:120) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:57) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.io.sstable.format.big.BigTableScanner$KeyScanningIterator$1.initializeIterator(BigTableScanner.java:362) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.maybeInit(LazilyInitializedUnfilteredRowIterator.java:48) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.isReverseOrder(LazilyInitializedUnfilteredRowIterator.java:65) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1.reduce(UnfilteredPartitionIterators.java:103) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$1.reduce(UnfilteredPartitionIterators.java:94) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:442) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$2.hasNext(UnfilteredPartitionIterators.java:144) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:92) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.CompactionIterator.hasNext(CompactionIterator.java:227) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:190) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:89) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.db.compaction.CompactionManager$8.runMayThrow(CompactionManager.java:675) ~[dtest-3.0.19.jar:na]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[dtest-3.0.19.jar:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_121]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_121]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_121]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:83) [dtest-3.0.19.jar:na]
	at java.lang.Thread.run(Thread.java:745) ~[na:1.8.0_121]
{code}",,aleksey,benedict,jasonstack,jeromatron,marcuse,mshuler,sumanth.pasupuleti,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Normal,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Oct 24 09:27:05 UTC 2019,,,,,,,All,,,,,"0|z07q1c:",9223372036854775807,,,,,,,aleksey,benedict,,,Critical,,3.0.0,,,https://github.com/apache/cassandra/commit/9081d0088119f02c8118598ac9fdb8ce39f0033b,,,,,,,,,adds an in-jvm upgrade dtest,,,,,"18/Oct/19 09:14;marcuse;note that the broken sstables are fixable by transferring them to a 2.1 node, running upgradesstables, and transferring them back and replacing the broken sstables;;;","18/Oct/19 09:29;marcuse;When reading from the memtable we create a SearchIterator to only grab the columns requested. If there is a range tombstone covering any of these columns, we create a fake row deletion which covers only that column: [AbstractBTreePartition|https://github.com/apache/cassandra/blob/93815db9853cb592edf13d82e91dc2e9d172f01f/src/java/org/apache/cassandra/db/partitions/AbstractBTreePartition.java#L158]. problem is that if there is an active partition deletion, we also create a row deletion with the same timestamps as the partition deletion. We should not create these row tombstones if the partition deletion supersedes them.

Patch [here|https://github.com/krummas/cassandra/commits/marcuse/15363] (with dtest changes needed in 2.2 [here|https://github.com/krummas/cassandra/commits/marcuse/15363-2.2]

[circleci|https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15363];;;","18/Oct/19 12:38;benedict;To expand upon this for those readers who aren't quite sure what is happening:

* These fake row tombstones are generated when serving point or {{IN}} clause queries, instead of RT bounds for each slice for sstable reads
* For compact storage tables without a clustering key, {{SELECT}} on a subset of columns gets translated into an {{IN}} clause, since cells are stored as rows; this causes us to generate a ""row"" deletion for each selected column in a partition, whether or not there was any data for it
* These may then be read-repaired to other nodes _as_ row range tombstones, through {{LegacyLayout.fromRow}} translation, which make no sense for compact storage tables
* After upgrade the 3.0 nodes attempt to read the data that was written to them during read-repair as a 2.1 node, and {{LegacyLayout}} complains that these range tombstones occurring within a static row make no sense (except for really weird and implausible thrift use cases we've never encountered, so never implemented)

The fix resolves this by ensuring this can never occur in the ""static"" compact storage case, which simply means a compact storage table with no clustering key.  This works because we cannot have a range tombstone deletion of these rows; they can only be deleted through cell deletes (via setting the column to null) or by partition-level deletes.  So we can always expect that {{activeDeletion == partitionLevelDeletion}} in this case.  Since it is anyway unnecessary to generate these tombstones in this case, we get both a minor optimisation for all use cases, and avoid this problem.;;;","18/Oct/19 13:12;benedict;The patch LGTM.  I've done my best to guarantee we cannot erroneously produce a row marker in 3.0:

[here|https://github.com/apache/cassandra/blob/665f74740ce48bf476a9941793d44d9c588eca3c/src/java/org/apache/cassandra/db/RowUpdateBuilder.java#L89] we guarantee only to produce row markers for CQL3 tables, which is defined as excluding static compact tables [here|https://github.com/apache/cassandra/blob/fc862e207b04ed92f15b2129ae7738186ebc6d69/src/java/org/apache/cassandra/config/CFMetaData.java#L1172]

However, in 2.1 we _seem_ to be able to [here|https://github.com/apache/cassandra/blob/64d8a1d9fdacf3e7396cdf2f5c61171c1c9bccf2/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java#L940], and nowhere else.  It seems to be an oversight that we do so here, as all other commentary and locations rule out doing so.  Since this is only on _select_, I don't think any problematic scenario can arise that wouldn't already be covered by the patch.

Marcus has also experimentally tried to produce such a row marker in a mixed-mode cluster and failed to do so.  So I think we're OK in that respect.

However, I do wonder if it would be worth being ultra paranoid and modifying {{LegacyLayout.fromRow}} to simply never convert any row deletion into a range tombstone for compact tables, since they make no sense - a compact table row deletion is equivalent to a cell deletion.;;;","22/Oct/19 11:36;aleksey;LGTM, +1;;;","24/Oct/19 09:27;marcuse;thanks, committed

test runs:
[3.0|https://circleci.com/workflow-run/52f37962-3d24-4dc8-a534-615a33f3b96f]
[3.11|https://circleci.com/workflow-run/38e7098e-7ae7-4781-bbf0-f2fb886e84a6] - the failing tests look unrelated and pass locally (except for the thrift hsha one)
[trunk|https://circleci.com/workflow-run/a7b9f31b-ee4d-44c2-85a9-9eb40fac390f] - DatabaseDescriptorRefTest failure is tracked in CASSANDRA-15357 and the failing dtests pass locally, except for `rebuild_test.py::TestRebuild::test_resumable_rebuild` which also fails on trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LARGE_MESSAGE connection allocates heap buffer when BufferPool exhausted,CASSANDRA-15358,13262655,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,SRAM85,SRAM85,16/Oct/19 15:39,15/May/20 08:54,13/Jul/23 08:38,10/Apr/20 00:32,4.0,4.0-alpha4,,,,Test/benchmark,,,,0,4.0,alpha,,"Hitting a bug with cassandra 4 alpha version. The same bug is repeated with difefrent version of Java(8,11 &12) [~benedict]

 

Stack trace:
{code:java}
INFO [main] 2019-10-11 16:07:12,024 Server.java:164 - Starting listening for CQL clients on /1.3.0.6:9042 (unencrypted)...
WARN [OptionalTasks:1] 2019-10-11 16:07:13,961 CassandraRoleManager.java:343 - CassandraRoleManager skipped default role setup: some nodes were not ready
INFO [OptionalTasks:1] 2019-10-11 16:07:13,961 CassandraRoleManager.java:369 - Setup task failed with error, rescheduling
WARN [Messaging-EventLoop-3-2] 2019-10-11 16:07:22,038 NoSpamLogger.java:94 - 10.3x.4x.5x:7000->1.3.0.5:7000-LARGE_MESSAGES-[no-channel] dropping message of type PING_REQ whose timeout expired before reaching the network
WARN [OptionalTasks:1] 2019-10-11 16:07:23,963 CassandraRoleManager.java:343 - CassandraRoleManager skipped default role setup: some nodes were not ready
INFO [OptionalTasks:1] 2019-10-11 16:07:23,963 CassandraRoleManager.java:369 - Setup task failed with error, rescheduling
INFO [Messaging-EventLoop-3-6] 2019-10-11 16:07:32,759 NoSpamLogger.java:91 - 10.3x.4x.5x:7000->1.3.0.2:7000-URGENT_MESSAGES-[no-channel] failed to connect
io.netty.channel.AbstractChannel$AnnotatedConnectException: finishConnect(..) failed: Connection refused: /1.3.0.2:7000
Caused by: java.net.ConnectException: finishConnect(..) failed: Connection refused
at io.netty.channel.unix.Errors.throwConnectException(Errors.java:124)
at io.netty.channel.unix.Socket.finishConnect(Socket.java:243)
at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.doFinishConnect(AbstractEpollChannel.java:667)
at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.finishConnect(AbstractEpollChannel.java:644)
at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe.epollOutReady(AbstractEpollChannel.java:524)
at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:414)
at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:326)
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:834)

WARN [Messaging-EventLoop-3-3] 2019-10-11 16:11:32,639 NoSpamLogger.java:94 - 1.3.4.6:7000->1.3.4.5:7000-URGENT_MESSAGES-[no-channel] dropping message of type GOSSIP_DIGEST_SYN whose timeout expired before reaching the network
INFO [Messaging-EventLoop-3-18] 2019-10-11 16:11:33,077 NoSpamLogger.java:91 - 1.3.4.5:7000->1.3.4.4:7000-URGENT_MESSAGES-[no-channel] failed to connect
 
ERROR [Messaging-EventLoop-3-11] 2019-10-10 01:34:34,407 InboundMessageHandler.java:657 - 1.3.4.5:7000->1.3.4.8:7000-LARGE_MESSAGES-0b7d09cd unexpected exception caught while processing inbound messages; terminating connection
java.lang.IllegalArgumentException: initialBuffer is not a direct buffer.
at io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:87)
at io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:59)
at org.apache.cassandra.net.BufferPoolAllocator$Wrapped.<init>(BufferPoolAllocator.java:95)
at org.apache.cassandra.net.BufferPoolAllocator.newDirectBuffer(BufferPoolAllocator.java:56)
at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
at io.netty.channel.unix.PreferredDirectByteBufAllocator.ioBuffer(PreferredDirectByteBufAllocator.java:53)
at io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114)
at io.netty.channel.epoll.EpollRecvByteAllocatorHandle.allocate(EpollRecvByteAllocatorHandle.java:75)
at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:777)
at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:424)
at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:326)
at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
at java.base/java.lang.Thread.run(Thread.java:835)
{code}",,aleksey,benedict,csplinter,dcapwell,e.dimitrova,G-Ashok,jasonstack,jeromatron,jmckenzie,mck,mshuler,progval,sbtourist,sermandurai,SRAM85,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/19 18:18;progval;all_errors.txt;https://issues.apache.org/jira/secure/attachment/12985099/all_errors.txt","23/Jan/20 07:48;G-Ashok;debug_logs_during_repair.txt;https://issues.apache.org/jira/secure/attachment/12991613/debug_logs_during_repair.txt","23/Jan/20 07:46;G-Ashok;repair_1_trace.txt;https://issues.apache.org/jira/secure/attachment/12991611/repair_1_trace.txt","06/Nov/19 18:14;progval;verbose_logs.diff;https://issues.apache.org/jira/secure/attachment/12985097/verbose_logs.diff","06/Nov/19 18:14;progval;verbose_logs.txt;https://issues.apache.org/jira/secure/attachment/12985098/verbose_logs.txt",,,,,,,,,,,,,,,,,,,,,,,,,5.0,benedict,,,,,,,,,,,,Availability -> Response Crash,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Mar 31 16:01:46 UTC 2020,,,,,,,All,,,,,"0|z07nls:",9223372036854775807,,,,,,,dcapwell,djoshi,,,Normal,,4.0,,,https://github.com/apache/cassandra/commit/443bca18839268cd100930b380e0534b052a8c89,,,,,,,,,n/a - entirely removed causative mechanism,,,,,"29/Oct/19 10:53;benedict;[~SRAM85], could you provide some reproduction steps, and other environmental information?

I cannot see an easy way for this to happen without the {{cassandra.test.disable_buffer_pool}} system property being set.;;;","31/Oct/19 14:03;progval;Hello,

 

I had a similar issue earlier today. It was triggered by a client using a code similar to this one: [https://docs.datastax.com/en/developer/python-driver/3.20/query_paging/#handling-paged-results-with-callbacks] with these changes:
 * typo fix ( {{handle_err}} ->  {{handle_error}})
 *  {{""SELECT * FROM users""}} replaced with  {{SimpleStatement(""SELECT * FROM revision"", fetch_size=100)}}

 

The table I'm querying had ~1 billion keys, and is defined with:

 
{code:java}
CREATE TYPE IF NOT EXISTS person (
    fullname    blob,
    name        blob,
    email       blob
);

CREATE TYPE IF NOT EXISTS microtimestamp (
    seconds             bigint,
    microseconds        int
);

CREATE TYPE IF NOT EXISTS microtimestamp_with_timezone (
    timestamp           frozen<microtimestamp>,
    offset              smallint,
    negative_utc        boolean
);

CREATE TABLE IF NOT EXISTS revision (
    id                              blob PRIMARY KEY,
    date                            microtimestamp_with_timezone,
    committer_date                  microtimestamp_with_timezone,
    type                            ascii,
    directory                       blob,
    message                         blob,
    author                          person,
    committer                       person,
    parents                         frozen<list<blob>>,
    synthetic                       boolean,
    metadata                        text
);
{code}
Cluster was initially created with Cassandra 3.11.4, but was migrated to 4.0-alpha1 a few weeks ago. There are four nodes in my cluster, and no replication.

cassandra.yaml was the same as the one shipped with 4.0-alpha1, except for some paths/IP changes, and these changes:
{code:java}
concurrent_reads: 32
concurrent_writes: 64
concurrent_counter_writes: 32
trickle_fsync: true
enable_user_defined_functions: true{code}
After a restart, I am unable to reproduce the issue, so I cannot tell if the issue was caused by my config.;;;","31/Oct/19 17:14;benedict;[~progval] do you have your stack trace to hand, to confirm it's precisely the same issue?;;;","31/Oct/19 20:12;progval;{code:java}
INFO  [Messaging-EventLoop-3-8] 2019-10-31 14:02:31,917 InboundConnectionInitiator.java:450 - 128.93.66.187:7000(128.93.66.187:43306)->128.93.66.190:7000-LARGE_MESSAGES-d60d4cc1 connection established, version = 12, framing = CRC, encryption = disabled
ERROR [Messaging-EventLoop-3-8] 2019-10-31 14:02:31,920 InboundMessageHandler.java:657 - 128.93.66.187:7000->128.93.66.190:7000-LARGE_MESSAGES-d60d4cc1 unexpected exception caught while processing inbound messages; terminating connection
java.lang.IllegalArgumentException: initialBuffer is not a direct buffer.
        at io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:87)
        at io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:59)
        at org.apache.cassandra.net.BufferPoolAllocator$Wrapped.<init>(BufferPoolAllocator.java:95)
        at org.apache.cassandra.net.BufferPoolAllocator.newDirectBuffer(BufferPoolAllocator.java:56)
        at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
        at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
        at io.netty.channel.unix.PreferredDirectByteBufAllocator.ioBuffer(PreferredDirectByteBufAllocator.java:53)
        at io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114)
        at io.netty.channel.epoll.EpollRecvByteAllocatorHandle.allocate(EpollRecvByteAllocatorHandle.java:75)
        at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:777)
        at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$1.run(AbstractEpollChannel.java:382)
        at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
        at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:416)
        at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:331)
        at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
        at java.base/java.lang.Thread.run(Thread.java:834)
DEBUG [Native-Transport-Requests-1] 2019-10-31 14:02:41,855 ReadCallback.java:114 - Timed out; received 0 of 1 responses

{code};;;","01/Nov/19 23:21;SRAM85;[~benedict], below is the setup

Hardware - Azure vms L series (nvme's)

OS - CentOS 7.6

JDK - Java8,11,12,13

GC collector - G!GC,ZGC

Test conditions and results:

TLP-Stress : Keyvalue 

/usr/local/bin/tlp-stress run KeyValue -n 10b -t 400 --cl LOCAL_ONE --compaction lcs --ho 1.3.0.6 --port 9042 -r 0.0 --field.keyvalue.value='book(3000,5000)'

Result : Ran absolutely fantastic no issues , throughput has gone upto 1.2milllion write ops with latency  ~300microseconds.

 

App test: Spark job (real production application)

no of columns : 100

batch size : 100

local_quorum

RF 3

 

Result: we run into the above exception. The nodes are not failing anymore but then the latency and throughput is impacted. we are only able to do 30k write ops with latency up to 80ms.;;;","04/Nov/19 14:49;progval;I am actually able to reproduce the issue (this allowed me to trim down the config I posted above).

Restarts temporarily solve the issue, but after a couple million records fetched with the script above, the error happens again on one of the nodes; and keeps happening every time I run the query, until I restart the node.

I just upgraded to 4.0-alpha2, and the problem is unchanged.;;;","04/Nov/19 14:51;benedict;Interesting. Could you:
 # Find the log line that begins with ""Global buffer pool is enabled, when pool is exhausted"" and reproduce it in its entirety here?
 # Find if you have any log lines beginning with ""Maximum memory usage reached"" and reproduce them here?

Thanks!;;;","04/Nov/19 15:10;progval;1:
{code:java}
INFO [SSTableBatchOpen:1] 2019-11-04 16:02:29,594 BufferPool.java:216 - Global buffer pool is enabled, when pool is exhausted (max is 512.000MiB) it will allocate on heap{code}
2:
{code:java}
INFO  [ReadStage-1] 2019-11-04 16:04:57,612 NoSpamLogger.java:91 - Maximum memory usage reached (512.000MiB), cannot allocate chunk of 8.000MiB
ERROR [Messaging-EventLoop-3-1] 2019-11-04 16:06:40,133 InboundMessageHandler.java:657 - 128.93.66.191:7000->128.93.64.42:7000-LARGE_MESSAGES-0b6d3aaa unexpected exception caught while processing inbound messages; terminating connection
java.lang.IllegalArgumentException: initialBuffer is not a direct buffer. 
       at io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:87)
       at io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:59)
[...]
{code}
 ;;;","04/Nov/19 16:41;benedict;Does ""{{Maximum memory usage reached}}"" always precede the first ""{{java.lang.IllegalArgumentException: initialBuffer is not a direct buffer.}}""?

There's a bug I spotted on inspection, but it shouldn't be possible to encounter, as it would entail converting the heap {{ByteBuffer}} allocated by the first message to a read only {{ByteBuffer}} before being recycled into the {{BufferPool}}, causing it to be treated as though it were a {{DirectByteBuffer}}.  I cannot imagine a scenario where this could happen, but that doesn't mean that it doesn't.  I will upload a branch to see if fixing this resolves the problem.;;;","04/Nov/19 17:06;progval;> Does ""{{Maximum memory usage reached}}"" always precede the first ""{{java.lang.IllegalArgumentException: initialBuffer is not a direct buffer.}}""?

 

Yes, ""Maximum memory usage reached"" is always printed right before the exception is printed for the first time (except for one occurence where there's some chatter about slow requests).

 

If I run my query again without restarting the process, the exception is raised again, without ""Maximum memory usage reached"" before it.

""Maximum memory usage reached"" does get printed again every ~15min if I leave the process running, though.;;;","05/Nov/19 15:24;benedict;Could you try running the following branch? [15358|https://github.geo.apple.com/belliottsmith/cassandra/tree/15358]

It is based on trunk, so let me know if you would prefer it rebased to {{4.0-alpha1}} or {{4.0-alpha2}}, though I don't believe a great deal has changed since.

Given the log statements, there's a good chance this is the problem.  Even though I still think it's nearly impossible for a read-only buffer to be created and returned to the pool, I cannot find another more plausible cause.  If this doesn't fix the problem, I'll see if I can put together a debug build that can maybe report where this {{ByteBuffer}} materialises from.;;;","05/Nov/19 16:54;SRAM85;[~benedict]

Our spark test was run on the version taken from the trunk. ;;;","06/Nov/19 15:42;progval;I compiled a .jar from your commit (`CASSANDRA_USE_JDK11=true ant`) and copied the .jar in place of the one from the release.

 

There is no change:

 
{code:java}
INFO  [ReadStage-1] 2019-11-06 16:36:16,308 NoSpamLogger.java:91 - Maximum memory usage reached (512.000MiB), cannot allocate chunk of 8.000MiB
ERROR [Messaging-EventLoop-3-8] 2019-11-06 16:38:12,051 InboundMessageHandler.java:656 - 128.93.66.191:7000->128.93.64.42:7000-LARGE_MESSAGES-0890f5a2 unexpected exception caught while processing inbound messages; terminating connection
java.lang.IllegalArgumentException: initialBuffer is not a direct buffer.
	at io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:87)
	at io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:59)
	at org.apache.cassandra.net.BufferPoolAllocator$Wrapped.<init>(BufferPoolAllocator.java:95)
	at org.apache.cassandra.net.BufferPoolAllocator.newDirectBuffer(BufferPoolAllocator.java:56)
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
	at io.netty.channel.unix.PreferredDirectByteBufAllocator.ioBuffer(PreferredDirectByteBufAllocator.java:53)
	at io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114)
	at io.netty.channel.epoll.EpollRecvByteAllocatorHandle.allocate(EpollRecvByteAllocatorHandle.java:75)
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:777)
	at io.netty.channel.epoll.AbstractEpollChannel$AbstractEpollUnsafe$1.run(AbstractEpollChannel.java:382)
	at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:416)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:331)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
DEBUG [Native-Transport-Requests-1] 2019-11-06 16:38:21,922 ReadCallback.java:114 - Timed out; received 0 of 1 responses

{code};;;","06/Nov/19 15:51;benedict;Just to confirm: you did restart the process after replacing the jar? (sorry if this is a silly question, it's just that this problem might turn into quite the wild goose chase, so want to be absolutely sure);;;","06/Nov/19 15:53;benedict;Could you also, as one last debug effort, try setting the following config property?

{{buffer_pool_use_heap_if_exhausted: false}};;;","06/Nov/19 16:05;progval;> you did restart the process after replacing the jar?

Yes. (Don't worry I know what it feels like debugging this kind of issues remotely)

 

> Could you also, as one last debug effort, try setting the following config property?
 {{> buffer_pool_use_heap_if_exhausted: false}}

No apparent change. Here is the new stack trace, in case you see a difference:
{code:java}
INFO  [ReadStage-1] 2019-11-06 17:02:16,511 NoSpamLogger.java:91 - Maximum memory usage reached (512.000MiB), cannot allocate chunk of 8.000MiB
ERROR [Messaging-EventLoop-3-8] 2019-11-06 17:04:06,423 InboundMessageHandler.java:656 - 128.93.66.191:7000->128.93.64.42:7000-LARGE_MESSAGES-b5938d12 unexpected exception caught while processing inbound messages; terminating connection
java.lang.IllegalArgumentException: initialBuffer is not a direct buffer.
	at io.netty.buffer.UnpooledDirectByteBuf.<init>(UnpooledDirectByteBuf.java:87)
	at io.netty.buffer.UnpooledUnsafeDirectByteBuf.<init>(UnpooledUnsafeDirectByteBuf.java:59)
	at org.apache.cassandra.net.BufferPoolAllocator$Wrapped.<init>(BufferPoolAllocator.java:95)
	at org.apache.cassandra.net.BufferPoolAllocator.newDirectBuffer(BufferPoolAllocator.java:56)
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:187)
	at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:178)
	at io.netty.channel.unix.PreferredDirectByteBufAllocator.ioBuffer(PreferredDirectByteBufAllocator.java:53)
	at io.netty.channel.DefaultMaxMessagesRecvByteBufAllocator$MaxMessageHandle.allocate(DefaultMaxMessagesRecvByteBufAllocator.java:114)
	at io.netty.channel.epoll.EpollRecvByteAllocatorHandle.allocate(EpollRecvByteAllocatorHandle.java:75)
	at io.netty.channel.epoll.AbstractEpollStreamChannel$EpollStreamUnsafe.epollInReady(AbstractEpollStreamChannel.java:777)
	at io.netty.channel.epoll.EpollEventLoop.processReady(EpollEventLoop.java:424)
	at io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:326)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:918)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:834)
{code}
(Only tried with the jar compiled from your commit);;;","06/Nov/19 16:12;benedict;bq. No apparent change. Here is the new stack trace, in case you see a change:

Hmm, that's _really_ interesting.  That rules out any of the remotely plausible scenario I can imagine.  I'm going to go and take another look at the code, and see what the best debug build is that I can put together to figure out what's happening here.

bq. Yes. (Don't worry I know what it feels like debugging this kind of issues remotely)

Thanks for being understanding :);;;","06/Nov/19 17:39;benedict;Could you grep your log for every instance of this exception, and post them as a file to the ticket?;;;","06/Nov/19 18:19;progval;I just tried adding some assertions and logging, and it gives interesting/weird results, see attachments: [^verbose_logs.diff]   [^verbose_logs.txt]. I hope this will help.

 

Results of all instances of this exception (captured with `zgrep IllegalArgumentException apache-cassandra-4.0-alpha2/logs/debug.log* -A 20 -B 3`): [^all_errors.txt];;;","06/Nov/19 19:54;benedict;Sorry, just for my sanity can you corroborate that the config setting is being picked up, and reproduce the log line ""{{Global buffer pool is enabled, when pool is exhausted}}"" from the latest run(s)?
;;;","06/Nov/19 20:15;benedict;Nevermind.  I have found the problem, it was right there all along, hiding in plain sight.

The {{BufferPool}} API needs to be cleaned up, as it's too easy to pass the wrong parameter.  In this case {{LocalBufferPoolAllocator.getAtLeast}} requests an on-heap buffer _when the pool is exhausted_.  So you can reproduce this easily on the {{LARGE_MESSAGE}} connection.

Sorry, this should have been easier to spot.  Thanks for the report, I will upload a fix shortly, and then follow-up with a modest refactor of {{BufferPool}}.  I may also simply remove entirely the option for on-heap buffers, since there's only one place this is permitted that isn't a clear mistake, and it's not clearly beneficial there.;;;","06/Nov/19 20:17;benedict;I've pushed a fix to the branch.  Thank you for your patience in helping to diagnose it.;;;","06/Nov/19 20:32;benedict;I've also pushed some follow-up cleanups.  Not as much as I would like - I think I would prefer to entirely remove on-heap {{ByteBuffer}} from {{BufferPool}} entirely.  But at least now it is explicit what type you are requesting, by provision of enum, and you only get that type of buffer back.  Only one place in the codebase uses on-heap buffers here, and that is {{Deflate}}.  I'm sure we could get a compatible {{Deflate}} implementation and remove this cruft.;;;","06/Nov/19 20:35;benedict;On a separate note, this issue is also implicitly related to CASSANDRA-15229, as the {{ChunkCache}} is over-committing {{BufferPool}} resources, leaving none for the remainder of the application.  So we must go to {{malloc}} for memory each time.;;;","07/Nov/19 13:06;progval;Thanks you for you help!

 

Unfortunately, I tried running from either of the last two commits of your branch (da3f8d521f556b521f6cb0bdc6abefde87df734f and c15cade21665d34665217a76911979763f909414), and it doesn't fix the issue;;;","07/Nov/19 14:15;benedict;Hmm.  A colleague managed to reproduce this, and the patch fixed the issue for them.  I guess there could be another source of issue, but this looked to fully explain the behaviour you were witnessing, particularly the coincidence with the LARGE_MESSAGE connection and exhaustion.;;;","07/Nov/19 14:27;progval;I missed the rebase on -alpha3, so I kept installing the .jar I compiled yesterday instead of the one compiled today.

 

It works now, thanks a lot!;;;","07/Nov/19 14:30;benedict;Fantastic.  Thanks for your assistance diagnosing the issue!;;;","06/Dec/19 17:43;sermandurai;Thanks [~benedict] for fixing it. We are testing this change and will update you in a day time whether we are seeing this issue (context: Me and [~SRAM85] are in same project)

When this change will be merged to master trunk and also when is the first stable cassandra 4 version release is expected.

Thanks.;;;","10/Dec/19 14:29;benedict;I'm afraid I don't know the answer to either question.  I'm waiting for a review for the fix to be merged into trunk, and maybe I can try to accelerate that by poking some colleagues.  However a stable release of 4.0 is probably months away, given current community resources and where they are focused.

nb: it's worth noting this delay is partially attributable to the community aiming for a much lower defect rate in the first GA release of this major version, and a large amount of effort is currently being invested in automated testing.  Depending on your prior approach to GA releases you might be comfortable treating a beta or RC release as usable, which I hope will come much sooner.;;;","23/Jan/20 07:50;G-Ashok;Hi,

I also experienced this error while testing out 4.alpha2 version. 

I have been testing the incremental repair.

In one of the test a node was missing 30GB of data. (~ 100GB total data, #nodes = 3, rf = 3). Soon after starting the repair, I get this exception and the repair process stops making any progress.

I do get ""Maximum memory usage reached (2.000GiB), cannot allocate chunk of 8.000MiB"" before the exception.

I am attaching the trace of a repair process and the debug logs.

[^repair_1_trace.txt]

[^debug_logs_during_repair.txt]

 ;;;","25/Feb/20 15:51;dcapwell;I’ll try to start review later today ;;;","27/Feb/20 23:17;dcapwell;Update (I am not ignoring this patch!)

I am making sure we have tests which are able to confirm the reported issue (I did manually, I want our tests to as well).  My tests are now able to hit it frequently but there was one thing that popped up that I want to debug a bit more before really getting into the change; I hope to get back to this tomorrow.;;;","29/Feb/20 19:04;benedict;rebased and pushed [here|http://github.com/belliottsmith/cassandra/tree/15358];;;","03/Mar/20 02:45;dcapwell;Been running this for a while and the patch does look to resolve it; no longer able to replicate.

Ill dive into the code tomorrow.;;;","04/Mar/20 03:08;dcapwell;Sorry its been so long...

I put this under load several times without the patch and replicated with easy, and with the patch I am no longer able to replicate.  I want to do one more test where I set the pool to be 0 or 1mb, just to see if I can flesh out any more issues.

As far as I can tell the core change which appears to fix the issue is [here|https://github.com/apache/cassandra/compare/trunk...belliottsmith:15358#diff-613d97c28af63fff3cf1f52baa7f6caaR647]; we switch from heap buffers to direct buffers when out of space and org.apache.cassandra.utils.memory.BufferPool.LocalPool#put(java.nio.ByteBuffer) will just release right away.

General comments:
* +1 to removing ALLOCATE_ON_HEAP_WHEN_EXAHUSTED and DISABLED
* I don't think it makes sense to have the get methods take a BufferType, if you write 

{code}
get(42, BufferType.ON_HEAP) instanceOf HeapByteBuffer
{code}

you expect that to return true (it does) AND that its buffered (it is not).  I feel that every call to BufferPool with BufferType.ON_HEAP should instead just allocate the ByteBuffer directly; I do know that ChunkCache relies on this (figures out what type to do based off org.apache.cassandra.io.util.ChunkReader#preferredBufferType) but I don't feel it should in these cases. This is not a blocking comment and I am 100% fine if a different JIRA addresses.


To be clear, I have not +1 because I want to test more, my goal is to sign off this week;;;","04/Mar/20 10:22;benedict;{quote}
+1 to removing ALLOCATE_ON_HEAP_WHEN_EXAHUSTED and DISABLED
I don't think it makes sense to have the get methods take a BufferType, if you write
{quote}

Great, I'll rustle up a modified patch with those changes, as I agree.  At some point we might want to start offering pooled heap buffers (which would be a simple change), but there's no call for it at the moment, and I prefer to remove disused options.;;;","04/Mar/20 11:38;benedict;bq. I don't think it makes sense to have the get methods take a BufferType, if you write

Ok, so I went to do this and it's not trivial to do as part of this ticket.  I think the right thing to do is to entirely remove {{compressor.preferredBufferType()}}, as the need for the parameter vanishes, but this means dropping support for {{Deflate}}, at least for JDK8.  I would be fine requiring JDK11 for Deflate support in 4.0 though, WDYT?

We anyway need to address the {{ChunkCache}} use of {{BufferPool}} in a separate ticket.

We do have a simple option, which is to rename the {{get}} method to e.g. {{getPooledIfOffHeap}}, or renaming {{BufferPool}} to {{OffHeapBufferPool}} so that it's clear we never pool on-heap.  Or alternatively we could start pooling on-heap, but that should probably be a separate ticket (and is probably unjustified compared to the above options).;;;","04/Mar/20 16:43;dcapwell;bq.  but this means dropping support for Deflate, at least for JDK8. I would be fine requiring JDK11 for Deflate support in 4.0 though, WDYT?

We should support JDK8, that isn't going away any time soon and we would still be able to keep it even if it didn't use the pool.

To be clear, my statement was that ""if you are requested unpooled buffers you should not be using this API"", if you request a native buffer I am fine with using it (though would still prefer isolated memory, thats a different issue though ^_^).

bq. We anyway need to address the ChunkCache use of BufferPool in a separate ticket.

Works for me, can you create a JIRA to address it and link with this one?

bq. rename the get method to e.g. getPooledIfOffHeap, or renaming BufferPool to OffHeapBufferPool so that it's clear we never pool on-heap

The class rename makes the most sense to me (self documenting) but we would still need to guard access at the call sites

{code}
ByteBuffer bb = compressor.preferredBufferType() == ON_HEAP ? ByteBuffer.allocate(size) : OffHeapBufferPool.get(size)
{code}

bq.  Or alternatively we could start pooling on-heap, but that should probably be a separate ticket (and is probably unjustified compared to the above options).

This really should be a different JIRA.  There is still a lot of code which assumes the array isn't shared so would need to do a lot of work to find all those patterns and change them (found org.apache.cassandra.gms.TokenSerializer#serialize with a quick search, this isn't the only one).  

In doing that scan, most of the sites which require array buffers could be changed to support native (or do and special case) as well which makes me ask a different question, ""why not drop array buffer support""; again, different JIRA should tackle this question.;;;","04/Mar/20 16:57;benedict;bq. We should support JDK8, that isn't going away any time soon and we would still be able to keep it even if it didn't use the pool.

We're only talking about dropping {{Deflate}} support for JDK8, and I believe the project is unlikely to support JDK8 in our next major release anyway (since JDK8 is no longer freely supported).  Since {{Deflate}} is very much not recommended, the restriction is minimal and solves this problem neatly.

bq. though would still prefer isolated memory

?

bq. ""if you are requested unpooled buffers you should not be using this API"",

To also be clear, none of the users of this API when requesting {{ON_HEAP}} really require it to be pooled.  There is no guarantee that {{get}} must supply a pooled buffer, and in fact will not pool if the pool threshold has been exceeded (which can be modelled to be zero for heap buffers, which is a reasonable pool size given the very different impact on GC).

So at best we want to reconfigure your expectations of a combination of method and class name to better self-document, or otherwise actual documention making it clear that {{get}} only returns a pooled buffer for {{OFF_HEAP}} would also be fine.  

Literally none of this is a real issue in my book, and I am actually fine leaving this unchanged in all honesty, so I'm not going to spend ages agonising over it.  The only thing I wish is that we do not even permit the option of {{ON_HEAP}}, because it only exists for something nobody uses in practice, since {{Deflate}} is a really bad idea (since it is cripplingly slow) and completely superseded by {{Zstd}}.

bq. Works for me, can you create a JIRA to address it and link with this one?

It already exists, you and I have even discussed it recently (CASSANDRA-15229).;;;","06/Mar/20 06:23;dcapwell;bq. We're only talking about dropping Deflate support for JDK8

Even then we should keep it; I don't disagree there is better.

Sorry, I wanted to run the test this week but 3 things came up, ill try for next week but I may not be able to.

[~benedict] if you could run the test that would be great =);;;","06/Mar/20 09:20;benedict;bq. Sorry, I wanted to run the test this week but 3 things came up, ill try for next week but I may not be able to.

What test?  From my perspective this patch is both ready to merge, and can wait (as I have other priorities right now), so probably not.;;;","12/Mar/20 20:46;dcapwell;+1. tests look good, only issues that stand out look unrelated to this work.;;;","24/Mar/20 08:48;jasonstack;Just FYI:

{{BufferPoolTest#testBufferPoolDisabled}} failed because we cannot disable buffer pool any more. Should we remove it?
 {{BufferPoolTest#testPageAligned}} failed because {{testBufferPoolDisabled}} didn't release the non-page-aligned buffer on failure.;;;","31/Mar/20 16:01;jmckenzie;[~benedict] - anything I or others can do to help get this merged in? We're testing things and hitting this bug quite frequently.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add client testing capabilities to in-jvm tests,CASSANDRA-15347,13261062,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,drohrer,ifesdjeen,ifesdjeen,08/Oct/19 08:20,27/Aug/20 15:09,13/Jul/23 08:38,11/Nov/19 15:03,2.2.16,3.0.20,3.11.6,4.0,4.0-alpha3,Test/dtest/java,,,,0,patch-available,pull-request-available,,Allow testing native transport code path using in-jvm tests.,,drohrer,ifesdjeen,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,,,,,"JeetKunDoug commented on pull request #374: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/374
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Nov/19 13:17;githubbot;600","JeetKunDoug commented on pull request #375: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/375
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Nov/19 13:19;githubbot;600","JeetKunDoug commented on pull request #376: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/376
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Nov/19 13:20;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Nov/19 13:20;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343836399
 
 

 ##########
 File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
 ##########
 @@ -357,13 +364,53 @@ public void uncaughtException(Thread t, Throwable e)
         int rpcPort = DatabaseDescriptor.getRpcPort();
         int listenBacklog = DatabaseDescriptor.getRpcListenBacklog();
         thriftServer = new ThriftServer(rpcAddr, rpcPort, listenBacklog);
+        initializeNativeTransport();
+
+        completeSetup();
+    }
 
+    public void initializeNativeTransport()
+    {
         // Native transport
         InetAddress nativeAddr = DatabaseDescriptor.getRpcAddress();
         int nativePort = DatabaseDescriptor.getNativeTransportPort();
         nativeServer = new org.apache.cassandra.transport.Server(nativeAddr, nativePort);
+    }
 
-        completeSetup();
+    public void startNativeTransport()
+    {
+        validateTransportsCanStart();
+
+        if (nativeServer == null)
+            throw new IllegalStateException(""setup() must be called first for CassandraDaemon"");
 
 Review comment:
   Maybe we can say something like ""native transport should be set up before it can be started"" ?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343875251
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/Instance.java
 ##########
 @@ -536,6 +546,12 @@ private void initializeRing(ICluster cluster)
                                 .thenRun(super::shutdown);
     }
 
+    // NOTE: This method requires the instance to be initialized and running
+    public int liveMemberCount()
+    {
+        return callsOnInstance(() -> Gossiper.instance.getLiveMembers().size()).call();
 
 Review comment:
   We can add check for `Gossiper#isEnabled` and throw a reasonable exception instead of relying on the note.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343836721
 
 

 ##########
 File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
 ##########
 @@ -357,13 +364,53 @@ public void uncaughtException(Thread t, Throwable e)
         int rpcPort = DatabaseDescriptor.getRpcPort();
         int listenBacklog = DatabaseDescriptor.getRpcListenBacklog();
         thriftServer = new ThriftServer(rpcAddr, rpcPort, listenBacklog);
+        initializeNativeTransport();
+
+        completeSetup();
+    }
 
+    public void initializeNativeTransport()
+    {
         // Native transport
         InetAddress nativeAddr = DatabaseDescriptor.getRpcAddress();
         int nativePort = DatabaseDescriptor.getNativeTransportPort();
         nativeServer = new org.apache.cassandra.transport.Server(nativeAddr, nativePort);
+    }
 
-        completeSetup();
+    public void startNativeTransport()
+    {
+        validateTransportsCanStart();
+
+        if (nativeServer == null)
+            throw new IllegalStateException(""setup() must be called first for CassandraDaemon"");
+        else
 
 Review comment:
   we don't really need `else` here, since control flow will be outside the method if we throw anyways
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343874030
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/Instance.java
 ##########
 @@ -400,12 +402,18 @@ public void startup(ICluster cluster)
                 if (config.has(GOSSIP))
                 {
                     StorageService.instance.initServer();
+                    StorageService.instance.removeShutdownHook();
                 }
                 else
                 {
                     initializeRing(cluster);
                 }
 
+                if (config.has(NATIVE_PROTOCOL)) {
 
 Review comment:
   Can we move startup of the native protocol for until after we've finished setting up traces and finished startup? This should always be the last step.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343893476
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java
 ##########
 @@ -301,33 +306,26 @@ private void updateMessagingVersions()
         }
     }
 
-    /**
-     * Will wait for a schema change AND agreement that occurs after it is created
-     * (and precedes the invocation to waitForAgreement)
-     *
-     * Works by simply checking if all UUIDs agree after any schema version change event,
-     * so long as the waitForAgreement method has been entered (indicating the change has
-     * taken place on the coordinator)
-     *
-     * This could perhaps be made a little more robust, but this should more than suffice.
-     */
-    public class SchemaChangeMonitor implements AutoCloseable
+    public abstract class ChangeMonitor<I extends IInstance> implements AutoCloseable
 
 Review comment:
   Generic seems to be never used here. Especially given it's not a static class and we have access to `I` of the parent.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343896135
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java
 ##########
 @@ -337,20 +335,91 @@ public void close()
                 cancel.cancel();
         }
 
-        public void waitForAgreement()
+        public void waitForCompletion()
         {
-            schemaHasChanged = true;
+            this.changed = true;
+            signal();
+            startPolling();
+            changed = true;
 
 Review comment:
   `changed` is set twice.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343891043
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/test/NativeProtocolTest.java
 ##########
 @@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import com.datastax.driver.core.ConsistencyLevel;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.Session;
+import com.datastax.driver.core.SimpleStatement;
+import com.datastax.driver.core.Statement;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.impl.RowUtil;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.util.Iterator;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+
+public class NativeProtocolTest extends DistributedTestBase
+{
+    public static final String RANGE_MOVEMENT_PROP = ""cassandra.consistent.rangemovement"";
+
+    @Test
+    public void withClientRequests() throws Throwable
+    {
+        try (Cluster ignored = init(Cluster.create(3,
+                                                   config -> config.with(GOSSIP, NETWORK, NATIVE_PROTOCOL))))
+        {
+            final com.datastax.driver.core.Cluster cluster = com.datastax.driver.core.Cluster.builder().addContactPoint(""127.0.0.1"").build();
+            Session session = cluster.connect();
+            session.execute(""CREATE TABLE "" + KEYSPACE + "".tbl (pk int, ck int, v int, PRIMARY KEY (pk, ck));"");
+            session.execute(""INSERT INTO "" + KEYSPACE + "".tbl (pk, ck, v) values (1,1,1);"");
+            Statement select = new SimpleStatement(""select * from "" + KEYSPACE + "".tbl;"").setConsistencyLevel(ConsistencyLevel.ALL);
+            final ResultSet resultSet = session.execute(select);
+            Iterator<Object[]> rows = RowUtil.toObjects(resultSet);
+            assertRows(rows, row(1, 1, 1));
 
 Review comment:
   Should we just add `assertRows` that receives a resultset?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343874214
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/Instance.java
 ##########
 @@ -400,12 +402,18 @@ public void startup(ICluster cluster)
                 if (config.has(GOSSIP))
                 {
                     StorageService.instance.initServer();
+                    StorageService.instance.removeShutdownHook();
                 }
                 else
                 {
                     initializeRing(cluster);
                 }
 
+                if (config.has(NATIVE_PROTOCOL)) {
 
 Review comment:
   Also a tiny nit: line break before `{` 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343890376
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/test/NativeProtocolTest.java
 ##########
 @@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import com.datastax.driver.core.ConsistencyLevel;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.Session;
+import com.datastax.driver.core.SimpleStatement;
+import com.datastax.driver.core.Statement;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.impl.RowUtil;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.util.Iterator;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+
+public class NativeProtocolTest extends DistributedTestBase
+{
+    public static final String RANGE_MOVEMENT_PROP = ""cassandra.consistent.rangemovement"";
 
 Review comment:
   Seems not to be used.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343891832
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java
 ##########
 @@ -337,20 +335,91 @@ public void close()
                 cancel.cancel();
         }
 
-        public void waitForAgreement()
+        public void waitForCompletion()
         {
-            schemaHasChanged = true;
+            this.changed = true;
+            signal();
+            startPolling();
+            changed = true;
             signal();
             try
             {
-                if (!agreement.await(1L, TimeUnit.MINUTES))
+                if (!completed.await(timeOut, timeoutUnit))
                     throw new InterruptedException();
             }
             catch (InterruptedException e)
             {
-                throw new IllegalStateException(""Schema agreement not reached"");
+                throw new IllegalStateException(getMonitorTimeoutMessage());
+            }
+        }
+
+        private void startPolling() {
+            for (IInstance instance: instances) {
+                cleanup.add(startPolling(instance));
             }
         }
+
+        protected abstract IListen.Cancel startPolling(IInstance instance);
+
+        protected abstract boolean isCompleted();
+
+        protected abstract String getMonitorTimeoutMessage();
+    }
+
+
+
+    /**
+     * Will wait for a schema change AND agreement that occurs after it is created
+     * (and precedes the invocation to waitForAgreement)
+     *
+     * Works by simply checking if all UUIDs agree after any schema version change event,
+     * so long as the waitForAgreement method has been entered (indicating the change has
+     * taken place on the coordinator)
+     *
+     * This could perhaps be made a little more robust, but this should more than suffice.
+     */
+    public class SchemaChangeMonitor extends ChangeMonitor<I>
+    {
+        public SchemaChangeMonitor()
+        {
+            super(70, TimeUnit.SECONDS);
+        }
+
+        protected IListen.Cancel startPolling(IInstance instance)
+        {
+            return instance.listen().schema(this::signal);
+        }
+
+        protected boolean isCompleted()
+        {
+            return 1 == instances.stream().map(IInstance::schemaVersion).distinct().count();
+        }
+
+        protected String getMonitorTimeoutMessage()
+        {
+            return ""Schema agreement not reached"";
+        }
+    }
+
+    public class LiveMemberAgreementMonitor extends ChangeMonitor<I> {
 
 Review comment:
   Nit: Should we call it `LiveMemberCountMonitor`? To be more consistent with `SchemaChangeMonitor`? 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","ifesdjeen commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r343888239
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/Listen.java
 ##########
 @@ -34,20 +36,31 @@ public Listen(Instance instance)
 
     public Cancel schema(Runnable onChange)
     {
-        final AtomicBoolean cancel = new AtomicBoolean();
+        return start(onChange, instance::schemaVersion);
+    }
+
+    public Cancel liveMembers(Runnable onChange)
+    {
+        return start(onChange, () -> instance.callsOnInstance(() -> Gossiper.instance.getLiveMembers()).call());
 
 Review comment:
   Don't we now have `getLiveMembers` right on the `Instance`? 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Nov/19 22:39;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344170761
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/Instance.java
 ##########
 @@ -536,6 +546,12 @@ private void initializeRing(ICluster cluster)
                                 .thenRun(super::shutdown);
     }
 
+    // NOTE: This method requires the instance to be initialized and running
+    public int liveMemberCount()
+    {
+        return callsOnInstance(() -> Gossiper.instance.getLiveMembers().size()).call();
 
 Review comment:
   Unfortunately no - gossip can in fact be disabled but you can check the live member count (in the dtest case where we don't enable the `GOSSIP` feature, instances are automatically added to the live member set on instance startup, so this check works even with gossip disabled). Not sure if there's a better way, as the flag that says if an instance is started or not only exists on the Wrapper class. I haven't yet dug into why that's the case though.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 13:24;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344171572
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/Listen.java
 ##########
 @@ -34,20 +36,31 @@ public Listen(Instance instance)
 
     public Cancel schema(Runnable onChange)
     {
-        final AtomicBoolean cancel = new AtomicBoolean();
+        return start(onChange, instance::schemaVersion);
+    }
+
+    public Cancel liveMembers(Runnable onChange)
+    {
+        return start(onChange, () -> instance.callsOnInstance(() -> Gossiper.instance.getLiveMembers()).call());
 
 Review comment:
   We have `getLiveMemberCount` but not the complete list. I could add that as well, or change the count method to return the whole list and just do the `size` call outside the instance itself if that makes more sense.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 13:26;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344172078
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/test/NativeProtocolTest.java
 ##########
 @@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import com.datastax.driver.core.ConsistencyLevel;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.Session;
+import com.datastax.driver.core.SimpleStatement;
+import com.datastax.driver.core.Statement;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.impl.RowUtil;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.util.Iterator;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+
+public class NativeProtocolTest extends DistributedTestBase
+{
+    public static final String RANGE_MOVEMENT_PROP = ""cassandra.consistent.rangemovement"";
 
 Review comment:
   Right - this was from a previous attempt to get the parallel startup stuff working before I found that we could universally just disable auto_bootstrap. Removed.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 13:27;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344174354
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/test/NativeProtocolTest.java
 ##########
 @@ -0,0 +1,61 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import com.datastax.driver.core.ConsistencyLevel;
+import com.datastax.driver.core.ResultSet;
+import com.datastax.driver.core.Session;
+import com.datastax.driver.core.SimpleStatement;
+import com.datastax.driver.core.Statement;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.impl.RowUtil;
+
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.util.Iterator;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+
+public class NativeProtocolTest extends DistributedTestBase
+{
+    public static final String RANGE_MOVEMENT_PROP = ""cassandra.consistent.rangemovement"";
+
+    @Test
+    public void withClientRequests() throws Throwable
+    {
+        try (Cluster ignored = init(Cluster.create(3,
+                                                   config -> config.with(GOSSIP, NETWORK, NATIVE_PROTOCOL))))
+        {
+            final com.datastax.driver.core.Cluster cluster = com.datastax.driver.core.Cluster.builder().addContactPoint(""127.0.0.1"").build();
+            Session session = cluster.connect();
+            session.execute(""CREATE TABLE "" + KEYSPACE + "".tbl (pk int, ck int, v int, PRIMARY KEY (pk, ck));"");
+            session.execute(""INSERT INTO "" + KEYSPACE + "".tbl (pk, ck, v) values (1,1,1);"");
+            Statement select = new SimpleStatement(""select * from "" + KEYSPACE + "".tbl;"").setConsistencyLevel(ConsistencyLevel.ALL);
+            final ResultSet resultSet = session.execute(select);
+            Iterator<Object[]> rows = RowUtil.toObjects(resultSet);
+            assertRows(rows, row(1, 1, 1));
 
 Review comment:
   It's an option - everything in `assertRows` today deals with Objects and Object arrays, so it seemed to make sense to put the conversion outside, but for future consumers it probably makes sense to wrap it in an easier to use call. Will do this.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 13:33;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344175943
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java
 ##########
 @@ -337,20 +335,91 @@ public void close()
                 cancel.cancel();
         }
 
-        public void waitForAgreement()
+        public void waitForCompletion()
         {
-            schemaHasChanged = true;
+            this.changed = true;
+            signal();
+            startPolling();
+            changed = true;
             signal();
             try
             {
-                if (!agreement.await(1L, TimeUnit.MINUTES))
+                if (!completed.await(timeOut, timeoutUnit))
                     throw new InterruptedException();
             }
             catch (InterruptedException e)
             {
-                throw new IllegalStateException(""Schema agreement not reached"");
+                throw new IllegalStateException(getMonitorTimeoutMessage());
+            }
+        }
+
+        private void startPolling() {
+            for (IInstance instance: instances) {
+                cleanup.add(startPolling(instance));
             }
         }
+
+        protected abstract IListen.Cancel startPolling(IInstance instance);
+
+        protected abstract boolean isCompleted();
+
+        protected abstract String getMonitorTimeoutMessage();
+    }
+
+
+
+    /**
+     * Will wait for a schema change AND agreement that occurs after it is created
+     * (and precedes the invocation to waitForAgreement)
+     *
+     * Works by simply checking if all UUIDs agree after any schema version change event,
+     * so long as the waitForAgreement method has been entered (indicating the change has
+     * taken place on the coordinator)
+     *
+     * This could perhaps be made a little more robust, but this should more than suffice.
+     */
+    public class SchemaChangeMonitor extends ChangeMonitor<I>
+    {
+        public SchemaChangeMonitor()
+        {
+            super(70, TimeUnit.SECONDS);
+        }
+
+        protected IListen.Cancel startPolling(IInstance instance)
+        {
+            return instance.listen().schema(this::signal);
+        }
+
+        protected boolean isCompleted()
+        {
+            return 1 == instances.stream().map(IInstance::schemaVersion).distinct().count();
+        }
+
+        protected String getMonitorTimeoutMessage()
+        {
+            return ""Schema agreement not reached"";
+        }
+    }
+
+    public class LiveMemberAgreementMonitor extends ChangeMonitor<I> {
 
 Review comment:
   +1
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 13:36;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344198928
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/Listen.java
 ##########
 @@ -34,20 +36,31 @@ public Listen(Instance instance)
 
     public Cancel schema(Runnable onChange)
     {
-        final AtomicBoolean cancel = new AtomicBoolean();
+        return start(onChange, instance::schemaVersion);
+    }
+
+    public Cancel liveMembers(Runnable onChange)
+    {
+        return start(onChange, () -> instance.callsOnInstance(() -> Gossiper.instance.getLiveMembers()).call());
 
 Review comment:
   Remembered why I didn't expose `liveMembers` directly. In `trunk`, `liveMembers` is a `Set<InetAddressAndPort>` whereas in earlier versions it's just `Set<InetAddress>` and I wasn't sure how we'd want to expose it in the cross-version API. Since the `Listen` class just needed the values to compare, it could encapsulate the differing return values and just work.
   Also, exposing either class across the isolated class loader boundary would be an issue I think, which means we'd have to duplicate it in the api package of the distributed tests and copy the values out of the version's copy, which gets a bit ugly.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 14:29;githubbot;600","JeetKunDoug commented on issue #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#issuecomment-551858231
 
 
   WRT auto_bootstrap and the `parallelForeach` on startup, I've added code to partition the instances by their `auto_bootstrap` setting, starting up the ones set to true first (using `forEach`), and then doing all of the non-bootstrap instances together in a `parallelForEach` call. There are current no tests that actually use the non-parallel path, but I've tested it both ways by changing the default in `InstanceConfig` and it does what it's supposed to do.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 14:53;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344233071
 
 

 ##########
 File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
 ##########
 @@ -357,13 +364,53 @@ public void uncaughtException(Thread t, Throwable e)
         int rpcPort = DatabaseDescriptor.getRpcPort();
         int listenBacklog = DatabaseDescriptor.getRpcListenBacklog();
         thriftServer = new ThriftServer(rpcAddr, rpcPort, listenBacklog);
+        initializeNativeTransport();
+
+        completeSetup();
+    }
 
+    public void initializeNativeTransport()
+    {
         // Native transport
         InetAddress nativeAddr = DatabaseDescriptor.getRpcAddress();
         int nativePort = DatabaseDescriptor.getNativeTransportPort();
         nativeServer = new org.apache.cassandra.transport.Server(nativeAddr, nativePort);
+    }
 
-        completeSetup();
+    public void startNativeTransport()
+    {
+        validateTransportsCanStart();
+
+        if (nativeServer == null)
+            throw new IllegalStateException(""setup() must be called first for CassandraDaemon"");
 
 Review comment:
   Done
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 15:37;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344233111
 
 

 ##########
 File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
 ##########
 @@ -357,13 +364,53 @@ public void uncaughtException(Thread t, Throwable e)
         int rpcPort = DatabaseDescriptor.getRpcPort();
         int listenBacklog = DatabaseDescriptor.getRpcListenBacklog();
         thriftServer = new ThriftServer(rpcAddr, rpcPort, listenBacklog);
+        initializeNativeTransport();
+
+        completeSetup();
+    }
 
+    public void initializeNativeTransport()
+    {
         // Native transport
         InetAddress nativeAddr = DatabaseDescriptor.getRpcAddress();
         int nativePort = DatabaseDescriptor.getNativeTransportPort();
         nativeServer = new org.apache.cassandra.transport.Server(nativeAddr, nativePort);
+    }
 
-        completeSetup();
+    public void startNativeTransport()
+    {
+        validateTransportsCanStart();
+
+        if (nativeServer == null)
+            throw new IllegalStateException(""setup() must be called first for CassandraDaemon"");
+        else
 
 Review comment:
   done
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 15:37;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344233151
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/Instance.java
 ##########
 @@ -400,12 +402,18 @@ public void startup(ICluster cluster)
                 if (config.has(GOSSIP))
                 {
                     StorageService.instance.initServer();
+                    StorageService.instance.removeShutdownHook();
                 }
                 else
                 {
                     initializeRing(cluster);
                 }
 
+                if (config.has(NATIVE_PROTOCOL)) {
 
 Review comment:
   done
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 15:37;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344254917
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/Instance.java
 ##########
 @@ -536,6 +546,12 @@ private void initializeRing(ICluster cluster)
                                 .thenRun(super::shutdown);
     }
 
+    // NOTE: This method requires the instance to be initialized and running
+    public int liveMemberCount()
+    {
+        return callsOnInstance(() -> Gossiper.instance.getLiveMembers().size()).call();
 
 Review comment:
   Ends up the check for liveliness really belongs in the Wrapper, which can throw the correct exception. Added the check there and removed the comment here.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 16:21;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344255725
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java
 ##########
 @@ -337,20 +335,91 @@ public void close()
                 cancel.cancel();
         }
 
-        public void waitForAgreement()
+        public void waitForCompletion()
         {
-            schemaHasChanged = true;
+            this.changed = true;
+            signal();
+            startPolling();
+            changed = true;
 
 Review comment:
   fixed by taking your suggested changes to the overall class.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 16:23;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#discussion_r344256865
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/impl/AbstractCluster.java
 ##########
 @@ -301,33 +306,26 @@ private void updateMessagingVersions()
         }
     }
 
-    /**
-     * Will wait for a schema change AND agreement that occurs after it is created
-     * (and precedes the invocation to waitForAgreement)
-     *
-     * Works by simply checking if all UUIDs agree after any schema version change event,
-     * so long as the waitForAgreement method has been entered (indicating the change has
-     * taken place on the coordinator)
-     *
-     * This could perhaps be made a little more robust, but this should more than suffice.
-     */
-    public class SchemaChangeMonitor implements AutoCloseable
+    public abstract class ChangeMonitor<I extends IInstance> implements AutoCloseable
 
 Review comment:
   Removed
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;08/Nov/19 16:25;githubbot;600","JeetKunDoug commented on issue #374: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/374#issuecomment-552482083
 
 
   Committed with https://github.com/apache/cassandra/commit/fa85ff978bae303fe1b06dce64b758b635278a4d - thanks @ifesdjeen for all the help & review.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 15:07;githubbot;600","JeetKunDoug commented on pull request #374: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/374
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 15:07;githubbot;600","JeetKunDoug commented on issue #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377#issuecomment-552482293
 
 
   Committed with https://github.com/apache/cassandra/commit/d90dc87bd3f8a149d98ccf40b40bf152405fbbec
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 15:07;githubbot;600","JeetKunDoug commented on pull request #377: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/377
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 15:07;githubbot;600","JeetKunDoug commented on issue #376: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/376#issuecomment-552482457
 
 
   Committed with https://github.com/apache/cassandra/commit/d90dc87bd3f8a149d98ccf40b40bf152405fbbec
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 15:08;githubbot;600","JeetKunDoug commented on pull request #376: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/376
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 15:08;githubbot;600","JeetKunDoug commented on issue #375: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/375#issuecomment-552482692
 
 
   Committed with https://github.com/apache/cassandra/commit/0d5ccb9c2f65ddee4a1b18b76800b7b88d3c6379
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 15:08;githubbot;600","JeetKunDoug commented on pull request #375: CASSANDRA-15347 - Add client testing capabilities to in-jvm tests
URL: https://github.com/apache/cassandra/pull/375
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/19 15:08;githubbot;600",,0,18600,,,0,18600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,drohrer,,,,,,,,,,,,Code,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Nov 11 15:02:39 UTC 2019,,,,,,,All,,,,,"0|z07ds0:",9223372036854775807,,,,,,,ifesdjeen,,,,Normal,,2.2.15,,,https://github.com/apache/cassandra/commit/50b7094278241f389d3b0b49b02e893fd4322b12,,,,,,,,,See comments for PRs.,,,,,"01/Nov/19 14:11;drohrer;This set of PRs allows the in-jvm dtest framework to support native protocol clients, which allows for testing of the Java client and other use-cases where it makes sense to test from ""outside"" (Spark, for example).

 

Four PRs for different Cassandra versions:

2.2 [changes|https://github.com/apache/cassandra/pull/377] [Circle|https://circleci.com/workflow-run/19f5082f-eedc-4d8e-8d33-558848fddc77]
 3.0 [changes|https://github.com/apache/cassandra/pull/376] [Circle|https://circleci.com/workflow-run/ddf5b452-2a51-4d3a-9cd4-d4b279e0f280]
 3.11 [changes|https://github.com/apache/cassandra/pull/375] [Circle|https://circleci.com/workflow-run/59c4d1b6-c0c2-4179-b719-a8c041c849ff]
 Trunk [changes|https://github.com/apache/cassandra/pull/374] [Circle|https://circleci.com/workflow-run/fea3a793-bf13-4652-8b88-d29e1b513254] 

The changes are more extensive than just ""Add Native Transport Support,"" as I ran into several reliability issues with the tests once we started allowing connectivity via the native transport, but may have already been causing some level of instability, and to speed up test execution times. These changes include:
 - Setting {{auto_bootstrap}} to false by default for in-jvm dtests. There was no reason to wait for instances to bootstrap before starting tests, as the cluster is empty, which could slow down test execution and caused some test timing issues where requests could be made before the instance was fully ready. Tests that may need {{auto_bootstrap}} later can always set it explicitly.
 - It was possible, especially in {{trunk}}, for tests to fail to be able to create the initial keyspace requested in {{DistributedTestBase.init}} because of a race between a hard-coded 60-second timeout in MigrationManager {{MIGRATION_DELAY_IN_MS}} and an identical 60-second hard-coded wait timeout in the {{SchemaChangeMonitor}}. This could occur if the instance where the schema change was submitted did not yet see one or more other instances in its live member list when first gossiping the schema change. There were two changes made to alleviate this issue:
 ** Extend the {{SchemaChangeMonitor}}'s delay to 70 seconds to accommodate the {{MigrationManager}}'s 60-second delay
 **  In order to avoid the root cause, and the potential of a 70 second delay if tests hit the race, also added a new monitor {{LiveMemberAgreementMonitor}} which waits for all instances to agree that the live member count is equal to our expected count of instances running before moving on from Cluster.startup. This adds a very minor potential delay to cluster startup as we wait for the members to all see each other, but completely avoids the possibility that the subsequent schema change will be delayed by up to 60 seconds.

There are a few other minor changes/refactorings that were picked up from Alex's original patch for this change, which was never submitted to C*, so he was kind enough to help me put this together and has done some early code review as well. A new test {{NativeTransportTest}} was added to cover the native transport functionality and a new {{ResourceLeakTest}} to make sure we weren't introducing any cross-classloader references that would block collection of classes and exhaust java's metaspace.;;;","07/Nov/19 22:42;ifesdjeen;Thank you for the patch! I've left several comments here: https://github.com/apache/cassandra/pull/377#pullrequestreview-313587421 

I'll check out the other branches as soon as we have an agreement on 2.x branch.;;;","11/Nov/19 15:02;ifesdjeen;During commit, I've noticed a couple of minor problems: 

  * in 2.2, we have to await for shutdown of the transport, since otherwise Netty cleanup task may attempt to call allocator, which may be unloaded by that time
  * in 3.11, the approach I've suggested with polling didn't work, since it was causing gossiper instance to initialise token metadata before database descriptor, resulting into empty partitioner. I've fixed the order there, too.

There were several minor changes on commit, but all of them were cosmetic.

Committed to 2.2 with [d90dc87bd3f8a149d98ccf40b40bf152405fbbec|https://github.com/apache/cassandra/commit/d90dc87bd3f8a149d98ccf40b40bf152405fbbec] and merged up to [3.0|https://github.com/apache/cassandra/commit/d90dc87bd3f8a149d98ccf40b40bf152405fbbec], [3.11|https://github.com/apache/cassandra/commit/0d5ccb9c2f65ddee4a1b18b76800b7b88d3c6379], and [trunk|https://github.com/apache/cassandra/commit/fa85ff978bae303fe1b06dce64b758b635278a4d].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource leak in CompressedSequentialWriter,CASSANDRA-15340,13259743,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,30/Sep/19 19:48,15/May/20 08:36,13/Jul/23 08:38,07/Oct/19 18:23,3.0.19,3.11.5,4.0,4.0-alpha2,,Legacy/Local Write-Read Paths,,,,0,,,,"In CompressedSequentialWriter, we reallocate the {{compressed}} buffer if the existing buffer is not large enough. These buffers are usually direct byte buffers, and we don't explicitly release their memory here, which delays release until the buffer is gc'd",,bdeggleston,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Degradation -> Resource Management,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Oct 07 18:23:46 UTC 2019,,,,,,,All,,,,,"0|z0761c:",9223372036854775807,,,,,,,marcuse,,,,Low,,3.0.0,,,https://github.com/apache/cassandra/commit/cbecd5f2834f233f1c7c912407343cc3a9b517bb,,,,,,,,,circle,,,,,"30/Sep/19 19:58;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/15340-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15340-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/15340-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15340-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/15340-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15340-trunk]|;;;","04/Oct/19 07:17;marcuse;+1;;;","07/Oct/19 18:23;bdeggleston;committed to 3.0 as [cbecd5f2834f233f1c7c912407343cc3a9b517bb |https://github.com/apache/cassandra/commit/cbecd5f2834f233f1c7c912407343cc3a9b517bb] and merged up;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flakey testMessagePurging - org.apache.cassandra.net.ConnectionTest,CASSANDRA-15338,13259322,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yifanc,dcapwell,dcapwell,27/Sep/19 17:39,21/Dec/20 08:08,13/Jul/23 08:38,16/Apr/20 12:16,4.0,4.0-beta1,,,,Test/unit,,,,0,pull-request-available,,,"Example failure: [https://circleci.com/gh/dcapwell/cassandra/11#artifacts/containers/1]
  
{code:java}
Testcase: testMessagePurging(org.apache.cassandra.net.ConnectionTest):  FAILED
 expected:<0> but was:<1>
 junit.framework.AssertionFailedError: expected:<0> but was:<1>
   at org.apache.cassandra.net.ConnectionTest.lambda$testMessagePurging$38(ConnectionTest.java:625)
   at org.apache.cassandra.net.ConnectionTest.doTestManual(ConnectionTest.java:258)
   at org.apache.cassandra.net.ConnectionTest.testManual(ConnectionTest.java:231)
   at org.apache.cassandra.net.ConnectionTest.testMessagePurging(ConnectionTest.java:584){code}

  
 Looking closer at org.apache.cassandra.net.OutboundConnection.Delivery#stopAndRun it seems that the run method is called before org.apache.cassandra.net.OutboundConnection.Delivery#doRun which may lead to a test race condition where the CountDownLatch completes before executing",,adelapena,benedict,dcapwell,e.dimitrova,jasonstack,jeromatron,yifanc,,,,,,,,,,,,,,,,,,,,,"yifan-c commented on pull request #466: CASSANDRA-15338 Fix flakey testMessagePurging - org.apache.cassandra.net.ConnectionTest
URL: https://github.com/apache/cassandra/pull/466
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Mar/20 02:47;githubbot;600","yifan-c closed pull request #466:
URL: https://github.com/apache/cassandra/pull/466


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/20 18:51;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,CASSANDRA-15308,CASSANDRA-15958,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/20 02:48;yifanc;CASS-15338-Docker.zip;https://issues.apache.org/jira/secure/attachment/12996236/CASS-15338-Docker.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,yifanc,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Aug 04 20:25:30 UTC 2020,,,,,,,All,,,,,"0|z073fs:",9223372036854775807,,,,,,,adelapena,,,,Low,,4.0-alpha,4.0-alpha1,,https://ci-cassandra.apache.org/job/Cassandra-devbranch-test/50/,,,,,,,,,unit test,,,,,"30/Sep/19 09:13;benedict;cc [~ifesdjeen]

;;;","18/Feb/20 22:17;yifanc;The test failure reported here is caused by the flaky {{testAcquireReleaseOutbound}} in CASSANDRA-15308. 
After fixing {{testAcquireReleaseOutbound}}, all tests in {{ConnectionTest}} can pass happily. 

The repeated local runs of {{ConnectionTest}} using either Java 8 and 11 proved the test failure as described in this ticket did not show up. 

{code:java}
Switched to Java 8
12:42:00 in cassandra on CASSANDRA-15308
➜ while [[ ""$(ant testclasslist -Dtest.classlistfile=<(echo org/apache/cassandra/net/ConnectionTest.java) | grep -c 'BUILD SUCCESSFUL')"" == ""1"" ]]; do echo ""It was a good run.""; done
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
^C%
13:10:47 in cassandra on CASSANDRA-15308 took 28m 39s
➜ j11
Switched to Java 11
13:10:49 in cassandra on CASSANDRA-15308
➜  while [[ ""$(ant testclasslist -Dtest.classlistfile=<(echo org/apache/cassandra/net/ConnectionTest.java) -Duse.jdk11=true | grep -c 'BUILD SUCCESSFUL')"" == ""1"" ]]; do echo ""It was a good run.""; done
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
It was a good run.
^C%
{code}
 ;;;","10/Mar/20 02:53;yifanc;The test failure was not able to be reproduced when simply running it from my laptop. 
 
However, it can be easily reproduced when running in a docker container with limited CPUs (i.e., 2). 
 
After multiple runs, the observation was that the test runs only failed when testing with LargeMessage. It indicated that the failures were probably related with {{LargeMessageDelivery}}. 
 
The following is what I think have happened. 
# When the {{inbound}} just opened and the first message gets queued into the {{outbound}}, handshake happens and the execution was deferred once the connection was established (executeAgain). 
# Since enqueue is not blocking, the next line, {{unsafeRunOnDelivery}} runs immediately. The effect is that the runnable gets registered, but not run yet. 
# Connection is established, so we {{executeAgain()}}. Because the runnable {{stopAndRun}} is present, and at this point, the {{inProgress}} flag is still false. The test runs the runnable, which counts down {{deliveryDone}} unexpectedly. 
# Delivery proceeds to flush the message. In {{LargeMessageDelivery}}, the flush is async and race condition can happen.
   ## when the inbound has received message (and countdown receiveDone)
   ## {{LargeMessageDelivery}} is still polling for the completion of flush, so not yet release capacity. 

Therefore, the assertion on the pendingCount failed. 
 
There are 2 places in the test flow are (or can go) wrong. See step 3 and step 4. 

Regarding step 3, the runnable {{stopAndRun}} should not be registered when establishing the connection. In production, is there a case that a {{stopAndRun}} being registered this early? Probably not.

Regarding step 4, the {{outbound}} has no knowledge about whether the {{inbound}} has received any message. Test should register the runnable {{stopAndRun}} at the message handler to count down the {{deliveryDone}}. Therefore, the runnable can correctly wait for the current delivery to complete. Then it runs. 
 
PR is here: https://github.com/apache/cassandra/pull/466

As mentioned, I reproduced using the docker. Here is the bundle that one can simply download and run.  [^CASS-15338-Docker.zip] It runs {{ConnectionTest}} repeatedly until failures.
I have included the patch within the image too. 

To reproduce, run
{code:bash}
bash build_and_run.sh
{code}

To see the runs with the patch, run
{code:bash}
bash build_and_run.sh patched
{code};;;","18/Mar/20 14:13;e.dimitrova;[~yifanc] [~dcapwell] [~ifesdjeen] is anyone reviewing this one?;;;","18/Mar/20 18:04;dcapwell;not started yet, looked at the other one only.;;;","06/Apr/20 20:17;yifanc;[~benedict][~dcapwell], do you want to take a look?;;;","06/Apr/20 22:47;dcapwell;I have a few things on my plate, I should be able to look end of the week?;;;","10/Apr/20 16:03;e.dimitrova;I [~dcapwell] are you still on that?;;;","10/Apr/20 17:04;dcapwell;nope, still too much on my plate.  This patch requires me to review closer which I am struggling with the time on atm =(;;;","13/Apr/20 10:53;adelapena;[~dcapwell] I can start reviewing this one tomorrow, if it's ok with you;;;","13/Apr/20 16:47;dcapwell;Sounds great thanks!;;;","15/Apr/20 16:32;adelapena;Looks good to me. [~yifanc] do you need me to commit it?;;;","15/Apr/20 16:41;yifanc;Thanks [~adelapena]. That will be great. Please proceed. ;;;","16/Apr/20 10:47;adelapena;Committed to trunk as [753b40eb0f570fc88b5211b9bcea04761a240071|https://github.com/apache/cassandra/commit/753b40eb0f570fc88b5211b9bcea04761a240071].;;;","29/Jul/20 00:09;dcapwell;[~yifanc] this test is still flakey, but now hitting TimeoutException: https://app.circleci.com/pipelines/github/dcapwell/cassandra/362/workflows/c04020b0-d13e-4e18-ae27-0277e636b73d/jobs/1858
;;;","29/Jul/20 00:19;yifanc;Thanks for reporting, [~dcapwell].

The test failure in the link is timeout (30 seconds) when closing the inbound connection. I will take a closer look later. ;;;","04/Aug/20 20:25;yifanc;Just took another look. The exception is not expected. Pasting the stack trace.
{code:java}
java.util.concurrent.TimeoutException
	at org.apache.cassandra.net.AsyncPromise.get(AsyncPromise.java:258)
	at org.apache.cassandra.net.FutureDelegate.get(FutureDelegate.java:143)
	at org.apache.cassandra.net.ConnectionTest.doTestManual(ConnectionTest.java:268)
	at org.apache.cassandra.net.ConnectionTest.testManual(ConnectionTest.java:236)
	at org.apache.cassandra.net.ConnectionTest.testMessagePurging(ConnectionTest.java:679)
{code}

In the test, testMessagePurging, it already closed the inbound connection. The test does not throw at that line, so I would assume that inbound is closed successfully. 

If so, we should not see the Timeout exception from {{doTestManual(ConnectionTest.java:268)}}. Because the inbound is closed already. 

I suspect that there might be some hidden issue in the {{close()}} method. I will open a new ticket if it is. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node can corrupt gossip state and become unreplaceable,CASSANDRA-15335,13258615,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,24/Sep/19 18:10,15/May/20 08:36,13/Jul/23 08:38,07/Oct/19 18:50,3.0.19,3.11.5,4.0,4.0-alpha2,,Cluster/Gossip,,,,0,,,,"In {{StorageService#prepareToJoin}}, a starting node first sends out an endpoint state without any tokens. Later, in {{StorageService#finishJoiningRing}} it sends out an endpoint state _with_ it’s tokens. If that node dies between these 2 events and cannot be restarted due to some unrecoverable error, the ring’s gossip state will be missing tokens for that node. This won’t cause any immediate data loss since TMD is populated from system.peers, but it will prevent a replacement node from associating that address with it’s tokens and replacing it. It could also cause data loss if other nodes are added to the ring and don’t see an owned token where there should be one.",,bdeggleston,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Correctness -> Consistency,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Oct 07 18:50:14 UTC 2019,,,,,,,All,,,,,"0|z06z2g:",9223372036854775807,,,,,,,marcuse,samt,,,Low,,3.0.0,,,https://github.com/apache/cassandra/commit/39a431d9d821cf6bcb54da7198b6a64e09b0f120,,,,,,,,,circle,,,,,"24/Sep/19 18:13;bdeggleston;[~brandon.williams] do you know if there's a problem we're trying to avoid by omitting tokens in the first endpoint state? It seems like, if system.local says we've bootstrapped and has tokens, we should be ok to include them;;;","24/Sep/19 18:23;brandon.williams;I _think_ that's just an artifact of how the code is organized.  I can't think of a reason not to advertise them if we know they are set.;;;","30/Sep/19 20:01;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/15335-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15335-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/15335-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15335-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/15335-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15335-trunk]|;;;","03/Oct/19 20:19;samt;+1 LGTM too;;;","04/Oct/19 06:43;marcuse;+1;;;","07/Oct/19 18:50;bdeggleston;committed to 3.0 as [39a431d9d821cf6bcb54da7198b6a64e09b0f120 |https://github.com/apache/cassandra/commit/39a431d9d821cf6bcb54da7198b6a64e09b0f120] and merged up, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"The release process does not incremental the version, nor document the need to",CASSANDRA-15333,13258108,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mck,mck,mck,21/Sep/19 16:52,21/Dec/20 08:08,13/Jul/23 08:38,22/Sep/19 16:31,4.0,4.0-alpha2,,,,Documentation/Website,,,,0,,,,"Incrementing the {{`base.version`}} in {{build.xml}} has remained a manual, and easily forgotten, part of the release process.

This patch adds the how and when to perform that step into the existing release process documentation: 
 https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_15333",,mck,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mck,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 22 16:31:04 UTC 2019,,,,,,,All,,,,,"0|z06vy8:",9223372036854775807,,,,,,,mshuler,,,,Low,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/b0f9d72840ec13030ad97ad77bf7478a079c2f6f ,,,,,,,,,is a fix to documentation,,,,,"21/Sep/19 17:00;mck;[~mshuler], have you an opportunity to review this, it is but a few lines of docs.;;;","22/Sep/19 15:57;mshuler;Looks good, feel free to commit. Thanks!;;;","22/Sep/19 16:31;mck;Committed as b0f9d72840ec13030ad97ad77bf7478a079c2f6f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When repair is running with tracing, if a CorruptSSTableException is thrown while building Merkle Trees the DiskFailurePolicy does not get applied",CASSANDRA-15332,13258071,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,dcapwell,dcapwell,dcapwell,21/Sep/19 01:31,15/May/20 08:39,13/Jul/23 08:38,15/Nov/19 17:18,4.0,4.0-alpha3,,,,Consistency/Repair,Observability/Tracing,,,0,pull-request-available,,,"When a repair is in the validation phase and is building MerkleTrees, if a corrupt SSTable exception is thrown the disk failure policy does not get applied",,dcapwell,jeromatron,jwest,marcuse,,,,,,,,,,,,,,,,,,,,,,,,"dcapwell commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Oct/19 17:22;githubbot;600","krummas commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r337561935
 
 

 ##########
 File path: build.xml
 ##########
 @@ -514,6 +514,7 @@
           <dependency groupId=""org.yaml"" artifactId=""snakeyaml"" version=""1.11""/>
           <dependency groupId=""junit"" artifactId=""junit"" version=""4.12"" />
           <dependency groupId=""org.quicktheories"" artifactId=""quicktheories"" version=""0.25"" />
+          <dependency groupId=""org.mockito"" artifactId=""mockito-core"" version=""3.0.0""/>
 
 Review comment:
   no need for this now right?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 09:15;githubbot;600","krummas commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340113038
 
 

 ##########
 File path: src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
 ##########
 @@ -282,30 +310,79 @@ public static Throwable extractThrowable(Runnable runnable)
                 return e.getCause();
             }
         }
-
         return null;
     }
 
+    /**
+     * If a task wraps a {@link Future} then it should implement this interface to expose the underlining future for
+     * {@link #extractThrowable(Runnable)} to handle.
+     *
+     * @param <T>
+     */
+    private interface HasDelegateFuture<T>
+    {
+        Future<T> getDelegate();
+    }
+
     /**
      * Used to wrap a Runnable or Callable passed to submit or execute so we can clone the ExecutorLocals and move
      * them into the worker thread.
      *
+     * The {@link DebuggableThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)}
+     * method is called after the runnable completes, which will then call {@link #extractThrowable(Runnable)} to
+     * attempt to get the ""hidden"" throwable from a task which implements {@link Future}.  The problem is that {@link LocalSessionWrapper}
+     * expects that the {@link Callable} provided to it will throw; which is not true for {@link RunnableFuture} tasks;
+     * the expected semantic in this case is to have the LocalSessionWrapper future be successful and a new implementation
+     * {@link FutureLocalSessionWrapper} is created to expose the underline {@link Future} for {@link #extractThrowable(Runnable)}.
+     *
+     * If a task is a {@link Runnable} the create family of methods should be called rather than {@link Executors#callable(Runnable)}
+     * since they will handle the case where the task is also a future, and will make sure the {@link #extractThrowable(Runnable)}
+     * is able to detect the task's underline exception.
+     *
      * @param <T>
      */
     private static class LocalSessionWrapper<T> extends FutureTask<T>
     {
         private final ExecutorLocals locals;
 
-        public LocalSessionWrapper(Callable<T> callable)
+        private LocalSessionWrapper(Callable<T> callable, ExecutorLocals locals)
         {
             super(callable);
-            locals = ExecutorLocals.create();
+            this.locals = locals;
         }
 
-        public LocalSessionWrapper(Runnable command, ExecutorLocals locals)
+        static LocalSessionWrapper<Object> create(Runnable command)
         {
-            super(command, null);
-            this.locals = locals;
+            return create(command, ExecutorLocals.create());
+        }
+
+        static LocalSessionWrapper<Object> create(Runnable command, ExecutorLocals locals)
+        {
+            if (command instanceof RunnableFuture)
+                return new FutureLocalSessionWrapper<>((RunnableFuture) command, null, locals);
+            return new LocalSessionWrapper<>(Executors.callable(command), locals);
+        }
+
+        static <T> LocalSessionWrapper<T> create(Runnable command, T result)
+        {
+            return create(command, result, ExecutorLocals.create());
+        }
+
+        static <T> LocalSessionWrapper<T> create(Runnable command, T result, ExecutorLocals locals)
+        {
+            if (command instanceof RunnableFuture)
+                return new FutureLocalSessionWrapper<>((RunnableFuture) command, result, locals);
+            return new LocalSessionWrapper<>(Executors.callable(command, result), locals);
+        }
+
+        static <T> LocalSessionWrapper<T> create(Callable<T> command)
+        {
+            return create(command, ExecutorLocals.create());
+        }
+
+        static <T> LocalSessionWrapper<T> create(Callable<T> command, ExecutorLocals locals)
 
 Review comment:
   only called from the method above - remove?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 09:15;githubbot;600","krummas commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340105176
 
 

 ##########
 File path: src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
 ##########
 @@ -282,30 +310,79 @@ public static Throwable extractThrowable(Runnable runnable)
                 return e.getCause();
             }
         }
-
         return null;
     }
 
+    /**
+     * If a task wraps a {@link Future} then it should implement this interface to expose the underlining future for
+     * {@link #extractThrowable(Runnable)} to handle.
+     *
+     * @param <T>
+     */
+    private interface HasDelegateFuture<T>
 
 Review comment:
   no need for `<T>` here, can return `Future<?>` instead
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 09:15;githubbot;600","krummas commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340121848
 
 

 ##########
 File path: src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
 ##########
 @@ -282,30 +310,79 @@ public static Throwable extractThrowable(Runnable runnable)
                 return e.getCause();
             }
         }
-
         return null;
     }
 
+    /**
+     * If a task wraps a {@link Future} then it should implement this interface to expose the underlining future for
+     * {@link #extractThrowable(Runnable)} to handle.
+     *
+     * @param <T>
+     */
+    private interface HasDelegateFuture<T>
+    {
+        Future<T> getDelegate();
+    }
+
     /**
      * Used to wrap a Runnable or Callable passed to submit or execute so we can clone the ExecutorLocals and move
      * them into the worker thread.
      *
+     * The {@link DebuggableThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)}
+     * method is called after the runnable completes, which will then call {@link #extractThrowable(Runnable)} to
+     * attempt to get the ""hidden"" throwable from a task which implements {@link Future}.  The problem is that {@link LocalSessionWrapper}
+     * expects that the {@link Callable} provided to it will throw; which is not true for {@link RunnableFuture} tasks;
+     * the expected semantic in this case is to have the LocalSessionWrapper future be successful and a new implementation
+     * {@link FutureLocalSessionWrapper} is created to expose the underline {@link Future} for {@link #extractThrowable(Runnable)}.
+     *
+     * If a task is a {@link Runnable} the create family of methods should be called rather than {@link Executors#callable(Runnable)}
+     * since they will handle the case where the task is also a future, and will make sure the {@link #extractThrowable(Runnable)}
+     * is able to detect the task's underline exception.
+     *
      * @param <T>
      */
     private static class LocalSessionWrapper<T> extends FutureTask<T>
     {
         private final ExecutorLocals locals;
 
-        public LocalSessionWrapper(Callable<T> callable)
+        private LocalSessionWrapper(Callable<T> callable, ExecutorLocals locals)
         {
             super(callable);
-            locals = ExecutorLocals.create();
+            this.locals = locals;
         }
 
-        public LocalSessionWrapper(Runnable command, ExecutorLocals locals)
+        static LocalSessionWrapper<Object> create(Runnable command)
         {
-            super(command, null);
-            this.locals = locals;
+            return create(command, ExecutorLocals.create());
+        }
+
+        static LocalSessionWrapper<Object> create(Runnable command, ExecutorLocals locals)
+        {
+            if (command instanceof RunnableFuture)
 
 Review comment:
   can call `create(command, null, locals)` to avoid duplicating the instanceof logic
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 09:15;githubbot;600","krummas commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340470246
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/api/IInstance.java
 ##########
 @@ -24,7 +24,7 @@
 import java.util.concurrent.Future;
 
 // The cross-version API requires that an Instance has a constructor signature of (IInstanceConfig, ClassLoader)
-public interface IInstance extends IIsolatedExecutor
+public interface IInstance extends IIsolatedExecutor, Thread.UncaughtExceptionHandler
 
 Review comment:
   Maybe add the `uncaughtException` method explicitly to `IInstance` instead of implementing this interface since we never use it as an actual uncaught exception handler
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 09:15;githubbot;600","dcapwell commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340684140
 
 

 ##########
 File path: build.xml
 ##########
 @@ -514,6 +514,7 @@
           <dependency groupId=""org.yaml"" artifactId=""snakeyaml"" version=""1.11""/>
           <dependency groupId=""junit"" artifactId=""junit"" version=""4.12"" />
           <dependency groupId=""org.quicktheories"" artifactId=""quicktheories"" version=""0.25"" />
+          <dependency groupId=""org.mockito"" artifactId=""mockito-core"" version=""3.0.0""/>
 
 Review comment:
   Sorry should remove; this was when the tests included streaming failures
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 15:22;githubbot;600","dcapwell commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340684664
 
 

 ##########
 File path: src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
 ##########
 @@ -282,30 +310,79 @@ public static Throwable extractThrowable(Runnable runnable)
                 return e.getCause();
             }
         }
-
         return null;
     }
 
+    /**
+     * If a task wraps a {@link Future} then it should implement this interface to expose the underlining future for
+     * {@link #extractThrowable(Runnable)} to handle.
+     *
+     * @param <T>
+     */
+    private interface HasDelegateFuture<T>
+    {
+        Future<T> getDelegate();
+    }
+
     /**
      * Used to wrap a Runnable or Callable passed to submit or execute so we can clone the ExecutorLocals and move
      * them into the worker thread.
      *
+     * The {@link DebuggableThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)}
+     * method is called after the runnable completes, which will then call {@link #extractThrowable(Runnable)} to
+     * attempt to get the ""hidden"" throwable from a task which implements {@link Future}.  The problem is that {@link LocalSessionWrapper}
+     * expects that the {@link Callable} provided to it will throw; which is not true for {@link RunnableFuture} tasks;
+     * the expected semantic in this case is to have the LocalSessionWrapper future be successful and a new implementation
+     * {@link FutureLocalSessionWrapper} is created to expose the underline {@link Future} for {@link #extractThrowable(Runnable)}.
+     *
+     * If a task is a {@link Runnable} the create family of methods should be called rather than {@link Executors#callable(Runnable)}
+     * since they will handle the case where the task is also a future, and will make sure the {@link #extractThrowable(Runnable)}
+     * is able to detect the task's underline exception.
+     *
      * @param <T>
      */
     private static class LocalSessionWrapper<T> extends FutureTask<T>
     {
         private final ExecutorLocals locals;
 
-        public LocalSessionWrapper(Callable<T> callable)
+        private LocalSessionWrapper(Callable<T> callable, ExecutorLocals locals)
         {
             super(callable);
-            locals = ExecutorLocals.create();
+            this.locals = locals;
         }
 
-        public LocalSessionWrapper(Runnable command, ExecutorLocals locals)
+        static LocalSessionWrapper<Object> create(Runnable command)
         {
-            super(command, null);
-            this.locals = locals;
+            return create(command, ExecutorLocals.create());
+        }
+
+        static LocalSessionWrapper<Object> create(Runnable command, ExecutorLocals locals)
+        {
+            if (command instanceof RunnableFuture)
+                return new FutureLocalSessionWrapper<>((RunnableFuture) command, null, locals);
+            return new LocalSessionWrapper<>(Executors.callable(command), locals);
+        }
+
+        static <T> LocalSessionWrapper<T> create(Runnable command, T result)
+        {
+            return create(command, result, ExecutorLocals.create());
+        }
+
+        static <T> LocalSessionWrapper<T> create(Runnable command, T result, ExecutorLocals locals)
+        {
+            if (command instanceof RunnableFuture)
+                return new FutureLocalSessionWrapper<>((RunnableFuture) command, result, locals);
+            return new LocalSessionWrapper<>(Executors.callable(command, result), locals);
+        }
+
+        static <T> LocalSessionWrapper<T> create(Callable<T> command)
+        {
+            return create(command, ExecutorLocals.create());
+        }
+
+        static <T> LocalSessionWrapper<T> create(Callable<T> command, ExecutorLocals locals)
 
 Review comment:
   Only had for consistency; can remove
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 15:23;githubbot;600","dcapwell commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340738626
 
 

 ##########
 File path: test/distributed/org/apache/cassandra/distributed/api/IInstance.java
 ##########
 @@ -24,7 +24,7 @@
 import java.util.concurrent.Future;
 
 // The cross-version API requires that an Instance has a constructor signature of (IInstanceConfig, ClassLoader)
-public interface IInstance extends IIsolatedExecutor
+public interface IInstance extends IIsolatedExecutor, Thread.UncaughtExceptionHandler
 
 Review comment:
   done
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 16:53;githubbot;600","dcapwell commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340738876
 
 

 ##########
 File path: src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
 ##########
 @@ -282,30 +310,79 @@ public static Throwable extractThrowable(Runnable runnable)
                 return e.getCause();
             }
         }
-
         return null;
     }
 
+    /**
+     * If a task wraps a {@link Future} then it should implement this interface to expose the underlining future for
+     * {@link #extractThrowable(Runnable)} to handle.
+     *
+     * @param <T>
+     */
+    private interface HasDelegateFuture<T>
+    {
+        Future<T> getDelegate();
+    }
+
     /**
      * Used to wrap a Runnable or Callable passed to submit or execute so we can clone the ExecutorLocals and move
      * them into the worker thread.
      *
+     * The {@link DebuggableThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)}
+     * method is called after the runnable completes, which will then call {@link #extractThrowable(Runnable)} to
+     * attempt to get the ""hidden"" throwable from a task which implements {@link Future}.  The problem is that {@link LocalSessionWrapper}
+     * expects that the {@link Callable} provided to it will throw; which is not true for {@link RunnableFuture} tasks;
+     * the expected semantic in this case is to have the LocalSessionWrapper future be successful and a new implementation
+     * {@link FutureLocalSessionWrapper} is created to expose the underline {@link Future} for {@link #extractThrowable(Runnable)}.
+     *
+     * If a task is a {@link Runnable} the create family of methods should be called rather than {@link Executors#callable(Runnable)}
+     * since they will handle the case where the task is also a future, and will make sure the {@link #extractThrowable(Runnable)}
+     * is able to detect the task's underline exception.
+     *
      * @param <T>
      */
     private static class LocalSessionWrapper<T> extends FutureTask<T>
     {
         private final ExecutorLocals locals;
 
-        public LocalSessionWrapper(Callable<T> callable)
+        private LocalSessionWrapper(Callable<T> callable, ExecutorLocals locals)
         {
             super(callable);
-            locals = ExecutorLocals.create();
+            this.locals = locals;
         }
 
-        public LocalSessionWrapper(Runnable command, ExecutorLocals locals)
+        static LocalSessionWrapper<Object> create(Runnable command)
         {
-            super(command, null);
-            this.locals = locals;
+            return create(command, ExecutorLocals.create());
+        }
+
+        static LocalSessionWrapper<Object> create(Runnable command, ExecutorLocals locals)
+        {
+            if (command instanceof RunnableFuture)
 
 Review comment:
   done
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 16:54;githubbot;600","dcapwell commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340738962
 
 

 ##########
 File path: src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
 ##########
 @@ -282,30 +310,79 @@ public static Throwable extractThrowable(Runnable runnable)
                 return e.getCause();
             }
         }
-
         return null;
     }
 
+    /**
+     * If a task wraps a {@link Future} then it should implement this interface to expose the underlining future for
+     * {@link #extractThrowable(Runnable)} to handle.
+     *
+     * @param <T>
+     */
+    private interface HasDelegateFuture<T>
+    {
+        Future<T> getDelegate();
+    }
+
     /**
      * Used to wrap a Runnable or Callable passed to submit or execute so we can clone the ExecutorLocals and move
      * them into the worker thread.
      *
+     * The {@link DebuggableThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)}
+     * method is called after the runnable completes, which will then call {@link #extractThrowable(Runnable)} to
+     * attempt to get the ""hidden"" throwable from a task which implements {@link Future}.  The problem is that {@link LocalSessionWrapper}
+     * expects that the {@link Callable} provided to it will throw; which is not true for {@link RunnableFuture} tasks;
+     * the expected semantic in this case is to have the LocalSessionWrapper future be successful and a new implementation
+     * {@link FutureLocalSessionWrapper} is created to expose the underline {@link Future} for {@link #extractThrowable(Runnable)}.
+     *
+     * If a task is a {@link Runnable} the create family of methods should be called rather than {@link Executors#callable(Runnable)}
+     * since they will handle the case where the task is also a future, and will make sure the {@link #extractThrowable(Runnable)}
+     * is able to detect the task's underline exception.
+     *
      * @param <T>
      */
     private static class LocalSessionWrapper<T> extends FutureTask<T>
     {
         private final ExecutorLocals locals;
 
-        public LocalSessionWrapper(Callable<T> callable)
+        private LocalSessionWrapper(Callable<T> callable, ExecutorLocals locals)
         {
             super(callable);
-            locals = ExecutorLocals.create();
+            this.locals = locals;
         }
 
-        public LocalSessionWrapper(Runnable command, ExecutorLocals locals)
+        static LocalSessionWrapper<Object> create(Runnable command)
         {
-            super(command, null);
-            this.locals = locals;
+            return create(command, ExecutorLocals.create());
+        }
+
+        static LocalSessionWrapper<Object> create(Runnable command, ExecutorLocals locals)
+        {
+            if (command instanceof RunnableFuture)
+                return new FutureLocalSessionWrapper<>((RunnableFuture) command, null, locals);
+            return new LocalSessionWrapper<>(Executors.callable(command), locals);
+        }
+
+        static <T> LocalSessionWrapper<T> create(Runnable command, T result)
+        {
+            return create(command, result, ExecutorLocals.create());
+        }
+
+        static <T> LocalSessionWrapper<T> create(Runnable command, T result, ExecutorLocals locals)
+        {
+            if (command instanceof RunnableFuture)
+                return new FutureLocalSessionWrapper<>((RunnableFuture) command, result, locals);
+            return new LocalSessionWrapper<>(Executors.callable(command, result), locals);
+        }
+
+        static <T> LocalSessionWrapper<T> create(Callable<T> command)
+        {
+            return create(command, ExecutorLocals.create());
+        }
+
+        static <T> LocalSessionWrapper<T> create(Callable<T> command, ExecutorLocals locals)
 
 Review comment:
   done
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 16:54;githubbot;600","dcapwell commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340739020
 
 

 ##########
 File path: src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
 ##########
 @@ -282,30 +310,79 @@ public static Throwable extractThrowable(Runnable runnable)
                 return e.getCause();
             }
         }
-
         return null;
     }
 
+    /**
+     * If a task wraps a {@link Future} then it should implement this interface to expose the underlining future for
+     * {@link #extractThrowable(Runnable)} to handle.
+     *
+     * @param <T>
+     */
+    private interface HasDelegateFuture<T>
 
 Review comment:
   done
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 16:54;githubbot;600","dcapwell commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368#discussion_r340739076
 
 

 ##########
 File path: build.xml
 ##########
 @@ -514,6 +514,7 @@
           <dependency groupId=""org.yaml"" artifactId=""snakeyaml"" version=""1.11""/>
           <dependency groupId=""junit"" artifactId=""junit"" version=""4.12"" />
           <dependency groupId=""org.quicktheories"" artifactId=""quicktheories"" version=""0.25"" />
+          <dependency groupId=""org.mockito"" artifactId=""mockito-core"" version=""3.0.0""/>
 
 Review comment:
   done
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;30/Oct/19 16:54;githubbot;600","dcapwell commented on pull request #368: [CASSANDRA-15332] When building MerkleTrees if a CorruptSSTableException is thrown the DiskFailurePolicy does not get applied
URL: https://github.com/apache/cassandra/pull/368
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;28/Jan/20 00:47;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,0,7200,,,0,7200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,dcapwell,,,,,,,,,,,,Degradation -> Other Exception,,,,,,,,Low Hanging Fruit,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Nov 15 17:18:37 UTC 2019,,,,,,,All,,,,,"0|z06vq0:",9223372036854775807,,,,,,,jwest,marcuse,,,Normal,,2.1 beta1,,,https://github.com/apache/cassandra/commit/c98a31bef0d3071bc8ebfd358584aadf9e787fb8,,,,,,,,,"||branch||diff||tests||pr||
|[branch|https://github.com/dcapwell/cassandra/tree/repairCorruption]|[diff|https://github.com/apache/cassandra/compare/trunk...dcapwell:repairCorruption]|[tests|https://circleci.com/gh/dcapwell/workflows/cassandra/tree/repairCorruption]|[pr|https://github.com/apache/cassandra/pull/368]|



Changes:

1) JVMStabilityInspector checks for corruption or FSError and delegates to FileUtils for handling

2) FileUtils now implements the die behavior

3) JVMStabilityInspector now walks suppressed exceptions as well as the caused exception (edge case not in streaming where the second exception is fserror or corruption, so just added since saw suppression was common)

4) dtest updated to keep track of the number of kill attempts (though does not kill, mostly so tests can reuse the cluster)

Test case added which causes a corruption exception to be thrown from a SSTable during validation and shows that the handling makes its way to JVMStabilities kill instance method.",,,,,"21/Sep/19 01:32;dcapwell;Found with the following in-JVM dtest: https://github.com/dcapwell/cassandra/blob/repairDtest/test/distributed/org/apache/cassandra/distributed/test/FailingRepairTest.java;;;","15/Oct/19 00:50;dcapwell;||branch||diff||tests||
|[branch|https://github.com/dcapwell/cassandra/tree/repairCorruption]|[diff|https://github.com/apache/cassandra/compare/trunk...dcapwell:repairCorruption]|[tests|https://circleci.com/workflow-run/760a2f16-1084-4cf4-b789-008c7e325602]|



Changes:

1) JVMStabilityInspector checks for corruption or FSError and delegates to FileUtils for handling

2) FileUtils now implements the die behavior

3) JVMStabilityInspector now walks suppressed exceptions as well as the caused exception (edge case not in streaming where the second exception is fserror or corruption, so just added since saw suppression was common)

4) dtest updated to keep track of the number of kill attempts (though does not kill, mostly so tests can reuse the cluster);;;","17/Oct/19 01:57;jwest;Mostly minor its and optional suggestions but the first comment I think is important to address:

 * In the uncaught exception handler setup by {{CassandraDaemon}}, it looks like we no longer log the {{FSError}}/{{CorruptSSTableException}} exactly once if its a cause instead of the original. In particular, the {{FSError}}/{{CorruptSStableException}} are not logged at all now if they are a cause and the disk failure policy is not {{die}}.
* It would be beneficial for tests to not have to be aware of/call {{InstanceKiller#clear()}}. Maybe call it immediately after its set in {{Instance#startup}}?
* It could be useful to future test authors if you document the returns values from `IInstance#killAttempts()` given the comment in the {{AbstractCluster.Wrapper}} implementation. 
* {{Instance#killAttempts()}}: use {{callOnInstance(…)}} instead of {{callsOnInstance(…).call()}}. 
* {{ForwardingSSTableReader}} constructor: is there a reason you explicitly call {{addComponents}} vs. passing the result of {{SSTable.componentsFor(descriptor.delegate)}} to {{super}}? Testing locally: all the new tests pass with the latter change made.  
* {{FailingRepairTest#L114-115}}: so its not accidentally missed, consider making these changes in CASSANDRA-15356 instead of leaving them commented out here
* {{JVMStabilityInspector#L91-92}}: the comment is somewhat contextual to the original code being changed here, perhaps “its expected the fs handler will kill the jvm if unstable” is sufficient?
* {{FailingRepairTest#L220}}: minor typo, “expected”
* {{FailingRepairTest#L221}}: You can use {{callOnInstance}} instead of {{callsOnInstance(...).call()}}
* {{FailingRepairTest#L238}}: this failure message could be a bit more descriptive, e.g. “repair return status was 0, expected non-zero return status, 0 indicates repair not submitted”
* {{FailingRepairTest#testFailingMessage}}: consider using the {{replica}} and {{coordinator}} variables throughout the function — some initial node lookups explicitly use {{1}} & {{2}} to identify nodes.
* {{FailingRepairTest#L240-244/253-254}}: +1 on the “wait until” approach. It could be useful to pull these out into utility methods for others to use and to add a timeout — I see the test itself has a timeout but adding the control to the “wait until” methods could be indicative for future test authors. 
* {{FailingRepairTest#L248}}: consider making the failure message more descriptive, e.g. “Expected FAILED repair status, but status was: [status]”
* {{FailingRepairTest#L257}}: typo, “coordinator”


Also, while we are updating {{OutOfSpaceTest}}, some minor improvements you might consider:
    * testFlushUnwritableStop should also check the native transports have been disabled and that the killer was not invoked
    * testFlushUnwritableIgnore: consider checking that things that would happen on die or stop don’t happen (e.g. native transport/gossip are still running and killer was not invoked). 
    * can the makeTable() calls in each test be moved to a @Before or @BeforeClass?
    * L125: use ColumnFamilyStore#forceBlockingFlush instead of ColumnFamilyStore.forceFlush().get();;;","17/Oct/19 15:27;dcapwell;{quote}
In the uncaught exception handler setup by CassandraDaemon, it looks like we no longer log the FSError/CorruptSSTableException exactly once if its a cause instead of the original. In particular, the FSError/CorruptSStableException are not logged at all now if they are a cause and the disk failure policy is not die.
{quote}

The following will log it, though I do admit the logging is changed

{code:java}
logger.error(""Exception in thread "" + t, e);
{code}

There are 3 cases (all will be logged)
* FSError/CorruptSStableException are the root exception, then behavior is basically not changed
* FSError/CorruptSStableException is the cause of the exception, behavior is changed but the cause by section will show this
* FSError/CorruptSStableException is suppressed, the behavior is unchanged in this scenario
;;;","17/Oct/19 16:01;dcapwell;{quote}
It would be beneficial for tests to not have to be aware of/call InstanceKiller#clear(). Maybe call it immediately after its set in Instance#startup?
{quote}

Sorry, I don't follow.  Do you mean these tests or in general?  InstanceKiller is local to the instance (class loader isolation in dtest) so if tests create new clusters for each test case (the norm so far) then the state is reset in each test.

Now, in these tests, I rely on a shared cluster, so I don't teardown between test cases.  This causes the killed count to be incremented multiple times for each test case, so each test case needs to clear it (since the cluster is shared state).

{quote}
It could be useful to future test authors if you document the returns values from `IInstance#killAttempts()` given the comment in the AbstractCluster.Wrapper implementation.
{quote}
Done.


{quote}
Instance#killAttempts(): use callOnInstance(…) instead of callsOnInstance(…).call().
{quote}
Done

{quote}
* ForwardingSSTableReader constructor: is there a reason you explicitly call addComponents vs. passing the result of SSTable.componentsFor(descriptor.delegate) to super? Testing locally: all the new tests pass with the latter change made.  
{quote}
Changed.

{quote}
FailingRepairTest#L114-115: so its not accidentally missed, consider making these changes in CASSANDRA-15356 instead of leaving them commented out here
{quote}
Fair.  I tried working on this and streaming requires the die policy in order to have the coordinator detect the issue, so need to actually shutdown the instance (even though the test is ""die"", dtest treats it as ""ignore""; was complicated to shutdown on InstanceKiller and share the state that it was killed for tests).

I really should refactor this test to go through all policies, but ill try doing that in CASSANDRA-15356

Fixed. Removed all code for streaming.

{quote}
JVMStabilityInspector#L91-92: the comment is somewhat contextual to the original code being changed here, perhaps “its expected the fs handler will kill the jvm if unstable” is sufficient?
{quote}

Switched to ""disk failure policy is handled by FileUtils, so delegate""

{quote}
FailingRepairTest#L220: minor typo, “expected”
{quote}
fixed

{quote}
FailingRepairTest#L221: You can use callOnInstance instead of callsOnInstance(...).call()
{quote}
done

{quote}
FailingRepairTest#L238: this failure message could be a bit more descriptive, e.g. “repair return status was 0, expected non-zero return status, 0 indicates repair not submitted”
{quote}
fixed

{quote}
FailingRepairTest#testFailingMessage: consider using the replica and coordinator variables throughout the function — some initial node lookups explicitly use 1 & 2 to identify nodes.
{quote}
fixed

{quote}
FailingRepairTest#L240-244/253-254: +1 on the “wait until” approach. It could be useful to pull these out into utility methods for others to use and to add a timeout — I see the test itself has a timeout but adding the control to the “wait until” methods could be indicative for future test authors.
{quote}

My personal style is to not create generic utilities until there is the second case, so would prefer not.  

This is more of a case of ""retry until"" which I see as its own generic utility, though don't know if C* has that for me to reuse?

{quote}
FailingRepairTest#L248: consider making the failure message more descriptive, e.g. “Expected FAILED repair status, but status was: [status]”
{quote}
Don't I get that for free since I am using assertEquals?  The message block is mostly because the status is hidden in the array, so in some cases the array has more details.


{quote}
FailingRepairTest#L257: typo, “coordinator”
{quote}

With all the code moving around... I think I fixed it =D;;;","17/Oct/19 16:02;dcapwell;[~jrwest];;;","17/Oct/19 17:44;jwest;Thanks for making the changes. They LGTM. +1;;;","21/Oct/19 13:43;marcuse;bq.  if a corrupt SSTable exception is thrown the disk failure policy does not get applied
Looks like the reason for this is that we don't set the uncaught exception handler in the in-jvm dtests - adding the same one as in {{CassandraDaemon}} in {{Instance#startup}} makes us invoke the policy when there is a corrupt sstable during validation.;;;","21/Oct/19 14:00;dcapwell;[~marcuse] I see that for streaming but repair is getting caught and put into the futures failure case; so never makes it to the default handler (nothing uses the future later);;;","21/Oct/19 14:04;marcuse;it gets called via {{CompactionExecutor#afterExecute}} https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1807 -> https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java#L248;;;","21/Oct/19 14:23;dcapwell;Thanks, I'll try to replicate later.;;;","21/Oct/19 22:01;dcapwell;Synced with [~marcuse] and turns out we are both correct.

 

The tests added to this Jira run with tracing enabled where as Marcus' test ran without tracing; turns out the issue in the description happens when tracing is enabled.

 

Ill repurpose this Jira to fix tracing, and update the tests to run with and without tracing.;;;","21/Oct/19 23:05;dcapwell;[~marcuse] [~jrwest] I pushed a change to my branch based off the finding that tracing is the root cause; I reverted all changed to src/java and replaced with a fix to DebuggableThreadPoolExecutor.  I also updated the test to run with and without tracing.

 

I prefer the changes I made to JVMStabilityInspector but they are no longer relevant to this Jira, so will open a different Jira and apply those changes there.

 

Please review again, thanks!;;;","21/Oct/19 23:06;dcapwell;I updated the Jira title to better reflect the issue.;;;","25/Oct/19 18:45;dcapwell;Found out that my change only got execute to work, and that submit had the same behavior with and without tracing (that the exception is lost).

I rewrote my patch to make sure execute and submit methods matches the behavior of normal ExecutorService calls, but did not loose the underline exception. 

I also added documentation to help explain how this works, though its rather ugly so the documentation may not be enough.

Let me know what you think.;;;","30/Oct/19 09:17;marcuse;this lgtm, left a few minor comments on the PR;;;","30/Oct/19 17:12;dcapwell;[~marcuse] I pushed all changes based off your feedback.

[~jrwest] can you re-review?  The changes to src/java are completely different since your last review, so good to look at that again (tests are mostly the same, small changes);;;","13/Nov/19 01:17;jwest;LGTM. Sorry for the delay. ;;;","13/Nov/19 22:39;dcapwell;thanks!  [~marcuse] everything good then?

Thanks everyone for the reviews!;;;","15/Nov/19 17:18;marcuse;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
in-JVM dtest fails on java 11 since system ClassLoader is not a URLClassLoader,CASSANDRA-15329,13257797,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,dcapwell,dcapwell,dcapwell,19/Sep/19 22:59,27/Aug/20 15:09,13/Jul/23 08:38,10/Oct/19 08:59,,,,,,Test/dtest/java,,,,0,,,,"When running the in-JVM dtests on on java 11 they fail while trying to cast the Versions.class.getClassLoader to URLClassLoader, which is no longer the default ClassLoader on java 11.",,cscotta,dcapwell,ifesdjeen,jmeredithco,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/19 23:03;dcapwell;CASSANDRA-15329.patch;https://issues.apache.org/jira/secure/attachment/12980793/CASSANDRA-15329.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,dcapwell,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Oct 10 08:59:27 UTC 2019,,,,,,,All,,,,,"0|z06u14:",9223372036854775807,,,,,,,ifesdjeen,jmeredithco,,,,,,,,https://github.com/apache/cassandra/commit/c55d727bbe8d66e87497d6c8b6301a767b11bb4c,,,,,,,,,,,,,,"25/Sep/19 20:26;jmeredithco;+1 from me, this makes it possible to run the in-JVM dtests with JDK11.  I still get some failures (including a SIGBUS inside IntellIJ when running the DistributedReadWriteTest suite, may be a local set up issue) and StreamingTest fails when run as {{ant test-jvm-dtest}} but it's much better than all tests failing.;;;","26/Sep/19 17:05;dcapwell;Moved patch to GitHub, can be found [here|https://github.com/dcapwell/cassandra/commit/15dc0531631032fe60ff52e416739f4fd3108bb2];;;","07/Oct/19 15:16;ifesdjeen;Thank you for the patch, +1!

I have only one small remark [~dcapwell], would you mind if I switched from stream API to regular array allocation? I think there's a lot of value in streams when we're using them for multi-step operations I'd still stick to array allocation. However, I do not have a strong opinion about it, just let me know and I'll commit as-is or with a proposed change. Thanks!
;;;","07/Oct/19 19:17;dcapwell;""would you mind if I switched from stream API to regular array allocation?""

 

I don't mind.  If you want I can do that later today.;;;","10/Oct/19 08:59;ifesdjeen;Thank you! Committed as [c55d727bbe8d66e87497d6c8b6301a767b11bb4c|https://github.com/apache/cassandra/commit/c55d727bbe8d66e87497d6c8b6301a767b11bb4c] to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to compute ceiling for max when histogram overflowed,CASSANDRA-15326,13256243,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,mlhsu,mlhsu,12/Sep/19 02:39,10/Sep/20 22:56,13/Jul/23 08:38,02/Sep/20 16:14,,,,,,Consistency/Batch Log,,,,0,,,,"I have 9 cassandra nodes. When I create a keyspace that has 15 tables and the record count of all table are about 8.2 billion. I imported data through my java loaders, and I found out the system.log has error message. What happened and how can I solve the error?

Exception in thread Thread[CompactionExecutor:113041,1,main] 
java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
	at org.apache.cassandra.utils.EstimatedHistogram.rawMean(EstimatedHistogram.java:231) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.utils.EstimatedHistogram.mean(EstimatedHistogram.java:220) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.io.sstable.metadata.StatsMetadata.getEstimatedDroppableTombstoneRatio(StatsMetadata.java:115) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.io.sstable.format.SSTableReader.getEstimatedDroppableTombstoneRatio(SSTableReader.java:1926) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:424) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:99) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:183) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:153) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:260) ~[apache-cassandra-3.11.4.jar:3.11.4]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_191]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_191]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_191]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_191]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.4.jar:3.11.4]
	at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_191]",,maedhroz,mlhsu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15325,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Sep 10 20:23:44 UTC 2020,,,,,,,All,,,,,"0|z06kgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/20 20:23;maedhroz;[~mlhsu] When you say ""through my java loaders"", what particular tooling would that be? Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to compute ceiling for max when histogram overflowed,CASSANDRA-15325,13256242,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,mlhsu,mlhsu,12/Sep/19 02:38,02/Sep/20 16:14,13/Jul/23 08:38,02/Sep/20 16:12,,,,,,Consistency/Batch Log,CQL/Interpreter,,,0,,,,"I have 9 cassandra nodes. When I create a keyspace that has 15 tables and the record count of all table are about 8.2 billion. I imported data through my java loaders, and I found out the system.log has error message. What happened and how can I solve the error?",,mlhsu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15326,CASSANDRA-15164,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/19 02:33;mlhsu;log.txt;https://issues.apache.org/jira/secure/attachment/12980142/log.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,2019-09-12 02:38:09.0,,,,,,,All,,,,,"0|z06kg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MessagingServiceTest failed with method listenRequiredSecureConnectionWithBroadcastAddr    ON MAC OS  ,CASSANDRA-15323,13255788,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,maxwellguo,maxwellguo,maxwellguo,10/Sep/19 03:38,15/May/20 08:36,13/Jul/23 08:38,10/Sep/19 03:40,4.0,4.0-alpha2,,,,Test/unit,,,,0,,,,"when I do unit test on mac os for cassandra4.0 tag version , I found that the unit test failed when doing MessagingServiceTest  on method listenRequiredSecureConnectionWithBroadcastAddr. 

I found out that it is because that the mac os can not get connect to ip address 127.0.0.2 on default. 
so when you doing : ant test -Dtest.name=MessagingServiceTest -Dtest.methods=listenRequiredSecureConnectionWithBroadcastAddr

you can get a bind exception : can not assign request address. 
 !exception.png! 

what to do with it ,you can just set the 127.0.0.2  by :
sudo ifconfig lo0 alias 127.0.0.2 netmask 0xFFFFFFFF

then the unit can run successfully .",,maxwellguo,polo-language,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/19 03:36;maxwellguo;exception.png;https://issues.apache.org/jira/secure/attachment/12979916/exception.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,maxwellguo,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon May 11 08:32:17 UTC 2020,,,,,,,All,,,,,"0|z06hnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/19 03:40;maxwellguo;just modify the mac os ip ;;;","11/May/20 08:32;polo-language;Adding here for reference: the problem and fix provided in the description apply to FreeBSD as well.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra 4.0-alpha1 released with SNAPSHOT dependencies,CASSANDRA-15321,13255745,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,velobr,velobr,velobr,09/Sep/19 22:28,21/Dec/20 08:07,13/Jul/23 08:38,22/Sep/19 17:47,4.0,4.0-alpha2,,,,Build,,,,0,pull-request-available,,,"I just noticed that for cassandra 4.0-alpha1, the {{cassandra-all}} has a dependency to {{chronicle-core}} version {{1.16.3-SNAPSHOT}}. and {{cassandra-driver 3.4.0-SNAPSHOT}}

[http://repo1.maven.org/maven2/org/apache/cassandra/cassandra-all/4.0-alpha1/cassandra-all-4.0-alpha1.pom]

This snapshots dependencies are not available on maven central, meaning {{cassandra-all}} can't be used as dependency for maven projects as is.

 

Also, noticed that {{carrotsearch}} was missing from the dependency list.

 

PR available on github

https://github.com/apache/cassandra/pull/358",,mck,velobr,,,,,,,,,,,,,,,,,,,,,,,,,,"michaelsembwever commented on issue #358: CASSANDRA-15321 replace SNAPSHOT dependencies with STABLE
URL: https://github.com/apache/cassandra/pull/358#issuecomment-529783038
 
 
   build failure with the driver version change. looking into it…
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Sep/19 05:58;githubbot;600","velo commented on issue #358: CASSANDRA-15321 replace SNAPSHOT dependencies with STABLE
URL: https://github.com/apache/cassandra/pull/358#issuecomment-529784558
 
 
   @mck where is the build being executed? Can I see the error?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Sep/19 06:04;githubbot;600","velo commented on issue #358: CASSANDRA-15321 replace SNAPSHOT dependencies with STABLE
URL: https://github.com/apache/cassandra/pull/358#issuecomment-529784558
 
 
   @michaelsembwever  where is the build being executed? Can I see the error?
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Sep/19 06:04;githubbot;600","michaelsembwever commented on issue #358: CASSANDRA-15321 replace SNAPSHOT dependencies with STABLE
URL: https://github.com/apache/cassandra/pull/358#issuecomment-529956940
 
 
   > where is the build being executed? Can I see the error?
   
   @velo if you do `ant antifacts` 
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Sep/19 14:17;githubbot;600","michaelsembwever commented on issue #358: CASSANDRA-15321 replace SNAPSHOT dependencies with STABLE
URL: https://github.com/apache/cassandra/pull/358#issuecomment-533822625
 
 
   > build failure with the driver version change. looking into it…
   
   if you undo the java-driver change, will maven builds using `cassandra-all` work again @velo ?
   
   (i don't believe the name of the jar files themselves impact the maven declared dependencies. the java-driver used is a problem, it contains a version and code that is not found anywhere on the java-driver github repository, but i believe this can be fixed separately and is not part of your breakage here.)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Sep/19 19:03;githubbot;600","velo commented on issue #358: CASSANDRA-15321 replace SNAPSHOT dependencies with STABLE
URL: https://github.com/apache/cassandra/pull/358#issuecomment-533833229
 
 
   > if you undo the java-driver change, will maven builds using `cassandra-all` work again @velo ?
   
   Yeah, I think that would be good enough for me.
   
   
   BTW, replacing `/lib` folder with https://ant.apache.org/ivy/ would prevent this problem (and the cassandra driver one) from ever happening again.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Sep/19 22:00;githubbot;600","asfgit commented on pull request #358: CASSANDRA-15321 replace SNAPSHOT dependencies with STABLE
URL: https://github.com/apache/cassandra/pull/358
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Sep/19 17:45;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4200,,,0,4200,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15334,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,velobr,,,,,,,,,,,,Code,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Sun Sep 22 17:47:09 UTC 2019,,,,,,,All,,,,,"0|z06hds:",9223372036854775807,,,,,,,mck,,,,Normal,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/bc5fc8bc2dc517e2749edd73f6f28be3ce2fdb95 ,,,,,,,,,maven build using cassandra-all passing,,,,,"22/Sep/19 08:43;mck;[~velobr] has taken the cassandra-driver-core update out of the patch. The names of the jars under {{lib/}} are not related to, or involving, the fault described, although and despite consistency between declared dependency versions and those bundled jar files being desired.

The cassandra-driver-core involves a more complicated issue. In Cassandra multiple ports per node was implemented in [CASSANDRA-7544|https://issues.apache.org/jira/browse/CASSANDRA-7544] and in the java-driver implemented under [JAVA-1388|https://datastax-oss.atlassian.net/browse/JAVA-1388]. What's currently included in {{lib/cassandra-driver-core-3.4.0-shaded.jar}} is a custom build of code that is not found in any of the github repo's code (branches or tags). It was built off a [forked branch|https://github.com/datastax/java-driver/pull/931] that was never accepted into the driver. It was implemented instead by the java-driver team in a different [way|https://github.com/datastax/java-driver/pull/1065]. Restoring the version of the java-driver used has been entered in as issue [CASSANDRA-15334|https://issues.apache.org/jira/browse/CASSANDRA-15334].;;;","22/Sep/19 17:47;mck;Committed as bc5fc8bc2dc517e2749edd73f6f28be3ce2fdb95;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix failing test - test_rolling_upgrade_with_internode_ssl - upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD,CASSANDRA-15315,13255381,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,vinaykumarcse,vinaykumarcse,06/Sep/19 22:43,21/Dec/20 08:07,13/Jul/23 08:38,18/Mar/20 20:37,4.0,4.0-alpha4,,,,Test/dtest/python,,,,0,,,,"Example failure:

[https://circleci.com/gh/vinaykumarchella/cassandra/468#tests/containers/11]

[https://circleci.com/gh/vinaykumarchella/cassandra/451#tests/containers/11]


{code:java}
ccmlib.node.TimeoutError: 06 Sep 2019 20:21:39 [node2] Missing: ['127.0.0.1.* now UP']: INFO  [HANDSHAKE-/127.0.0.1] 2019-09-06 20:17:43,8..... See system.log for remainder
self = <upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD object at 0x7fbb75245a90>

    @pytest.mark.timeout(3000)
    def test_rolling_upgrade_with_internode_ssl(self):
        """"""
            Rolling upgrade test using internode ssl.
            """"""
>       self.upgrade_scenario(rolling=True, internode_ssl=True)

upgrade_tests/upgrade_through_versions_test.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
upgrade_tests/upgrade_through_versions_test.py:352: in upgrade_scenario
    self.upgrade_to_version(version_meta, partial=True, nodes=(node,), internode_ssl=internode_ssl)
upgrade_tests/upgrade_through_versions_test.py:456: in upgrade_to_version
    node.start(wait_other_notice=240, wait_for_binary_proto=True)
../env/src/ccm/ccmlib/node.py:751: in start
    node.watch_log_for_alive(self, from_mark=mark, timeout=wait_other_notice)
../env/src/ccm/ccmlib/node.py:568: in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ccmlib.node.Node object at 0x7fbb7538b748>
exprs = ['127.0.0.1.* now UP'], from_mark = 151813, timeout = 240
process = None, verbose = False, filename = 'system.log'

    def watch_log_for(self, exprs, from_mark=None, timeout=600, process=None, verbose=False, filename='system.log'):
        """"""
            Watch the log until one or more (regular) expression are found.
            This methods when all the expressions have been found or the method
            timeouts (a TimeoutError is then raised). On successful completion,
            a list of pair (line matched, match object) is returned.
            """"""
        start = time.time()
        tofind = [exprs] if isinstance(exprs, string_types) else exprs
        tofind = [re.compile(e) for e in tofind]
        matchings = []
        reads = """"
        if len(tofind) == 0:
            return None
    
        log_file = os.path.join(self.get_path(), 'logs', filename)
        output_read = False
        while not os.path.exists(log_file):
            time.sleep(.5)
            if start + timeout < time.time():
                raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Timed out waiting for {} to be created."".format(log_file))
            if process and not output_read:
                process.poll()
                if process.returncode is not None:
                    self.print_process_output(self.name, process, verbose)
                    output_read = True
                    if process.returncode != 0:
                        raise RuntimeError()  # Shouldn't reuse RuntimeError but I'm lazy
    
        with open(log_file) as f:
            if from_mark:
                f.seek(from_mark)
    
            while True:
                # First, if we have a process to check, then check it.
                # Skip on Windows - stdout/stderr is cassandra.bat
                if not common.is_win() and not output_read:
                    if process:
                        process.poll()
                        if process.returncode is not None:
                            self.print_process_output(self.name, process, verbose)
                            output_read = True
                            if process.returncode != 0:
                                raise RuntimeError()  # Shouldn't reuse RuntimeError but I'm lazy
    
                line = f.readline()
                if line:
                    reads = reads + line
                    for e in tofind:
                        m = e.search(line)
                        if m:
                            matchings.append((line, m))
                            tofind.remove(e)
                            if len(tofind) == 0:
                                return matchings[0] if isinstance(exprs, string_types) else matchings
                else:
                    # yep, it's ugly
                    time.sleep(1)
                    if start + timeout < time.time():
>                       raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
E                       ccmlib.node.TimeoutError: 06 Sep 2019 20:21:39 [node2] Missing: ['127.0.0.1.* now UP']:
E                       INFO  [HANDSHAKE-/127.0.0.1] 2019-09-06 20:17:43,8.....
E                       See system.log for remainder

../env/src/ccm/ccmlib/node.py:536: TimeoutError
{code}
",,csplinter,e.dimitrova,jeromatron,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/20 20:34;e.dimitrova;CASSANDRA-15315.txt;https://issues.apache.org/jira/secure/attachment/12997059/CASSANDRA-15315.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Mar 18 20:36:26 UTC 2020,,,,,,,All,,,,,"0|z06f4w:",9223372036854775807,,,,,,,e.dimitrova,,,,Normal,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra-dtest/commit/ed55a4961f424f8456e125fdeb70ca644e8572c9,,,,,,,,,none,,,,,"13/Mar/20 15:28;brandon.williams;This is also solved by the patch on CASSANDRA-15314.

https://github.com/driftx/cassandra-dtest/tree/CASSANDRA-15314;;;","17/Mar/20 19:37;e.dimitrova;As we talked already on Slack last week, unfortunately, the tests are still flakey for me with the same failure.

Let me know if you want me to get back to these now when I can again run them only machine.

Turning back this to in progress. Thanks;;;","18/Mar/20 14:49;brandon.williams;Does the revised solution from CASSANDRA-15314 also solve this one for you?;;;","18/Mar/20 17:23;e.dimitrova;I ran it already 13 times in a row without failures. I am planning to wait until 20 and I guess we can close it then
I'll ping you on slack;;;","18/Mar/20 20:35;e.dimitrova;23 successful runs.
Issues solved, closing. Thanks for all the work!!;;;","18/Mar/20 20:36;e.dimitrova;Patch already committed in CASSANDRA-15314;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix failing test - test_rolling_upgrade_with_internode_ssl - upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_EndsAt_Trunk_HEAD,CASSANDRA-15314,13255380,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,vinaykumarcse,vinaykumarcse,06/Sep/19 22:41,21/Dec/20 08:07,13/Jul/23 08:38,18/Mar/20 14:48,4.0,4.0-alpha4,,,,Test/dtest/python,,,,0,dtest,,,"Example failure: [https://circleci.com/gh/vinaykumarchella/cassandra/468#tests/containers/11]

 
{code:java}
ccmlib.node.TimeoutError: 06 Sep 2019 20:23:57 [node2] Missing: ['127.0.0.1.* now UP']: INFO  [HANDSHAKE-/127.0.0.1] 2019-09-06 20:20:01,7..... See system.log for remainder
self = <upgrade_tests.upgrade_through_versions_test.TestProtoV4Upgrade_AllVersions_EndsAt_Trunk_HEAD object at 0x7f6d90d43b38>

    @pytest.mark.timeout(3000)
    def test_rolling_upgrade_with_internode_ssl(self):
        """"""
            Rolling upgrade test using internode ssl.
            """"""
>       self.upgrade_scenario(rolling=True, internode_ssl=True)

upgrade_tests/upgrade_through_versions_test.py:296: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
upgrade_tests/upgrade_through_versions_test.py:352: in upgrade_scenario
    self.upgrade_to_version(version_meta, partial=True, nodes=(node,), internode_ssl=internode_ssl)
upgrade_tests/upgrade_through_versions_test.py:456: in upgrade_to_version
    node.start(wait_other_notice=240, wait_for_binary_proto=True)
../env/src/ccm/ccmlib/node.py:751: in start
    node.watch_log_for_alive(self, from_mark=mark, timeout=wait_other_notice)
../env/src/ccm/ccmlib/node.py:568: in watch_log_for_alive
    self.watch_log_for(tofind, from_mark=from_mark, timeout=timeout, filename=filename)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ccmlib.node.Node object at 0x7f6d90e705f8>
exprs = ['127.0.0.1.* now UP'], from_mark = 150742, timeout = 240
process = None, verbose = False, filename = 'system.log'

    def watch_log_for(self, exprs, from_mark=None, timeout=600, process=None, verbose=False, filename='system.log'):
        """"""
            Watch the log until one or more (regular) expression are found.
            This methods when all the expressions have been found or the method
            timeouts (a TimeoutError is then raised). On successful completion,
            a list of pair (line matched, match object) is returned.
            """"""
        start = time.time()
        tofind = [exprs] if isinstance(exprs, string_types) else exprs
        tofind = [re.compile(e) for e in tofind]
        matchings = []
        reads = """"
        if len(tofind) == 0:
            return None
    
        log_file = os.path.join(self.get_path(), 'logs', filename)
        output_read = False
        while not os.path.exists(log_file):
            time.sleep(.5)
            if start + timeout < time.time():
                raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Timed out waiting for {} to be created."".format(log_file))
            if process and not output_read:
                process.poll()
                if process.returncode is not None:
                    self.print_process_output(self.name, process, verbose)
                    output_read = True
                    if process.returncode != 0:
                        raise RuntimeError()  # Shouldn't reuse RuntimeError but I'm lazy
    
        with open(log_file) as f:
            if from_mark:
                f.seek(from_mark)
    
            while True:
                # First, if we have a process to check, then check it.
                # Skip on Windows - stdout/stderr is cassandra.bat
                if not common.is_win() and not output_read:
                    if process:
                        process.poll()
                        if process.returncode is not None:
                            self.print_process_output(self.name, process, verbose)
                            output_read = True
                            if process.returncode != 0:
                                raise RuntimeError()  # Shouldn't reuse RuntimeError but I'm lazy
    
                line = f.readline()
                if line:
                    reads = reads + line
                    for e in tofind:
                        m = e.search(line)
                        if m:
                            matchings.append((line, m))
                            tofind.remove(e)
                            if len(tofind) == 0:
                                return matchings[0] if isinstance(exprs, string_types) else matchings
                else:
                    # yep, it's ugly
                    time.sleep(1)
                    if start + timeout < time.time():
>                       raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
E                       ccmlib.node.TimeoutError: 06 Sep 2019 20:23:57 [node2] Missing: ['127.0.0.1.* now UP']:
E                       INFO  [HANDSHAKE-/127.0.0.1] 2019-09-06 20:20:01,7.....
E                       See system.log for remainder

../env/src/ccm/ccmlib/node.py:536: TimeoutError{code}",,brandon.williams,csplinter,e.dimitrova,jasonstack,jeromatron,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/20 13:08;e.dimitrova;CASSANDRA-15314.txt;https://issues.apache.org/jira/secure/attachment/12997023/CASSANDRA-15314.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Mar 18 13:07:18 UTC 2020,,,,,,,All,,,,,"0|z06f4o:",9223372036854775807,,,,,,,e.dimitrova,,,,Normal,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra-dtest/commit/ed55a4961f424f8456e125fdeb70ca644e8572c9,,,,,,,,,none,,,,,"15/Jan/20 18:08;e.dimitrova;[~vinaykumarcse], please, correct me if I'm wrong but I think this one is a duplicate of CASSANDRA-15315? Shall we close this one and work on the other one?;;;","16/Jan/20 01:33;vinaykumarcse;I believe these two(TestProtoV4Upgrade_AllVersions_EndsAt_Trunk_HEAD, TestProtoV4Upgrade_AllVersions_RandomPartitioner_EndsAt_Trunk_HEAD) are different test cases, could be failing for the same reason. I would be happy to help with review both if you have a patch.;;;","16/Jan/20 03:26;e.dimitrova;I just started looking into it. I will let you know when there is a patch available for review. Thanks ;;;","29/Jan/20 23:08;e.dimitrova;Changed wait_other_notice from 240 to 400 in node.start on start with the new version after upgrade.

Made it only flakey. Should inspect the logs more deeply to find what I am missing in the big picture.

 

 ;;;","12/Feb/20 20:44;e.dimitrova;[~rssvihla] was looking into this one but now he is on vacation for the next two weeks, not sure about the latest update whether he managed to find out something more.

[~brandon.williams] do you think you can also look into it when you have some time, please?;;;","13/Mar/20 15:27;brandon.williams;Patch here: https://github.com/driftx/cassandra-dtest/tree/CASSANDRA-15314 which when upgrading beyond 3.11 and using internode ssl, modifies all the seeds to add port 7001.  This also solves the flakiness in CASSANDRA-15315.;;;","17/Mar/20 19:36;e.dimitrova;As we talked already on Slack last week, unfortunately, the tests are still flakey for me with the same failure.

Let me know if you want me to get back to these now when I can again run them only machine.

Turning back this to in progress. Thanks;;;","18/Mar/20 13:06;e.dimitrova;[~brandon.williams], your patch + the following on line 463 looks like solves the issue with this test:
node.start(wait_other_notice=400, wait_for_binary_proto=True)
35 times successful run.
I will attach the log.
Please add this line and I think it is ready for commit;;;","18/Mar/20 13:07;e.dimitrova;Further to the suggested patch, line 463 should be corrected to: node.start(wait_other_notice=400, wait_for_binary_proto=True)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flaky - ChecksummingTransformerTest - org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest,CASSANDRA-15313,13255377,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,vinaykumarcse,vinaykumarcse,06/Sep/19 22:06,21/Dec/20 08:56,13/Jul/23 08:38,16/Dec/20 16:06,4.0,4.0-beta4,,,,Test/unit,,,,0,,,,"During the recent runs, this test appears to be flaky.

Example failure: [https://circleci.com/gh/vinaykumarchella/cassandra/459#tests/containers/94]

corruptionCausesFailure-compression - org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest
{code:java}
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at org.quicktheories.impl.Precursor.<init>(Precursor.java:17)
	at org.quicktheories.impl.ConcreteDetachedSource.<init>(ConcreteDetachedSource.java:8)
	at org.quicktheories.impl.ConcreteDetachedSource.detach(ConcreteDetachedSource.java:23)
	at org.quicktheories.generators.Retry.generate(CodePoints.java:51)
	at org.quicktheories.generators.Generate.lambda$intArrays$10(Generate.java:190)
	at org.quicktheories.generators.Generate$$Lambda$17/1847008471.generate(Unknown Source)
	at org.quicktheories.core.DescribingGenerator.generate(Gen.java:255)
	at org.quicktheories.core.Gen.lambda$map$0(Gen.java:36)
	at org.quicktheories.core.Gen$$Lambda$20/71399214.generate(Unknown Source)
	at org.quicktheories.core.Gen.lambda$map$0(Gen.java:36)
	at org.quicktheories.core.Gen$$Lambda$20/71399214.generate(Unknown Source)
	at org.quicktheories.core.Gen.lambda$mix$10(Gen.java:184)
	at org.quicktheories.core.Gen$$Lambda$45/802243390.generate(Unknown Source)
	at org.quicktheories.core.Gen.lambda$flatMap$5(Gen.java:93)
	at org.quicktheories.core.Gen$$Lambda$48/363509958.generate(Unknown Source)
	at org.quicktheories.dsl.TheoryBuilder4.lambda$prgnToTuple$12(TheoryBuilder4.java:188)
	at org.quicktheories.dsl.TheoryBuilder4$$Lambda$40/2003496028.generate(Unknown Source)
	at org.quicktheories.core.DescribingGenerator.generate(Gen.java:255)
	at org.quicktheories.core.FilteredGenerator.generate(Gen.java:225)
	at org.quicktheories.core.Gen.lambda$map$0(Gen.java:36)
	at org.quicktheories.core.Gen$$Lambda$20/71399214.generate(Unknown Source)
	at org.quicktheories.impl.Core.generate(Core.java:150)
	at org.quicktheories.impl.Core.shrink(Core.java:103)
	at org.quicktheories.impl.Core.run(Core.java:39)
	at org.quicktheories.impl.TheoryRunner.check(TheoryRunner.java:35)
	at org.quicktheories.dsl.TheoryBuilder4.check(TheoryBuilder4.java:150)
	at org.quicktheories.dsl.TheoryBuilder4.checkAssert(TheoryBuilder4.java:162)
	at org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest.corruptionCausesFailure(ChecksummingTransformerTest.java:87)
{code}",,aholmber,colinkuo,csplinter,dcapwell,eduard.tudenhoefner,jasonstack,jeromatron,jmckenzie,jwest,maedhroz,polo-language,samt,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,CASSANDRA-15299,,CASSANDRA-15786,,,,,,CASSANDRA-15299,CASSANDRA-15554,CASSANDRA-15556,,,,,,,,,,,,,,,,,,,,,,"06/Feb/20 05:10;dcapwell;CASSANDRA-15313-hack.patch;https://issues.apache.org/jira/secure/attachment/12992749/CASSANDRA-15313-hack.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Dec 16 16:04:02 UTC 2020,,,,,,,All,,,,,"0|z06f40:",9223372036854775807,,,,,,,,,,,Normal,,,,,,,,,,,,,,Trying to get this thing to Blocked. That's the plan.,,,,,"04/Feb/20 21:58;brandon.williams;This is mostly due to the two places where a rather large example size is used, 500.  I did many runs lowering this number until I could be sure there were no timeouts, and that number is 35.  However, once the test was not always timing out (at around 40 examples) it discovered a seed that always fails:
{code:java}
[junit-timeout] Seed was 552252992721215
[junit-timeout]         at org.quicktheories.core.ExceptionReporter.falsify(ExceptionReporter.java:43)
[junit-timeout]         at org.quicktheories.core.ExceptionReporter.falisification(ExceptionReporter.java:37)
[junit-timeout]         at org.quicktheories.impl.TheoryRunner.reportFalsification(TheoryRunner.java:48)
[junit-timeout]         at org.quicktheories.impl.TheoryRunner.check(TheoryRunner.java:37)
[junit-timeout]         at org.quicktheories.dsl.TheoryBuilder4.check(TheoryBuilder4.java:150)
[junit-timeout]         at org.quicktheories.dsl.TheoryBuilder4.checkAssert(TheoryBuilder4.java:162)
[junit-timeout]         at org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest.corruptionCausesFailure(ChecksummingTransformerTest.java:88)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Test org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest FAILED
{code}
Sometimes instead of failing the test, it will crash the jvm here:
{code:java}
[junit-timeout] # C  [liblz4-java4100719362944375598.so+0x5eb7]  LZ4_decompress_fast+0x117
{code}
I've reproduced this against lz4 1.4.0, 1.4.1, 1.5.0, 1.5.1, 1.6.0, 1.7.0, and 1.7.1. I think beyond the flakiness which can be solved by reducing the amount of examples, we've got a legitimate bug in lz4. I pushed the branch to reproduce here: [https://github.com/driftx/cassandra/tree/CASSANDRA-15313] You'll need to override the test timeout as this takes around 7 minutes.
 WDYT, [~samt]?;;;","05/Feb/20 15:23;samt;Probably not a surprise, but it doesn't seem to repro on mac or in circle. I do see the timeout on linux with lz4 1.7.1, although I didn't manage to trigger the crash. I agree looks like maybe a bug in lz4, I don't have time right now, but when I do I'll pull out a minimal repro and submit it to the lz4 project.

 

+1 on lowering the number of examples to stabilise the test.;;;","05/Feb/20 17:20;brandon.williams;bq.  I do see the timeout on linux with lz4 1.7.1, although I didn't manage to trigger the crash

You probably need to override the test timeout so it can complete.

bq. +1 on lowering the number of examples to stabilise the test.

I committed the reduction to 35 examples.;;;","05/Feb/20 17:30;dcapwell;Lowering the examples means we need more runs to see it.  Looking at the JVM we spin up I see its -Xmx1024m so we have 1g of heap; If you run the test with 2-4 does it still fail?

CI runs the test with the following

{code}
ant testclasslist -Dtest.classlistfile=<(echo org/apache/cassandra/transport/frame/checksum/ChecksummingTransformerTest.java) -Dtest.classlistprefix=unit
{code}

Quickly looking at the code it transfers Strings as a wrapper for byte[].  This is more garbage to generate and toss so could also be reduced by using byte[] instead.  

Do we have a heap dump of the test so we know what the top memory was from?  500 isn't a large example size, so this sounds more like we could improve the generators.;;;","06/Feb/20 01:46;brandon.williams;bq. Lowering the examples means we need more runs to see it.

Sorry, what is 'it' here?

bq.  If you run the test with 2-4 does it still fail?

I ran at 4G and it still failed at 200 examples (I didn't try any other sizes though);;;","06/Feb/20 05:08;dcapwell;""it"" is the failing case; sorry for the confusion.  By lowering the examples we will need to run the test more before we see the failing case.

bq. I ran at 4G and it still failed at 200 examples (I didn't try any other sizes though)

This then sounds like the generators produce too much garbage. I took a stab at fixing it and attaching a hack patch to the JIRA to show; basically move away from String since that produces a ton of garbage.  With the patch and a failing test case, the test completes in 200ms (900ms if you run in isolation).

[~samt] don't know the test well but when I run it (not patched) in a debugger with 1000 examples, I see the call https://github.com/apache/cassandra/blob/trunk/test/unit/org/apache/cassandra/transport/frame/checksum/ChecksummingTransformerTest.java#L118 2 times; so only 2 out of 1k runs hits that check.  My patch doesn't hit it (after a few runs), but could always tweak the bytes to match the expected input (always throws ProtocolException).;;;","06/Feb/20 15:36;jwest;[~samt] actually included the test on my behalf. Its not surprising that this is a memory issue. Thats one motivation for https://issues.apache.org/jira/browse/CASSANDRA-15348 which we could convert to when ready. But in the meantime let me see what we can do to reduce the memory usage of the test. 

Interesting its also potentially finding an issue with LZ4. ;;;","06/Feb/20 16:12;brandon.williams;bq. ""it"" is the failing case; sorry for the confusion. By lowering the examples we will need to run the test more before we see the failing case.

That is not true, though.  The key to reproducing the failing case is defining the seed that makes it fail, which can then reproduce it with only 20 examples (I updated my branch to reflect this, and isolated it to the corruptionCausesFailure test.  You still need to override the timeout to be long enough for it to fail, I use -Dtest.timeout=60000000)  I probably did a few thousand runs before I came across that seed, though.

bq.  I took a stab at fixing it and attaching a hack patch to the JIRA to show; 

This looks good but you also removed the examples from corruptionCausesFailure.  With them added back everything seems to pass fine at 200 examples.  I'll try another run with 500 and report back.;;;","06/Feb/20 16:18;jwest;Not a huge fan of fixing the seed. The intention is for the test to search different permutations each time its run, not a fixed set (the seed is meant to help debug once you find that failure -- assuming its deterministic). One seed may find the original bug this test found but not other/future bugs. 

Ideally we'd keep the example count high but balancing that against memory usage and runtime goals/constraints so we don't make the suites unnecessarily long for code that hasn't changed. ;;;","06/Feb/20 16:21;brandon.williams;bq. Not a huge fan of fixing the seed.

I'm not suggesting that, I only kept it to demonstrate the failure that is possibly an LZ4 bug.;;;","06/Feb/20 16:21;jwest;Ah sorry misunderstood. Definitely good to keep that around.;;;","06/Feb/20 16:31;dcapwell;bq. That is not true, though. The key to reproducing the failing case is defining the seed that makes it fail, which can then reproduce it with only 20 examples... I probably did a few thousand runs before I came across that seed, though.

That was the point I was making, you need to run the test a lot more to find the seed.  The default is 1k examples so based off your statement < 30 runs should be enough to reproduce with default, with 35 examples we need a few thousand runs.

The seed in your branch is 552252992721215L which produces this for me (without my patch)

{code}
INFO  [main] 2020-02-06 08:27:52,044 ChecksummingTransformer.java:348 - IO error during decompression of frame body chunk
java.io.IOException: Error caught during LZ4 decompression
	at org.apache.cassandra.transport.frame.compress.LZ4Compressor.decompress(LZ4Compressor.java:65)
	at org.apache.cassandra.transport.frame.checksum.ChecksummingTransformer.maybeDecompress(ChecksummingTransformer.java:344)
	at org.apache.cassandra.transport.frame.checksum.ChecksummingTransformer.transformInbound(ChecksummingTransformer.java:292)
	at org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest.roundTripWithCorruption(ChecksummingTransformerTest.java:114)
	at org.quicktheories.dsl.TheoryBuilder4.lambda$checkAssert$9(TheoryBuilder4.java:163)
	at org.quicktheories.dsl.TheoryBuilder4.lambda$check$8(TheoryBuilder4.java:151)
	at org.quicktheories.impl.Property.tryFalsification(Property.java:23)
	at org.quicktheories.impl.Core.shrink(Core.java:111)
	at org.quicktheories.impl.Core.run(Core.java:39)
	at org.quicktheories.impl.TheoryRunner.check(TheoryRunner.java:35)
	at org.quicktheories.dsl.TheoryBuilder4.check(TheoryBuilder4.java:150)
	at org.quicktheories.dsl.TheoryBuilder4.checkAssert(TheoryBuilder4.java:162)
	at org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest.corruptionCausesFailure(ChecksummingTransformerTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
Caused by: net.jpountz.lz4.LZ4Exception: Error decoding offset 11025 of input buffer
	at net.jpountz.lz4.LZ4JNIFastDecompressor.decompress(LZ4JNIFastDecompressor.java:39)
	at net.jpountz.lz4.LZ4FastDecompressor.decompress(LZ4FastDecompressor.java:100)
	at org.apache.cassandra.transport.frame.compress.LZ4Compressor.decompress(LZ4Compressor.java:61)
	... 35 common frames omitted
{code}

and 

{code}
java.lang.AssertionError: Property falsified after 15 example(s) 
Smallest found falsifying value(s) :-
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccccdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecddcddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdcdccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcccccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdccceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedceccceccceccccccccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccccdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdcccccccecccccccccceccccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccccccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
Cause was :-
java.lang.AssertionError: expected:<UnpooledHeapByteBuf(ridx: 0, widx: 25159, cap: 25159/25159)> but was:<UnpooledSlicedByteBuf(ridx: 0, widx: 25159, cap: 25159/25159, unwrapped: UnpooledHeapByteBuf(ridx: 0, widx: 48912, cap: 48912/48912))>
	at org.junit.Assert.fail(Assert.java:88)
	at org.junit.Assert.failNotEquals(Assert.java:834)
	at org.junit.Assert.assertEquals(Assert.java:118)
	at org.junit.Assert.assertEquals(Assert.java:144)
	at org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest.roundTripWithCorruption(ChecksummingTransformerTest.java:118)
	at org.quicktheories.dsl.TheoryBuilder4.lambda$checkAssert$9(TheoryBuilder4.java:163)
	at org.quicktheories.dsl.TheoryBuilder4.lambda$check$8(TheoryBuilder4.java:151)
	at org.quicktheories.impl.Property.tryFalsification(Property.java:23)
	at org.quicktheories.impl.Core.shrink(Core.java:111)
	at org.quicktheories.impl.Core.run(Core.java:39)
	at org.quicktheories.impl.TheoryRunner.check(TheoryRunner.java:35)
	at org.quicktheories.dsl.TheoryBuilder4.check(TheoryBuilder4.java:150)
	at org.quicktheories.dsl.TheoryBuilder4.checkAssert(TheoryBuilder4.java:162)
	at org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest.corruptionCausesFailure(ChecksummingTransformerTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)

Other found falsifying value(s) :- 
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccccdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecddcddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdcdccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdccceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedceccceccceccccccccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccccdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdcccccccecccccccccceccccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccccccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccccdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecdecddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdcdccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdccceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedceccceccceccccccccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccccdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdcccccccecccccccccceccccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccccccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccccdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecdecddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdcdccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdccceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedcecccecccecccccdccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccccdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdcccccccecccccccccceccccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccccccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccccdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecdecddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdddccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdccceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedcecccecccecccccdccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccccdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdcccccccecccccccccceccccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccccccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccccdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecdecddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdddccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdccceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedcecccecccecccccdccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccdcdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdcccccccecccccccccceccccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccccccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccccdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecdecddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdddccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdccceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedcecccecccecccccdccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccdcdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdccccccceccccccccccedcccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccccccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccccdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecdecddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdddccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdcdceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedcecccecccecccccdccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccdcdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdccccccceccccccccccedcccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccccccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccdcdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecdecddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdddccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdcdceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedcecccecccecccccdccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccdcdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdccccccceccccccccccedcccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccccccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccdcdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecdecddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdddccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdcdceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccccdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedcecccecccecccccdccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccdcdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdccccccceccccccccccedcccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccdcccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
{(dcecdeccdcdccedcedcdeddcccceddeddeeedcecdeedcdcdceccceceeeccdceceddeeedecdececddecdedeccccceccedcdcccccecdeccdecdececdddcceddcdeeccccccdcdedcdedcddececceeeecceddcccededdeddcddddecdccdccdeeedccdecceceeccccdccdccedcdddcdeeddddcdcddecddddccddecedcceddcecddeeddecddccccddcddddcdcceedceeeccccededceeddcedccccdddcdcccddeedecddcceceedceccdeddcecccccdcecddcdcedecececdceedccdeeccdcccccdcccceedececedccdeedceddcdddedecccdeecedcccceccddcddddceccccecdccccccdcecddddeeceeedceccecccdccedddddcdcdccceccddccccccdedcccecddcccddcdeccccdcccccedceccccdcdcddcdccdcdcdccddcceecdceeccedceccceddecdcdcccdccddcddcccdceecccceccdccdddcecceccedccecddcdcedccceccdcccdeccdccdceccccecceccdcdcecddeeddecccdcddccceecccddcccddddcdcddedceecceeddeccddcccccccdecdcccccccceccddceccdcceecddcecddccecccddcecdceddcccecdcccecdddcdcceddcecedceeccccccccccecdcddccccdccccdcccccccccdcccdccccccccedecdecccceccecdccccccccdeeeeedceccdceccddcccdeccdccceeccccedddcecdcdcdcdeddecddccddddcdddcdcecdceeeeecdcdccecceccdcccddccceccccdecdcddccedccdceccecccecccecccccccdccdccdedceddeddedcccccdccceccccdcecccccddcddcdcddeeccddccccdccceedccdccdcdcecccdcdcdcddedcedcccdedceccccdccccdddcccdedceceededdcccdcdeddcccdcdccceedccccccecdeecccccccdcccccccccccecdeeccddecccdcccdedcdcccceecddccdecdccccddecdcdcccccccecccdccccdcdddcccdccccddcdeeddccecccceddcccccdeccddccceecccdddcccdccdcccceceddecddcceccccccdccdcccccddeccedceccdcdeccccccdcccddddedcccedccdcdcccdcccccccccdceeecddedcccecccccccdccdccecccdecceecddccddcccccccccdccccccccdecdcccecccddccddcdecccecccccccdedcdccedceeccccedccdcccdcdcdedecccccccdccddccdcecdcdcccdccdeecceccdcddccddcdcccdeeccccdeccccecceceeedddcdcecedcccdddcccccececcdccececddccddcdececccdedcccddececccdcdcddcdddcedcccecdedddecedccdecccceecdedcccceedddedccccdcccccccdeeddceccccccdccccecccccdeccddeddcdddeccececccceeecdeceeddccccececdcdccdcccecddccdccdccdeccdcdcceccccddeddccdcdecdeeccccccedcdccddecdddcdecccdddddceccdcccdcececccccdedcdcdddceecdceccedecececddcddccecccdecddcdcedddddccceddeddcccdccdeccdedcdccececdccdccdccdeddceceecccdeccdcceccdccdecdccceccccccccdddcecccceddcecccedccccedccecccdcddccddccdccccccccccdcdccecdcceccccccddcccecdccccdccdcdecccdccddccceeddccdcccdcceeecccccdccccccdeddccdcccccccdccecddcddddddcccdeeedeeecdddccccdccdccccccddcdecddedccccdecdccecdcecccccdccdcceecccccdcccddccdccccccdcdceeccececccccccccdcedcceccddedcdcceeedccceeccccccdccccccdeeeecccdceddcdcccecccdddcecdccccddcededeccdeceeeddccdeeccccccddcededdcdcedceddeccceeecedcccddedeccccccccccdccdcdcccdececccdcdcedcdccdcddddcdedeedccceedddccdecccceecddecddedccecccdccccdccddcdcecedccdcdeccceceddccdecccdccecccccddcccccededcccecddceddccccdccdecddcecceeedccceddeccdccdeddcccdcccccccccdceddccdccccdecdedeececcdceccdcededdddedeccdddccdeccceeccdeccccccdedccdeceeddeccdccedccccdecdcceeccceecdceccecccccddcdcccdcecccddcdeceecccdedcceeeddcceccecccccccdedececccdececccccdeccccdcceccccdcdedecccccdcceccddccddcdccecdedceddccccccdcccecccdddccccddecddedeeeeddcccdeecccecdcededdccdcceccccddeccdcceecdeeedecdcccccdeeeecddcccdccddcedcdecedccccceccceeccdddcdeeccccccddcddedccedccceccccdcdcdecdecddecdeecdccedddcceccccedcccecddddcdeccccdccccccdddedccdccccdcccccccccececcceceecccddcccdcccdcddcccccccceceddccedcedcdcceddcecdcccedccccccccedecedccccdccceccecdcdccccccceeddceedecccedcecccdeceddcdccecceddcccedececcdeeccccccdcddccddddcccccccccccdcccdeccdccdcdcecccddecdcdccddcccedcdcecddcdccecdcedeecdccdedccedeeccccecccccccecceeeccddcccdcccdeccceccdeccdccecdccccccedccecccccdccccdccdeccdccceeccdccddcddccddcceccccccccdcddccdcdcccccccecdcceeccccccccdeecccdccdccdcdccddcddcedcdeccdedcececcccececcccecccccdccccdccdecedcccdceecededcecceccecdcccdccccccdccccccdccdddccecccdcccccccecccedcceeeccceecccccccceccdccccecccdcecedcdceccccdcdccdccccecdcccecceecccccdccdccecccdcccddccccdcccccccdecccccddedececeeedccccdccdccccccccdededdcdcccdcccccdccccccccedcccecdddcdceeecdcccccccccdcdccdccccccccececdddcddedccccccdccdceccececceecececcccceeeccccecccceddcccdcdcedecccccceccceeecccccceecccccdcecccdeccdcdcccccdeccedeccccdcdddecdcdecdccceccdcdecedccccccdcccccedddcccccccecdcdceecddccddddcccecddceccdcdcddcdccccdcccecdccdccceceeeeccddccddccccdcedcddccedcdcdcecdccdcdcececeeccdcccccccdcececceceecccdcddccddcdcececcdccdccceccccccccdcceccccccccceccdedccddccccccdedccccceccedccdedccccedccdcdececccccccedcccedcdcdcccececccdecddccedcdcddceeecdccdedccecdccdeccdccdcdccdedeecccdcedeeddecdddecdccccdcdcccecdccdccecccdddcccccdccccccdcdcccdeccdeccccececccccddcccccccedcdcccccdccccedcedcceeccccecddddedcdeececdddccedcdecdcdcdeeccccccccccdcccedcccdddccceddedeeccddccdddecddecdcccedccecccccdcceccecccdccecddceccecedccecedcecccccdccdedcccedeccecccecccccccdccedccccdccecdccddedccdcccccccddecdddccecdcdcccdedccedcdeedeecccdccdddcddcdeddcccedceddccccccdccccdccdccccccccecdcccddccccccdccdddcdddccddecccceccdecccddccccdcccdccccdccceccccdcceccccdedcdcccddedcdcccecdececccccccdddcccedeccecddcccecdccccccdccdcccedccdeedcecedcddccceccccdccddcdecccdccccccccceccdcdcccdeddddcdcecdcccedcceddecdccccceccdcccddeccdecccceccedccedecececcdcccdccdcdecccdceceedcccdccddccccddddcddddcdccdcddcccdeceeccceccdeeccccccccdcccedccecececdddcecccdcecdeccccccccdcccdcccccccdddccccccccccccdcccecccececcceccccccececdcccdccccceccddcedcccccccdcddcceccceccdccccccccdecececcccdcddccdccdedcedcecccceccdcdedcedccdedceccddcccdccdccdcccccccccdccdceeccdcccdccccecccdcddcedccccddcddececcccccdccecdccdedccddccccdccededdecccececcdededeecdcddcdcececdcccccdccedccdcccccdcceccceccdcccccccccdccccdcdeccccccdcccdcdcdcddedcecccdececcccceecccceccccccccceccccecdcdecccdcccccdcccddddcccdcccccddcccddccccccceececcceeeccddccdcccedcdcddccdddceecdcccccdddcccccecdcecccccdccccccddcdccccccdecccdcddddecccccdccecdceccdcccccddccececccecdcedecddcccedddceddcdcccecdecccdcdccecdccddcdcdccccccceccddccddecdcececdceccdcccccecdedcdddccdeccddccccccdccccccccccccceecdcddecdddcdcdeddceddcccccccccdecccccccedccdddcdecccdddcccceeeccdeddcdcddcdcdccccdedddcccecccddccccecdccccdcdecdddcdcccdccdecdeccdceceedcccededecdcccecdccdcdccdcecdddccceceddcceccccdcccccccdeecddccdcecccccccdcddcedeccccececdddcccddccccccceeccdcccecddddcccccccccdcdcecceccdcccccdcdeccddeeecdccdcdcddccddcccddeeedcdcececdceddccccdcecddccdcccedcdcdccccdddecccdddccceeecccdcccccceccccdecccddccccddceceeccdceecccecddccccceccddcecdececdcecdceceecdccecccddceccececccecdcccdecccccccedccdccccecccdcedcddccececdddceddedcdedcccceedeedcdcdedddcccdccceedcdcededdddecccdccdcecccdeccecccdccceedceedcdcccdcdcecedccdeddcdddecccdccccccdceccdcdcccedcecccedcdcedcecdcccccccdccdceedcdcecddcdcddcccceccccccddccddecccdcdcdcceccdeecdecececcccdccedcccccccddcdcdcdccecedccdceccccdccedececdcceeccdccddddcccdeedcccceccceddeddcccddddeccedeccedccccddccceccccccedcddceeccedccdcddceceeccccdccccddcdddcccececcccdcddeddcdccccdcdcdccdceccccddcdcdcdcccdcdcccdddccdcecedccdccccdcccccceccdcddccdcccccdcddccecdecccddecceccdcdeccdcdedccecedcceecdceeddddcdcccdccccddeddeeccccdcdccdcccdccccdcccddcedccdcccdcddccccdcccecddcccddcdddcecedccecccccccdccecccecccecceceeccccccedccedcedcccdcdcedcccdcccceccccccddedccdcedcccccccedcceccccedccccccccccdcccccccccddcddccdccecccccccecccdcecddddecdcddccccecdeccdcccdddcccccdccceccdccddccdcccdcecccccdcccddccccccecccdccccceedcdcccecdcecceddceccecccdcccdcccdccceccddcccedcdceeedecceceddecccceccccddcccdccecdccccceecccdddededcccccdcdedcdcdcccecccccccececcccccdccccccdecddcccdedcceeecccdcecceecdecdcccedcdcdccddcdeeccccccdcccdceeccdeecdcecdccccdcccccddcdecddcecddcceccccedccceccccddddccdcccccccccccdccccdcccccccddcdeccccdceccccccedccceddcdedcccccecccecdccccceccdddccddeeeccecdceddccccdcddcddcccdcccdddcceecdcececcddcdedcdcdcdccdceccdeccccccccdccddccccdecedceccdccccccccccdcecdcccedcdcccccddccccdddccceeccdececcccecccdccdcddeecccdcccedcdceecccddcdccccceeeccdccccddecdcdecddddccccdcccddccccccccceccdcccdeceddcdcecddccccccccdccddccccdcedceccdecceddeccdceeecceccedceccddcdccdccccddcdcdccdcdceedcecedcdeeeecccdcedcdcdcccddcdecceecdeceedcccdcccdccecccccdccccddcccccdceecdcddcccedcccccdcccdceddeceddcdcdccecddecccccdceccccecccdecccccccdedecccceddecdcdcccccdecedcdecccccdeccedccdcccdeeececccccdceecccccccdceccdccdccccdeeecccdcccdccceccecdcdccdcccdcdeecccceccdcdcccdcccddccedcdddccceccdcecccccdccdccdccdecccedccddccccdcedccddeccdccccccdecedccccecccddcceccedccccdcccccccedcdcdccecccdccdccdcccdddcddcceecccdddddecccddeccdecccecdccccceeccccccccdcdcccccecccccceccecdcccdeccccceecdcdcedccceedccecccedecdedceccecccdccdccdcccccccdcdccdddeecccccecdcecccecccceccdccdcecddddcdcccccceccececddccdccccdcdecdcdcccdccceccceeccdececdcdccdcccdcccdeeedcddccccececdccdccddcccecdccccececccecccdceddccddcdcddceccccccccdcceeeccecccdcdecddecdcccccdccccecccdcdccccccedcccccccccdcdcedecececdcddeeeccddccccccccccecceccccccedceceeedececddddecdddcecddedcdcdccccccdecdecedcdcecceddeccecdcecdcdeeceedeccccccccceccdddcdccccccddccddddecceeccdcccdccccccedcdcccccccdcccdeccccceeccdccccddccccdcecccdecccecedcceecccddcdcdedccdcdcddedeeececdddccdcccdceccccddceccddddcdcdcccccccccecceedcdcddccccedccccdccdcccccccccdcdccdccccccdcdcdcccdedccccddcccddccdecececccddcddcccececddcdcdcdddcedcdceccccccddccdccddccdccccdcdddeccccedccdcdcccdeeccdeccddceddcdedddccecccccccccdcdececcccecdcdcccccdeecccdecccecdccdedcccdecdcccedccccddcedcccccccdccecccccceccdcccdceccddceccccccedccccdccdeccdccccdcccdcdccccccddcdccdddcecdcececccedcdcccdccdcccdcecccccecddccccdceecdcecccccddddccedddddddecdcdcddcddddccdcdcddccdddeecdcececcccccccdccccdeccecdcedcdccecceedecccdccdccddcddcccdcccdccccdccdcccdcdcececccedcdcccccdcecccececccccccceccedcdceddccdcdcccdccdcccdcccccccccececccceeedccecdccedecdccccdccddcccecdcccccddcccdccccccdecceecccceccccdcdcdeececcceccdddcecccdcececcdcddeeedededdcdecccdccdccdecdcecccecdcddccccddcceccccdcddeedecdcedccccddcccdedcdccdccddccedccccdcdccdcdccedccccecdcdccccccceccccdddcccceedeccedcccdcccddccdcecccccecccccccdccdcdecceddcccecececdccddcdcddcceccecdccccccdccdddcccccccddeeccecccdcceccccdccdcdeecddceddcccdecccccddcdcdccccdcdccccecdcdccccdcccccdcccecddcceccecedccccccdcccecdddeecccccccedcccccccdcdceccdcccdcccccecccececdccdecdccccecdcedccddccdcdccdecccdccdddccedcececcdecdddcccccecdcdeccccdcdccccdcdccdccedeededccedceedcccceccdccdecdccedcccdcccddeccccececcccdcddeedceeccdccdcccccccdcccedcddccccdccecdcddcccdceccdcddcccecccdcedccdeedccdccdcccdcdccccdddccddcdcceccdcceccecceccdcecedcdcccdddccccddddcecdccdcedccccdcecdcccdcdddccccdcceccccecedccccdcccdcccdcecccdedcecccceccdcceeccdcdcecddedceccccccccdddcdcccccdccddcdccccccccdddddccccdcccecddeccdcccccdceccccdccccececcccccdeececdcccdeccedccdeccecccedccdceccccccccceeccccccdccccedccececddcccccccccddececddccccdcccdcdcddccdedecccccdcedccccdcccccccceccdccdcecdeccddccddccecdceccecccdccdeececeedcccccccdcccdcececcdccdecccecccddcccecdccceccdcdccecdccccccccddcdcdcdeeccccccceccecdceccccecdcceeccdccccdcccdcddcddccecccdcecdcddddddccedecccecdcdccccdecccceccedcecdecccddddcccdcdeecdececedccddceddcdcdeeddcdccccccddeccdecceecdedcccdcececcecccddccccedccdcccccccdcececccdecddccedcdddcdccddeeddcddddcdcdcdecddccdeeccccedcdcdceddcedccdddcdccddccddccddddedcddcdccdccddcceecddececcccccccdeeedccdcccecdccccccceddccdecdecdddddceeccccccccccdccedceccddcdccdcccddcececccccccdcccccddddcdddcdcdddccceedecdddccecccccceccdcccccccecdcccccccccdddcecccceeccceeddddecdcdccccdcdccdcdddceccdccdccdcddcedddccccccdeeccdecdcceccedccccdecccdccddecedccccccccceccdddccddccccccdcceccdccddeedccdcecccccdcccecdcccdccdcceccdcdeccecdcccdcdecccccccceccddcdcccdececccccecccedccccedddccedddcceceddcccecdcdcdddcccccddcceccedccdeedccdeccceddccdcdcecdcccccddccdccceeddccccdcccedcdecdcedceceecccccecccddedccddcccdcdeedcedcccdcddceccccdcdccdddecddceceedcccdcceececccdcdccdcedcceecdecdedddeccdcdccccdcddcdccdcdcdccccdccddccccececdccdccccccccddcccedcccccdccccdcccdcceccccdedcccdcdcccedecdedceddcdcdccccddcedccccdececccdcdcccdcccceccedccdcdccdcecdccecdcecccdddcddeddeccccdeccddecccdeccceecddddcccdcdedecdddceccdcccdccccdcddceddccdccdecedddccddcdcddddcccdedcccccdccdedceececceccccdccddedcceecccddcdccccdcdccdcddcccecccccccddcccceccceccccdddddcdddcdccddcdcecedcccccccddddecccccdccccccccdcdcdccccddceccdcddcccecceddcdccdcccdcdddecdececccddccedccedcdcecdcccccdcdeedcddcdcdccccdccdddedcdecdcccccccdeccdddcdccecccdccccccedcdcecdcdcdceccddcccdcddcccececddcccdcdecddecdcdeeccecceccecddcecddcdcecddccccccccedcdcccdcdccdddccddecdcdcdddcdcccddcceeeeddcceccccdcdcccccddcccccccccccceccededccdccccccdccccccccceceeccccdccccccccccdccceedcecdcdcccdcccccccececcedecdccdeccdccecedcceccedceedcecccccccccccddcdcccccddcdcedcccedcccceccccccddccddcccccccccccdededceccecdeeccedccdcdcdeccccdcdcdccccccccccccceeecddecdccecccddcceecceccdcccecdccdddccdecccccdeecddccccccdecccccccccecccdcceccccccdedecccddccdcdccccdecccccccccececdcdccddccccdddccdcdcccdccedcccdedecdecdcddddecedccdccccccddcdccccccccccccccddceedcdcccddcccdecccdeccccceccccccdccdddccddccdcddeccccccdceceeddddcccdeccecccdcdeccecceceeccdcdddcccecdccccdceeccecdcccdcccddccdcccccccecccdcddccccccccccccccccdcccdcdecccccdcccccdddccecdceedcccdcdccdceddcccddccceccccdedddccdcccdcccccccddeecccecccccdccccddecccccdcdedecccdcdeccccdeccedceccccccccdccccccccccccccdcccccccccddcccdecedddddecddcecccccddccdccceeccddcddccdecccccccccceccccccceecedccccdcdccddeeccdceccdcedcdcccccceccdcccceeeeecdeccecceddeddccccccccdecddeccccdeecceddccccdcccccccccedccceccdcceddccccdeddecccceeccecedccccdccccccccccccccecedeeccccecccccedcddccccccdcdcdedccdcceccecdccddddccdccececedccceccccccedccdcccccdcdcdcdecccececdcdecccdcdcecececddccdedccdcceeecddcccdcccccedcecceeccdcdcdcdeedccdddccceccddccddccccceccdcccccccdccdcdcdccccdcdccddcccecdceccccccdcccccedddcddeccccccdccccccddcedceeccedccccceccdecccdcdccecddcccdcecedccccccccdccdceccddcccccccdddcdcdcdccecddccecdccdccecedcdcdecddcccdeccdcdedccdecddccccccdeecedccccedcdececcddcdcdceceedcededcccdccccccecccccccedccedcccccdecdccccdccdcceeccdcdeccedecddcccecccdcecdcccccccdccccccccdccccccccccdcecdcdcddcccedeccccedecccddddccccddecdcdccccdcddeeccccdccdcecdcdddccccccdcccedddeedcecccecccecccccdccccceccccdccedceeecdecdeecdeeeccceccdeccccccccccddcccccccedccccdceecccccdcceccedeceeccccddcecdddcdcccceddecdcecddcccccddccedcdcecceccdecccdedcceccdcceccceccecdccddccecccccecccceeeddcedececcdccceccddcddedccddcdccdddcccccccdedccccddcdcecececcccddccdcccccdceceecdcdccccccceccccdccdcdcdcdcdddcccdcedceceeecdcdccccdcceccdccdcddddeccddcccccccceddcedcdccedcceecedcccddcdcdcececcdccdecdcdecdcedcedccddeddcddcccceccccccecdcddcceccccccedcccccccdcccdccccddcddcdcccccecdecccdcdcedddceddcdececcdcccdcccccdececececcdccceecccccdccceccdddcccdecccccecccddecccccccccdcccddcddccccccdccceccdccccddccdcccccccddccdcccececcddedecceeccccccccdcdccdedcedcccdccddcdccecccccceddccedcdccecdeddddcccdccccccccececceccdcceedcccdccccccccecccdcccdcccecccddcceccdcddcccccccccdcccccccdcccdcccccdccdccddcdccddcdccededccdccdcccecccdedcccccccdcedccccccccecedecccccccccddcccdcdccceecdccddccddcccdcccdeecccdececedccccdceccdcccccdccccdcdcdcccccdedccecdececcddeccecccccdccecdcccdcdccccccccdeecddcdecccccddddcdcdcccdeceeeddccecccdcddddcccdcccccdedcdcccddedeccccdccecccecccccceeecccccccedceccccedccceccdccccdccccccccddececcdddecccccdddccdcceeecdddcdcdccdcccddeecccceeecccccdecdcdccecccdecceedeccccecccdccddeddcccdcdcddcddcdcccccccddcccccceecdccdccddcceddceccdcccccdddecdcceccdccdccccccdecdcccccecccccccdcceecdeccccdeccdccceccccccccdddecdeccecceeccdddcccccccdcdcedcdcecddccddceccccdccedcccccddcccccccddccdccccdcddcdecddcdccecccccccdcceccccddcdccccdcccdcddeccddcdcdccddcdcedddcdcecccceecdcdcceeccccdccdccccceddecccccccccceccccdccddcdcdccedcccccccedcccdecceedcdccdccecdcccccccddccccccccccceccdcccdccceddcdcdddcccdcedcdccccdcccddecddecdcceccecdecedccccccdccccdcccccdedcddccccddcccedcdeeccdccccccccdcedccddcecccddcddcccccceccedecccceccceddcccdceddecdcecccccdccccdcccddddcccccccecdcccccdcecdecccecdddcdcddedcecdeeccccccdcddddcdeccccccdcceccccceecccecccccdeccccccceccccdcdecdccccedcdececccccdccccdcccdcccecdccecccecccdccccccecccdcceddddccccdcccccdecccccdccdceccedccdccdcdcddccccccdcccccddcccdccccddedccceddcceccdccdcccdcedcdcdccccceccecdecccccccccccdcdceccccdcccccccccdecececccececdcccccecccccccccccecccddccccdcddcddcccccececcdccccdcdccddccceccccceddcccccccdcdccccccdcccecdccdddcececddccececcedcccccccdcccccccdccdcdcddcccdddccccccdcccdcecccdccdccccdcdccdcccedcccdcdcddcdccdccccccceccccccccccedcccdcecccccdcccccccccdccccdcccccececcccccdccceccccccccceccccecccdcccccccdcccdccccccccccdcccdcccccccdcccccccccccdcccdccceccccccccccceccceccccdcccccccdcccdccccdcccccccccccccccdccedccccccccccccccccdccddccdcccccddcdccccccccdccccccdccccdccececccccedcddccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccdccdcccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccdcccccccdcccccccccdcccccccccccccccccccccccccccccdccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdcccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcdccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccecccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccccccccccccccccdcccccccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccceccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccdcccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccccdcccccccccccccccdccccccccccccccccccccccdcccccccccccccccccccccccccccccccccccccccc,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@4009e306, ADLER32}
 
Seed was 552252992721215

	at org.quicktheories.core.ExceptionReporter.falsify(ExceptionReporter.java:43)
	at org.quicktheories.core.ExceptionReporter.falisification(ExceptionReporter.java:37)
	at org.quicktheories.impl.TheoryRunner.reportFalsification(TheoryRunner.java:48)
	at org.quicktheories.impl.TheoryRunner.check(TheoryRunner.java:37)
	at org.quicktheories.dsl.TheoryBuilder4.check(TheoryBuilder4.java:150)
	at org.quicktheories.dsl.TheoryBuilder4.checkAssert(TheoryBuilder4.java:162)
	at org.apache.cassandra.transport.frame.checksum.ChecksummingTransformerTest.corruptionCausesFailure(ChecksummingTransformerTest.java:87)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}


Is this your finding?;;;","06/Feb/20 16:33;dcapwell;bq. I'm not suggesting that, I only kept it to demonstrate the failure that is possibly an LZ4 bug.

ok, sounds like we are on the same page =);;;","06/Feb/20 16:59;dcapwell;The output from quick theories doesn't work in isolation; the way the string is generated then converted to bytes ins't reproducible without relying on the seed to create the same string.  I changed the text to print out the hex of the bytes and when you run with those bytes the test fails

Here is the hex for the bytes failing

{code}
64636563646563636463646363656463656463646564646363636365646565646465656564636563646565646364656563656363636563656565636364636563656464656565646563646563656364646563646564656363636363656463656463646363636363656464656363646563646563656364646463636564646364656563636363636464636465646364656463646465636563636565656564646564646563636564656564656464636464646465636563636463636465656564636364656563656365656363636564646364646465646364646463646565646464646364636464656364656465636465646563656463646564646365636564656564646563646463636363656463646464646364636365656463656565636363656564656463656564646365646364646364646463646363646564656565656365646563656465656463656363656564646365636363636565636563646463646365646564656465656463656564636364656563646463656363636463636564656564656365636564636364656565636564656365656565646563656364656563656463636363656363656463646464646365636363636563646363636563636463656564656464656563656565646365636365636363656363656465656464636463646464636563636564636464656464656564636463656364646363646464636465656463636463656365636564636564656463646364646564636464636463646464636364656563656563646365656563656463656364636564646564646364636364646363656563646465636564636565646364656563646463636464646465636465646365646363656365646364646564646363656364646363636465636364636364636564636464656364656363646365636563646565656464656364646463656463646565656363636564636465646465646464636464656563656563636565646565636364646363636363656364656364636363636363636465636364646365636364636565656364646365656564646565636563646563656364636564646563636563646565656564646464636564636564646365656565636565646565646465636464646565656564656563636364656363636464646564646564636565646564656464636563636463656465636465636363636563636563646363636363646563656565656565646365646364646564646465646463646564656565636465656465646465646464636565646564636463646565646563646463636464646463646564636564656364636565656565646464646463656563656363646463636464646364656365656464656465656464646565646363646365646365636463656364646563656465656363646463646563646564636564656565646565636565656564646563656363656365656563656365636465636464636463656465656365656565636563656563636565646363646464656464656563646564636464646464656564646564636563646564636565636363656563646464646463646364656463656365656465656465636364646465646464646565636463636365656564646365656365646465656363656363636365656363646365636363646365636565656563646565636364646464656465646365656565646565656465646365656464646364636464656364636563636563656363656365656463636463656364646564636365646465636464656465656565656365646365656565656364656363646563636465636363656563636364646563636364646364636465646565656464656364646363656464636363636463656465636363636464656365656463656563646364656564656365636463646364656564656563646465646563646564636463646363636365656365636465656565636464656463636365636363636463646563646463636563636364656364656563646463646564636363646363636563646464636564636363646563646363636563636365656463656463646565636365656363636463636465646365656365656465656563646365646364646363636463656364656465646563636463656464636464646564636565646564646364656463646565636365636464636464636364646464636463646565636363656465636563636563636565656565656564636563656465646365636464646365636363656365636364646365636563646463636465636465636563636364656463646364646563656364636464646365646364646563656463656465646465646464656465646565646563656563656563656564636363646565646465656464656363656563646463636364656564646365636465656365656364656565646364636364656563646465646463656464656564656565636363636565656364656365656464636564646563656364646465656565636365656465636465636565646464656563656464636465636363646564656464636564636565646465656363646563656565636464656464656364656465656563636464646564646365656364656464656565636564646365656465646364636565646365656364636563636564656365646563656464646465646563656464656465646365636564646464646365646564646564646463646564646465656365656463646363656465636464656465636464636565646463656365656464646465646364636365636364646364656364646563656463646365636464656464656564646564656564656565636365646463636465656364656564636563646563636565636364636563636463636464656564646363656364636465636563656464646563646365646563636363646363646365656465656465636465636564656565656363646563636565636565656363646363646465656365636465646463646463636463656565646463656464656364646464646463636364656565646565656364656463636363656365656363646463646464636465636564656463636563646563656563656464636565636365636564656564636565636564656364646463656463646563636365646564636465656563646565656363656363636364636464656464636564636464656565646565656565646364636565656364656464646563656463636465656565636363646365646463646364636565646464646463656364646565656564636564656465636364656365656564646363646565636465656365646463656465646564646365646365646465636563656565656564636365646465646563646363646363636364646364646364636364656564656364656463656565656364636364646464646463646564656564636364656564646463656465636363636565636564656365646565636565646563646363646465656364646364646563656463636564646563636365646564646465656563636365646465636364636364646363636363656565656363656563646563656464646365656565656465636464646563636565656563636365646465656364636564656464656463646363636365656463656463656464646364636363646565636465646565636563636463656363656465646564646464656565636564646465636465656365656564636465636363646564646564646364656365656464656364646465656463646565646565646364656563636365656364646563646563636364636564636563636364636565636464646364656365656364646465646564656565646463636563636563636363636465656564656365636365646564656364636463656563636463646363656464646364646465656565636463656463636563656464646364646565646465636465646365646463656465656365656363656463646464646363636364646565646465656565656564646563636465656364636565646365646564646365646364656464636564646564656464656565636465656564656364636463646364656565656364646465636463656465656564636565656564636363636565646463656563646465656464656563636563636364646365646564656465646363636563636465656365656465636565636464656464656563646363656464646364656364646365656364646563656465646464656565656364636564636363646464656563636463656463646364646463646363636563656363636563656563636364646363636463636364636564636363636464656365646564656564656563656465646563656464636565646365646564646563636465636365646563656463636463646564656563636563646464636463636463636565646463656565656463636564656563656365656365646563646464656463656464636365656465646563636465656463636363646463656463636464646463636363636365646363636463636364656363646564656364656564636364646563646564636464646363636564636463656565646364646365636464656465656464636364656463636565656563636365656463636563646365656465656565656564636463646363636465646463656364646564646464636565646463636363656565636365646365656564656364636563636465656364646363656564646463636464636464636364646565656363636465656463646464646363646365646565646364656564646564656563646364636364636465656563636563646463656463646565646464646465656464646563636465646465636563636464656365656365636565636365636564636363646363646564656563646464646565636565656465656463656363656364636363646465636363656463636363656464656465646563656565656365636363636564646563636365646564656565646363656564636365636363636563656464636563656363636463656365656464646563656365646465656365646563646564646363646563636565646365636364646565636565656564646564636565636365636463636365646365646564646363636465656465656563656565646363646564636364636565656465636564656465646463646363656464656564656463646563656365636564656565656565646464656465656563646463636563646365636563646363646363636464636563656565656564646365646564656363656563646363646365636365656563636565656565656465636364656565656363646564636564656464636363646564656564656563636363646565646565656565636365636465656363636463646565636363646563636463646364646365646565636564656563636364636465646563656364656364636363656363646464656465646363636465646564656365636564646463656364656563656464636463656563646463636564646463636565656564636563636464646364646364636463646465636365656565636463636565636565656563636464636364646463636364646564646464636365656465636463656365636564646464656365636565636364636563636363656464656565636365636565636563646364646463646465646465656564646464636463636365636364646464646364646365656563656363636363656563646564636364646463636463636465646563636364656363656565646565656563636465646363646364656365636465636363636564636465656463646364636363656365636563646563646563636564656465646464656565636463646565646364656364646364656363646563646364636564656465656563636463656565656464656364646565636463646564646465636364656364646365646365636563646464636464646364636365636463656364646363646563636565636365636564656463656563656463636465636363656463646564646564646364636365646365646463656563656363656564646464656565646565646564646464636365656365656365636463656565636463636364646464636463636365646363636464656363636565646564656565636464656364646565646464656365646363656563636563636363636563646563636565636564656365636464646563636563656463656564656463656363636363646463646565636465656465656365646363656363636364636364646365656463636364646565636463636564656463636463656463646563646465636464646363656364636564636364656563646565636465656565656463646464636464656365646365656465656364656464656564646363636363646365636565636564646363646563636565656463636364646563646464636463636465646564646464646465656363636465636364656563656464636364636463636365646364646565636365636463636563636563656564646564636465646564646564656463636565656465636565646363636563656464656563656465636465636465656363656364636463636363656463646563636565646364656564656563656463646463656465656463636464646464646565636463656363646365646564656563636465656364636465656564646364636563656364636565646465646465646464636465636563646464636464646565646465636465636565636564636465646563656565636365636563646363656464656364646463656565656465636564656564656363656564646465636464656463646465646564646363636465636565636364656364656565646563636365656364646363656465656563656365646465646365636563646365646465636364646463646364656363646363646564646564656565646563646364636365636464656463656464636565656363646565636463636565646564646363636463636365656563636465636564646463636363646463646463636563636465636465636365656463656365656365636565656363646364646463646463646564656564636563636464656363646364656463656463636465646465646365646463646563636464636565636363646363636564636565636565636364646564646464636565636364646465646365646364636365646564646565656363636564636565656565656363646564636364646364636365636365656564646564656365636563636565646564656565646564646365636563656364646363636565656365646365656364646464646464656563636563636564636463656565656465636565646563646564656364636365636365646464656565656465646565636564646564656363636365656563636463656563636463646565636563636364656364636465646363646363646365656465636464646463636364646364636564646564636565656365646564636565636563646565656564636465656364656363656463646564646364646464636565636464636563636465646363646464656364656564656363656463636364646365646364646363656465646565636365656464646465656463646365646365646465656564656464636365656565656563656463636563646365646564656565656565646464636564646364636364656365656464636563646365656364636464646364636465636563656364656363646465636564656364636564656465636563636464636463636563646565636465656465656564646565646563656365656465656563646363636364636465656364656464656364646463656364656564636564646564646563656464646465656463636363636564646364646464646563636364646465636363656565636465656465656464646463656464646563636465646465656564656363646464656363636563646364646464646565636465646365656564656365646563656563636463656365656465646365646564656364636564656364636565636465646465656464646463636365646565646364656364656364636363636365656565656464646364656465636564636365636464656464656465636365636565656464646563656464656363636563636365656364646564636563646564646363636565636464636463646365636365656364656363646365636465656565656565656364636364646565646463636465636565656465656564636463656465636565656464636363646564656364646363656363636564656463646364636365646465656564646464646363656565636463646463636363636565636363646563636364646363636464656365636565636365656565646363656365646363656463656364646463656365656565636464656365646563656565646464656563656464636563636563656363636563646563646465646363636363636564636365636463656563636364646564646565636465656565656464636564646564646465646363636365656565656463646364656464646563656465636365656563646365646565646464656364656463636465656363636465656365636563646365636565646465656463646363656463656565656564636564656564636464656563656465636363656563646365646364646564636465646365646463656463646365646365656565656563646563656563646465656463646365636464636563656463636363656363636363656464656465656564636464636463646463656565656565636465636563656365646365646365646363656563646464646365646463646363656365656463656465636364636563646564656365636463656565656364636364646564636363656565646363636365636463656464656464636363646564646563636564656364656463656363646463636465656464636465656563646565656565636564636364646464656563656563636363656364636364646364646463636365636565636563656564646564646465656463636463646364636464636563646363646463646365646465656364656563656464656465636463656565656363646464636364636363636565656464656364646363646364636363646365646365656364656363656464656363656364646465656364646565656465656563656565636565636563656564646465636463646565636565646565656464656563636363646464656364636363646363656364636364646465656465636464656364656465636464646563656365656464636563646564656564636565656463656563636363646365656564656463646565646365636365636565636463636563656563636564636565636363656465646564646565646563656365636463646364646465646463656565656365636465636365646563656563636465656364646364656464656365656465646365636565646464646464646464636563656563656564656363656463656364646465656364636464656363646563646563646465656564646465636464636463636365646364636564646363646363636463656365646563646363646565636464656363656364636463636363656565646564636364656564646564636564646465636365646363646365636463636464656465656565646463646565646365656565656465636365636565646563646463656463636364646365636463636565646465636565656563636464646465646564636363646464636465646564636463636365656363646364636563656363636365656463646364636364656364646563636465646363656565646365646365656365656365656364656565656465646364646364646564656563636363646564656563646365656365646565636463656464636364646563636565656564636465646464646563656465636563636363656563636365636363656464646563636464656363636363656563646565646363646363646564636364656364656363656364636563636564636365646363656564656364656563646363636563656365656463636565636565636564656364656465656563646563646465646564636365646365646364656463636464656364646463636565636463656365636364646464656465646364636465646463656364646564656564646364646463636464636363636565636565636565636465646363656565646463656465656464656365656564646363646564656364636464646463656465656563646563656463656565656365656465656465646565636464656463636564646565656565646364646464636465636365656565636563636363646465636463656563646565656363636365656464646463636565636363646465636364636565646563656464656465656364656463636565646363646464646463636365646365656365646364656365656464656365656465656563636563636565646564636464636564636463636363656564646564636364636465656565646563656463646565656563636364636565656464646363656564646465646565656365656465656463636364656563646363656363636563646363636365646564656363646365656364636465656363656564646363636463646364646565646563656464636464646463656364646564636364656463656363636365646365646563636363656363646564656563636365646465646463646463636365646563656563646564636363646465636365646363646363636465656565656363656363646465656363656363636364656563636463656463656465646565656363656563636364656564656365656364646563646464646364646565656463646565636465636463646464636464646465646565636464656364636564636465656364636365646364646363646464646565636465646565646564636363646365646464646465646365636363636464646563656563636565656363656564646365656365646363656364656363646363656565656465646363656364636565646464636464646364656463646565636565636363656464646465636465646465656364656363636563656363636363656564636464656563646464656363636563656565636365636563636563646363646465636363646465656364636463656465656365656463636563646465646564656565656563636563636364636564646364636363636463646463646363646564656563646464636563646365646563656364656365656364636364656563646465646464636463636563656463656365636465646564636364656564646563646364656465656564646563636465656364646565656365636465656463636465646563646565656563646463656364656465636465636463636464646364656464636465636563656464636563636464636564646463646564656564646465636363636564636564636365656564636563646465646565656564656364636363656465636463636563636564646464656365636465646564646365636465636563656365646565656465636463646465656564636464636363646363636364636563646563656563656365656465636565656465636563646564646564646565646563646465656564636464646365636364656364656365646364656563636564646564636563646565656564646565636565656564646363656463636565636564646465646464646363656464636364646464656365656563636463646365656363636364656564646363636365636364656363656563656365636565636364656364656465636364636463656463636465636364656365656363656563636365646464646465646364646564636464656565656563656564656465656563636364636563636365646563656364656464646364646463636363636363646365646365656463646565646563656365646363656464636465636363646464636364656364636364636363646465646564636464636364656464636463646563646464646363646563656465636363646463646463646565636563646464646364636564646365656364646563646363636364646364646363646463636463636365646564646465636365656564636364636465656565656564656565636465646365646565646564646463636563646363636364656365656465656563656363656365646563646565636565656563636465636364656365656564656463636364656464646363656463646463656465656463636363656365646364656463636364636563636464636365646564646464636565636465646365646363636364636465656364656363636365646463646464646363636363656465656363656464636563646365656564636365656364636363656363656465636465656464646463656364646363636464636565636463656363636365646464646363656465656564646465636463646364656465646464646364646464646463636464646565636465656365646363646463646364656363636465636365636463656463646463656365656564656364636464636464636464636464636363656363636463646565646363646364636463656365656563636365656464656363636464646564636465656563636465656463636565646565646463656464656365636563646364636364656563646365636363656363656564656363656465656565646465656463636564656464656363646465646464636464656464636363646464646364646463636363656365656363656563646563656365636564646465646565636563636365646564646463656365646465656365656364656464656565646564656464636565656463646465646465656563656365636564656464656564636563636564646365636363636463656465656465636465656563636565646563636364656563656365656363656565636565656565636464646363646365636465646365636465656563646364636363636365636363656464656363656465656465636365656464636565636464646363646465646365636365636565646363636465636463646563636564646564636563656465656463636464636563646464646564636564646465636465656465636564646464636563636364646565636365636363646365656463636464636564646465656364646365646563656465656363656465646463646365656364636463646564636565636465656363636464646464646464636363656564646363656464656465646463646463636564656465656465656565656463636463636564636463656364636464656365656464646565646364646563656564646564656464656564656465636565636563646365646563646463636464656364646563636364636465656463636564636563656463646563646564646365656365646463646563636563656565656465656464646465656363656565656465656563656463656565636365656565636565656565636464646564646563656363646464656363646565636564636563646564656565646565656565646565656463646464636364636365656465656465636363646463656565636464646564646565646565646465656363656363636463656464636565656463656564656464646365646563646463656464646464646464656365646364636565646465636565636364636563656465656364636464646463636365646564636564646363646365646365636365636565656363646563646464656363636463646563656465656365646564636364636463646363636563656365636565646365646363636565646465636565636464636465656364646564636564636364646364636464646464656363656364636365656364636363636363636364646565656463646365656363656364646563636463636464646464656363636364636565636563656464636465636465656465636463646465656363656563636565636365636363656463636463656365636364636463636565636563636363646565646365646363656565646465646364636365636563646465636565646563636364656363656464656364646363646564656363646463646365646365636364646463656463636365636365636564636563646563636464636364646365656364636563636564636364656364656563656465656563636363636363646363636463656465636365636564656365636563646564646363636563646365636565646465656564656364636464636463646564656365636463656565636363636365646563656565646365636363636565656363656563646463656563646363656465646463646463646563636565646563646364656465656464646564656363656563646564646364636465646564636563636564636563656563636364646464656363656364656565646564656365646363656565656564636465646565646464656363636363646464656563656565636565636465646564636464656365656565656364646563636463656463646463636463636565646365656564656365656365646365656564646564656463656564656564656564656564646464646364656364656364646565646563636564646565656365656563656465656464646364646365646464656464636464646465646465646364636464636564656364656564656465636563636363646364636465656564636365636564656364646364636363656565646463646563646563646464646463656565636363636364646463646363656465656363646464646463646463646564636564656363636363636364636564646464656464646464656565646464656364636565656563656465636365636363636564656363646363636564636565646463646363636563646364646463656464646465656365646565646464646563656564636564636563646563646564646463656363656364646464646364646365646564636365636463646565656364656364636465636565656563636464656563656463636564656365646463656363656464636563636464646465646465636365636464646365636464636364656565646363656365646363636364656363656364636465646463646363656363646364656464656364646363646364656563656463636465656565646465656563646465636564636563646564646465656464636365656464636365656464646565636564656363636564646464636464646463656363646465636565636564646364656565646464656365636564656465646364656565646563636363646463656563636565656464636363656563656365646564656364646564636564656563636363656563636465646564646364646364646565646565646365646364636465646463656564646364656463636564646563646463656465656463656364636465656565636363646565636364646565636565656365656565656464656563636463646563656365636465636463646465656364656464656463636464636465656564656364646364636464656564636364646364636564636464636364636364646463646464656365646364636465656363656464656464636564656365656463656465636463656363636364656365646363636364656365636363646364656464646563646565646365646363646565646465636565646563656364636563636465646463656465646465656365646565656365646563636365656563636565656564646464646465636565656563646564636563656465636364636364636463646563656464636464636464656365656464636464646464646465646463646364656463646464636463636465656365656365636365656363646465636464656465636565636363656565646363646464646564636463646465646465636364646363646465656363636564656365636363636465646464636465656364636564646464656563656464636363636363646465646564636563656465636363636364636463646465636464636464636564636463656463636365656365656463646365646463636465646564656465656365636363646565646565636365646564646564656363646463656564656565646564656464646463636565636365646465656364656364636463636564636465656564656464646365656563636463646463636365656364646564656464636463656363646463656564636464636363656365636564636364656564656364646563646364656563656563636563646563646564656465646365636563646563656365646364646565646464646364646464646464646565656465636463646565646463646364656464656365656565646563636564636563646365636363646364656564636463636464636364646563636564656465636465656464636365636465656364636464656565656364646364636363636364646563656464656465656564656365646464636364636464646364646564656563656465636464636565646364636465636564636365636365646365656465656565636465636464656463646463646363636363656465656465656365636564636363636563646363636464646564646563636365646363656363656565656564636565636563656565636465646464656365636565636564636464656465636564636463636365636363636565656364646563646364656365636465646365656364656565646563656563646563656464636464656565646463646565636464656463656563646563636463656363646465646563646363656363646565646565646564636565646363656364636364656465656463636363656463656565636463656464646563636465656464636565636463636464646565646363636465646563646564646464656465656565646463646563636363636464656564646363636463646363646365656464656565656465636363646464636365656363646565646565636365636363636363646564656465646365656563656364646563636364646464656563656564646464636563656564636563656364636565636465646465636565636364646564646363646564646363646364636565636465646463636464646363656564636563636363646565656564636565646463636563636363656363656464646463646463636463646563636364636463636363656564646363656564636565656564636463656465646465656463646364646365636563656563656564656563636465646364656364636365646465656564656365636363656464646563646464656464646463646564656465646465656364656364636564656363656565656365636365636364646464646465636363636464636363646464636463646465646465636363656563656464646464656364646465636363636564646365646465636565636364646565646565646563636463656463636565656364636563656465656365646464656365636463636564656563646463656563646565646365636463656364656363656363646365656565656565656363656563656464656564646363656563636365656364646565656364646565646565646465636364656563646563636564636564636363656363656563656464656464636465646465646563636565646565646564636463646565646465636463636464646365636563656465656363636365636363646565646364646465636363636463646364656565636463636565646563656463646464646464646363656565646564636364656463636364656565636564636465656364636464646364656363636565656364636465636363656365636563656565636465636365656463636463636565656464646363636465646363656564636563636565656365646563646465656565656365656464636365656564646464646463646363636564636464636463636365656563646364636463636563646364636564646565636564646565656365636465646463656563656565656464656564646463636364636365646463646463656563656563636565646563636365646464656464646463646563656564646363636465656365646363636363636363646363646465656364646363656464656465646463646364646463646565656465656563646365656364656365646564646465646464656363646563636464646565636364656364646564636563636465656365646363636365646365656365636364656464646464656465656563656465646463636563636363656465646565636364656565636465646564656365646563656463636365646364646465656364646364656463656465636564646563656365636563656464636463636463636463636465656465656463636463636463646365646465636464646364646365636564656563656565646564636564646464636563636464656365636563646563646465646565636363636464636464656365656464646563646465636464636465656464656564656563646465646463656363636363646363636363656463636364646365646365656563646564646565656565656565656565636564656463656363636364636365656364636363646465646363636364646565636463636464646465656365656565656563636565646463656464646464656463636365656465636464656464646463656463646463636565646564656364656363646564636465656464636563636463656563636465656465646565636465636365636364646565636463636565656465646564656365646364636463656365646565656465646563646564656363646564656363636563646565656463646364646564636563656465646364646464656364636464636364636563656563646364656363636364646564636365646563646464636563646464656464636364636564636563656565636464646363646464636465636364636364646564646565646464646364646363656463656464636564636463656564646365656565646464656464636465646565636563656464636565646464656563656565646365646363646565646563646464646365656363636364636565646465646363656464636363656565656364656463656463636464636465646464636465636464646464656563646564636364646465656464646465646465656563656363656565636563646463636465656563656365636364636365656565636363636464656365646364646463646364656363636363656363656564656364636463656363656563636465646364656564646364636463646465636464656363636464636364646564656465636565636465646465656465656364646564656363656564636563656365636463656364646564646564636365656463646463646563656364636364636564656465656463646363656365656464646563646464636363636464646565656563636563636463656565646464656465636465636364656564656365636363656363646565636564656565656565646365656463636363636365636463636363636363656564636464636564646463636464636564656465646564636563636565656564636465636564656565636363656564656365656563636465656464636463636465656563656465636363636363646364646463646365646565636365656565656364646463656464646365646565646565636463646563656465646363636364646563646463636363646464636563646365636464636463636565646363656564656365636564646563656563656363636564656563656463636564646363636364656465646565656464646465656365636564646464636464656364636465636565656464646565656364646364656564646364656364636465646565636463636464646564656563636364646365636463656563636564646565656363646364636465646365646363656565656565656363656463636365636363656465656365656563656463646464656365636463656564636364636365656563656464646463646465646363636464656563636363656565636365656565656364656463656564636365656563656565656363656365636464646363646565646563636464646563646464656463656363646565656564656465646464656565636563636463636564636465646463656363646364646464646465656364636365636364646364646464636463646564646464636363656365636563636464636365656464656465656465656565646464646563656463636364636464656563646563646563636565636364646463636365656363646364646565656563656364646463656464656364636364656365646363636365646464636563656464646463636463656365646364656565656564656465646365656363656563636564636565636364646463656364656364656563646564656563636564656564646365646565646465646465656464656463646465656564646465636565646363636465636465646363656565656564636563636365636564656363656365646465646364636565636565636363646364636564636365656564636565646364646365636465656465636465636363646464646365636364636364646563656465636365636363656464636464646564636463646465646365636463636564646465656565646565636463636564636563646563656463636364646464636565656463646563636465646364646363636565656465656564636465656363646463636463646364646365656365646465656563636464636464646363636564656365656465636363646563656365646463636565636565646564646365636363636364656564646465636364656564636465636463636563646464656365646365636565636363656364646464646364646564646564656565646363636464646364656464636465646563646363656463656363646364656565646465636363646365656463636363636565636464656565646564646463656365646364656365636364646465656463646563656365656563656365646465636363656364636465636363636365636464646363656464646463646463656564646564646563636565636465636464656563656563636464636465656564646365646365656563646364646465656463646564646364646565636565656464636365646464656365636363656365646364656564646364636563656563646563656565636363646465636563646365646363636563656464646364646565656565636565656565636564656363656364656364636463636363646463656464656464636363636464656465646463656465636563656463646463636364636464636565636463656363636565656564636363646363646564656363636463636463636465656564636564646365636563646464636565656563656564636465636564646463636364656365646464656564646463636365656463636565656364636365646465646463646563646365656465646464636565636365646363656463656365646564636364646563636364636563646463636364656363656464646365646563656565646464636565636465646564656464636465636463636564656364646465656463656365646565646363646565656564636365636565656363636463636564636365646563646465636363636564656464656363636465636364636364636463636465636465646463646564646564656464646564646463636563636565646563656465656363636464656565656465636563656363636465636564636563656364636465646564646563646563656463656365646365636465636464646464646463636464656365636564636365636364646365656465656463646464646565646564636363646564656464646365656563636563656565636563636564646464636563636364636563646565656463636465656565636365656565636565636464636365636465636363656364636365636563656464636364636563636564636564646463636465636465636564636565656563636564646365646564636465636565656365646465646364636363656464636463656563656564656563636563646565636364656363646565656565646363636365656363646565646463646465646465636465656563636365656465646363646563656564646565646563646363656563656464636464656365636563636564646364636464656364636563636564656363646365646563656365636463656463656565656465646365656464646463656564636563656563656365636365646464656463646363636365656464656563636563646364646564656465646463646365646363656364646365646365656563636363656463636465646464636565636563646564636463656564636563646564646565656364636464646463636464656563646563646465646365656464646465636364646563636464646365646564636365646364656564656363646365646464636563646365656365656465636365636464636565636464646463636463656363636565646463646564636463646563646463636464646563646364656364646464646463636564636563636365656365646463646563656465656465646465656565646563646365656465636565656463656464646365646563636365656564656565646364656464636565646365656463646564636465636463656364656465656563636563636463636363656565646363636463646465656365656363636364636563656464636565656563636564646365636564646465636565656365646463646564636363646365636564656364636565646463636363636565646463636463656464656565656364636465636364646364636563636364646463646464636463646365636565636465656365636563656365646363646364656463636464636564646564636563656565656363646365646565656365646563636565636464656364636464646564656565646464646563646363636463646364636565636565656365636463646465646565646365646563636364646563656565636565656465646365646563656563636464646463646464646564656363636364656564646564646463636563646565636563656563656464636365646563656363656465656365636463636363636564636563646465636364636565636564636464636464646463646565646364656565636563646364656564636565656365636363636464656363656465656563646364656565646463636365636365646564636463656563646363636365636463656563636563646463656563636365636564636564656565646463656565656564636463656565656465636465646365636564636565646464636465636364656564646564646365636365646363656363636563636463656563656564636563646463656565636563656465656465656463646364646563636564646564636465636365646363656464636463636364656463646565656563646564646563646565656465646363636364656465636363656364646363646463646564646563636464646565636364646565636465656463636365646463646564656564656364636364656365646464646364636363636365656363636565636363636465636364636363646465646563646463636563646465636365646363646563636365636364646365636564636465656464636464646563656463646364636465636364656564646565636364636365656565656364646463646565656465636364656364646564656364656365656364636464646565656565646464646364636463656464656465646365656563656364656465656364646365646464646363636363636463636465636565636364646563636365646565636463636564636363656563656463646565636565646564636463636465656564636365636365656364646465656563636463636364636463636464656363646564646365656565646464656463656365656464636363636463656364656365646464646363656464646563636363656463646464646465656365656364636363636464636563646565636363656463656563656364636464646364656564646563646363646565656563636464636363646363656463646465656364646465636363636463656465656564656465636463646563636564646565636364646365636465646464646363636364646564646464636364656564636363636463656464636563646363636464646364636363636365646564646563636363646363636463646564646564636463636365656363656463656365636464636564646463656564636465646463646563636564636465636565656363646363656363646363646465656563636464636363646465636464656564656564656465636363636565646464646365656365646363646563656463646364656364646563636463646565656364636363656363646363646563656465656365646463656463636364646363636463656463656364636364636364646565636563656465646564646565636365656563646464636564656365636464656565646463646564636363656465656363656563656463646465656365636363656364636465656465646563656565656563656365636463646463646464656364656565646565656365646364646565646364656564636564636463636364636465646564656463636464636464636564656464636565636565646564656365646563636465656565656565656464646464656364656363646565636465656463656365636364656564656464636364646363646563636465636465636565646463656365636465656365646464656363656464636363656464656563646564656463636565656464646563656465656464636564636363646364636563646365646364636464646365646363656463646365636363636365636565646364646464636564656564656565656465646564656563646563646363636564646563646463636565656365656463646463646363646565656563636364646365646564656563646365636365636564636564636565636563656563656365656463656364636365656564646463656365656464656563656565636563636464656365646365656564646564646464656463656464646465656565656565646463636563636563636565636565656564656465646365656464646365636565636365646564656565646365636365656365636465656563646564656363646463636465646565656464656465656464656364636463656565656564656463636563656465656564646464636565636564656365656365636563656565636465656463656465646464656565656465656363646365646565656464656364636365656563656564646365646464646563636564656463656564636365656563656365656465646465636365646363656563656363636464656464656563656565636563636363636463636364636463656565636463656463636463656463636364636564656464656565636563656363636563636364656464656463656465656365646365646564656564656463636365636365646564646464646565646364656563656363646365636565656465656365646365636364646364656464636564636463646465656365636365646364636363656363636464646464636363636463656465646465646365656464656363636565656365646465646565656565636564636564636364636563646365646365656565636463646563656364636363636565646565656564636465636464636564646564656565646365646465646565656565636364656464656363636463646465636563636464656364646464646465646465636364656564636365636364636565646463646364646565656565646463646365646463646565636564656563656563636464646364656564636363656463656565646365636564636364656563636563636363646565636464636465636463646564636364646363646564636365636363636364636563656563646365636565636565646464656464646465636365656463656564656363656363656563656365646563646565646464646465656565646365646565646363636564656465656364636564656563646365656363636463646464656564636464646463656363656465646463646563646364656564656563636565656364646365646565656364656363656563646564636464646564656564646464656564646463656464646363646463646364656465636565646565636564656463646564656564656465656363646363646464646563656564646365646465656464656465636563636463646564656363646364646364646564636465636464656565636464636364656463646465636463636363646463636565646364636465636563656365636563656365646463636464656465646364656363636464656565636464636365646565636565636365656564636563646465656464636464656464646563646464636463646563646564646364656464636565646364656463656365656563646564646364646564646465656565636365656563656465656465656563636465646463636463646463646563646565636564646564636465646563646563646464636565646364656465636464656365636363636564636464636563646563646465646465646464656465636365636364646565656464646363636364656365646465646465646563636564656363636363646564656563646463656564646565656564636464646365646364646363646465656465656365646465636464656364656563646365646364646564656565646565636365656363636465646563646565646565646564636563646463646463636363656465636564646364656565636463636565656465636564656564646463646464636363656465656464646563646463636364656363636463636564636363656364656364646463636563656463636364636364656564646465646563656365656365656463646465636564636564636365656465636463656464656563636565646364656364656365656565636363636463646463646465636565646463646465636365636363646365646564636465636365656564656463656563636364656564636565646365656363656465636364646465656463636563646465646564646565656464656363636364646365646464636463646364646563636365656564656465656565636564636463656564656365636363646563656565636365656463636364636465656464636463636464656365656364646465656365656364646465636363656365646465636564656465636563636463656465646465646564646363646565636364646563636563646365656564646565636363646464656363646565636465646465656464636365646463636364636564646464636563646463636564636364656463656563656465656463646364636363646564636565656563636363636363646465646364656563636564656465656564656564656465636565656565656363636465646565656564656563636565646465646365646465656363646563646565656564646564636464646365636463656565636564656565646364636564636364636364656563656565656464636463656465646365646364656464656465656363636464656363646364636563636464636465646565656565636364656565636365636365646565656465636564656565646563636365636464646565656464646363646463646365656565646565636563646564636363636465656465646463646465636464646464646565636564656465636363636363636365656564646463636464636365656564646464646363646365646365656363646465646464656365656364656563636364646365646564656563636364656464636465656364656365656565636464656565646464656364646465636365646564646363646365636565656365656564646365636363656463656463636465646463646363646463636564656563656465656463646363646364656563636563646563636463646563656365636363646565646463646465656563636565636464656363656563656565646465656364636363656563646565636364636463636563636563646365646463656365636363636463646463656364656564636465636465656565646464656363656564656565636565646365636565646364656563656464636464646565646465656464646564656463646464656365646364646563656564656565656564646365636463656364636565646363636565656364656563636464636363636364646465656565646365646465636364646365656563656364646465646565636564646465636563646463646365656565656364656464656564636363636464656463646463636363636364636563656364656365656463656563646465636463646364646564636365646364636364636563646465646563646364636365646364646564656463656364656563646465656364636364646463656465636464636464636363636564656365646465636365636463636565646363646465636563636465636464646464636564656465656363646565636463656365636563656564636565656463656564636364646364646565646563646465636464646563646465646465656565656565646563636363646364646363636363646364656464646463656365646464656463636363656563656365656363636565636565636364656364656363636465656465656565636464656565656365636563656564636363636563646363656563636464646565646365646564646464646563636565636465656463646564656464646363646563656565656463646365646563636364636364656364646565636363646563636563646465646365656363656464646464646463636364636364656463636565656565646564636564656364636563636365656465646363646563636363636565636463646364656364656564646564656465656563656463646465646364646364656365646465646363636363656464636363636365646363646465656364636464636463636364636364656564636365646563656365646563646465636464646365646464646564656465656363646463646465656365636465636363646563646463656563646463636564636463636465646364646465656465646464636564636565656364656464636564636565636565646363656564646463656465646364636363636565646464636363636564646464646463636464636563646465636464646564656465656463656565636565646365656563656563636564646565646564646365656463636363656564656463636464646564636465636365656463656363656363646564656463636364646464656463636364646465646563646465646464636565646364636363656463646465646464656564646565656364656564646463646365636564656563656564636463636365646364646565646363656563656463636565656363646464656564636365636363656364646363636563636364646364656563656465636365656465656363646565646464636465646363656565646563656465646565646565646363646465636463646464646565646363636364656364646363646564646563656564636463646463636363656465656364636365646364636563646365636464646565656564646464636464636363656365656364646365656565646564656565646563646464646465646565636463656364656364656563656464636365646465656363656364656563636363646465646564646363656563636565656564636364656563646465636363636365636563656364646563646463636563636565636363636463636565656565646463656565636364656465646563636463636364646465656463636564636465636565646463636563646363636464646463646463656465656365636363656464656463646363656363656365646363636365656364656464636463656463636465656363646564636565656563656364646563636463656364646363646364646365646463646564656364646463656463636563656465646463656463646365646363636464646465636463636564656363636364636564646565646563656363636564656365636465646465646465646364636565636363656465656364636363636464656464656463656465646563646564636563636564656364636464636364636563636565646365656465636363646364636464656464656365646463656465656463646363656463646465646564636563636363646564636463656464656564646563656565646463656463646564646363646363646563656465636363656363636564646463646565656363636563656563646565636564636363636464636365656364636563646363646363646364646364646465636363656363646465646363646363656463636464636563646363646364636464636464656565656564656365636563656465636364646364656565656565656365656364646463646364636464646364656364656365646465646365656564656565646465636364656364636465646564656565646565646564646464636464636365656564656465636363636364656565636364656563646564656365646465636365656365656464646463656364656565656465636463656563646364656464656565656464656464646363636565636364656465646564636563646363656365636564646363636564646464656564646365656463656463646465646364636363646463636364656464656463636363636564646465636363636365656464646365646463646365636465636563646563636363646564646363656564636565636563656565646365646365656465656563656364636364656564656365636564656363636564636363636363636463636365636465636365646465646364646465636563656463656564656564646563646463636363656565656364636463636365646565636363636363636464656563636565636363646565636465646465636465636564646465636364656463656464646565636464636464636564636363636363656563646463646565636565656364646364636463646363646564646364636365656564656463646563636365656563646464656465636564646464656364646463636565636464636465646465646464646464636465636363646365636464636465656564646465656463646565656465646363646564646563656463646465636463646363656563646563636463646363646463646364656465646565636363656464656365636365656464646364646364636364646563656465636563656564646563636465646463636364646465656464646464636465656464636563646465636463646365636465646464646364646463656464646563646463646463646363656463656465636463646464656365656565656564646364636364656463636465656463656363656364646463636364636465646465646463636465636465656363636564636364636365636565656564636464646563636463636364636365636364636364646464636463656463656464646365636365656363646365656464656364656365636564646563646365646463656463656463646363656564646363636465646365656464636365646564656465656564636463646463646464646465646364646364656364646464636364656364656463636464636364656563656465636465636363646564646463646564636563636563656563646463646463646565646365646563646565636465636565656463636365636363656365636364656463646465636364636463646365646364646365646565656363656465656564636565656564656363656564636564646464646364656563646364636564636565646363636365636565656463656463656365656365646464646363656564646365636564656464646463636365636365656464646465646565636465636465656563636363636463646463636564656564636565646464636364646363636463636465656363636563646463646363646465656563656565656465646364656464656364646464636565646465656464636365656563656565656465646365656464656364656363656363656564656364636565646464646563636363636564636364656365636363656464656364636363636363636464636363636363646363646465636465646465656563636464656365646464636465636365646564636465656365636365656565646363656464646364646364636563646463636463646564646563656463646364646464636363646565656563656565656563646364646564656364636563646463656564656365636465656563656464636564656565636365646464656364646464656363636463656564656465636364656464656363646565646463646364646365656565636563646463636465636564656565656565656464656365646365656463636565656564646363656463646464646565646465656563636564656363636365636464636563646365656563646363646464656563656565636565656464636565636364646563636564656565646564656465646364646365636565636465646565636464656365636565646364656365646563646364636463636465656564656564636563636465636465656365636565636463656363636364646564646563646564656364646565636564646565646364636363646463656464636565656465636364636465646365646465636465646463656564656465656364656564646564646565656565646364656463636464636363636463656565656363656464656365636365636564636464646563636564636563636464636363656363656365656465646563646463636564656365656564636564636564646363636365636364646564636365656365636364646363646365646465646365656464636564646365636465636463636363656464656563656464656563656363646463656463646365636563646465646364656465656363656564636364656465656563646464636563646365636564656463636364636363636465646463656565646364646564646365656464646565646563636464646363656564636464636365646365646563636364656465646365656463646564636365636563646564636464656465636463656464636565646464636365656365636463656563656363636565636363636364656363636563656565636563636365656464656465636464636563656463636363646565646564636363646465636465646363656464636565656464636564636364656365636363636364636465636563646365636463656364656463636565656565646364646564646365636565636463646563636463646464646565636464646565636364656365656365656364656463646464646363636564656464646465636465646565656563656564646464656364636464636363646564646365646363636365646463646364646463646363646365646363646365636465656463636463656564646464646365636363656564646365656464646564646464636365646465636565656364646565656564636465646563636463636365656463636365636563636564656363636365656364646363656563646464636364656365636364656464636563646463636564636463656463646565646365646463656564646364646564646464656464646364636363656464656463636565636565656563656365636465636365646365656464636465646363636563646364636365636564646364646365636365646464636363646565656363656364646463656564656463646463636463636365656363656464636464646365636464646465646363656364636365636564656563636363646564656564646363646563646465656364656565646463636363656463646565646564636565646463636464646565636364646363646565656364646565656564656563656563636364636364656563636363636365636465646364646564636564636464646463656563646464646364656564636364656365646363636364636564656464646563656365636363646563646564636564646465636364646364636364646364646363636465646565
{code}

byteToCorrupt = 10599

test params

{code}
{([B@246f8b8b,10599), 17, org.apache.cassandra.transport.frame.compress.LZ4Compressor@10a9d961, ADLER32}
{code};;;","06/Feb/20 17:16;dcapwell;In slack [~aleksey] pointed out that this is a known issue and called out in CASSANDRA-15299;;;","06/Feb/20 17:24;brandon.williams;bq. Is this your finding?

Yes, that matches what I see (or sometimes the jvm segfaults in LZ4).  Your patch with 500 examples added back in corruptionCausesFailure passes consistently.;;;","06/Feb/20 17:45;dcapwell;ok, have a new version of the patch which is able to hit the failure case, but we went from ""flaky"" to ""every single run""... progress? =D;;;","06/Feb/20 17:46;dcapwell;The root change was that I didn't follow the style of

{code}
private Gen<String> inputs()
    {
        Gen<String> randomStrings = strings().basicMultilingualPlaneAlphabet().ofLengthBetween(0, MAX_INPUT_SIZE);
        Gen<String> highlyCompressable = strings().betweenCodePoints('c', 'e').ofLengthBetween(1, MAX_INPUT_SIZE);
        return randomStrings.mix(highlyCompressable, 50);
    }
{code}

instead my first patch was basically the same as randomStrings.  If you run with just randomStrings you don't see this after a ton of runs.  If you run with highlyCompressable you will eventually see this.

After changing the byte generator to follow this range of bytes I was able to hit it every run...;;;","06/Feb/20 18:29;dcapwell;With my new patch I get the following failure

{code}
java.lang.AssertionError: Property falsified after 4675 example(s) 
Smallest found falsifying value(s) :-
{(6364636563646464636365656365636565636564656463636565636464656563656565646364646363646563646363646363656464636465656364636365656564636365636565636463646565636463636364656365646363656563656365636464646464646464646363646464656463636463646564636464646363656464636364646363646365636365646463656465636565646465656464636364646364646565636564656563656563646563636565646563636563646565636363636563656364646363656564656364646463636364646565656363656464636564646463656463656363636565656563646364656363636565636363656363636563636464636364636464636363646564646563646465636564636464636365646465636565636363646564656463646565646364636565636464636365656463636464656364646564636464636465636563656365636563646464646563646564646564656463646563636465656364646565646465656363636363636563656363656563636364646363646464656564656364646364636563656465636563646463646364656365636464656364636564646465646465646564656363656464656563636565646564646565656363636465646463656463646365656463636363646563656365656363636563646465636563636464636363636363656364646365656363636365646364656364636565646363646564656363656464656563656465636365636465656364656364656564636465646464646464646365636365646465646463656363636465636464656564646464646464636464636363646564646365656565636564646564636364636363656565636365656465636363646363656564636563656363636365646564656564646463636364656463636364636464636464646565656463656365636565636363636564636564656463656363646365636363646465646464636464636564636563646463646363646364656364636365646363656465636365646465646563636463646363636465646465646463636463646364646463636464656363636464656565656365656364646565656564636563636565636465636363636364646465646363646463646564646465656464636563636363636363656365656563656365656363656563646364646365636364636463646464646565646463646465656364646463656463646364636465636463656363646364636463656365636364636363636364636365636364646364646465636365646465656463636<...>636364646363636565646365646463636363636563646363656464646364646364636364656363646363656463636363656363636563646563636563656363656363646363636363636363646363646563636363646365636363636465656463636363646363636563636463656363636365656363656364646363646365636463646363636363646363646463636463636363636365636363636363646363636363636363636365636464646363636565636363646363636363636363636364636365656364656363636463636363636363636365636463656563656364646365636564636364656364646363636364636463656463636363636365646364636565656363636363636365636563646363656365636364656364646363636363646564636563636464636363656363646363636363636364636563636363636563636363656463636363636363636363656363636363636463636365656363636364636464656463646363646364636463636363636363636363636465646464636363646363636363646563636463636563636563636463646563656463636463636463636565646363646363636463636363636563636363636463656463656364646363636365636363636364636364636563656364646463646363646363636563636363646364646563636365636364636365636363636565636364646465636363636364646363636365656363656464646363636363646463636464636364656363636464656363636563636364646363636363636463646364636365636363646363646363636463656463646564636364646363636463636463636363636363636364636564636464656363636463646363636364636363636363646363646464636363646363636464636464636363636365646364646563656563656363646563636364636363636364646365636363656463646363636363636364636363646363636365646365646363646364646363636363636565656463646363656363646364636363636365636363646365646363636363636363636564636565646463636563646465646463636463636363656365646364636465636463646563636364646363636463636563646363646365646364646363636363656365636363636363636364636563636363656364656463646363636463636363636563636463656365636363636363646364656363636363636365636563636364646363636463636465636463656464646563646363656564656463646563656363636463656464636364636563,72535), 14, org.apache.cassandra.transport.frame.compress.SnappyCompressor@4b41dd5c, ADLER32}
 
Seed was 113149736037976
{code}

and

{code}
java.lang.AssertionError: Property falsified after 21514 example(s) 
Smallest found falsifying value(s) :-
{(636463656465646463636465636563656364646563656364646465656363656465646465656364656363646465636363646563636365636563636563636463656463636363646565636463636364656365636365646565656364656564656464656463656563656565646463656465656364656564636463636465656464636363646564636563646363646565656364636464656564636563636363636363646465656463656564646463636365656363636363656363636565646563656364656365656564646565636565636364636365646463656564656363656464656464646363636365646465656363656563636563636365636363656564656563636564646463646363646564636364636565636364646563656364636363646463646564636464656463656564656364646463656564636363636364636365636464656463636564646364636465646563636563636465636565646563636465656565636365636564636463656364656565636364636564656563636365646365636365636363656465646563656365636565656565656363636564656563646364656465656463646464656365636463646363636363646463656363636365646564636565646565646365636563636565646365636563656364636563656363646363636464656464636465636364646363646364656363636363656465636463656463656365636465646565656463636364636365656364636464656363656463636564646364646363646563636363636365656463636564656563636363656463656565636564646563656464646465636563646565656464636364646363636363646465636564646463636563656364636564636565656364636563636464656564646565646464636363636364646363636463656563656564656565656563636464636463646363646564636563636463656365646563656464646565636565656563636364636464656365646563646464656565656464646565646465636363656563646363646565636364646464636465656565656463646363656463636563636564656463636563646464656563656465636364646365656464656364656565636365636564656564636565646364636465646364636363656565646463636564636564646364646463656465646565636464656364656363646364636463646563656363636464646365656563646465636364656563636463636365646364636465636464646463646564646465636464636363636363646465646364646463636364656463636364646365<...>363636563636365636363636363636563636363636463636563656563636363636363656363656363636363636364646563636363636363646363636363636363636464636363636364646364636363656563636364636363636363636463636364636363636364636363636563636363636365636363646363636464636363636363636364636363636363646463636463636364636365656363636363636363636363636363636364636563636363646363656563646465636364636363636363636464646363636563646463636363636363636363636365646364656463636463636363636364656464636464636363646464636365636363646364636363646363636463646463636364656363636563636464636463646363636363636363636463636363656365636363636463636363646363646363646363646364636363636363636563636463636363636563636363646463636363646364636364636363656363656363636463636463656363636364646463636363636463646363636463636363646563636364646363636364636463636363636363646363636363636364646363646464646363636363636564646363636363636463636364636463646363656364636565636463656363656365636364636363636463656363646363646563636563646363636564636364636363636463636363636563636363656363636363656363636363636463646563636363636364636463656363646363636365656463636363636365636364646363636365656565656464656363636363636463646363646363656364636363636365636463636563636364636365636364636364636363636363636363636364636463646463636363646364636363636363636463636364656363656363636563646463636564636365636363636363656463656363636363636363646463636363646363646564636363636564646363656463636364636363646363636363636363636363636464636363636465636463656463656363636364656363636363646365636463636363636364636364636363636363636363636363636563656465636363636464636365636363636363646463656365636363636565646464636364646365636363636464636363656463636463636365636363636363636363636563636363636363636363636464636363636363646363636363636363636563646463636463636363636363636463636464636563636363636363636463636364636563646363636463636363636363636463636363646363,13508), 74, org.apache.cassandra.transport.frame.compress.LZ4Compressor@61544ae6, ADLER32}
 
Seed was 115043474316658
{code}

{code}
java.lang.AssertionError: Property falsified after 302 example(s) 
Smallest found falsifying value(s) :-
{(63646564636465636564656464636364636563656564656463656365636464636363636563656363646564646364646363636464646564656564636565646364646564656563636365636565656565656363646463636465656564656364656363646365636363646364636363656365636365636563656365656464656563636364646363636364636565646564646363636563636365646465646565646465656364646563646564636464636464636563636365646365636365646363646464646463646464656465656365646363646463636463656363636463656365636463646465656365636363646563656463656465636363656364646364656365636465656363646563636363656463656464646463656464656364636564646463646465656364646363656463656463656464636463636465636363636464646464636365636363656463636365636365636465636564656463636363656363636463646563646564656464656363656565636564646465636463636363646365636365646563646565656365636363656363656463636364646363656564656464636463636363656564636364636364636463636364636563636463636364636563656464636565636563646364646363646565646365656363636363636364656563656364636563656364636464636364636464656463656364646363646363636364636363636363646363636464656463656365646364646365656563636463646364636464636563636564646364636363636464646363636363646463636465636463646565646465656363636465636565646363636363646365646464646365636363636463646564636565646364646365636363636363636364636364636563636564646463646463646463636465656563656463636363636363656565656363646563636364636365636363646463646365656563646564636364636564636363646465636364656564636564636463636365656564636464656364656565636363646363636364656564636563646363656363646365636365636365636363656465636463636564656565646563646563656563656464636464646463636565646464636365636465636563636565636463656364636363646564636563646364656364636365636564646464636464656363656464636463646565636364636565646565646464636563646465636463636563636563656364646363636563656463636363646363656363656363636465636363656365656365656465636363646463656464636563636463<...>363636363636463636363636363636363636363636363636363636364636463636363636363636363636363636463636363636363636563636363636363636363636363636363636363636363636563636363656363636363636363636363646363636363636363636363636363636363636363636363646363636364636363636363636363636363636363636463646364636363636363636463636363636363636363636364636463636363636363636363636363636365636363636363636363656363636363636363636363646363636363636364636364636563636363636363636363636363636363636363636363636363636363636363636363636363636363636363636364636363636363636363636363636363636363636364636363636363636363636363646363636363636363636363636363656363636363636364636363636463636363636364636363636363636363636363636365646363636363646363646363636363636363636363636363636463636365636363636363636363636363636363636363656363636363636363636363636363646463636363636363646363656363636363636363636363636363646363636363636363636363636363636363636363636463646363636363636363636463636363636363646363636363636363636363636363636363636363636363636364646364636363636363636363636363636363636363636463636363636363636363636363636363636363636563636363636363636363636363636363636363636363636364636363636363636363636363636363636363636363636363636363636463636363636363636363646363636463636363646363636363636363636363636363636363636364646363636363636463636363636363636363636363636364636363636363636363636363636363636363636363636363636363636363636363636463646363636364636363636363636563636363636363646363636365636363636363636463636463636363656363636363636363636363636363636463636363636363646363636363636463636363656363636363636363636363636363636363636363636364636363636364636363636363636363636363636363636363636363636364636363636363636363636363636363636363636363636364636363636363636363636363636363636363636363636363636363636364636363636363636363636363636363636363636363636463646363636463636363636363636363636363656363636363646363,18453), 31, org.apache.cassandra.transport.frame.compress.LZ4Compressor@61544ae6, ADLER32}
 
Seed was 115716255584197
{code}

{code}
java.lang.AssertionError: Property falsified after 499 example(s) 
Smallest found falsifying value(s) :-
{(63636463656365656463656565646364646565646363636463636565656363656565656463636564636364636464646564656564636463636564646564646465636365646363636363636464636364636565636365636565636465636465646564656563656364636465646565656463656363656364646363646463656364636365646365656563656363656364656463646464646565646365656564646563656564656365656564636365656465636463656565656464656563646463656365636564636363636464636365636563636363636464656364636363646463646364646364646464636363636465656565646465636563656364646365646363646363656465646463636463646364636563656464636465636465646563656363656564646565636565636365646565636465646564646363636363656363636564636464636364646565656463656464646463646365636463656463656563636564636364656464636464646564646463656564646365656364656365636464656363656565646564646563656363656364646364646565656463656365636364636463646563646365656564656464656564646364656563636365646565656365656465656563636563656363636464636565636463656363636363656563646364636465636364646463656364646365646365656563646464636463646364646363646364636465636465656465656564636365646565646365646463636364656564636463646565656463636563636363646563636564646564646563646363646464636364656364656464636363636364646363656465636465636464646463646564646464646564636564656463636464646365636565646465646564646365656365636364646464656363636464646563636364656365656565646564636463646564646363636363656364646564636364646563656364656364636364636563636563646563636465646364636563636464646565646464636363646464636465636364636465656463636364646565646563636565656364636564636565636565656465646463636564646463636563636363646465656363636363636465646463656563636363646364646464646364646565636463636564656564646465636365636564646564646463646364646563646564656365646363636564646563656365636563646563636364636465636365646465636565646363646363656464656364646363646365646564636465636465656364656465646363656563656465636365656565656363<...>636363636365636463646463636363636463636463656463636363636363636364636363636363636363636363636363636364636365636363636463636363636463646564636463636363636363636365636363636363636465636463636364636363636364636363636563636564646564636463636363636363646564636463636463636564646363636464646464636363636365636363636364636364636363646365656363646364656363636363636363636364636363636363636363636363646363636365646564636363646564636363656364636564636363636463646364636364646363636363636363636363636465636363636363646363636364656364646364636364636464636464636364636363636363646464646364646364656363646363636463636365636364636463636363636365646363636363636363646363636463636363636365636463636464656564646363636363636463656363636364636363636463636364636364646363636364656363646364636363636363636563656363636365636463646365636365636563636365636363636363656363636463636363636364636363636363646463636363636363636463636363636363636365636563646365636463646363636363646363636363636364646365636565656563636464636363636463636463636563656563636363656363646363636363656463636363636363646363636563636563636363646365636363646463636463656465636363636363646463636363636463636364636364636365636365636463636364646363646565646463636363636464636363646463636463636363646365636363636563636464636363636363636563646365646363636363636363636365656363646363646463636463636363636563636365636564636363636363636563636463636564636463636365636563636363636363636363636365636465636363656463646464636463646363646363636565646363656363636363636363636364636364636464656463646363646363636463646364636364636363636365646363656363656363646363656364636364646365646363636363656363656365636363636363636363636465636463636563656563656365646463636463636364646363636363656363636363646463636363636364636363646363636363636363646364636563636463646465636463646463636363636565636363636365636463636363636365636364656465636363636363636563646364656463,13308), 75, org.apache.cassandra.transport.frame.compress.SnappyCompressor@10e31a9a, ADLER32}
 
Seed was 121866756470991
{code}

Im trying to get the data for the core dump, but made the test stop having GC issues (didn't fix the fact the test is finding failures);;;","06/Feb/20 22:47;dcapwell;Created a different JIRA for the memory issue since I didn't make the test not flaky, I just made it so it doesn't GC anymore.

I published the test which causes the JVM to crash here https://github.com/dcapwell/cassandra/commit/355b3299ba77f91d418850eb75908b5d6461e6a8;;;","07/Feb/20 05:42;dcapwell;Sent patch for CASSANDRA-15556 which makes it so the JVM doesn't crash anymore; with that in the only remaining issue I know of is

{code}
transformInbound(transformOutbound(input)) != input
{code}

[~samt] would it be possible for you to take a look? I provided a set of seeds which fail, so can rerun the test with that seed to reproduce the failure.;;;","07/Feb/20 09:08;samt;{quote}[~samt] would it be possible for you to take a look?{quote}

Sure, but I won't be able to get to it immediately. This will go away with CASSANDRA-15299, so if it's causing test failures maybe we can {{@Ignore}} the test for now and I'll come back to it if 15299 gets bumped from 4.0.;;;","07/Feb/20 09:44;jwest;If we aren't able to fix it now / are waiting for a future change, can we mark it as flaky (maybe this JIRA being open is fine) in some way but not @Ignore it? I think that would more likely ensure we don't forget it -- especially since its been effective at finding bugs. The test should apply to the code post-15529 as well. ;;;","07/Feb/20 10:25;samt;I wasn't suggesting we close this JIRA, so it would still effectively be marked flaky and a blocker for 4.0. Marking it {{@Ignore}} would just improve the signal:noise ratio for builds broken by other things.;;;","07/Feb/20 15:21;dcapwell;If the argument that everything will be better with that Jira, I’m fine for now. I am curious why the condition happens but if checksum is done on compressed data it’s not likely to hit it.

I rather keep the test running.  Yes it’s red, yes it’s noise; it’s calling out a bug that isn’t fixed yet though. ;;;","07/Feb/20 18:39;jwest;agreed re: keeping test running [~dcapwell];;;","09/Mar/20 15:57;spod;Reverting CASSANDRA-15555 fixes corruptionCausesFailure with seed 71671740653044L for me. But semantics for generators change with that as well, so I'm not 100% sure its the actual cause.;;;","10/Mar/20 03:16;dcapwell;bq. Reverting CASSANDRA-15555 fixes corruptionCausesFailure with seed 71671740653044L for me

Sorry I don't fully follow, can you elaborate [~spod]?  There are 3 issues with the test/feature (may have missed something, going off memory).

1) corruption can cause lz4 to crash the JVM.  This was fixed in CASSANDRA-15556 by using the ""safe"" methods rather than ""fast""
2) corrupted lz4 stream may not fail and may produce output != input; this is still an issue and the tests fail periodically with this.
3) generators generated too much garbage.  CASSANDRA-15555 switched to fixed memory and switched from strings (charset depends on the test environment since it doesn't define which charset to use) to raw bytes.  Given the fact the generated data changed the seeds which failed before no longer fail, and the seeds that fail now did not fail with the old generators; both generators were able to reproduce #2, but with different seeds.;;;","08/Apr/20 09:08;eduard.tudenhoefner;For completeness, here's the Jenkins history for this test failure: https://ci-cassandra.apache.org/view/branches/job/Cassandra-trunk/45/testReport/junit/org.apache.cassandra.transport.frame.checksum/ChecksummingTransformerTest/history/;;;","08/May/20 14:50;jmckenzie;Moving fixver to beta since this is blocked by a beta ticket.;;;","22/Sep/20 20:32;maedhroz;Reproduced again here: https://app.circleci.com/pipelines/github/maedhroz/cassandra/118/workflows/3cecc61c-043b-48c9-ad10-88495df8b6d1/jobs/640;;;","07/Oct/20 19:42;maedhroz;Just for visibility for whoever is watching this, {{ChecksummingTransformerTest}} will be removed in CASSANDRA-15299. We can keep this open until that merges though...;;;","16/Dec/20 16:04;aholmber;The checksumming stuff was merged. Should this be resolved now?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flakey - testIdleDisconnect - org.apache.cassandra.transport.IdleDisconnectTest,CASSANDRA-15310,13255304,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aprudhomme,jolynch,jolynch,06/Sep/19 15:44,15/May/20 08:55,13/Jul/23 08:38,08/Feb/20 15:53,4.0,4.0-alpha4,,,,Test/unit,,,,0,,,,"Example run: [https://circleci.com/gh/jolynch/cassandra/561#tests/containers/86]

 
{noformat}
Your job ran 4428 tests with 1 failure
- testIdleDisconnect - org.apache.cassandra.transport.IdleDisconnectTestjunit.framework.AssertionFailedError
	at org.apache.cassandra.transport.IdleDisconnectTest.testIdleDisconnect(IdleDisconnectTest.java:56)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) {noformat}",,aprudhomme,csplinter,e.dimitrova,jeromatron,jolynch,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15382,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aprudhomme,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Sat Feb 08 15:53:02 UTC 2020,,,,,,,All,Java11,,,,"0|z06ens:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/19 19:33;vinaykumarcse;This test failed in my recent runs too, failed with the same exception. 

https://circleci.com/gh/vinaykumarchella/cassandra/453#tests/containers/48
https://circleci.com/gh/vinaykumarchella/cassandra/463#tests/containers/86;;;","20/Sep/19 06:56;aprudhomme;Looks like the measured time interval does not entirely cover the timeout. The tests in java 11 were sometime falling a few ms short. Adding a fudge factor should help.

edit: On second thought, it is probably better to just refresh the timeout, like in testIdleDisconnectProlonged.

[15310-trunk|https://github.com/aprudhomme/cassandra/commit/bf279fd16960827bb64c39c9d6dc0fe68b067852] [cci_8|https://circleci.com/workflow-run/fc2e076c-fca3-497a-b9aa-db66aa2905f3] [cci_11|https://circleci.com/workflow-run/97c01d17-2ef6-494f-afea-35eba3395350];;;","30/Jan/20 20:13;e.dimitrova;Hi [~vinaykumarcse] and [~aprudhomme],

Are you still working on this one? Any help needed?

 ;;;","30/Jan/20 21:53;aprudhomme;Thanks [~e.dimitrova]. This test is flakey in alpha1/2, but seems fine on the current trunk. Doing some digging, it looks like it got fixed as part of a different commit [https://github.com/apache/cassandra/commit/3a8300e0b86c4acfb7b7702197d36cc39ebe94bc#diff-0da71200299fb3393ea8f95eae9124ea]

If no one is seeing this anymore, the ticket can probably be closed.;;;","30/Jan/20 22:04;e.dimitrova;Thanks for the fast response [~aprudhomme].

Looking at the OSS Jenkins I see it marked as stable here(if I am looking at the right place):

https://builds.apache.org/view/A-D/view/Cassandra%20trunk/job/Cassandra-trunk-test/1047/testReport/org.apache.cassandra.transport/IdleDisconnectTest/;;;","31/Jan/20 01:22;aprudhomme;[~e.dimitrova] I am seeing the same with local testing. On alpha2 it fails most every time. On trunk it hasn't failed in 100+ runs.

I don't think this is a problem anymore.;;;","08/Feb/20 15:53;brandon.williams;Ran this over a thousand times without error on trunk.  Closing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make the upgrade tests run on trunk,CASSANDRA-15309,13255303,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vinaykumarcse,jolynch,jolynch,06/Sep/19 15:42,15/May/20 08:53,13/Jul/23 08:38,18/Nov/19 04:26,4.0,4.0-alpha3,,,,Test/dtest/python,,,,0,,,,"It appears that the upgrade tests (j8_upgradetests-no-vnodes circleci target) don't really work on trunk right now, it appears to be a java home issue potentially. Example run: https://circleci.com/gh/jolynch/cassandra/553
{noformat}
 Your job ran 4412 tests with 3923 failures

- test_IN_clause_on_last_key - upgrade_tests.cql_tests.TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_1_xupgrade_tests/cql_tests.pymajor_version_int = 8

    def switch_jdks(major_version_int):
        """"""
        Changes the jdk version globally, by setting JAVA_HOME = JAVA[N]_HOME.
        This means the environment must have JAVA[N]_HOME set to switch to jdk version N.
        """"""
        new_java_home = 'JAVA{}_HOME'.format(major_version_int)
    
        try:
>           os.environ[new_java_home]

upgrade_tests/upgrade_base.py:25: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = environ({'PYTHONUNBUFFERED': 'true', 'DEFAULT_DIR': '/home/cassandra/cassandra-dtest', 'CIRCLE_NODE_INDEX': '47', 'CIR...ade_tests/cql_tests.py::TestCQLNodes2RF1_Upgrade_current_2_1_x_To_indev_2_1_x::()::test_IN_clause_on_last_key (call)'})
key = 'JAVA8_HOME'

    def __getitem__(self, key):
        try:
            value = self._data[self.encodekey(key)]
        except KeyError:
            # raise KeyError with the original key value
>           raise KeyError(key) from None
E           KeyError: 'JAVA8_HOME'{noformat}",,djoshi,jolynch,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,vinaykumarcse,,,,,,,,,,,,Code,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Nov 18 04:26:23 UTC 2019,,,,,,,All,,,,,"0|z06enk:",9223372036854775807,,,,,,,jolynch,,,,Low,,4.0,,,https://github.com/apache/cassandra/commit/dd974b4b5f7770b293fdd8e4e76d7043508abec4,,,,,,,,,"Branch with fixes is [here|https://github.com/vinaykumarchella/cassandra/tree/CASSANDRA_15309]
 Successful upgrade Tests: [CircleCI tests|https://circleci.com/workflow-run/bc17668a-becc-46c4-bc70-cad21e75e968]",,,,,"25/Oct/19 07:08;vinaykumarcse;Fixed broken JAVA8_HOME in upgrade tests configurations. 

Branch with fixes is [here|https://github.com/vinaykumarchella/cassandra/tree/CASSANDRA_15309]
 Successful upgrade Tests: [CircleCI tests|https://circleci.com/workflow-run/bc17668a-becc-46c4-bc70-cad21e75e968];;;","04/Nov/19 20:39;jolynch;+1 on commit [f1aae0e6|https://github.com/apache/cassandra/commit/f1aae0e6524cd1c345a95000b9c28774b2f66418]

We can follow up on fixing the failing upgrade tests separately.;;;","04/Nov/19 21:12;djoshi;+1. Please revert the CircleCI changes before committing.;;;","18/Nov/19 04:26;vinaykumarcse;Thank you for reviewing [~djoshi] and [~jolynch]. Committed as [dd974b4b|https://github.com/apache/cassandra/commit/dd974b4b5f7770b293fdd8e4e76d7043508abec4];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flakey testAcquireReleaseOutbound - org.apache.cassandra.net.ConnectionTest,CASSANDRA-15308,13255300,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yifanc,jolynch,jolynch,06/Sep/19 15:35,21/Dec/20 08:07,13/Jul/23 08:38,03/Mar/20 16:30,4.0,4.0-alpha4,,,,Test/unit,,,,0,,,,"Example failure: https://circleci.com/gh/jolynch/cassandra/554#tests/containers/61
{noformat}
Your job ran 4428 tests with 1 failure
- testAcquireReleaseOutbound - org.apache.cassandra.net.ConnectionTest

junit.framework.AssertionFailedError
	at org.apache.cassandra.net.ConnectionTest.lambda$testAcquireReleaseOutbound$53(ConnectionTest.java:770)
	at org.apache.cassandra.net.ConnectionTest.lambda$doTest$8(ConnectionTest.java:238)
	at org.apache.cassandra.net.ConnectionTest.doTestManual(ConnectionTest.java:258)
	at org.apache.cassandra.net.ConnectionTest.doTest(ConnectionTest.java:236)
	at org.apache.cassandra.net.ConnectionTest.test(ConnectionTest.java:225)
	at org.apache.cassandra.net.ConnectionTest.testAcquireReleaseOutbound(ConnectionTest.java:767) {noformat}",,benedict,csplinter,dcapwell,e.dimitrova,jeromatron,jolynch,yifanc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,yifanc,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Low Hanging Fruit,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Feb 25 18:43:04 UTC 2020,,,,,,,All,,,,,"0|z06emw:",9223372036854775807,,,,,,,benedict,dcapwell,,,Low,,4.0-alpha,4.0-alpha1,,"[abeaa3ea5ef99691cc1b29787cfcd573a90e34fb|https://github.com/apache/cassandra/commit/abeaa3ea5ef99691cc1b29787cfcd573a90e34fb]",,,,,,,,,unit test and CI,,,,,"28/Jan/20 19:14;yifanc;[Code|https://github.com/yifan-c/cassandra/tree/CASSANDRA-15308], [PR|https://github.com/apache/cassandra/pull/433], [Test|https://app.circleci.com/github/yifan-c/cassandra/pipelines/55450fa4-3a68-4136-aa35-927daf158687/workflows/94e4335b-a73f-43c2-9a5a-4e5e6ad43c49]

{{testAcquireReleaseOutbound}} starts multiple testing rounds. In each round, it set up the test settings and starts N threads to contend for capacity. 
 The test fails due to leaking resources (insufficient global capacity). 
 The global limit that tracks the global capacity usage is shred among different rounds, and the reserved capacity is not completely released in each round.

The patch fixes the test by releasing all pending capacity at the end of each round.

_Update 1/29_
 The test result linked failed due to some other test. 
 All unit test passed in the recent run. [https://app.circleci.com/jobs/github/yifan-c/cassandra/214];;;","29/Jan/20 23:02;yifanc;cc [~benedict]
Hopefully, the fix did not change the intention of the test. ;;;","29/Jan/20 23:07;benedict;bq. the reserved capacity is not completely released in each round.

I haven't looked closely at this test, but I worry that this is unintentional and might indicate a bug.  Do we know why there is unreleased capacity at the end of the test?;;;","29/Jan/20 23:34;yifanc;bq. Do we know why there is unreleased capacity at the end of the test?

Yes. At the beginning of each round, new outbound was created with the shared variable {{MessagingService#outboundGlobalReserveLimit}}. Why a shared one is used? Because {{outboundTemplate}} is created with default, which picks up {{MessagingService#outboundGlobalReserveLimit}}. Therefore the acquired capacity from the previous round was counted in the {{global}} limit and carried over to the next one. However, the {{endpoint}} limit was reset for each round. 
;;;","30/Jan/20 00:10;benedict;This doesn't indicate why there is any unreleased capacity at the end of the test, only why there is some still allocated at the beginning of the next one.

A reasonable expectation of these tests is that they naturally release any allocated capacity before they complete.  Since the point of the test is to determine if allocation/release is working, artificially releasing any that is still held might mask a bug.;;;","30/Jan/20 00:28;yifanc;The unreleased capacity at the end of each round is the cause of ""there is some still allocated at the beginning of the next one.""

In the previous version of the test, this round *just ended* after asserting the {{outbound.pendingBytes()}} equals to the amount of acquired capacity. The pending bytes (i.e. still acquired capacity) were not released. Another potential issue is that if the assertion failed, the pending bytes are not going to be released as well. That is why the releasing is in the {{finally}} block.
{quote}A reasonable expectation of these tests is that they naturally release any allocated capacity before they complete.
{quote}
I agree. The previous test did not do that (i.e. not releasing the capacity before the round complete). The patch changed the test to have the behavior as you described.;;;","17/Feb/20 23:42;benedict;I don't know if I was just expressing myself poorly, but I don't _think_ your comment addressed my concern.  However, I've taken a look at the test myself and the issue I worried about is not a problem.

Is there a reason you don't simply release all remaining capacity, as opposed to looping releasing {{acquireStep}}?;;;","18/Feb/20 01:53;yifanc;I got your concern. It was probably mainly that I failed to explain it clearly. IMO, only the test was wrong for not cleaning up the remaining in the {{global}} and the same {{global}} is referenced in the next run. The {{global}} was supposed to be shared. So the implementation looked good to me. Just the test was wrong.
{quote}Is there a reason you don't simply release all remaining capacity, as opposed to looping releasing acquireStep?
{quote}
There is no API from {{OutboundConnection}} to release all remaining capacity. I do not feel like to add one more method just for testing purpose. Therefore, it uses the existing one and release the capacity by looping.

There is one potential place that can release all remaining capacity in the {{ConnectionTest#doTestManual}} by releasing the {{reserveCapacityInBytes}}. But I think it is more proper to clean up the remaining capacity in the test itself. Because only {{testAcquireReleaseOutbound}} tests the contention scenario by explicitly acquiring and releasing capacity.;;;","19/Feb/20 17:07;yifanc;[~benedict] review wanted :);;;","19/Feb/20 18:13;benedict;bq. There is no API from OutboundConnection to release all remaining capacity

There isn't, but we _can_ know how much we should release, and simply invoke {{unsafeReleaseCapacity}} with the amount, no?

This test could also do with some better comments, such as explaining that we ""reserve capacity"" before we begin releasing to ensure we cannot release more than we have acquired.  This is particularly important given the misleading comment at the bottom indicating the ability to release more than we acquire (since it only really means more than we acquired concurrently, else we would be at risk of assertion failures);;;","20/Feb/20 00:18;yifanc;I pushed a new commit to address the comments. [~benedict]

Comments are added to the test case to describe the what exactly the test does and what the result we expect. A testing method {{void unsafeReleaseCapacity(long count, long amount)}} was added to not looping release the remaining capacity. However, it need to be aware that it is a testing method that is located in the production scope. We already have several such methods declared in that class. 

Since they must only be used in test, potentially we can do the below assertion at the start of the methods annotated with {{VisibleForTesting}}, or make a new annotation that inserts the assertion. I did not add it in the code. Just mentioned here. 

{code:java}
assert Stream.of(Thread.currentThread().getStackTrace()).map(trace -> trace.getClassName().toLowerCase()).anyMatch(name -> name.contains(""test""));
{code}

;;;","20/Feb/20 14:54;benedict;Thanks, it looks good.  I've pushed some [minor suggestions|http://github.com/belliottsmith/cassandra/tree/15308-suggest] to further improve the test, specifically imposing a bound on the number of failures we witness, and randomising the invocation order of the concurrent operations so as to vary the behaviour a bit more between runs.;;;","20/Feb/20 19:50;yifanc;Cool. I like the idea of randomizing the invocation order and asserting the failures number upper bound.

The PR was updated according to the suggestions with several changes made on top of it.
 # Declare a fixed {{maxFailures}} as upper bound and calculate the {{acquireStep}} based on that.
 # Simplified the helper method that randomizes invoke oder.
 # The if/else added to the unsafe method is redundant. It got removed.

 ;;;","25/Feb/20 18:43;dcapwell;It seems that the try/finally is the actual reason the test is stable and all the other changes are style or improvement to the existing test.  All my comments were nit-picks or style things so I leave it to your discretion; no blocking comments.

+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flakey  test_remote_query - cql_test.TestCQLSlowQuery test,CASSANDRA-15307,13255298,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,jolynch,jolynch,06/Sep/19 15:32,15/May/20 08:38,13/Jul/23 08:38,30/Jan/20 15:45,4.0,4.0-alpha3,,,,Test/dtest/python,,,,0,,,,"Example failure: [https://circleci.com/gh/jolynch/cassandra/554#tests/containers/61]

 
{noformat}
Your job ran 959 tests with 1 failure
- test_remote_query cql_test.TestCQLSlowQuerycql_test.py

ccmlib.node.TimeoutError: 05 Sep 2019 23:05:07 [node2] Missing: ['operations were slow', 'SELECT \\* FROM ks.test2 WHERE id = 1']: DEBUG [BatchlogTasks:1] 2019-09-05 23:04:24,437 Ba..... See debug.log for remainder
self = <cql_test.TestCQLSlowQuery object at 0x7f4309528898>

    def test_remote_query(self):
        """"""
            Check that a query running on a node other than the coordinator is reported as slow:
    
            - populate the cluster with 2 nodes
            - start one node without having it join the ring
            - start the other one node with slow_query_log_timeout_in_ms set to a small value
              and the read request timeouts set to a large value (to ensure the query is not aborted) and
              read_iteration_delay set to a value big enough for the query to exceed slow_query_log_timeout_in_ms
              (this will cause read queries to take longer than the slow query timeout)
            - CREATE a table
            - INSERT 5000 rows on a session on the node that is not a member of the ring
            - run SELECT statements and check that the slow query messages are present in the debug logs
              (we cannot check the logs at info level because the no spam logger has unpredictable results)
    
            @jira_ticket CASSANDRA-12403
            """"""
        cluster = self.cluster
        cluster.set_configuration_options(values={'slow_query_log_timeout_in_ms': 10,
                                                  'request_timeout_in_ms': 120000,
                                                  'read_request_timeout_in_ms': 120000,
                                                  'range_request_timeout_in_ms': 120000})
    
        cluster.populate(2)
        node1, node2 = cluster.nodelist()
    
        node1.start(wait_for_binary_proto=True, join_ring=False)  # ensure other node executes queries
        node2.start(wait_for_binary_proto=True,
                    jvm_args=[""-Dcassandra.monitoring_report_interval_ms=10"",
                              ""-Dcassandra.test.read_iteration_delay_ms=1""])  # see above for explanation
    
        session = self.patient_exclusive_cql_connection(node1)
    
        create_ks(session, 'ks', 1)
        session.execute(""""""
                CREATE TABLE test2 (
                    id int,
                    col int,
                    val text,
                    PRIMARY KEY(id, col)
                );
            """""")
    
        for i, j in itertools.product(list(range(100)), list(range(10))):
            session.execute(""INSERT INTO test2 (id, col, val) VALUES ({}, {}, 'foo')"".format(i, j))
    
        # only check debug logs because at INFO level the no-spam logger has unpredictable results
        mark = node2.mark_log(filename='debug.log')
        session.execute(SimpleStatement(""SELECT * from test2"",
                                        consistency_level=ConsistencyLevel.ONE,
                                        retry_policy=FallthroughRetryPolicy()))
        node2.watch_log_for([""operations were slow"", ""SELECT \* FROM ks.test2""],
                            from_mark=mark, filename='debug.log', timeout=60)
    
    
        mark = node2.mark_log(filename='debug.log')
        session.execute(SimpleStatement(""SELECT * from test2 where id = 1"",
                                        consistency_level=ConsistencyLevel.ONE,
                                        retry_policy=FallthroughRetryPolicy()))
        node2.watch_log_for([""operations were slow"", ""SELECT \* FROM ks.test2 WHERE id = 1""],
>                           from_mark=mark, filename='debug.log', timeout=60)

cql_test.py:1150: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ccmlib.node.Node object at 0x7f430b0cfcc0>
exprs = ['operations were slow', 'SELECT \\* FROM ks.test2 WHERE id = 1']
from_mark = 166214, timeout = 60, process = None, verbose = False
filename = 'debug.log'

    def watch_log_for(self, exprs, from_mark=None, timeout=600, process=None, verbose=False, filename='system.log'):
        """"""
            Watch the log until one or more (regular) expression are found.
            This methods when all the expressions have been found or the method
            timeouts (a TimeoutError is then raised). On successful completion,
            a list of pair (line matched, match object) is returned.
            """"""
        start = time.time()
        tofind = [exprs] if isinstance(exprs, string_types) else exprs
        tofind = [re.compile(e) for e in tofind]
        matchings = []
        reads = """"
        if len(tofind) == 0:
            return None
    
        log_file = os.path.join(self.get_path(), 'logs', filename)
        output_read = False
        while not os.path.exists(log_file):
            time.sleep(.5)
            if start + timeout < time.time():
                raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Timed out waiting for {} to be created."".format(log_file))
            if process and not output_read:
                process.poll()
                if process.returncode is not None:
                    self.print_process_output(self.name, process, verbose)
                    output_read = True
                    if process.returncode != 0:
                        raise RuntimeError()  # Shouldn't reuse RuntimeError but I'm lazy
    
        with open(log_file) as f:
            if from_mark:
                f.seek(from_mark)
    
            while True:
                # First, if we have a process to check, then check it.
                # Skip on Windows - stdout/stderr is cassandra.bat
                if not common.is_win() and not output_read:
                    if process:
                        process.poll()
                        if process.returncode is not None:
                            self.print_process_output(self.name, process, verbose)
                            output_read = True
                            if process.returncode != 0:
                                raise RuntimeError()  # Shouldn't reuse RuntimeError but I'm lazy
    
                line = f.readline()
                if line:
                    reads = reads + line
                    for e in tofind:
                        m = e.search(line)
                        if m:
                            matchings.append((line, m))
                            tofind.remove(e)
                            if len(tofind) == 0:
                                return matchings[0] if isinstance(exprs, string_types) else matchings
                else:
                    # yep, it's ugly
                    time.sleep(1)
                    if start + timeout < time.time():
>                       raise TimeoutError(time.strftime(""%d %b %Y %H:%M:%S"", time.gmtime()) + "" ["" + self.name + ""] Missing: "" + str([e.pattern for e in tofind]) + "":\n"" + reads[:50] + "".....\nSee {} for remainder"".format(filename))
E                       ccmlib.node.TimeoutError: 05 Sep 2019 23:05:07 [node2] Missing: ['operations were slow', 'SELECT \\* FROM ks.test2 WHERE id = 1']:
E                       DEBUG [BatchlogTasks:1] 2019-09-05 23:04:24,437 Ba.....
E                       See debug.log for remainder

../env/src/ccm/ccmlib/node.py:536: TimeoutError {noformat}",,csplinter,e.dimitrova,jeromatron,jolynch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/20 14:11;e.dimitrova;CASSANDRA-15307-fixed.txt;https://issues.apache.org/jira/secure/attachment/12992235/CASSANDRA-15307-fixed.txt","29/Jan/20 18:48;e.dimitrova;CASSANDRA-15307.txt;https://issues.apache.org/jira/secure/attachment/12992158/CASSANDRA-15307.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,e.dimitrova,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Jan 30 15:45:08 UTC 2020,,,,,,,All,,,,,"0|z06emg:",9223372036854775807,,,,,,,brandon.williams,,,,Low,,3.2,,,https://github.com/apache/cassandra-dtest/commit/986318e6fe027272338ef48da7cb2b86656db94b,,,,,,,,,"[Patch|https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15307].  [Pull request|https://github.com/ekaterinadimitrova2/cassandra-dtest/pull/2]",,,,,"29/Jan/20 19:02;e.dimitrova;Output of running it 50 times locally on trunk attached. 

 ;;;","29/Jan/20 23:02;e.dimitrova;After checking the initial implementation at CASSANDRA-12403, the fix at CASSANDRA-13050, and inspecting the logs - I changed Dcassandra.test.read_iteration_delay_ms to 2 milliseconds and it looks like this solves the flakiness also for v4 (it was already solved for 3.11 In CASSANDRA-13050).

Preliminary testing of 50 runs shows 100% success rate.

I will run it 300 times tonight to ensure the fix is stable.  

 ;;;","30/Jan/20 14:52;e.dimitrova;300 runs completed successfully. Attached is the output.

There are a couple of tests that show longer completion as my laptop fell asleep. 

[Patch|https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15307].  [Pull request|https://github.com/ekaterinadimitrova2/cassandra-dtest/pull/2]

[~brandon.williams], can you, please, review it? Thanks;;;","30/Jan/20 15:45;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix multi DC nodetool status output,CASSANDRA-15305,13255289,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,clohfink,cnlwsu,cnlwsu,06/Sep/19 14:40,21/Dec/20 08:07,13/Jul/23 08:38,17/Feb/20 21:54,4.0,4.0-alpha4,,,,Tool/nodetool,,,,0,,,,"The DC headers are printed before the entire table output

{code}
Datacenter: eu-west-1
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
Datacenter: us-east-1
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load        Tokens  Owns (effective)  Host ID                               Rack      
UN  100.67.118.115  76.85 KiB   1       16.7%             e1bb616a-a22d-4452-a2eb-f14faa6c7900  eu-west-1c
UN  100.67.97.192   110.42 KiB  1       16.7%             fe552b3e-33bf-42c2-9c71-c4e49a034b70  eu-west-1a
UN  100.67.107.135  76.85 KiB   1       16.7%             a9079fbe-8d7b-4f07-8d5f-b9e70a63d799  eu-west-1b
UN  100.67.97.213   94.59 KiB   1       16.7%             948e123a-7a92-4b35-974f-c7b380998e37  eu-west-1a
UN  100.67.118.203  110.42 KiB  1       16.7%             6a7d1845-e966-4aba-9245-ed3f5c0805d6  eu-west-1c
UN  100.67.107.110  110.42 KiB  1       16.7%             6e23da83-e9c8-4c43-a9d1-fb0d6e775cdc  eu-west-1b
--  Address         Load        Tokens  Owns (effective)  Host ID                               Rack      
UN  100.66.217.160  110.42 KiB  1       16.7%             7abef190-f4e7-46e6-9e8d-d4441e6b2e33  us-east-1c
UN  100.66.220.37   110.42 KiB  1       16.7%             6712d2bf-c912-460f-abeb-278770ad7cb4  us-east-1e
UN  100.66.220.90   110.43 KiB  1       16.7%             dcb25670-4011-457c-8018-04b22042dde4  us-east-1e
UN  100.66.216.184  110.42 KiB  1       16.7%             b620b09a-3de8-41c3-a330-6cee808b4a84  us-east-1c
UN  100.66.219.60   110.42 KiB  1       16.7%             34769195-c582-413f-a663-82f8544d9327  us-east-1d
UN  100.66.219.93   110.42 KiB  1       16.7%             9ae3372c-1a04-457b-bcce-3562a328565e  us-east-1d
{code}",,cnlwsu,csplinter,e.dimitrova,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,clohfink,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Feb 17 21:55:26 UTC 2020,,,,,,,All,,,,,"0|z06ekg:",9223372036854775807,,,,,,,brandon.williams,cnlwsu,,,Low,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/d9798369dd99cfe44b10aa0aefb5e20ca702e8c2,,,,,,,,,na,,,,,"06/Sep/19 14:45;cnlwsu;Half the code for this is copy and pasted for if its {{HostStatWithPort}} or {{HostStat}} when the two objects are essentially identical so I also changed that part to just be once while fixing this.

{code}
Datacenter: dc1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load        Tokens  Owns (effective)  Host ID                               Rack
UN  127.0.0.2  173.46 KiB  1       ?                 0787644a-fe22-43dc-9013-d1819e2c6344  r1  
UN  127.0.0.1  1.08 GiB    1       ?                 fe364237-7a05-4621-b105-8edb483b1fca  r1  

Datacenter: dc2
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load        Tokens  Owns (effective)  Host ID                               Rack
UN  127.0.0.4  135.64 KiB  1       ?                 a618ace4-0047-411b-9d89-f80ae3a41e08  r1  
UN  127.0.0.3  135.72 KiB  1       ?                 f7cc3d2a-7fb0-41fb-8ad1-be1b3515abd8  r1  


{code};;;","16/Jan/20 15:06;brandon.williams;+1;;;","17/Feb/20 21:55;cnlwsu;Thanks for the review [~brandon.williams]! 

| [tests|https://circleci.com/gh/clohfink/cassandra/694] | [commit|https://github.com/apache/cassandra/commit/d9798369dd99cfe44b10aa0aefb5e20ca702e8c2] |;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
drop column statement should not initialize timestamp because of statement cache,CASSANDRA-15303,13255141,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,jasonstack,jasonstack,06/Sep/19 03:23,15/May/20 08:41,13/Jul/23 08:38,16/Mar/20 15:21,4.0,4.0-alpha4,,,,CQL/Interpreter,,,,0,,,,"When executing drop-column query without timestamp, {{AlterTableStatement#Raw}} initializes a default timestamp and then the prepared statement is cached. The same timestamp will be reused for the same drop-column query.  (related to CASSANDRA-13426)

 

The fix is to use NULL timestamp to indicate: using statement execution time instead.

 

patch: [https://github.com/jasonstack/cassandra/commits/fix-drop-column-timestamp]",,aleksey,blerer,ifesdjeen,jasonstack,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jasonstack,,,,,,,,,,,,Correctness,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Mar 20 16:59:21 UTC 2020,,,,,,,All,,,,,"0|z06dnk:",9223372036854775807,,,,,,,blerer,,,,Normal,,4.0,,,https://github.com/apache/cassandra/commit/722d10b2c5999f19f3a912dc2fbaeb71cef65a07,,,,,,,,,Verified by unit test,,,,,"06/Sep/19 03:24;jasonstack;cc [~iamaleksey];;;","16/Mar/20 11:25;blerer;The fix looks good. I just believe that we should fix the {{AlterTest}} test to not use prepared statements for schema changes by default. I pushed a patch for that [here|https://github.com/apache/cassandra/commit/56c1b0538e344f030cec622f05b9bb220d93ef9f]. [~jasonstack] If my change looks good to you, I think we should push it at the same time as your fix.  ;;;","16/Mar/20 11:37;jasonstack;[~blerer] the commit looks good to me!;;;","16/Mar/20 15:21;blerer;Committed into trunk at 722d10b2c5999f19f3a912dc2fbaeb71cef65a07;;;","20/Mar/20 16:49;ifesdjeen;This commit has broken AlterTest. I've ninja-commited the fix for trunk here [b29af2925cddacb4ab8b429b31917748781fbe5d|https://github.com/apache/cassandra/commit/b29af2925cddacb4ab8b429b31917748781fbe5d];;;","20/Mar/20 16:59;jasonstack;[~ifesdjeen] thanks.. It looks the fix is required for ""testCreateSimpleAlterNTSDefaults()"" as well..

I previously made [a fix|https://github.com/jasonstack/cassandra/commits/CASSANDRA-15303-trunk-fix] to move `testCreateAlterNetworkTopologyWithDefaults / testCreateSimpleAlterNTSDefaults ` into another class.. 

 [CI|https://circleci.com/workflow-run/59ea8e29-df8b-40fd-9cfc-1a8d8b78d3a7] unit tests passed, dtest failure is not related..;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4.0 rpmbuild spec file is missing auditlogviewer and fqltool,CASSANDRA-15300,13254333,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ysakanaka,ysakanaka,ysakanaka,03/Sep/19 06:51,15/May/20 08:41,13/Jul/23 08:38,14/Feb/20 06:29,4.0,4.0-alpha4,,,,Packaging,,,,0,pull-request-available,,,"The spec file on the current trunk branch (cassandra 4.0) is missing auditlogviewer and fqltool.

I tried rpmbuild on trunk brunch, but it failed with unpacked files error.

RPM build errors:

    Installed (but unpackaged) file(s) found:

   /usr/bin/auditlogviewer

   /usr/bin/fqltool

I guess the committers will modify this file in the future because they are new features but I suggest that the following lines be added into the spec file.

 

%attr(755,root,root) %\{_bindir}/auditlogviewer

%attr(755,root,root) %\{_bindir}/fqltool

 

thanks.

 

[PATCH] Add auditlogviewer and fqltool into rpmbuild spec file. patch

 by ysakanaka; for CASSANDRA-15300

 

—

 redhat/cassandra.spec | 2 ++

 1 file changed, 2 insertions(+)

 

diff --git a/redhat/cassandra.spec b/redhat/cassandra.spec

index eaf7565..0aedbd7 100644

— a/redhat/cassandra.spec

+++ b/redhat/cassandra.spec

@@ -173,6 +173,8 @@ This package contains extra tools for working with Cassandra clusters.

 %attr(755,root,root) %\{_bindir}/sstableofflinerelevel

 %attr(755,root,root) %\{_bindir}/sstablerepairedset

 %attr(755,root,root) %\{_bindir}/sstablesplit

+%attr(755,root,root) %\{_bindir}/auditlogviewer

+%attr(755,root,root) %\{_bindir}/fqltool

 

 

 %changelog

-- 

1.8.3.1

 ",,jeromatron,ysakanaka,,,,,,,,,,,,,,,,,,,,,,,,,,"ysakanaka commented on pull request #351: Add auditlogviewer and fqltool into rpmbuild spec file for  CASSANDRA-15300
URL: https://github.com/apache/cassandra/pull/351
 
 
   patch by ysakanaka; for [CASSANDRA-15300](https://issues.apache.org/jira/browse/CASSANDRA-15300)
   
   The spec file on the current trunk branch (cassandra 4.0) is missing auditlogviewer and fqltool.
   I tried rpmbuild on trunk brunch, but it failed with unpacked files error.
   RPM build errors:
       Installed (but unpackaged) file(s) found:
      /usr/bin/auditlogviewer
      /usr/bin/fqltool
   I guess the committers will modify this file in the future because they are new features but I suggest that the following lines be added into the spec file.
    
   %attr(755,root,root) %{_bindir}/auditlogviewer
   %attr(755,root,root) %{_bindir}/fqltool
    
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Sep/19 07:57;githubbot;600","ysakanaka commented on pull request #351: Add auditlogviewer and fqltool into rpmbuild spec file for  CASSANDRA-15300
URL: https://github.com/apache/cassandra/pull/351
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/20 16:12;githubbot;600","ysakanaka commented on pull request #351: Add auditlogviewer and fqltool into rpmbuild spec file for  CASSANDRA-15300
URL: https://github.com/apache/cassandra/pull/351
 
 
   patch by ysakanaka; for [CASSANDRA-15300](https://issues.apache.org/jira/browse/CASSANDRA-15300)
   
   The spec file on the current trunk branch (cassandra 4.0) is missing auditlogviewer and fqltool.
   I tried rpmbuild on trunk brunch, but it failed with unpacked files error.
   RPM build errors:
       Installed (but unpackaged) file(s) found:
      /usr/bin/auditlogviewer
      /usr/bin/fqltool
   I guess the committers will modify this file in the future because they are new features but I suggest that the following lines be added into the spec file.
    
   %attr(755,root,root) %{_bindir}/auditlogviewer
   %attr(755,root,root) %{_bindir}/fqltool
    
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/20 16:12;githubbot;600","ysakanaka commented on issue #351: Add auditlogviewer and fqltool into rpmbuild spec file for  CASSANDRA-15300
URL: https://github.com/apache/cassandra/pull/351#issuecomment-596103947
 
 
   Fixed by others.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/20 16:13;githubbot;600","ysakanaka commented on pull request #351: Add auditlogviewer and fqltool into rpmbuild spec file for  CASSANDRA-15300
URL: https://github.com/apache/cassandra/pull/351
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Mar/20 16:13;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/19 07:21;ysakanaka;15300-4.0.txt;https://issues.apache.org/jira/secure/attachment/12979192/15300-4.0.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,ysakanaka,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,Workload Replay,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Feb 14 06:29:55 UTC 2020,,,,,,,All,,,,,"0|z069j4:",9223372036854775807,,,,,,,yukim,,,,Normal,,,,,,,,,,,,,,"From 95e764f65c736045569a512ce14caa5f2fdb0ecd Mon Sep 17 00:00:00 2001
From: ysakanaka <ysakanaka@users.noreply.github.com>
Date: Tue, 3 Sep 2019 16:08:18 +0900
Subject: [PATCH] Add auditlogviewer and fqltool into rpmbuild spec file. patch
 by ysakanaka; for CASSANDRA-15300

---
 redhat/cassandra.spec | 2 ++
 1 file changed, 2 insertions(+)

diff --git a/redhat/cassandra.spec b/redhat/cassandra.spec
index eaf7565..0aedbd7 100644
--- a/redhat/cassandra.spec
+++ b/redhat/cassandra.spec
@@ -173,6 +173,8 @@ This package contains extra tools for working with Cassandra clusters.
 %attr(755,root,root) %\{_bindir}/sstableofflinerelevel
 %attr(755,root,root) %\{_bindir}/sstablerepairedset
 %attr(755,root,root) %\{_bindir}/sstablesplit
+%attr(755,root,root) %\{_bindir}/auditlogviewer
+%attr(755,root,root) %\{_bindir}/fqltool
 
 
 %changelog
-- 
1.8.3.1",,,,,"14/Feb/20 06:26;yukim;Looks like the fix for this is already committed in [fc4381ca89ab39a82c9018e5171975285cc3bfe7|https://github.com/apache/cassandra/commit/fc4381ca89ab39a82c9018e5171975285cc3bfe7].
This can be closed.;;;","14/Feb/20 06:29;yukim;Closing as fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool can not create snapshot with snapshot name that have special character,CASSANDRA-15297,13254184,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,saranya_k,maxwellguo,maxwellguo,02/Sep/19 08:45,27/May/22 19:24,13/Jul/23 08:38,18/Mar/22 14:17,4.0.4,4.1,4.1-alpha1,,,Tool/nodetool,,,,0,pull-request-available,,,"we make snapshot through ""nodetool snapshot -t snapshotname "" , when snapshotname contains special characters like ""/"", the make snapshot process successfully , but the result 
can be different ,when we check the data file directory or use ""nodetool listsnapshots"".
here is some case :

1. nodetool snapshot -t ""p/s""
the listsnapshot resturns snapshot  p for all table but actually the snapshot name is ""p/s"";
also the data directory is like the format : datapath/snapshots/p/s/snapshot-datafile-link
  !snapshot-p-s.jpg! 
 !listsnapshots-p-s.jpg! 
2. nodetool snapshot -t ""/""
the listsnapshot resturns ""there is not snapshot""; but the make snapshot process return successfully and the data directory is like the format : datapath/snapshots/snapshot-datafile-link
 !snapshot-listsnapshot-.jpg! 

the Attachements are the result under our environment.

so for me , we suggest that the snapshot name should not contains special character. just throw exception and told the user not to use  special character.

",,blerer,cnlwsu,frankgh,maxwellguo,saranya_k,,,,,,,,,,,,,,,,,,,,,,,"cclive1601 commented on pull request #349: Fix snapshot name with special character for Path CASSANDRA-15297
URL: https://github.com/apache/cassandra/pull/349
 
 
   nodetool can not create snapshot with snapshot name that have special character, when user make snapshot with name contains ""/"", then an exception is thrown
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;02/Sep/19 09:31;githubbot;600","cclive1601 commented on pull request #349: Fix snapshot name with special character for Path CASSANDRA-15297
URL: https://github.com/apache/cassandra/pull/349
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 03:09;githubbot;600","cclive1601 commented on pull request #352: Fix snapshot name with special character for Path CASSANDRA-15297
URL: https://github.com/apache/cassandra/pull/352
 
 
   nodetool can not create snapshot with snapshot name that have special character, when user make snapshot with name contains ""/"", then an exception is thrown
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Sep/19 03:12;githubbot;600","smiklosovic closed pull request #352:
URL: https://github.com/apache/cassandra/pull/352


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Mar/22 19:12;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/19 08:58;maxwellguo;after-fix.jpg;https://issues.apache.org/jira/secure/attachment/12979121/after-fix.jpg","06/Sep/19 01:48;cnlwsu;image.png;https://issues.apache.org/jira/secure/attachment/12979596/image.png","02/Sep/19 09:01;maxwellguo;listsnapshots-p-s.jpg;https://issues.apache.org/jira/secure/attachment/12979124/listsnapshots-p-s.jpg","02/Sep/19 09:03;maxwellguo;snapshot-listsnapshot-.jpg;https://issues.apache.org/jira/secure/attachment/12979125/snapshot-listsnapshot-.jpg","02/Sep/19 09:01;maxwellguo;snapshot-p-s.jpg;https://issues.apache.org/jira/secure/attachment/12979122/snapshot-p-s.jpg",,,,,,,,,,,,,,,,,,,,,,,,,5.0,saranya_k,,,,,,,,,,,,Correctness -> Unrecoverable Corruption / Loss,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Mar 18 14:17:17 UTC 2022,,,,,,,All,,,,,"0|z068m0:",9223372036854775807,,,,,,,blerer,cnlwsu,smiklosovic,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/e773bbd9c6c52a2fedc127cb7ab77a1fdbeb63d2,,,,,,,,,"for this is just filter the special charter ,so we do normal test with snapshot and snapshot with special charter in snapshot name and got the result we need",,,,,"02/Sep/19 08:58;maxwellguo;after we fix the bug the resultt should be like this :  !after-fix.jpg! ;;;","06/Sep/19 01:56;cnlwsu;Nitpicking there an extra trailing on string and since its just {{File.separator}} can probably just say in message {code}""Snapshot name cannot contain "" +  File.separator{code}

With that it looks good to me.;;;","06/Sep/19 02:03;maxwellguo;Thank you for your advice, i will modify it .;;;","06/Sep/19 03:19;maxwellguo;[~cnlwsu] hi, i just modify the message and some space issue. For my local environment got some problem ,so i just made a new PR. url : https://github.com/apache/cassandra/pull/352
;;;","06/Sep/19 20:03;cnlwsu;LGTM +1;;;","06/Sep/19 22:37;maxwellguo;thank you .;;;","20/Aug/21 14:51;blerer;[~maxwellguo] The fix look good but there is no test for it. Would you mind adding a nodetool test for that scenario?;;;","11/Dec/21 00:52;saranya_k;[~blerer] I see this PR is still open, can I take it up and complete?;;;","11/Dec/21 13:36;blerer;[~saranya_k] Yes. It just need some nodetool unit test.;;;","21/Jan/22 20:48;saranya_k;[~blerer] I created a PR after adding unit test https://github.com/apache/cassandra/pull/1420;;;","31/Jan/22 15:49;blerer;[~saranya_k] Thanks for the test. It looks good :-);;;","31/Jan/22 16:30;blerer;CI runs: [4.0|https://app.circleci.com/pipelines/github/blerer/cassandra/257/workflows/4f2fec7f-41ed-4863-ae0a-e2c2404a38d1] and [trunk|https://app.circleci.com/pipelines/github/blerer/cassandra/256/workflows/4d3cc691-b340-4bac-801a-4a760e7ca161];;;","01/Feb/22 13:33;blerer;[~saranya_k] on 4.0 some of the new tests are failing [here|https://app.circleci.com/pipelines/github/blerer/cassandra/257/workflows/4f2fec7f-41ed-4863-ae0a-e2c2404a38d1/jobs/2333] and the {{File}} import used for the {{File.separator}} is making the build fail. [~samt], [~benedict] do you know what should be use to avoid this problem ?  ;;;","01/Feb/22 14:23;benedict;bq. the File import used for the File.separator 

Link?;;;","01/Feb/22 14:34;brandon.williams;I think now that we don't use (and forbid via checkstyle) File and friends, we need to know what the usable equivalent is to File.separator.

I think now that we don't support windows this probably isn't even needed?;;;","01/Feb/22 14:48;benedict;There's {{File.pathSeparator()}} in the replacement for {{File}};;;","08/Feb/22 00:47;saranya_k;Thanks for pointing, made the change to use 
{code:java}
File.pathSeparator() {code};;;","18/Mar/22 14:10;blerer;|| Branches || CI ||
|[4.0|https://github.com/frankgh/cassandra/commit/2e90cd45505d139ee2310069d845f6e4091f235f] | [j8|https://app.circleci.com/pipelines/github/frankgh/cassandra?branch=CASSANDRA-15297-trunk&filter=all] |
|[trunk|https://github.com/frankgh/cassandra/commit/23c9550eac057eeab1e7e6b02c73ccd845618681] | [j8|https://app.circleci.com/pipelines/github/frankgh/cassandra?branch=CASSANDRA-15297-trunk&filter=all] |;;;","18/Mar/22 14:15;blerer;Thanks [~maxwellguo], [~saranya_k] and [~frankgh] for the patch.;;;","18/Mar/22 14:17;blerer;Committed in 4.0 at e773bbd9c6c52a2fedc127cb7ab77a1fdbeb63d2 and merged into trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZstdCompressor compression_level setting,CASSANDRA-15296,13254013,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rha,dvohra,dvohra,30/Aug/19 21:50,15/May/20 08:54,13/Jul/23 08:38,24/Sep/19 18:54,4.0,4.0-alpha2,,,,Dependencies,Feature/Compression,,,0,,,,"The DEFAULT_COMPRESSION_LEVEL for ZstdCompressor is set to 3, but its range for compression_level is indicated to be between -131072 and 2.  The default value is outside the range. Is it by design or a bug?

{code:java}

- ``compression_level`` is only applicable for ``ZstdCompressor`` and accepts values between ``-131072`` and ``2``.

// Compressor Defaults
    public static final int DEFAULT_COMPRESSION_LEVEL = 3;

{code}

https://github.com/apache/cassandra/commit/dccf53061a61e7c632669c60cd94626e405518e9",,djoshi,dvohra,jeromatron,rha,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/19 17:41;rha;15296-trunk.txt;https://issues.apache.org/jira/secure/attachment/12979167/15296-trunk.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,rha,,,,,,,,,,,,Code,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Sep 24 18:54:36 UTC 2019,,,,,,,All,,,,,"0|z067k0:",9223372036854775807,,,,,,,djoshi,,,,Low,,4.0,,,https://github.com/apache/cassandra/commit/a7f89688e5b4fdc2f8990fc42cb593da4f6a923f,,,,,,,,,This is a documentation bug.,,,,,"02/Sep/19 17:02;rha; TLDR: the compression level interval is {{[-(1<<17), 22]}} i.e. {{[-131072, 22]}}

Looking at Z Standard C sources, the maximum is 22, not 2 (it's a typo):
{code:c}
#define ZSTD_MAX_CLEVEL     22
int ZSTD_maxCLevel(void) { return ZSTD_MAX_CLEVEL; }
int ZSTD_minCLevel(void) { return (int)-ZSTD_TARGETLENGTH_MAX; }
{code}
[https://github.com/facebook/zstd/blob/519834738228cc810b48a2d44ff295b845842af4/lib/compress/zstd_compress.c#L3823]

 
 {{ZSTD_TARGETLENGTH_MAX}} is defined as:
{code:c}
#define ZSTD_TARGETLENGTH_MAX    ZSTD_BLOCKSIZE_MAX
{code}
{code:c}
#define ZSTD_BLOCKSIZELOG_MAX  17
#define ZSTD_BLOCKSIZE_MAX     (1<<ZSTD_BLOCKSIZELOG_MAX)
{code};;;","02/Sep/19 17:42;rha;Patch to fix the typo and add information about compression levels.;;;","02/Sep/19 17:48;djoshi;Hi [~rha] and [~dvohra]. Thanks for reporting this issue. It looks like this might be a typo in the documentation.;;;","03/Sep/19 04:34;dvohra;Thanks [~djoshi3] for fixing the bug.;;;","24/Sep/19 18:51;djoshi;+1 LGTM;;;","24/Sep/19 18:54;djoshi;Committed. Thanks for the report [~dvohra] and patch [~rha].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Running into deadlock when do CommitLog initialization,CASSANDRA-15295,13253939,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gzh1992n,gzh1992n,gzh1992n,30/Aug/19 13:41,16/Mar/22 12:29,13/Jul/23 08:38,12/Dec/19 18:17,4.0,4.0-alpha3,,,,Local/Commit Log,,,,0,,,,"Recently, I found a cassandra(3.11.4) node stuck in STARTING status for a long time.
 I used jstack to saw what happened. The main thread stuck in *AbstractCommitLogSegmentManager.awaitAvailableSegment*
 !screenshot-1.png! 

The strange thing is COMMIT-LOG-ALLOCATOR thread state was runnable but it was not actually running.  
 !screenshot-2.png! 

And then I used pstack to troubleshoot. I found COMMIT-LOG-ALLOCATOR block on java class initialization.
  !screenshot-3.png! 

This is a deadlock obviously. CommitLog waits for a CommitLogSegment when initializing. In this moment, the CommitLog class is not initialized and the main thread holds the class lock. After that, COMMIT-LOG-ALLOCATOR creates a CommitLogSegment with exception and call *CommitLog.handleCommitError*(static method).  COMMIT-LOG-ALLOCATOR will block on this line because CommitLog class is still initializing.



 

 ",,dikanggu,djoshi,gzh1992n,jasonstack,jeromatron,jjirsa,jwest,weideng,,,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #347:
URL: https://github.com/apache/cassandra/pull/347


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:29;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,CASSANDRA-15692,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/19 00:05;dcapwell;image.png;https://issues.apache.org/jira/secure/attachment/12983788/image.png","30/Aug/19 13:44;gzh1992n;jstack.log;https://issues.apache.org/jira/secure/attachment/12978973/jstack.log","30/Aug/19 13:45;gzh1992n;pstack.log;https://issues.apache.org/jira/secure/attachment/12978974/pstack.log","30/Aug/19 13:53;gzh1992n;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/12978975/screenshot-1.png","30/Aug/19 13:54;gzh1992n;screenshot-2.png;https://issues.apache.org/jira/secure/attachment/12978976/screenshot-2.png","30/Aug/19 13:54;gzh1992n;screenshot-3.png;https://issues.apache.org/jira/secure/attachment/12978977/screenshot-3.png",,,,,,,,,,,,,,,,,,,,,,,,6.0,djoshi,gzh1992n,,,,,,,,,,,Availability -> Unavailable,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Dec 12 18:17:17 UTC 2019,,,,,,,All,,,,,"0|z0673k:",9223372036854775807,,,,,,,djoshi,jwest,,,Normal,,4.0,,,https://github.com/apache/cassandra/commit/3a8300e0b86c4acfb7b7702197d36cc39ebe94bc,,,,,,,,,Unspecified ,,,,,"02/Sep/19 03:48;jjirsa;PR from reporter at https://github.com/apache/cassandra/pull/347;;;","03/Sep/19 06:18;gzh1992n;Thank you [~jjirsa] for your review and I look forward to your feedback.;;;","06/Sep/19 01:35;jwest;Thanks for the report [~gzh1992n]. I was able to easily reproduce the exact stack trace by introducing an artificial exception into the try block in {{runMayThrow}}. This is clearly an issue with using the class from another thread during initialization. I will take a look at your patch.  ;;;","06/Sep/19 13:24;gzh1992n;Thanks for review [~jrwest].;;;","17/Sep/19 20:58;jwest;Happy to [~gzh1992n]. A few comments:
 
 * I verified it does not affect 3.0 branch because {{CommitLogSegmentManager#start}} exits immediately after starting {{managerThread}} instead of calling {{advaceAllocatingFrom(null)}};
 * The database doesn’t start, which causes many tests to fail as well, because there is no default commit log segment manager factory set. Test runs: https://circleci.com/gh/jrwest/cassandra/tree/bug-commitlog-deadlock
 * CommitLogInitWithExpcetionTest#L63 - should check prior to this call that initThread is not null

Minor naming nits (do with them what you please):
 * Rename KillerHook => OnKillHook, and onKill => execute
 * Drop the “I*” naming for the CommitLogSegmentMgrFactoryInterface. Consider renaming it CommitLogSegmentManagerFactory;;;","18/Sep/19 11:49;gzh1992n;Thanks for the comments [~jrwest]. I will update the patch as soon as possible.;;;","23/Sep/19 16:47;jwest;[~gzh1992n] Thanks! If you would like me to go ahead and make the changes let me know. It would be great to get this into the future 4.0-alpha2 release as well as the 3.11 branch. ;;;","24/Sep/19 03:37;gzh1992n;I am sorry to reply you so late [~jrwest]. I had updated the PR. Please see:  https://github.com/apache/cassandra/pull/347;;;","01/Oct/19 03:09;jwest;Thanks [~gzh1992n]. I've verified the database starts with the most recent changes. The only thing I'm not familiar with is the changes in {{DatabaseDescriptorRefTest}}. I'll need to take a closer look. ;;;","02/Oct/19 21:57;jwest;Took a closer look at {{DatabaseDescriptorRefTest}}. Thank you for ensuring that was updated. [~djoshi] is going to help w/ a second review so we can get this committed. 

Test runs can be found here: https://circleci.com/gh/jrwest/cassandra/tree/bug-commitlog-deadlock;;;","09/Oct/19 23:22;djoshi;Hi [~gzh1992n], thanks for the patch. When I looked at the issue closely, the deadlock can be avoided if we don't start a {{Thread}} in a static initializer block and calling a method on an partially initialized object. This is a classic concurrency issue. Now, the way you have solved it is by moving the error handling to a different class but I think it is still needs to go a bit further. I have mocked up a very minimal change here: https://github.com/apache/cassandra/compare/trunk...dineshjoshi:15295-trunk?expand=1 It is the minimal set of changes required to avoid the deadlock and it also ensures that we operate on a fully initialized object. We can incorporate your refactor as well but I think it is important to get the correctness issue resolved first. It also requires a bit more guarding in {{CommitLog::start()}} so it's not started twice.

I have also not completed evaluating whether this change will cause any other issues as we are changing initialization behavior.;;;","10/Oct/19 07:15;gzh1992n;Hi, [~djoshi] thanks for the review. I agree with most of what you said. Your change is very minimal for this issue. But something else I have to remind you.

1. Most of the changes in my patch are to build a UT to check the exception case for CommitLog. It's worth it for Cassandra. Not only we have to fix this problem but we also need to understand the root cause of the problem (lack of exception tests).

2. Your change introduces the risk of starting twice. CommitLog was designed as a singleton and It manages lifecycle by itself. When other modules call CommitLog.instance, they expect an initialized CommitLog. You change the original initialization process.

3. The major change (move to a different class) is very simple. The change DOES NOT change any original initialization process.

4. I agree with that ""I think it is important to get the correctness issue resolved first"".  Don't you think that moving the code to another class is the easiest?


I respect your decision to incorporate my patch to get a better one. What's next?;;;","24/Oct/19 06:09;djoshi;Hi [~gzh1992n], I finally had a chance to go over your changes and I also fleshed out my patch which include your changes. Here's my [branch|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:15295-trunk-2]. Apart from pulling in your changes, I am trying to get rid of the static initialization of the {{CommitLog}}. The circleci test run is [here|https://circleci.com/workflow-run/5e861d0a-609f-4d7f-81b8-5370c8001d74]. Let me know what you think.;;;","24/Oct/19 07:04;gzh1992n;LGTM (y)
It gets rid of the potential for the bug to be reintroduced completely. Thanks for your work [~djoshi] .;;;","31/Oct/19 21:25;djoshi;[~jrwest] could you please take a look at this change?;;;","12/Nov/19 17:52;jwest;Hi [~djoshi] , I like the approach to removing the potential of re-introducing the bug. However, it seems like {{CommitLog}} was changed only partially to be thread-safe. For example, 
 concurrent calls to {{#start()}} and {{#shutdownBlocking()}} could leave the {{CommitLog}} in an invalid state: If the thread executing {{#start()}} pauses before calling {{executor.start()}}, and resumes
 only after a separate thread executing {{#shutdownBlocking}} calls {{executor.shutdown()}} and {{executor.awaitTermination}} (which will immediately exit since {{thread == null}}), the {{segmentManager}} will be shutdown but the {{executor}} will still be running.

 

Is it necessary to make {{CommitLog}} thread-safe? Removing the singleton doesn’t really change the odds of it being used from multiple threads and the original version wasn’t thread-safe either w.r.t to these functions.

Some other comments/minor nits:
 * I like moving the factory code to a function to reduce the amount of new code
 * Is it necessary to change AbstractCommitLogSegmentManager#shutdown() to no longer use an assert? That seems like a semantic change that is stylistic but I may be missing a further motivation for this change.
 * Minor style nit: In CommitLog#start, {{if (started) return true;}} should be on two lines per the Cassandra style guides;;;","19/Nov/19 21:36;djoshi;Thanks for the review [~jrwest]. I am not attempting to make the {{CommitLog}} class thread safe. Originally, the {{CommitLog::start()}} method would be called during the static block initialization which implicitly held a lock. This prevented two threads from starting the {{CommitLog}} twice. Now that I have removed the static initializer, we need to retain the same guarantees. 

I have pushed a few changes based on your feedback.;;;","03/Dec/19 18:07;jwest;LGTM. +1. Thanks for all the revisions along the way [~gzh1992n]  [~djoshi] ;;;","03/Dec/19 21:11;djoshi;Thanks for the review [~jrwest].;;;","12/Dec/19 18:17;djoshi;Thanks for the patch [~gzh1992n] & review [~jrwest]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Static columns not include in mutation size calculation,CASSANDRA-15293,13252909,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,VincentWhite,VincentWhite,26/Aug/19 04:08,15/May/20 08:55,13/Jul/23 08:38,10/Jan/20 10:45,3.0.20,3.11.6,4.0,4.0-alpha3,,Observability/Logging,Observability/Metrics,,,0,,,,"Patch to include any updates to static columns in the data size calculation of PartitionUpdate.


||Patch||
|[Trunk/3.11|https://github.com/vincewhite/cassandra/commits/include_update_to_static_columns_in_update_size]|

",,mck,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,VincentWhite,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Jan 10 10:45:54 UTC 2020,,,,,,,All,,,,,"0|z060qw:",9223372036854775807,,,,,,,mck,,,,Low,,3.0 alpha 1,,,https://github.com/apache/cassandra/commit/c2f201f147029c044b7a1f774cba7b6ab615945e,,,,,,,,,unit test added,,,,,"01/Jan/20 21:34;mck;||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[cassandra-3.0_15293|https://github.com/apache/cassandra/compare/cassandra-3.0...thelastpickle:mck/cassandra-3.0_15293]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.0_15293]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/45/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/45/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/719/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/719]|
|[cassandra-3.11_15293|https://github.com/apache/cassandra/compare/cassandra-3.11...thelastpickle:mck/cassandra-3.11_15293]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_15293]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/46/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/46/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/720/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/720]|
|[trunk_15293|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_15293]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_15293]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/47/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/47/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/721/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/721]|;;;","01/Jan/20 21:35;mck;[~VincentWhite], would it be possible to add a simple unit test to the patch?;;;","10/Jan/20 02:45;VincentWhite;||Branch||
|[Trunk unit test|https://github.com/vincewhite/cassandra/commits/15293_trunk]|
|[3.11 unit test|https://github.com/vincewhite/cassandra/commits/include_update_to_static_columns_in_update_size]|;;;","10/Jan/20 08:05;mck;Thanks [~VincentWhite].

||branch||circleci||asf jenkins pipeline||asf jenkins dtests||
|[cassandra-3.0_15293|https://github.com/apache/cassandra/compare/cassandra-3.0...thelastpickle:mck/cassandra-3.0_15293]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.0_15293]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/53/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/53/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/719/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/719]|
|[cassandra-3.11_15293|https://github.com/apache/cassandra/compare/cassandra-3.11...thelastpickle:mck/cassandra-3.11_15293]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_15293]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/54/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/54/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/720/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/720]|
|[trunk_15293|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_15293]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_15293]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/55/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/55/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/721/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/721]|;;;","10/Jan/20 10:45;mck;Committed as c2f201f147029c044b7a1f774cba7b6ab615945e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Point-in-time recovery ignoring timestamp of static column updates,CASSANDRA-15292,13252902,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,VincentWhite,VincentWhite,26/Aug/19 02:36,15/May/20 08:53,13/Jul/23 08:38,02/Jan/20 11:31,3.0.20,3.11.6,4.0,4.0-alpha3,,Local/Commit Log,,,,0,,,,"During point-in-time recovery org.apache.cassandra.db.partitions.PartitionUpdate#maxTimestamp is checked to see if any write timestamps in the update exceed the recovery point. If any of the timestamps do exceed this point the the commit log replay is stopped.

Currently maxTimestamp only iterates over the regular rows in the update and doesn't check for any included updates to static columns. If a ParitionUpdate only contains updates to static columns then maxTimestamp will return Long.MIN_VALUE and always be replayed. 

This generally isn't much of an issue, except for non-dense compact storage tables which are implemented in the 3.x storage engine in large part with static columns. In this case the commit log will always continue applying updates to them past the recovery point until it hits an update to a different table with regular columns or reaches the end of the commit logs.

 
||Patch||
|[3.11|https://github.com/vincewhite/cassandra/commits/3_11_check_static_column_timestamps_commit_log_archive]|
|[Trunk|https://github.com/vincewhite/cassandra/commits/trunk_check_static_column_timestamps]|",,jeromatron,mck,shaurya10000,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,VincentWhite,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Jan 02 11:31:32 UTC 2020,,,,,,,All,,,,,"0|z060pc:",9223372036854775807,,,,,,,mck,,,,Normal,,3.0 alpha 1,,,https://github.com/apache/cassandra/commit/577a9e34f5ed28640c5abe4476f094c2ad2c51aa,,,,,,,,,unit test,,,,,"01/Jan/20 21:18;mck;||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[cassandra-3.0_15292|https://github.com/apache/cassandra/compare/cassandra-3.0...thelastpickle:mck/cassandra-3.0_15292]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.0_15292]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/42/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/42/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/716/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/716]|
|[cassandra-3.11_15292|https://github.com/apache/cassandra/compare/cassandra-3.11...thelastpickle:mck/cassandra-3.11_15292]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_15292]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/43/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/43/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/717/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/717]|
|[trunk_15292|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_15292]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_15292]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/44/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/44/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/718/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/718]|;;;","02/Jan/20 11:31;mck;Committed as 577a9e34f5ed28640c5abe4476f094c2ad2c51aa;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bad merge reverted CASSANDRA-14993,CASSANDRA-15289,13252723,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,23/Aug/19 17:35,15/May/20 08:36,13/Jul/23 08:38,07/Oct/19 18:11,3.0.19,3.11.5,4.0,4.0-alpha2,,Local/SSTable,,,,0,,,,,,bdeggleston,benedict,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Degradation -> Other Exception,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Oct 07 18:11:16 UTC 2019,,,,,,,All,,,,,"0|z05zls:",9223372036854775807,,,,,,,spod,,,,Normal,,3.0.18,,,https://github.com/apache/cassandra/commit/a4af4aa60ad0aa6fbb134b44e162970c7a1a20da,,,,,,,,,circle,,,,,"23/Aug/19 17:52;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/15289-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15289-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/15289-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15289-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/15289-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15289-trunk]|;;;","27/Aug/19 07:53;spod;LGTM, that's what went in with CASSANDRA-14993;;;","27/Aug/19 16:50;benedict;+1, good catch, thanks for fixing - really confused as to what happened here, given this doesn't match either parent commit.;;;","07/Oct/19 18:11;bdeggleston;committed to 3.0 as  [a4af4aa60ad0aa6fbb134b44e162970c7a1a20da |https://github.com/apache/cassandra/commit/a4af4aa60ad0aa6fbb134b44e162970c7a1a20da]and merged up;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inaccurate exception message with nodetool snapshot ,CASSANDRA-15287,13251728,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tdl-jturner,dvohra,dvohra,20/Aug/19 00:16,29/May/21 14:37,13/Jul/23 08:38,24/Aug/19 04:40,4.0,4.0-alpha1,,,,Tool/nodetool,,,,0,,,,"1. Create a keyspace _cqlkeyspace_ (arbitrary name)
2. Create two tables (t1 and  t2)
3. Run _nodetool snapshot_ to create snapshot for multiple tables in same command. Include the keyspace name as argument (which is not what the command syntax allows)

{code:java}
nodetool snapshot --kt-list cqlkeyspace.t1,cqlkeyspace.t2 --tag multi-table   --cqlkeyspace
{code}

4. Exception message is inaccurate in that it is referring to columnfamily when it should be referring to keyspace.

{code:java}
java.io.IOException: When specifying the Keyspace columfamily list for a snapshot, you should not specify columnfamily
{code}

Exception message should be:

{code:java}
java.io.IOException: When specifying the Keyspace columfamily list for a snapshot, you should not specify keyspace
{code}
",,dvohra,jjirsa,kadirselcuk,tdl-jturner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/19 15:32;tdl-jturner;15287.trunk;https://issues.apache.org/jira/secure/attachment/12978306/15287.trunk","22/Aug/19 15:32;tdl-jturner;15287.trunk.withchanges;https://issues.apache.org/jira/secure/attachment/12978307/15287.trunk.withchanges",,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,tdl-jturner,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Sat Aug 24 04:40:09 UTC 2019,,,,,,,All,,,,,"0|z05th4:",9223372036854775807,,,,,,,jjirsa,,,,Low,,2.1.5,,,https://github.com/apache/cassandra/commit/a145a485206f6a32b07979dfb1b983addf49dd10,,,,,,,,,"Reproduced error message:

 
{code:java}
bin/cqlsh << EOF
CREATE KEYSPACE k1 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes = true;

CREATE TABLE k1.t2 (
    id uuid PRIMARY KEY,
    val text
);

CREATE TABLE k1.t1 (
    id uuid PRIMARY KEY,
    val text
);
EOF

bin/nodetool snapshot --kt-list k1.t1,k1.t2 --k1
{code}
 

 

Before Patch:

 
{code:java}
error: When specifying the Keyspace columfamily list for a snapshot, you should not specify columnfamilyerror: When specifying the Keyspace columfamily list for a snapshot, you should not specify columnfamily
-- StackTrace --
java.io.IOException: When specifying the Keyspace columfamily list for a snapshot, you should not specify columnfamily at org.apache.cassandra.tools.nodetool.Snapshot.execute(Snapshot.java:73) at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:307) at org.apache.cassandra.tools.NodeTool.main(NodeTool.java:218)
{code}
 

 

After Patch:

 
{code:java}
error: When specifying the Keyspace columfamily list for a snapshot, you should not specify keyspaceerror: When specifying the Keyspace columfamily list for a snapshot, you should not specify keyspace
-- StackTrace --
java.io.IOException: When specifying the Keyspace columfamily list for a snapshot, you should not specify keyspace at org.apache.cassandra.tools.nodetool.Snapshot.execute(Snapshot.java:73) at org.apache.cassandra.tools.NodeTool$NodeToolCmd.run(NodeTool.java:307) at org.apache.cassandra.tools.NodeTool.main(NodeTool.java:218)
{code}
 

 ",,,,,"20/Aug/19 00:17;dvohra;Version - 4.0 build in trunk;;;","22/Aug/19 15:34;tdl-jturner;Confirmed the bug and see that it originated in Cassandra 2.2. The 15287.trunk patch applies cleanly to 2.2, 3.0, 3.11, and trunk. I've split the patch in two files as I'm not sure the proper version to modify for CHANGES.txt. ;;;","24/Aug/19 04:40;jjirsa;Thanks for the patch. I've taken the liberty of rewording it SLIGHTLY to make it more clear. Hope you don't object. Committed as [a145a485206f6a32b07979dfb1b983addf49dd10|https://github.com/apache/cassandra/commit/a145a485206f6a32b07979dfb1b983addf49dd10];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool tablestats does not list bytes repaired/unrepaired,CASSANDRA-15282,13251319,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tdl-jturner,dvohra,dvohra,16/Aug/19 19:04,27/May/22 19:24,13/Jul/23 08:38,01/Jul/21 19:29,4.0.1,4.1,4.1-alpha1,,,Tool/nodetool,,,,0,,,,"According to _CASSANDRA-13774_
_add bytes repaired/unrepaired in nodetool tablestats_

But, only the percent is listed, and not the actual bytes with latest 4.0 build.
{code:java}
nodetool tablestats --sort local_read_count --top 3
...
Percent repaired: 0.0
...{code}",,dvohra,marcuse,tdl-jturner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/19 18:37;tdl-jturner;15282.trunk;https://issues.apache.org/jira/secure/attachment/12978616/15282.trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,tdl-jturner,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Jul 01 19:29:44 UTC 2021,,,,,,,All,,,,,"0|z05r2g:",9223372036854775807,,,,,,,brandon.williams,marcuse,,,Low,,4.0-alpha1,,,https://github.com/apache/cassandra/commit/d7270725a1a0807e99b1e23a2d02c38fe8ddd94a,,,,,,,,,"Improved UnitTest org.apache.cassandra.tools.nodetool.stats.TableStatsPrinterTest to include this feature in output check and ran it successfully. 

 ",,,,,"26/Aug/19 18:36;tdl-jturner;Functionality got removed by CASSANDRA-13889. I've readded the code for this feature and updated TableStatsPrinterTest to account for this changed output.

 ;;;","01/Jul/21 17:28;brandon.williams;||Branch||CI||
|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-15282]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-15282]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-15282-trunk]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-15282-trunk]

Slightly rebased Josh's patch above.;;;","01/Jul/21 17:34;marcuse;+1 assuming clean ci;;;","01/Jul/21 19:29;brandon.williams;CI is clean, committed.  Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove overly conservative check breaking VirtualTable unit test,CASSANDRA-15279,13250837,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,cnlwsu,cnlwsu,cnlwsu,14/Aug/19 17:40,16/Mar/22 12:26,13/Jul/23 08:38,14/Aug/19 19:10,4.0,4.0-alpha1,,,,Test/unit,,,,0,,,,CASSANDRA-15194 introduced a check to make it easier to debug bad values being passed to SimpleDataSet but it was too aggressive and actually blocks valid cases which are shown in unit tests.,,bdeggleston,cnlwsu,,,,,,,,,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #341:
URL: https://github.com/apache/cassandra/pull/341


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:26;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cnlwsu,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Aug 14 18:21:04 UTC 2019,,,,,,,All,,,,,"0|z05o3c:",9223372036854775807,,,,,,,bdeggleston,,,,Normal,,4.0,,,https://github.com/apache/cassandra/commit/fcb4d52403a3de893eb2813468a788b0c8fa6fc7,,,,,,,,,na,,,,,"14/Aug/19 18:05;cnlwsu;[fixed unit tests|https://circleci.com/gh/clohfink/cassandra/592];;;","14/Aug/19 18:21;bdeggleston;+1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra does not start with new systemd version,CASSANDRA-15273,13250196,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,mck,Aleksandr Yatskin,Aleksandr Yatskin,12/Aug/19 07:04,15/May/20 08:54,13/Jul/23 08:38,03/Mar/20 21:55,2.2.17,3.0.21,3.11.7,4.0,4.0-alpha4,Packaging,,,,10,,,,"After update systemd with  fixed vulnerability https://access.redhat.com/security/cve/cve-2018-16888, the cassandra service does not start correctly.

Environment: RHEL 7, systemd-219-67.el7_7.1, cassandra-3.11.4-1 (https://www.apache.org/dist/cassandra/redhat/311x/cassandra-3.11.4-1.noarch.rpm)

---------------------------------------------------------------

systemctl status cassandra
● cassandra.service - LSB: distributed storage system for structured data
 Loaded: loaded (/etc/rc.d/init.d/cassandra; bad; vendor preset: disabled)
 Active: failed (Result: resources) since Fri 2019-08-09 17:20:26 MSK; 1s ago
 Docs: man:systemd-sysv-generator(8)
 Process: 2414 ExecStop=/etc/rc.d/init.d/cassandra stop (code=exited, status=0/SUCCESS)
 Process: 2463 ExecStart=/etc/rc.d/init.d/cassandra start (code=exited, status=0/SUCCESS)
 Main PID: 1884 (code=exited, status=143)

Aug 09 17:20:23 desktop43.example.com systemd[1]: Unit cassandra.service entered failed state.
Aug 09 17:20:23 desktop43.example.com systemd[1]: cassandra.service failed.
Aug 09 17:20:23 desktop43.example.com systemd[1]: Starting LSB: distributed storage system for structured data...
Aug 09 17:20:23 desktop43.example.com su[2473]: (to cassandra) root on none
Aug 09 17:20:26 desktop43.example.com cassandra[2463]: Starting Cassandra: OK
Aug 09 17:20:26 desktop43.example.com systemd[1]: New main PID 2545 does not belong to service, and PID file is not owned by root. Refusing.
Aug 09 17:20:26 desktop43.example.com systemd[1]: New main PID 2545 does not belong to service, and PID file is not owned by root. Refusing.
Aug 09 17:20:26 desktop43.example.com systemd[1]: Failed to start LSB: distributed storage system for structured data.
Aug 09 17:20:26 desktop43.example.com systemd[1]: Unit cassandra.service entered failed state.
Aug 09 17:20:26 desktop43.example.com systemd[1]: cassandra.service failed.",,acerna,Aleksandr Yatskin,demsey,fcampuzano,jalbersdorfer,jeromatron,jonasbartho,JonasBarthol,kwizart,maxwellguo,mck,McNamara-Dale,musinsky,pioto,rtib,swapnil.dubey,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13148,,,,,,,,,,,,,"19/Feb/20 18:56;pioto;0001-Fix-Red-Hat-init-script-on-newer-systemd-versions.patch;https://issues.apache.org/jira/secure/attachment/12993922/0001-Fix-Red-Hat-init-script-on-newer-systemd-versions.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,pioto,,,,,,,,,,,,Code,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Mar 10 09:08:35 UTC 2020,,,,,,,Linux,,,,,"0|z05k54:",9223372036854775807,,,,,,,mck,,,,Normal,,0.8 beta 1,,,https://github.com/apache/cassandra/commit/9105dcd99e61537e8d177b41b7d38c5569412230,,,,,,,,,This will involve testing on an RPM + systemd system.,,,,,"13/Aug/19 02:40;maxwellguo;I think you should attach some logs then we can get more information for this question. the older version of cassandra should also be attached .;;;","13/Aug/19 06:22;Aleksandr Yatskin;The cassandra starts, but the systemd cannot control it. The cause is that when the cassandra starts, the old initialization SysV script is used, in which it is obviously impossible to specify the user and group to start the service.

It's about user/group options for systemd:
 -----------------
 _[Service]_
 _User=cassandra_
 _Group=cassandra_
 -----------------

But since the process pid is created with the permissions of the cassandra user, and the user and group are not specified in the initialization script, the systemd consider that it uses the root to start the service (by default) and does not allow creating the pid with cassandra user permissions.
 ------------------
 _systemd[1]: New main PID 2545 does not belong to service, and PID file is not owned by root. Refusing._
 ------------------

More details in CVE-2018-16888 ([https://access.redhat.com/security/cve/cve-2018-16888])
 ------------------
 _It was discovered systemd does not correctly check the content of PIDFile files before using it to kill processes. When a service is run from an unprivileged user (e.g. User field set in the service file), a local attacker who is able to write to the PIDFile of the mentioned service may use this flaw to trick systemd into killing other services and/or privileged processes._
 ------------------;;;","21/Aug/19 07:11;jonasbartho;We are experiencing the same! We are on  cassandra-version 3.11.1-1.

What is the status on this?;;;","26/Sep/19 11:12;JonasBarthol;Hi,

Is there any update on this? This is still a big issue.

We cannot patch our systems due to this bug. :/;;;","26/Sep/19 11:15;JonasBarthol;[~jolynch] [~jasobrown], can any of you guys assist with this issue? :);;;","16/Oct/19 13:22;musinsky;For anyone who wants a quick and dirty patch to the SysV init script, this should work.
{noformat}
 --- /etc/rc.d/init.d/cassandra	2017-06-19 20:09:05.000000000 +0000
+++ cassandra	2019-10-16 13:19:48.527564181 +0000
@@ -69,7 +69,8 @@
         echo -n ""Starting Cassandra: ""
         [ -d `dirname ""$pid_file""` ] || \
             install -m 755 -o $CASSANDRA_OWNR -g $CASSANDRA_OWNR -d `dirname $pid_file`
-        su $CASSANDRA_OWNR -c ""$CASSANDRA_PROG -p $pid_file"" > $log_file 2>&1
+        runuser -u $CASSANDRA_OWNR -- $CASSANDRA_PROG -p $pid_file > $log_file 2>&1 
+        chown root.root $pid_file
         retval=$?
         [ $retval -eq 0 ] && touch $lock_file
         echo ""OK""
{noformat}

;;;","16/Oct/19 15:00;McNamara-Dale;Thanks [~musinsky], that fixed it for me!;;;","07/Nov/19 15:08;demsey;h5. Shouldn't cassandra register services in the systemd like other standard services?

For example in RHEL 7:
{{/usr/lib/systemd/system/sshd.service}}
{code:bash}
[Unit]
Description=OpenSSH server daemon
Documentation=man:sshd(8) man:sshd_config(5)
After=network.target sshd-keygen.service
Wants=sshd-keygen.service

[Service]
Type=notify
EnvironmentFile=/etc/sysconfig/sshd
ExecStart=/usr/sbin/sshd -D $OPTIONS
ExecReload=/bin/kill -HUP $MAINPID
KillMode=process
Restart=on-failure
RestartSec=42s

[Install]
WantedBy=multi-user.target
{code}
{{Of course /etc/init.d/cassandra is then superfluous :)}}

 

As example it is {{/usr/lib/systemd/system/cassandra.service}} from OpenSUSE 15.1
{code:bash}
[Unit]
Description=Cassandra
After=network.target

[Service]
Environment=CASSANDRA_HOME=/usr/share/cassandra CASSANDRA_CONF=/etc/cassandra/conf CASSANDRA_INCLUDE=/usr/share/cassandra/cassandra.in.sh
EnvironmentFile=/etc/sysconfig/cassandra
User=cassandra
ExecStart=/usr/sbin/cassandra -f
ExecStopPost=/usr/bin/sleep 5 ; /usr/bin/rm -f /var/lock/subsys/cassandra
StandardOutput=journal
StandardError=journal
LimitNOFILE=100000
LimitMEMLOCK=infinity
LimitNPROC=32768
LimitAS=infinity
SuccessExitStatus=143
TimeoutStopSec=60
Restart=on-failure

[Install]
WantedBy=multi-user.target
{code}
 ;;;","22/Nov/19 15:37;fcampuzano;Hi Guys, I have the same problem, in Centos7 and Redhat7 with CASSANDRA. could you help me to execute the scrip I tried but it did not work, where I must run it, first I must create a file with the scrip and then execute it so it would be the procedure .. thanks.

 

[root@localhost ~]# service cassandra start
Starting cassandra (via systemctl): Job for cassandra.service failed because a configured resource limit was exceeded. See ""systemctl status cassandra.service"" and ""journalctl -xe"" for details.
[FAILED]


[root@localhost ~]# systemctl status cassandra.service
● cassandra.service - LSB: distributed storage system for structured data
 Loaded: loaded (/etc/rc.d/init.d/cassandra; bad; vendor preset: disabled)
 Active: failed (Result: resources) since Fri 2019-11-22 10:19:29 -05; 8s ago
 Docs: man:systemd-sysv-generator(8)
 Process: 10565 ExecStart=/etc/rc.d/init.d/cassandra start (code=exited, status=0/SUCCESS)

Nov 22 10:19:29 localhost.localdomain systemd[1]: Starting LSB: distributed storage system for structured data...
Nov 22 10:19:29 localhost.localdomain su[10575]: (to cassandra) root on none
Nov 22 10:19:29 localhost.localdomain systemd[1]: New main PID 10649 does not belong to service, and PID file is not owned by root. Refusing.
Nov 22 10:19:29 localhost.localdomain cassandra[10565]: Starting Cassandra: OK
Nov 22 10:19:29 localhost.localdomain systemd[1]: New main PID 10649 does not belong to service, and PID file is not owned by root. Refusing.
Nov 22 10:19:29 localhost.localdomain systemd[1]: Failed to start LSB: distributed storage system for structured data.
Nov 22 10:19:29 localhost.localdomain systemd[1]: Unit cassandra.service entered failed state.
Nov 22 10:19:29 localhost.localdomain systemd[1]: cassandra.service failed.
[root@localhost ~]#;;;","25/Nov/19 23:21;pioto;The fix from [~musinsky] doesn't seem to have helped for me. I'm now seeing:

{noformat}
Nov 25 18:19:31 test.example.com systemd[1]: Starting LSB: distributed storage system for structured data...
-- Subject: Unit cassandra.service has begun start-up
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit cassandra.service has begun starting up.
Nov 25 18:19:31 test.example.com runuser[7211]: pam_unix(runuser:session): session opened for user cassandra by (uid=0)
Nov 25 18:19:32 test.example.com cassandra[7201]: Starting Cassandra: OK
Nov 25 18:19:32 test.example.com systemd[1]: New main PID 7289 does not exist or is a zombie.
Nov 25 18:19:32 test.example.com systemd[1]: Failed to start LSB: distributed storage system for structured data.
-- Subject: Unit cassandra.service has failed
-- Defined-By: systemd
-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel
--
-- Unit cassandra.service has failed.
--
-- The result is failed.
Nov 25 18:19:32 test.example.com systemd[1]: Unit cassandra.service entered failed state.
Nov 25 18:19:32 test.example.com systemd[1]: cassandra.service failed.
{noformat};;;","25/Nov/19 23:34;pioto;Nevermind, seems I had a different issue, my JAVA_HOME globally had changed to point to JDK 11.

Adding {{JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk}} to {{/etc/default/cassandra}}, along with the changes suggested by [~musinsky], were sufficient to get Cassandra running again.

Thanks!;;;","26/Nov/19 22:55;acerna;Hello, the solution for the start of the service was good, but I had to modify the command for RHEL 7.7 but when I execute the stop, the message status=1/FAILURE appears and that I also changed the command syntax.

I have changed the command in the script this way:

runuser $ CASSANDRA_OWNR -c ""kill` cat $ pid_file` ""

I have tried to make changes in the scritp but it has not worked, I just have to change the value from 1 to 0 in this part:

if [$ retval -eq 3]; then
I miss ""OK""
else
echo ""ERROR: could not stop $ NAME""
# exit 1
exit 0

If you already have the solution please let her know, I remain attentive to your comments.

 ;;;","10/Feb/20 15:03;jalbersdorfer;As a followup to [~musinsky] and as an Answer for [~acerna], I fixed both {{start}} and {{stop}} as following:
{noformat}
--- /etc/rc.d/init.d/cassandra.old       2020-02-10 15:23:38.852120933 +0100
+++ /etc/rc.d/init.d/cassandra.new       2020-02-10 15:23:10.610076622 +0100
@@ -71,7 +71,9 @@
        chown -R cassandra:cassandra ""$(dirname ""$log_file"")""
         [ -d `dirname ""$pid_file""` ] || \
             install -m 755 -o $CASSANDRA_OWNR -g $CASSANDRA_OWNR -d `dirname $pid_file`
-        su $CASSANDRA_OWNR -c ""$CASSANDRA_PROG -p $pid_file"" > $log_file 2>&1
+        # su $CASSANDRA_OWNR -c ""$CASSANDRA_PROG -p $pid_file"" > $log_file 2>&1
+        runuser -u $CASSANDRA_OWNR -- $CASSANDRA_PROG -p $pid_file > $log_file 2>&1
+        chown root.root $pid_file
         retval=$?
         [ $retval -eq 0 ] && touch $lock_file
         echo ""OK""
@@ -79,7 +81,8 @@
     stop)
         # Cassandra shutdown
         echo -n ""Shutdown Cassandra: ""
-        su $CASSANDRA_OWNR -c ""kill `cat $pid_file`""
+        # su $CASSANDRA_OWNR -c ""kill `cat $pid_file`""
+        runuser -u $CASSANDRA_OWNR -- kill `cat $pid_file`
         retval=$?
         [ $retval -eq 0 ] && rm -f $lock_file
         for t in `seq 40`; do
{noformat};;;","12/Feb/20 04:19;zznate;If someone wants to distill this into a patch, we would be interested! cc/ [~mshuler] and [~mck2] sense they are wrestling some build stuff right now.;;;","19/Feb/20 18:56;pioto;I've attached a patch against the {{cassandra-3.11}} branch which should work.

;;;","25/Feb/20 01:55;swapnil.dubey;Thank you [~pioto]

I followed the Code mentioned in [^0001-Fix-Red-Hat-init-script-on-newer-systemd-versions.patch]

^Then run the command ""systemctl daemon-reload"" and it worked.^

Regards,

Swapnil;;;","03/Mar/20 16:04;mck;||branch||circleci||
|[cassandra_2.2_15273|https://github.com/apache/cassandra/compare/cassandra-2.2...thelastpickle:mck/cassandra-2.2_15273]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-2.2_15273]|
|[cassandra_3.0_15273|https://github.com/apache/cassandra/compare/cassandra-3.0...thelastpickle:mck/cassandra-3.0_15273]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.0_15273]|
|[cassandra_3.11_15273|https://github.com/apache/cassandra/compare/cassandra-3.11...thelastpickle:mck/cassandra-3.11_15273]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_15273]|
|[trunk_15273|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_15273]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_15273]|

To test this, rather than following the instructions in https://github.com/apache/cassandra/tree/trunk/redhat , I've taken the following approach:
{code}
docker image rm -f  `docker images -f label=org.cassandra.buildenv=centos -q`
docker build --build-arg CASSANDRA_GIT_URL=${CASSANDRA_GIT_URL} -f docker/centos7-image.docker docker/
mkdir /tmp/cassandra-rpms
docker run --rm -v /tmp/cassandra-rpms:/dist `docker images -f label=org.cassandra.buildenv=centos -q` /home/build/build-rpms.sh ${CASSANDRA_GIT_BRANCH}

docker run -v /tmp/cassandra-rpms:/dist -it centos /bin/sh
yum install java-1.8.0-openjdk
rpm -ivh /dist/cassandra-4.0~alpha4-20200303git90a391e.noarch.rpm
rm /etc/security/limits.d/cassandra.conf # this should be fixed
/etc/init.d/cassandra start
tail -F /var/log/cassandra/system.log
{code}

This is the same approach as we used when cutting and staging releases, ref https://github.com/thelastpickle/cassandra-builds/blob/mck/14970_sha512-checksums/cassandra-release/prepare_release.sh#L299-L302

Note, we are short of packaging experience in the community and really do need extra contributors on this front. Replacing the old initialisation SysV script with a service script (CASSANDRA-13148), and testing the building of packages in the CI pipeline, as well as others like CASSANDRA-13433 , are obvious subsequent steps that are needed. ;;;","03/Mar/20 21:55;mck;Committed as 9105dcd99e61537e8d177b41b7d38c5569412230;;;","10/Mar/20 09:08;kwizart;It would be appropriate to introduce systemd support for 4.0 instead 
See also:
https://issues.apache.org/jira/browse/CASSANDRA-13148

https://github.com/apache/cassandra/pull/398

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CASSANDRA-13426 changed the behavior of ""CLUSTERING ORDER"" on ""CREATE TABLE""",CASSANDRA-15271,13249941,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,jjordan,jjordan,09/Aug/19 17:06,21/Dec/20 08:07,13/Jul/23 08:38,27/Feb/20 13:30,4.0,4.0-alpha4,,,,CQL/Syntax,,,,0,,,,"CASSANDRA-13426 changed the behavior of ""CLUSTERING ORDER"" on ""CREATE TABLE"", it now complains if you don't specify all the columns.

It was nice that previously you could just specify to make the first clustering DESC and leave the rest ASC without needing to specify them.  Also it would be nice I think to avoid breaking changes to the CREATE TABLE syntax.

We should either update NEWS.txt to call out the breaking change there, or update the new code to be able to default the columns which were not mentioned.",,aleksey,csplinter,e.dimitrova,jeromatron,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/20 19:40;e.dimitrova;CASSANDRA-15271-testall.txt;https://issues.apache.org/jira/secure/attachment/12994695/CASSANDRA-15271-testall.txt","26/Feb/20 19:43;e.dimitrova;CASSANDRA-15271.zip;https://issues.apache.org/jira/secure/attachment/12994697/CASSANDRA-15271.zip","26/Feb/20 19:40;e.dimitrova;Screen Shot 2020-02-26 at 11.25.29 AM.png;https://issues.apache.org/jira/secure/attachment/12994696/Screen+Shot+2020-02-26+at+11.25.29+AM.png",,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,e.dimitrova,,,,,,,,,,,,Correctness,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Feb 27 14:09:26 UTC 2020,,,,,,,All,,,,,"0|z05iko:",9223372036854775807,,,,,,,aleksey,,,,Low,,4.0-alpha,4.0-alpha1,,"[1f242c900bb087770a4cffeade6986ef19ba303d|https://github.com/apache/cassandra/commit/1f242c900bb087770a4cffeade6986ef19ba303d]",,,,,,,,,"[Branch|https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15271]",,,,,"26/Feb/20 19:51;e.dimitrova;[Branch|https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15271]

[Pull Request|https://github.com/apache/cassandra/pull/455]

[~aleksey] , [~jjordan] - please let me know if you have any concerns.

 ;;;","26/Feb/20 19:52;e.dimitrova;Attached are the results from CI. I didn't find any newly introduced issues;;;","27/Feb/20 13:30;aleksey;Committed with some minor touch-ups and nomenclature correction (we don't have clustering key or clustering key columns anymore, the preferred term is just ""clustering columns"").;;;","27/Feb/20 14:09;e.dimitrova;Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra fails to process OperationExecutionException which causes ClassCastException,CASSANDRA-15269,13249598,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bereng,Override,Override,08/Aug/19 08:23,16/Mar/22 12:28,13/Jul/23 08:38,10/Sep/21 07:38,4.0.2,,,,,CQL/Interpreter,,,,0,,,,"While working on CASSANDRA-15232 I noticed that OperationExecutionException is not processed correctly.

How to reproduce the issue:
 1. {{create table d (numerator decimal primary key, denominator decimal);}}
 2. {{insert into d (numerator, denominator) values (123456789112345678921234567893123456, 2);}}
 3. {{select numerator % denominator from d;}}

What happens:
 1. remainder operation throws ArithmeticException (BigDecimal:1854)
 2. The exception is wrapped in OperationExecutionException
 3. ClassCastException appears (OperationExecutionException cannot be cast to FunctionExecutionException at ErrorMessage.java:280)

What should happen:
OperationExecutionException with message ""the operation 'decimal % decimal' failed: Division impossible"" should be delivered to user 

Note that after fixing CASSANDRA-15232 {{select numerator % denominator from d;}} will produce correct result of remainder operation.
 Currently I am not aware of other cases when OperationExecutionException may be treated as FunctionExecutionException",,adelapena,adutra,andrew.tolbert,benedict,bereng,blerer,e.dimitrova,Override,samt,zznate,,,,,,,,,,,,,,,,,,"smiklosovic closed pull request #345:
URL: https://github.com/apache/cassandra/pull/345


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:28;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,600,,,0,600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bereng,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Sep 06 12:45:48 UTC 2021,,,,,,,All,,,,,"0|z05ggg:",9223372036854775807,,,,,,,blerer,samt,,,Low,,4.0,,,https://github.com/apache/cassandra/commit/8b0b22e166e2845a7c61af21cf13d8e0ff19efd6,,,,,,,,,"Documented, not tested (should not affect any tests)",,,,,"20/Aug/19 10:06;Override;Each {{ExceptionCode}} correspond to specific exception class.
 {{OperationExecutionException}} did not have it's own code, instead it used {{FUNCTION_FAILURE}} code. So the exception was treated as {{FunctionExecutionException}}

I added {{OPERATION_FAILURE}} code

How to test:
 revert locally d60e7988736ed4358595e9c781b110a5bbb5f812 commit (void result truncate in decimal operations)
Or throw {{ArithmeticException}} in {{DecimalType.mod}}
{noformat}
try (CqlSession session = CqlSession.builder().build()) {
    session.execute(""drop table if exists d"");
    session.execute(""create table d (n decimal primary key, d decimal)"");
    session.execute(""insert into d (n, d) values (123456789112345678921234567893123456, 2)"");
    session.execute(""select n % d from my.d"");
}
{noformat}
What used to be:
{noformat}
Exception in thread ""main"" com.datastax.oss.driver.api.core.servererrors.ServerError: java.lang.ClassCastException: org.apache.cassandra.exceptions.OperationExecutionException cannot be cast to org.apache.cassandra.exceptions.FunctionExecutionException
	at com.datastax.oss.driver.api.core.servererrors.ServerError.copy(ServerError.java:54)
{noformat}
What is now:
{noformat}
Exception in thread ""main"" com.datastax.oss.driver.api.core.DriverExecutionException
	at com.datastax.oss.driver.internal.core.util.concurrent.CompletableFutures.getUninterruptibly(CompletableFutures.java:150)
...
Caused by: java.lang.IllegalArgumentException: Unsupported error code: 5888
{noformat}
New error code should be added to Datastax Cassandra Native Protocol in [ProtocolConstants.java|https://github.com/datastax/native-protocol/blob/1.x/src/main/java/com/datastax/oss/protocol/internal/ProtocolConstants.java#L70]

If this commit is merged, I'll write a patch to the protocol;;;","20/Aug/19 21:48;andrew.tolbert;I made a small comment that this should be part of the protocol v5 spec instead of v4, but other than that I think this looks good.  I went ahead and created a java driver ticket for awareness: [JAVA-2397|https://datastax-oss.atlassian.net/browse/JAVA-2397]. The native protocol change will be great for the 4.0 java driver.  We should also consider making the change in the 3.0 driver since that intends on supporting C* 4.0 as well.;;;","21/Aug/19 05:42;Override;Hi [~andrew.tolbert],

Thank you for reviewing my commit!

I moved error code description from v4 to v5
 How to find out versions of Cassandra and CQL that will use v5?
 I know that I can execute {{""select release_version, cql_version, native_protocol_version from system.local""}}, but it gives v4 protocol for 4.0-SNAPSHOT Cassandra and 3.4.5 cql;;;","04/May/21 09:14;samt;Sorry for the extremely late feedback on this, I was completely unaware of the ticket. Unfortunately, protocol v5 has been finalised so a change like this one would need to target v6. 

That said, I'm not really sure that a new exception code is necessary here. Technically, what is occurring _is_ a problem with function execution, that function being a {{NumericOperationFunction}}, which is a subclass of {{OperationFunction}} and ultimately {{AbstractFunction}}. So I don't think the {{FUNCTION_FAILURE}} response is actually incorrect. My suggestion would be that we fix the class cast exception, perhaps by having {{OEE}} extend {{FEE}}. {{OperationFcts::getFunctionNameFromOperator}} can be used to obtain a {{FunctionName}} for the operation.
;;;","09/Jun/21 08:37;bereng;[~Override] do you mind I hijack this one from you?;;;","10/Jun/21 14:27;Override;[~bereng], no problem :);;;","11/Jun/21 08:33;bereng;Thx!

So if I didn't get things wrong is this what we're after?

{noformat}
[cqlsh 6.0.0 | Cassandra 4.0-rc2-SNAPSHOT | CQL spec 3.4.5 | Native protocol v5]
Use HELP for help.
cqlsh> select numerator % denominator from test.d;
Traceback (most recent call last):
  File ""/home/bereng/work/repos/bdpWS/15269/bin/cqlsh.py"", line 1076, in perform_simple_statement
    result = future.result()
  File ""/home/bereng/work/repos/bdpWS/15269/bin/../lib/cassandra-driver-internal-only-3.25.0.zip/cassandra-driver-3.25.0/cassandra/cluster.py"", line 4894, in result
    raise self._final_exception
cassandra.FunctionFailure: Error from server: code=1400 [User Defined Function failure] message=""execution of 'system._modulo[decimal, decimal]' failed: the operation 'decimal % decimal' failed: Division impossible""

cqlsh> select * from test.d;

 numerator                            | denominator
--------------------------------------+-------------
 123456789112345678921234567893123456 |           2

(1 rows)
{noformat}

PR is [here|https://github.com/apache/cassandra/pull/1057] with the mentioned revert so that it can be tested with the repro steps at the top of the ticket. Once we agree is good I'll remove the revert.;;;","11/Jun/21 12:40;blerer;You should not get: 'system._modulo[decimal, decimal]' as it is supposed to be hidden from the user. Instead we should have 'decimal % decimal' (as it is a bit further in the message).;;;","16/Jun/21 04:47;bereng;There it goes:

{noformat}
[cqlsh 6.0.0 | Cassandra 4.0-rc2-SNAPSHOT | CQL spec 3.4.5 | Native protocol v5]
Use HELP for help.
cqlsh> select numerator % denominator from test.d;
Traceback (most recent call last):
  File ""/home/bereng/work/repos/bdpWS/15269/bin/cqlsh.py"", line 1076, in perform_simple_statement
    result = future.result()
  File ""/home/bereng/work/repos/bdpWS/15269/bin/../lib/cassandra-driver-internal-only-3.25.0.zip/cassandra-driver-3.25.0/cassandra/cluster.py"", line 4894, in result
    raise self._final_exception
cassandra.FunctionFailure: Error from server: code=1400 [User Defined Function failure] message=""The operation 'decimal % decimal' failed: Division impossible""

cqlsh> select * from test.d;

 numerator                            | denominator
--------------------------------------+-------------
 123456789112345678921234567893123456 |           2

(1 rows)
{noformat};;;","02/Sep/21 08:20;blerer;[~bereng] Sorry, for the delay. There are a simple way to trigger the {{ClassCastException}} without changing anything to the code.
I pushed a new version of the patch [here|https://github.com/apache/cassandra/commit/473e67199eb29f831000782ab25cdf56629af1c8]. CI results: [j8|https://app.circleci.com/pipelines/github/blerer/cassandra/191/workflows/d85e9e9f-7b7e-4d4b-83d2-53a6515a7c50] and [j11|https://app.circleci.com/pipelines/github/blerer/cassandra/191/workflows/4b73dbbb-1451-426e-8217-0af2fef0bf55]

Tell me if the changes look good to you.   ;;;","03/Sep/21 06:33;bereng;[~blerer] lgtm but I a missing sthg here. What's the benefit of the new PR vs the old one, they both seem equally involved. You mention the driver and I am not familiar with that side of things so what am I missing? Thx;;;","03/Sep/21 07:17;blerer;The old PR was relying on a hack for testing the fix. The new PR unit reproduce the scenario that lead to the {{ClassCastException}} without relying on any hack.  ;;;","03/Sep/21 07:59;blerer;Sorry, I realize that my commit description was confusing. The {{ClassCastException}} is happening on the server NOT on the driver side but it is occuring while the sever serialize the message to the driver. The new unit test check exactly the code path that was leading to the  {{ClassCastException}} . ;;;","06/Sep/21 05:34;bereng;[~blerer] lgtm +1. Your PR fails to run the test on some rat report error but upon rebase vs 4.0 it works. I left a minor nit comment in the PR. I can move this forward across the finish line if you're busy somewhere else.;;;","06/Sep/21 07:02;blerer;{quote}I can move this forward across the finish line if you're busy somewhere else.{quote}

It would be nice.;;;","06/Sep/21 12:45;bereng;[~blerer] I added CI to the PRs. I'll assume a +1 and leave you until tomorrow, if you want to take a last look, when I'll commit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java internal exception on attempt to UPDATE a row using CONTAINS operator,CASSANDRA-15266,13249412,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,masokol,Osipov,Osipov,07/Aug/19 17:04,27/May/22 19:25,13/Jul/23 08:38,10/Mar/22 13:32,3.0.27,3.11.13,4.0.4,4.1,4.1-alpha1,Legacy/CQL,,,,0,lhf,,,"kostja@atlas ~ % cqlsh -ucassandra -pcassandra
Connected to My Cluster at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.4 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cassandra@cqlsh> CREATE KEYSPACE t1 WITH replication = \{'class':'SimpleStrategy', 'replication_factor' : 1};
cassandra@cqlsh> use t1;
cassandra@cqlsh:t1> create table t (a int, b frozen<map<int, int>>, c int, primary key (a, b));
cassandra@cqlsh:t1> insert into t (a, b, c) values (1, \{1:1, 2:2}, 3);
cassandra@cqlsh:t1> update t set c=3 where a=1 and b contains 1;
ServerError: java.lang.UnsupportedOperationException

 

Server log file:

```

ERROR [Native-Transport-Requests-1] 2019-08-07 17:02:59,283 QueryMessage.java:129 - Unexpected error during query 
java.lang.UnsupportedOperationException: null
 at org.apache.cassandra.cql3.restrictions.SingleColumnRestriction$ContainsRestriction.appendTo(SingleColumnRestriction.java:454) ~[a
pache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.restrictions.ClusteringColumnRestrictions.valuesAsClustering(ClusteringColumnRestrictions.java:109) ~[a
pache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.restrictions.StatementRestrictions.getClusteringColumns(StatementRestrictions.java:770) ~[apache-cassan
dra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.createClustering(ModificationStatement.java:312) ~[apache-cassandra-3.
11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.addUpdates(ModificationStatement.java:677) ~[apache-cassandra-3.11.4.j
ar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:635) ~[apache-cassandra-3.11.4
.jar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:437) ~[apache-cassa
ndra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:425) ~[apache-cassandra-3.11.4.jar:
3.11.4]
 at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:225) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:256) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:241) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:566) [apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.4.jar:3.11.4]
 at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44
...

 ```
+Additional information for newcomers:+

{{CONTAINS}} and {{CONTAINS KEY}} restrictions are not supported for {{UPDATE}} or {{DELETE}} operations but they should be properly rejected with a proper error message.
To fix that problem a new check should be added in the {{StatementRestrictions}} constructor to thrown an {{InvalidRequestException}} if the relation operator is a {{CONTAINS}} or {{CONTAINS_KEY}} and the {{StatementType}} an {{UPDATE}} or a {{DELETION}}.
Some unit tests should be added to {{UpdateTest}} an {{DeleteTest}} to test the behavior.
",,adelapena,blerer,masokol,Osipov,,,,,,,,,,,,,,,,,,,,,,,,"adelapena commented on a change in pull request #1458:
URL: https://github.com/apache/cassandra/pull/1458#discussion_r816669088



##########
File path: src/java/org/apache/cassandra/cql3/restrictions/StatementRestrictions.java
##########
@@ -150,9 +150,14 @@ public StatementRestrictions(StatementType type,
          *     allow two IN for the same entity but that doesn't seem very useful)
          *   - The value_alias cannot be restricted in any way (we don't support wide rows with indexed value
          *     in CQL so far)
+         *   - CONTAINS and CONTAINS_KEY cannot be used with UPDATE or DELETE
          */
         for (Relation relation : whereClause.relations)
         {
+            if ((relation.isContains() || relation.isContainsKey()) && (type.isUpdate() || type.isDelete()))
+            {
+                throw invalidRequest(""Cannot use %s with %s"", type, relation.operator());
+            }

Review comment:
       Nit: blank line after the `if` block

##########
File path: test/unit/org/apache/cassandra/cql3/validation/operations/DeleteTest.java
##########
@@ -162,6 +164,39 @@ public void testDeletion() throws Throwable
                    row(""abc"", 4, ""xyz"", ""some other value""));
     }
 
+    @Test
+    public void testDeletionWithContains() throws Throwable
+    {
+        createTable(""CREATE TABLE %s (a int, b frozen<map<int, int>>, c int, primary key (a, b))"");
+
+        Map<Integer, Integer> testMap = new HashMap<>();
+        testMap.put(1, 1);
+        testMap.put(2, 2);
+        execute(""INSERT INTO %s (a, b, c) VALUES (?, ?, ?)"", 1, testMap, 3);

Review comment:
       We don't strictly need to insert data to verify that the `DELETE` query fails its validation. If we do so, it can be useful to verify that the data is still there after rejecting the `DELETE` query, so we verify that the `DELETE` query has been actually rejected. So, simplifying the code a little bit:
   ```java
   createTable(""CREATE TABLE %s (a int, b frozen<map<int, int>>, c int, PRIMARY KEY (a, b))"");
   
   Object[] row = row(1, map(1, 1, 2, 3), 3);
   execute(""INSERT INTO %s (a, b, c) VALUES (?, ?, ?)"", row);
   assertRows(execute(""SELECT * FROM %s""), row);
   
   assertInvalidMessage(""Cannot use DELETE with CONTAINS"",
                        ""DELETE FROM %s WHERE a=1 AND b CONTAINS 1"");
   
   // verify that the previous invalid DELETE query hasn't been applied
   assertRows(execute(""SELECT * FROM %s""), row);
   ```
   This can also be applied to `testDeletionWithContainsKey` and the new analogous tests on `UpdateTest`, wdyt?

##########
File path: src/java/org/apache/cassandra/cql3/restrictions/StatementRestrictions.java
##########
@@ -150,9 +150,14 @@ public StatementRestrictions(StatementType type,
          *     allow two IN for the same entity but that doesn't seem very useful)
          *   - The value_alias cannot be restricted in any way (we don't support wide rows with indexed value
          *     in CQL so far)
+         *   - CONTAINS and CONTAINS_KEY cannot be used with UPDATE or DELETE
          */
         for (Relation relation : whereClause.relations)
         {
+            if ((relation.isContains() || relation.isContainsKey()) && (type.isUpdate() || type.isDelete()))
+            {
+                throw invalidRequest(""Cannot use %s with %s"", type, relation.operator());

Review comment:
       Placing this new validation on first place produces changes in the error message that is thrown in some queries that were also correctly rejected before, as it can see in the modified tests. For example:
   ```
   CREATE TABLE t (pk int, ck int, v int, PRIMARY KEY (pk, ck));
   UPDATE t SET value = ? WHERE pk CONTAINS ? AND ck = ?
   ```
   Will throw the new message instead of the previous `Cannot use CONTAINS on non-collection column pk`.
   
   Both options are correct, but I think that throwing the new query-level generic error message is preferable to throwing the column-level error messages that were previously thrown. Thus, I think that placing this check on the first place is the best approach, although it changes behaviour a bit. @blerer wdyt?




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/22 11:34;githubbot;600","masokol commented on a change in pull request #1458:
URL: https://github.com/apache/cassandra/pull/1458#discussion_r816706610



##########
File path: test/unit/org/apache/cassandra/cql3/validation/operations/DeleteTest.java
##########
@@ -162,6 +164,39 @@ public void testDeletion() throws Throwable
                    row(""abc"", 4, ""xyz"", ""some other value""));
     }
 
+    @Test
+    public void testDeletionWithContains() throws Throwable
+    {
+        createTable(""CREATE TABLE %s (a int, b frozen<map<int, int>>, c int, primary key (a, b))"");
+
+        Map<Integer, Integer> testMap = new HashMap<>();
+        testMap.put(1, 1);
+        testMap.put(2, 2);
+        execute(""INSERT INTO %s (a, b, c) VALUES (?, ?, ?)"", 1, testMap, 3);

Review comment:
       Agree, will update to also check that data is still there after delete.




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/22 12:00;githubbot;600","blerer commented on a change in pull request #1458:
URL: https://github.com/apache/cassandra/pull/1458#discussion_r816711654



##########
File path: src/java/org/apache/cassandra/cql3/restrictions/StatementRestrictions.java
##########
@@ -150,9 +150,14 @@ public StatementRestrictions(StatementType type,
          *     allow two IN for the same entity but that doesn't seem very useful)
          *   - The value_alias cannot be restricted in any way (we don't support wide rows with indexed value
          *     in CQL so far)
+         *   - CONTAINS and CONTAINS_KEY cannot be used with UPDATE or DELETE
          */
         for (Relation relation : whereClause.relations)
         {
+            if ((relation.isContains() || relation.isContainsKey()) && (type.isUpdate() || type.isDelete()))
+            {
+                throw invalidRequest(""Cannot use %s with %s"", type, relation.operator());

Review comment:
       I agree. This message makes more sense.

##########
File path: test/unit/org/apache/cassandra/cql3/validation/operations/UpdateTest.java
##########
@@ -189,6 +191,34 @@ private void testUpdate(boolean forceFlush) throws Throwable
         }
     }
 
+    @Test
+    public void testUpdateWithContains() throws Throwable
+    {
+        createTable(""CREATE TABLE %s (a int, b frozen<map<int, int>>, c int, primary key (a, b))"");
+        Map<Integer, Integer> testMap = new HashMap<>();
+        testMap.put(1, 1);
+        testMap.put(2, 2);
+        execute(""INSERT INTO %s (a, b, c) VALUES (?, ?, ?)"", 1, testMap, 3);
+        assertRows(execute(""SELECT * FROM %s""),
+                   row(1, testMap, 3));
+        assertInvalidMessage(""Cannot use UPDATE with CONTAINS"",
+                             ""UPDATE %s SET c=3 WHERE a=1 AND b CONTAINS 1"");
+    }
+
+    @Test
+    public void testUpdateWithContainsKey() throws Throwable
+    {
+        createTable(""CREATE TABLE %s (a int, b frozen<map<int, int>>, c int, primary key (a, b))"");
+        Map<Integer, Integer> testMap = new HashMap<>();
+        testMap.put(1, 1);
+        testMap.put(2, 2);
+        execute(""INSERT INTO %s (a, b, c) VALUES (?, ?, ?)"", 1, testMap, 3);
+        assertRows(execute(""SELECT * FROM %s""),
+                   row(1, testMap, 3));
+        assertInvalidMessage(""Cannot use UPDATE with CONTAINS KEY"",
+                             ""UPDATE %s SET c=3 WHERE a=1 AND b CONTAINS KEY 1"");
+    }
+

Review comment:
       I think that we could merge the 2 tests into one. 
   As each test requires cleaning up the tables and other slow operations having both tests together will minimize the amount of time spend to run the tests. 
   Same thing for the DELETE tests.




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/22 12:08;githubbot;600","adelapena commented on a change in pull request #1458:
URL: https://github.com/apache/cassandra/pull/1458#discussion_r816837536



##########
File path: test/unit/org/apache/cassandra/cql3/validation/operations/DeleteTest.java
##########
@@ -21,7 +21,9 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
+import java.util.HashMap;

Review comment:
       Nit: unused imports, same for `UpdateTest`




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Mar/22 14:48;githubbot;600","smiklosovic closed pull request #1473:
URL: https://github.com/apache/cassandra/pull/1473


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 07:31;githubbot;600","smiklosovic closed pull request #1472:
URL: https://github.com/apache/cassandra/pull/1472


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 07:31;githubbot;600","smiklosovic closed pull request #1474:
URL: https://github.com/apache/cassandra/pull/1474


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 07:31;githubbot;600","smiklosovic closed pull request #1458:
URL: https://github.com/apache/cassandra/pull/1458


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 07:53;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,masokol,,,,,,,,,,,,Code,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Mar 10 13:32:04 UTC 2022,,,b.lerer@gmail.com,,,,All,,,,,"0|z05fb4:",9223372036854775807,,,,,,,adelapena,blerer,,,Low,,NA,,,https://github.com/apache/cassandra/commit/c52b5ab95cf314a7983141f74c97744ff3ecbc81,,,,,,,,,Unittests added and adjusted.,,,,,"17/Feb/22 07:19;masokol;Patch based on 3.0 [here|https://github.com/masokol/cassandra/tree/CASSANDRA-15266];;;","28/Feb/22 15:38;adelapena;[~masokol] thanks for the patch. Would you mind creating a pull request so we can add feedback on it?

CI for the patch is running [here|https://app.circleci.com/pipelines/github/adelapena/cassandra/1317/workflows/8f3c93e4-6bd2-4af8-ba63-feca8cdd43b5].;;;","01/Mar/22 06:18;masokol;First time contributing so was kinda confused whether you wanted a pullrequest or not, anyway here it is: [#1458 |https://github.com/apache/cassandra/pull/1458];;;","01/Mar/22 11:37;adelapena;Thanks for the PR, I think they are not mandatory but I find them very useful. Overall the changes look good to me, I have left a couple of minor comments. CI is looking good too.;;;","01/Mar/22 12:26;masokol;Thanks, I've addressed all comments posted by you and blerer, let me know if you want me to squash if that makes reviewing easier.;;;","01/Mar/22 15:04;adelapena;Thanks, the patch looks good to me. Could you please prepare patches for 3.11, 4.0 and trunk, so I run CI for them? I guess that there won't be any major conflicts in those branches.;;;","02/Mar/22 07:13;masokol;Sure, here you go:

[3.11|https://github.com/apache/cassandra/pull/1472]

[4.0|https://github.com/apache/cassandra/pull/1473]

[trunk|https://github.com/apache/cassandra/pull/1474];;;","02/Mar/22 11:37;adelapena;Running CI:

||PR||CI||
|[3.0|https://github.com/apache/cassandra/pull/1458]  |[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1320/workflows/9b7e616e-79b1-48ef-90dc-f39c79405412]|
|[3.11|https://github.com/apache/cassandra/pull/1472] |[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1319/workflows/cdbb4cf8-bbb4-480c-a326-8839c2733837]|
|[4.0|https://github.com/apache/cassandra/pull/1473]  |[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1318/workflows/80b6490e-5d01-4b8a-be6b-82469139198a] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1318/workflows/3f7f2809-a8ac-467e-936e-bb5254a601a2]|
|[trunk|https://github.com/apache/cassandra/pull/1474]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1321/workflows/1c0706ca-29d2-494b-b6df-9013568307f6] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1321/workflows/ccdd920f-c0e3-46a1-97b4-ab0f429755e6]|;;;","02/Mar/22 12:33;adelapena;The above CI runs show five tests within {{CompactStorageTest}} failing in 4.0 and trunk due to the new error message. 3.0 and 3.11 are not affected because the tests just don't exist in those branches. The fix seems as easy as updating the expected error message in the failing tests.;;;","02/Mar/22 12:51;masokol;I've updated the failing tests for 4.0 and trunk.;;;","02/Mar/22 13:25;adelapena;Great, thanks. Here we go again:

||PR||CI||
|[4.0|https://github.com/apache/cassandra/pull/1473]  |[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1323/workflows/bc9f1450-6a7f-4f67-bfbf-cf5d702d1198] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1323/workflows/61838613-cd4c-41fc-9691-60dfba587d2e]|
|[trunk|https://github.com/apache/cassandra/pull/1474]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1322/workflows/03069161-042a-4061-a583-59f9f6220370] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1322/workflows/a5655d43-41d1-415d-9a61-0625a88177b0]|;;;","03/Mar/22 11:28;adelapena;The failures of {{test_sstableloader_empty_stream}} in 4.0 on the above runs are due to missing the changes recently introduced by CASSANDRA-17367, they go away after rebasing. Here is a final CI round after rebasing without conflicts:
||PR||CI||
|[3.0|https://github.com/apache/cassandra/pull/1458]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1330/workflows/836a5035-4a8f-4197-9e08-60358792e5ba]|
|[3.11|https://github.com/apache/cassandra/pull/1472]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1329/workflows/da76f88f-5896-4902-ad5c-274437f09c35]|
|[4.0|https://github.com/apache/cassandra/pull/1473]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1331/workflows/beb36191-3029-4a5d-9cb7-cfbe78336cbe] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1331/workflows/41ba8047-debb-4f4f-b34f-846fba4e277b]|
|[trunk|https://github.com/apache/cassandra/pull/1474]|[j8|https://app.circleci.com/pipelines/github/adelapena/cassandra/1332/workflows/eee3f571-809c-4bee-acb9-25c4ad21424a] [j11|https://app.circleci.com/pipelines/github/adelapena/cassandra/1332/workflows/c0323446-6fcf-4263-9961-36698baa7f53]|

Unless there is any surprise in these runs, changes look good to me, +1.;;;","08/Mar/22 11:45;blerer;The patches look good. Thanks [~masokol];;;","10/Mar/22 13:32;adelapena;Committed to 3.0 as [c52b5ab95cf314a7983141f74c97744ff3ecbc81|https://github.com/apache/cassandra/commit/c52b5ab95cf314a7983141f74c97744ff3ecbc81] and merged to [3.11|https://github.com/apache/cassandra/commit/fcdb3d154ed111dbeefe8283bf729d6bd172384a], [4.0|https://github.com/apache/cassandra/commit/bdeafa0b79697b481ef4272fee606e70dad6764c] and [trunk|https://github.com/apache/cassandra/commit/eb960b19170e36bd9c52365ef15514a4d1ed9419].

Thanks for the patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Index summary redistribution can start even when compactions are paused,CASSANDRA-15265,13249286,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,07/Aug/19 09:13,15/May/20 08:38,13/Jul/23 08:38,31/Oct/19 13:41,3.0.20,3.11.6,4.0,4.0-alpha3,,Local/Compaction,,,,0,,,,"When we pause autocompaction for upgradesstables/scrub/cleanup etc we pause all compaction strategies to make sure we can grab all sstables, index summary redistribution does not pause and this can cause us to fail the operation.",,benedict,cscotta,marcuse,mbyrd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Degradation -> Other Exception,,,,,,,,Low Hanging Fruit,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Oct 31 13:41:14 UTC 2019,,,,,,,All,,,,,"0|z05ej4:",9223372036854775807,,,,,,,benedict,,,,Low,,3.0.0,,,https://github.com/apache/cassandra/commit/b40d79cf1cc6b4210edfa86d52546fd3880e22f0,,,,,,,,,new unit test,,,,,"08/Aug/19 05:24;marcuse;Patch adds a flag in `CompactionManager` which states if non-strategy compactions should be paused, only index summary redistributions uses it for now:

[3.0|https://github.com/krummas/cassandra/commits/marcuse/15265-3.0], [cci|https://circleci.com/workflow-run/3caa20be-4cca-4075-bf91-b5209e4d5abf]
[3.11|https://github.com/krummas/cassandra/commits/marcuse/15265-3.11], [cci|https://circleci.com/workflow-run/3737dab7-d8bb-4d74-9b49-c73f062e59ad]
[trunk|https://github.com/krummas/cassandra/commits/marcuse/15265-trunk], [cci|https://circleci.com/workflow-run/2a3a6917-4fde-43d1-b896-42b0e7186115];;;","31/Oct/19 11:13;benedict;LGTM.  

One tiny stylistic suggestion: I think it can be clearer to a reader when predicates are phrased so that they read like a predicate.  In this case we can't easily use our normal {{is}} prefix, but I guess we could use {{areGlobalCompactionsPaused}} or alternatively {{isGlobalCompactionStopRequested}} for consistency with the {{isStopRequested}}

Absolutely not blocking; happy for you to name and commit however you like.;;;","31/Oct/19 13:41;marcuse;changed the method to {{isGlobalCompactionPaused()}}, added a comment around {{ensureCapacity()}} and committed, thanks

tests: [3.0|https://circleci.com/workflow-run/8882a8a6-8593-4d3e-8ec1-05bcab855a44] [3.11|https://circleci.com/workflow-run/6b057c7e-1b4a-4f11-9af8-eb3ec2dd8cc9] [trunk|https://circleci.com/workflow-run/457f8304-c477-45e7-b195-06cf67c22450];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update defaults for server and client TLS settings,CASSANDRA-15262,13249218,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jolynch,jolynch,jolynch,06/Aug/19 21:59,21/Jun/20 02:39,13/Jul/23 08:38,21/Jun/20 02:39,4.0,4.0-beta1,,,,Local/Config,,,,0,,,,"The current `server_encryption_options` configuration options are as follows:
{noformat}
server_encryption_options:
    # set to true for allowing secure incoming connections
    enabled: false
    # If enabled and optional are both set to true, encrypted and unencrypted connections are handled on the storage_port
    optional: false
    # if enabled, will open up an encrypted listening socket on ssl_storage_port. Should be used
    # during upgrade to 4.0; otherwise, set to false.
    enable_legacy_ssl_storage_port: false
    # on outbound connections, determine which type of peers to securely connect to. 'enabled' must be set to true.
    internode_encryption: none
    keystore: conf/.keystore
    keystore_password: cassandra
    truststore: conf/.truststore
    truststore_password: cassandra
    # More advanced defaults below:
    # protocol: TLS
    # store_type: JKS
    # cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]
    # require_client_auth: false
    # require_endpoint_verification: false
{noformat}

A couple of issues here:
1. optional defaults to false, which will break existing TLS configurations for (from what I can tell) no particularly good reason
2. The provided protocol and cipher suites are not good ideas (in particular encouraging anyone to use CBC ciphers is a bad plan

I propose that before the 4.0 cut we fixup server_encryption_options and even client_encryption_options :
# Change the default {{optional}} setting to true. As the new Netty code intelligently decides to open a TLS connection or not this is the more sensible default (saves operators a step while transitioning to TLS as well)
# Update the defaults to what netty actually defaults to",,aholmber,e.dimitrova,jasonstack,jeromatron,jmckenzie,jolynch,mbyrd,mck,spmallette,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15146,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jolynch,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Low Hanging Fruit,Performance Regression Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Sun Jun 21 02:39:07 UTC 2020,,,,,,,All,,,,,"0|z05e40:",9223372036854775807,,,,,,,benedict,e.dimitrova,,,Low,,4.0-alpha1,,,https://github.com/apache/cassandra/commit/674b6cc1a5e905a9c234c649adaad2de79cfa560,,,,,,,,,"[trunk|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15262]
[dtest|https://github.com/apache/cassandra-dtest/commit/98c0be8789f1a016a1038bf3337c0fbbc8580bd6]

",,,,,"29/Aug/19 21:14;jolynch;This could slip to 4.0-beta if we had to, but it is going to be annoying for folks testing with TLS (it was for us).;;;","11/Mar/20 18:12;aholmber;[~jolynch] you mentioned having WIP that you might be able to hand off to someone. Will you be able to push that soon?;;;","17/Mar/20 17:58;spmallette;I took a swipe at this one just given the issue description - i think the following covers ""Change the default optional setting to true"" though i'm not completely sure there aren't other spots the change might be required:

https://github.com/apache/cassandra/compare/trunk...spmallette:15624-4.0

As for:

> Update the defaults to what netty actually defaults to

I wasn't sure what that referred to - the ciphers or protocols or both/more. It looks like netty (a version on the 4.1.x line slightly ahead of 4.1.37 anyway) has these defaults:

{code}
gremlin> io.netty.handler.ssl.SslContextBuilder.forClient().build().cipherSuites()
==>TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
==>TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
==>TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
==>TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
==>TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
==>TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA
==>TLS_RSA_WITH_AES_128_GCM_SHA256
==>TLS_RSA_WITH_AES_128_CBC_SHA
==>TLS_RSA_WITH_AES_256_CBC_SHA
{code}

If any of this on the right path and with a bit more guidance I could probably form this all up into a pull request. Any direction is appreciated!


;;;","17/Mar/20 19:02;jolynch;Hi, yes I just uploaded my mostly complete patch for this:
 * [branch|https://github.com/jolynch/cassandra/tree/CASSANDRA-15262]
 * [46347bbeab0a53f9a546346dac685294751e4119|https://github.com/apache/cassandra/commit/46347bbeab0a53f9a546346dac685294751e4119]
 * [tests|https://circleci.com/workflow-run/4a67351b-4fb6-4491-9b60-017597be14d8]

Unfortunately the server enabled flag got put a bunch of places in trunk (tools, virtual tables, unit tests, configuration startup checks) so I think the only safe way to remove it from server options is to make the member private and use a proper getter that the subclass overrides. The alternative is to make the member mutable but I'd prefer not to do that).

I still need to:
 * Generate local keystores/truststores and verify that various configurations of nodes can start using CCM with multiple datacenters and various values of internode_encryption
 * Update the CHANGES to indicate that the optional default is changing in client_envryption_options (this could be a security vulnerability for existing users)
 * Verify dtests work properly with the changes;;;","10/Apr/20 15:58;e.dimitrova;Hey [~jolynch],
Thanks for the update. Are you still working on this? Should we find anyone else to complete the rest of it if you are overloaded?;;;","24/Apr/20 18:29;e.dimitrova;Based on our conversation with [~jolynch] in Slack, the following scenarios should be confirmed as still working in a ~3 node cluster:
- No TLS certificates, default settings -> cluster works
- Add TLS certificates to 2/3 nodes, after restart the cluster is healthy (the two with TLS are talking over TLS and the one without TLS is talking plaintext)
- Add TLS cert to 3/3 nodes, switch optional to false -> cluster healthy
- Remove TLS cert from 1/3 nodes, restart -> other nodes should not see this node (because TLS is no longer optional) 

I will handle this later today or tomorrow.;;;","25/Apr/20 23:41;e.dimitrova;[~jolynch], just rebased and tried to start the server before running CI but the following error appears when I try to Start Cassandra:

*_INFO  [main] 2020-04-25 19:35:50,175 YamlConfigurationLoader.java:89 - Configuration location: file:/Users/ekaterina.dimitri/CASSANDRA-15262/cassandra/conf/cassandra.yaml
Exception (org.apache.cassandra.exceptions.ConfigurationException) encountered during startup: Invalid yaml: file:/Users/ekaterina.dimitri/CASSANDRA-15262/cassandra/conf/cassandra.yaml
 Error: null; Can't construct a java object for tag:yaml.org,2002:org.apache.cassandra.config.Config; exception=Cannot create property=client_encryption_options for JavaBean=org.apache.cassandra.config.Config@18078bef; Cannot create property=enabled for JavaBean=org.apache.cassandra.config.EncryptionOptions@ee7e1c03; Unable to find property 'enabled' on class: org.apache.cassandra.config.EncryptionOptions;  in 'reader', line 10, column 1:
    cluster_name: 'Test Cluster'
    ^

That is because we still have it in config but we completely remove it from the yaml. I just saw an old comment where you mention this issue and realized it wasn't actually fixed on your branch. As you mentioned on Slack that only tests are needed, I was wondering whether you have it already fixed (maybe locally?) or I should take care of the issue?
I also checked whether we already have the tests as you mentioned on Slack that it might be the case, but looks like there are some tests but not exactly what we need.
 ;;;","27/Apr/20 19:23;e.dimitrova;
Setter added to solve the issue:

[branch|https://github.com/jolynch/cassandra/commits/CASSANDRA-15262]

[commit|https://github.com/jolynch/cassandra/commit/2eea573e63b3d6505e1d79bf552c9a2867f10f14]

;;;","28/Apr/20 02:55;e.dimitrova;Looks like the tests in sslnodetonode_test.py fail - 5 out of 8 locally. I guess they should be corrected too and it is not the patch but it's late now.
I am running now the full CI in CirclCI to get the full picture.  I will proceed tomorrow.
;;;","30/Apr/20 01:47;e.dimitrova;[~jolynch], I am not sure whether you had the opportunity to look at the failures we talked about last night. 

Update on my end:
Currently the following two D-tests fail:
_     _native_transport_ssl_test.py::TestNativeTransportSSL::test_connect_to_ssl_
_     _TestNodeToNodeSSLEncryption::test_optional_outbound_tls_


- native_transport_ssl_test.py::TestNativeTransportSSL::test_connect_to_ssl fails. - _client_encryption_options:optional_ should be set to_ False_ in _native_transport_ssl_test.py::TestNativeTransportSSL::test_connect_to_ssl._
- _TestNodeToNodeSSLEncryption::test_optional_outbound_tls_ fails at the point when we restart node1 with encryption being enabled and plaintext port shutdown. The inbound connection node2 is refused because of _InvalidLegacyProtocolMagic: Read 369295616, Expected -900387334_.  Making server_encryption_options:enabled private is not working.
I double-checked the changes with the private variable and they look correct. This means that there is something more non-trivial? (I am new to this part of the codebase, probably you might know something more to be considered than the public to private change)

I don't see an issue with the change of optional to default value of True in cassandra.yaml but I suggest if we want to setup _server_encryption_options:enabled_ to _True_ as an immutable value, just to leave it commented and if the user tries to change it to false - to reject the change with a warning. We can mark it as Deprecated in the DatabaseDescriptor as other parameters. I tried simply to comment server_encryption_options:enabled in cassandra.yaml and the tests are passing. 

[~jolynch], considering you mentioned there is already a ticket for improvement to land in 4.1 which suggests to split _ServerEncryptionOptions_ class from _EncryptionOptions_ class, I would personally leave this patch in the suggested way with minimum changes applied to the code as we are in code freeze. WDYT?
 









;;;","30/Apr/20 15:19;e.dimitrova;I found CASSANDRA-15146 which is supposed to remove *enabled* completely in beta. Is it worth it to keep on digging on this issue then, considering this is the last alpha ticket?
;;;","06/May/20 06:36;jolynch;Got a chance to look at this today, the first test failure was just because the default for client optional switched to true. The second failure was because I was still referencing the enabled logic in the server so we were not entering transitional mode.

||Cassandra Branch||Dtest Branch||
|[jolynch:CASSANDRA-15262|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15262]|[jolynch:CASSANDRA_15262|https://github.com/apache/cassandra-dtest/commit/98c0be8789f1a016a1038bf3337c0fbbc8580bd6]|

Running dtests now.

I agree let's get this change just far enough so we can commit it to allow beta testers to not break with the upgrade and we can revisit the naming of optional and internode_encryption_options in 15146 in 4.0-beta.

I think because we added optional in 4.0 we can rename it to like mode: <off, on, transitional> or something in the beta... We can figure out the naming in 15146.
;;;","07/May/20 18:02;jolynch;Alright CI runs for:
* [trunk|https://app.circleci.com/pipelines/github/jolynch/cassandra/14/workflows/e8af29fd-1232-43cf-85ad-f7539e16b301], only two unrelated dtest failures.
* [2.2|https://app.circleci.com/pipelines/github/jolynch/cassandra/16/workflows/e8b14e66-80fd-4574-9acd-9740e44dbd15] has some errors, will double check they're unrelated
* [3.0|https://app.circleci.com/pipelines/github/jolynch/cassandra/15/workflows/be9286c2-cb73-4ff2-9db2-394ee2251d6f] has some errors, will double check they're unrelated.

I think this patch is ready for review. I've put code comments indicating where we need to do more refactoring in the future tickets (specifically splitting these classes and changing the names to be sensible, e.g. require_client_auth is just odd in server options, but let's defer that work to beta or rc).

[~benedict] or [~e.dimitrova] please let me know if you have any feedback on the patch (and dtest patch) as is.;;;","07/May/20 19:06;e.dimitrova;The patch looks good to me. 
On trunk I see two new failures which as per Jenkins were not presented up to now but they don't look related on high level:
- test_dead_sync_initiator - repair_tests.repair_test.TestRepair
- test_resume_stopped_build - materialized_views_test.TestMaterializedViews

 I am lost with 2.2 and 3.0 as I haven't been working on them for quite some time and I am not acquainted with the current CI issues there. Looks like more of cqlsh unrelated ones?;;;","08/May/20 05:54;mck;ci-cassandra.apache.org run [here|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/118/pipeline]. (I had to specify the dtest branch rather than commit, so the tox commit is included.);;;","18/May/20 00:03;e.dimitrova;I compared the test results with the Jenkins reports.

test_closing_connections - thrift_hsha_test.TestThriftHSHA is the only test that is failing on 2.2 and 3.0 currently with this patch but not in Jenkins.


;;;","30/May/20 16:22;jmckenzie;What's our status here [~jolynch] ?;;;","13/Jun/20 23:41;jmckenzie;Spoke w/Joey offline and the plan is to tidy up that one outstanding test issue and merge in.;;;","14/Jun/20 23:53;jolynch;I didn't make any server changes on 2.2 or 3.0, I was only running them to check that my dtest change still works with them. If neither native_transport_ssl_test nor sslnodetonode_test fail against 2.2 or 3.0 I think we are good.

I've rebased, cleaned up the patches per the commit guidelines and staged the server and dtest patches for commit:
||Repo||Branch||Commit||
|Cassandra|[CASSANDRA-15262-final|https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-15262-final]|[08520aa3|https://github.com/apache/cassandra/commit/08520aa39aa3e884dae2f13652fa98d315371a1e]|
|Dtest|[CASSANDRA_15262|https://github.com/apache/cassandra-dtest/compare/master...jolynch:CASSANDRA-15262]|[11ac60df|https://github.com/jolynch/cassandra-dtest/commit/11ac60df9497c6f9ea277aad42b465117a43b8c8]|

Testing in progress:
 * [Trunk - Java 8 Tests|https://app.circleci.com/pipelines/github/jolynch/cassandra/20/workflows/db3d061a-7c5f-4811-bae9-f7469386cc2b]
 * [Trunk - Java 11 Tests|https://app.circleci.com/pipelines/github/jolynch/cassandra/20/workflows/9d6c3b86-6207-4ead-aa4b-79022fc84182]
 * [2.2 - Tests|https://app.circleci.com/pipelines/github/jolynch/cassandra/21/workflows/c441a44a-9c0b-4035-812b-46385c5ce626]
 * [3.0 - Tests|https://app.circleci.com/pipelines/github/jolynch/cassandra/22/workflows/10b2afed-cabb-4d4d-a599-4ce3fffac719];;;","15/Jun/20 13:25;e.dimitrova;Ok, looks like going back and forth I lost track at some point where it is committed.

Thank you for the patch and the rebase. I just went through the CI runs results. LGTM +1

I am not a committer, unfortunately.;;;","21/Jun/20 01:47;jolynch;The dtest failures on trunk appear to me to be unrelated:
* pushed_notifications_test.TestPushedNotifications (reliably failing on all dtest runs)
* repair_tests.repair_test.TestRepair
* read_repair_test.TestSpeculativeReadRepair
* test_cleanup - bootstrap_test.TestBootstrap
* disk_balance_test.TestDiskBalance

The java 8 upgrade tests are rather red, but I don't think those are related either.

I did not see sslnodetonode_test or native_transport_ssl_test failing in the 2.2 or 3.0 runs.

Given the +1 and (mostly) green tests, marking this ready to commit.;;;","21/Jun/20 02:39;jolynch;Committed as [674b6cc1|https://github.com/apache/cassandra/commit/674b6cc1a5e905a9c234c649adaad2de79cfa560] in cassandra and [a0f55239|https://github.com/apache/cassandra-dtest/commit/a0f5523910dcc96f204220e3c5ba24f7be15f37b] in cassandra-dtest.

Thanks [~e.dimitrova], apologies for the delays.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Selecting Index by Lowest Mean Column Count Selects Random Index,CASSANDRA-15259,13248920,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jwest,jwest,jwest,05/Aug/19 16:39,01/Aug/21 11:17,13/Jul/23 08:38,06/Aug/19 17:27,3.0.19,3.11.7,4.0,4.0-alpha1,,Feature/2i Index,,,,0,,,,"{{CassandraIndex}} uses [{{ColumnFamilyStore#getMeanColumns}}|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/index/internal/CassandraIndex.java#L273], average columns per partition, which always returns the same answer for index CFs because they contain no regular columns and clustering columns aren't included in the count in Cassandra 3.0+.

 

 ",,bdeggleston,cscotta,jwest,mbyrd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jwest,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Aug 06 17:27:48 UTC 2019,,,,,,,All,,,,,"0|z05ca0:",9223372036854775807,,,,,,,bdeggleston,,,,Critical,,3.0.0,,,https://github.com/apache/cassandra/commit/da8d41f497efedf57e335ec2664680da583a3aba,,,,,,,,,Regression test added as part of patch,,,,,"05/Aug/19 18:50;jwest;||Branch||Tests||
|[trunk|https://github.com/apache/cassandra/compare/trunk...jrwest:jwest/15259-trunk]|[cci|https://circleci.com/workflow-run/fd4a72ce-d1fd-4c15-8b2a-a20544b658c8]|
|[3.11|https://github.com/apache/cassandra/compare/trunk...jrwest:jwest/15259-3.11]|[cci|https://circleci.com/workflow-run/69faf98e-7cc8-4f68-9af9-d6317e29923d]|
|[3.0|https://github.com/apache/cassandra/compare/trunk...jrwest:jwest/15259-3.0]|[cci|https://circleci.com/workflow-run/b958fc7e-91f1-4b30-a7df-89ea7941ee60]|;;;","05/Aug/19 20:54;bdeggleston;The {{totalRows}} metadata element was added to the sstable format in 3.0, so we’ll still need to use the old method for 2.x sstables. Looks good otherwise.;;;","06/Aug/19 03:26;jwest;[~bdeggleston] good catch re: 2.x sstables. I see two ways to handle that off the top of my head – besides not including the legacy sstables in the calculation which is broken.

I think I prefer {{getMeanRowCount2}} (average of the row count and column count) because in the case of 100% legacy sstables or 100% new sstables it degrades to {{getMeanColumns}} or the original {{getMeanRowCount.}} Neither implementation is ideal since we have to handle it at the per sstable level and what that means for an average is ambiguous. 

Also, I wonder if the method name should change and/or if the logic should be moved to somewhere index specific like {{CassandraIndex}}, now that what its doing is a bit more specialized and less clear. WDYT?

 
{code:java}
public int getMeanRowCount()
{
    long totalRows = 0;
    long totalPartitions = 0;
    for (SSTableReader sstable : getSSTables(SSTableSet.CANONICAL))
    {
        if (sstable.descriptor.version.storeRows())
        {
            totalPartitions += sstable.getEstimatedPartitionSize().count();
            totalRows += sstable.getTotalRows();
        } else
        {
            long colCount = sstable.getEstimatedColumnCount().count();
            totalPartitions += colCount;
            totalRows += sstable.getEstimatedColumnCount().mean() * colCount;
        }
    }

    return totalPartitions > 0 ? (int) (totalRows / totalPartitions) : 0;
}

public int getMeanRowCount2()
{
    long totalRows = 0;
    long totalPartitions = 0;
    long legacyCols = 0;
    long legacyTotal = 0;
    for (SSTableReader sstable : getSSTables(SSTableSet.CANONICAL))
    {
        if (sstable.descriptor.version.storeRows())
        {
            totalPartitions += sstable.getEstimatedPartitionSize().count();
            totalRows += sstable.getTotalRows();
        } else
        {
            long colCount = sstable.getEstimatedColumnCount().count();
            legacyCols += sstable.getEstimatedColumnCount().mean() * colCount;
            legacyTotal += colCount;
        }
    }

    int rowMean = totalPartitions > 0 ? (int) (totalRows / totalPartitions) : 0;
    int legacyMean = legacyTotal > 0 ? (int) (legacyCols / legacyTotal) : 0;

    return (int) (((rowMean * totalPartitions) + (legacyMean * legacyTotal)) / (totalPartitions + legacyTotal));
}
{code}
 ;;;","06/Aug/19 15:54;bdeggleston;I’m not 100% sure, but I think the math in both methods commute to the same calculation, in which case I’d prefer {{getMeanRowCount}} for it’s simplicity.

I do agree this should move into {{CassandraIndex}} though, since it’s pretty specific to that use case.;;;","06/Aug/19 16:09;jwest;[~bdeggleston] pushed updates for 3.0 and 3.11. If I'm reading the code right the legacy sstables aren't supported in trunk so I left the code as is since it seemed cleaner and generally useful. Happy to change trunk as well though if you prefer. ;;;","06/Aug/19 17:27;bdeggleston;+1, committed to 3.0 as [da8d41f497efedf57e335ec2664680da583a3aba|https://github.com/apache/cassandra/commit/da8d41f497efedf57e335ec2664680da583a3aba] and merged up to trunk. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't consider current keyspace in prepared statement id when the query is qualified,CASSANDRA-15252,13248454,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,omichallat,omichallat,02/Aug/19 00:32,27/May/22 19:25,13/Jul/23 08:38,01/Oct/21 16:18,3.0.26,3.11.12,4.0.2,4.1,4.1-alpha1,Messaging/Client,,,,0,,,,"{{QueryProcessor.computeId}} takes into account the session's current keyspace in the MD5 digest.
{code}
String toHash = keyspace == null ? queryString : keyspace + queryString;
{code}
This is desirable for unqualified queries, because switching to a different keyspace produces a different statement. However, for a qualified query, the current keyspace makes no difference, the prepared id should always be the same.

This can lead to an infinite reprepare loop on the client. Consider this example (Java driver 3.x):
{code}
    Cluster cluster = null;
    try {
      cluster = Cluster.builder().addContactPoint(""127.0.0.1"").build();
      Session session = cluster.connect();

      session.execute(
          ""CREATE KEYSPACE IF NOT EXISTS test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}"");
      session.execute(""CREATE TABLE IF NOT EXISTS test.foo(k int PRIMARY KEY)"");

      PreparedStatement pst = session.prepare(""SELECT * FROM test.foo WHERE k=?"");

      // Drop and recreate the table to invalidate the prepared statement server-side
      session.execute(""DROP TABLE test.foo"");
      session.execute(""CREATE TABLE test.foo(k int PRIMARY KEY)"");

      session.execute(""USE test"");

      // This will try to reprepare on the fly
      session.execute(pst.bind(0));
    } finally {
      if (cluster != null) cluster.close();
    }
{code}
When the driver goes to execute the bound statement (last line before the finally block), it will get an UNPREPARED response because the statement was evicted from the server cache (as a result of dropping the table earlier).
In those cases, the driver recovers transparently by sending another PREPARE message and retrying the bound statement.
However, that second PREPARE cached the statement under a different id, because we switched to another keyspace. Yet the driver is still using the original id (stored in {{pst}}) when it retries, so it will get UNPREPARED again, etc.

I would consider this low priority because issuing a {{USE}} statement after having prepared statements is a bad idea to begin with. And even if we fix the generated id for qualified query strings, the issue will remain for unqualified ones.

We'll add a check in the driver to fail fast and avoid the infinite loop if the id returned by the second PREPARE doesn't match the original one. That might be enough to cover this issue.",,bereng,e.dimitrova,ifesdjeen,jeromatron,jorgebg,marcuse,omichallat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-17349,CASSANDRA-17328,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,Availability -> Response Crash,,,,,,,,Challenging,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Dec 09 07:38:48 UTC 2021,,,,,,,All,,,,,"0|z059eg:",9223372036854775807,,,,,,,ifesdjeen,marcuse,,,Critical,,3.0.0,,,https://github.com/apache/cassandra/commit/13632e9a99e8256a565bd6919d2d11b3e476e973,,,,,,,,,Tests are included in the patch ,,,,,"10/Aug/21 10:52;ifesdjeen;Patches for all branches: 

|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...ifesdjeen:CASSANDRA-15252-3.0]|[tests|https://app.circleci.com/pipelines/github/ifesdjeen/cassandra?branch=CASSANDRA-15252-3.0]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ifesdjeen:CASSANDRA-15252-3.11]|[tests|https://app.circleci.com/pipelines/github/ifesdjeen/cassandra?branch=CASSANDRA-15252-3.11]|
|[4.0|https://github.com/apache/cassandra/compare/cassandra-4.0...ifesdjeen:CASSANDRA-15252-4.0]|[tests|https://app.circleci.com/pipelines/github/ifesdjeen/cassandra?branch=CASSANDRA-15252-4.0]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15252-trunk]|[tests|https://app.circleci.com/pipelines/github/ifesdjeen/cassandra?branch=CASSANDRA-15252-trunk]|

I've made a test run on [trunk|https://app.circleci.com/pipelines/github/ifesdjeen/cassandra?branch=run-tests-trunk] and [3.0|https://app.circleci.com/pipelines/github/ifesdjeen/cassandra?branch=run-tests-3.0] to check what's up with dtests, and they seem to be just broken on all branches.;;;","15/Sep/21 14:41;marcuse;+1, minor comments;
- we should probably not log error + stack trace when failing to parse the cassandra version
- add {{JVMStabilityInspector.inspectThrowable(t);}} when we catch {{Throwable}}

;;;","23/Sep/21 12:38;ifesdjeen;Thank you for the review! 

I've pushed the fix with [inspectThrowable|https://github.com/apache/cassandra/commit/fb3708a0bffd0de23f0d81242533a8b1be852c46] to all branches, are we good to commit this with this change? ;;;","23/Sep/21 12:48;marcuse;+1;;;","01/Oct/21 16:17;ifesdjeen;Committed to 3.0 with [13632e9a99e8256a565bd6919d2d11b3e476e973 |https://github.com/apache/cassandra/commit/13632e9a99e8256a565bd6919d2d11b3e476e973], and merged up to [3.11|https://github.com/apache/cassandra/commit/32a15f0e9b90ff467cc8a686461cb1b769539672], [4.0|https://github.com/apache/cassandra/commit/3660a58ab458b90fdf9de5cda83099b87aad4c86] and [trunk|https://github.com/apache/cassandra/commit/c3c2c7efecb23d7627aa8ffda72603dc88f1ed37].;;;","03/Dec/21 08:46;bereng;Hi,

if my bisect kung-fu is good enough I think this commit breaks [some upgrade tests|https://ci-cassandra.apache.org/job/Cassandra-4.0/295/testReport/junit/dtest-upgrade.upgrade_tests.upgrade_through_versions_test/TestUpgrade_current_3_0_x_To_indev_4_0_x/test_rolling_upgrade_with_internode_ssl/] and similar ones at least on 4.0 and probably the ones in trunk as well.

3660a58ab458b90fdf9de5cda83099b87aad4c86 fails whereas the previous one on 4.0 b22749bbb2c5f5dc91312c7e0f7107e9a301b8eb passes the following:

{noformat}
pytest -vv --log-cli-level=DEBUG --junit-xml=nosetests.xml --junit-prefix=dtest-upgrade -s --cassandra-dir=<yourPathHere> --execute-upgrade-tests-only --upgrade-target-version-only --upgrade-version-selection all upgrade_tests/upgrade_through_versions_test.py::TestUpgrade_current_3_0_x_To_indev_4_0_x::test_rolling_upgrade_with_internode_ssl
{noformat}
;;;","08/Dec/21 01:11;e.dimitrova;I think also previous versions have failing rolling upgrade tests. There is ticket CASSANDRA-17140 and I was also bisecting today as I didn't know about [~bereng] 's findings until I read this one but I have reached to the same conclusion that this Is where the failures started, same day. Jenkins lacks some runs history for newer branches but looking into 3.0 now where we also have the rolling upgrades failing I can see them failing also with the previous patch that day - CASSANDRA-17014.

Please check [here|https://ci-cassandra.apache.org/job/Cassandra-3.0/204/].  for 3.0 and also failing in Jenkins for 3.11 [here|https://jenkins-cm4.apache.org/job/Cassandra-3.11/265/#showFailuresLink]

The tests were passing in 3.0 for CASSANDRA-16795 [here|https://ci-cassandra.apache.org/job/Cassandra-3.0/200/].

If it is not CASSANDRA-17014, then there are only two other option possible after CASSANDRA-16795 - 

CASSANDRA-16959 or CASSANDRA-14612.

Unfortunately, there is no more history in Jenkins and I can't run them locally for some reason now to test. And with this I will move the discussion to CASSANDRA-17140, just wanted to clarify. ;;;","08/Dec/21 06:27;bereng;I'd rather wait you can repro locally to confirm than rely on jenkins history given the back & forth, restarts, etc that jenkins gets every now and then. Local repro seems more reliable?;;;","09/Dec/21 07:38;bereng;[~e.dimitrova] and I did more bisecting [here|https://issues.apache.org/jira/browse/CASSANDRA-17140?focusedCommentId=17456203&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17456203] and imo it still looks like CASSANDRA-15252 is the best candidate so far.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition between flushing and compaction stalls compaction indefinitely,CASSANDRA-15242,13246282,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,tvdw,tvdw,22/Jul/19 09:32,20/Jun/22 14:43,13/Jul/23 08:38,08/Sep/20 11:16,3.11.9,4.0,4.0-beta3,,,Local/Compaction/LCS,,,,0,,,,"Seen on Cassandra 3.11.4 with OpenJDK 8u212, although I've seen this a few times before, also on 3.11.3. It's a rare issue so I've not bothered with trying to trace it until now.
{noformat}
DEBUG [NativePoolCleaner] 2019-07-18 01:12:41,799 ColumnFamilyStore.java:1325 - Flushing largest CFS(Keyspace='keyspacename', ColumnFamily='tablename') to free up room. Used total: 0.10/0.33, live: 0.10/0.33, flushing: 0.00/0.00, this: 0.09/0.19
DEBUG [NativePoolCleaner] 2019-07-18 01:12:41,800 ColumnFamilyStore.java:935 - Enqueuing flush of tablename: 267.930MiB (9%) on-heap, 575.580MiB (19%) off-heap
DEBUG [PerDiskMemtableFlushWriter_0:204] 2019-07-18 01:12:42,480 Memtable.java:456 - Writing Memtable-tablename@498336646(520.721MiB serialized bytes, 870200 ops, 9%/19% of on/off-heap limit), flushed range = (min(-9223372036854775808), max(9223372036854775807)]
INFO  [Service Thread] 2019-07-18 01:12:43,616 GCInspector.java:284 - G1 Young Generation GC in 227ms.  G1 Eden Space: 14713618432 -> 0; G1 Old Gen: 13240876928 -> 13259198848; G1 Survivor Space: 276824064 -> 268435456;
INFO  [Service Thread] 2019-07-18 01:12:56,251 GCInspector.java:284 - G1 Young Generation GC in 206ms.  G1 Eden Space: 14713618432 -> 0; G1 Old Gen: 13259198848 -> 13285123456; G1 Survivor Space: 268435456 -> 285212672;
DEBUG [PerDiskMemtableFlushWriter_0:204] 2019-07-18 01:12:56,693 Memtable.java:485 - Completed flushing /cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db (524.023MiB) for commitlog position CommitLogPosition(segmentId=1563386911266, position=32127822)
DEBUG [MemtableFlushWriter:204] 2019-07-18 01:12:57,620 ColumnFamilyStore.java:1233 - Flushed to [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] (1 sstables, 518.714MiB), biggest 518.714MiB, smallest 518.714MiB
WARN  [CompactionExecutor:1617] 2019-07-18 01:12:57,628 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.
{noformat}
This final line then starts repeating about once per minute:
{noformat}
WARN  [CompactionExecutor:1610] 2019-07-18 01:13:18,898 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1611] 2019-07-18 01:14:18,899 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1622] 2019-07-18 01:15:18,899 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1436] 2019-07-18 01:16:15,073 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1618] 2019-07-18 01:16:18,899 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1611] 2019-07-18 01:17:18,900 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1606] 2019-07-18 01:18:18,900 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1630] 2019-07-18 01:19:18,902 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1627] 2019-07-18 01:20:18,904 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1638] 2019-07-18 01:21:18,904 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1631] 2019-07-18 01:22:18,905 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1636] 2019-07-18 01:22:58,220 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.

WARN  [CompactionExecutor:1625] 2019-07-18 01:23:18,905 LeveledCompactionStrategy.java:144 - Could not acquire references for compacting SSTables [BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292363-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292348-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292358-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292342-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292344-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292343-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292340-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292338-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292336-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292335-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292337-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292346-big-Data.db'), BigTableReader(path='/cassandra-data/data/keyspacename/tablename-2c5325e042e911e8a07e9db72d27cf67/md-292349-big-Data.db')] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.{noformat}
It will keep going like this for days, until restarted, but compaction won't run until then, so sstables pile up.",,jtgalbraith,marcuse,tvdw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14103,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/19 10:09;tvdw;snippet.txt;https://issues.apache.org/jira/secure/attachment/12975390/snippet.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Sep 08 11:16:07 UTC 2020,,,,,,,All,,,,,"0|z04w0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/19 09:55;marcuse;Could you post more logs? It is either {{md-292348-big-Data.db}} or {{md-292358-big-Data.db}} causing this, so logs starting with when {{md-292348-big-Data.db}} was created would be really helpful;;;","22/Jul/19 10:09;tvdw;[^snippet.txt];;;","22/Jul/19 10:12;tvdw;I think you're right, I may have been chasing the wrong thing. I've attached the part of debug.log from the moment the first of the two sstables got written until the first error message (~5200 lines). One thing that now stands out to me:

{noformat}DEBUG [IndexSummaryManager:1] 2019-07-18 01:09:18,530 CompactionStrategyManager.java:380 - Recreating compaction strategy - disk boundaries are out of date for keyspacename.tablename.
DEBUG [IndexSummaryManager:1] 2019-07-18 01:09:18,530 DiskBoundaryManager.java:53 - Refreshing disk boundary cache for keyspacename.tablename{noformat};;;","08/Sep/20 11:16;marcuse;this is probably the same issue as in CASSANDRA-14103;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standardise config and JVM parameters,CASSANDRA-15234,13245388,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,benedict,benedict,17/Jul/19 07:32,24/Jan/23 19:23,13/Jul/23 08:38,06/Feb/22 03:56,4.1,4.1-alpha1,,,,Local/Config,,,,0,,,,"We have a bunch of inconsistent names and config patterns in the codebase, both from the yams and JVM properties.  It would be nice to standardise the naming (such as otc_ vs internode_) as well as the provision of values with units - while maintaining perpetual backwards compatibility with the old parameter names, of course.

For temporal units, I would propose parsing strings with suffixes of:

{{code}}
u|micros(econds?)?
ms|millis(econds?)?
s(econds?)?
m(inutes?)?
h(ours?)?
d(ays?)?
mo(nths?)?
{{code}}

For rate units, I would propose parsing any of the standard {{B/s, KiB/s, MiB/s, GiB/s, TiB/s}}.
Perhaps for avoiding ambiguity we could not accept bauds {{bs, Mbps}} or powers of 1000 such as {{KB/s}}, given these are regularly used for either their old or new definition e.g. {{KiB/s}}, or we could support them and simply log the value in bytes/s.",,aleksey,benedict,blerer,cnlwsu,colinkuo,dcapwell,djoshi,e.dimitrova,ifesdjeen,jasonstack,jeromatron,jmckenzie,jmeredithco,jwest,lorina@datastax.com,maedhroz,mck,n.v.harikrishna,newkek,pauloricardomg,polandll,PuerTea,rssvihla,rustyrazorblade,sbtourist,stefan.miklosovic,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-17166,CASSANDRA-17074,,CASSANDRA-15146,,,,CASSANDRA-16152,CASSANDRA-9691,CASSANDRA-17574,CASSANDRA-17141,CASSANDRA-17160,CASSANDRA-17379,CASSANDRA-17243,CASSANDRA-17246,CASSANDRA-17148,CASSANDRA-17431,CASSANDRA-17949,CASSANDRA-15146,CASSANDRA-16038,CASSANDRA-17132,CASSANDRA-17131,CASSANDRA-17118,CASSANDRA-17135,,,,,,,,"05/Jun/20 03:36;e.dimitrova;CASSANDRA-15234-3-DTests-JAVA8.txt;https://issues.apache.org/jira/secure/attachment/13004872/CASSANDRA-15234-3-DTests-JAVA8.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,e.dimitrova,,,,,,,,,,,,Correctness,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Jan 20 17:36:00 UTC 2023,,,,,,,All,,,,,"0|z04qy8:",9223372036854775807,,,,,,,blerer,dcapwell,maedhroz,mck,Normal,,,,,https://github.com/apache/cassandra/commit/db9f7a67ec4b03413c10034956e2cf18739ca4b1,,,,,,,,,"CCM patch committed [here|https://github.com/riptano/ccm/commit/6d676896fb2ced4b6ac62fdba4ae2570351b38df] and retagged.

DTest patch committed [here|https://github.com/apache/cassandra-dtest/commit/3935906a685640b2f6a2058b38fdf45d917edfc9]

Trunk patch split to 13 patches starts from [this one|https://github.com/apache/cassandra/commit/db9f7a67ec4b03413c10034956e2cf18739ca4b1] on.

The existing tests plus new unit tests added. Also, dtests exercise the backward compatibility and test that way that ccm supports the same behavior as Cassandra as regards to configuration parameters loading. 

Docs in review - CASSANDRA-17246",,,,,"17/Jul/19 11:16;aleksey;Would be nice, while at it, to pull all JVM args into one class, a-la {{Config}}, with strongly typed accessors and proper documentation for each arg.

Right now you have to look for usage of {{System.getProperty()}}, {{Boolean.getBoolean()}}, etc., to enumerate them all, which is a mess.;;;","17/Jul/19 11:16;benedict;Definitely.;;;","28/Dec/19 19:55;rustyrazorblade;Probably want ms (milliseconds) in there as well.;;;","28/Dec/19 20:17;benedict;Yep.  Hopefully I'd have noticed when implementing, but thanks for making sure!;;;","29/Jan/20 22:47;dcapwell;I see a few things in this JIRA

1) centralize location of JVM args
2) backwards compatible way to rename properties (JVM args and yaml)
3) support common unit conversions (time, space, etc.)
4) type safe values (Rate, Duration, Space, etc)
5) backwards compatible value parsing
6) document all the things

I should be able to start taking on parts of this in a few weeks, unless anyone else wants to in the mean time.;;;","03/Mar/20 18:46;e.dimitrova;[~dcapwell]. did you start this one? ;;;","03/Mar/20 20:47;dcapwell;Nope. All yours if you want;;;","03/Mar/20 20:49;mck;And there are a lot of accessing jvm args using the ""cassandra."" string literal prefix, instead of {{Config.PROPERTY_PREFIX}}

And, this ticket would help simplify {{SystemPropertiesTable.CASSANDRA_RELEVANT_PROPERTIES}} in CASSANDRA-15616;;;","03/Mar/20 21:20;e.dimitrova;Great, then I am gonna start working on it tomorrow!

Probably will ask some questions  at some point.

Thanks!;;;","12/Mar/20 13:57;rssvihla;I think this is overall a good idea, but it definitely needs some limits on the number of combinations supported. Let's just look at a hypothetical new configuration `compaction_throughput` instead of `compaction_throughput_mb_per_sec` (which is a poster child for a difficult to remember name in need of review):

* Any downstream tooling that reads configuration will have to take on everything we add, which is fine, but the more math we require the worse it is on them to get updated. An older tool will read compaction_throughput_mb_per_sec and it was self documenting. A new tool will have to take into account every variant we support for MiB/MB/mb/bytes/etc/etc
* what about `500mb` vs `500mb/s` vs `500 mbs` (note the space) vs `500MiB/s` vs `500  mb/s` (note the 2 spaces). Which of those is obviously valid or wrong at a glance to a new user? So if we're going to do it, definitely only accept one valid that's the same for everything..I still think it's adding some learning curve for new users.;;;","12/Mar/20 14:29;e.dimitrova;Ok, back to this one. Unfortunately, I was parallelizing with some other stuff at the beginning of the week. 

Thanks [~mck] for sharing that ticket, that was helpful.

I started from looking into standardizing the parameters names and  1) of the plan [~dcapwell] shared.

Also, my idea was to reshuffle a bit the order of the parameters in the yaml and try to put them in sections, for example:
 * Quick start
The minimal properties needed for configuring a cluster.

 * Commonly used
Properties most frequently used when configuring Cassandra.

 * Performance Tuning
Tuning performance and system resource utilization, including commit log, compaction, memory, disk I/O, CPU, reads, and writes.

 * Advanced
Properties for advanced users or properties that are less commonly used.

 * SecurityServer and client security settings.

Yes, this idea came after looking at online documentations and trying to think from user's perspective.

On another topic, looking at the plan, I realized that you might want actually a parser and unit converter for the values in order to give freedom to the users to add whatever value they would like in the yaml. Initially, I was thinking that we talk about the suffixes of the parameters names, the way we see them now and some name changes and backward compatibilities. 

So my question is what is the reason behind doing this and shall we split this Jira then in a couple of smaller ones if we go into that direction? 

 

 ;;;","12/Mar/20 15:38;benedict;My personal view is that there's no need to overcomplicate this ticket, and that perhaps David Capwell anticipates greater complexity in addressing it than I do.

My goal is to make the config more intuitive, consistent and also to not make any assumptions about reasonable units.  Every property has its own units picked arbitrarily, and this is confusing.  Publishing a simple regex to validate the value types for tools seems sufficient to address [~rssvihla]'s concerns, I think?  And I don't think we should make it pluggable, just accept quantities and rates, and we can be quite restrictive, even just accepting GiB/s, MiB/s and KiB/s.

My personal approach would be to, in separate commits but one ticket
# Move properties to a single file, with strongly typed methods and sensible names for fetching the property.
# Note property names present in either {{Config}} or via system properties, that relate to the same concept but use different terms - not just prefixes like otc_, but also:
#* Why do we use {{enable}}, {{enabled}} and {{disable}} in our property names?  Why does it sometimes go at the start or end?
#* Why do we use both {{dc}} and {{datacenter}}?
#* Who does {{max}} sometimes go at the start, sometimes in the middle?
#* ...
# Rename things, and make sure {{Config}} and system properties look for both old and new forms
# Support super simple parsing for rate and size
# Done;;;","12/Mar/20 17:48;dcapwell;Yep, think we are on the same page here; I just prefer smaller patches so rather have ~3-4 (based off your split) patches rather than the same number of commits but one patch.  ;;;","17/Mar/20 17:33;e.dimitrova;Thanks [~benedict] for the confirmation on what you had in mind here. Makes sense to me.

So just to confirm I am not doing something stupid.

For the backward compatibility we are simply using either the old or new parameter from yaml in Config/SysProperties, depending on which one the user used there? In the Database Descriptor I will add a warning which will also warn the developers that they can already use the new one. How about that? Or something else is meant in this case for backward compatibility? 

I already created a class of accessors for the SystemProperties. Just want to think also maybe about some of the environment variables too.

The config is reshuffled and many renames are already in place so I guess after this confirmation and a couple of other things to complete, I will submit soon the commits for review.  Thanks;;;","23/Mar/20 21:59;e.dimitrova;[Part1|https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-2] - dtests are still running and have to clear/correct some comments in config but all in-jvm and unit tests look fine to me.
- New cassandra.yaml ---> currently cassandra-new.yaml in the branch as I wanted to test the backward compatibility on the old version (will have to rename properly also these yaml files with version probably)
    # reorganization of the file
    # renames to make the names more consistent
- Backward compatibility with the old names - my approach is:
   # Cassandra comes with the new version of the yaml to ensure any new builds will be using it
   # During load of the yaml file I check whether old names are identified(means someone upgraded to 4 and wants to still use the old yaml). If this is the case, we load the values from the old yaml to the new variables (the already renamed parameters). Might need some refactoring here and there but I found this as the easiest and least disruptive approach. Also, there is a warning to the user that there is new yaml version and he/she can consider an update. Also TO DO for later: change cassandra.yaml in conf and test/conf and put cassandra-old.yaml in the test/conf....Should update also CHANGES.txt and NEWS.txt at some point when I am done...

Still have to complete some parts of the other stuff before sharing. For example, still wondering a bit about the best place to incorporate the parsing(also the unit converts...) to ensure least disruptions in the current code logic. Looks to me that it might be good again to call it during the yaml file load. Should think about it a bit more.   

Any constructive feedback is always appreciated. 



;;;","24/Mar/20 17:12;dcapwell;https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-2#diff-b66584c9ce7b64019b5db5a531deeda1R189

I prefer native_transport_max_threads over max_native_transport_threads, the native_transport prefix scopes things better IMO and matches the other configs as well (such as native_transport_flush_in_batches_legacy).

https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-2#diff-5749c5e00813f3ae16b68ff8de095cc0R238

I feel it makes sense to put that in the if block https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-2#diff-5749c5e00813f3ae16b68ff8de095cc0R227 and remove isOldYAML

https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-2#diff-5749c5e00813f3ae16b68ff8de095cc0R305

log isn't actionable, personally feel logging which properties were used but deprecated is easier to act on (so may be better to log per property?)

For the parsing remapping logic, I am good with this, it keeps Config.java clean and removes the need for DatabaseDescriptor to do the remapping.  My main question is how this works with a nested property?  Lets say we want to change the yaml to

{code}
seed_provider:
  # renamed class_name to name
  - name: org.apache.cassandra.locator.SimpleSeedProvider
     parameters:
      - seeds: ""127.0.0.1:7000""
{code}

Not sure if we care about something like that now, but its the main question I have on the remapping (and potential conflict where top level != nested).


bq.  For example, still wondering a bit about the best place to incorporate the parsing(also the unit converts...) to ensure least disruptions in the current code logic. Looks to me that it might be good again to call it during the yaml file load.

I personally prefer the parsing in the file loading since it can give more meaningful feedback to users than failing somewhere else.  Also would mean that Config.java always has the final results;;;","24/Mar/20 22:25;e.dimitrova;Hi [~dcapwell]
Thanks a lot for your feedback!
           ""_https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-2#diff-b66584c9ce7b64019b5db5a531deeda1R189_
           _I prefer native_transport_max_threads over max_native_transport_threads, the native_transport prefix scopes things better IMO and matches the other               configs as well (such as native_transport_flush_in_batches_legacy)._""

I agree with this. On the other hand, we keep all max/min/etc at the end/start of a name, that was the initial plan I think.

      ""_log isn't actionable, personally feel logging which properties were used but deprecated is easier to act on (so may be better to log per property?)_""

I was thinking about this, too. But in this case we don't change only one or two parameters but 15 for now. And when I also add the change of those which had a unit at the back... they will be a lot. Also, my point was that we change completely the version of the yaml. Did you check it how it looks like now?
So from now on any changes/comments/recommendations will go there. We don't want people to start making changes on the old yaml but potentially to update it with the new one.  

     ""_For the parsing remapping logic, I am good with this, it keeps Config.java clean and removes the need for DatabaseDescriptor to do the remapping._
_Yes, that was exactly my point. Also, we can easily get rid of this backward compatibility one day in the future if we want to._

   _My main question is how this works with a nested property? Lets say we want to change the yaml to_
_seed_provider:_
  _# renamed class_name to name_
  _- name: org.apache.cassandra.locator.SimpleSeedProvider_
     _parameters:_
     _ - seeds: ""127.0.0.1:7000""_
_Not sure if we care about something like that now, but its the main question I have on the remapping (and potential conflict where top level != nested)._""

We don't have that issue now but it looks like snakeyaml has everything in place in order to deal with nested parameters, too. Not sure I responded to your question.

        ""_I personally prefer the parsing in the file loading since it can give more meaningful feedback to users than failing somewhere else. Also would mean that   Config.java always has the final results_""

Same here, thanks. 



;;;","24/Mar/20 23:43;dcapwell;bq. I agree with this. On the other hand, we keep all max/min/etc at the end/start of a name, that was the initial plan I think.

naming is hard, ill defer to others who have stronger opinions

bq. they will be a lot. 

yep, which is why it would be good to tell the users which ones are deprecated and replaced with what.  If we just say ""there is a problem"" they will find out what the problem was in 5.0 (and only then try to figure out how to migrate).

bq. Also, my point was that we change completely the version of the yaml. 

I saw you have v2 logic, but don't understand why given the current patch; is there more pending which change the structure?

bq. Did you check it how it looks like now?

Do you mean the YAML?  the log looks the [same|https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-2#diff-5749c5e00813f3ae16b68ff8de095cc0R305] to me.  I see you have the two yaml; not sure about others but IMO we should only have 1 file (since users see it), two could be confusing.

bq. We don't have that issue now but it looks like snakeyaml has everything in place in order to deal with nested parameters, too. Not sure I responded to your question.

the current patch doesn't have the need, was just thinking about if it came up and the impact to the rename logic; the patch currently makes sense for top level names.;;;","25/Mar/20 00:31;e.dimitrova;I mentioned in the initial submission... I propose the new version. Cassandra should come only with one version, the new one.  
But the old one was kept for now for the backward compatibilities CI run and verification. 
The idea is to have the new one v2. 
The old one won't be there anymore. I can leave it only in test/conf for test purposes.
So that's why I didn't consider just to list all the parameters and say rename so they rename them in the old version.
v2 logic I mentioned before that I am gonna add it and no one objected. 

This is initial version to get some feedback while  completing the rest of the stuff. Thank you for providing it.
;;;","25/Mar/20 00:35;e.dimitrova;And I forgot....yes, nested parameter backward compatibility does not exist in the current change as we don't need it at this point.
But I think (speculating as I didn't do it but it looks like that) should be easy to be added in case we need it at some point. Would be added on per case basis.;;;","25/Mar/20 00:50;dcapwell;bq. The old one won't be there anymore. I can leave it only in test/conf for test purposes.

sounds good to me.;;;","25/Mar/20 15:55;e.dimitrova;The branch is not compiling after I made some changes by mistake.
I'll fix this later ;;;","05/Apr/20 22:04;e.dimitrova;Part 1 was fixed and CI run showed no issues, forgot to update the ticket. Part II(capability for the users to pickup units) and Part III(the system properties accessors) will have to pass CI run tonight. 
I am currently using the ByteUnit approach (referring to the DatabaseDescriptor) for the part II conversion of space and rate units. Please let me know if I am missing some other utility that could be used better for this goal.;;;","17/Apr/20 22:42;e.dimitrova;Done but with one caveat which was caught thankfully with the DTests. Unit tests were even on 100% green at some runs. (And my unit test was not covering the problem conversion explained below, I should add it!)
 I am new to snakeyaml so I might be wrong, I would appreciate if someone who has the experience proves me wrong.
 From the DTest failures I realized that turning into Strings the values from the YAML we hit one issue. To me it looks like we already have it actually with Integer and String parameters, just they are not many so it didn't bite us up to now. Not sure whether this was considered when the Config class was built or not.

So the issue with int parameters in Config in my solution is:
 Currently in trunk, when we parse to int variable, If a parameter is commented in the cassandra.yaml, this is observed as null and it will raise an exception in some cases. (i.e. commitlog_sync_period_in_ms) But if it is not commented, just not assigned a value, then this is considered as 0 (as we parse to int). 
 The situation is different with String, Integer. They will be in both cases null. No differentiator. But currently we don't have many parameters being Integer and String and this wasn't a problem.
 Now with the units attached to the parameters' values in the cassandra.yaml file, we need to parse strings with snakeyaml and then convert to the proper int values in Config.

A way to cope with this issue is to make the users use """". In this way, if we have a parameter not commented, but not also having an assigned value in cassandra.yaml, we can identify it by checking a string for being empty. If the parameter is commented or not presented, it will be null. But this would be one more new thing for the users to consider. That's why I came back to snakeyaml again and tried to think of a different way utilizing it. Unfortunately, up to now I didn't find any different solution. Also, the links to the examples in the official snakeyaml documentation are broken. Not sure whether there they had some interesting stuff. Maybe something with customized constructor but also, I try to make as less as possible disruptive changes in the codebase in order not to mess up something with the Config which might be extremely unpleasant.

I will be happy to hear an experienced opinion here. Does anyone know a better solution? Or are we ok to introduce """" in the yaml for empty parameters/Strings recognition?

[Part1|https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-2] (This part was already revised by [~dcapwell] but I paste the explanation again here for completeness).
 - New cassandra.yaml ---> currently cassandra-new.yaml in the branch as I wanted to test the backward compatibility on the old version (will have to rename properly also these yaml files with version probably)
 reorganization of the file renames to make the names more consistent
 - Backward compatibility with the old names - my approach is:
 Cassandra comes with the new version of the yaml to ensure any new builds will be using it
 During load of the yaml file I check whether old names are identified(means someone upgraded to 4 and wants to still use the old yaml). If this is the case, we load the values from the old yaml to the new variables (the already renamed parameters). Also, there is a warning to the user that there is new cassandra.yaml version and he/she can consider an update.

[Part2|https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-3] - units added to the values in the cassandra.yaml file. Parsing and unit conversion in place. One unit test added, should be further developed.

[DTests|https://github.com/ekaterinadimitrova2/cassandra-dtest] updated accordingly for v.4

*EDIT:* Part3 has been moved to a separate ticket - CASSANDRA-15876.

-[Part3|https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-5] - SysProperties class, accessors, hope I didn't miss anything... most of them are actually just using System.getProperty method.-
 TO DO: Extract the parser/converter in a separate class from Config

[~mck]? [~dcapwell]? [~benedict]? Anyone else any thoughts here?

NOTE: The branches are not fully cleaned, this is still work in progress but we need to take a decision on the String issue first, I think.;;;","21/Apr/20 21:48;dcapwell;Most of my experience with SnakeYaml is as a plugin to Jackson, normally Jackson is better at converting JSON/YAML -> Object and offers more flexibility; I may have misread you but the ""int"" null case isn't normally an issue in Jackson (assuming you define defaults).  If this is a real limitation, wonder if it's better to use Jackson as the interface with SnakeYaml as the parser?

I skimmed part 2 quickly.  What I see is you offer two fields: string, and parsed value.  If the user provides the string version we attempt to parse it and update the parsed value.  In order to add a new field we would need to define the string and parsed versions, and provide the mapping to the MEM_UNITS_MAP map; my first thought is ""why not make this a type?""

I hope this example is clear to my thinking

{code}
class Memory {
  long bytes;
  public Memory(String value) {
    this.bytes = parse(value);
  }
  

  public long getBytes() { return bytes};
  public long getKillaBytes() { return bytes * 1024 };
 ...
}
{code}

{code}
Memory max_value_size = Memory.ofMb(256);
{code}

I quickly prototyped this for the field I showed and it parses just fine, also has the benefit that we drop a lot of unit conversions since config spec != runtime value (config is mb, runtime is bytes).

My thinking is having types rather than the primitives is easier to read, easier to work with, easier to use with properties, etc; it also means we don't need to maintain the mapping from string to parsed fields.

The other benefit is it plays nicely with part 1.  We only have a single field, and if the user defines the new and the old the logic for which is used is centralized in a single place.

Ill start reading part 3 now.;;;","21/Apr/20 22:16;dcapwell;Part 3 appears to be a mix of com.google.common.base.StandardSystemProperty, JMX properties, and 3 Cassandra properties (focus on starting up; I know this is showing the idea, so totally fine with this); how I read the class is as the DatabaseDescriptor for system properties/env, is this the thinking?

I kinda feel that it would be best to split this in 3

1) guava's StandardSystemProperty, if there is anything missing, see #2
2) JavaProperties (not tied to a name, just a placeholder).  This stores the JMX and anything not in StandardSystemProperty
3) SysProperties.  This manages Cassandra specific configs.

Im not tied to that so flexible if people only want one class to rule them all...

For part 3 I am more curious on how this ties into part 1 and 2; prototype or small informal writeup (1-2 sentences, just want to know your thinking) would be good.;;;","22/Apr/20 00:03;e.dimitrova;@David, thanks for the review and the clarifications and Slack constructive discussion!

So looks like we have the same vision and I missed one thing. Let me wrap up here for the record and [~mck] who is also looking into it.
 Part2 - null is a legit issue and no one of us likes the idea of adding """" in the yaml file. Still have to spend time to think about a workaround and that we already have it there.

The Memory class is kind of what I meant when I said I have to pull the current implementation separately.
 So the suggestion is to pull into three additional classes - Memory, Throughput and TimeDuration (or something similar)

The confusion was about a thing I saw and forgot to take care. :\
 There are 11 getters in the DatabaseDescriptor. 
 They need to be changed to only getters without doing one more layer of conversion to bytes. Example:
 getMaxValueSizeInBytes()

{ return maxValue.toBytes(); }

-> Memory getMaxValueSize()

{ return maxValue}

This will clean the code and make it easy for maintenance.

*EDIT:* Part3 has been moved to a separate ticket - CASSANDRA-15876.

-Part 3 - code-wise not related to Part1 and 2, just additional small task which was requested in the previous comments, considering we are trying to do standardization as part of this ticket.- 
 -Whether we keep all of them in one class or we split is a matter of agreement I think. As soon as we get it, I will correct it. This is real quick.-;;;","22/Apr/20 08:22;mck;bq. Part2 - null is a legit issue and no one of us likes the idea of adding """" in the yaml file. Still have to spend time to think about a workaround and that we already have it there.

Some properties are nullable numbers, ie null is be a valid value. I would rather not permit/encourage the use of magic numbers (eg -1, or MIN_VALUE or MAX_VALUE) to notate the value as being undefined.

An example is {{allocate_tokens_for_local_replication_factor}}. Its use as a nullable is in [BootStrapper.getBootstrapTokens(..)|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/dht/BootStrapper.java#L157-L179]  

Is this problem restricted to the parsing of deprecated (old yaml) properties? How many of the properties that are getting renamed were nullables?;;;","22/Apr/20 18:27;mck;After a chat with [~e.dimitrova] I understand the null issue a bit better now.

In the current codebase the {{int}} properties can not be left blank, but it can be commented out. 
- A commented value means a default of {{0}} (though it will also report an error in system.log). 
- A blank {{int}} property throws an exception.

In this patch some of these {{int}} properties will become {{String}} properties. For example {{commitlog_sync_period_in_ms}} will become {{commitlog_sync_period}}, changing its type from {{int}} to {{String}}. The values will then permit unit suffixes, eg {{100ms}} and {{3s}}.

These values still need to support the existing behaviour: blank throws an exception and commented out means zero. This is not possible with snakeyaml as, for {{String}} properties, both commented out and blank get interpreted as {{null}}.

A suggestion to differentiate between a commented out {{String}} yaml property versus a blank {{String}} yaml property was to add a simple {{isPropertyBlank(name)}} method that performs a regular expression match against the plaintext of the yaml file. It is simple because the parsing of units suffix and {{int}} prefix happens in the C* code. The regexp match would then be: {{""^"" + property + "" *: *$""}};;;","24/Apr/20 04:13;e.dimitrova;Latest update:
 Part1
 - format issues fixed
 - The isOldYAML logger info changed to warn, and it lists also the old properties used
 - The format of yaml props is noun_verb. native_transport props adjusted to follow this. Max sent to the back as in native_transport_threads_max.
 - MAP_OLD_TO_NEW_PARAMETERS is not static anymore(so it is freed memory after startup)

Part2 - will be completed in the suggested way probably tomorrow (working asynchronously)

*EDIT:* Part3 has been moved to a separate ticket - CASSANDRA-15876.

-Part3 - anyone any thoughts whether it should be 1 class or maybe divided into 3 classes, as suggested by- [~dcapwell]-?-;;;","28/Apr/20 18:56;mck;bq. Part3 - anyone any thoughts whether it should be 1 class or maybe divided into 3 classes, as suggested by David Capwell?

I'm ok with one class.

But it would be nice to list the properties and environments used in enums, so it is quick to list/read what Cassandra accesses. For example {{SystemPropertiesTable.java}} should be not be defining its own enums nor the string literals to the properties and environments.

Otherwise part 3 looks good. What test coverage is there (or will there be) here?
;;;","28/Apr/20 19:21;dcapwell;I am ok with enums;;;","30/May/20 16:17;jmckenzie;Where do we sit with this? One of our few beta 1 blockers. Thanks!;;;","31/May/20 00:22;e.dimitrova;99% done.

I already took care of all the things requested in the initial review. A couple of things we agreed on offline.

Spent some time on the testing as all Dtests and CCM required edits because of all the renaming of parameters. 

Will be done for final review Monday. (running some final tests on my end after edit);;;","02/Jun/20 03:25;e.dimitrova;I think that with my CCM patch all dtests should be fine now but I didn't manage to run them with my CCM patch in CircleCI. I was advised to change only the repo in requirements.txt but configuration errors appear and I can't run the tests.

[~mck], is there a way this to be done in Jenkins?

Otherwise I can try locally but this will take at least half a day to run the dtests... 

[~dcapwell] [~mck] if you have any ideas how to deal with this task efficiently will be appreciated.;;;","02/Jun/20 09:59;mck;bq. Michael Semb Wever, is there a way this to be done in Jenkins?

Currently the CCM repository is not a parameter to the Cassandra-devbranch pipeline.

So I can only recommend the same approach, by updating the repo url in the cassandra-dtests/requirements.txt and starting the devbranch pipeline off against your patched dtest repo.;;;","02/Jun/20 12:56;e.dimitrova;Thanks [~mck], I will do the testing locally then and attach the logs here. ;;;","02/Jun/20 13:38;mck;[~e.dimitrova], looks like you'll need the fix [~snazy] added [here|https://github.com/apache/cassandra-builds/commit/bf6bc98085330172f323c27b21810ac0eb0cc732].;;;","03/Jun/20 02:20;e.dimitrova;Thanks [~mck] and [~snazy], it worked.

pip3 install was provided with --upgrade option in the  circleci config file. 

Final clean of the code tomorrow morning. 

 ;;;","04/Jun/20 05:40;e.dimitrova;[ [Part 2 |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-3] ] [ [DTests |https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15234-3] ] [ [CCM |https://github.com/ekaterinadimitrova2/ccm/tree/CASSANDRA-15234-3] ] [ [CI run |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/187/workflows/615cdebf-1d49-47ab-93a4-efad25a247b1] ]

 - Unit tests updated

- Getters and Setters names in the DatabaseDescriptor were updated to follow the new names of the parameters updated
 - The conversion of some of the parameters values which is happening in their getters is kept as it is needed in runtime. Some parameters are used with different units in the different areas of the codebase. So I left this part untouched.

 - New class created, ParseAndConvertUnits - it pulls all the functionality needed to cover the addition of units suffixes to the Memory, Duration, and Rate parameters in the cassandra.yaml

- the cassandra.yaml file in this branch contains currently the new parameters with new names and unit suffixes but the arrangement of the sections is the old one as this was causing me issues every time I rebase/commit. If we confirm the format, this will be the last step to switch to the new format

- Added some additional parsing and supporting conversion functions for the double duration parameters.

- The 2 failing cqlsh tests were fixed locally and tested. It was formatting issue. 

-1 unit test failed - it doesn't look a related issue but will check the config loaded to ensure that tomorrow

- 8 failing dtest, it doesn't look related but I am gonna compare tomorrow morning

 - Current version of the code needs a final rebase/merge of the last 6 commits on trunk.  Will do tomorrow after I recheck the CI and current version

- Have to return the default requirements.txt and config.yml files. The current versions were required only for testing purposes with CircleCI

*WARNING:* we should be careful with the dtests and ccm. One missed check for old/new parameter and default value of the respective parameter from Config will be loaded. This might not lead to failing test and thus silently cover the miss.

*NOTE:* Custom Types, as they were suggested, don't really work with SnakeYAML, unfortunately.  SnakeYAML supports nothing more than mapping between the yaml and the Config class.
 * _So something like that: Memory max_value = new Memory(""5ms"") wouldn't work. If there is no value in the yaml, the config default  max_value will be parsed properly but if there is value assigned to max_value at the yaml, it won't be assigned properly in Config. Classes are used for nested parameters. Also, currently there won't be a big benefit of the custom types as the different parameters validations in the DatabaseDescriptor are taking different approaches, there is no general framework/approach per group of parameters (Memory, Duration, Rate)._

_---------_

_*EDIT:* Part3 has been moved to a separate ticket - CASSANDRA-15876._

-The last part was updated as per the recommendations. CI ran successfully. Need to rebase and recheck nothing that might need the attention of this patch has changed in the meanwhile. Three weeks passed since I took care of this one.- 
 -[ [Part3 |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-5] ] [  [CI JAVA8 |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/146/workflows/fae5da98-acf5-4824-bf61-3ffab3d86cd6] ] [ [CI Java11 |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/146/workflows/21b088c4-8984-4b20-8302-2ff8adaf55f7] ]-;;;","04/Jun/20 18:36;e.dimitrova;*UPDATE:* 
- the failing unit test is a flakey test. The configuration is properly loaded so it is not caused by this patch.
- small issue with Auth_test.py was fixed.
- the rest of the failing DTests are related to current failures we already see on trunk. Some of them are already fixed in the newer commits. Rebase and run new CI now to confirm the latest status.;;;","05/Jun/20 03:35;e.dimitrova;[ [trunk |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-3-rebase] ] [ [DTests |https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15234-3] ] [ [CCM |https://github.com/ekaterinadimitrova2/ccm/tree/CASSANDRA-15234-3] ] [ [JAVA8 |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/193/workflows/718c8a04-385b-4ddc-ac2e-9291c37b1f1e] ] [ [JAVA11 |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/193/workflows/6cd6048d-5daf-4490-a5d3-b5c3178a0757] ]

Attached to the ticket is the log of the successful runs of the few failing JAVA8 DTests in CircleCI. I ran them locally after fixing lost line of code as it didn't make sense to me to rerun the whole suite of tests.

*Last update:* after rebase and a few nits on my end, we have only 1 DTest and 1 Unit test failing in JAVA11. Unrelated known failures.
 - added additional note on the possible units to be used as suffixes in both NEWS.txt and cassandra.yaml

CQLSH JAVA11 tests are not currently available in CircleCI.
----
*EDIT:* This work has been moved to a separate ticket - CASSANDRA-15876.

-The last part is all green after rebase and CI run.-
 - -""line.separator"" getter added-
 -[ [trunk |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-5] ] [ [JAVA8 CI |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/195/workflows/ebd771ac-d71c-49ea-872b-00b534d9d5c6] ] [ [JAVA11 CI |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/195/workflows/19a3254b-cd7a-4e4f-856f-f314546ff9d7] ]-

-There are 1 DTest and 1 Unit unrelated failing tests in JAVA11. Should check for tickets logged.-

*NOTE:* Before commit return to the default config.yml and requirements.txt files;;;","05/Jun/20 14:24;e.dimitrova;This one is ready for final review I think.
[~mck] I am not a committer but I think it would be great also to run the tests in Jenkins. Thank you;;;","05/Jun/20 17:41;e.dimitrova;*Summary:*
 This branch contains:
 - [first commit |https://github.com/ekaterinadimitrova2/cassandra/commit/a8a9145a032a9f2d9b74369f2e4474a210aa5b99] which contains parameters name changes as requested; backward compatibility with the old names; Proposed new structure into sections for cassandra.yaml.
 - the [second commit |https://github.com/ekaterinadimitrova2/cassandra/commit/032fbd8cca1d7b5b96738bd96d9672864426d718] implements the functionality which gives the end users the opportunity to attach units suffixes to Memory, Duration, Rates parameters.
 - [third commit |https://github.com/ekaterinadimitrova2/cassandra/commit/68f89503e6e6f30e8f6df1374cb553f243f0068e] is a ninja fix of one line which was changed in a wrong way during the latest rebase.
 - [fourth commit |https://github.com/ekaterinadimitrova2/cassandra/commit/8389010d704a9524a1c36d96f9e4d8179a8e2044] is to return the default config.yml

This patch requires custom [ [DTests |https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15234-3] ] which run successfully with the following patch of [CCM |https://github.com/ekaterinadimitrova2/ccm/tree/CASSANDRA-15234-3]
 DTests - requirements.txt should be updated back to the original one before commit!

The DTests are not backward compatible with the yaml file without units suffixes.
 This part is covered by a unit test which tests the successful load of the values of the parameters without units as per the old format.

[ [trunk |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-3-rebase] ] [ [DTests |https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15234-3] ] [ [CCM |https://github.com/ekaterinadimitrova2/ccm/tree/CASSANDRA-15234-3] ] [ [JAVA8 |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/193/workflows/718c8a04-385b-4ddc-ac2e-9291c37b1f1e] ] [ [JAVA11 |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/193/workflows/6cd6048d-5daf-4490-a5d3-b5c3178a0757] ]

Attached to the ticket is the log of the successful runs of the few failing JAVA8 DTests in CircleCI. I ran them locally after fixing lost line of code as it didn't make sense to me to rerun the whole suite of tests.

*Last update:* after rebase and a few nits on my end, we have only 1 DTest and 1 Unit test failing in JAVA11. Unrelated known failures.
 CQLSH JAVA11 tests are not currently available in CircleCI.

*WARNING:* Before commit return to the default requirements.txt file

*Order of commits:* 1) [ [CASSANDRA branch |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-3-rebase] ] 2) [ [CCM branch |https://github.com/ekaterinadimitrova2/ccm/tree/CASSANDRA-15234-3] ] 3) [ [DTests branch |https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15234-3] ]
----
*EDIT:* This work has been moved to a separate ticket - CASSANDRA-15876.

-This [branch |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-5] is a class of Accessors for System properties. *This patch could be considered separate of the previous one, it doesn't matter* *if it is committed* *first or second.* It also doesn't require custom DTests or custom CCM.-
 -[ [trunk |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-5] ] [ [JAVA8 CI |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/195/workflows/ebd771ac-d71c-49ea-872b-00b534d9d5c6] ] [ [JAVA11 CI |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/195/workflows/19a3254b-cd7a-4e4f-856f-f314546ff9d7] ]-

-There are 1 DTest and 1 Unit unrelated failing tests in JAVA11. Should check for tickets logged.-;;;","05/Jun/20 18:19;mck;Thanks for the summary post [~e.dimitrova].

For the first [part|https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-3-rebase] (including the dtest and ccm changes), the ASF CI pipeline results are [here|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/144/]. In addition the ASF CI dtest upgrade test results are [here|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch-dtest-upgrade/10/].

For the second [part|https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-5] the ASF CI results are [here|https://ci-cassandra.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/145/pipeline].;;;","08/Jun/20 03:56;e.dimitrova;Thanks to a recent patch I managed to run the upgrade tests in CircleCI. 

The [results |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/207/workflows/1d0a0a87-16d9-467b-9299-bb148cb10f68/jobs/1223] look good(more than 700 failures but they are those which are currently presented on trunk in CircleCI and mostly related to the environment) when we use the old current DTest suite, CCM and cassandra.yaml. Using the old cassandra.yaml is actually the way a real upgrade will happen.

In order to run the upgrade tests using the newly proposed version of the DTests suite and CCM, I need to take care about a couple of places where the suite checks for Cassandra version and tries respectively to load some of the parameters with their new names which causes cluster start issues, preventing some of the upgrade tests from run. 

Currently identified the primary conflict to be in dtest_setup.py. After that scripts is adjusted there are only around 20 tests which fail [here |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/206/workflows/1f7b0a54-c9e2-4305-8c94-be08d07846dd/jobs/1217/tests].  One of the other issues is the new name _materialized_views_enabled due_ {color:#172b4d}to which some of the cql tests fail.{color}

Quick intermediate solution to the mentioned problems would be to add a script which:

1) runs before the upgrade tests start

2) replaces the new  cassandra.yaml file with the old one

3) sets the names of the parameters which would cause issues to their old names which as far as I see are 6-7 compared to almost 70 changed names according to this patch.

Then I can open a follow-up ticket for fix of the tests so we don't use this customization in the long term but I find this current issue as not being a blocker for the patch or Beta. 

*TO DO:* Update the Upgrade documentation with the steps of upgrading to version 4 with the old cassandra.yaml and then updating the cassandra.yaml.

 ;;;","15/Jun/20 19:58;mck;[~e.dimitrova], can we pull out the part 3 (Accessors for System properties) patch into a separate ticket.  If it has no dependencies on the other patches in this ticket, then there's no reason it can't get reviewed and committed separately.;;;","15/Jun/20 21:57;e.dimitrova;Thank you [~mck]

I agree with you, CASSANDRA-15876 was created to facilitate this work. It is also not a  4.0 Beta blocker(I set the fix version to beta there) and I think we might be even able to further develop it. Let's continue the discussion there. ;;;","18/Jun/20 00:49;dcapwell;This isn’t a review and I’m aware I said I didn’t have time to review it a month ago, but I glanced at the patch and it seems quite different from the approach discussed in https://the-asf.slack.com/archives/DSP76MVPC/p1589983705004900 and https://the-asf.slack.com/archives/DSP76MVPC/p1590006263021200. 

Can you say a bit about why you changed your approach?;;;","18/Jun/20 16:49;e.dimitrova;[~dcapwell], I am returning this to ""in progress"" as things have changed and to prevent people from confusion. Your concerns are valid and addressed. Change has happened because of some misunderstanding on my end but it was corrected already. New submission to follow soon:
 * custom types (as per the initial agreement)
 * annotations as suggested by you for easier maintenance
 * backward compatibility will be on property level instead of yaml file level

New branch will be shared soon. Thank you!;;;","25/Jun/20 07:25;e.dimitrova;This patch implements the functionality which gives the end users the opportunity to attach units suffixes to DataStorage, Duration, BitRates parameters. New custom types for the three group of parameters were added.

It renames certain parameters with main goal - standardization. 

Backward compatibility implemented on parameter level utilizing annotations.

[Branch |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-new]

This patch requires custom [ [DTests |https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15234-new]] which run successfully with the following patch of [CCM |https://github.com/ekaterinadimitrova2/ccm/tree/CASSANDRA-15234-new]

(The failed test in circle are failing because the CCM patch didn't apply due to config issue. Also, one exclusion of warning about using the old parameters names should be added. Upgrade tests still running)

DTests - requirements.txt should be updated back to the original one before commit!

[ [JAVA8 |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/257/workflows/98c356fb-b16a-4508-bda8-ff877569b0f5]] [ [JAVA11 |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/257/workflows/d49295a8-d8f5-4836-a18a-df25ff955219]]

*TO DO:* IN-JVM tests are not using snakeyaml so I have to update them to use its converter in order for the tests to work with this patch. This is feasible solution aligned with Alex Petrov and [~dcapwell].

I will provide the additional patch/commit until the end of the week but didn't want to delay the review of the main patch further.

*WARNING:* Before commit return to the default requirements.txt file

*Order of commits:* 1) [ [CASSANDRA branch |https://github.com/ekaterinadimitrova2/cassandra/tree/CASSANDRA-15234-new]] 2) [ [CCM branch |https://github.com/ekaterinadimitrova2/ccm/tree/CASSANDRA-15234-new]] 3) [ [DTests branch |https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15234-new]];;;","25/Jun/20 17:11;dcapwell;This is a large patch, so will take a few attempts to review; below is my first pass (mostly skim).

* Config. otc_coalescing_window_us_default was not renamed but the other otc* configs were.  https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-new#diff-b66584c9ce7b64019b5db5a531deeda1R429
* block_for_peers_timeout_in_secs and block_for_peers_in_remote_dcs are exposed in yaml, the comment says ""No need of unit conversion as this parameter is not exposed in the yaml file"", can you explain why this comment?
*  https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-new#diff-76c34368f112a0a4e6da5b22c8b50300R353-R376. enums can use ""abstract"" methods, so you can do that rather than use a default which throws
* in database descriptor I see you use toString to compare, why not convert to the expected unit?  https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-new#diff-a8a9935b164cd23da473fd45784fd1ddR381. this could be {code}if (conf.commitlog_sync_period.toMillis() != 0){code}.  This is true for all examples I see in this method
* we should add docs for Replaces and ReplacesList (also calling out that you shouldn't use ReplacesList directly)
* for the new types, would be good to have a property test which validates that {code}type(value, unit).equals(type.parse(type(value, unit).toString())){code}.
* LoadOldYAMLBackwardCompatibilityTest and ParseAndConvertUnitsTest look to be copy/paste.  junit supports inheritance so could remove the copy based by having LoadOldYAMLBackwardCompatibilityTest extend ParseAndConvertUnitsTest but use the other config.


Overall I am good with this patch, need to do a closer review of the details still.;;;","25/Jun/20 19:10;e.dimitrova;Thank you [~dcapwell]!
 Please find my comments below in Italic. I will also make a pass later again as I admit I was in a hurry to roll it out last night so it can be visible for people in more time zones before the end of the week to express any bigger concerns that I can address as this is a beta blocker.
 * Config. otc_coalescing_window_us_default was not renamed but the other otc* configs were. --> _will be corrected_
 * block_for_peers_timeout_in_secs and block_for_peers_in_remote_dcs are exposed in yaml, the comment says ""No need of unit conversion as this parameter is not exposed in the yaml file"", can you explain why this comment? --> _it is not exposed in the yaml for the users so default value and unit is used as per the Config class setup_
 * in database descriptor I see you use toString to compare, why not convert to the expected unit --> _I was testing and forgot to correct it, will make a pass to ensure it is corrected everywhere, thanks!_
 * we should add docs for Replaces and ReplacesList (also calling out that you shouldn't use ReplacesList directly) --> _noted_
 * for the new types, would be good to have a property test which validates that
{code:java}
type(value, unit).equals(type.parse(type(value, unit).toString())){code}
--> _noted_
 * LoadOldYAMLBackwardCompatibilityTest and ParseAndConvertUnitsTest look to be copy/paste. junit supports inheritance so could remove the copy based by having LoadOldYAMLBackwardCompatibilityTest extend ParseAndConvertUnitsTest but use the other config --> _will definitely take care of that, left over from the previous version, apologize_;;;","28/Jun/20 16:35;mck;I've added some (very minor) comments on the [commit|https://github.com/ekaterinadimitrova2/cassandra/commit/f7ed1e00db5cc5810779a3d71dc065c750ba8e6b]. 

Higher level comments (while testing)…
 - during startup warnings about legacy yaml names are logged multiple times…
 - {{""it is scheduled to be removed by 5.0""}} would be more precise as {{""the old name is scheduled to be removed by 5.0""}} (as opposed to the setting+ value being removed),
 - if I define setting  to a blank value, I get a error message like: {{""Invalid yaml. Those properties [max_hint_window] are not valid""}}.  This falsely tells me thtat the setting name is wrong, rather than the value. (Invalid non-blank values provide useful error messages.)
 - integer overflow could have a better error message, currently {{""NumberFormatException: For input string: ""9223372036854775808""""}}
 - the under/over flow protection between units is nice. {{Duration.toMilliseconds()}}  apidoc also describe the overflow to  MAX_VALUE. (same goes for similar default-unit methods in BitsRate and DataStorage)
 - awesome that you can use {{µs}} :-);;;","28/Jun/20 20:41;benedict;> it is scheduled to be removed by 5.0

Do we need to remove the legacy parameters if they continue to mean the same thing?  I think we should generally defer breaking deprecated public APIs until it's useful or necessary.  We're doing this to cleanup for ourselves and for future users, not to make existing users have a bad time.
;;;","30/Jun/20 08:29;blerer;{quote}Do we need to remove the legacy parameters if they continue to mean the same thing? I think we should generally defer breaking deprecated public APIs until it's useful or necessary. We're doing this to cleanup for ourselves and for future users, not to make existing users have a bad time.
{quote}
[~benedict] Do you want to keep the backward compatibility forever or do you think that 5.0 is too soon to remove it?;;;","30/Jun/20 08:36;benedict;My personal view is that for configuration parameters, so long as it is costless to maintain backwards compatibility we should do so.  This includes parameters we remove between versions because we no longer use them (unless there is some semantic implication that the operator needs to consider).  So, yes, indefinitely (but also with continued logging that the parameter is deprecated and might be removed).

I think the same should be true of all of our public APIs: deprecate immediately, but only remove once the cost to maintain becomes unbearable.;;;","30/Jun/20 09:04;mck;bq. I think the same should be true of all of our public APIs: deprecate immediately, but only remove once the cost to maintain becomes unbearable.

Is there a problem with advertising that the deprecation only lasts one major version, while we uphold that^ principle internally? 

i.e. we not making any commitment to the ""scheduled to be removed by"" statement, it's just a warning. ;;;","30/Jun/20 09:15;blerer;We could change the message to something like ""will be removed in a futur major version"". We can then decided later on when we wish to remove it.

The JDK used some warning like that if I am not mistaken.;;;","30/Jun/20 17:33;dcapwell;Given the framework provided by this patch, the following wouldn't be hard to support (all, not just 1 or 2)

1) no warning or plan to remove
2) warning that it will be removed some day
3) warning on specific version which will remove

So, we could do the following

{code}
// provide a warning that this will not longer be supported after 5.0
@Replaces(oldName = ""native_transport_idle_timeout_in_ms"", scheduledRemoveBy = ""5.0"")
public volatile Duration native_transport_idle_timeout = new Duration(""0ms"");

// provide a warning that the property is deprecated and will be removed one day
@Replaces(oldName = ""native_transport_idle_timeout_in_ms"", deprecated = true)
public volatile Duration native_transport_idle_timeout = new Duration(""0ms"");

// no warning, both properties are fully supported
@Replaces(oldName = ""native_transport_idle_timeout_in_ms"")
public volatile Duration native_transport_idle_timeout = new Duration(""0ms"");
{code}

Given the framework makes it trivial to support old names, having no properties marked for removal of 5.0 works for me. If we really want to migrate usage to a new name, then mark it to be removed one day, and stuff which is personal preference (such as enable at the start or end of the name) can have no warning; does this make sense?;;;","30/Jun/20 17:39;benedict;bq. Given the framework makes it trivial to support old names, having no properties marked for removal of 5.0 works for me

+1;;;","02/Jul/20 12:01;blerer;Some comments on the current patch:

* I would prefer to avoid reading twice the configuration file if possible. It should be possible to replace {{YamlConfigurationLoader.readStorageConfig}} and {{isConfigFileValid}} by some checks in {{getProperty}} or by using the specific converters. As a side note: {{isConfigFileValid}} checks twice for the same condition on each of the {{if}} conditions.
* The patch can be simplified by making {{Converter}} an {{enum}}. It will allow to use the enum value directly into the {{Replaces}} annotation. Doing so will remove the need to use reflection to instantiate the conveters and the use of a cache to avoid multiple instantiation.
* The change in {{ConfigurationException}} are from some experimentations and should be removed.
* I would as discussed previously defer the date of removal and remove {{scheduledRemoveBy}} from the {{Replaces}} annotation.
* It would be nice to have more javadoc on the new code.;;;","02/Jul/20 18:38;benedict;I haven't looked closely at this patch, but [~maedhroz] pointed out on another ticket that namespaces are a possibility, and this could dramatically simplify understanding the config file, and improve parameter naming as we try to shoe horn fewer things into a crazy long snake case name.  Has this been considered?

I don't want to muck things up this late in the game, but if we're doing this once, we should consider all the options - and I hope this change would be relatively minor compared to all the work [~e.dimitrova] has done (if we decide it makes sense, even, of course).;;;","02/Jul/20 20:55;maedhroz;{quote}but if we're doing this once, we should consider all the options
{quote}
We tend to follow the example of what has come before. (I fell into that trap initially trying to name a _new_ YAML option in CASSANDRA-15907.) If we can change it now, we can make that basic tendency work _for_ us. Agreed that it's a bit late in the process though.;;;","02/Jul/20 22:38;e.dimitrova;Thanks everyone for making a review/ providing a valuable input/advice.
 
{quote}Has this been considered?
{quote}

No, it wasn't considered. I did the renaming and its backward compatibility as per  [the agreed plan | https://issues.apache.org/jira/browse/CASSANDRA-15234?focusedCommentId=17058024&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17058024] .

For the rest of the audience, I reached out [~maedhroz] in Slack to understand better what is the suggestion. 

The proposed idea is to group parameters in the yaml the way it already happens with the encryption options.

*Example:*

Moving in the yaml from:
{code:java}
     cached_rows_warn_threshold: 1000
     cached_rows_fail_threshold: 16000
{code}
{{to:}}
{code:java}
replica_filtering_protection:
     - cached_rows_warn_threshold: 1000
     - cached_rows_fail_threshold: 16000
{code}
*My personal opinion:*

if this is really last beta blocker I wouldn't go for a new round of rework and full validation. This patch touches already more than 50 parameters and requires backward compatibility. Every change of the framework requires full validation and discovers new edge cases as those parameters touch all over the code.

*My suggestion:* 

Follow the suggestion for new parameters creation. Implement the suggested refactoring in the next major version. 

 ;;;","03/Jul/20 00:02;jmckenzie;{quote}Follow the suggestion for new parameters creation. Implement the suggested refactoring in the next major version.
{quote}
+1. This is one of 3 tickets left before beta and a new round of rework and validation this late in the alpha is inappropriate IMO.

I'm sympathetic to the grouping; I find it superior aesthetically and more comprehensible, however balancing that against the need to get beta out and get people actively testing the DB leaves things in favor of the latter for me.;;;","03/Jul/20 10:00;benedict;If we agree we will be changing the config parameters next version to this alternative approach, then I am fairly opposed to shipping this in its current form and creating more churn for operators when we change it again in the near future.  The point of this exercise is to clean things up, not make them messier, and this includes the perspective of the end user and their ability to keep up with our nomenclature.
 
I also think it would be a real shame not to include this work at all, given how awful and confusing our parameter naming situation is today.

I don't think this is an absolute necessity for pre-beta, however, given that we have agreed for it to be backwards compatible.  I don't see any issue with landing this in a later beta.;;;","04/Jul/20 09:20;mck;bq. I don't think this is an absolute necessity for pre-beta, however, given that we have agreed for it to be backwards compatible. I don't see any issue with landing this in a later beta.

Move it out to a separate ticket, like was done for CASSANDRA-15876. To bring additional spec to the table, no matter how good it is, so late in the ticket and so close to the first beta, is particularly painful. As a separate ticket it can be done during the beta phase. 

Otherwise the settings now have a consistent naming pattern {{<noun>_<verb>}} which, with no unit suffix noise, and better grouping the settings in the yaml, already brings a huge improvement.;;;","04/Jul/20 13:52;benedict;> I am fairly opposed to shipping this in its current form and creating more churn for operators when we change it again in the near future

To be more explicit: I am (binding) -1 knowingly shipping an _interim_ solution for a public API.  It is user unfriendly.

I agree it is suboptimal to bring this to the ticket late in the game, but that is how the world sometimes works.  This was not capricious or deliberate; nobody had thought of or suggested this until now, and everyone has since agreed (or implied) this is a superior approach.

If we change our minds and agree the current approach is the final and superior solution, that is another matter.

The approaching release of 4.0 should not reduce our attention to detail or quality.  We should merge tickets only when we agree they are ready.  This is particularly important for tickets whose main impact is improving a public API.

P.S. I would like to take a moment to thank [~e.dimitrova] for her work on this (and other tickets).  OSS work can be frustrating sometimes, and I'm sorry if this ticket becomes a source of frustration.  It's unfortunately not uncommon to have to revisit work, or have work delayed, but please know that it's valued.  I am excited for the end product; I think it will be a huge improvement when it lands.;;;","04/Jul/20 14:48;jmckenzie;{quote}bq. To be more explicit: I am (binding) -1 knowingly shipping an _interim_ solution for a public API. It is user unfriendly.
{quote}
We as a project need to mindfully balance when something is ""good enough"" and when the trade-off is in favor of getting a release out vs. pursuing another incremental improvement to an API. In my opinion this is classic bike-shedding.

Given a day focused on nothing but thinking of how to improve our config API I firmly believe we could come up with something that a simple majority considered an improvement over what's proposed on this ticket + config grouping in a vacuum. _I personally prefer the config param grouping_, but not when weighed against delaying the beta when it's this been long since a prior release.

I perceive on balance what is in our users' best interests is having a release with all the defect fixes and features available in 4.0. This point of view stems from the empirical evidence of multiple users asking if the project is dead and expressing a strong interest in running 4.0 having heard of the proximity of the beta. I have no evidence of users stating that a lack of configuration grouping in the project causes them equal or greater pain, nor have I seen a cogent argument for that point of view.

The underlying question we face is how we balance the value to end-users of having as optimal of an API as we can possibly come up in a specific domain vs. the value to end-users of the other things that are delayed due to that effort. Your point of view [~benedict] _appears to be_ that delaying the beta and the engagement of the user community's testing of the 4.0 release is justified by the value gained in grouping config params and not having a renamed paradigm for 4.0 and subsequent grouping paradigm in our next major. My apologies if I'm mis-characterizing your point of view here.

It's within any committers prerogative to binding -1 things on justified technical grounds. I'd be curious what your perspective is on how we determine what qualifies as justified [~benedict], [~mck2] , and [~maedhroz] in a context such as this, as you all have expressed points of view here.;;;","04/Jul/20 18:28;mck;I'm struggling to see how a request to break work down into separate tickets and steps constitutes a technical reasoning that should block the work. The valid concern around the api churn it introduces is addressed by committing the new ticket to 4.0-beta.

The more we can do incrementally the better. From reviewing, to testing, to the compartmentalising of jira ticket discussions. (I still miss jira comment threads, even if folk never really used them consistently.)

bq. … everyone has since agreed (or implied) this is a superior approach.

Not on my behalf. And I don't believe all others presumed answers to all questions that need to be asked for it to be a validated proposal. For my part, it is a good suggestion that warrants further investigation. It still needs to be investigated and spec'd out. 

bq. If we change our minds and agree the current approach is the final and superior solution, that is another matter.

Not sure I agree with the onus here, based on this being an un-investigated and un-specified suggestion. The onus I think should be the opposite: those wanting this to be a blocker should do the work in determining the value (investigation) and design (specification) to the suggestion. 

The suggestion is not a fault or bug in this ticket, there is nothing in the suggestion that *breaks* the patch, it is imho rather a subsequent suggested improvement. Even if it was a suggestion that replaced this patch it should be dealt with in the same way. And of course making such a suggestion first here is natural because the author _may_ take it on in-stride, but it should be the author's prerogative to request it be ""the next step"".


In a new ticket: we can better investigate the value that grouping brings and other questions like…
 - how many settings does it apply to? 
 - is taxonomy based on a technical or user perspective? 
 - if user/operator based, how many people need to be involved to get it right?
 - if user/operator based, what if one property applies to multiple concerns? 
 - how does the {{@Replace}} annotation work between levels in the grouping? 
 - does this introduce more complexity/variations in what has to be tested? (since yaml can consist of old and new setting names)
;;;","06/Jul/20 10:38;benedict;bq. The valid concern around the api churn it introduces is addressed by committing the new ticket to 4.0-beta.

This ticket’s API replaces the current API, is mutually exclusive with the alternative proposal, and would be deprecated by it.  If we introduce them both in 4.0-beta, we must maintain them both and go through the full deprecation process.  So unfortunately no churn is avoided.

> I'd be curious what your perspective is on how we determine what qualifies as justified

It is customary that, before work is committed, alternative proposals are engaged with on their technical merits.  It occurs to me we recently worked to mandate this as part of the process, in fact.  In this case we _seem_ in danger of subordinating this to beliefs about scheduling.

If you like, I can formulate a legally airtight veto, but my goal is only for you to engage briefly with the proposal and determine for yourselves which is superior.  If the new proposal is _technically_* superior, and of similar complexity, then you are my justification.  If you disagree, however - and importantly we agree that we do not intend to pursue the alternative approach in future - I would consider my veto invalid (and would anyway withdraw it).

> having heard of the proximity of the beta

Perhaps we can also directly address people’s thoughts on deferral to 4.0-beta?  This should surely alleviate concerns around delaying 4.0?  I do understand the imperative to get 4.0 out the door, but I also know we all want to ship the best product we can as well.  If we can achieve both, we should. APIs matter, and avoiding API churn is an important part of our user/operator story.

\* I _hope_ we can avoid an epistemic battle about the word ""technical,"" and accept that API design is a technical endeavour to convey meaning.;;;","06/Jul/20 14:16;jmckenzie;{quote}{color:#172b4d}It is customary that, before work is committed, alternative proposals are engaged with on their technical merits. It occurs to me we recently worked to mandate this as part of the process, in fact. In this case we {color}_seem_{color:#172b4d} in danger of subordinating this to beliefs about scheduling.{color}
{quote}
{color:#172b4d}Unfortunately it's customary on this project to do that right up until the last moment before something is committed (and even beyond) with no weighting of the value on things actually being in the hands of users vs. in tree and unreleased.{color}

{color:#172b4d}We have un-tested beliefs about a potentially superior design against un-tested beliefs about the negative impact of the project on further delay. This is not a situation in which we can expect to make progress on a discussion until and unless both sides collect some empirical evidence about their position as well as spend real time investigating and exploring the position of other people engaged.{color}

{color:#172b4d}Unfortunately I'm well past the time I personally have available to engage on this ticket; I'll defer to other people to take it from here.{color};;;","06/Jul/20 15:39;blerer;Argeeing on grouping will take a significant amount of time. Specially now where a lot of people are pretty busy with other tasks.
There are no true urgency to fix that ticket so if we want to go the grouping way within the scope of that ticket we should move it to 4.X.

 
 


;;;","06/Jul/20 15:40;benedict;> We have un-tested beliefs about a potentially superior design

We don't ordinarily label our judgements about design ""un-tested beliefs"" and I think it would help to avoid this kind of rhetoric.  If we all start labelling design decisions in this way the project might grind to a halt.  I have anyway tried specifically to sidestep this kind of accusation, by leaving the ball in your court.  I am simply asking those pushing to move ahead with the current proposal to endorse the view that it is superior.  This is a very weak criteria to meet, and involves no beliefs external to yourselves.
;;;","06/Jul/20 15:47;benedict;> There are no true urgency to fix that ticket so if we want to go the grouping way within the scope of that ticket we should move it to 4.X.

I think this is indeed preferable to releasing an API we already expect to deprecate, however I think we're overstating the difficulty here.  We haven't debated the parameter naming much at all, and we can easily land this in 4.0-beta.  If [~e.dimitrova] doesn't have the time, and 4.0-beta is an acceptable window to land the work, I can take a look in a few weeks.
;;;","06/Jul/20 16:54;e.dimitrova;Apologize for my late response. I was a bit sick these days and tried to disengage from work and take some rest over the weekend.

With all my respect to everyone's opinion and experience on the project, I have two points here:
 - I truly support [~mck]'s questions. I believe they should be responded before any decision is taken and someone jumps into actual work.
{quote}how many settings does it apply to?
 is taxonomy based on a technical or user perspective?
 if user/operator based, how many people need to be involved to get it right?
 if user/operator based, what if one property applies to multiple concerns?
 how does the @Replace annotation work between levels in the grouping?
 does this introduce more complexity/variations in what has to be tested? (since yaml can consist of old and new setting names)
{quote}
 - I was also wondering today while I was trying to be open-minded and look from all perspectives at this ticket/patch... Did anyone check the first [commit |https://github.com/ekaterinadimitrova2/cassandra/blob/CASSANDRA-15234-1-outdated/conf/cassandra.yaml] where I was suggesting reorganizing of the text into the yaml into sections? I also put it into the ticket thread . 
 This was a quick draft shared two months ago that could be reworked to sections that satisfy the users' requirements for clarity and consistency.

Do we see any big difference for the users between:
{code:java}
#*Replica Filtering Protection*

cached_rows_warn_threshold: 1000
cached_rows_fail_threshold: 16000
{code}
and:
{code:java}
replica_filtering_protection:
     - cached_rows_warn_threshold: 1000
     - cached_rows_fail_threshold: 16000
{code}
From that perspective, I think the C* community can accept this patch and then we can raise a new ticket) to improve the internals from our engineering perspective in Beta(refactoring the Config class and the backward compatibility framework), as suggested by [~mck]. I think this work could be really considered incremental work.

Having that in mind, honestly, I don't find a justification to spend my time to rework and fully re-test the patch at this point in time.

I am fine to be proved wrong in a justified way. [~benedict], [~blerer], [~mck], do you agree with me on my suggestion(reorganizing the yaml file and doing the nested parameters approach later)?

 

{quote} I think this is indeed preferable to releasing an API we already expect to deprecate, however I think we're overstating the difficulty here. We haven't debated the parameter naming much at all, and we can easily land this in 4.0-beta. If [~e.dimitrova] doesn't have the time, and 4.0-beta is an acceptable window to land the work, I can take a look in a few weeks.  {quote}

 

I want to be clear - it is not about difficulty, this patch is time consuming. It needs attention to the detail and look at the whole config which touches the code at many places(also ccm, dtests, in-jvm tests, etc)

 ;;;","06/Jul/20 17:59;maedhroz;My mental model of why grouping _might_ be valuable:
 * It provides a logical place to describe/comment on entire features in the YAML.
 * It avoids duplicate/unwieldy prefixing without sacrificing intelligibility/specificity.
 * It doesn't rely on the presence of comments.

My understanding of the changes here is that there are dozens of options that have already been renamed. Assuming we proceed with grouping, supporting three different forms of these options doesn't seem like the outcome we want. There are really only a handful of groupings that would be interesting and obvious. Essentially, hinted handoff, commitlog, memtable, rpc, compaction, and maybe the caches. (Timeouts seem a bit scattered.)

What I'm most worried about is the number of versions we have to support at any given time, not whether we change some option grouping early in the beta period. My vote, at this point, would be to just move this issue to beta and hash out a proposal for the (somewhat obvious) option groups I've mentioned above.;;;","06/Jul/20 19:01;mck;{quote}I am fine to be proved wrong in a justified way. Benedict Elliott Smith, Benjamin Lerer, Michael Semb Wever, do you agree with me on my suggestion(reorganizing the yaml file and doing the nested parameters approach later)?
{quote}
Let's keep listening to what everyone has to say. Some of us are better with the written word than others, it is a second language for some, and for me as a native-english-speaker it is still all too easy to miss things the first time they are said. On that, I believe everyone hears and recognises what [~e.dimitrova] is saying here regarding frustrations about such a substantial change being suggested so late in the game and the amount of time that's been asked to re-invest. Especially when an almost identical user experience improvement was presented two months ago. But it should be said again.

On a side-note, it would have really helped me a lot if the comment [above|https://issues.apache.org/jira/browse/CASSANDRA-15234?focusedCommentId=17150521&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17150521] back-referenced [this discussion|https://github.com/apache/cassandra/pull/659#discussion_r449201020] where it originated. I know the ticket was referenced, but that discussion thread is the source of the suggestion.

 
{quote}This ticket’s API replaces the current API, is mutually exclusive with the alternative proposal, and would be deprecated by it. If we introduce them both in 4.0-beta, we must maintain them both and go through the full deprecation process. So unfortunately no churn is avoided.
{quote}
AFAIK this is the only ""grounded"" justification for the veto. I don't agree that we are forced into that premise. We can get around those compatibility rules with a minimal amount of effort, by not deprecating the old API and not announcing (in docs or yaml) the new API. (I would expect everyone to intuitively treat private undocumented and un-referenced APIs, only ever available in alpha and beta releases, as unsupported.)  All that ""compatibility change"" can be left and just done once in the separate ticket. The underlying framework and bulk of this patch can still be merged.

 

Based on that I see three possible courses of action:
 1. Accept investigating the alternative proposal, and include it in this ticket, delaying our first 4.0-beta release,
 2. As (1) but requesting this ticket to be merged during 4.0-beta, so we can release 4.0-beta now,
 3. Spin out the new suggestion and all public API changes to a separate ticket, slated for 4.0-beta, and merging this ticket.

 

I suspect, since you have offered to help [~benedict], that most are in favour of (2) ?

 ;;;","06/Jul/20 19:27;dcapwell;Reread the conversations that have been going on over the past 3 days several times, sorry if I missed anything or didn't grasp all points.

Most of the thread is about doing an all or nothing approach, thanks [~mck] for trying to argue for incremental improvement.  Looking at the list of properties impacted (see https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-new#diff-4302f2407249672d7845cd58027ff6e9R257-R339) it looks like a subset would be clearly impacted by the grouping approach, and others not so much or are complementary; given this we could accept a hand full of the properties and move the other properties into the grouping work (stuff such as read_request_timeout_in_ms to read_request_timeout I feel are fine even with the grouping approach, but stuff which renames things maybe leave out for now, such as enable_user_defined_functions to user_defined_functions_enabled).

I do agree with [~benedict] that it isn't ok to keep changing our config API since this is user facing, we should be strict about user facing changes and try to help more than harm.  If there is a belief that one structure is better than another then I value this dialog and hope we can get more eyes from the users/operators to see their thoughts; for this work we should really speak about the YAML representation rather than the code so we can agree on the final result.  Also, given the framework that is provided by this patch, I don't see that work as throwing everything away, instead I see it benefiting from the work which is started.  Given the work involved is to add support for ""moving"" a field (current ""rename"" is a special case of move where the move is at the same level) from one location to another (rename and conversion already supported), this adds complexity for the case where the new and the old field are both used and may hit complexity issues with SnakeYaml implementation.  I do believe we should have this discussion and settle on a solution before releasing 4.0.0, but I do not feel that this discussion blocks a beta release. There is a lot of chatting about being a beta blocker but I don't really follow why this JIRA (or the grouping one) is a blocker.  Reading https://cwiki.apache.org/confluence/display/CASSANDRA/Release+Lifecycle I don't see why this JIRA could not be done during beta, it meets every requirement for this phase.

Given all the comments above, my TL;DR

* Can we find a subset of properties with the current patch which are not discarded by the grouping work (sample given above)
* Can we start the conversation and start asking operators of Cassandra clusters on their thoughts on grouping vs not grouping?  Grouping could be nice for humans but could be horrid for some automation (I am neither pro or against grouping, I defer to operators preference here).
* Can we mark this ticket and the grouping one as non-blocking for beta;;;","07/Jul/20 09:15;benedict;bq. We can ... not deprecat[e] the old API and not announc[e] ... the new API

This would be fine by me.  My -1 as expressed applied only to the public API changes.  All of the other changes in this ticket are absolutely good to go (once review is completed, of course).

Also to reiterate again (since things easily get lost in such a contentious exchange): 
* I think this work is great, and am really glad [~e.dimitrova] undertook it.
* I don't personally mind too strongly _which_ API we settle on, just expect that we settle on one upfront before committing the userbase to it.
* I think it's a shame that this has seemingly turned into a heightened and acrimonious exchange.
* It's worth remembering this situation is not unique; we have all experienced having to revisit work (or abandon it, even - my GitHub is a graveyard of good intentions) to our frustration and disappointment; it's a part of OSS development.
;;;","07/Jul/20 09:23;blerer;{quote}Can we mark this ticket and the grouping one as non-blocking for beta{quote}

I will do that.

{quote}Can we start the conversation and start asking operators of Cassandra clusters on their thoughts on grouping vs not grouping? Grouping could be nice for humans but could be horrid for some automation (I am neither pro or against grouping, I defer to operators preference here).{quote}

[~benedict], [~maedhroz] as you are the ones that suggested the change, would you mind starting the discussion on the dev list?;;;","07/Jul/20 09:49;benedict;bq. would you mind starting the discussion on the dev list?

Happy to get more directly involved to move this along, if you like.  In this case I will put together a specific proposal to bring to the dev list, to avoid it being an aimless discussion.;;;","15/Jul/20 06:41;e.dimitrova;[~maedhroz] pointed to me correctly that the new yaml was missing at the last submitted branch as I was testing the backward compatibility with the old cassandra.yaml.
[This | https://github.com/ekaterinadimitrova2/cassandra/commits/CASSANDRA-15234-new2] is a link to a WIP branch where the new yaml could be found.
The last two commits are respectively the change to a new structure and the change of parameters names. ;;;","17/Jul/20 05:30;maedhroz;[~e.dimitrova] [~benedict] I've posted [a rough sketch|https://github.com/maedhroz/cassandra/commit/49e83c70eba3357978d1081ecf500bbbdee960d8] of what a fairly aggressive application of option grouping would look like for {{cassandra.yaml}}. Hopefully that helps keep the conversation moving forward. The biggest change I made to the existing structure was placing options for the same component in the same group, rather than having ""commonly used"" and ""advanced"" options (in some cases) in totally different parts of the file.;;;","17/Jul/20 10:15;benedict;Thanks Caleb for the reminder.  I've [pushed another approach|https://github.com/belliottsmith/cassandra/tree/CASSANDRA-15234-grouping-ideas], that groups options by the reason the operator cares about them, namely: 
* {{cluster}}-wide settings (partitioner, token etc)
* {{disk}} options that specify strategy, throttle throughput etc
* {{memory}} options that allocate heap or direct memory resources
* {{concurrency}} that constrain the number of operations or threads committed to tasks
* {{internode}} and {{client}} networking options 
* {{feature}} options; and
* {{log}} options for our warning thresholds etc

It was taking a while, and it might not be well received, so I only went about 90% of the way so that the approach can receive some feedback.  Obviously, this would necessitate a different approach to the headline comments, wherein we might want to list the parameters users might care about in prose alongside explanations for why they might care.

I personally would like to propose we also introduce a dual system for updating properties, wherein we can accept the nested namespace versions, as well as e.g. dot-delimited versions.  e.g. {{disk.throttle.compaction: 10MiB/s}}. This should mitigate any risk to simplistic tools, as well as maybe providing us a simple route to permitting common options being given at the start of the file, without confusing the overall approach.

[~e.dimitrova]: in going through the yaml, I noticed that you have used mbps in many places, but I thought we had previously agreed that any {{bps}} was ambiguous, since it can mean bits or bytes?  I thought we had settled on MiB/s so that there could be no ambiguity?  (Since MB/s is also ambiguous - technically meaning 1000s, but often meaning powers of 2)?;;;","17/Jul/20 23:08;dcapwell;Thanks [~maedhroz] and [~benedict] for the YAML examples; it would be great to start this dialog on user@ to get peoples feedback.

bq. I personally would like to propose we also introduce a dual system for updating properties, wherein we can accept the nested namespace versions, as well as e.g. dot-delimited versions. e.g. disk.throttle.compaction: 10MiB/s. This should mitigate any risk to simplistic tools, as well as maybe providing us a simple route to permitting common options being given at the start of the file, without confusing the overall approach.

So the following would be supported?

{code}
compaction_throughput_mb_per_sec: 42
disk:
  throttle:
    compaction: 42MiB/s
disk.throttle.compaction: 42MiB/s
{code}

I assume the semantics would require mixed to work (part of the disk.throttle is nested, other parts are not) and wouldn't allow duplication definitions with other versions (can only use 1 of the 3 forms, not all), so doesn't seem that bad as that logic already exists.  We walk the class to discover the previous names, so can do the same or collect the shape while we walk.  

My main concern is if this makes it more confusing for users; they may find some docs which say the old name, some using the nested name, and others using the flat name.;;;","21/Jul/20 17:41;e.dimitrova;Good catch [~benedict] . Thank you. My bad, just pushed a fast commit [here|https://github.com/ekaterinadimitrova2/cassandra/commit/b4eebe080835da79d032f9314262c268b71172a8] for the three rate parameters we have. 

For the record, I stopped working on this patch until it is clear whether the code will be used at all. As far as I understand, you took the lead on a POC development for the newly suggested approach. Unfortunately, I don't have time to support this now but I would be happy to give feedback when it is shaped already. 

If you decide at some point this work or part of it to be committed, let me know, I will complete whatever is outstanding. The main framework is in place.

A couple of things to keep in mind:
 * As suggested by Benjamin, the patch can be simplified by making {{Converter}} an {{enum}}. It will allow to use the enum value directly into the {{Replaces}} annotation. Doing so will remove the need to use reflection to instantiate the converters and the use of a cache to avoid multiple instantiation.
 * In-jvm tests - loading config parameters should be reworked as currently they don't work with custom types (like the newly introduced Duration, etc). The suggested approach by [~dcapwell] would work but it requires also api changes. I suggest a separate ticket for this part to be opened. Instead of using reflection, the suggestion is to use SnakeYAML. In order not to slow down the tests, no yaml files will be introduced but there will be a function to build the yaml nodes for us. This was a quick POC by [~dcapwell] but there are parameters which will need additional work and attention:

 
{code:java}
public class MapToConfigTest
 {
     @Test
     public void test()
     {
          Map<String, Object> map = ImmutableMap.<String, Object>builder()
                                                          .put(""auto_bootstrap"", false)
                                                          .put(""permissions_validity_in_ms"", 10)
                                                          .put(""role_manager"", ""some value"")
                                                          .build(); Constructor constructor = new YamlConfigurationLoader.CustomConstructor(Config.class);
         PropertiesChecker propertiesChecker = new PropertiesChecker();
         constructor.setPropertyUtils(propertiesChecker);
         constructor.setComposer(new Composer(null, null)
         {
              public Node getSingleNode()
             {
                return toNode(map);
             }
          });
            Config config = (Config) constructor.getSingleData(Config.class);
           System.out.println(""trap"");
     }

     public static Node toNode(Object object)
     {
          if (object instanceof Map)
          {
               List<NodeTuple> values = new ArrayList<>();
               for (Map.Entry<Object, Object> e : ((Map<Object, Object>) object).entrySet())
               {
                    values.add(new NodeTuple(toNode(e.getKey()), toNode(e.getValue())));
               }
               return new MappingNode(FAKE_TAG, values, null);
           }
           else if (object instanceof Number || object instanceof String || object instanceof Boolean)
           {
               return new ScalarNode(FAKE_TAG, object.toString(), FAKE_MARK, FAKE_MARK, '\''); 
           }
          else
          {
                throw new UnsupportedOperationException(""unexpected type found: given "" + object.getClass());
          }
      }
      private static final Tag FAKE_TAG = new Tag(""ignore"");
      private static final Mark FAKE_MARK = new Mark(""ignore"", 0, 0, 0, """", 0);
 }
{code}
 

The rest are small things which I also partially already handled. 

[~maedhroz], [~benedict], please, let me know if you have any other questions around this patch.;;;","21/Jul/20 18:26;benedict;Thanks [~e.dimitrova]

bq. If you decide at some point this work or part of it to be committed

It was a confusing disagreement, certainly.  I understood us to have agreed that - if wanted - we could commit the work that's been reviewed, excluding only the new config file (and the new config parameter name tests etc).  It's up to you of course.  I think there could be value in this, since it might make the config file change more manageable, but I'm personally neutral on sequencing.

I'm fairly certain the main body of this work should be utilised to support whatever our new config file layout is; whether we commit it first or all together is fine by me, and I'm happy to defer to others' preferences (most importantly yours, as the author);;;","21/Jul/20 21:07;benedict;bq.  it would be great to start this dialog on user@

I'd personally prefer to hash out here for a week or so, to get a good proposal to take to the user list, or a couple of competing proposals.  I find public engagements works best when there's a good case to challenge/consider.

It's a difficult balancing act getting any given approach right, and there are multiple approaches.  I would love to see another approach taken more to its conclusion for comparison.

I've made some further changes, and to make it clearer created a [yaml|https://github.com/belliottsmith/cassandra/blob/5f80d1c0d38873b7a27dc137656d8b81f8e6bbd7/conf/cassandra_nocomment.yaml] with comments mostly stripped.

In this version, there are basic settings for network, disk etc all grouped together, followed by operator tuneables mostly under {{limits}} within which we now have {{throughput}}, {{concurrency}}, {{capacity}}.  This leads to settings for some features being kept separate (most notably for caching), but helps the operator understand what they have to play with for controlling resource consumption.

It's still incomplete, but 90%+ done, and thoughts would be most welcome.

bq. My main concern is if this makes it more confusing for users; they may find some docs which say the old name, some using the nested name, and others using the flat name.

This is a possibility, and I'm not wed to the idea - but I think the balance of benefit to risk is probably pretty good, particularly since the names are fairly consistent (and we can have a blurb at the start to explain the dual system), so I doubt it should lead to too much confusion if we opt for it.
;;;","21/Jul/20 21:12;lorina@datastax.com;I will be happy to write clear information about this topic in the docs
after implementation, to explain old/new.

Lorina
Lorina Poland
e. lorina@datastax.com
w. www.datastax.com



On Tue, Jul 21, 2020 at 2:09 PM Benedict Elliott Smith (Jira) <

;;;","22/Jul/20 21:03;dcapwell;bq. I'd personally prefer to hash out here for a week or so, to get a good proposal to take to the user list, or a couple of competing proposals. I find public engagements works best when there's a good case to challenge/consider.

+1

bq. I will be happy to write clear information about this topic in the docs after implementation, to explain old/new.

Thats great Lorina!;;;","24/Jul/20 20:52;e.dimitrova;[~benedict] I am sorry for my delayed responses but I am multi-tasking a lot these days. 
{quote}I'm fairly certain the main body of this work should be utilised to support whatever our new config file layout is; whether we commit it first or all together is fine by me, and I'm happy to defer to others' preferences (most importantly yours, as the author)
{quote}
I think the backward compatibility is the primary part that will be most probably reworked to accommodate the new approach. I think the moment we moved the ticket to beta and the discussion continued here, that was the time we kind of agreed through our actions that this work will be continued/completed/committed later. I might have gotten it wrong. As it looks like this is already happening, I suggest to shape it and then move on with the actual changes. Maybe we can only create now a separate new ticket on the in-jvm api change so a snapshot release could be voted, etc. But also, we need to see what yaml nodes will have to be created according to what we need, so that work could also wait a bit until the api part is agreed, I think.

About the dual approach, I am not sure I understand you correct what you mean in this citation:
{quote}providing us a simple route to permitting common options being given at the start of the file, without confusing the overall approach.
{quote}
I understand having third version will help any simplistic tools in case they can't deal with the nested approach but to me personally it would be confusing. I am pretty sure [~lorina@datastax.com] will update perfectly the documentation (as usual, thanks a lot [~lorina@datastax.com]!) but at the same time if someone is in a hurry during a production issue, they will just jump on random link from google probably and maybe be get confused about what/how. Here, for this part, probably the survey will show the best.
 Otherwise, the grouping sounds reasonable to me.

The reason why I suggested Quick Start, Advanced options etc is new users. I kind of feel that for someone brand new to C* this might help. But also if we make groups into nested parameters, sure this becomes irrelevant and those sections will not really make sense anymore, not in their current version.

Excited to see the end result/user reactions.

Last but not least, I also agree with you on the below:
{quote}I'd personally prefer to hash out here for a week or so, to get a good proposal to take to the user list, or a couple of competing proposals. I find public engagements works best when there's a good case to challenge/consider.
{quote};;;","05/Aug/20 22:19;maedhroz;bq. Maybe we can only create now a separate new ticket on the in-jvm api change so a snapshot release could be voted, etc.

[~e.dimitrova] I like that idea. I'd volunteer to review a separate Jira...;;;","29/Sep/20 12:52;pauloricardomg;Despite the awesome work (thanks for leading it [~e.dimitrova]) and productive discussion that went into this issue, we didn't seem to reach a strong agreement here and it seems to me it's a bit late in the 4.0 release cycle to land this?

In the spirit of expediting 4.0RC release I propose we postpone this to 4.X, and resume this with high priority earlier in the next release cycle. What do you think?;;;","30/Sep/20 00:02;jmeredithco;The improvements are definitely very valuable and make configuration much cleaner and more flexible, but I'm also concerned it's too late in the cycle.

Although the patch goes to great lengths to be backward compatible, people that have been working towards getting ready for production deployments would need to re-test all the configurations they've worked through so far which would certainly cause rework to validate the release.

 

 

 ;;;","07/Oct/20 04:58;maedhroz;bq. In the spirit of expediting 4.0RC release I propose we postpone this to 4.X, and resume this with high priority earlier in the next release cycle.

I'm more or less in agreement that we push this to the next major along with CASSANDRA-16038, unless some unrelated need for the latter arises.;;;","07/Oct/20 07:32;blerer;{quote}In the spirit of expediting 4.0RC release I propose we postpone this to 4.X, and resume this with high priority earlier in the next release cycle.{quote}

This is a major configuration change. As such, it should be part of a {{major}} release and not of a {{minor}} one. I am fine postponing this ticket but the next target for it should be {{5.0}} in my opinion not {{4.X.}} ;;;","07/Oct/20 12:51;pauloricardomg;bq. I am fine postponing this ticket but the next target for it should be 5.0 in my opinion not 4.X.

+1, I actually meant next major by 4.X;;;","07/Oct/20 13:15;e.dimitrova;+1

Changed the version to 4.x only because there is no 5 yet. If there is other way to mark it that I missed, please, feel free to correct me :) ;;;","07/Oct/20 13:23;benedict;Not sure if you need special permissions, but you can just create a new version.

Sorry for my having not had time to finish my proposal - but I agree having a bit more time to decide and consult on the best layout is probably a good thing.;;;","07/Oct/20 14:03;e.dimitrova;Thanks [~benedict]. I think maybe it is a matter of Jira permissions. My only option is to select only from a list of already created versions. 

 ;;;","05/Jul/21 13:06;blerer;Changing the status to {{IN PROGRESS}} as the current patch need to update once we agree on the expected outcome.;;;","07/Jul/21 15:40;e.dimitrova;Reading back the thread here and reminding myself of what we discussed I guess the next step is to hash out a proposal for the user list and get some feedback?
{quote}bq. I'd personally prefer to hash out here for a week or so, to get a good proposal to take to the user list, or a couple of competing proposals. I find public engagements works best when there's a good case to challenge/consider.
{quote}
 ;;;","07/Jul/21 16:05;benedict;So, I produced an initial proposal [here|https://github.com/belliottsmith/cassandra/blob/5f80d1c0d38873b7a27dc137656d8b81f8e6bbd7/conf/cassandra_nocomment.yaml] in [this comment|https://issues.apache.org/jira/browse/CASSANDRA-15234?focusedCommentId=17162342&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17162342].

It is not complete, but the shape of the idea is pretty clear, I think - it would be good to get some initial feedback on this approach to grouping of config options, as optimising the groups is a laborious job that there's no point pursuing if it's otherwise unwelcome. We can no doubt bike shed those to death as well, if we so choose.

;;;","19/Jul/21 17:07;e.dimitrova;Thank you [~benedict] and apologize for the late response. There was a lot of multitasking lately.
{quote}it would be good to get some initial feedback on this approach to grouping of config options, as optimising the groups is a laborious job that there's no point pursuing if it's otherwise unwelcome.
{quote}
Absolutely agree with you. I personally have only that question for feedback, whether they would prefer the sections or the nested version you suggested. 

[~maedhroz], [~mck], [~paulo], [~blerer], [~dcapwell], [~lorina@datastax.com]   (maybe [~stefan.miklosovic] as I know he was also interested into this work at some point) - do you have anything to add here as concerns/comment before we bring it to the user list? Any other proposals?

Also, should it come from a PMC? Survey or just a discussion mail thread? I am not familiar of any similar discussions before so I am not sure what would be the right approach, I am open for suggestions/advice anyone might have. Thank you.

 

 ;;;","25/Oct/21 14:35;e.dimitrova;For the record, this is the discussion thread from the dev mailing list - [https://lists.apache.org/thread.html/r507be1624a568765f9d5ec5f6b561885129d0aaeb982e9bd9bf5e01b%40%3Cdev.cassandra.apache.org%3E]

Based on it, I am getting back to this work.;;;","28/Oct/21 23:12;e.dimitrova;Rebase and cleaning done successfully.

I cleaned Unit tests and distributed In-jvm tests, passing up to current trunk state. I should finish updating the Python DTests for the new config parameters' names and check for failures when the java upgrade tests are fixed.

Topic:
 * the upgrade tests will be testing with the new names and config parameters provided with the units for trunk. We have unit tests that check the backward compatibility parsing. I guess we don't plan to run the upgrades also with the old config?

I plan to publish the full patch for new round of review probably early next week.;;;","22/Nov/21 03:21;e.dimitrova;[~dcapwell],[~blerer], submitting for new round of review. 
I have some issue that I can't always trigger in CI the newest version I push to ccm. Still it seems Jenkins has more success than CircleCI.

Until I figure it out, these are the patches for initial round of review.

[trunk|https://github.com/apache/cassandra/compare/trunk...ekaterinadimitrova2:CASSANDRA-15234-take2?expand=1] | [dtest|https://github.com/ekaterinadimitrova2/cassandra-dtest/pull/new/CASSANDRA-15234-take2] | [ccm|https://github.com/ekaterinadimitrova2/ccm/pull/new/CASSANDRA-15234] 

I cleaned all tests without full Jenkins confirmation of the Python DTests yet due to the ccm issue mentioned.

Also, this patch latest rebase was also a week ago and I will rebase again when I have to do next round of review changes and I fix ccm as otherwise on every rebase something new appears as trunk is quite alive and the config is all over the place. Let me first stabilize CI and we can agree on the patch, then I will clean this.

Now a few important comments:
 * ASCII doc migration is still not done but I will open a ticket to add docs for us for adding new config and docs for our users around the new and old types and names.
 * DTests were not updated to the new names and format as this will take forever. Suggestion - leave the old ones and add new tests with new ones. We can also open a follow up ticket but now this exercises the backward compatibility which is good as this is also how I figured out one more change needed for CCM.
 * CCM was updated to be able to handle the backward compatibility and mimic the Cassandra behavior. 
 * I suggest not to overwhelm the patch with the reorganization of cassandra.yaml I suggested earlier on this ticket. I will pull it in a separate one and I can trigger a new discuss thread on the mailing list to confirm that. 
 * Converter was not changed to an enum because we need generics support for that and  [JEP301|http://openjdk.java.net/jeps/301] was withdrawn. 
 * Intentionally left the commits after the old patch not squashed to provide some input and hopefully make the review a bit more easy. [~blerer]  and [~dcapwell] , please, let me know if you want them in a different way for the review.
 * Now when 4.0 was released I had to add also Virtual Table backward compatibility and the easiest way was just to have the old names and types and the new ones both listed in the Settings table, available for our users. 
 * _commitlog_sync_batch_in_ms_ was left for another ticket as the in-jvm upgrade tests will need work to make it possible to support it due to their nature as it seems we have the lower branch _DatabaseDescriptor_ class loaded and it makes checks on trunk and complains if we change to the custom type. (CASSANDRA-17161)
 * Any new parameters added post 4.0 in trunk were/will be directly changed to support the new format as they are still not in any release. 
 * CCM_41_YAML_OPTIONS in ccm should be well documented and people warned that they need to keep it up to date on changes in config. I hope we won't need this to happen a lot after this update but we need to ensure people are aware of it. It seems a bit clumsy but I didn't find a better way to handle ccm at this point. I am open for ideas. 
 * I can think of adding some additional test for the virtual table. 
 * I also suggest we add a warning to the users when they start Cassandra that if they add more than once a property (even if it is the old name), the latest occurrence from the yaml file will be the one assigned to the parameter. I would say we add also info to the docs and we don't complicate to add checks whether we have or not more occurrences during startup and emitting warnings on occurrence as it seems a bit too much overhead on startup. WDYT?
 * There were deprecation warnings emitted more than once. I fixed this. This fix should be also ported to 4.0 where we had to make small port of the name change because of two parameters issues. (FTR - CASSANDRA-17141) In 4.0 they are not only two properties with the warnings twice so it was harder to spot it. ;;;","30/Nov/21 23:42;dcapwell;patch is massive; no way I can finish before I go on break Friday =(

Can you actually send a PR?  easier to leave comments;;;","30/Nov/21 23:49;e.dimitrova;Sure, [PR|https://github.com/apache/cassandra/pull/1350] created
{quote}patch is massive; no way I can finish before I go on break Friday =(
{quote}
This waited whole year, it can wait while you take well deserved break. I will be off on Friday and then in two weeks again. Hopefully the next rebase won't be too painful :) Hopefully the community won't work too much around the holidays :D  ;;;","30/Nov/21 23:54;dcapwell;bq. DTests were not updated to the new names and format as this will take forever. Suggestion - leave the old ones and add new tests with new ones. We can also open a follow up ticket but now this exercises the backward compatibility which is good as this is also how I figured out one more change needed for CCM.

works for me; more testing we didn't break things ^_^

bq. I suggest not to overwhelm the patch with the reorganization of cassandra.yaml I suggested earlier on this ticket. I will pull it in a separate one and I can trigger a new discuss thread on the mailing list to confirm that. 

Works for me


bq. Converter was not changed to an enum because we need generics support for that and  JEP301 was withdrawn. 

Works for me

bq. Intentionally left the commits after the old patch not squashed to provide some input and hopefully make the review a bit more easy. Benjamin Lerer  and David Capwell , please, let me know if you want them in a different way for the review.

Personally prefer PRs, easier to leave comments than commits


bq. Any new parameters added post 4.0 in trunk were/will be directly changed to support the new format as they are still not in any release. 

Saw the TrackWarning changes.... <3

bq. I can think of adding some additional test for the virtual table. 

I am 100% cool leaving the vtable problem to another ticket =). Right now it will show the new names, if we want different behavior we can solve it outside of this JIRA.  We are having side conversations on how can we unify (at least I am pushing this) properties access.

bq. I also suggest we add a warning to the users when they start Cassandra that if they add more than once a property (even if it is the old name), the latest occurrence from the yaml file will be the one assigned to the parameter. I would say we add also info to the docs and we don't complicate to add checks whether we have or not more occurrences during startup and emitting warnings on occurrence as it seems a bit too much overhead on startup. WDYT?

Ideally I like this, but also 100% cool not doing in this patch.  

{code}
a: b
a: c
{code}

that has worked for years without anyone noticing... if adding a warning is low-hanging-fruit then I am cool, but I wouldn't hold up this ticket (I like splitting tickets if you have not noticed).
;;;","01/Dec/21 17:41;e.dimitrova;Moving the ticket to ""In Review"" as [~dcapwell]  is already looking into it.

Also, for visibility and context as it seems I missed in my summary - there is a separate ticket linked to this one(CASSANDRA-9691) which was mentioning liberating metrics and JMX from units or just moving to lowest unit at least. I left this part to be handled there but I forgot to mention it in my summary. Apologize for that

My thought was currently to leave the  JMX and metrics for now and assume the old units are in place but [~dcapwell] is right we need at least to verify (and convert if needed) that in Config the old unit was used with the new custom types and we didn't use something different as this can be a mess if we leave it to people and reading docs. Something similar will also be needed for VTs where we will have to check the Converter in the new @Replaces annotation. There was idea for getting the VT backward compatibility out until we figure out how we are going to handle the VT as there is ongoing discussion around that, I will leave this now aside for a moment until we clear a bit the discussion. CC [~blerer] as I know he is working on a patch for CASSANDRA-15254

Also, I will be looking again into the ccm issue in the afternoon. 

 ;;;","02/Dec/21 02:53;e.dimitrova;Quick update - I figured it is purely pip3 issue how we install ccm. The time things worked was when I had a bug in my code, but in general CCM is not changing to my new version when I change it in requirements.txt

 

What I found in CircleCI logs:
{code:java}
Collecting ccm
  Cloning https://github.com/ekaterinadimitrova2/ccm.git (to revision CASSANDRA-15234) to /tmp/pip-install-dvfww32x/ccm_727b34808faa4db6904e104a715a22d2
  Running command git clone -q https://github.com/ekaterinadimitrova2/ccm.git /tmp/pip-install-dvfww32x/ccm_727b34808faa4db6904e104a715a22d2
  Running command git checkout -b CASSANDRA-15234 --track origin/CASSANDRA-15234
  Switched to a new branch 'CASSANDRA-15234'
  Branch 'CASSANDRA-15234' set up to track remote branch 'CASSANDRA-15234' from 'origin'.
  Resolved https://github.com/ekaterinadimitrova2/ccm.git to commit ee52d120ea34d44500c64bfb3b9d3f517b0865f1
{code}
 But then
{code:java}
 pip3 freeze
{code}
output shows:
{code:java}
ccm @ git+https://github.com/riptano/ccm.git@ce612ea71587bf263ed513cb8f8d5dfcf7c8dadb
{code}

I will debug this tomorrow but it is the way we update ccm not CircleCI caching or so;;;","02/Dec/21 23:46;e.dimitrova;Short update:
 - I figured out what needs to be done in order to be able to test with our own CCM branch - ticket to update our docs opened CASSANDRA-17182.
 - I was also thinking of adding a test to the ccm tests but it seems they are not run in CI, not sure what is the story there. ([~mck], any ideas?)
 - Almost all tests pass now, it seems I have an issue with the assert warnings only(a few Python DTests failed), I have to correct the string pattern used. I did it [here|https://github.com/apache/cassandra-dtest/pull/169/commits/7c417847092524d46de9e03d11d6acc63159e2dd] but I haven't run whole CI for this now. [Java8 CI|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1239/workflows/345d82ca-1e2a-4db9-b660-a88324793a40] | [Java11 CI |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1239/workflows/d0f8f584-ede1-42f1-96ad-9fbc2d981a56]. This branch was not rebased in the past two weeks so some of the test fixes are not applied.
 - I will do the JMX change next week in a new commit, it also shouldn't bother any reviews to happen in the meantime if/when people are available. I know [~dcapwell] and [~blerer] are busy and also holidays are approaching.;;;","06/Dec/21 20:07;maedhroz;Note: I think the {{TestClientRequestMetrics}} [failure|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1239/workflows/d0f8f584-ede1-42f1-96ad-9fbc2d981a56/jobs/7427] is related to CASSANDRA-17155. With everything rebased, it should wash out. CC [~yifanc];;;","09/Dec/21 23:09;maedhroz;Finished my first pass of review and left comments inline in the PR. (Can take a look at CCM next...);;;","10/Dec/21 17:03;maedhroz;CCM changes look reasonable, and I didn't have trouble running a cluster with it against your branch.;;;","15/Dec/21 20:35;e.dimitrova;Thank you [~maedhroz] 

I made all changes as per the review comments and our discussions. I also left responses on the PR. I have three commits on [this|https://github.com/ekaterinadimitrova2/cassandra/pull/191/files] new PR, as we talked (The old PR stays there, nothing added, nothing squashed; kept for reference) Next round of review on the [new |https://github.com/ekaterinadimitrova2/cassandra/pull/191/files] PR, the following commits:
 * [the custom types and the related tests|https://github.com/ekaterinadimitrova2/cassandra/pull/191/commits/c5b19d441949a4bbf0a4395517f8f637b9ee1ddf]. All tests pass
 * [The annotations and converters|https://github.com/ekaterinadimitrova2/cassandra/pull/191/commits/4fda1efee2cb9e066661982f79d1aa99ad8fbf8b]  for backward compatibility. I [ran |https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1253/workflows/864beb33-9051-4ef7-a96d-4e235fa96e4e] the test suite to ensure any changes in the YAML loader didn't break anything. From what I see the only failures are known ones which after rebase of the patch should go away, most of them.
 * [The big change |https://github.com/ekaterinadimitrova2/cassandra/pull/191/commits/6a4cf1d5925da154fd39c35e367fb1aa0f447ca3]- all the parameters plus adding backward compatibility for virtual tables. [CI run|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1255/workflows/89b0d74c-f745-436a-b2bb-bc7b5ea721e6] Java 8 (I will run again also Java 11 later but there was no reason to waste resources now). The only related to this patch failure is the one of _test_sstablelevelreset ._ I have to adjust the warning pattern in the dtest repo.

Side Note: There are two circle ci related commits for testing purposes. Please ignore them. I will remove them at the end.

I haven't touched the docs as they will have to be reworked after the migration to adoc.

JMX methods conversion to the old units is in there but I will add new JMX methods in a separate commit. (There is another ticket linked to this one for that but there was a discussion better to add them here) This doesn't stop the rest of the work from being revised in the meantime.

I have to add also that warning we discussed in case some parameters are set more than once in cassandra.yaml. I will do it in a separate commit.

After we confirm the current work, I will do new rebase and change any new parameters in trunk to the new format (we still don't have alpha version so it is safe to just change the names and types and no backward compatibility is needed).

Order of commits for now should be:

1) Cassandra repo [(the custom types and related tests)|https://github.com/ekaterinadimitrova2/cassandra/pull/191/commits/c5b19d441949a4bbf0a4395517f8f637b9ee1ddf]

2) Cassandra repo ([The backward compatibility framework |https://github.com/ekaterinadimitrova2/cassandra/pull/191/commits/4fda1efee2cb9e066661982f79d1aa99ad8fbf8b])

3) CCM patch

4) dtest patch

5) The [big change of parameters|https://github.com/ekaterinadimitrova2/cassandra/pull/191/commits/6a4cf1d5925da154fd39c35e367fb1aa0f447ca3]

 ;;;","06/Jan/22 07:05;maedhroz;[~e.dimitrova] I've made my final pass at review and left one more small set of nits inline in the PR. At a high level, there are only two things that linger a bit in my mind. The first, which I've mentioned before, is that we would probably save ourselves some UX trouble later on if we allow a single space between values and units. (ex. Allow both {{10 MiB}} and {{10 MiB}}.) The second is trying, where possible, to leave the cases where we throw {{ConfigurationException}} intact. (This mostly applies to the cases where we might throw it via JMX, i.e. we have something of a contract at the moment.)

+1;;;","06/Jan/22 07:08;maedhroz;Ah, and of course, we'll have to make a decision in terms of whether this hits 4.1 or not. If it doesn't, it seems like we wouldn't be able to merge to trunk.;;;","06/Jan/22 10:16;mck;bq. Ah, and of course, we'll have to make a decision in terms of whether this hits 4.1 or not. If it doesn't, it seems like we wouldn't be able to merge to trunk.

It's ok that the patch bumps build.xml to 5.0, if warranted (on the presumption we are honouring capability or providing a deprecation cycle where appropriate).;;;","06/Jan/22 16:15;e.dimitrova;Thank you [~maedhroz] . I already addressed the first two commits review comments, I will address the third commit comments now. Then I will rebase on the latest changes from December and take care of the newest added config in a new commit. I will ping you when I am done. 
{quote}It's ok that the patch bumps build.xml to 5.0, if warranted (on the presumption we are honouring capability or providing a deprecation cycle where appropriate).
{quote}
In theory we have backward compatibility with the old yaml&config, no breaking changes which someone might say it qualifies for 4.1. Should I hit the mailing list about that?

Also, for general visibility - I opted out of adding new JMX methods as there are already tickets for updating config through Virtual tables people work on and also there is no plan to change nodetool. ;;;","06/Jan/22 17:42;e.dimitrova;Moved back to 4.x as the patch has backward compatibility, introducing no breaking changes which aligns with our release discussions. 
{quote}we would probably save ourselves some UX trouble later on if we allow a single space between values and units. (ex. Allow both 10MiB and 10 MiB.)
{quote}
This is very easy to introduce but I think it is confusing to have both, similar to what we decided about the letter case. I would prefer to keep the current version only. Let me know what you think.
{quote}The second is trying, where possible, to leave the cases where we throw {{ConfigurationException}} intact. (This mostly applies to the cases where we might throw it via JMX, i.e. we have something of a contract at the moment.)
{quote}
Good point, I will handle it, thanks. ;;;","13/Jan/22 03:22;e.dimitrova;While rebasing and adding more parameters I hit a corner case, three duration parameters without unit suffixes which I personally don't think they need name change. That's fine. A converter was easy to add for them, BUT... what about Virtual tables? In order to support backward compatibility we currently list old parameters, those with new names are available in the old and the new form. For example:
{code:java}
memtable_heap_space   2018MiB
memtable_heap_space_in_mb    2018
{code}
The parameters that I personally didn't find a reason to change names but only the value formatting (adding the units to the value) are:

_key_cache_save_period, row_cache_save_period and counter_cache_save_period._ I can easily say we introduce a breaking change and leave those alone in the new format but I am open to hear also others' opinion. I feel I am getting too used to this work and I might be missing something. Also, IMHO it's worth it not to introduce breaking changes when/if possible.

In other news during the latest rebase and addition of parameters I found we need to improve the precision for the _DataRateSpec._ I will publish this in a separate patch soon, already handled.

I am doing last cleaning of tests and I will publish the final version with all latest changes from November until now for the reviewers' consideration. 

 ;;;","15/Jan/22 03:42;e.dimitrova;The [PR|https://github.com/ekaterinadimitrova2/cassandra/pull/191/files] is ready for final round in my opinion. Circle CI run [here|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1270/workflows/ecf6b25a-8849-4605-af66-9fbd53281629]. (Only Java 8 until we have +1, then I will have also the Java 11; also I submitted a Jenkins run, see below)

All three branches - [CCM|https://github.com/ekaterinadimitrova2/ccm/tree/CASSANDRA-15234], [Cassandra|https://github.com/ekaterinadimitrova2/cassandra/tree/15234-take2-review] and [DTest repo|https://github.com/ekaterinadimitrova2/cassandra-dtest/tree/CASSANDRA-15234-take2] were rebased and any outstanding issues fixed.

+*Exceptions:*+ 
 * The VT Settings related tests are failing because of the issue I mentioned in my previous comment around _key_cache_save_period, row_cache_save_period and counter_cache_save_period._ 
 * There are a few tests failing with byteman related issues. I suspect CircleCI might be acting weird. The tests complete fine locally. I just pushed a run in Jenkins [here|https://jenkins-cm4.apache.org/job/Cassandra-devbranch/1371/] which I will double check tomorrow. ;;;","20/Jan/22 07:22;maedhroz;[~e.dimitrova] Just reviewed the latest commits and dropped a few comments in the PR. Assuming there isn't anything controversial among them, my +1 stands. (For the {{*_save_period}} items in the virtual table, my guess is just displaying them there in the new format w/ units would be fine.);;;","20/Jan/22 22:48;e.dimitrova;[~maedhroz] I just rebased (both Cassandra and Cassandra-dtest) and pushed the updated as per your review patch. Still keeping things in separate commits for historical purposes until I have a go to squash things. Starting CI here - [j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1276/workflows/2a49cd4f-d543-4b4c-966b-7cfe0c80f592] and [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1276/workflows/39a8545b-b44d-46e9-a374-acc86dbe5da0];;;","21/Jan/22 02:52;e.dimitrova;The CI lgtm, there are all known failures. The byteman issues appear again but they turned to be general problem with CircleCI. There is already a ticket for investigation.

One thing on my mind is - there was a question whether we should use ""fail"" or ""abort"" if I am not mistaken. I noticed that only recently ""abort"" was introduced, ""fail"" is widely used in our config. I would suggest to stick to ""fail"" as it is already common for our users. Of course, I will be happy to hear if someone has a different perspective on this topic.

Also, my understanding is that [~dcapwell](from offline discussion) and [~maedhroz]  are done with their reviews. [~blerer] is swamped at the moment but I want to thank him for all his support and feedback at the early stages. My understanding is [~mck] is about to do a final review and then we can commit when he is ready and of course, I address any potential concerns he might have. I will also do another pass tomorrow as the patch is big and widely spread around the codebase. [~mck] , please, let me know if you want me to squash already the work; what is easier for you to review.;;;","21/Jan/22 12:02;benedict;+1 for {{fail}}

;;;","21/Jan/22 16:06;maedhroz;+1 on ""fail""...we already use ""abort"" in many situations that relate to transactions;;;","06/Feb/22 03:55;e.dimitrova;Approvals taken on GitHub and confirmed in Slack.

Many rounds of testing on the different commits were executed, but this is the final CI:

[j8|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1337/workflows/38632935-e720-4f5a-85d7-570fc52dbbee]  [j11|https://app.circleci.com/pipelines/github/ekaterinadimitrova2/cassandra/1337/workflows/16bfc433-02fc-4164-ae3a-9d66c577c708] 

All issues have tickets except the one test that failed on Java 11 but it is some issue with port already occupied which I've seen also with other tests and runs and it is infra related. 

CCM patch committed [here|https://github.com/riptano/ccm/commit/6d676896fb2ced4b6ac62fdba4ae2570351b38df] and retagged.

DTest patch committed [here|https://github.com/apache/cassandra-dtest/commit/3935906a685640b2f6a2058b38fdf45d917edfc9]

Trunk patch split to 13 patches starts from [this one|https://github.com/apache/cassandra/commit/db9f7a67ec4b03413c10034956e2cf18739ca4b1] on.

Thank you and please don't hate me if Jenkins get down after seeing so many commits :D 

 ;;;","06/Feb/22 05:52;e.dimitrova;I received an email that travis has failed for cassandra-dtest. It shows me that it failed with syntax error during execution pytest --metatests. I pulled the latest ccm locally and reran that step. It completed fine, same with my branch.

In the meantime I saw that I missed to refert to riptano/ccm in requirements.txt. It seems it wasn't in the separate commit as I thought. I just ninja fixed that and I am going to see whether travis will fail again.

Jenkins is still running, nothing unusual there for now. ;;;","06/Feb/22 05:53;e.dimitrova;Ok, travis just completed successfully after the ninja fix. I have no clue why was that error on a line in a file I haven't touched.;;;","06/Feb/22 06:01;e.dimitrova;Jenkins failed and I found in the logs the same error. I just restarted the build... Considering that in travis things completed fine now, maybe here the same will happen. I am totally not sure where this came from... I will keep an eye on it;;;","06/Feb/22 23:52;e.dimitrova;Seems like CCM retagging worked with Jenkins but not with CircleCI anymore. In order CircleCI to pickup the new tag, we need to add -e at the moment. 

I opened a follow up ticket CASSANDRA-17351 to fix this. Until then, please, use -e. More details sent to the mailing list [here|https://lists.apache.org/thread/x6vojt50z5gcq9n3xfjvgb5cptnh36mz]. ;;;","18/Mar/22 18:57;e.dimitrova;CCM issue was fixed last week. No workarounds needed.

Doc committed here - [https://github.com/apache/cassandra/commit/d67be0def4085863a039d5d3809a9457e883919b];;;","09/Jan/23 12:41;ifesdjeen;[~e.dimitrova] I realise that this was committed a while ago, but I got a question: there is a small change in output of nodetool commands that use `FileUtils#stringifyFileSize` that might break any parser that relies on the format to be in KB not in KiB etc. This constitutes a regression, doesn't it, as we've set a precedent in -CASSANDRA-17683?-

cc [~dcapwell], as we've briefly talked about this. ;;;","09/Jan/23 15:28;e.dimitrova;Hi [~ifesdjeen],  it seems  `FileUtils#stringifyFileSize` is used for nodetool output which I  missed when we were revising to revert changes after the discussions raised around CASSANDRA-17683. So yes, I agree with you. I will open a ticket to revert the change later today. Thank you for raising the issue.;;;","09/Jan/23 15:36;e.dimitrova;I just opened CASSANDRA-18139. We can move any discussions there.;;;","20/Jan/23 17:36;e.dimitrova;FYI - just for completeness, posting also here. When I started working on CASSANDRA-18139 I noticed the change is not the output, units are binary since Cassandra 3.6, CASSANDRA-9692. The change done here was to the constants names. More info - CASSANDRA-18139;;;"
Resizing window aborts cqlsh COPY: Interrupted system call,CASSANDRA-15230,13245324,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,weisslj,weisslj,weisslj,16/Jul/19 20:48,17/Feb/22 08:17,13/Jul/23 08:38,11/Jan/22 14:28,3.0.26,3.11.12,4.0.2,,,Tool/cqlsh,,,,1,,,,"When resizing a terminal window running cqlsh COPY, the Python program aborts immediately with:

{{<stdin>:1:(4, 'Interrupted system call')}}

This is very annoying, as COPY commands usually run for a long time, and e.g re-attaching to a screen session with a different terminal size aborts the command. This bug affects versions 2.1, 2.2, 3.0, 3.x, and trunk.",,bereng,fmasa,jeromatron,weisslj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/19 21:16;weisslj;15230-2.1.txt;https://issues.apache.org/jira/secure/attachment/12974874/15230-2.1.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,weisslj,,,,,,,,,,,,Degradation,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Jan 11 14:27:53 UTC 2022,,,,,,,All,,,,,"0|z04qk0:",9223372036854775807,,,,,,,bereng,brandon.williams,,,Low,,NA,,,https://github.com/apache/cassandra/commit/75e0e8cf41f8cac1a8d6abf5b19dba9968cb4316,,,,,,,,,run CI,,,,,"16/Jul/19 21:06;weisslj;CC [~stefania_alborghetti] [~brandon.williams] [~iamaleksey]

Solution is to continue ""select"" on received EINTR. Python 3.5 does this automatically: [https://docs.python.org/3/whatsnew/3.5.html#pep-475-retry-system-calls-failing-with-eintr]

Patch in [https://github.com/weisslj/cassandra/commit/7e4857dd9b].;;;","12/Aug/19 11:24;weisslj;I am unassigning myself, maybe this is the reason why nobody has commented on this yet?;;;","03/Jan/22 23:01;fmasa;I encountered the same issue on several boxes.

[~weisslj] Is there anything I could do to help get the fixed merged?;;;","05/Jan/22 18:48;brandon.williams;This LGTM.;;;","09/Jan/22 17:38;weisslj;[~fmasa] no idea how you could help, but I think [~brandon.williams] is on it, thanks! I am using this patch in production since I have submitted it.;;;","10/Jan/22 10:11;bereng;Hi [~weisslj],

at this point afaik we would expect PRs for all the affected versions + matching CI runs. I can do the CI runs for you once the PRs are up if you want.;;;","10/Jan/22 13:27;brandon.williams;There is a patch, PRs are not a requirement. I will run CI today.;;;","10/Jan/22 16:08;brandon.williams;||Branch||CI||
|[3.0|https://github.com/driftx/cassandra/tree/CASSANDRA-15230-3.0]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-15230-3.0&filter=all]|
|[3.11|https://github.com/driftx/cassandra/tree/CASSANDRA-15230-3.11]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-15230-3.11&filter=all]|
|[4.0|https://github.com/driftx/cassandra/tree/CASSANDRA-15230-4.0]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-15230-4.0&filter=all]|
|[trunk|https://github.com/driftx/cassandra/tree/CASSANDRA-15230-trunk]|[circle|https://app.circleci.com/pipelines/github/driftx/cassandra?branch=CASSANDRA-15230-trunk&filter=all]|

;;;","10/Jan/22 18:13;weisslj;[~brandon.williams] thanks a lot!;;;","10/Jan/22 19:19;brandon.williams;Looks like I broke something in the 4.0 and beyond merge.;;;","11/Jan/22 09:21;bereng;[~brandon.williams] you need to change

_readable, _, _ = select.select(self._readers, [], [], timeout)_

for

_readable, _, _ = select(self._readers, [], [], timeout)_

4.0 onward for it to work. I managed to repro/fix it locally that way. I was about to push and trigger CI but circle is not collaborating with me today so I will have to leave it to you unfortunately :(

Edit: I am fighting CI but so far:
 * 4.0 Branch is [here|https://github.com/bereng/cassandra/tree/CASSANDRA-15230-4.0] and CI [here|https://app.circleci.com/pipelines/github/bereng/cassandra?branch=CASSANDRA-15230-4.0&filter=all].
 * Trunk branch is [here|https://github.com/bereng/cassandra/tree/CASSANDRA-15230-trunk] and CI [here|https://app.circleci.com/pipelines/github/bereng/cassandra?branch=CASSANDRA-15230-trunk&filter=all]

With some of yours & mine branches I think we have a complete solution?;;;","11/Jan/22 14:27;brandon.williams;Thank you!  Everything looks good to me, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Segregate Network and Chunk Cache BufferPools and Recirculate Partially Freed Chunks,CASSANDRA-15229,13245177,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasonstack,benedict,benedict,16/Jul/19 10:07,21/Dec/20 09:29,13/Jul/23 08:38,15/Oct/20 15:17,4.0,4.0-beta3,,,,Local/Caching,,,,0,,,,"The BufferPool was never intended to be used for a {{ChunkCache}}, and we need to either change our behaviour to handle uncorrelated lifetimes or use something else.  This is particularly important with the default chunk size for compressed sstables being reduced.  If we address the problem, we should also utilise the BufferPool for native transport connections like we do for internode messaging, and reduce the number of pooling solutions we employ.
Probably the best thing to do is to improve BufferPool’s behaviour when used for things with uncorrelated lifetimes, which essentially boils down to tracking those chunks that have not been freed and re-circulating them when we run out of completely free blocks.  We should probably also permit instantiating separate {{BufferPool}}, so that we can insulate internode messaging from the {{ChunkCache}}, or at least have separate memory bounds for each, and only share fully-freed chunks.
With these improvements we can also safely increase the {{BufferPool}} chunk size to 128KiB or 256KiB, to guarantee we can fit compressed pages and reduce the amount of global coordination and per-allocation overhead.  We don’t need 1KiB granularity for allocations, nor 16 byte granularity for tiny allocations.

-----
Since CASSANDRA-5863, chunk cache is implemented to use buffer pool. When local pool is full, one of its chunks will be evicted and only put back to global pool when all buffers in the evicted chunk are released. But due to chunk cache, buffers can be held for long period of time, preventing evicted chunk to be recycled even though most of space in the evicted chunk are free.

There two things need to be improved:
1. Evicted chunk with free space should be recycled to global pool, even if it's not fully free. It's doable in 4.0.
2. Reduce fragmentation caused by different buffer size. With #1, partially freed chunk will be available for allocation, but ""holes"" in the partially freed chunk are with different sizes. We should consider allocating fixed buffer size which is unlikely to fit in 4.0.",,aleksey,benedict,colinkuo,dcapwell,djatnieks,jasonstack,jeromatron,jmckenzie,maedhroz,n.v.harikrishna,samt,saprykin,sbtourist,stefania,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-16036,CASSANDRA-16158,,,,,,,,,,,,"10/Apr/20 07:10;jasonstack;15229-count.png;https://issues.apache.org/jira/secure/attachment/12999513/15229-count.png","13/Apr/20 05:40;jasonstack;15229-direct.png;https://issues.apache.org/jira/secure/attachment/12999741/15229-direct.png","10/Apr/20 07:10;jasonstack;15229-hit-rate.png;https://issues.apache.org/jira/secure/attachment/12999514/15229-hit-rate.png","10/Apr/20 07:10;jasonstack;15229-recirculate-count.png;https://issues.apache.org/jira/secure/attachment/12999515/15229-recirculate-count.png","10/Apr/20 07:10;jasonstack;15229-recirculate-hit-rate.png;https://issues.apache.org/jira/secure/attachment/12999516/15229-recirculate-hit-rate.png","10/Apr/20 07:10;jasonstack;15229-recirculate-size.png;https://issues.apache.org/jira/secure/attachment/12999517/15229-recirculate-size.png","10/Apr/20 07:11;jasonstack;15229-recirculate.png;https://issues.apache.org/jira/secure/attachment/12999519/15229-recirculate.png","10/Apr/20 07:10;jasonstack;15229-size.png;https://issues.apache.org/jira/secure/attachment/12999518/15229-size.png","13/Apr/20 05:40;jasonstack;15229-unsafe.png;https://issues.apache.org/jira/secure/attachment/12999742/15229-unsafe.png",,,,,,,,,,,,,,,,,,,,,9.0,jasonstack,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Challenging,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Oct 15 15:17:42 UTC 2020,,,,,,,All,,,,,"0|z04pnk:",9223372036854775807,,,,,,,aleksey,maedhroz,,,Normal,,3.6,,,https://github.com/apache/cassandra/commit/699a1f74fcc1da1952da6b2b0309c9e2474c67f4,,,,,,,,,"added unit test and tested performance.

https://app.circleci.com/pipelines/github/jasonstack/cassandra/319/workflows/1a931613-d2d0-402e-b21b-058f4b8614c3

CI running: https://ci-cassandra.apache.org/job/Cassandra-devbranch/83/",,,,,"06/Nov/19 20:36;benedict;CASSANDRA-15358 highlights that we should also have different arenas for memory, or at least distinct limits even if the underlying memory pool is shared.  It should never be the case that {{ChunkCache}} can prevent network or other operations from getting memory from the pool;;;","19/Mar/20 15:31;jasonstack;It looks like current {{normal}} chunk size is already 128kb with 2kb allocation granularity and 32byte tiny allocation granularity.

[~benedict] do you plan to work on this ticket after CASSANDRA-15358? if not, I can take a stab at this ticket - seems fun and challenging..;;;","19/Mar/20 15:36;samt;bq. If we address the problem, we should also utilise the BufferPool for native transport connections like we do for internode messaging, and reduce the number of pooling solutions we employ.

Just FYI, I'm looking at this aspect of the ticket for CASSANDRA-15299;;;","24/Mar/20 08:04;jasonstack;I am planning to implement the multiple buffer pool approach on top of CASSANDRA-15358:
* Permanent Pool - used by chunk cache, hints and other disk access - bounded by file_cache_size_in_mb
* Ephemeral Pool - used by internode connection, client-server native transport - bounded by network_cache_size_in_mb, default min(128mb, system_memory/8)

bq. With these improvements we can also safely increase the BufferPool chunk size to 128KiB or 256KiB, to guarantee we can fit compressed pages and reduce the amount of global coordination and per-allocation overhead. We don’t need 1KiB granularity for allocations, nor 16 byte granularity for tiny allocations.

I am not sure if it's still necessary to increase chunk-size and min-allocation size, given they are already 128kb and 2kb..;;;","24/Mar/20 09:57;benedict;I won't be looking at this for some time, so you're welcome to have a go.

fwiw, this approach is fine but not sufficient.  The underlying implementation needs to be updated to ensure it is not wasting memory in the ""permanent"" pool, as currently it will be dramatically worse than just allocating and freeing system memory for the permanent pool.  Perhaps we should in fact consider just allocating system memory for the permanent pool.

wrt naming, transient and permanent both have the same etymology, so sound more related, even if ephemeral and permanent are technically equally good as antonyms.;;;","25/Mar/20 09:17;jasonstack;bq. The underlying implementation needs to be updated to ensure it is not wasting memory in the ""permanent"" pool, as currently it will be dramatically worse than just allocating and freeing system memory for the permanent pool. 

Good point. If the hit rate in permanent pool is low because pool is full most of the time, then it makes sense to just allocate system direct memory and avoid the overhead of accessing buffer pool.. I will run some multi-node benchmark to find out.

bq.  transient and permanent both have the same etymology, so sound more related,

I am open to change the naming.. Didn't pick ""transient"" in the first place because it is used as java key word and also used by transient replicas..;;;","01/Apr/20 15:53;stefania;We hit this buffer pool regression problem in our DSE fork a while ago. Because our chunk cache became much larger when it replaced the OS page cache, off-heap memory was growing significantly beyond the limits configured. This was partly due to some leaks, but the fragmentation in the current design of the buffer pool was a big part of it.

This is how we solved it:

 - a bump-the-pointer slab approach for the transient pool, not to dissimilar from the current implementation. We then exploit our thread per core architecture: core threads get a dedicated slab each, other threads share a global slab.

 - a bitmap-based slab approach for the permanent pool, which is only used by the chunk cache. These slabs can only issue buffers of the same size, one bit is flipped in the bitmap for each buffer issued. When multiple buffers are requested, the slab tries to issue consecutive addresses but this is not guaranteed since we want to avoid memory fragmentation. We have global lists of these slabs, sorted by buffer size where each size is a power-of-two. Slabs are taken out of these lists when they are full, and they are put back into circulation when they have space available. The lists are global but core threads get a thread-local stash of buffers, i.e. they request multiple buffers at the same time in order to reduce contention on the global lists.

We changed the chunk cache to always store buffers of the same size. If we need to read chunks of a different size, we use an array of buffers in the cache and we request multiple buffers at the same time. If we get consecutive addresses, we optimize for this case by building a single byte buffer over the first address. We also optimized the chunk cache to store memory addresses rather than byte buffers, which significantly reduced heap usage. The byte buffers are materialized on the fly.

For the permanent case, we made the choice of constraining the size of the buffers in the cache so that memory in the pool could be fully used. This may or may not be what people prefer. Our choice was due to the large size of the cache, 20+ GB. An approach that allows some memory fragmentation may be sufficient for smaller cache sizes.

Please let me know if there is interest in porting this solution to 4.0 or 4.x. I can share the code if needed.

;;;","03/Apr/20 15:23;benedict;bq. a bump-the-pointer slab approach for the transient pool, not to dissimilar from the current implementation. We then exploit our thread per core architecture: core threads get a dedicated slab each, other threads share a global slab.

The current implementation isn't really a bump the pointer allocator?  It's bitmap based, though with a very tiny bitmap.  Could you elaborate on how these work, as my intuition is that anything designed for a thread-per-core architecture probably won't translate so well to the present state of the world.  Though, either way, I suppose this is probably orthogonal to this ticket as we only need to address the {{ChunkCache}} part.

bq. We also optimized the chunk cache to store memory addresses rather than byte buffers, which significantly reduced heap usage. The byte buffers are materialized on the fly.

This would be a huge improvement, and a welcome backport if it is easy - though it might (I would guess) depend on {{Unsafe}}, which may be going away soon.  It's orthogonal to this ticket, though, I think.

bq. We changed the chunk cache to always store buffers of the same size.
bq. We have global lists of these slabs, sorted by buffer size where each size is a power-of-two.

How do these two statements reconcile?

Is it your opinion that your entire {{ChunkCache}} implementation can be dropped wholesale into 4.0?  I would assume it is still primarily multi-threaded.  If so, it might be preferable to trying to fix the existing {{ChunkCache}};;;","06/Apr/20 13:40;stefania;bq. The current implementation isn't really a bump the pointer allocator? It's bitmap based, though with a very tiny bitmap. 

Sorry it's been a while. Of course the current implementation is also bitmap based. The point is that it is not suitable for long lived buffers, similarly to our bump the pointer strategy. The transient case is easy to solve, either approach would work.

bq. Could you elaborate on how these work, as my intuition is that anything designed for a thread-per-core architecture probably won't translate so well to the present state of the world. Though, either way, I suppose this is probably orthogonal to this ticket as we only need to address the {{ChunkCache}} part.

The thread-per-core architecture makes it easy to identify threads that do most of the work and cause most of the contention. However, thread identification can be achieved also with thread pools or we can simply give all threads a local stash of buffers, provided that we return it when the thread dies. I don't think there is any other dependency on TPC beyond this. 

The design choice was mostly dictated by the size of the cache: with AIO reads the OS page cache is bypassed, and the chunk cache needs therefore to be very large, which is not the case if we use Java NIO reads or if we eventually implement asynchronous reads with the new uring API, bypassing AIO completely (which I do recommend). 

bq. We also optimized the chunk cache to store memory addresses rather than byte buffers, which significantly reduced heap usage. The byte buffers are materialized on the fly.

bq. This would be a huge improvement, and a welcome backport if it is easy - though it might (I would guess) depend on Unsafe, which may be going away soon. It's orthogonal to this ticket, though, I think

Yes it's based on the Unsafe. The addresses come from the slabs, and then we use the Unsafe to create hollow buffers and to set the address. This is an optimization and it clearly belongs to a separate ticket.

{quote}
    We changed the chunk cache to always store buffers of the same size.

    We have global lists of these slabs, sorted by buffer size where each size is a power-of-two.

How do these two statements reconcile?
{quote}

So let's assume the current workload is mostly on a table with 4k chunks, which translate to 4k buffers in the cache. Let's also assume that the workload is shifting towards another table, with 8k chunks. Alternatively, let's assume compression is ON, and an ALTER TABLE changes the chunk size. So now the chunk cache is slowly evicting 4k buffers and retaining 8k buffers. These buffers come from two different lists: the list of slabs serving 4k and the list serving 8k. Even if we collect all unused 4k slabs, until each slab has every single buffer returned, there will be wasted memory and we do not control how long that will take. To be fair, it's an extreme case, and we were perhaps over cautions in addressing this possibility by fixing the size of buffers in the cache. So it's possible that the redesigned buffer pool may work even with the current chunk cache implementation. 

bq. Is it your opinion that your entire ChunkCache implementation can be dropped wholesale into 4.0? I would assume it is still primarily multi-threaded. If so, it might be preferable to trying to fix the existing ChunkCache

The changes to the chunk cache are not trivial and should be left as a follow up for 4.x or later in my opinion. 

The changes to the buffer pool can be dropped in 4.0 if you think that:

- they are safe even in the presence of the case described above. 
- they are justified: memory wasted due to fragmentation is perhaps not an issue with a cache as little as 512 MB

I'll try to share some code so you can have a clearer picture. 
;;;","06/Apr/20 22:05;benedict;bq. memory wasted due to fragmentation is perhaps not an issue with a cache as little as 512 MB

My view is that having a significant proportion of memory wasted to fragmentation is a serious bug, irregardless of the total amount of memory that is wasted.

bq. The point is that it is not suitable for long lived buffers, similarly to our bump the pointer strategy.

It's not poorly suited to long lived buffers its it?  Only to buffers with widely divergent lifetimes.  If the lifetimes are loosely correlated then the length of the lifetime is mostly irrelevant I think.

bq. The changes to the buffer pool can be dropped in 4.0 if you think that

If you mean introducing a new pool specifically for {{ChunkCache}}. I'm fine with it as an alternative to permitting {{BufferPool}} to mitigate worst case behaviour for the {{ChunkCache}}.  But verifying a replacement for {{BufferPool}} is a lot more work, and we use the {{BufferPool}} extensively in networking now, which requires non-uniform buffer sizes.

Honestly, given chunks are normally the same size, simply re-using the evicted buffer if possible, and if not allocating new system memory, seems probably sufficient to me.

bq. I'll try to share some code so you can have a clearer picture.

Thanks, that sounds great.  I may not get to it immediately, but look forward to taking a look hopefully soon.;;;","07/Apr/20 20:16;stefania;{quote}My view is that having a significant proportion of memory wasted to fragmentation is a serious bug, irregardless of the total amount of memory that is wasted.
{quote}
 

That's absolutely true. However, it's also true that none of our users reported any problems when the cache was 512 MB and the default file access mode was mmap. Perhaps there are users in open source that reported problems, I haven't done a Jira search. So my point was simply meant to say that we should be mindful of changing critical code late in a release cycle if the existing code is performing adequately.
{quote}It's not poorly suited to long lived buffers its it? Only to buffers with widely divergent lifetimes.
{quote}
I implied the fact that lifetimes are divergent, since we're trying to support a cache, sorry about the confusion.

 
{quote}Honestly, given chunks are normally the same size, simply re-using the evicted buffer if possible, and if not allocating new system memory, seems probably sufficient to me.
{quote}
I'm not too sure that chunks are normally the same size. For data files, they depend on the compression parameters or on the partition sizes, both could be different for different tables. Also, indexes would use different chunk sizes surely? We observed that the chunk cache gradually tends to shift from buffers coming from data files to buffers coming from index files, as indexes are accessed more frequently. We have a different index implementation though.

 
{quote}{quote}I'll try to share some code so you can have a clearer picture.
{quote}
Thanks, that sounds great. I may not get to it immediately, but look forward to taking a look hopefully soon.
{quote}
I've dropped some files on this [branch|https://github.com/stef1927/cassandra/tree/15229-4.0]. The buffer pool is in org.apache.cassandra.utils.memory.buffers.  The starting point is the [BufferPool|https://github.com/apache/cassandra/compare/trunk...stef1927:15229-4.0#diff-72046b5d367f6e120594b58c973bed71R24] and its concrete implementations or the [BufferFactory|https://github.com/apache/cassandra/compare/trunk...stef1927:15229-4.0#diff-4fc5fae1de112fc5eb0bd865af532f0aR31]. I've also dropped some related utility classes but not all of them, so clearly the code doesn't compile and the unit tests are also missing.

 ;;;","07/Apr/20 22:18;benedict;{quote}That's absolutely true. However, it's also true that none of our users reported any problems when the cache was 512 MB...  So my point was simply meant to say that we should be mindful of changing critical code late in a release cycle if the existing code is performing adequately.
{quote}
There has been at least one report, I'm fairly sure, but it is besides the point IMO.  I personally don't endorse this adequacy criterion: we've done a lot of inadequate things users haven't noticed.

As also mentioned in other fora (perhaps not this ticket), the situation will be much worse in 4.0, since the default page size has shrunk and the {{BufferPool}} page size has grown.  These changes are very likely to lead to a minority of assigned memory being usable, and a majority being served from system memory.  In which case we might as well just do that for everything.  This is a simple solution with similar performance characteristics and no wasted memory.  Given the cache fronts decompression the advantage of an internal memory pool here was anyway surely always fairly minimal?  Was it ever demonstrated to be beneficial?

An alternative is perhaps to remove the {{ChunkCache}} entirely, or at least deprecate it, or mark it ""experimental""
{quote}Also, indexes would use different chunk sizes surely?
{quote}
Perhaps, but should we be caching indexes at all?  I thought the purpose of the chunk cache was for compressed data that has been uncompressed, and I'm fairly sure we don't compress indexes?  Since it is probably of limited value for data that is already uncompressed, versus the file system cache.  But I have not looked closely at any of the {{ChunkCache;}} only close enough  to notice the problematic behaviour mentioned in this ticket.
{quote}I've dropped some files on this [branch|https://github.com/stef1927/cassandra/tree/15229-4.0].
{quote}
Thanks, that's greatly appreciated, though I won't be able to take a look for a couple of weeks at least unfortunately.;;;","10/Apr/20 07:46;jasonstack;Discussed with [~stefania] offline, there are two issues with buffer pool:
 * Chunk cache holds a piece of buffer preventing entire chunk from recycling for arbitrary period.
 * Even if we recirculate the partially freed chunk, due to different allocation sizes, fragmentation will reduce utilization. That's why forked version uses uniform allocation size.

The first issue should be solvable and less risky for 4.0.. Here is the performance comparison against recirculating partially freed chunk.

Setup: single node 16T - 8GB heap - 250m rows - mixed read 40k qps - write 10k qps - with 128 file cache
 [baseline|https://github.com/jasonstack/cassandra/pull/8]: initiate 2 buffer pools, one for chunk cache, one for network.
 [recirculate-partially-freed-chunk|https://github.com/jasonstack/cassandra/pull/11/files]: baseline + partially freed chunk recirculation.

baseline:
| !15229-hit-rate.png|height=400,width=400! | !15229-count.png|height=400,width=400! |

recirculation: 
| !15229-recirculate-hit-rate.png|height=400,width=400! | !15229-recirculate-count.png|height=400,width=400! |

QPS:  
!15229-recirculate.png|height=600,width=600! 

With partially freed chunk recirculation, latency is improved and buffer pool misses are reduced..

Should we proceed with recirculating partially freed chunk + a separate pool for network cache in 4.0 and then port forked buffer pool with uniform allocation size in 4.x?;;;","10/Apr/20 14:38;benedict;Hi [~jasonstack], I'm on leave at the moment, so I cannot properly review your patch.  This was in the vicinity of what I had originally been considering, however I see one potential problem with this specific instantiation of the approach, and wonder anyway if we shouldn't take a different tack:

Recirculating immediately will lead to greater inefficiency in allocation, as we will attempt to reuse partially freed chunks in preference to entirely freed chunks, leading to a great deal more churn in the active blocks. This will affect the networking pooling as much as the chunk cache.  At the very least this behaviour should be enabled only for the {{ChunkCache}}, but ideally might have e.g. two queues, one with guaranteed-free chunks, another (perhaps for ease a superset) containing those chunks that might or mightn't be free.

This also isn't a _trivial_ behavioural change, and I continue to wonder if using {{Unsafe.allocateMemory}} wouldn't be simpler, more efficient, less risky and produce less fragmentation.;;;","13/Apr/20 06:02;jasonstack;{quote}
Recirculating immediately will lead to greater inefficiency in allocation, as we will attempt to reuse partially freed chunks in preference to entirely freed chunks, leading to a great deal more churn in the active blocks. This will affect the networking pooling as much as the chunk cache.
{quote}

In networking, most of the time, buffer will be release immediately after allocation and  with {{recycleWhenFree=false}}, fully freed chunk will be reused instead of being recycled to global list. Partial-recycle is unlikely affect networking usage. I am happy to test it..

{quote}
 At the very least this behaviour should be enabled only for the ChunkCache, but ideally might have e.g. two queues, one with guaranteed-free chunks, another (perhaps for ease a superset) containing those chunks that might or mightn't be free.
{quote}

It's a good idea to have a separate queue and let partially freed chunk to have lower priority than fully freed chunk. So partially freed chunks will likely have larger freed space comparing to reusing them immediately.

{quote}if using Unsafe.allocateMemory wouldn't be simpler, more efficient, less risky and produce less fragmentation.
{quote}

It is simpler, but not efficient.. Without slab allocation, will it create fragmentation in system direct memory? 

I tested with ""Bytebuffer#allocateDirect"" and ""Unsafe#allocateMemory"", both latencies are slightly worse than baseline. 

btw, I think it'd be nice to add a new metrics to track direct bytebuffer allocation outside of buffer pool because they may be held by chunk cache for a long time.

Chunk cache with [Bytebuffer.allocateDirect|https://github.com/jasonstack/cassandra/commit/c3f286c1148d13f00364872413733822a4a2c475]:
 !15229-direct.png|width=600,height=400!

Chunk cache with [Unsafe.allocateMemory|https://github.com/jasonstack/cassandra/commit/3dadd884ff0d8e19d3dd46a07a290762755df312]:
 !15229-unsafe.png|width=600,height=400!;;;","13/Apr/20 10:04;benedict;bq. In networking, most of the time, buffer will be release immediately after allocation and with recycleWhenFree=false, fully freed chunk will be reused instead of being recycled to global list. Partial-recycle is unlikely affect networking usage. I am happy to test it..

It is famously difficult to prove a negative, particularly via external testing.  It will be untrue in some circumstances, most notably large message processing (which happens asynchronously).  I would need to review the buffer control flow in messaging to confirm it is sufficiently low risk to modify the behaviour here, so I would prefer we not modify it in a way that is not easily verified.

bq. will it create fragmentation in system direct memory?

-Not easily completely ruled out, but given this data will be allocated mostly in its own virtual page space (given all allocations are much larger than a normal page), it hopefully shouldn't be an insurmountable problem for most allocators given the availability of almost unlimited virtual page space on modern systems.-

edit: while this may be true, it's a bit of a stretch as I haven't looked at any modern allocator remotely recently, and I should not extrapolate in this way (however it's anyway probably not something to worry about if we're allocating relatively regular sizes)

bq. I tested with ""Bytebuffer#allocateDirect"" and ""Unsafe#allocateMemory"", both latencies are slightly worse than baseline.

Did you perform the simple optimisation of rounding up to the >= 2KiB boundary (for equivalent behaviour), then re-using any buffer that is correctly sized when evicting to make room for a new item?  It might well be possible to make this yet more efficient than {{BufferPool}} by reducing this boundary to e.g. 1KiB, or perhaps as little as 512B.

So if I were doing this myself, I think I would be starting at this point and if necessary would move towards further reusing the buffers we already have in the cache - since it is already a pool of them.  I would just be looking to smooth out the random distribution of sizes used with e.g. a handful of queues each containing a single size of buffer and at most a handful of items each.  This feels like a simpler solution to me, particularly as it does not affect any other pool users.

However, I’m not doing the work (nor maybe reviewing it), so if you are willing to at least enable the behaviour only for the ChunkCache so this change cannot have any unintended negative effect for those users not expected to benefit, my main concern will be alleviated.
;;;","06/May/20 20:10;jmckenzie;If it's not too much bother, could we update the Since Version on this and add a little detail in the description as to the impact of this regression on the system?;;;","11/May/20 16:06;jasonstack;{quote}I would just be looking to smooth out the random distribution of sizes used with e.g. a handful of queues each containing a single size of buffer and at most a handful of items each.
{quote}
It looks simpler in its initial form, but I am wondering whether it will eventually grow/evolve into another buffer pool.
{quote}so if you are willing to at least enable the behaviour only for the ChunkCache so this change cannot have any unintended negative effect for those users not expected to benefit, my main concern will be alleviated.
{quote}
+1, partially freed chunk recirculation is only enabled for permanent pool, not for temporary pool.
----
[Patch|https://github.com/apache/cassandra/pull/535/files] / [Circle|https://circleci.com/workflow-run/096afbe1-ec99-4d5f-bdaa-06f538b8280f]:
 * Initialize 2 buffer pool instances, one for chunk cache (default 512mb) called {{""Permanent Pool""}}, one for network (default 128mb) called {{""Temporary Pool""}}. So they won't interfere each other.
 * Improve buffer pool metrics to track:
 ** {{""overflowSize""}} - buffer size that is allocated outside of buffer pool.
 ** {{""UsedSize""}} - buffer size that is currently being allocated.
 * Allow partially freed chunk to be recycled in Permanent Pool to improve cache utilization due to chunk cache holding buffer for arbitrary time period. Note that due to various allocation sizes, fragmentation still exists in partially freed chunk.;;;","23/Jul/20 13:52;aleksey;Would be great to have a second reviewer here (I volunteer to be one of the two).;;;","23/Jul/20 13:55;benedict;I can, but probably not for a few weeks yet.;;;","30/Sep/20 14:43;aleksey;Thanks for fixing the test issues in the past couple commits (and sorry for the delay in review).

One thing I'm not a fan of is names of the two pools - permanent and temporary - as neither describe their respective pools. Something along the lines of 'long lived' and 'short-lived' would work better. Or, perhaps, name them after their use cases - 'chunk-cache' and 'networking' pools.

Other than that:

1. {{PermanentBufferPool}} - unused class
2. {{Chunk#fullyRecycled}} is never read, only written to
3. {{putUnusedPortion()}} probably shouldn’t update overflow metric, as this will double-count some of the size when it’s {{put()}} back
4. nit: {{else if}} on L807 doesn’t need a pair of braces for the first two conditions
;;;","30/Sep/20 15:24;maedhroz;I'm still in the middle of my review, but just want to +1 something like {{chunkCache}} and {{networking}} for the names of the pools.;;;","30/Sep/20 16:16;benedict;Ditto;;;","30/Sep/20 21:36;maedhroz;Finished my review, and dropped my comment inline in the PR.

Looking at the larger picture of this issue, CASSANDRA-16036, and our general goal of making sure 4.0 does not have any egregious performance issues around the chunk cache, it seems like the most pressing thing in front of us is making sure compaction doesn't absolutely trash the chunk cache. [~dcapwell] seems to have [documented this|https://issues.apache.org/jira/browse/CASSANDRA-16036?focusedCommentId=17173291&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17173291] pretty clearly, and both tests done in that issue and Fallout tests done by [~jasonstack] would be capable of verifying the effectiveness of any changes around this. The question is whether we should work through that here or as part of another issue.

I don't know the compaction code super well, but it seems like we could avoid most of the cache churn mess by having the {{ISSTableScanner}} implementations returned by {{SSTableReader#getScanner()}} use file handles that don't use {{CachingRebufferer}}. {{FileHandle.Builder#complete()}} already seems to roughly have the logic we would need to produce the correct (uncached) {{RebuffererFactory}}. If it's that simple, and we've already got a performance testing scaffolding set up here, perhaps it would make sense to roll into this Jira...;;;","30/Sep/20 21:59;dcapwell;bq. The question is whether we should work through that here or as part of another issue.

I am 100% fine tackling that in a different issue.  Looking at the first line in the description ""The BufferPool was never intended to be used for a ChunkCache, and we need to either change our behaviour to handle uncorrelated lifetimes or use something else"", the separate pools does isolate the issue so networking isn't impacted by the chunk cache; this gives breathing room to figure scope of chunk cache work.;;;","30/Sep/20 22:28;maedhroz;I'm okay with another issue as well, but the issue title here is pretty broad ;)

(Also looking for feedback on the last paragraph of my last comment...);;;","05/Oct/20 13:38;aleksey;LGTM with most recent feedback addressed.;;;","05/Oct/20 19:11;maedhroz;[~jasonstack] The only thing left to resolve seems like the discussion [here|https://github.com/apache/cassandra/pull/535#discussion_r497796381]. Otherwise, LGTM;;;","15/Oct/20 15:17;jasonstack;thanks for the review and feedback, merged to [trunk|https://github.com/apache/cassandra/commit/699a1f74fcc1da1952da6b2b0309c9e2474c67f4];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FileUtils.close() does not handle non-IOException,CASSANDRA-15225,13245171,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Override,benedict,benedict,16/Jul/19 10:01,15/May/20 08:01,13/Jul/23 08:38,07/Aug/19 12:11,2.2.15,3.0.19,3.11.5,4.0,4.0-alpha1,Local/SSTable,,,,0,pull-request-available,,,This can lead to {{close}} not being invoked on remaining items,,benedict,cnlwsu,jeromatron,n.v.harikrishna,Override,,,,,,,,,,,,,,,,,,,,,,,"kornilova-l commented on pull request #332: CASSANDRA-15225 Handle all exceptions in FileUtils.close()
URL: https://github.com/apache/cassandra/pull/332
 
 
   see [CASSANDRA-15225](https://issues.apache.org/jira/browse/CASSANDRA-15225)
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/19 08:32;githubbot;600","belliottsmith commented on pull request #332: CASSANDRA-15225 Handle all exceptions in FileUtils.close()
URL: https://github.com/apache/cassandra/pull/332
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;07/Aug/19 11:57;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,Override,,,,,,,,,,,,Degradation -> Resource Management,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Aug 07 11:11:16 UTC 2019,,,,,,,All,,,,,"0|z04pm8:",9223372036854775807,,,,,,,benedict,,,,Low,,1.0.0,,,"[f21106fcd0e5d870cf9d85b2d396eab9fe4515cd|https://github.com/apache/cassandra/commit/f21106fcd0e5d870cf9d85b2d396eab9fe4515cd]",,,,,,,,,unnecessary,,,,,"18/Jul/19 20:15;n.v.harikrishna;[~Override] Instead of delivering last exception as IOException, can we use {{Throwable.addSuppressed()}} ?

cc: [~benedict];;;","19/Jul/19 07:31;Override;Hi [~n.v.harikrishna],

Done

All IOExceptions except first one are also added to suppressed (see [updated commit|https://github.com/apache/cassandra/pull/332/files]);;;","31/Jul/19 18:32;n.v.harikrishna;1) Insteasd of modifying already thrown exception, better to create new exception and call _newException_.addSuppressed. When the caller calls _newException_.getSuppressed, accurate list exceptions are returned.

2) Log statement has only one '{}'. I don't think it will print exception. I prefer caller to handle the logging part.;;;","01/Aug/19 09:35;benedict;{quote}2) Log statement has only one '{}'. I don't think it will print exception. I prefer caller to handle the logging part.
{quote}
This (the way [~Override] has it) is the correct way to print exceptions; otherwise only their {{toString()}} method is invoked.
{quote}1) Insteasd of modifying already thrown exception, better to create new exception and call _newException_.addSuppressed. When the caller calls _newException_.getSuppressed, accurate list exceptions are returned.
{quote}
I don't think it is correct to add all exceptions with {{addSuppressed}}.  

It _is_ the case that often a new exception is created in any place exceptions are caught, so that the re-throwing method is captured in the trace, however you would always want a {{cause}} in preference to adding a member of {{getSuppressed}}.  

However, I don't think this anyway always adds value.  If I were personally writing it, I would likely have done it the way [~Override] has done.  There's an alternative, which is to create a new exception whose {{cause}} is the first caught exception, and to {{addSuppressed}} any remaining exceptions to this new exception.  But I think it's preferable to rethrow the original exception here, to avoid polluting the printed stack trace, and minimise the risk that ""helpful"" tools truncate some of this trace

[~Override]: LGTM, I will merge it alongside your other patch and some others I will be reviewing hopefully today or tomorrow - unless you want to make any changes in light of [~n.v.harikrishna]'s feedback?;;;","05/Aug/19 08:24;Override;{quote}2) Log statement has only one '{}'. I don't think it will print exception. I prefer caller to handle the logging part.
{quote}
I checked this. An exception is extracted here ch.qos.logback.classic.spi.LoggingEvent:49 and it is printed
{quote}unless you want to make any changes in light of [~n.v.harikrishna]'s feedback?
{quote}
I decided to change the code such that new exception is created. 
 I see 2 advantages of this version:
 1. {{Throwable}} will not be lost if it appears before {{IOException}} or {{IOException}} does not appear at all. {{Throwable}} will be rethrown as IOException with cause.
 2. Catch clauses become identical and they can be collapsed to one {{Throwable}} clause

I pushed a separate commit so you can see the difference. I can squash commits if you wish;;;","06/Aug/19 16:54;benedict;Thanks for the patch [~Override].

There's a third option, namely accumulating the throwable in a {{Throwable}} variable, and maintaining the single catch clause.  We have a utility method, {{Throwables.maybeFail}} that takes a checked exception class, and rethrows the exception as its checked-type if possible, or as an unchecked type if possible, and otherwise wraps it in a {{RuntimeException}}.

Does that sound reasonable to you?

;;;","07/Aug/19 10:36;Override;Thank you for reviewing the code, [~benedict]

It sounds like a better approach because some {{Throwables}} should not be re-thrown as checked (e.g. {{Error}} is a ""serious problems that a reasonable application should not try to catch""). Also no additional wrappers

I pushed third commit;;;","07/Aug/19 10:46;benedict;Thanks [~Override].  One tiny nit: {{maybeFail}} doesn't require the null check, you can simply invoke it, and if the parameter is {{null}} nothing will happen.

If you want to prepare the final patch for commit, by squashing, adding a CHANGES.txt entry, and adding a commit message of the form described on CASSANDRA-15246, I'll commit it to trunk.

Welcome to the contributor community!;;;","07/Aug/19 11:11;Override;[~benedict], done!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross node message creation times are disabled by default,CASSANDRA-15216,13245159,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,benedict,benedict,16/Jul/19 09:37,15/May/20 08:54,13/Jul/23 08:38,15/Jan/20 22:39,4.0,4.0-alpha3,,,,Messaging/Internode,,,,0,,,,"This can cause a lot of wasted work for messages that have timed out on the coordinator.  We should generally assume that our users have setup NTP on their clusters, and that clocks are modestly in sync, since it’s a requirement for general correctness of last write wins.",,aleksey,benedict,brandon.williams,e.dimitrova,jeromatron,n.v.harikrishna,stillalex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/20 22:04;e.dimitrova;Screen Shot 2020-01-15 at 5.01.06 PM.png;https://issues.apache.org/jira/secure/attachment/12991031/Screen+Shot+2020-01-15+at+5.01.06+PM.png","15/Jan/20 22:04;e.dimitrova;Screen Shot 2020-01-15 at 5.01.33 PM.png;https://issues.apache.org/jira/secure/attachment/12991032/Screen+Shot+2020-01-15+at+5.01.33+PM.png","15/Jan/20 22:04;e.dimitrova;Screen Shot 2020-01-15 at 5.02.22 PM.png;https://issues.apache.org/jira/secure/attachment/12991033/Screen+Shot+2020-01-15+at+5.02.22+PM.png",,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,e.dimitrova,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,Docs,,Thu Jan 16 14:47:23 UTC 2020,,,,,,,All,,,,,"0|z04pjk:",9223372036854775807,,,,,,,brandon.williams,,,,Normal,,4.0,,,https://github.com/apache/cassandra/commit/9d2ffad6b6d09761a03aeb1a207e9780d1174046,,,,,,,,,Documented in NEWS.txt,,,,,"31/Aug/19 12:59;stillalex;I'd be interested in providing a patch. Could someone provide some pointers?;;;","02/Sep/19 10:14;benedict;Thanks for the offer.  This is a two word change, though (from {{false}} to  {{true}}, in {{cassandra.yaml}} and {{Config}}), and probably too trivial to even bother with submission and review; it can probably be ninja'd in.  The only requirement is a brief discussion to confirm nobody disagrees this should happen.;;;","09/Jan/20 21:42;e.dimitrova;[~benedict] The proposed change and reasoning looks good to me.

[~aleksey] [~ifesdjeen] [~JoshuaMcKenzie]  any thoughts?;;;","10/Jan/20 14:17;aleksey;[~e.dimitrova] go for it. We could have a wider discussion, but it's hard for me to imagine why someone would object to this default switch.

 

Just make sure to update NEWS.txt to note that the change has taken place.;;;","15/Jan/20 22:09;e.dimitrova;Patch available for trunk [here|https://github.com/ekaterinadimitrova2/cassandra/tree/trunk-CASSANDRA-15216]

[Pull request|https://github.com/ekaterinadimitrova2/cassandra/pull/17]

Screenshots from the CI ran attached. If you look at the failures of ""test all"", there are some which I don't see when I am running CI on trunk but most of them are marked as flaky. I think it should be good. NEWS.txt updated as agreed earlier. ;;;","16/Jan/20 13:23;aleksey;Looks good to me (except you want to trim your CHANGES.txt entry to one shortish line preferably). If Brandon is a bit busy, I can +1 and commit myself - just lmk on Slack or here. Cheers.;;;","16/Jan/20 14:47;brandon.williams;I committed this yesterday, but did not trim the CHANGES.txt entry.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VIntCoding should read and write more efficiently,CASSANDRA-15215,13245158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Gerrrr,benedict,benedict,16/Jul/19 09:36,11/Nov/22 19:00,13/Jul/23 08:38,02/Feb/22 12:20,4.1,4.1-alpha1,,,,Local/Compaction,Local/SSTable,,,0,,,,"Most vints occupy significantly fewer than 8 bytes, and most buffers have >= 8 bytes spare, in which case we can construct the relevant bytes in a register and memcpy them to the correct position.  Since we read and write a lot of vints, this waste is probably measurable, particularly during compaction and flush, and can probably be considered a performance bug.",,aweisberg,benedict,blambov,blerer,Gerrrr,jeromatron,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,"Gerrrr opened a new pull request #1343:
URL: https://github.com/apache/cassandra/pull/1343


   In the cases where VInt occupies less than or equal to 8 bytes
   and the underlying buffer has at least 8 bytes, VIntCoding writes the
   entire register in a single operation and then adjusts the buffer position.
   
   Co-authored-by: Benedict Elliott Smith <benedict@apache.org>
   Co-authored-by: Branimir Lambov <branimir.lambov@datastax.com>


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/21 09:28;githubbot;600","Gerrrr opened a new pull request #1346:
URL: https://github.com/apache/cassandra/pull/1346


   In the cases where VInt occupies less than or equal to 8 bytes
   and the underlying buffer has at least 8 bytes, VIntCoding writes the
   entire register in a single operation and then adjusts the buffer position.
   
   Co-authored-by: Benedict Elliott Smith <benedict@apache.org>
   Co-authored-by: Branimir Lambov <branimir.lambov@datastax.com>


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/21 09:28;githubbot;600","Gerrrr opened a new pull request #1344:
URL: https://github.com/apache/cassandra/pull/1344


   In the cases where VInt occupies less than or equal to 8 bytes
   and the underlying buffer has at least 8 bytes, VIntCoding writes the
   entire register in a single operation and then adjusts the buffer position.
   
   Co-authored-by: Benedict Elliott Smith <benedict@apache.org>
   Co-authored-by: Branimir Lambov <branimir.lambov@datastax.com>


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/21 09:28;githubbot;600","Gerrrr opened a new pull request #1345:
URL: https://github.com/apache/cassandra/pull/1345


   In the cases where VInt occupies less than or equal to 8 bytes
   and the underlying buffer has at least 8 bytes, VIntCoding writes the
   entire register in a single operation and then adjusts the buffer position.
   
   Co-authored-by: Benedict Elliott Smith <benedict@apache.org>
   Co-authored-by: Branimir Lambov <branimir.lambov@datastax.com>


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;26/Nov/21 09:28;githubbot;600","smiklosovic closed pull request #1346:
URL: https://github.com/apache/cassandra/pull/1346


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 08:43;githubbot;600","smiklosovic closed pull request #1344:
URL: https://github.com/apache/cassandra/pull/1344


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 08:43;githubbot;600","smiklosovic closed pull request #1345:
URL: https://github.com/apache/cassandra/pull/1345


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 08:43;githubbot;600","smiklosovic closed pull request #1343:
URL: https://github.com/apache/cassandra/pull/1343


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 08:43;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/21 09:31;Gerrrr;testWriteRandomLongDOP_final.png;https://issues.apache.org/jira/secure/attachment/13036644/testWriteRandomLongDOP_final.png","16/Nov/21 16:18;Gerrrr;writeUnsignedVInt_megamorphic_BB.png;https://issues.apache.org/jira/secure/attachment/13036159/writeUnsignedVInt_megamorphic_BB.png","16/Nov/21 16:18;Gerrrr;writeUnsignedVInt_megamorphic_DOP.png;https://issues.apache.org/jira/secure/attachment/13036160/writeUnsignedVInt_megamorphic_DOP.png",,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,Gerrrr,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Nov 11 19:00:05 UTC 2022,,,,,,,All,,,,,"0|z04pjc:",9223372036854775807,,,,,,,benedict,blambov,,,Normal,,4.1,,,"[35dbcc2c2dbe1c826fd6ecd6e8357f0f5a9bab02|https://github.com/apache/cassandra/commit/35dbcc2c2dbe1c826fd6ecd6e8357f0f5a9bab02], [5cd012736e4680bbb25928ee7fbcea4859878fff|https://github.com/apache/cassandra/commit/5cd012736e4680bbb25928ee7fbcea4859878fff]",,,,,,,,,Added unit tests for new methods and benchmarks to show performance improvements.,,,,,"31/Oct/21 17:27;Gerrrr;[~benedict] if I understood your idea correctly, you suggest writing relevant bytes directly instead of preparing the thread-local byte array and memcpy'ing it into the given buffer. Is my interpretation correct? In the title and description you also mentioned reads, but I haven't figured how to adjust this idea there, so here are results for writes only.

h2. Code
||Branch||Description||
|[baseline|https://github.com/apache/cassandra/compare/trunk...Gerrrr:15215-baseline-trunk?expand=1]|trunk + benchmark|
|[patch|https://github.com/apache/cassandra/compare/trunk...Gerrrr:15215-trunk?expand=1]|patch + benchmark|
h2. Setup

Each benchmark does {{VIntCoding.writeUnsignedVInt}} on a {{long}} from 1 to 9 bytes. The write target is a {{ByteBuffer}} - both on- and off- heap.

The results are produced on a MBP 2019 - 2,3 GHz 8-Core Intel Core i9.
h2. Results
h3. Baseline
{noformat}
Benchmark                        (allocation)  Mode  Cnt   Score   Error  Units
VIntCodingBench.testWrite1Byte           HEAP  avgt   15   9.084 ± 5.196  ns/op
VIntCodingBench.testWrite1Byte         DIRECT  avgt   15   5.037 ± 0.638  ns/op
VIntCodingBench.testWrite2Bytes          HEAP  avgt   15  15.604 ± 0.646  ns/op
VIntCodingBench.testWrite2Bytes        DIRECT  avgt   15  15.028 ± 0.568  ns/op
VIntCodingBench.testWrite3Bytes          HEAP  avgt   15  16.704 ± 0.461  ns/op
VIntCodingBench.testWrite3Bytes        DIRECT  avgt   15  17.410 ± 0.489  ns/op
VIntCodingBench.testWrite4Bytes          HEAP  avgt   15  17.086 ± 0.527  ns/op
VIntCodingBench.testWrite4Bytes        DIRECT  avgt   15  20.307 ± 0.705  ns/op
VIntCodingBench.testWrite5Bytes          HEAP  avgt   15  17.395 ± 0.578  ns/op
VIntCodingBench.testWrite5Bytes        DIRECT  avgt   15  17.558 ± 0.512  ns/op
VIntCodingBench.testWrite6Bytes          HEAP  avgt   15  18.114 ± 0.967  ns/op
VIntCodingBench.testWrite6Bytes        DIRECT  avgt   15  19.023 ± 0.591  ns/op
VIntCodingBench.testWrite7Bytes          HEAP  avgt   15  18.004 ± 0.298  ns/op
VIntCodingBench.testWrite7Bytes        DIRECT  avgt   15  19.081 ± 0.601  ns/op
VIntCodingBench.testWrite8Bytes          HEAP  avgt   15  18.466 ± 0.463  ns/op
VIntCodingBench.testWrite8Bytes        DIRECT  avgt   15  20.228 ± 5.620  ns/op
VIntCodingBench.testWrite9Bytes          HEAP  avgt   15  18.553 ± 0.537  ns/op
VIntCodingBench.testWrite9Bytes        DIRECT  avgt   15  20.101 ± 0.476  ns/op
{noformat}
h3. Patch
{noformat}
Benchmark                        (allocation)  Mode  Cnt   Score   Error  Units
VIntCodingBench.testWrite1Byte           HEAP  avgt   15   4.728 ± 0.077  ns/op
VIntCodingBench.testWrite1Byte         DIRECT  avgt   15   6.415 ± 3.157  ns/op
VIntCodingBench.testWrite2Bytes          HEAP  avgt   15   8.244 ± 0.440  ns/op
VIntCodingBench.testWrite2Bytes        DIRECT  avgt   15   9.136 ± 3.979  ns/op
VIntCodingBench.testWrite3Bytes          HEAP  avgt   15   8.714 ± 0.134  ns/op
VIntCodingBench.testWrite3Bytes        DIRECT  avgt   15   9.690 ± 2.735  ns/op
VIntCodingBench.testWrite4Bytes          HEAP  avgt   15   8.634 ± 0.164  ns/op
VIntCodingBench.testWrite4Bytes        DIRECT  avgt   15   6.830 ± 0.061  ns/op
VIntCodingBench.testWrite5Bytes          HEAP  avgt   15   8.389 ± 0.207  ns/op
VIntCodingBench.testWrite5Bytes        DIRECT  avgt   15   8.059 ± 1.537  ns/op
VIntCodingBench.testWrite6Bytes          HEAP  avgt   15  10.861 ± 0.336  ns/op
VIntCodingBench.testWrite6Bytes        DIRECT  avgt   15   9.816 ± 1.482  ns/op
VIntCodingBench.testWrite7Bytes          HEAP  avgt   15  11.045 ± 0.419  ns/op
VIntCodingBench.testWrite7Bytes        DIRECT  avgt   15  10.702 ± 2.377  ns/op
VIntCodingBench.testWrite8Bytes          HEAP  avgt   15  10.375 ± 0.423  ns/op
VIntCodingBench.testWrite8Bytes        DIRECT  avgt   15   7.237 ± 0.176  ns/op
VIntCodingBench.testWrite9Bytes          HEAP  avgt   15  11.200 ± 0.365  ns/op
VIntCodingBench.testWrite9Bytes        DIRECT  avgt   15   8.152 ± 0.282  ns/op
{noformat};;;","31/Oct/21 17:55;benedict;Hi [~Gerrrr],

Thanks for picking this up, but what you've proposed in the patch isn't quite what I had in mind. The premise is to introduce a new method to {{DataOutputPlus}} that is able to write up to 8 bytes from a long, so it would be something like {{putBytes(long register, int bytes)}} where {{bytes<=8}}. If there are more than 8 bytes left in the underlying storage then {{putLong(offset, register)}} is invoked on any underlying {{ByteBuffer}}, and the offset counter is incremented only by {{bytes}}.

This same approach can of course be performed directly to a {{ByteBuffer}}, and symmetrically for {{readBytes(bytes)}} with {{DataInputPlus}}.

This way the vint can be assembled and inserted using simple register instructions while there is >= 8 bytes in the underlying storage buffer, i.e. the majority of instances. Otherwise we could fall back to this suggested approach or any other.;;;","31/Oct/21 18:26;benedict;Worth noting that the relative performance of the {{ByteBuffer}} variants is probably harder to predict, and depends a lot on how smart HotSpot is, and the morphism of the call-sites.

Also, your approach already appears to be an improvement over the status quo which is great! Though it might be less impressive for {{DataOutput}} due to the greater complexity of implementation for their corresponding methods. I think it is _likely_ that they will all be monomorphic call-sites which should mitigate it, but they may have to potentially invoke {{flush()}} multiple times which at minimum leads to much larger methods.;;;","31/Oct/21 19:10;Gerrrr;Hey [~benedict]! Thank you so much for a quick and elaborate answer! 

As a next step, I am going to extend the benchmark for implementations of {{DataOutput}} to estimate how well my patch works there. After that, I will extend {{DataOutputPlus}} and {{DataInputPlus}} as you suggested. 

As I am working on this patch in my free time, it might take a bit. I hope to provide an update by the end of next week.;;;","02/Nov/21 09:48;blerer;This ticket might be a good place to also improve the performance of the [computeUnsignedVIntSize|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/vint/VIntCoding.java#L279] method which relies on a division operation. 
[~blambov] mentioned to me that: 
{code}
return 9 - ((magnitude - 1) / 7);
{code}
could be replaced by
{code}
return (639 - magnitude * 9) >> 6;
{code}
which produce the same result and avoid the division high cost in term of CPU time.  ;;;","02/Nov/21 10:04;Gerrrr;Thank you for the suggestion! I will add a benchmark to see if there is measurable difference.;;;","02/Nov/21 16:15;blambov;A small suggestion: we also need a benchmark that randomly chooses a length, so that the branch doesn't always hit the same target and isn't predictable. Because in our code we call this encoding for many different types of values, I expect unpredictable length to be common.

Also, because we are switching on the exact value of {{size}}, it makes sense to put the mask explicitly in the specific cases. To me that also makes it a bit easier to read.

If you do use the magic numbers above, please add a comment that they are hand-picked to match {{9 - ((magnitude - 1) / 7)}} for {{magnitude}} between 0 and 63.;;;","02/Nov/21 16:19;benedict;If we're improving the benchmark we might also want to consider increasing the morphism of the call sites, i.e. randomly supplying either a {{DirectByteBuffer}} or {{HeapByteBuffer}}, and maybe even a {{MappedByteBuffer}} to make it megamorphic.

Preferably we would have three benchmarks, namely monomorphic (HeapByteBuffer only), bimorphic (HeapByteBuffer and DirectByteBuffer selected randomly), megamorphic (HBB, DBB and MappedByteBuffer))

The {{DataOutput}} variants of the test can similarly be updated to pick two or three variants.

FWIW, my expectation is that there will be no relative difference between approaches at different levels of morphism, but HotSpot _might_ make poor decisions particularly for the above {{ByteBuffer}} approach. We could also just check the byte code, but that is only a snapshot of HotSpot's decision-making, and its validity depends on us not changing the code to break it (not that we unfortunately currently monitor these benchmarks for regressions).;;;","07/Nov/21 18:44;Gerrrr;Short status update: I just pushed changes to the benchmarks as suggested by Branimir and Benedict. The changes are:
 * All tests have monomorphic, bimorphic, and megamorphic versions.
 * Added a test that writes random longs to the ByteBuffer.
 * Added a test for calculating VInt size and applied Branimir's formula.

Results:
h4. Baseline
{noformat}
Benchmark                                    (allocation)  Mode  Cnt   Score   Error  Units
VIntCodingBench.testComputeUnsignedVIntSize   monomorphic  avgt   15  17.069 ± 1.087  ns/op
VIntCodingBench.testComputeUnsignedVIntSize     bimorphic  avgt   15  17.323 ± 0.656  ns/op
VIntCodingBench.testComputeUnsignedVIntSize   megamorphic  avgt   15  16.791 ± 0.473  ns/op
VIntCodingBench.testWrite1Byte                monomorphic  avgt   15   9.047 ± 0.254  ns/op
VIntCodingBench.testWrite1Byte                  bimorphic  avgt   15  16.935 ± 0.207  ns/op
VIntCodingBench.testWrite1Byte                megamorphic  avgt   15  17.835 ± 0.090  ns/op
VIntCodingBench.testWrite2Bytes               monomorphic  avgt   15  18.612 ± 0.194  ns/op
VIntCodingBench.testWrite2Bytes                 bimorphic  avgt   15  25.033 ± 0.239  ns/op
VIntCodingBench.testWrite2Bytes               megamorphic  avgt   15  28.352 ± 0.115  ns/op
VIntCodingBench.testWrite3Bytes               monomorphic  avgt   15  21.333 ± 0.197  ns/op
VIntCodingBench.testWrite3Bytes                 bimorphic  avgt   15  26.173 ± 0.170  ns/op
VIntCodingBench.testWrite3Bytes               megamorphic  avgt   15  29.983 ± 0.208  ns/op
VIntCodingBench.testWrite4Bytes               monomorphic  avgt   15  21.229 ± 0.245  ns/op
VIntCodingBench.testWrite4Bytes                 bimorphic  avgt   15  28.966 ± 0.606  ns/op
VIntCodingBench.testWrite4Bytes               megamorphic  avgt   15  33.219 ± 1.276  ns/op
VIntCodingBench.testWrite5Bytes               monomorphic  avgt   15  22.886 ± 0.602  ns/op
VIntCodingBench.testWrite5Bytes                 bimorphic  avgt   15  29.209 ± 1.077  ns/op
VIntCodingBench.testWrite5Bytes               megamorphic  avgt   15  32.731 ± 0.944  ns/op
VIntCodingBench.testWrite6Bytes               monomorphic  avgt   15  22.579 ± 0.794  ns/op
VIntCodingBench.testWrite6Bytes                 bimorphic  avgt   15  29.067 ± 0.678  ns/op
VIntCodingBench.testWrite6Bytes               megamorphic  avgt   15  35.419 ± 1.496  ns/op
VIntCodingBench.testWrite7Bytes               monomorphic  avgt   15  22.823 ± 0.527  ns/op
VIntCodingBench.testWrite7Bytes                 bimorphic  avgt   15  29.521 ± 1.216  ns/op
VIntCodingBench.testWrite7Bytes               megamorphic  avgt   15  34.295 ± 2.327  ns/op
VIntCodingBench.testWrite8Bytes               monomorphic  avgt   15  22.032 ± 0.918  ns/op
VIntCodingBench.testWrite8Bytes                 bimorphic  avgt   15  30.388 ± 1.015  ns/op
VIntCodingBench.testWrite8Bytes               megamorphic  avgt   15  33.632 ± 1.200  ns/op
VIntCodingBench.testWrite9Bytes               monomorphic  avgt   15  22.616 ± 1.309  ns/op
VIntCodingBench.testWrite9Bytes                 bimorphic  avgt   15  29.291 ± 1.096  ns/op
VIntCodingBench.testWrite9Bytes               megamorphic  avgt   15  32.597 ± 0.807  ns/op
VIntCodingBench.testWriteRandomLong           monomorphic  avgt   15  35.010 ± 1.145  ns/op
VIntCodingBench.testWriteRandomLong             bimorphic  avgt   15  43.090 ± 0.615  ns/op
VIntCodingBench.testWriteRandomLong           megamorphic  avgt   15  43.196 ± 1.742  ns/op
{noformat}
h4. Patch
{noformat}
VIntCodingBench.testComputeUnsignedVIntSize   monomorphic  avgt   15  16.339 ± 0.418  ns/op
VIntCodingBench.testComputeUnsignedVIntSize     bimorphic  avgt   15  16.340 ± 0.417  ns/op
VIntCodingBench.testComputeUnsignedVIntSize   megamorphic  avgt   15  16.435 ± 0.408  ns/op
VIntCodingBench.testWrite1Byte                monomorphic  avgt   15   9.362 ± 0.208  ns/op
VIntCodingBench.testWrite1Byte                  bimorphic  avgt   15  18.164 ± 0.839  ns/op
VIntCodingBench.testWrite1Byte                megamorphic  avgt   15  19.800 ± 0.942  ns/op
VIntCodingBench.testWrite2Bytes               monomorphic  avgt   15  10.094 ± 0.444  ns/op
VIntCodingBench.testWrite2Bytes                 bimorphic  avgt   15  18.310 ± 0.813  ns/op
VIntCodingBench.testWrite2Bytes               megamorphic  avgt   15  19.685 ± 0.692  ns/op
VIntCodingBench.testWrite3Bytes               monomorphic  avgt   15  11.541 ± 0.433  ns/op
VIntCodingBench.testWrite3Bytes                 bimorphic  avgt   15  19.087 ± 0.720  ns/op
VIntCodingBench.testWrite3Bytes               megamorphic  avgt   15  20.518 ± 1.035  ns/op
VIntCodingBench.testWrite4Bytes               monomorphic  avgt   15  10.677 ± 0.417  ns/op
VIntCodingBench.testWrite4Bytes                 bimorphic  avgt   15  18.815 ± 0.921  ns/op
VIntCodingBench.testWrite4Bytes               megamorphic  avgt   15  21.106 ± 0.731  ns/op
VIntCodingBench.testWrite5Bytes               monomorphic  avgt   15  11.564 ± 0.549  ns/op
VIntCodingBench.testWrite5Bytes                 bimorphic  avgt   15  18.961 ± 0.783  ns/op
VIntCodingBench.testWrite5Bytes               megamorphic  avgt   15  20.939 ± 0.541  ns/op
VIntCodingBench.testWrite6Bytes               monomorphic  avgt   15  13.463 ± 0.441  ns/op
VIntCodingBench.testWrite6Bytes                 bimorphic  avgt   15  20.188 ± 0.792  ns/op
VIntCodingBench.testWrite6Bytes               megamorphic  avgt   15  23.087 ± 1.044  ns/op
VIntCodingBench.testWrite7Bytes               monomorphic  avgt   15  12.655 ± 0.347  ns/op
VIntCodingBench.testWrite7Bytes                 bimorphic  avgt   15  21.307 ± 0.892  ns/op
VIntCodingBench.testWrite7Bytes               megamorphic  avgt   15  23.209 ± 0.526  ns/op
VIntCodingBench.testWrite8Bytes               monomorphic  avgt   15  11.712 ± 0.404  ns/op
VIntCodingBench.testWrite8Bytes                 bimorphic  avgt   15  19.179 ± 0.730  ns/op
VIntCodingBench.testWrite8Bytes               megamorphic  avgt   15  20.692 ± 0.627  ns/op
VIntCodingBench.testWrite9Bytes               monomorphic  avgt   15  12.029 ± 0.387  ns/op
VIntCodingBench.testWrite9Bytes                 bimorphic  avgt   15  19.826 ± 0.971  ns/op
VIntCodingBench.testWrite9Bytes               megamorphic  avgt   15  21.953 ± 0.450  ns/op
VIntCodingBench.testWriteRandomLong           monomorphic  avgt   15  24.668 ± 0.523  ns/op
VIntCodingBench.testWriteRandomLong             bimorphic  avgt   15  31.157 ± 0.832  ns/op
VIntCodingBench.testWriteRandomLong           megamorphic  avgt   15  32.698 ± 1.376  ns/op
{noformat}
Next week I plan to start working on {{{}DataOutputPlus#writeBytes(long register, int bytes){}}}.;;;","08/Nov/21 22:42;benedict;This is great, thanks [~Gerrrr]! Looks like pretty significant improvements for such common code paths. I'll be very interested to see how the register approach compares, both for {{ByteBuffer}} and {{DataOutputPlus}}.;;;","16/Nov/21 16:22;Gerrrr;I implemented {{DataOutputPlus#writeBytes}} and added benchmarks that use the {{DataOutputPlus}} version of the method.

The register approach definitely improves write throughput. Due to increased number of benchmarks, I also added a visualization for megamorphic calls in addition to the raw results. ""Multiple writes"" below refers to the initial approach I tried with switch-cases for different number of bytes.

I am going to apply the same register approach to reads next.

!writeUnsignedVInt_megamorphic_DOP.png|width=800!

!writeUnsignedVInt_megamorphic_BB.png|width=800!

h4. Register
{noformat}
Benchmark                                    (allocation)  Mode  Cnt   Score   Error  Units
VIntCodingBench.testComputeUnsignedVIntSize   monomorphic  avgt   15  15.939 ± 0.235  ns/op
VIntCodingBench.testComputeUnsignedVIntSize     bimorphic  avgt   15  15.972 ± 0.170  ns/op
VIntCodingBench.testComputeUnsignedVIntSize   megamorphic  avgt   15  15.976 ± 0.225  ns/op
VIntCodingBench.testWrite1ByteBB              monomorphic  avgt   15   9.555 ± 0.059  ns/op
VIntCodingBench.testWrite1ByteBB                bimorphic  avgt   15  16.777 ± 0.107  ns/op
VIntCodingBench.testWrite1ByteBB              megamorphic  avgt   15  18.286 ± 0.155  ns/op
VIntCodingBench.testWrite1ByteDOP             monomorphic  avgt   15  10.507 ± 0.522  ns/op
VIntCodingBench.testWrite1ByteDOP               bimorphic  avgt   15  19.048 ± 0.262  ns/op
VIntCodingBench.testWrite1ByteDOP             megamorphic  avgt   15  19.339 ± 0.155  ns/op
VIntCodingBench.testWrite2BytesBB             monomorphic  avgt   15  14.688 ± 0.170  ns/op
VIntCodingBench.testWrite2BytesBB               bimorphic  avgt   15  19.421 ± 0.115  ns/op
VIntCodingBench.testWrite2BytesBB             megamorphic  avgt   15  21.975 ± 0.110  ns/op
VIntCodingBench.testWrite2BytesDOP            monomorphic  avgt   15  14.675 ± 0.102  ns/op
VIntCodingBench.testWrite2BytesDOP              bimorphic  avgt   15  22.644 ± 0.217  ns/op
VIntCodingBench.testWrite2BytesDOP            megamorphic  avgt   15  22.789 ± 0.854  ns/op
VIntCodingBench.testWrite3BytesBB             monomorphic  avgt   15  14.764 ± 0.112  ns/op
VIntCodingBench.testWrite3BytesBB               bimorphic  avgt   15  19.543 ± 0.363  ns/op
VIntCodingBench.testWrite3BytesBB             megamorphic  avgt   15  22.054 ± 0.138  ns/op
VIntCodingBench.testWrite3BytesDOP            monomorphic  avgt   15  14.706 ± 0.115  ns/op
VIntCodingBench.testWrite3BytesDOP              bimorphic  avgt   15  22.549 ± 0.151  ns/op
VIntCodingBench.testWrite3BytesDOP            megamorphic  avgt   15  22.560 ± 0.370  ns/op
VIntCodingBench.testWrite4BytesBB             monomorphic  avgt   15  14.679 ± 0.158  ns/op
VIntCodingBench.testWrite4BytesBB               bimorphic  avgt   15  19.593 ± 0.254  ns/op
VIntCodingBench.testWrite4BytesBB             megamorphic  avgt   15  22.202 ± 0.194  ns/op
VIntCodingBench.testWrite4BytesDOP            monomorphic  avgt   15  14.669 ± 0.098  ns/op
VIntCodingBench.testWrite4BytesDOP              bimorphic  avgt   15  22.469 ± 0.195  ns/op
VIntCodingBench.testWrite4BytesDOP            megamorphic  avgt   15  22.681 ± 0.643  ns/op
VIntCodingBench.testWrite5BytesBB             monomorphic  avgt   15  14.655 ± 0.142  ns/op
VIntCodingBench.testWrite5BytesBB               bimorphic  avgt   15  19.390 ± 0.100  ns/op
VIntCodingBench.testWrite5BytesBB             megamorphic  avgt   15  22.086 ± 0.185  ns/op
VIntCodingBench.testWrite5BytesDOP            monomorphic  avgt   15  14.668 ± 0.137  ns/op
VIntCodingBench.testWrite5BytesDOP              bimorphic  avgt   15  22.833 ± 0.615  ns/op
VIntCodingBench.testWrite5BytesDOP            megamorphic  avgt   15  22.127 ± 0.298  ns/op
VIntCodingBench.testWrite6BytesBB             monomorphic  avgt   15  14.766 ± 0.252  ns/op
VIntCodingBench.testWrite6BytesBB               bimorphic  avgt   15  19.502 ± 0.128  ns/op
VIntCodingBench.testWrite6BytesBB             megamorphic  avgt   15  22.386 ± 0.314  ns/op
VIntCodingBench.testWrite6BytesDOP            monomorphic  avgt   15  14.690 ± 0.122  ns/op
VIntCodingBench.testWrite6BytesDOP              bimorphic  avgt   15  22.543 ± 0.200  ns/op
VIntCodingBench.testWrite6BytesDOP            megamorphic  avgt   15  22.278 ± 0.469  ns/op
VIntCodingBench.testWrite7BytesBB             monomorphic  avgt   15  14.687 ± 0.268  ns/op
VIntCodingBench.testWrite7BytesBB               bimorphic  avgt   15  19.434 ± 0.179  ns/op
VIntCodingBench.testWrite7BytesBB             megamorphic  avgt   15  21.991 ± 0.160  ns/op
VIntCodingBench.testWrite7BytesDOP            monomorphic  avgt   15  14.677 ± 0.131  ns/op
VIntCodingBench.testWrite7BytesDOP              bimorphic  avgt   15  22.819 ± 0.553  ns/op
VIntCodingBench.testWrite7BytesDOP            megamorphic  avgt   15  22.893 ± 0.736  ns/op
VIntCodingBench.testWrite8BytesBB             monomorphic  avgt   15  13.047 ± 0.072  ns/op
VIntCodingBench.testWrite8BytesBB               bimorphic  avgt   15  17.629 ± 0.192  ns/op
VIntCodingBench.testWrite8BytesBB             megamorphic  avgt   15  20.674 ± 0.216  ns/op
VIntCodingBench.testWrite8BytesDOP            monomorphic  avgt   15  14.785 ± 0.316  ns/op
VIntCodingBench.testWrite8BytesDOP              bimorphic  avgt   15  22.605 ± 0.143  ns/op
VIntCodingBench.testWrite8BytesDOP            megamorphic  avgt   15  22.411 ± 0.907  ns/op
VIntCodingBench.testWrite9BytesBB             monomorphic  avgt   15  14.495 ± 0.123  ns/op
VIntCodingBench.testWrite9BytesBB               bimorphic  avgt   15  20.330 ± 1.697  ns/op
VIntCodingBench.testWrite9BytesBB             megamorphic  avgt   15  22.011 ± 0.637  ns/op
VIntCodingBench.testWrite9BytesDOP            monomorphic  avgt   15  13.706 ± 0.089  ns/op
VIntCodingBench.testWrite9BytesDOP              bimorphic  avgt   15  21.891 ± 0.435  ns/op
VIntCodingBench.testWrite9BytesDOP            megamorphic  avgt   15  22.970 ± 0.246  ns/op
VIntCodingBench.testWriteRandomLongBB         monomorphic  avgt   15  26.456 ± 0.536  ns/op
VIntCodingBench.testWriteRandomLongBB           bimorphic  avgt   15  32.810 ± 0.580  ns/op
VIntCodingBench.testWriteRandomLongBB         megamorphic  avgt   15  32.634 ± 0.602  ns/op
VIntCodingBench.testWriteRandomLongDOP        monomorphic  avgt   15  26.408 ± 0.988  ns/op
VIntCodingBench.testWriteRandomLongDOP          bimorphic  avgt   15  32.097 ± 2.057  ns/op
VIntCodingBench.testWriteRandomLongDOP        megamorphic  avgt   15  34.598 ± 1.319  ns/op
{noformat}

h4. Multiple writes
{noformat}
Benchmark                                    (allocation)  Mode  Cnt   Score   Error  Units
VIntCodingBench.testComputeUnsignedVIntSize   monomorphic  avgt   15  17.127 ± 0.709  ns/op
VIntCodingBench.testComputeUnsignedVIntSize     bimorphic  avgt   15  17.389 ± 1.306  ns/op
VIntCodingBench.testComputeUnsignedVIntSize   megamorphic  avgt   15  16.287 ± 0.481  ns/op
VIntCodingBench.testWrite1ByteBB              monomorphic  avgt   15   9.417 ± 0.284  ns/op
VIntCodingBench.testWrite1ByteBB                bimorphic  avgt   15  17.806 ± 0.589  ns/op
VIntCodingBench.testWrite1ByteBB              megamorphic  avgt   15  18.623 ± 0.787  ns/op
VIntCodingBench.testWrite1ByteDOP             monomorphic  avgt   15  13.974 ± 0.576  ns/op
VIntCodingBench.testWrite1ByteDOP               bimorphic  avgt   15  22.223 ± 0.883  ns/op
VIntCodingBench.testWrite1ByteDOP             megamorphic  avgt   15  23.301 ± 0.685  ns/op
VIntCodingBench.testWrite2BytesBB             monomorphic  avgt   15   9.537 ± 0.314  ns/op
VIntCodingBench.testWrite2BytesBB               bimorphic  avgt   15  17.371 ± 0.472  ns/op
VIntCodingBench.testWrite2BytesBB             megamorphic  avgt   15  18.636 ± 0.539  ns/op
VIntCodingBench.testWrite2BytesDOP            monomorphic  avgt   15  14.921 ± 0.661  ns/op
VIntCodingBench.testWrite2BytesDOP              bimorphic  avgt   15  22.561 ± 0.448  ns/op
VIntCodingBench.testWrite2BytesDOP            megamorphic  avgt   15  24.397 ± 0.745  ns/op
VIntCodingBench.testWrite3BytesBB             monomorphic  avgt   15  11.220 ± 0.337  ns/op
VIntCodingBench.testWrite3BytesBB               bimorphic  avgt   15  19.180 ± 0.398  ns/op
VIntCodingBench.testWrite3BytesBB             megamorphic  avgt   15  20.009 ± 0.592  ns/op
VIntCodingBench.testWrite3BytesDOP            monomorphic  avgt   15  16.094 ± 0.481  ns/op
VIntCodingBench.testWrite3BytesDOP              bimorphic  avgt   15  25.855 ± 0.873  ns/op
VIntCodingBench.testWrite3BytesDOP            megamorphic  avgt   15  26.087 ± 0.735  ns/op
VIntCodingBench.testWrite4BytesBB             monomorphic  avgt   15  10.252 ± 0.344  ns/op
VIntCodingBench.testWrite4BytesBB               bimorphic  avgt   15  18.760 ± 2.626  ns/op
VIntCodingBench.testWrite4BytesBB             megamorphic  avgt   15  19.677 ± 0.642  ns/op
VIntCodingBench.testWrite4BytesDOP            monomorphic  avgt   15  15.099 ± 0.317  ns/op
VIntCodingBench.testWrite4BytesDOP              bimorphic  avgt   15  22.329 ± 0.629  ns/op
VIntCodingBench.testWrite4BytesDOP            megamorphic  avgt   15  25.075 ± 0.973  ns/op
VIntCodingBench.testWrite5BytesBB             monomorphic  avgt   15  10.561 ± 0.354  ns/op
VIntCodingBench.testWrite5BytesBB               bimorphic  avgt   15  18.056 ± 0.582  ns/op
VIntCodingBench.testWrite5BytesBB             megamorphic  avgt   15  19.491 ± 0.598  ns/op
VIntCodingBench.testWrite5BytesDOP            monomorphic  avgt   15  16.228 ± 0.480  ns/op
VIntCodingBench.testWrite5BytesDOP              bimorphic  avgt   15  24.403 ± 0.744  ns/op
VIntCodingBench.testWrite5BytesDOP            megamorphic  avgt   15  25.119 ± 0.904  ns/op
VIntCodingBench.testWrite6BytesBB             monomorphic  avgt   15  12.688 ± 0.388  ns/op
VIntCodingBench.testWrite6BytesBB               bimorphic  avgt   15  19.904 ± 0.730  ns/op
VIntCodingBench.testWrite6BytesBB             megamorphic  avgt   15  21.964 ± 0.772  ns/op
VIntCodingBench.testWrite6BytesDOP            monomorphic  avgt   15  17.678 ± 0.797  ns/op
VIntCodingBench.testWrite6BytesDOP              bimorphic  avgt   15  24.909 ± 1.068  ns/op
VIntCodingBench.testWrite6BytesDOP            megamorphic  avgt   15  28.334 ± 0.732  ns/op
VIntCodingBench.testWrite7BytesBB             monomorphic  avgt   15  12.502 ± 0.410  ns/op
VIntCodingBench.testWrite7BytesBB               bimorphic  avgt   15  20.105 ± 0.653  ns/op
VIntCodingBench.testWrite7BytesBB             megamorphic  avgt   15  23.053 ± 0.923  ns/op
VIntCodingBench.testWrite7BytesDOP            monomorphic  avgt   15  18.053 ± 1.386  ns/op
VIntCodingBench.testWrite7BytesDOP              bimorphic  avgt   15  27.000 ± 0.889  ns/op
VIntCodingBench.testWrite7BytesDOP            megamorphic  avgt   15  30.684 ± 1.024  ns/op
VIntCodingBench.testWrite8BytesBB             monomorphic  avgt   15  12.383 ± 0.630  ns/op
VIntCodingBench.testWrite8BytesBB               bimorphic  avgt   15  19.600 ± 0.543  ns/op
VIntCodingBench.testWrite8BytesBB             megamorphic  avgt   15  20.995 ± 0.918  ns/op
VIntCodingBench.testWrite8BytesDOP            monomorphic  avgt   15  16.830 ± 0.372  ns/op
VIntCodingBench.testWrite8BytesDOP              bimorphic  avgt   15  25.022 ± 0.615  ns/op
VIntCodingBench.testWrite8BytesDOP            megamorphic  avgt   15  25.871 ± 0.845  ns/op
VIntCodingBench.testWrite9BytesBB             monomorphic  avgt   15  12.396 ± 0.604  ns/op
VIntCodingBench.testWrite9BytesBB               bimorphic  avgt   15  20.006 ± 0.586  ns/op
VIntCodingBench.testWrite9BytesBB             megamorphic  avgt   15  22.612 ± 0.670  ns/op
VIntCodingBench.testWrite9BytesDOP            monomorphic  avgt   15  18.393 ± 0.587  ns/op
VIntCodingBench.testWrite9BytesDOP              bimorphic  avgt   15  24.908 ± 0.714  ns/op
VIntCodingBench.testWrite9BytesDOP            megamorphic  avgt   15  29.258 ± 1.792  ns/op
VIntCodingBench.testWriteRandomLongBB         monomorphic  avgt   15  25.575 ± 0.933  ns/op
VIntCodingBench.testWriteRandomLongBB           bimorphic  avgt   15  33.336 ± 0.926  ns/op
VIntCodingBench.testWriteRandomLongBB         megamorphic  avgt   15  34.463 ± 2.112  ns/op
VIntCodingBench.testWriteRandomLongDOP        monomorphic  avgt   15  29.358 ± 0.785  ns/op
VIntCodingBench.testWriteRandomLongDOP          bimorphic  avgt   15  34.367 ± 1.122  ns/op
VIntCodingBench.testWriteRandomLongDOP        megamorphic  avgt   15  36.373 ± 0.995  ns/op
{noformat}

h4. Baseline
{noformat}
Benchmark                                    (allocation)  Mode  Cnt   Score   Error  Units
VIntCodingBench.testComputeUnsignedVIntSize   monomorphic  avgt   15  16.237 ± 0.144  ns/op
VIntCodingBench.testComputeUnsignedVIntSize     bimorphic  avgt   15  16.273 ± 0.193  ns/op
VIntCodingBench.testComputeUnsignedVIntSize   megamorphic  avgt   15  16.251 ± 0.226  ns/op
VIntCodingBench.testWrite1ByteBB              monomorphic  avgt   15   8.722 ± 0.049  ns/op
VIntCodingBench.testWrite1ByteBB                bimorphic  avgt   15  17.095 ± 0.656  ns/op
VIntCodingBench.testWrite1ByteBB              megamorphic  avgt   15  17.856 ± 0.366  ns/op
VIntCodingBench.testWrite1ByteDOP             monomorphic  avgt   15  10.539 ± 0.304  ns/op
VIntCodingBench.testWrite1ByteDOP               bimorphic  avgt   15  19.000 ± 0.229  ns/op
VIntCodingBench.testWrite1ByteDOP             megamorphic  avgt   15  19.755 ± 0.332  ns/op
VIntCodingBench.testWrite2BytesBB             monomorphic  avgt   15  19.322 ± 0.125  ns/op
VIntCodingBench.testWrite2BytesBB               bimorphic  avgt   15  25.739 ± 0.445  ns/op
VIntCodingBench.testWrite2BytesBB             megamorphic  avgt   15  29.106 ± 0.706  ns/op
VIntCodingBench.testWrite2BytesDOP            monomorphic  avgt   15  25.940 ± 0.161  ns/op
VIntCodingBench.testWrite2BytesDOP              bimorphic  avgt   15  33.446 ± 0.527  ns/op
VIntCodingBench.testWrite2BytesDOP            megamorphic  avgt   15  37.912 ± 1.958  ns/op
VIntCodingBench.testWrite3BytesBB             monomorphic  avgt   15  22.309 ± 2.962  ns/op
VIntCodingBench.testWrite3BytesBB               bimorphic  avgt   15  26.560 ± 0.684  ns/op
VIntCodingBench.testWrite3BytesBB             megamorphic  avgt   15  30.398 ± 0.843  ns/op
VIntCodingBench.testWrite3BytesDOP            monomorphic  avgt   15  27.975 ± 1.135  ns/op
VIntCodingBench.testWrite3BytesDOP              bimorphic  avgt   15  34.763 ± 0.654  ns/op
VIntCodingBench.testWrite3BytesDOP            megamorphic  avgt   15  39.441 ± 0.284  ns/op
VIntCodingBench.testWrite4BytesBB             monomorphic  avgt   15  21.715 ± 0.365  ns/op
VIntCodingBench.testWrite4BytesBB               bimorphic  avgt   15  28.755 ± 0.317  ns/op
VIntCodingBench.testWrite4BytesBB             megamorphic  avgt   15  31.959 ± 0.522  ns/op
VIntCodingBench.testWrite4BytesDOP            monomorphic  avgt   15  27.525 ± 0.526  ns/op
VIntCodingBench.testWrite4BytesDOP              bimorphic  avgt   15  36.992 ± 0.201  ns/op
VIntCodingBench.testWrite4BytesDOP            megamorphic  avgt   15  39.074 ± 0.808  ns/op
VIntCodingBench.testWrite5BytesBB             monomorphic  avgt   15  21.620 ± 0.509  ns/op
VIntCodingBench.testWrite5BytesBB               bimorphic  avgt   15  27.592 ± 0.259  ns/op
VIntCodingBench.testWrite5BytesBB             megamorphic  avgt   15  31.718 ± 0.809  ns/op
VIntCodingBench.testWrite5BytesDOP            monomorphic  avgt   15  26.949 ± 0.189  ns/op
VIntCodingBench.testWrite5BytesDOP              bimorphic  avgt   15  34.916 ± 0.386  ns/op
VIntCodingBench.testWrite5BytesDOP            megamorphic  avgt   15  39.920 ± 1.518  ns/op
VIntCodingBench.testWrite6BytesBB             monomorphic  avgt   15  21.572 ± 0.435  ns/op
VIntCodingBench.testWrite6BytesBB               bimorphic  avgt   15  28.094 ± 0.450  ns/op
VIntCodingBench.testWrite6BytesBB             megamorphic  avgt   15  32.456 ± 0.488  ns/op
VIntCodingBench.testWrite6BytesDOP            monomorphic  avgt   15  27.655 ± 0.453  ns/op
VIntCodingBench.testWrite6BytesDOP              bimorphic  avgt   15  39.025 ± 0.517  ns/op
VIntCodingBench.testWrite6BytesDOP            megamorphic  avgt   15  39.899 ± 1.011  ns/op
VIntCodingBench.testWrite7BytesBB             monomorphic  avgt   15  21.973 ± 0.426  ns/op
VIntCodingBench.testWrite7BytesBB               bimorphic  avgt   15  27.559 ± 0.316  ns/op
VIntCodingBench.testWrite7BytesBB             megamorphic  avgt   15  31.988 ± 0.373  ns/op
VIntCodingBench.testWrite7BytesDOP            monomorphic  avgt   15  28.257 ± 0.205  ns/op
VIntCodingBench.testWrite7BytesDOP              bimorphic  avgt   15  37.111 ± 0.625  ns/op
VIntCodingBench.testWrite7BytesDOP            megamorphic  avgt   15  38.689 ± 0.214  ns/op
VIntCodingBench.testWrite8BytesBB             monomorphic  avgt   15  23.458 ± 0.456  ns/op
VIntCodingBench.testWrite8BytesBB               bimorphic  avgt   15  28.696 ± 0.137  ns/op
VIntCodingBench.testWrite8BytesBB             megamorphic  avgt   15  32.025 ± 0.448  ns/op
VIntCodingBench.testWrite8BytesDOP            monomorphic  avgt   15  28.777 ± 0.374  ns/op
VIntCodingBench.testWrite8BytesDOP              bimorphic  avgt   15  37.232 ± 0.374  ns/op
VIntCodingBench.testWrite8BytesDOP            megamorphic  avgt   15  39.832 ± 0.825  ns/op
VIntCodingBench.testWrite9BytesBB             monomorphic  avgt   15  22.873 ± 1.074  ns/op
VIntCodingBench.testWrite9BytesBB               bimorphic  avgt   15  28.377 ± 0.501  ns/op
VIntCodingBench.testWrite9BytesBB             megamorphic  avgt   15  31.409 ± 0.611  ns/op
VIntCodingBench.testWrite9BytesDOP            monomorphic  avgt   15  29.532 ± 1.390  ns/op
VIntCodingBench.testWrite9BytesDOP              bimorphic  avgt   15  39.124 ± 3.493  ns/op
VIntCodingBench.testWrite9BytesDOP            megamorphic  avgt   15  39.400 ± 0.439  ns/op
VIntCodingBench.testWriteRandomLongBB         monomorphic  avgt   15  34.258 ± 0.192  ns/op
VIntCodingBench.testWriteRandomLongBB           bimorphic  avgt   15  43.513 ± 0.411  ns/op
VIntCodingBench.testWriteRandomLongBB         megamorphic  avgt   15  42.713 ± 0.514  ns/op
VIntCodingBench.testWriteRandomLongDOP        monomorphic  avgt   15  38.191 ± 0.280  ns/op
VIntCodingBench.testWriteRandomLongDOP          bimorphic  avgt   15  48.820 ± 0.327  ns/op
VIntCodingBench.testWriteRandomLongDOP        megamorphic  avgt   15  51.459 ± 0.399  ns/op
{noformat};;;","22/Nov/21 13:58;benedict;Thanks [~Gerrrr], I took a quick look to see if I could shave any further time and have the following [suggestions|https://github.com/belliottsmith/cassandra/tree/15215-trunk]. On my laptop at least the changes to {{ByteBuffer}} result in ~1ns reduction (or about 6% improvement). I also took the liberty of modifying the {{DataOutputPlus}} specification to use the top bytes of the register to permit the {{ByteBuffer}} and {{DataOutputPlus}} implementations to look approximately the same, and applied your original optimisation for VIntCoding to the default implementation of {{writeBytes}}.

We should probably add a unit test or two to cover {{DataOutputPlus}} vint coding and {{writeBytes}} before we commit, but overall the patch is looking good.;;;","22/Nov/21 14:18;Gerrrr;Thanks for the review and the suggestions [~benedict] ! This Wednesday I plan to work on read performance, applying your changes, and adding unit tests for new code branches. Hopefully, the patch is going to be ready by the end of the week.;;;","26/Nov/21 09:35;Gerrrr;While working on the read path I've realized that it already has the optimization we discussed since CASSANDRA-8630 - https://github.com/apache/cassandra/blob/951d72cd929d1f6c9329becbdd7604a9e709587b/src/java/org/apache/cassandra/io/util/RebufferingInputStream.java#L239-L268.

Since the last update I added new test cases to {{VIntCodingTest}} to cover buffered and unbuffered reads and writes as well as extended {{DataOutputTest}} to cover {{DataOutputPlus#writeBytes}}.

Patches:
* [3.0|https://github.com/apache/cassandra/pull/1343]
* [3.11|https://github.com/apache/cassandra/pull/1344]
* [4.0|https://github.com/apache/cassandra/pull/1345]
* [trunk|https://github.com/apache/cassandra/pull/1346]

To demonstrate the results I picked a single benchmark - {{testWriteRandomLongDOP}} as it shows overall performance improvement and is relevant for all Cassandra versions. The results are for the megamorphic benchmark variation.

 !testWriteRandomLongDOP_final.png|width=800px! 


[~benedict], [~blambov] Can you please review the patch and run the CI?
;;;","29/Nov/21 22:55;benedict;Looks like a lot of failures: https://app.circleci.com/pipelines/github/belliottsmith/cassandra/207/workflows/a73f8193-6149-4d8c-87f4-289e4ad76aaf
;;;","30/Nov/21 10:23;Gerrrr;I'll fix test failures closer to the end of the week, probably on the weekend.;;;","30/Nov/21 10:40;benedict;I suspect we just need a better unit test for the encode and decode methods, that test serialising and deserialising a sequence of random integers and verifies the sequence is intact, for each variant have implemented.;;;","05/Dec/21 14:14;Gerrrr;The issue was caused by the slow path in {{BufferedDataOutputStreamPlus#writeBytes}} when the underlying buffer has less than 8 bytes remaining. Previously, this method fell back to {{writeSlow}}. This was not correct because it writes N least significant bytes to the wire. As {{writeBytes}} treats the register as an optimized version of a byte array, it should write N most significant bytes instead. I added a test case that isolates the issue and fixed it in all branches. 

[~benedict] can you please re-run the CI?;;;","06/Dec/21 11:13;benedict;I've re-run [CI|https://app.circleci.com/pipelines/github/belliottsmith/cassandra?branch=15215-trunk] and it looks mostly clean but there are some remaining issues. I'm on holiday at the moment, and will get back to review and final sign off next week.;;;","07/Dec/21 11:01;Gerrrr;I'll fix the last test failures on the weekend. Enjoy your holiday :);;;","11/Dec/21 15:45;Gerrrr;I looked into the most recent test failures and I am fairly convinced that none of them are caused by this patch:
* [CQLConnectionTest test failures |https://app.circleci.com/pipelines/github/belliottsmith/cassandra/216/workflows/9b2ff75d-d2fd-47ad-a4d6-a407a649780c/jobs/5659/tests#failed-test-2] - there are recent bug reports regarding this test suite failing in various ways - CASSANDRA-16677 as an ""aggregate issue"" and a number of linked duplicates. One example is [this build|https://app.circleci.com/pipelines/github/dcapwell/cassandra/1037/workflows/c728d370-49b9-41aa-bdfb-8c41cf0355d8/jobs/6577/tests] from CASSANDRA-16949 that has exactly the same failures.
 * [TestClientRequestMetrics|https://app.circleci.com/pipelines/github/belliottsmith/cassandra/216/workflows/418d4b46-8d8b-41df-ad80-06f377593caf/jobs/5646/tests#failed-test-0] - was also observed in https://issues.apache.org/jira/browse/CASSANDRA-15234?focusedCommentId=17454221&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17454221.
* [MessagingServiceTest |https://app.circleci.com/pipelines/github/belliottsmith/cassandra/216/workflows/418d4b46-8d8b-41df-ad80-06f377593caf/jobs/5637] - CASSANDRA-17033.
;;;","13/Dec/21 13:49;benedict;Thanks for checking, and apologies about that - the CQLConnectionTest failures looked like encoding/decoding problems so I assumed plausibly connected to this patch, as I hadn't seen them before.

I've rebased, and introduced one [nit|https://github.com/belliottsmith/cassandra/commit/934fe98900b35c6a8639f1b6e0240b9a3aef36c9] commit to use the (presumably more efficient) {{DataOutputPlus.writeBytes}} as a fallback in {{BufferedDataOutputStreamPlus}}. Once CI is green, assuming you're OK with this change, I'll merge to trunk.;;;","14/Dec/21 10:27;Gerrrr;No worries, CQLConnectionTest failures indeed looked suspicious. I agree with your commit and am looking forward to green CI and merge :);;;","04/Jan/22 16:03;Gerrrr;I added [~blambov] as a reviewer as he approved the PR to trunk. [~benedict] is there anything I can do to facilitate the merge?;;;","05/Jan/22 15:54;benedict;Sorry [~Gerrrr] I've been on leave for most of December (and still am). I'll get to it soon, I promise!;;;","05/Jan/22 15:55;benedict;(In my opinion it's ready to go, just needs to be rebased, retested and merged);;;","02/Feb/22 10:45;benedict;Going to try and slowly nurse this to completion by CI;;;","02/Feb/22 12:19;benedict;Seemingly clean CI runs (looperTest was an error on my part, and the other two are tests that fail on other branches). Committed.

https://app.circleci.com/pipelines/github/belliottsmith/cassandra/228/workflows/15695a66-1233-4fd1-9a7d-63b59ab1242c
https://app.circleci.com/pipelines/github/belliottsmith/cassandra/228/workflows/d6491609-e540-4ac6-b2e6-93c84fa73402;;;","31/Oct/22 20:45;aweisberg;Maybe I am missing something, but I don't see where this [change|#L217]] handles the case where the buffer doesn't have 8 bytes remaining? It looks like it would generate a runtime error if the amount remaining was < 8 but >= size and that is not so bad, but in the unusual situation where the space remaining is < size it will silently not write the value.
{code:java}
else if (size < 9)
{
    int limit = output.limit();
    int pos = output.position();
    if (limit - pos >= size)
    {
        int shift = (8 - size) << 3;
        int extraBytes = size - 1;
        long mask = (long)VIntCoding.encodeExtraBytesToRead(extraBytes) << 56;
        long register = (value << shift) | mask;
        output.putLong(pos, register);
        output.position(pos + size);
    }
}{code};;;","31/Oct/22 22:01;benedict;Good catch. It looks like 1) this should probably have an else clause that invokes the slow path; 2) this is only used by {{TypeCodec.DurationCodes.serialize}}, so the exposure is fortunately pretty small.;;;","31/Oct/22 22:41;Gerrrr;Nice catch! I'll publish a fix tomorrow in the morning.;;;","01/Nov/22 12:23;Gerrrr;Patch - [https://github.com/apache/cassandra/compare/trunk...Gerrrr:cassandra:15215-fixup-trunk?expand=1]

CI (still running) - [https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/2052];;;","01/Nov/22 19:49;Gerrrr;CI completed and the failures do not seem to be related. [~benedict] could you please review?;;;","04/Nov/22 13:13;Gerrrr;[~aweisberg] Would you have time to review the fix for the issue you found?;;;","08/Nov/22 12:51;Gerrrr;Bump. I think it is important to include the fix into the 4.1-rc. ;;;","11/Nov/22 10:01;blerer;Thanks [~Gerrrr]. The patch looks good to me.;;;","11/Nov/22 12:03;brandon.williams;This would have better visibility had it been a new follow up ticket.;;;","11/Nov/22 19:00;Gerrrr;Committed [5cd012736e4680bbb25928ee7fbcea4859878fff|https://github.com/apache/cassandra/commit/5cd012736e4680bbb25928ee7fbcea4859878fff] at cassandra-4.1 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Internode messaging catches OOMs and does not rethrow,CASSANDRA-15214,13245157,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yifanc,benedict,benedict,16/Jul/19 09:31,16/Mar/22 14:20,13/Jul/23 08:38,19/Nov/20 19:22,4.0,4.0-beta4,,,,Messaging/Client,Messaging/Internode,,,0,,,,"Netty (at least, and perhaps elsewhere in Executors) catches all exceptions, so presently there is no way to ensure that an OOM reaches the JVM handler to trigger a crash/heapdump.

It may be that the simplest most consistent way to do this would be to have a single thread spawned at startup that waits for any exceptions we must propagate to the Runtime.

We could probably submit a patch upstream to Netty, but for a guaranteed future proof approach, it may be worth paying the cost of a single thread.",,benedict,colinkuo,dcapwell,djoshi,jasonstack,jeromatron,jolynch,jwest,manish.c.ghildiyal@gmail.com,mbyrd,n.v.harikrishna,paulo,skokoori,snazy,v_ganeshraju,yifanc,,,,,,,,,,,,"yifan-c opened a new pull request #761:
URL: https://github.com/apache/cassandra/pull/761


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;29/Sep/20 22:23;githubbot;600","dcapwell commented on a change in pull request #761:
URL: https://github.com/apache/cassandra/pull/761#discussion_r517678215



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +120,28 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     */
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (oom.getMessage() != null && !oom.getMessage().equals(""Direct buffer memory""))

Review comment:
       can simplify to the below as string equality checks null, so don't need a null check first.
   
   ```
   if (!""Direct buffer memory"".equals(oom.getMessage()))
   ```

##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +120,28 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     */
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (oom.getMessage() != null && !oom.getMessage().equals(""Direct buffer memory""))
+        {
+            return;
+        }
+        logger.warn(""Force heap space OutOfMemoryError in the presence of"", oom);
+        while (true)
+        {
+            // java.util.AbstractCollection.MAX_ARRAY_SIZE is defined as Integer.MAX_VALUE - 8
+            // so Integer.MAX_VALUE / 2 should be a large enough and safe size to request.
+            long[] ignored = new long[Integer.MAX_VALUE / 2];

Review comment:
       it may be good to collect the references in a list to make sure GC isn't able to free right away.
   
   ```
   List<long[]> ignored = new ArrayList<>();
   while (true)
   {
       ignored.add(new long[Integer.MAX_VALUE / 2]);
   }
   ```

##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +120,28 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     */
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (oom.getMessage() != null && !oom.getMessage().equals(""Direct buffer memory""))
+        {
+            return;
+        }
+        logger.warn(""Force heap space OutOfMemoryError in the presence of"", oom);

Review comment:
       thinking should be `error`

##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +120,28 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     */

Review comment:
       would be good to link to `CASSANDRA-15214`, something like
   
   ```
   * see CASSANDRA-15214 for more details
   ```

##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +120,28 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     */
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (oom.getMessage() != null && !oom.getMessage().equals(""Direct buffer memory""))
+        {
+            return;
+        }
+        logger.warn(""Force heap space OutOfMemoryError in the presence of"", oom);
+        while (true)
+        {
+            // java.util.AbstractCollection.MAX_ARRAY_SIZE is defined as Integer.MAX_VALUE - 8
+            // so Integer.MAX_VALUE / 2 should be a large enough and safe size to request.
+            long[] ignored = new long[Integer.MAX_VALUE / 2];

Review comment:
       Also, in case the while true loop never breaks (such as JVM gets smart enough to know this is not needed and avoids the allocation), we should probably start logging an error saying that this trick isn't doing what we hoped.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;04/Nov/20 22:58;githubbot;600","dcapwell commented on a change in pull request #761:
URL: https://github.com/apache/cassandra/pull/761#discussion_r518973639



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +120,28 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     */
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (oom.getMessage() != null && !oom.getMessage().equals(""Direct buffer memory""))
+        {
+            return;
+        }
+        logger.warn(""Force heap space OutOfMemoryError in the presence of"", oom);
+        while (true)
+        {
+            // java.util.AbstractCollection.MAX_ARRAY_SIZE is defined as Integer.MAX_VALUE - 8
+            // so Integer.MAX_VALUE / 2 should be a large enough and safe size to request.
+            long[] ignored = new long[Integer.MAX_VALUE / 2];

Review comment:
       spoke in slack, adding `@Exclude` to the method should remove the concern of ""smartness"" trying to avoid the allocation




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/20 19:59;githubbot;600","yifan-c commented on pull request #761:
URL: https://github.com/apache/cassandra/pull/761#issuecomment-723316645


   Thanks @dcapwell. 
   For the exception handler, how about having the inspector at the end of the method. so it makes the best effort to send back the error message then handles the throwable. 
   Since `inspectThrowable` rethrows and netty swallows any unhandled exceptions. I will just catch any re-thrown throwable and log, as [here](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/InboundConnectionInitiator.java#L347-L350) does.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/20 21:53;githubbot;600","yifan-c edited a comment on pull request #761:
URL: https://github.com/apache/cassandra/pull/761#issuecomment-723316645


   Thanks @dcapwell. 
   For the exception handler, how about having the inspector at the end of the method? So it makes the best effort to send back the error message then handles the throwable. 
   Since `inspectThrowable` rethrows and netty swallows any unhandled exceptions. I will just catch any re-thrown throwable and log, as [here](https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/InboundConnectionInitiator.java#L347-L350) does.


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;06/Nov/20 21:56;githubbot;600","jrwest commented on a change in pull request #761:
URL: https://github.com/apache/cassandra/pull/761#discussion_r521492490



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +123,31 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     * See CASSANDRA-15214 for more details
+     */
+    @Exclude // Exclude from just in time compilation.
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (!""Direct buffer memory"".equals(oom.getMessage()))

Review comment:
       Depending on a string from JVM internal libraries seems like it has the potential to break easily in the future. Unless the Error Strings are something the JVM API is committed to keeping (which I don't have any recollection of being the case). 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/20 16:45;githubbot;600","jrwest commented on a change in pull request #761:
URL: https://github.com/apache/cassandra/pull/761#discussion_r521493650



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +123,31 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     * See CASSANDRA-15214 for more details
+     */
+    @Exclude // Exclude from just in time compilation.
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (!""Direct buffer memory"".equals(oom.getMessage()))
+        {
+            return;
+        }
+        logger.error(""Force heap space OutOfMemoryError in the presence of"", oom);
+        List<long[]> ignored = new ArrayList<>();

Review comment:
       It took me a sec to realize the goal here is to literally OOM the process. It might be nice to add a comment before the loop to that effect. 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/20 16:47;githubbot;600","yifan-c commented on a change in pull request #761:
URL: https://github.com/apache/cassandra/pull/761#discussion_r521552554



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +123,31 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     * See CASSANDRA-15214 for more details
+     */
+    @Exclude // Exclude from just in time compilation.
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (!""Direct buffer memory"".equals(oom.getMessage()))

Review comment:
       Agree. It is a hacky approach.
   The message remains the same for jdk 11 and below. 
   But the error message has changed since jdk 13. [[1]](https://github.com/openjdk/jdk13u/blob/a0651dd95b40c41803c938781f810aa40c43737b/src/java.base/share/classes/java/nio/Bits.java#L175), [[2]](https://github.com/openjdk/jdk15u/blob/ac465695d5924dc2a634d660c60f2d54d3d2743d/src/java.base/share/classes/java/nio/Bits.java#L175)
   
   I think we can improve it by checking the stack trace that the OOM is thrown from `java.nio.Bits. reserveMemory`. It is more future proofing (valid through jdk15 at least). But still a hack though. 
   WDYT?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/20 18:21;githubbot;600","jrwest commented on a change in pull request #761:
URL: https://github.com/apache/cassandra/pull/761#discussion_r521560937



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +123,31 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     * See CASSANDRA-15214 for more details
+     */
+    @Exclude // Exclude from just in time compilation.
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (!""Direct buffer memory"".equals(oom.getMessage()))

Review comment:
       That’s more what I had in mind, yes. Not perfect but more future proof. 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/20 18:36;githubbot;600","yifan-c commented on a change in pull request #761:
URL: https://github.com/apache/cassandra/pull/761#discussion_r521564779



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -125,7 +123,31 @@ public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory, C
             killer.killCurrentJVM(t);
 
         if (t.getCause() != null)
-            inspectThrowable(t.getCause(), propagateOutOfMemory, fn);
+            inspectThrowable(t.getCause(), fn);
+    }
+
+    /**
+     * Intentionally produce a heap space OOM upon seeing a Direct buffer memory OOM.
+     * Direct buffer OOM cannot trigger JVM OOM error related options,
+     * e.g. OnOutOfMemoryError, HeapDumpOnOutOfMemoryError, etc.
+     * See CASSANDRA-15214 for more details
+     */
+    @Exclude // Exclude from just in time compilation.
+    private static void forceHeapSpaceOomMaybe(OutOfMemoryError oom)
+    {
+        // See the oom thrown from java.nio.Bits.reserveMemory.
+        if (!""Direct buffer memory"".equals(oom.getMessage()))

Review comment:
       A downside is that checking trace is more expensive.
   
   I can update the condition to short circuit when the message contains `""direct buffer memory""`
   
   
   




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Nov/20 18:43;githubbot;600","smiklosovic closed pull request #761:
URL: https://github.com/apache/cassandra/pull/761


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 14:20;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,0,6600,,,0,6600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/19 21:45;yifanc;oom-experiments.zip;https://issues.apache.org/jira/secure/attachment/12976875/oom-experiments.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,yifanc,,,,,,,,,,,,Availability -> Process Crash,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Dec 22 17:42:36 UTC 2021,,,,,,,All,,,,,"0|z04pj4:",9223372036854775807,,,,,,,dcapwell,jwest,,,Normal,,4.0-beta2,,,https://github.com/apache/cassandra/commit/46ee939b957528185dc6bbd3028c1d6e695163e7,,,,,,,,,ci,,,,,"16/Jul/19 21:34;jolynch;We've (Netlfix) found handling OOMs to be generally hard to do correctly in all the various Java codebases we have so we built an agent solution which attaches to the JVM in [https://github.com/Netflix-Skunkworks/jvmquake]. I think the only reason that we couldn't just directly include that in C* is because it's a C JVMTI agent instead of a Java one, but perhaps we could just solve this with some documentation and making it really easy to include agents (which is useful regardless)?  I can also spend some time and see if I can make it a java agent instead of a c one.

The following is the patch for supporting easy pluggable agents for C*:
{noformat}
diff --git a/conf/cassandra-env.sh b/conf/cassandra-env.sh
index d6c48be0a3..92061db3ab 100644
--- a/conf/cassandra-env.sh
+++ b/conf/cassandra-env.sh
@@ -134,6 +134,29 @@ do
   JVM_OPTS=""$JVM_OPTS $opt""
 done
 
+# Pull in any agents present in CASSANDRA_HOME
+for agent_file in ${CASSANDRA_HOME}/agents/*.jar; do
+  if [ -e ""${agent_file}"" ]; then
+    base_file=""${agent_file%.jar}""
+    if [ -s ""${base_file}.options"" ]; then
+      options=`cat ${base_file}.options`
+      agent_file=""${agent_file}=${options}""
+    fi
+    JVM_OPTS=""$JVM_OPTS -javaagent:${agent_file}""
+  fi
+done
+
+for agent_file in ${CASSANDRA_HOME}/agents/*.so; do
+  if [ -e ""${agent_file}"" ]; then
+    base_file=""${agent_file%.so}""
+    if [ -s ""${base_file}.options"" ]; then
+      options=`cat ${base_file}.options`
+      agent_file=""${agent_file}=${options}""
+    fi
+    JVM_OPTS=""$JVM_OPTS -agentpath:${agent_file}""
+  fi
+done
{noformat}
Then we can just drop agents into the {{CASSANDRA_HOME/agents}} folder and they are loaded automatically by Cassandra. From a security perspective this is identical to ""drop a jar"".;;;","21/Jul/19 17:07;Shestakov;There is two options to handle *OOM* in java version >= 8u92  [https://www.oracle.com/technetwork/java/javase/8u92-relnotes-2949471.html]

-XX:+ExitOnOutOfMemoryError

-XX:+CrashOnOutOfMemoryError;;;","05/Aug/19 20:07;djoshi;I think this issue might be related to https://bugs.openjdk.java.net/browse/JDK-8027434. Other projects that use the JVM have run into a similar issue and the usual solution is to use [jvmkill|https://github.com/airlift/jvmkill]. The issue at hand is that when a JVM runs out of memory (heap or otherwise), it enters an undefined state. In this situation, I would not expect the handlers to work as expected. I think we should either use jvmkill or [jvmquake|https://github.com/Netflix-Skunkworks/jvmquake] to solve this issue as it has proven to be reliable and Netflix, Facebook and other large JVM users are actively using it.;;;","05/Aug/19 20:31;benedict;Sorry, I completely forgot to respond to this ticket so thanks for bumping it [~djoshi3]

From my POV, including a C JVMTI agent is absolutely fine, [~jolynch].  We'd have to take a closer look at jvmkill and jvmquake, and do our own brief audit of the version we include to ensure it seems to behave reasonably.  But I don't see any problem with utilising non-Java functionality.;;;","05/Aug/19 21:04;djoshi;Sounds great. [~benedict] who would be able to take up the audit? Is this something I can help with?;;;","06/Aug/19 21:46;yifanc;Several experiments of the OOM scenario are made to check if the HotSpot handlers work as expected, namely kill the process. 
 
The result shows that the handlers, OnOutOfMemoryError and ExitOnOutOfMemoryError, are only effective for heap OOM. 
 
*Experiments*
 
The experiments are designed to emulate what happens in C* while being minimal. They have the Thread.setDefaultUncaughtExceptionHandler installed and just re-throw the OOM error hoping the handlers can take care. 
 
OpenJDK 8 was used.
 
 
You can find all the 5 experiments in the attached [^oom-experiments.zip].

{code:java}
├── OomExperimentExceedsDirectBuffer.java
├── OomExperimentExceedsDirectBufferRapidAlloc.java
├── OomExperimentExceedsHeap.java
├── OomExperimentSimple.java
└── OomExperimentSimpleJustExit.java{code}
Among those experiments, there is only one (OomExperimentExceedsHeap) can successfully trigger the handlers. 
 
The rest do throw the OutOfMemoryError, but the handlers are not triggered. 
 
*Some Research*
 
The cause could be due to the difference of the code path in JVM implementation to allocate memory on heap and for direct buffer. (OpenJDK8 is the reference)
 
Heap memory allocation happens at [collectedHeap.inline.hpp#CollectedHeap::common_mem_allocate_noinit|https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/aa318070b27849f1fe00d14684b2a40f7b29bf79/hotspot/src/share/vm/gc_interface/collectedHeap.inline.hpp#L149]. When it failed, it calls [report_java_out_of_memory|https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/aa318070b27849f1fe00d14684b2a40f7b29bf79/hotspot/src/share/vm/utilities/debug.cpp#L287], which is responsible to create a heap dump on OOM and run the handlers. 
 
Meanwhile, allocating direct buffer take a different path. In java.nio.DirectByteBuffer, OOM can happen at 2 places. 
1. Bits.reserveMemory, finds out there is not enough direct memory and throws OOM. In this case, I do not think the OOM is caught and handled in JVM to trigger report_java_out_of_memory.
2. unsafe.allocateMemory, which calls malloc directly, but [failed to allocate and throws OOM|https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/aa318070b27849f1fe00d14684b2a40f7b29bf79/hotspot/src/share/vm/prims/unsafe.cpp#L606]. Again, such OOM was throw in order to let the application to handle. 
 
Another proof is that [report_java_out_of_memory|https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/aa318070b27849f1fe00d14684b2a40f7b29bf79/hotspot/src/share/vm/utilities/debug.cpp#L287], the only place to trigger the handler, was not invoked during unsafe.allocateMemory. Here are [all the references of the method invocation|https://github.com/AdoptOpenJDK/openjdk-jdk8u/search?q=report_java_out_of_memory&unscoped_q=report_java_out_of_memory].
 
Because of that, jvmkill or jvmquake mentioned in the ticket might not work. The tool replies on the notification of the [JvmtiExport::post_resource_exhausted|https://github.com/AdoptOpenJDK/openjdk-jdk8u/blob/aa318070b27849f1fe00d14684b2a40f7b29bf79/hotspot/src/share/vm/gc_interface/collectedHeap.inline.hpp#L153], which does not present in the 2 places that direct buffer OOM can happen. Here is the implementation of [jvmkill|https://github.com/airlift/jvmkill/blob/master/jvmkill.c#L24] (less than 100 lines).;;;","06/Aug/19 23:25;jolynch;[~yifanc] If you are ok with it I can add your test cases to [jvmquake|https://github.com/Netflix-Skunkworks/jvmquake/tree/master/tests] to ensure it handles all edge cases. For what it's worth jvmquake is a strict superset of jvmkill and I wouldn't advocate for using jvmkill (I'm biased though). In my production experience jvmquake actually works at detecting GC spirals of death that C* runs into while jvmkill simply doesn't work as C* doesn't actually go OOM, it just death spirals. See the ""hard oom""  [test cases|https://github.com/Netflix-Skunkworks/jvmquake/blob/master/tests/test_hard_ooms.py] for example where jvmkill won't work while jvmquake will work.;;;","06/Aug/19 23:42;yifanc;[~jolynch], you are welcome. Please use them.

The test cases attached are more on the `Unsafe.allocateMemory` path. As far as I can see, they are different from the ones included in the jvmquake's test cases that only check the heap OOM. ;;;","15/Mar/20 02:29;djoshi;Followed up with [~jolynch] regarding his original comment about including C JVMTI agents in C*. If we build the agent for the officially supported JVMs, we should be good. We need to detect the platform, JVM combo and load it up. If the agent is unavailable for the specific VM/Platform combination, it can be disabled with a warning in the logs much like what we do with `NativeLibrary` except this will need to happen as part of the startup script.;;;","08/Apr/20 01:56;manish.c.ghildiyal@gmail.com;Please let me know if I can contribute here.;;;","05/May/20 05:56;jolynch;Quick update on this from the jvmquake side we are now building [architecture specific artifacts|https://github.com/Netflix-Skunkworks/jvmquake/releases] that will work with any JVM newer than Java 8, they link only against the platform specific libc (we're also now testing on Java 8 and 11, on both zulu and openjdk JVMs). I think this means it would be plausible to include the {{libjvmquake-linux-x86_64.so}} in {{libs}} and then have a switch on uname -s -m to determine to pick it up or not. Right now we're only building for linux amd64 but if there is interest I can generate more architectures (linux arm probably makes sense, and could do osx). I also still like the idea of having a agents/available and agents/enabled folder like apache does for modules, users can just symlink agents from one to the other to include them (and we can symlink jamm and jvmquake by default).

[~yifanc] I agree that the OutOfMemory conditions that do not result in ""true"" JVM OOM (meaning that it would cause a heapdump via {{HeapDumpOnOutOfMemory}}) such as direct buffer allocations will not get caught by jvmquake, my testing confirms your findings, although the jvmquake GC instability algorithm will still trigger in various real world scenarios I've run into.

I feel like the right move might be to walk back a small bit of CASSANDRA-13006 where we stopped forcibly killing the JVM ourselves and let the JVM do it. Specifically if the OOM message contains ""Direct buffer memory"" we could do what jvmquake does and force the JVM into a ""normal"" OOM by [allocating large long arrays|https://github.com/Netflix-Skunkworks/jvmquake/blob/master/src/jvmquake.c#L103]. This will then trigger a proper OOM and get us heap dumping. It's relatively easy to ignore the ""sacrificial"" long array in a heap dump and we could log clearly what is happening.;;;","05/May/20 07:07;yifanc;Thanks [~jolynch] for the update.
{quote}force the JVM into a ""normal"" OOM by [allocating large long arrays|https://github.com/Netflix-Skunkworks/jvmquake/blob/master/src/jvmquake.c#L103]
{quote}
An alternative way could be programmatically grab the heap dump via [JMX|https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/master/src/jdk.management/share/classes/com/sun/management/HotSpotDiagnosticMXBean.java#L75] and exit.;;;","05/May/20 16:44;jolynch;> An alternative way could be programmatically grab the heap dump via [JMX|https://github.com/AdoptOpenJDK/openjdk-jdk11/blob/master/src/jdk.management/share/classes/com/sun/management/HotSpotDiagnosticMXBean.java#L75] and exit.

I believe that was more or less what C* was doing before CASSANDRA-13006 if I'm reading the patch in [02aba73|https://github.com/apache/cassandra/commit/02aba73] correctly, and Eric Evans pointed out this approach in general can cause the C*'s jmap heap dump to race with the JVM heap dump and advocated for just letting the JVM handle it with built in options. The nice thing about the jvmquake technique of just running the heap out of memory is all the normal JVM options work as expected (logging and dumping heap to a particular location on disk mostly). That being said, I think that for the direct buffer issue in particular this won't be a problem since as we've established the JVM OOM report_java_out_of_memory isn't triggered on direct memory allocation failures.;;;","05/May/20 19:10;yifanc;Got it. I did not look closely enough at the discussions in  CASSANDRA-13006. 

I agree that leaving it to JVM is a more clean and general solution. Also as you mentioned, ""It's relatively easy to ignore the ""sacrificial"" long array in a heap dump and we could log clearly what is happening.""

Since one should be able to trigger the OOM by looping allocating large chunk of memory, e.g. array, in the java code. What is the benefit of doing it so using jvmquake? I can see that in the killer_thread callback function, it also does long array allocation once notified by the gc callback. 

The comment of the callback says
{quote}the only way to reliably trigger OutOfMemory
 when we are not actually out of memory (e.g. due to GC behavior) that I
 could find was to make JNI calls that allocate large blobs of memory which
 can only be done from outside of the GC callbacks.
{quote}
Can you elaborate more about preferring jvmquake? ;;;","05/May/20 23:49;jolynch;> Since one should be able to trigger the OOM by looping allocating large chunk of memory, e.g. array, in the java code. What is the benefit of doing it so using jvmquake? I can see that in the killer_thread callback function, it also does long array allocation once notified by the gc callback. 

Ah sorry I was not clear. I think the JVMStabilityDetector (which we call into via inspectThrowable all over the place) should allocate the long array if we see an OutOfMemoryError with message ""Direct buffer memory"", in turn triggering a Heap OOM (which will trigger the normal resource exhausted mechanism). Since we're not out of _heap_ memory we can trust that JVMStatbilityDetector can run.

I guess my proposal is to include jvmquake by default for linux deployments (I can add more architectures if we want more, easy to opt out), and if JVMStabilityDetector sees a ""Direct buffer memory"" OOM it should force the JVM into a heap OOM, triggering jvmquake's resource exhausted handler.

This setup would guarantee that C* dies (and produces a heap dump) if any of the following conditions hold:
 * The JVM is out of heap memory
 * The JVM has accumulated 30s of GC debt with 1:5 runtime weight (meaning that we had <85% throughput for at least 30s): aka ""GC spirals of death""
 * The JVM is out of metaspace memory
 * The JVM is out of threads
 * (best effort, likely true) The JVM is out of native memory (so basically C* is using 2x the heap size) -> triggers a heap oom -> triggers the first case

Unlike the built in JVM options jvmquake really actually works in these edge cases (not only is there a test suite to prove it that the built in Java options don't work but if you run inside the heap you fundamentally can't guarantee you will run, e.g. why the kill -9 approach never really works).;;;","09/May/20 00:10;yifanc;Sounds good [~jolynch]. 

So for this ticket, the goal is to force JVM to trigger a Heap OOM upon receiving the direct buffer OOM. (I can work on it.) 

Do you want to the jvmquake integration be addressed in a different ticket? ;;;","03/Jul/20 09:33;snazy;Just read this ticket and the approach looks absolutely reasonable to me.

One thing though is that the the (off-heap) row-cache isn't covered here - let me know whether it's reasonable to add some support regarding this ticket. IMHO, people shouldn't use the row-cache, but I'm not sure whether there are reasonable use cases out there in the wild. Don't want to start a discussion about the row-cache in this, just a heads-up.;;;","18/Sep/20 00:27;yifanc;> Netty (at least, and perhaps elsewhere in Executors) catches all exceptions, so presently there is no way to ensure that an OOM reaches the JVM handler to trigger a crash/heapdump.

Running a code inspection, the exception/throwable from Netty is already handled. 
For inbound, the {{InboundMessageHandler}} implements {{exceptionCaught()}} which invokes {{JVMStabilityInspector}}. The message handler is the last one in the inbound direction, and there is no previous handler that handles exceptions. So the message handler should handle all exceptions from that direction. However, the {{exceptionCaught()}} override in {{StreamingInboundHandler}} does not invoke  {{JVMStabilityInspector}}. It could swallow OOM errors. 
For outbound, {{JVMStabilityInspector}} is invoked when the channel future fails, and several other places. 

All the above callsites call {{JVMStabilityInspector}} with {{propagateOutOfMemory}} disabled. So the inspector just swallows the OOM errors and not let JVM to handle. [~benedict], what is the reason for doing so in the inbound/outbound connections? ;;;","18/Sep/20 09:05;benedict;Zoom out a bit - where would that exception end up if it were rethrown?  I can't remember precisely, but it is caught by Netty's default exception handling and iirc simply logged.;;;","18/Sep/20 23:26;yifanc;> where would that exception end up if it were rethrown

{{JVMStabilityInspector}} only re-throws {{OutOfMemoryError}}. Depending on the presence of those OOM-related JVM options, {{OnOutOfMemoryError}}, {{ExitOnOutOfMemoryError}} or {{HeapDumpOnOutOfMemoryError}}, the JVM exits and trigger a heap dump if it is a heap space OOM error. 

However, the call-sites indicate to not re-throw OOM error (e.g. [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/InboundMessageHandler.java#L647-L659]), which I'd like to learn why we do not let the JVM to exit.

Netty by default just logs the exception, when {{exceptionCaught()}} is _not_ implemented in any of the handler in the inbound direction. For the outbound, client code handles exception by adding listener to {{ChannelFuture}} or {{ChannelPromise}}. We have the handling in both directions. 
Besides the inbound/outbound pathes, it looks like that Netty does do a lot of catch-{{Throwable}}-and-swallow things in its code base. So it is possible that errors from Netty internal are not bubbled up. For example, this [issue|https://github.com/netty/netty/issues/6096]. ;;;","19/Sep/20 07:09;benedict;{quote}So it is possible that errors from Netty internal are not bubbled up
{quote}
As I have said, they are not, and that is the reason this ticket was filed.  I ascertained this at a time when I was intimately familiar with Netty’s workings, which I am not any longer.  I may have made a mistake, or Netty may have been updated to a version where this changed, but let's operate under the assumption I was correct at the time of filing this ticket, until proven otherwise.

The non-propagation of OOM by inspectThrowable was probably used precisely because propagating it was thought to achieve nothing besides logging against Netty's internal loggers (and failing to shutdown would leave the channel in a worse state, as we would not have finished tidying up as a result of the exception), but I agree we should have left a TODO directly in the code.;;;","29/Sep/20 22:55;yifanc;Talked with Benedict on Slack and cleaned up my confusion. So the {{JVMStabilityInspector}} is able to inspect the OOM error. But after it re-throws, Netty catches all throwables and simply logs. It happens [here|https://github.com/netty/netty/blob/4.1/transport/src/main/java/io/netty/channel/AbstractChannelHandlerContext.java#L303-L316]. Therefore, the {{propagateOutOfMemory}} parameter was added. 

I submitted a PR that allows to produce a heap space OOM error forcefully when catching a direct buffer OOM. 
The PR also removes the parameter {{propagateOutOfMemory}} in the {{JVMStabilityInspector}}. Because it makes sure the instance can crash/exit properly on OOM. (see the gist below)

PR: https://github.com/apache/cassandra/pull/761
CI: https://app.circleci.com/pipelines/github/yifan-c/cassandra/112/workflows/293a4334-d2df-43f9-b532-1d79876701c1

I have also created a separate demo to prove that JVM invokes the OOM handler even if such OOM error (not including the direct buffer one) is to be swallowed by a catch block. 
The code and the output can be found at the gist: https://gist.github.com/yifan-c/82ff4fd7fbe83fe41113f6f14cba4907.;;;","06/Nov/20 20:40;dcapwell;+1 from me with small comment, see PR.

I tested this patch by breaking byte buffer allocation to run out of direct memory, in doing so found an edge case on client (.transport package) code, so once that is fixed client and internode shut down on OOM.;;;","09/Nov/20 23:51;dcapwell;+1

Need second reviewer, can merge after.;;;","11/Nov/20 16:49;jwest;Left a couple minor comments on GH. Overall the patch looks good. I would like to see test runs for upgrade jvm dtests and the python dtests before merging. ;;;","11/Nov/20 21:31;yifanc;Thanks [~jwest]. Addressed you comments and run CI (unit, jvm dtest and dtest) after rebasing. There are a few test failures, but do not look related to the change. 

CI result: https://app.circleci.com/pipelines/github/yifan-c/cassandra/159/workflows/a37b8a85-b705-479e-b7ca-846bb71b36dc;;;","19/Nov/20 17:16;dcapwell;CI Results: Yellow.  Expected COMPACT STORAGE upgrade test that Alex is looking into, and org.apache.cassandra.distributed.test.SimpleReadWriteTest, though that test passes when I reran locally
||Branch||Source||Circle CI||Jenkins||
|trunk|[branch|https://github.com/dcapwell/cassandra/tree/commit_remote_branch/CASSANDRA-15214-trunk-C01561D4-BCE5-4B0B-B8F3-4D57E308657A]|[build|https://app.circleci.com/pipelines/github/dcapwell/cassandra?branch=commit_remote_branch%2FCASSANDRA-15214-trunk-C01561D4-BCE5-4B0B-B8F3-4D57E308657A]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/221/]|;;;","21/Dec/21 02:01;paulo;I noticed that after this patch we produce the synthetic heap OOM even if the crash/exit on oom flags are not set, which can happen if the user upgrades from an older version without updating ""cassandra-env.sh"". This can potentially leave the JVM in a worse state if we fill the heap and the JVM is not killed. Do you think it makes sense to only generate the Heap OOM when the crash/exit on OOM flags are set [~jolynch] [~yifanc] ?;;;","22/Dec/21 17:42;yifanc;[~paulo], your suggestion makes sense. The synthetic heap OOM assumes users run cassandra with the on OOM flags. So it is reasonable to check the existence of the related options. I started a separate ticket: CASSANDRA-17227. 
Would the JVM result into a worse state due to the synthetic heap OOM? Probably not. The (actual) OOM happened and the JVM is in a bad state already. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DecayingEstimatedHistogramReservoir Inefficiencies,CASSANDRA-15213,13245154,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jwest,benedict,benedict,16/Jul/19 09:20,21/Dec/20 08:08,13/Jul/23 08:38,19/Feb/20 11:34,4.0,4.0-alpha4,,,,Observability/Metrics,,,,0,,,,"* {{LongAdder}} introduced to trunk consumes 9MiB of heap without user schemas, and this will grow significantly under contention and user schemas with many tables.  This is because {{LongAdder}} is a very heavy class designed for single contended values.  
 ** This can likely be improved significantly, without significant loss of performance in the contended case, by simply increasing the size of our primitive backing array and providing multiple buckets, with each thread picking a bucket to increment, or simply multiple backing arrays.  Probably a better way still to do this would be to introduce some competition detection to the update, much like {{LongAdder}} utilises, that increases the number of backing arrays under competition.
 ** To save memory this approach could partition the space into chunks that are likely to be updated together, so that we do not need to duplicate the entire array under competition.
 * Similarly, binary search is costly and a measurable cost as a share of the new networking work (without filtering it was > 10% of the CPU used overall).  We can compute an approximation floor(log2 n / log2 1.2) extremely cheaply, to save the random memory access costs.",,benedict,cnlwsu,eperott,jeromatron,jwest,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14281,,,,,,,,,,,,,"06/Feb/20 21:54;jwest;15213-perf-branch;https://issues.apache.org/jira/secure/attachment/12992809/15213-perf-branch","06/Feb/20 21:54;jwest;15213-perf-trunk;https://issues.apache.org/jira/secure/attachment/12992810/15213-perf-trunk",,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,jwest,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Normal,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Feb 18 17:59:57 UTC 2020,,,,,,,All,,,,,"0|z04pig:",9223372036854775807,,,,,,,benedict,jwest,,,Low,,4.0-alpha,4.0-alpha1,,"[adc3cdde2ae2a78f7d2bb66da47a07545d3e06cf|https://github.com/apache/cassandra/commit/adc3cdde2ae2a78f7d2bb66da47a07545d3e06cf]",,,,,,,,,"Existing microbenchmarks, new cluster benchmarks, quick theories tests",,,,,"08/Jan/20 19:21;jwest;I’ve verified the reported {{LongAdder}} memory consumption. I’m seeing 13.5MiB right after startup.

A few questions about your proposed solutions?
 * By the backing array do you mean the original AtomicLongArray? Two issues I see with increasing the number of buckets: the maximum we can create are 237 (then the offset overflows) and increasing the number of buckets doesn’t necessarily reduce contention. This is because the resolution of the buckets doesn’t change except for the last bucket (which for microseconds is over 200 days and for nanoseconds is over 5 hours). Or are you suggesting increasing the bucket resolution by decreasing the factor in {{newOffsets}} when adding more buckets?
 * I’m not sure what you mean by “providing multiple buckets” but I suspect it might be what I am missing to better understand the proposal. Do you mean having multiple buckets per offset? Additionally, re: your {{LongAdder}} reference, are you proposing creating new buckets for the same offset under contention (like {{LongAdder}} does w/ {{Cell}})?
 * Regarding the binary search suggestion, are you suggesting using the approximation to replace the maximum index (e.g. binarySearch(offsets, 0, Math.floor(….) + 1, value)? Or to outright use that approximation? I wasn’t sure of your exact proposal but it does seem like using the equation you gave as an upper bound estimate and {{Math.floor(Math.log(Math.floor(n / 1.2)) / Math.log(1.2))}} as a lower bound estimate can reduce the space of the array we have to search by 25-60% (I saw up to 75% in one case with DEFAULT_BUCKET_COUNT but haven’t reproduced that high of a reduction since). I’ve added a QuickTheories test to show this here: [https://github.com/jrwest/cassandra/commit/a4795bc651cd0e0bd582a8df2414e312782b5020]. I should note however that this doesn't translate into a performance improvement in the microbenchmark -- perhaps because it really only reduces the number of accesses slightly at the cost of more calculations.
 * We could also circumvent the search entirely in the lowest buckets by using {{if value < bucketCount && bucketOffsets[value - 1] == value}} although given our use case its unlikely we will see sub 165 ns operations often.;;;","08/Jan/20 20:12;benedict;bq. Or are you suggesting increasing the bucket resolution
bq. Do you mean having multiple buckets per offset? 

Right.  I meant one option was to essentially inline the striping by introducing, e.g., 4x as many buckets as requested.  Each logical bucket can be represented by the sum of that many buckets.  The buckets would be offset from each other by at least two cache lines, and all buckets would ideally be more uniformly distributed in memory (by e.g. {{Integer.reverse(idx)}}), so that adjacent buckets that are most likely to be used together don't also compete for cache locks.

bq. Additionally, re: your LongAdder reference, are you proposing creating new buckets for the same offset under contention (like LongAdder does w/ Cell)?

I meant an alternative approach might be to batch a range of buckets together - say 16 or more, to amortise the object overhead - and to perform {{LongAdder}}’s inflation of the number of buckets on the detection of competition, so that we can gradually increase the number of cells needed.  This might have the added benefit of not wasting space for striping of buckets that are rarely used, but at the cost of greater complexity, object overheads and indirection.

bq. Regarding the binary search suggestion, are you suggesting using the approximation to replace the maximum index

I meant to use it as the floor for a linear search for the correct position, so that it would ordinarily be 1 or 2 comparisons, but typically only one cache miss.

Specifically, I meant to use an integer approximation, along the lines of...

{code}
private static final int TABLE_BITS = 4;
private static final int TABLE_MASK = -1 >>> (32 - TABLE_BITS);
private static final float[] LOG2_TABLE = computeTable(TABLE_BITS);
private static final float log2_12 = (float) slowLog2(1.2d);
private static final float log2_12_recp = (float) (1d / slowLog2(1.2d));

private static float[] computeTable(int bits)
{
    float[] table = new float[1 << bits];
    for (int i = 1 ; i < 1<<bits ; ++i)
        table[i] = (float) slowLog2(ratio(i, bits));
    return table;
}

private static float fastLog12(long v)
{
    return fastLog2(v) * log2_12_recp;
}

private static float fastLog2(long v)
{
    if (v == 0) return 0;
    int highestBitPosition = 63 - Long.numberOfLeadingZeros(v);
    v = Long.rotateRight(v, highestBitPosition - TABLE_BITS);
    int index = (int) (v & TABLE_MASK);
    float result = LOG2_TABLE[index];
    result += highestBitPosition;
    return result;
}

private static double slowLog2(double v)
{
    return Math.log(v) / Math.log(2);
}

private static double slowLog12(double v)
{
    return (Math.log(v) / Math.log(2)) * log2_12_recp;
}

private static double ratio(int i, int bits)
{
    return Float.intBitsToFloat((127 << 23) | (i << (23 - bits)));
}
{code}



;;;","08/Jan/20 21:56;benedict;FWIW, I think when benchmarking something like this you need to create a few hundred MiB worth of backing arrays, and cycle through them for each test.  Or, at least, there are different ways to achieve this but you ideally want tests that include memory latency and this is a simple mechanism to achieve that.;;;","09/Jan/20 07:05;jwest;Thanks for the clarification. I will take a stab at something to replace the usage of {{LongAdder[]}} along the lines of what you suggested and share when ready. I agree re: benchmarking and will try and come up with a more complete set -- clearly we used the existing benchmarks before without finding this issue. 

Regarding the binary search, I am still a bit confused. If you run: https://github.com/jrwest/cassandra/blob/jwest/15213/test/unit/org/apache/cassandra/metrics/DecayingEstimatedHistogramReservoirTest.java#L58 you will see that the proposed estimation (using the code provided) would result in several cases where a linear search would lead to over 10-50 accesses (as value grows the approximation gets more inaccurate) and {{Arrays.binarySearch(offsets, value) <= fastLog12(value)}} always (even when correctly adjusting the return value of the binary search for negative values). 

Fwiw, the proposed fast log code does improve performance of my local modifications when benchmarked but basically brings it back inline w/ the existing implementation (caveat: benchmarking issues already mentioned). ;;;","09/Jan/20 12:04;benedict;{quote}Fwiw, the proposed fast log code does improve performance of my local modifications when benchmarked but basically brings it back inline w/ the existing implementation (caveat: benchmarking issues already mentioned).
{quote}
Achieving equivalent performance in this synthetic benchmark is pretty much all we need, I think. This should predict a strict performance win once memory latency is taken into account, as our largest such array is ~1.2KiB, with 150 elements, leading to a predicted 5 memory stalls for binary search - although execution will proceed speculatively and may mask one or so of those stalls. If the approximation can achieve an optimal 1 memory stall in the worst case, with similar performance when the entire structure is in cache, it’s a win in my book.
{quote}would result in several cases where a linear search would lead to over 10-50 accesses
{quote}
Interesting, I’m not sure what’s happening in that case. For comparison, I ran the code below
{code:java}
long[] offsets = newOffsets(160, true);
for (int i = 0 ; i < offsets.length ; ++i)
{
    long start = i == 0 ? 0 : offsets[i - 1] + 1;
    long end = offsets[i];
    System.out.printf(""%d %d %d %d %.1f %.1f %.1f %.1f\n"", i, end, (int)fastLog12(start) - i, (int)fastLog12(end) - i, fastLog12(start), fastLog12(end), slowLog12(start), slowLog12(end));
}
{code}
Which yields the below results, with columns:
 # The index in the offsets array
 # The value associated with the bucket
 # The integer difference between fastLog(lowest) and this offset, where lowest is the smallest value we want to bucket at this offset
 # This integer difference between fastLog(highest)
 # fastLog(lowest)
 # fastLog(highest)
 # slowLog(lowest)
 # slowLog(highest)

It looks to me like the approximation is always within precisely 2 or 3 of the offset, but _ahead_ by 2 or 3, because the array doesn’t follow a strict 1.2 logarithm given the first few buckets must increment by whole integers. So for values > 2, we can subtract 3 before walking forwards at most 1, incurring only a single memory stall. If we wanted to be really clever this could be implemented branchlessly, so that this memory latency wouldn’t pause further execution, or invalidate the pipeline, only consume some reorder buffer slots and register file entries.
{code:java}
0 0 0 0 0.0 0.0 -Infinity -Infinity
1 1 -1 -1 0.0 0.0 0.0 0.0
2 2 1 1 3.8 3.8 3.8 3.8
3 3 3 3 6.0 6.0 6.0 6.0
4 4 3 3 7.6 7.6 7.6 7.6
5 5 3 3 8.8 8.8 8.8 8.8
6 6 3 3 9.8 9.8 9.8 9.8
7 7 3 3 10.7 10.7 10.7 10.7
8 8 3 3 11.4 11.4 11.4 11.4
9 10 3 3 12.1 12.6 12.1 12.6
10 12 3 3 13.2 13.6 13.2 13.6
11 14 3 3 14.1 14.5 14.1 14.5
12 17 2 3 14.9 15.5 14.9 15.5
13 20 2 3 15.9 16.4 15.9 16.4
14 24 2 3 16.7 17.4 16.7 17.4
15 29 2 3 17.7 18.5 17.7 18.5
16 35 2 3 18.7 19.3 18.7 19.5
17 42 2 3 19.7 20.5 19.7 20.5
18 50 2 3 20.5 21.5 20.6 21.5
19 60 2 3 21.5 22.5 21.6 22.5
20 72 2 3 22.5 23.5 22.5 23.5
21 86 2 3 23.5 24.3 23.5 24.4
22 103 2 3 24.3 25.3 24.5 25.4
23 124 2 3 25.5 26.4 25.5 26.4
24 149 2 3 26.4 27.3 26.5 27.4
25 179 2 3 27.3 28.4 27.5 28.5
26 215 2 3 28.4 29.3 28.5 29.5
27 258 2 3 29.5 30.4 29.5 30.5
28 310 2 3 30.4 31.4 30.5 31.5
29 372 2 3 31.4 32.4 31.5 32.5
30 446 2 3 32.4 33.3 32.5 33.5
31 535 2 3 33.3 34.2 33.5 34.5
32 642 2 3 34.2 35.4 34.5 35.5
33 770 2 3 35.4 36.4 35.5 36.5
34 924 2 3 36.4 37.3 36.5 37.5
35 1109 2 3 37.3 38.4 37.5 38.5
36 1331 2 3 38.4 39.2 38.5 39.5
37 1597 2 3 39.2 40.2 39.5 40.5
38 1916 2 3 40.2 41.3 40.5 41.5
39 2299 2 3 41.3 42.2 41.5 42.5
40 2759 2 3 42.2 43.3 42.5 43.5
41 3311 2 3 43.3 44.3 43.5 44.5
42 3973 2 3 44.3 45.4 44.5 45.5
43 4768 2 3 45.4 46.3 45.5 46.5
44 5722 2 3 46.3 47.4 46.5 47.5
45 6866 2 3 47.4 48.3 47.5 48.5
46 8239 2 3 48.3 49.4 48.5 49.5
47 9887 2 3 49.4 50.4 49.5 50.5
48 11864 2 3 50.4 51.4 50.5 51.5
49 14237 2 3 51.4 52.3 51.5 52.5
50 17084 2 3 52.3 53.2 52.5 53.5
51 20501 2 3 53.2 54.4 53.5 54.5
52 24601 2 3 54.4 55.4 54.5 55.5
53 29521 2 3 55.4 56.3 55.5 56.5
54 35425 2 3 56.3 57.4 56.5 57.5
55 42510 2 3 57.4 58.3 57.5 58.5
56 51012 2 3 58.3 59.3 58.5 59.5
57 61214 2 3 59.3 60.3 59.5 60.5
58 73457 2 3 60.3 61.2 60.5 61.5
59 88148 2 3 61.2 62.3 61.5 62.5
60 105778 2 3 62.3 63.3 62.5 63.5
61 126934 2 3 63.3 64.3 63.5 64.5
62 152321 2 3 64.3 65.3 64.5 65.5
63 182785 2 3 65.3 66.4 65.5 66.5
64 219342 2 3 66.4 67.3 66.5 67.5
65 263210 2 3 67.3 68.4 67.5 68.5
66 315852 2 3 68.4 69.4 68.5 69.5
67 379022 2 3 69.4 70.4 69.5 70.5
68 454826 2 3 70.4 71.3 70.5 71.5
69 545791 2 3 71.3 72.2 71.5 72.5
70 654949 2 3 72.2 73.2 72.5 73.5
71 785939 2 3 73.2 74.2 73.5 74.5
72 943127 2 3 74.2 75.3 74.5 75.5
73 1131752 2 3 75.3 76.4 75.5 76.5
74 1358102 2 3 76.4 77.3 76.5 77.5
75 1629722 2 3 77.3 78.3 77.5 78.5
76 1955666 2 3 78.3 79.3 78.5 79.5
77 2346799 2 3 79.3 80.2 79.5 80.5
78 2816159 2 3 80.2 81.3 80.5 81.5
79 3379391 2 3 81.3 82.3 81.5 82.5
80 4055269 2 3 82.3 83.3 82.5 83.5
81 4866323 2 3 83.3 84.3 83.5 84.5
82 5839588 2 3 84.3 85.4 84.5 85.5
83 7007506 2 3 85.4 86.3 85.5 86.5
84 8409007 2 3 86.3 87.4 86.5 87.5
85 10090808 2 3 87.4 88.4 87.5 88.5
86 12108970 2 3 88.4 89.4 88.5 89.5
87 14530764 2 3 89.4 90.3 89.5 90.5
88 17436917 2 3 90.3 91.2 90.5 91.5
89 20924300 2 3 91.2 92.2 91.5 92.5
90 25109160 2 3 92.2 93.2 92.5 93.5
91 30130992 2 3 93.2 94.3 93.5 94.5
92 36157190 2 3 94.3 95.4 94.5 95.5
93 43388628 2 3 95.4 96.3 95.5 96.5
94 52066354 2 3 96.3 97.3 96.5 97.5
95 62479625 2 3 97.3 98.3 97.5 98.5
96 74975550 2 3 98.3 99.2 98.5 99.5
97 89970660 2 3 99.2 100.3 99.5 100.5
98 107964792 2 3 100.3 101.3 100.5 101.5
99 129557750 2 3 101.3 102.3 101.5 102.5
100 155469300 2 3 102.3 103.3 102.5 103.5
101 186563160 2 3 103.3 104.4 103.5 104.5
102 223875792 2 3 104.4 105.3 104.5 105.5
103 268650950 2 3 105.3 106.4 105.5 106.5
104 322381140 2 3 106.4 107.4 106.5 107.5
105 386857368 2 3 107.4 108.4 107.5 108.5
106 464228842 2 3 108.4 109.3 108.5 109.5
107 557074610 2 3 109.3 110.3 109.5 110.5
108 668489532 2 3 110.3 111.2 110.5 111.5
109 802187438 2 3 111.2 112.2 111.5 112.5
110 962624926 2 3 112.2 113.3 112.5 113.5
111 1155149911 2 3 113.3 114.4 113.5 114.5
112 1386179893 2 3 114.4 115.3 114.5 115.5
113 1663415872 2 3 115.3 116.3 115.5 116.5
114 1996099046 2 3 116.3 117.3 116.5 117.5
115 2395318855 2 3 117.3 118.2 117.5 118.5
116 2874382626 2 3 118.2 119.3 118.5 119.5
117 3449259151 2 3 119.3 120.3 119.5 120.5
118 4139110981 2 3 120.3 121.3 120.5 121.5
119 4966933177 2 3 121.3 122.3 121.5 122.5
120 5960319812 2 3 122.3 123.4 122.5 123.5
121 7152383774 2 3 123.4 124.3 123.5 124.5
122 8582860529 2 3 124.3 125.3 124.5 125.5
123 10299432635 2 3 125.3 126.4 125.5 126.5
124 12359319162 2 3 126.4 127.4 126.5 127.5
125 14831182994 2 3 127.4 128.3 127.5 128.5
126 17797419593 2 3 128.3 129.3 128.5 129.5
127 21356903512 2 3 129.3 130.2 129.5 130.5
128 25628284214 2 3 130.2 131.3 130.5 131.5
129 30753941057 2 3 131.3 132.3 131.5 132.5
130 36904729268 2 3 132.3 133.4 132.5 133.5
131 44285675122 2 3 133.4 134.3 133.5 134.5
132 53142810146 2 3 134.3 135.3 134.5 135.5
133 63771372175 2 3 135.3 136.3 135.5 136.5
134 76525646610 2 3 136.3 137.2 136.5 137.5
135 91830775932 2 3 137.2 138.4 137.5 138.5
136 110196931118 2 3 138.4 139.3 138.5 139.5
137 132236317342 2 3 139.3 140.3 139.5 140.5
138 158683580810 2 3 140.3 141.3 140.5 141.5
139 190420296972 2 3 141.3 142.4 141.5 142.5
140 228504356366 2 3 142.4 143.3 142.5 143.5
141 274205227639 2 3 143.3 144.3 143.5 144.5
142 329046273167 2 3 144.3 145.4 144.5 145.5
143 394855527800 2 3 145.4 146.2 145.5 146.5
144 473826633360 2 3 146.2 147.3 146.5 147.5
145 568591960032 2 3 147.3 148.3 147.5 148.5
146 682310352038 2 3 148.3 149.2 148.5 149.5
147 818772422446 2 3 149.2 150.3 149.5 150.5
148 982526906935 2 3 150.3 151.3 150.5 151.5
149 1179032288322 2 3 151.3 152.4 151.5 152.5
150 1414838745986 2 3 152.4 153.3 152.5 153.5
151 1697806495183 2 3 153.3 154.3 153.5 154.5
152 2037367794220 2 3 154.3 155.3 154.5 155.5
153 2444841353064 2 3 155.3 156.2 155.5 156.5
154 2933809623677 2 3 156.2 157.4 156.5 157.5
155 3520571548412 2 3 157.4 158.3 157.5 158.5
156 4224685858094 2 3 158.3 159.3 158.5 159.5
157 5069623029713 2 3 159.3 160.3 159.5 160.5
158 6083547635656 2 3 160.3 161.4 160.5 161.5
159 7300257162787 2 3 161.4 162.3 161.5 162.5
160 8760308595344 2 3 162.3 163.3 162.5 163.5
{code};;;","09/Jan/20 12:38;benedict;bq. It looks to me like the approximation is always within precisely 2 or 3 of the offset

FWIW, this appears to hold just fine up to {{long}} overflow;;;","09/Jan/20 15:31;jwest;Ok I figured out what was going on. My linked test was generating only large values and the approximation of only the last bucket is very inaccurate as value grows but also easy to account for. Fixing this the test confirms your findings (sometimes I see a distance of 4 indexes but still a huge improvement):

Sample from 10k test run:
{code:java}
model = 80, estimate = 83
model = 83, estimate = 86
model = 81, estimate = 84
model = 82, estimate = 85
model = 69, estimate = 72
model = 79, estimate = 82
model = 81, estimate = 84
model = 79, estimate = 82
model = 75, estimate = 79
model = 83, estimate = 86
model = 77, estimate = 81
model = 72, estimate = 75
model = 84, estimate = 88
model = 61, estimate = 64
{code};;;","09/Jan/20 17:32;benedict;It might be the difference is whether or not we're including a zero bucket?  Because the fixed offset we use probably needs to be incremented by one in this case, since there's no logarithm of zero.

edit: yep, confirmed that if I pass {{false}} I get values of 3 or 4.;;;","10/Jan/20 00:41;jwest;My working branch is here: [https://github.com/jrwest/cassandra/tree/jwest/15213]

I've implemented the binary search replacement. Didn't make an attempt at going branchless but even as implemented its faster than the existing implementation. Working on the striping / replacement of {{LongAdder[]}} and will share when ready. 

 
{code:java}
Trunk:


     [java] Benchmark                                     Mode  Cnt         Score         Error  Units
     [java] LatencyTrackingBench.benchInsertToDEHR         thrpt    5  21887453.678 ± 2108481.285  ops/s
     [java] LatencyTrackingBench.benchLatencyMetricsWrite  thrpt    5   8908817.316 ±  115453.642  ops/s


15213 (linear search changes only):


     [java] Benchmark                                       Mode  Cnt         Score         Error  Units
     [java] LatencyTrackingBench.benchInsertToDEHR         thrpt    5  24646022.304 ± 1052105.818  ops/s
     [java] LatencyTrackingBench.benchLatencyMetricsWrite  thrpt    5   9175928.594 ±  269984.204  ops/s
 {code};;;","10/Jan/20 02:02;benedict;Looking good.  A couple of minor suggestions:

# The {{value <= 0}} check can be moved inside of the {{value <= 8}} check, since both will rarely be performed
# Can {{(int) value - (bucketOffsets[0] == 0 ? 0 : 1)}} be simplified to {{(int) value - bucketOffsets[0]}} ?
# {{value > bucketOffsets[bucketOffsets.length - 1]}} could be replaced by (e.g.) {{firstCandidate = min(firstCandidate,bucketOffsets.length-2)}}, which only requires touching the already-used header portion of the array, avoiding one potential cache-miss (though not one measured on the existing benchmarks I expect).

As it stands, I think in the normal course of execution it's already reasonable to expect no mispredicted branches in the implementation you've put up, and these tweaks should improve the situation a little.  I think it's fairly likely the final pair of return statements will be implemented as a {{cmov}}, but it would be interesting to take a look at the final assembly.  When we have a final product maybe we can take a look.;;;","10/Jan/20 07:04;jwest;I've updated [https://github.com/jrwest/cassandra/tree/jwest/15213] with the changes you suggested. Regarding the extra bucket, I realized that even more simply we could return {{bucketOffsets.length}} if the estimate is greater than or equal to it – saving us an access in this case altogether. 

Also added a very simple striping approach -- without any attempts at distributed the buckets more uniformly. The performance is about the same (the runs are within each others' margin of error) and still significantly better than before CASSANDRA-14281. From the memory consumption perspective, however, 4 stripes ends up allocating more than 12MiB after startup. This isn't surprising because with a single stripe (or pre CASSANDRA-14281) after startup was a little more than 3MiB and we're quadrupling the size of each array.  However, these won't grow under contention like {{LongAdder[]}} would: memory consumption is on the order of the number of histograms created not number created and contention as before. I've found comparable performance using 2 stripes on my 4 core machine. I have an 8 core available to me I can test with tomorrow. If the memory consumption is still a concern I can investigate on-demand striping but I prefer the simpler approach and think the trade-offs of 2-4 stripes is reasonable. 

 
{code:java}
 3.0 (4 core):
     [java] Benchmark                                       Mode  Cnt        Score        Error  Units
     [java] LatencyTrackingBench.benchInsertToDEHR         thrpt    5  5848504.963 ± 147881.511  ops/s
     [java] LatencyTrackingBench.benchLatencyMetricsWrite  thrpt    5  1946154.537 ± 623185.290  ops/s


Trunk (4 core)


     [java] Benchmark                                       Mode  Cnt         Score         Error  Units
     [java] LatencyTrackingBench.benchInsertToDEHR         thrpt    5  21887453.678 ± 2108481.285  ops/s
     [java] LatencyTrackingBench.benchLatencyMetricsWrite  thrpt    5   8908817.316 ±  115453.642  ops/s


15213 (4 core, linear search changes only):


     [java] Benchmark                                       Mode  Cnt         Score         Error  Units
     [java] LatencyTrackingBench.benchInsertToDEHR         thrpt    5  24646022.304 ± 1052105.818  ops/s
     [java] LatencyTrackingBench.benchLatencyMetricsWrite  thrpt    5   9175928.594 ±  269984.204  ops/s




15213 (4 core, 4 stripes, striping and linear search):


[java] Benchmark                                       Mode  Cnt         Score        Error  Units
     [java] LatencyTrackingBench.benchInsertToDEHR         thrpt    5  18818181.576 ± 506997.366  ops/s
     [java] LatencyTrackingBench.benchLatencyMetricsWrite  thrpt    5   8895569.814 ± 154219.113  ops/s


 {code}
 ;;;","10/Jan/20 18:43;benedict;I'm inclined to agree that a simple approach is best, and simply offering a tunable parameter to modify the number of stripes on startup might be sufficient.  Here's some further minor feedback/suggestions:

# It's faster to use {{addAndGet}} than {{compareAndSet}} because it's guaranteed to succeed - the increment occurs whilst holding the cache line in an exclusive state, so it will always execute as quickly as the first failed {{compareAndSet}}
# Combined with this, it probably makes most sense to pick a ""random"" bucket using e.g. {{Thread.currentThread().getId() & (stripes-1)}}, to always increment
# Finally, it might make sense to ""shuffle"" the ordinary buckets as well, so that logically adjacent buckets are not spatially adjacent; since a histogram is _likely_ to receive updates to mostly oscillating around a given median point, this means there won't be any false-sharing competition between two updates to two logically adjacent buckets

I think at this point it then makes most sense to improve the benchmarks, in one case to account for memory latency of lookup, and in another to produce more realistic distributions of values.
;;;","13/Jan/20 21:27;jwest;Using {{addAndGet}} I see performance comparable to 3.0 and prior to CASSANDRA-14281.

{code}
15213 (4 core, 4 stripes, striping & linear search, addAndGet):

     [java] Benchmark                                       Mode  Cnt        Score        Error  Units
     [java] LatencyTrackingBench.benchInsertToDEHR         thrpt    5  5742550.865 ± 256043.651  ops/s
     [java] LatencyTrackingBench.benchLatencyMetricsWrite  thrpt    5  1979885.731 ± 117381.276  ops/s

{code}

Here is how I implemented update bucket in case I missed something
{code:java}
    public void updateBucket(AtomicLongArray buckets, int index, long value)
    {
        int stripe = (int) (Thread.currentThread().getId() & (nStripes - 1));
        buckets.addAndGet(stripedIndex(index, stripe), value);
    }
{code};;;","13/Jan/20 21:53;benedict;Interesting.  I get:

{code}
addAndGet
# Run progress: 0.00% complete, ETA 00:00:13
# Fork: 1 of 1
# Warmup Iteration   1: 39047588.027 ops/s
# Warmup Iteration   2: 42432555.911 ops/s
# Warmup Iteration   3: 42290231.202 ops/s
Iteration   1: 42252484.876 ops/s
Iteration   2: 41715818.895 ops/s
Iteration   3: 40307394.422 ops/s
Iteration   4: 40383945.073 ops/s
Iteration   5: 38825162.703 ops/s


Result ""org.apache.cassandra.test.microbench.LatencyTrackingBench.benchInsertToDEHR"":
  40696961.194 ±(99.9%) 5170160.207 ops/s [Average]
{code}

{code}
compareAndSet
# Run progress: 0.00% complete, ETA 00:00:13
# Fork: 1 of 1
# Warmup Iteration   1: 18926088.938 ops/s
# Warmup Iteration   2: 19504293.182 ops/s
# Warmup Iteration   3: 19261074.598 ops/s
Iteration   1: 19399456.839 ops/s
Iteration   2: 18979470.788 ops/s
Iteration   3: 18720138.076 ops/s
Iteration   4: 18458839.475 ops/s
Iteration   5: 18776539.883 ops/s


Result ""org.apache.cassandra.test.microbench.LatencyTrackingBench.benchInsertToDEHR"":
  18866889.012 ±(99.9%) 1351167.820 ops/s [Average]
{code}

i.e. twice the throughput with {{addAndGet}}. There is no material difference to my version:

{code}
    public void updateBucket(AtomicLongArray buckets, int index, long value)
    {
        index = stripedIndex(index, (int) Thread.currentThread().getId() & (nStripes - 1));
        buckets.addAndGet(index, value);
    }
{code}

It's possible it's an issue of JDK?  Perhaps yours is not replacing {{addAndGet}} with the relevant {{lock xadd}} assembly?  This seems pretty unlikely, I think this has happened for a long time, but you could try looking at the assembly that's produced.;;;","13/Jan/20 22:31;benedict;Just to reiterate, though, it's still not a great test, and it might be worthwhile improving it first.  Ideally we would have tighter control on the amount of competition we're measuring, since that's what we're interested in.  We probably also do not want to visit the same values in the same order for every invocation, as this can lead to increased synchrony in execution lowering overall throughput artificially.  We also probably _do_ want to use the same random sequence on each run, to improve reproducibility.
;;;","13/Jan/20 22:54;jwest;PEBKAC. I was rushing to run the bm between meetings and used the wrong branch. I am able to reproduce your numbers: 

{code}
     [java] Benchmark                                       Mode  Cnt         Score         Error  Units
     [java] LatencyTrackingBench.benchInsertToDEHR         thrpt    5  39263498.481 ± 1809260.330  ops/s
     [java] LatencyTrackingBench.benchLatencyMetricsWrite  thrpt    5  10473724.356 ±  137670.100  ops/s

{code}

EDIT: I agree re: an improved benchmark but its encouraging we are seeing improved performance and memory usage with the test we have and this branch. ;;;","13/Jan/20 22:56;benedict;> but its encouraging we are seeing improved performance and memory usage with the test we have and this branch

(y)

Yep, it's looking really promising.;;;","13/Jan/20 23:03;jwest;(y) I'll add the configurable stripe count and look more into bucket distribution as well as better benchmarking;;;","15/Jan/20 00:11;benedict;fwiw, wrt bucket distribution, we should be able to do something as simple as multiplying by the smallest prime larger than two cache-lines (when multiplied by 8), that doesn't also divide the number of buckets. i.e., 17 (with verification it doesn't divide a custom number of buckets, and choosing another prime if it does)

i.e. 
{code}
stripedIndex(index, stripe) = ((index * prime) % (bucketOffsets.length + 1)) + ((bucketOffsets.length + 1) * stripe)
or
stripedIndex(index, stripe) = (((index * nStripes + stripe) * prime) % buckets.length())
{code}
;;;","15/Jan/20 14:51;jwest;Thanks. I'll start exploring that approach. I implemented a version using {{Integer.reverse}} (which distributed well) but didn't find an approach using it that didn't involve an extra read/load (was looking for something more along the lines of a simple calculation like this). Will report back with my testing results / findings. 

;;;","15/Jan/20 15:12;benedict;Also fwiw, it looks like the primes 17 and 19 are sufficient, so we can literally just try either of those. Proof:

{code}
        int[] primes = new int[] { 17, 19 };
        BitSet sizeWithoutConflict = new BitSet();
        for (int prime : primes)
        {
            for (int size = 1 ; size < 238 ; ++size)
            {
                BitSet conflict = new BitSet();
                boolean hasConflict = false;
                for (int i = 0 ; i < size ; ++i)
                {
                    if (conflict.get((i * prime) % size))
                        hasConflict = true;
                    conflict.set((i * prime) % size);
                }
                if (!hasConflict)
                    sizeWithoutConflict.set(size);
            }
        }
        for (int size = 1 ; size < 238 ; ++size)
        {
            if (!sizeWithoutConflict.get(size))
                System.out.println(size);
        }
{code};;;","15/Jan/20 15:18;jwest;Sounds good. Thanks for the test / proof. ;;;","15/Jan/20 15:19;benedict;Except if we use the cleaner {{stripedIndex}} calculation, we might need to go up to e.g. 8x stripes, in which case we'd need to throw {{23}} into the mix.  That seems to take us up to 16x, with {{29}} taking us all the way to 64, which is way over provisioning stripes.;;;","30/Jan/20 06:42;jwest;Pushed an update to the branch that includes prime distribution;;;","30/Jan/20 11:45;benedict;Awesome.  I think we're ready.  I've pushed one last set of minor suggestions, that _should_ permit C2 to produce branchless code for the calculation, at the expense only of values 0 through 8 being slower to compute (since these are likely extremely uncommon, this is probably preferable).  I think it would still be possible to reduce the total work by a few instructions with some time to think, but probably not worth it.

Entirely up to you if you prefer to use this suggestion, as it's not particularly important, and it's very hard to objectively determine the effect (since it will depend on branch predictor pollution).  Once you've decided, I'll get this committed.;;;","30/Jan/20 15:40;jwest;Incorporated your change, rebased/squashed, and pushed. Thanks [~benedict].

[branch | https://github.com/jrwest/cassandra/tree/jwest/15213] [tests | https://circleci.com/gh/jrwest/cassandra/tree/jwest%2F15213];;;","06/Feb/20 21:55;jwest;Was asked to run tlp-stress against this branch. After several runs I think at the cluster level its pretty much a wash between this branch and trunk (except the memory savings of this branch ofc). Added the output from one of the runs if folks want to look closer. Its worth noting: this was pretty small / limited hardware.

Workload:
{code}
./bin/tlp-stress run KeyValue --populate 500k --partitions 1m --threads 4 \
--replication ""{'class': 'NetworkTopologyStrategy', 'DC1': 3 }"" \
--duration 1h --username $CUSER --password $CPASS \
--host $HOST
{code};;;","18/Feb/20 13:46;benedict;[~jrwest] we discussed (somewhere) maybe reducing the default value to 2, at least initially?  I don't mind using 4, but I think doubling the memory we use here is probably a good first stab at balancing improvement for larger users (who are ordinarily better able to reconfigure) against the cost for users with smaller nodes?

Also, patch message formatting norms sign off:

patch by A, B; reviewed by C, D for CASSANDRA-XXXXX

This is particularly helpful if using awk for analysis, since this is the only standard we've ever followed almost consistently, so it's helpful to retain it.

Otherwise LGTM;;;","18/Feb/20 17:59;jwest;Updated the default and the commit message. [branch|https://github.com/jrwest/cassandra/commits/jwest/15213] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Toughen up column drop/recreate validations in 3.0/3.11,CASSANDRA-15204,13243922,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,09/Jul/19 16:18,10/Jul/19 14:34,13/Jul/23 08:38,10/Jul/19 14:34,3.0.19,3.11.5,,,,Cluster/Schema,,,,0,,,,"After CASSANDRA-8099 it’s no longer possible to safely drop/add columns with incompatible types. In 4.0 we validate this correctly, but in 3.0 we don’t,
and that can result in unreadable sstables (corrupted serialization headers causing simple columns to be read as complex or vice versa).

This patch brings 3.0 in line with 4.0 restrictions, making such corruption impossible.",,aleksey,jasonstack,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Normal,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Jul 10 14:34:49 UTC 2019,,,,,,,All,,,,,"0|z04i0w:",9223372036854775807,,,,,,,samt,,,,Normal,,3.0.0,,,"[855fc91596b10a89a504b3491952d06ffe93618d|https://github.com/apache/cassandra/commit/855fc91596b10a89a504b3491952d06ffe93618d]",,,,,,,,,New unit tests,,,,,"09/Jul/19 16:21;aleksey;Branch [here|https://github.com/iamaleksey/cassandra/commits/15204-3.0], [CI|https://circleci.com/workflow-run/f1cb4f1a-9cf6-4b04-bcfe-e396189fe78a].;;;","09/Jul/19 16:27;aleksey;The following simple test is sufficient to demonstrate corruption without this validation in place:

{code}
    @Test
    public void dropRecreateIncompatibleColumnTest() throws Throwable
    {
        try (Cluster cluster = init(Cluster.create(1)))
        {
            cluster.schemaChange(format(""CREATE TABLE %s.tbl (pk int PRIMARY KEY, foo int, bar set<text>);"", KEYSPACE));

            cluster.coordinator(1).execute(format(""INSERT INTO %s.tbl (pk, foo, bar) VALUES (1, 1, {'a'});"", KEYSPACE), ONE);
            cluster.coordinator(1).execute(format(""INSERT INTO %s.tbl (pk, foo, bar) VALUES (2, 2, {'b'});"", KEYSPACE), ONE);
            cluster.get(1).runOnInstance(() -> FBUtilities.waitOnFutures(Keyspace.open(KEYSPACE).flush()));

            // drop and recreate 'bar', now as a simple 'text' column and not a set
            cluster.schemaChange(format(""ALTER TABLE %s.tbl DROP bar;"", KEYSPACE));
            cluster.schemaChange(format(""ALTER TABLE %s.tbl ADD  bar text;"", KEYSPACE));

            cluster.coordinator(1).execute(format(""INSERT INTO %s.tbl (pk, foo, bar) VALUES (3, 3, 'c');"", KEYSPACE), ONE);
            cluster.coordinator(1).execute(format(""INSERT INTO %s.tbl (pk, foo, bar) VALUES (4, 4, 'd');"", KEYSPACE), ONE);
            cluster.get(1).runOnInstance(() -> FBUtilities.waitOnFutures(Keyspace.open(KEYSPACE).flush()));

            // compact the two sstables together
            cluster.get(1).runOnInstance(() -> ColumnFamilyStore.getIfExists(KEYSPACE, ""tbl"").forceMajorCompaction());

            // restart
            cluster.get(1).shutdown(true).get();
            cluster.get(1).startup();

            assertRows(cluster.coordinator(1).execute(format(""SELECT pk, foo, bar FROM %s.tbl"", KEYSPACE), ONE),
                       new Object[][] {{ 1, 1, null }, { 2, 2, null },
                                       { 4, 4, ""d""  }, { 3, 3, ""c""  }});
        }
    }
{code}

Resulting in:

{code}
WARN  [SharedPool-Worker-1] node1 2019-07-09 17:25:11,907 AbstractLocalAwareExecutorService.java:169 - Uncaught exception on thread Thread[SharedPool-Worker-1,5,node1]
java.lang.RuntimeException: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /private/var/folders/jr/z8lrbjsn3kz27v4wt8ntnfqh0000gn/T/dtests5570532941954556030/node1/data/distributed_test_keyspace/tbl-1d880c50a26611e9b4f2a7fdb3b01932/md-3-big-Data.db
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2470) ~[main/:na]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_212]
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:164) ~[main/:na]
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [main/:na]
	at java.lang.Thread.run(Thread.java:748) [na:1.8.0_212]
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: Corrupted: /private/var/folders/jr/z8lrbjsn3kz27v4wt8ntnfqh0000gn/T/dtests5570532941954556030/node1/data/distributed_test_keyspace/tbl-1d880c50a26611e9b4f2a7fdb3b01932/md-3-big-Data.db
	at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$Reader.hasNext(AbstractSSTableIterator.java:349) ~[main/:na]
	at org.apache.cassandra.db.filter.ClusteringIndexNamesFilter$1.hasNext(ClusteringIndexNamesFilter.java:157) ~[main/:na]
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:95) ~[main/:na]
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129) ~[main/:na]
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:95) ~[main/:na]
	at org.apache.cassandra.db.rows.LazilyInitializedUnfilteredRowIterator.computeNext(LazilyInitializedUnfilteredRowIterator.java:32) ~[main/:na]
	at org.apache.cassandra.utils.AbstractIterator.hasNext(AbstractIterator.java:47) ~[main/:na]
	at org.apache.cassandra.db.transform.BaseRows.hasNext(BaseRows.java:129) ~[main/:na]
	at org.apache.cassandra.db.transform.UnfilteredRows.isEmpty(UnfilteredRows.java:71) ~[main/:na]
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:75) ~[main/:na]
	at org.apache.cassandra.db.partitions.PurgeFunction.applyToPartition(PurgeFunction.java:26) ~[main/:na]
	at org.apache.cassandra.db.transform.BasePartitions.hasNext(BasePartitions.java:96) ~[main/:na]
	at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators$Serializer.serialize(UnfilteredPartitionIterators.java:289) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.build(ReadResponse.java:187) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:180) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse$LocalDataResponse.<init>(ReadResponse.java:176) ~[main/:na]
	at org.apache.cassandra.db.ReadResponse.createDataResponse(ReadResponse.java:76) ~[main/:na]
	at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:341) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1785) ~[main/:na]
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:2466) ~[main/:na]
	... 4 common frames omitted
Caused by: java.io.IOException: Invalid Columns subset bytes; too many bits set:1
	at org.apache.cassandra.db.Columns$Serializer.deserializeSubset(Columns.java:530) ~[main/:na]
	at org.apache.cassandra.db.rows.UnfilteredSerializer.deserializeRowBody(UnfilteredSerializer.java:484) ~[main/:na]
	at org.apache.cassandra.db.UnfilteredDeserializer$CurrentDeserializer.readNext(UnfilteredDeserializer.java:209) ~[main/:na]
	at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.computeNext(SSTableIterator.java:143) ~[main/:na]
	at org.apache.cassandra.db.columniterator.SSTableIterator$ForwardReader.hasNextInternal(SSTableIterator.java:172) ~[main/:na]
	at org.apache.cassandra.db.columniterator.AbstractSSTableIterator$Reader.hasNext(AbstractSSTableIterator.java:336) ~[main/:na]
	... 25 common frames omitted
{code};;;","10/Jul/19 09:50;samt;Is this the right time to backport CASSANDRA-14913? (you mentioned on that ticket that you planned to do it at some point). 

Nit: naming of {{AlterTest::testDropFrozenAddMultiCellFrozenColumn}} ;;;","10/Jul/19 11:33;aleksey;bq. Is this the right time to backport CASSANDRA-14913? (you mentioned on that ticket that you planned to do it at some point).

Perhaps, as follow up. Will address the nit on commit, thanks.;;;","10/Jul/19 11:37;samt;Alright, LGTM then.;;;","10/Jul/19 14:34;aleksey;Cheers, committed as [855fc91596b10a89a504b3491952d06ffe93618d|https://github.com/apache/cassandra/commit/855fc91596b10a89a504b3491952d06ffe93618d] to 3.0 and merged into 3.11.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AlterTableStatement dropped type validation order,CASSANDRA-15203,13243917,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,09/Jul/19 16:04,15/May/20 07:59,13/Jul/23 08:38,10/Jul/19 13:53,4.0,4.0-alpha1,,,,Cluster/Schema,,,,0,,,,"4.0 has a minor bug in AlterTableStatement, in which we compare value compatibility of dropped type with new type instead of the other way around (and order is significant here).
This results in more conversions identified as valid than should be allowed to.

The fix is a trivial one-liner.

Relatedly, we should audit all implementations of {{isValueCompatible()}} out there - at least one, {{BytesType}} - is no longer valid in 3.0+, since {{BytesType}} can no longer correctly read any complex column. And perhaps go even further, and restrict column recreation to only previously dropped type *precisely*, for which I have a couple arguments as well.

That said, I'd like to defer those to a different ticket.",,aleksey,benedict,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Jul 10 13:53:08 UTC 2019,,,,,,,All,,,,,"0|z04hzs:",9223372036854775807,,,,,,,samt,,,,Low,,4.0,,,"[08b2192da0eb6deddcd8f79cd180d069442223ae|https://github.com/apache/cassandra/commit/08b2192da0eb6deddcd8f79cd180d069442223ae]",,,,,,,,,N/A,,,,,"09/Jul/19 16:07;aleksey;Branch [here|https://github.com/iamaleksey/cassandra/commits/15203-4.0], [CI|https://circleci.com/workflow-run/ace8a7c2-7993-406c-9b3e-0b5d53e2323c].;;;","10/Jul/19 09:15;samt;+1. There's a dtest failure that I don't recall seeing before, but it's not caused by this patch;;;","10/Jul/19 09:24;benedict;Relatedly, CASSANDRA-14846;;;","10/Jul/19 13:53;aleksey;Cheers, committed as [08b2192da0eb6deddcd8f79cd180d069442223ae|https://github.com/apache/cassandra/commit/08b2192da0eb6deddcd8f79cd180d069442223ae];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LegacyLayout does not handle paging states that cross a collection column,CASSANDRA-15201,13243199,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,04/Jul/19 11:56,10/Jul/19 14:01,13/Jul/23 08:38,10/Jul/19 14:01,3.0.19,,,,,Messaging/Internode,,,,0,,,,"{{LegacyLayout.decodeBound}} assumes there is only a single extra component, referring to a column name.  In fact an encoded page boundary may include a collection column, and this occurs as a matter of course when paging a table whose last column is a collection.",,aleksey,benedict,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Jul 10 14:01:19 UTC 2019,,,,,,,All,,,,,"0|z04dko:",9223372036854775807,,,,,,,samt,,,,Normal,,3.0.0,,,"[d50ec52d6e6c9140080c86fcff08b3cd37a6ca3d|http://github.com/apache/cassandra/commit/d50ec52d6e6c9140080c86fcff08b3cd37a6ca3d]",,,,,,,,,Two unit tests included,,,,,"04/Jul/19 13:15;benedict;Patch available [here|https://github.com/belliottsmith/cassandra/tree/15201-3.0]

This includes an upgrade dtest (as well as two unit tests), unfortunately the upgrade dtest doesn't currently work for a few reasons - the new feature API hasn't been back ported, and the upgrade API also needs some work generally (in particular 2.2 seems to erroneously attempt to load the local cassandra.yaml, presumably because there's an early reference to {{DatabaseDescriptor}})

I'll file a follow-up ticket to fix these things to get the upgrade dtest working.;;;","04/Jul/19 15:03;benedict;On second thoughts, the dtest is of dubious value without a proper client connection to the old node, which we don't currently have very nice tooling for.  Perhaps we can follow-up, but I've removed the dtest for now.;;;","10/Jul/19 11:34;samt;+1 
Maybe the new checks could throw {{IllegalArgumentException}} rather than {{RuntimeException}}, but obviously this doesn't make any difference functionally, so I don't object if you prefer the way it is.;;;","10/Jul/19 11:37;samt;bq. the dtest is of dubious value without a proper client connection to the old node, which we don't currently have very nice tooling for

just FTR, I did verify this with a handrolled ccm test;;;","10/Jul/19 14:01;benedict;bq. Maybe the new checks could throw IllegalArgumentException rather than RuntimeException

Thanks.  I had considered introducing a new exception type to capture the situation before, but IAE works probably better than RE at least.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Preventing RuntimeException when the username or password is empty,CASSANDRA-15198,13242943,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,gzh1992n,gzh1992n,gzh1992n,03/Jul/19 08:59,01/Aug/21 12:22,13/Jul/23 08:38,08/Jul/19 22:33,3.0.19,3.11.7,4.0,4.0-alpha1,,Feature/Authorization,,,,0,pull-request-available,,," !empty_username_error.jpg! 

Although this does not affect the service, it's necessary to improve code robustness.",,bdeggleston,gzh1992n,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,"ZephyrGuo commented on pull request #330: CASSANDRA-15198: Preventing RuntimeException when the username or password is empty
URL: https://github.com/apache/cassandra/pull/330
 
 
   Please see https://issues.apache.org/jira/browse/CASSANDRA-15198 for more details.
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;03/Jul/19 09:28;githubbot;600","ZephyrGuo commented on issue #330: CASSANDRA-15198: Preventing RuntimeException when the username or password is empty
URL: https://github.com/apache/cassandra/pull/330#issuecomment-509474459
 
 
   Close.
   Committed on  [177a8e91e3f0ef85e2bc3f64b0e566ace6330071](https://github.com/apache/cassandra/commit/177a8e91e3f0ef85e2bc3f64b0e566ace6330071)
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 03:39;githubbot;600","ZephyrGuo commented on pull request #330: CASSANDRA-15198: Preventing RuntimeException when the username or password is empty
URL: https://github.com/apache/cassandra/pull/330
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;09/Jul/19 03:39;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1800,,,0,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/19 09:03;gzh1992n;CASSANDRA-15198-v1.patch;https://issues.apache.org/jira/secure/attachment/12973540/CASSANDRA-15198-v1.patch","03/Jul/19 08:58;gzh1992n;empty_username_error.jpg;https://issues.apache.org/jira/secure/attachment/12973539/empty_username_error.jpg",,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,gzh1992n,,,,,,,,,,,,Security -> Remote Code Execution,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,Security,,Mon Jul 08 22:33:25 UTC 2019,,,,,,,All,,,,,"0|z04c0g:",9223372036854775807,,,,,,,bdeggleston,,,,Low,,3.0.0,,,https://github.com/apache/cassandra/commit/177a8e91e3f0ef85e2bc3f64b0e566ace6330071,,,,,,,,,unit testing new validation,,,,,"03/Jul/19 20:12;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/15198-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15198-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/15198-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15198-3.11]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/15198-trunk]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15198-trunk]|;;;","05/Jul/19 09:29;gzh1992n;Thanks [~bdeggleston] for review.;;;","08/Jul/19 22:33;bdeggleston;Committed to 3.0 as [177a8e91e3f0ef85e2bc3f64b0e566ace6330071|https://github.com/apache/cassandra/commit/177a8e91e3f0ef85e2bc3f64b0e566ace6330071] and merged up to trunk. Thanks for the patch [~gzh1992n] and nice work on the tests!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve readability of Table metrics Virtual tables units,CASSANDRA-15194,13242794,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,cnlwsu,rustyrazorblade,rustyrazorblade,02/Jul/19 16:23,07/Mar/23 11:52,13/Jul/23 08:38,12/Aug/19 18:30,4.0,4.0-alpha1,,,,Feature/Virtual Tables,,,,0,,,,"I just noticed this strange output in the coordinator_reads output::

{code}
cqlsh:system_views> select * from coordinator_reads ;

 count | keyspace_name      | table_name                     | 99th | max | median | per_second
-------+--------------------+--------------------------------+------+-----+--------+------------
  7573 |         tlp_stress |                       keyvalue |    0 |   0 |      0 | 2.2375e-16
  6076 |         tlp_stress |                  random_access |    0 |   0 |      0 | 7.4126e-12
   390 |         tlp_stress |                sensor_data_udt |    0 |   0 |      0 | 1.7721e-64
    30 |             system |                          local |    0 |   0 |      0 |   0.006406
    11 |      system_schema |                        columns |    0 |   0 |      0 | 1.1192e-16
    11 |      system_schema |                        indexes |    0 |   0 |      0 | 1.1192e-16
    11 |      system_schema |                         tables |    0 |   0 |      0 | 1.1192e-16
    11 |      system_schema |                          views |    0 |   0 |      0 | 1.1192e-16
{code}

cc [~cnlwsu]

btw I realize the output is technically correct, but it's not very readable.  For practical purposes this should just say 0.",,aleksey,benedict,cnlwsu,jeromatron,rustyrazorblade,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cnlwsu,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Aug 12 14:02:11 UTC 2019,,,,,,,All,,,,,"0|z04b3c:",9223372036854775807,,,,,,,benedict,rustyrazorblade,,,Low,,5.0,,,https://github.com/apache/cassandra/commit/9a175a1697b1107fb63480fb86ffe37b02122267,,,,,,,,,unit tests and circleci,,,,,"03/Jul/19 08:58;benedict;We should probably standardise on number of digits after the decimal place in any regard, and forbid scientific notation (though probably if we solve 0 it would never get big enough to be a problem), to enhance readability;;;","03/Jul/19 15:53;cnlwsu;My thought here was to do:

* add units top the column names (max_ms, median_ms, disk_usage_mib)
* 0.000 precision
* have a rounding threshold, with our decaying algorithms they dont really go to zero as nothing occurs within the alpha period. They just get exponentially small. So if for latencies it goes below 1us and we displaying in ms with 0.000 precision we just display zero. And report space in MiB, with 0.000 precision, but not zeroing out if below, just show as 0.001 since we dont want to misrepresent in that direction.;;;","03/Jul/19 16:31;benedict;Sounds reasonable.  Could you clarify what you mean by a rounding threshold, though?  A precision of 0.000 would seem to provide that by itself, particularly if we just always truncate instead of round (i.e. set rounding mode to always round down, which seems fine to me across the board)

I think we should probably properly capitalise MiB, though, since Mib has a different meaning.;;;","03/Jul/19 17:39;cnlwsu;My intent was to set rounding mode to UP so we never underreport so it would stick at 0.001 for 2.2375e-16. If we just round down though yeah that would be sufficient and wouldnt need a threshold.

capitalization in cql is kinda painful since its case sensitive yet auto-uncapitalizes unquoted things. Also since everything else is in lower snake case (likely because of that sensitivity) it _looks weird_. I do agree that its ambiguous though, not sure best approach... could use mb and report in 1,000,000 of bytes or just stick with reporting bytes and punting;;;","03/Jul/19 17:56;rustyrazorblade;Or write megabytes / mebibytes, there's no ambiguity there.;;;","03/Jul/19 17:59;benedict;> yet auto-uncapitalizes unquoted things

Wow, I did not know that.  How awful.  Perhaps we should separately at least fix this for select queries so that, if a capitalised column exists, it selects it.  This seems strictly superior behaviour, though I guess you could have weird schema where case determines the field.  I'm sure we could have a safe migration here, if we wanted, and it seems like a silly behaviour right now.  We could definitely safely fix this for system tables.

However, I'm not sure this is a huge problem here, since we're likely to mostly be using ""select *"" I'd guess?

The problem is the other options are all even worse AFAICT, since {{mbs}} has an ambiguous meaning, as a lowercase {{b}} can be interpreted either as bits or bytes (and is I think non-standard for either).  Though we have generally interpreted it as bytes on this project, it's not great to continue the ambiguity.;;;","03/Jul/19 18:54;cnlwsu;I actually like idea of just calling the column ""mebibytes"" since the table names are things like ""disk_usage"" and ""max_partition_size"" its obvious what its measuring and then the unit is unambiguous and no annoying capitalization issues.;;;","03/Jul/19 19:16;benedict;That works for tables that have a clear name, which might well be all of the ones introduced that we're discussing.  I'll try to find time to take a closer look at all of the tables soon.

Whilst on that topic of table naming, I think some could be improved a little, and I'd be interested to hear what you think.  In particular, the latency tables should IMO probably include {{latency}} in the name - both because it isn't trivially obvious, and to not overly complicate future naming decisions that cover the same category of things.  {{live_scanned}} also probably has a fairly unclear meaning to anybody not really well versed in C*.

What do you think?;;;","03/Jul/19 19:20;cnlwsu;table naming changes sound good, I'll include them in patch. For now i think we can stick with ""mebibytes"" and if theres ever a table thats not obvious what it means that one can have special naming for it. 

thinking {{live_rows_scanned}}, {{rows_scanned_per_query}}?;;;","03/Jul/19 21:36;rustyrazorblade;rows_scanned_per_query is more obvious to beginners, I prefer that.;;;","02/Aug/19 19:46;rustyrazorblade;Just took a look at the patch and noticed a few things.

I ran a simple test using tlp-stress, a basic key value run:
{noformat}
bin/tlp-stress run KeyValue -d 1h --rate 1000 -r .5
{noformat}
When I checked the virtual tables I'm seeing a p99 of 73 seconds:
{noformat}
cqlsh:system_views> select * from local_read_latency  where keyspace_name = 'tlp_stress' allow filtering;

 keyspace_name | table_name  | 99th_ms | count | max_ms     | median_ms | per_second
---------------+-------------+---------+-------+------------+-----------+------------
    tlp_stress |    keyvalue |   73457 | 86523 | 1.3581e+06 |     11864 |    205.219
    tlp_stress | sensor_data |       0 |     0 |          0 |         0 |          0
{noformat}
However, I don't see anything close to that in tlp-stress:
{noformat}
                  Writes                                    Reads                  Errors
  Count  Latency (p99)  1min (req/s) |   Count  Latency (p99)  1min (req/s) |   Count  1min (errors/s)
  76557           0.38        501.04 |   76620           0.36        501.75 |       0                0
  78058           0.38        500.41 |   78119           0.36        502.15 |       0                0
  79503           0.37        500.41 |   79671           0.36        502.15 |       0                0
  80999           0.37        499.96 |   81174           0.37         502.4 |       0                0
  82553           0.37         500.4 |   82623           0.37        501.77 |       0                0
  84085           0.36         500.4 |   84092           0.38        501.77 |       0                0
  85544           0.37        500.49 |   85633           0.38         501.5 |       0                0
{noformat}
Nodetool tablehistograms agrees with tlp_stress that my laptop isn't perfoming that poorly:
{noformat}
$ nodetool tablehistograms tlp_stress keyvalue
tlp_stress/keyvalue histograms
Percentile      Read Latency     Write Latency          SSTables    Partition Size        Cell Count
                    (micros)          (micros)                             (bytes)
50%                    11.86              8.24              0.00               215                 1
75%                    17.08              9.89              0.00               215                 1
95%                    42.51             29.52              0.00               258                 1
98%                    61.21             35.43              0.00               258                 1
99%                    73.46             35.43              0.00               258                 1
Min                     3.97              2.76              0.00               125                 0
Max                   152.32             73.46              1.00               258                 1
{noformat}
The same issue pops up for other tables as well:
{noformat}
cqlsh:system_views> select * from local_scan_latency where keyspace_name = 'tlp_stress' allow filtering;

 keyspace_name | table_name  | 99th_ms    | count | max_ms     | median_ms  | per_second
---------------+-------------+------------+-------+------------+------------+------------
    tlp_stress |    keyvalue | 1.1318e+06 |    16 | 1.1318e+06 | 5.4579e+05 |       0.05
    tlp_stress | sensor_data |          0 |     0 |          0 |          0 |          0
{noformat}
I think starting at line 205 of TableMetricTables.java you'd want this:
{noformat}
add(result, MEDIAN + suffix, snapshot.getMedian() / NS_TO_MS);
add(result, P99 + suffix, snapshot.get99thPercentile() / NS_TO_MS);
add(result, MAX + suffix, (double) snapshot.getMax() / NS_TO_MS);
{noformat}
When I apply that change and rerun things, I get output that makes a lot more sense:
{noformat}
cqlsh:system_views> select * from local_read_latency  where keyspace_name = 'tlp_stress' allow filtering;

 keyspace_name | table_name  | 99th_ms | count | max_ms | median_ms | per_second
---------------+-------------+---------+-------+--------+-----------+------------
    tlp_stress |    keyvalue |   0.786 | 17660 |  7.008 |      0.03 |     53.331
    tlp_stress | sensor_data |       0 |     0 |      0 |         0 |          0
{noformat}
Other than that, I don't have any issues. We can merge once that's addressed.

We should create a follow up JIRA to document all the virtual tables.;;;","03/Aug/19 08:23;benedict;I'll try to take a quick look at this early next week too.  One little thing jumps out at me, based on Jon's comment, and that's that we should avoiding dividing by a constant for our floating point arithmetic if possible - it's better to save the reciprocal as a constant, and to multiply by it.  I'm unsure if the compiler would be willing to apply this optimisation for us, since this might lead to a slightly different answer.  

It looks like we also do this for the conversion to MiB because we convert to a double before dividing by the long constant, although this might be compiler-optimisable given it's a power-of-2.

Neither of these things are super important, of course.;;;","04/Aug/19 20:51;cnlwsu;Changed to multiplication to the reciprocal and fixed the missing ns to ms conversion;;;","05/Aug/19 19:07;rustyrazorblade;Patch looks great, thanks for those fixes.  +1 from me.;;;","07/Aug/19 14:30;benedict;Thanks, the patch looks good.

I've pushed some minor suggestions [here|https://github.com/belliottsmith/cassandra/tree/15194-suggest], solely to introduce static compilation checks of the types of the metric we're supplying with some fancy generics.

Some other suggestions for discussion, that I haven't implemented, but are easy to do so:

In {{LatencyTableMetric.add}} it is probably sufficient to test {{column.endsWith(suffix)}}, given we statically define all of the regular columns?  If we wanted to we could impose a runtime check when building the metadata that only the expected columns end with this suffix, but this is probably unnecessary.

While we're here, should we consider renaming median to 50th, so it sorts correctly wrt 99th?  For consistency I'd love to see 100th, but this would mess with order.  It might be clearer to name them p50, p99, though, so we can also introduce p999 and maintain sort order.;;;","09/Aug/19 23:38;cnlwsu;changed per feedback and squashed;;;","12/Aug/19 14:02;benedict;bq. While we're here, should we consider renaming median to 50th, so it sorts correctly wrt 99th? For consistency I'd love to see 100th, but this would mess with order. It might be clearer to name them p50, p99, though, so we can also introduce p999 and maintain sort order.

I realise this was really unclear, but I ended up suggesting p50, p99 etc as names, so that if we introduce p999 it makes sense (though I guess we could always call it 99.9th, and this should sort correctly still)

Not essential, just making sure my lack of clarity wasn't obscuring the discussion.

LGTM, +1 either way;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add ability to cap max negotiable protocol version,CASSANDRA-15193,13242706,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,02/Jul/19 09:59,15/May/20 08:54,13/Jul/23 08:38,08/Oct/19 19:31,3.0.19,3.11.5,4.0,4.0-alpha2,,Messaging/Client,,,,0,,,,"3.0 and native protocol V4 introduced a change to how PagingState is serialized. Unfortunately that can break requests during upgrades: since paging states are opaque, it's possible for a client to receive a paging state encoded as V3 on a 2.1 node, and then send it to a 3.0 node on a V4 session. The version of the current session will be used to deserialize the paging state, instead of the actual version used to serialize it, and the request will fail.

CASSANDRA-15176 solves half of this problem by enabling 3.0 nodes to serialize mis-versioned PagingStates. To address the other side of the issue, 2.1 nodes receiving V4 PagingStates, we can introduce a property to cap the max native protocol version that the 3.0 nodes will negotiate with clients. If we cap this to V3 during upgrades, no V4 connections will be established and so no incompatible PagingStates will be sent to clients.",,aleksey,ifesdjeen,jeromatron,mshuler,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Availability -> Response Crash,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Oct 08 19:31:31 UTC 2019,,,,,,,All,,,,,"0|z04ajs:",9223372036854775807,,,,,,,aleksey,ifesdjeen,,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/0388d89e29393d0b1f50baa24848bc8cb0a7c9a3,,,,,,,,,Expanded unit test coverage,,,,,"09/Jul/19 16:56;samt;Added a yaml setting to force the max negotiable protocol version to a specific value. If this setting isn't used, then the maximum version will be determined based on the C* version of the peers in the {{system.peers}} table. At startup, if any peer is believed to be running a version lower than {{3.0.0}} ({{2.2}} suffers the same problems as {{2.1}}), then the maximum protocol version that may be negotiated will be capped at {{V3}}. As updated peer info is received via gossip, the conditions are reevaluated and ultimately the cap will be removed when all known peers are reporting version {{3.0.0}} or above. It isn't safe to allow the cap to move in the other direction (i.e. to lower the max negotiable version) while there may be clients already connected, so this is essentially a one-way valve. There's also a system property that can be used to disable this automatic limiting: {{cassandra.disable_max_protocol_auto_override}}

 
||branch||CI||
|[15193-3.0|https://github.com/beobal/cassandra/tree/15193-3.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F15193-3.0]|
|[15193-3.11|https://github.com/beobal/cassandra/tree/15193-3.11]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F15193-3.11]|

This required a small tweak to one dtest (the circle workflows are using this branch), see [here|https://github.com/apache/cassandra-dtest/compare/master...beobal:15193];;;","13/Aug/19 11:14;ifesdjeen;The patch looks good to me, +1. 

One improvement we can do in {{Frame}}, when throwing a {{ProtocolException}}, is to use {{versionCap}} instead of {{CURRENT_VERSION}} in the error message.

I just have a couple of minor comments:

  * should we add a protocol negotiation test for cqlsh? It's a minor behaviour change, and we might want to ensure we preserve it.
  * should we stick to ""max native protocol version"" or to ""max negotiable protocol version""? I think both have similar semantics / meaning, but it might be not obvious from the first glance.
  * {{maybeUpdateVersion}} can be private
  * in {{maybeUpdateVersion}}, we can avoid doubly-nested {{if}} by checking for {{!enforceV3Cap}} and returning if it happens. I wouldn't say it makes a huge change. Please feel free to ignore.
  * in {{Server.java}}, we can use {{ProtocolVersionLimit}} interface instead of {{ConfiguredLimit}} class, and make {{ConfiguredLimit}} package-private.
  * in {{ProtocolNegotiationTest}}, we can avoid calling {{setStaticLimitInConfig}} in {{finally}}, because it's already called in {{Before}}.;;;","07/Oct/19 13:11;aleksey;The changes look good to me. I figure we don't really need to have {{SystemKeyspace::loadPeerVersions}} be a map, and it could just be a set, but it doesn't really matter or affect anything.

+1;;;","08/Oct/19 19:31;samt;Rebased and incorporated (most of) the review comments. There are no new test failures, so committed to 3.0 in {{0388d89e29393d0b1f50baa24848bc8cb0a7c9a3}} and merged to 3.11 and trunk. This isn't an issue in trunk so that only includes the new yaml setting, marked as deprecated and having no effect, to make upgrades from 2.1->3.x->4.0 less painful.
 
||branch||CI||
|[15193-3.0|https://github.com/beobal/cassandra/tree/15193-3.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/15193-3.0]|
|[15193-3.11|https://github.com/beobal/cassandra/tree/15193-3.11]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/15193-3.11]|
|[15193-trunk|https://github.com/beobal/cassandra/tree/15139-trunk]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/15139-trunk]|

dtests ran using the branch in this PR: https://github.com/apache/cassandra-dtest/pull/54
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stop_paranoid disk failure policy is ignored on CorruptSSTableException after node is up,CASSANDRA-15191,13242414,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefan.miklosovic,VincentWhite,VincentWhite,01/Jul/19 04:45,09/Mar/23 16:09,13/Jul/23 08:38,30/Jul/20 00:31,3.0.22,3.11.8,4.0,4.0-beta2,,Local/Config,,,,0,,,,"There is a bug when disk_failure_policy is set to stop_paranoid and CorruptSSTableException is thrown after server is up. The problem is that this setting is ignored. Normally, it should stop gossip and transport but it just continues to serve requests and an exception is just logged.

 

This patch unifies the exception handling in JVMStabilityInspector and code is reworked in such way that this inspector acts as a central place where such exceptions are inspected. 

 

The core reason for ignoring that exception is that thrown exception in AbstractLocalAwareExecturorService is not CorruptSSTableException but it is RuntimeException and that exception is as its cause. Hence it is better if we handle this in JVMStabilityInspector which can recursively examine it, hence act accordingly.

Behaviour before:

stop_paranoid of disk_failure_policy is ignored when CorruptSSTableException is thrown, e.g. on a regular select statement

Behaviour after:

Gossip and transport (cql) is turned off, JVM is still up for further investigation e.g. by jmx.",,brandon.williams,dcapwell,jeromatron,rtib,stefan.miklosovic,VincentWhite,,,,,,,,,,,,,,,,,,,,,,"smiklosovic opened a new pull request #681:
URL: https://github.com/apache/cassandra/pull/681


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Jul/20 15:19;githubbot;600","smiklosovic opened a new pull request #684:
URL: https://github.com/apache/cassandra/pull/684


   for trunk


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Jul/20 07:57;githubbot;600","dcapwell commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456728756



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       These tests don't replicate the bad logic reported in JIRA.  I took these tests and slightly modified them to not throw but update the thread error handler and both passed.  
   
   Can we update the tests to replicate the behavior reported?  Since the tests call `JVMStabilityInspector` directly they don't actually match the change made to AbstractLocalAwareExecutorService

##########
File path: src/java/org/apache/cassandra/service/CassandraDaemon.java
##########
@@ -463,14 +463,12 @@ public static void uncaughtException(Thread t, Throwable e)
             {
                 if (e2 != e) // make sure FSError gets logged exactly once.
                     logger.error(""Exception in thread "" + t, e2);
-                FileUtils.handleFSError((FSError) e2);

Review comment:
       since this is now handled by `JVMStabilityInspector.inspectThrowable` the logging should also be handled there.  Also, inspect is recursive so we don't need to walk the cause change anymore




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 00:57;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456764391



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       @dcapwell sorry I am not getting what you are trying to say.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 08:10;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456764628



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       @dcapwell If I have this method
   
   ```
       private static final class CorruptSSTableCallable implements SerializableCallable<Boolean[]>
       {
           public Boolean[] call()
           {
               final CassandraDaemon cassandraDaemon = CassandraDaemon.getInstanceForTesting();
               cassandraDaemon.completeSetup();
               StorageService.instance.registerDaemon(cassandraDaemon);
               FileUtils.setFSErrorHandler(new DefaultFSErrorHandler());
   
               JVMStabilityInspector.inspectThrowable(new CorruptSSTableException(new RuntimeException(""""), ""sstablepath""));
   
               return new Boolean[]{
               StorageService.instance.isNativeTransportRunning(),
               Gossiper.instance.isEnabled(),
               };
           }
       }
   ```
   And I comment out `JVMStabilityInspector.inspectThrowable....`
   
   my tests fails, which is right.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 08:13;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456764628



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       @dcapwell If I have this method
   
   ```
       private static final class CorruptSSTableCallable implements SerializableCallable<Boolean[]>
       {
           public Boolean[] call()
           {
               final CassandraDaemon cassandraDaemon = CassandraDaemon.getInstanceForTesting();
               cassandraDaemon.completeSetup();
               StorageService.instance.registerDaemon(cassandraDaemon);
               FileUtils.setFSErrorHandler(new DefaultFSErrorHandler());
   
               JVMStabilityInspector.inspectThrowable(new CorruptSSTableException(new RuntimeException(""""), ""sstablepath""));
   
               return new Boolean[]{
               StorageService.instance.isNativeTransportRunning(),
               Gossiper.instance.isEnabled(),
               };
           }
       }
   ```
   And I comment out `JVMStabilityInspector.inspectThrowable....`
   
   my tests fail, which is right.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 08:13;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456764391



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       @dcapwell sorry I am not getting what you are trying to say, especially the first paragraph. How can I replicate the ""bad behavior"" when I just fixed it?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 08:17;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456764391



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       @dcapwell sorry I am not getting what you are trying to say, especially the first paragraph. How can I replicate the ""bad behavior"" when I just fixed it?
   
   I am not sure what is wrong with your tests but when I comment out `JVMStabilityInspector.inspectThrowable` in that callable and I run the test, it fails on assertions in test method.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 08:20;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456769277



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       @dcapwell I have added the second commit where I am testing that executor service so I hope it helps.
   
   I am little bit lost on your first comment as I do not know how to proceed there. The looping over these exceptions there is done only for CorruptSSTableException and FSError exception but not for the rest, should I move this behavior to inspect method or we should take more general approach? If I am about to make this happen for CSE and FSE, to exactly copy the existing behaviour, I would have to propagate Thread into that inspect method too which would change method signature etc ... 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 09:08;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456769277



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       @dcapwell I have added the second commit where I am testing that executor service so I hope it helps.
   
   I am little bit lost on your first comment as I do not know how to proceed there. The looping and the logging over these exceptions there is done only for CorruptSSTableException and FSError exception but not for the rest, should I move this behavior to inspect method or we should take more general approach? If I am about to make this happen for CSE and FSE, to exactly copy the existing behaviour, I would have to propagate Thread into that inspect method too which would change method signature etc ... 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 09:09;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456769277



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       @dcapwell I have added the second commit where I am testing that executor service so I hope it helps.
   
   I am little bit lost on your first comment as I do not know how to proceed there. The looping and the logging over these exceptions there is done only for CorruptSSTableException and FSError exception but not for the rest, should I move this behavior to inspect method or we should take more general approach? If I am about to make this happen for CSE and FSE, to exactly copy the existing behaviour, I would have to propagate Thread into that inspect method too which would change method signature etc ... 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 09:50;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r456779967



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,170 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.Future;
+
+import org.junit.Test;
+
+import org.apache.cassandra.concurrent.LocalAwareExecutorService;
+import org.apache.cassandra.concurrent.SharedExecutorPool;
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl
+{
+    @Test
+    public void testAbstractLocalAwareExecutorService()
+    {
+        try (final Cluster cluster = init(getCluster(DiskFailurePolicy.ignore).start()))
+        {
+            final Boolean[] results = cluster.get(1).callsOnInstance(new CorruptSSTableInExecutorCallable()).call();
+
+            assertTrue(results[0]);
+            assertTrue(results[1]);
+            assertTrue(results[2]);
+            assertTrue(results[3]);
+            assertTrue(results[4]);
+        }
+        catch (final Exception ex)
+        {
+            fail(""Threw exception:"" + ex);
+        }
+    }
+
+    @Test
+    public void corruptSSTAbleOnIgnoreDiskFailurePolicyTest()
+    {
+        try (final Cluster cluster = init(getCluster(DiskFailurePolicy.ignore).start()))
+        {
+            final Boolean[] enabledServices = cluster.get(1).callsOnInstance(new CorruptSSTableCallable()).call();
+
+            // here we expect that Gossip and transport is NOT stopped as disk failure policy is ignore
+            assertTrue(enabledServices[0]);
+            assertTrue(enabledServices[1]);
+        }
+        catch (final Exception ex)
+        {
+            fail(""Threw exception:"" + ex);
+        }
+    }
+
+    @Test
+    public void corruptSSTableOnStopParanoidDiskFailurePolicyTest()
+    {
+        try (final Cluster cluster = init(getCluster(DiskFailurePolicy.stop_paranoid).start()))
+        {
+            final Boolean[] enabledServices = cluster.get(1).callsOnInstance(new CorruptSSTableCallable()).call();
+
+            // here we expect that Gossip and transport ARE stopped as disk failure policy is stop_paranoid
+            assertFalse(enabledServices[0]);
+            assertFalse(enabledServices[1]);
+        }
+        catch (final Exception ex)
+        {
+            fail(""Threw exception: "" + ex);
+        }
+    }
+
+    private static final class CorruptSSTableCallable implements SerializableCallable<Boolean[]>
+    {
+        public Boolean[] call()
+        {
+            final CassandraDaemon cassandraDaemon = CassandraDaemon.getInstanceForTesting();
+            cassandraDaemon.completeSetup();
+            StorageService.instance.registerDaemon(cassandraDaemon);
+            FileUtils.setFSErrorHandler(new DefaultFSErrorHandler());
+
+            JVMStabilityInspector.inspectThrowable(new CorruptSSTableException(new RuntimeException(""""), ""sstablepath""));
+
+            return new Boolean[]{
+            StorageService.instance.isNativeTransportRunning(),
+            Gossiper.instance.isEnabled(),
+            };
+        }
+    }
+
+    private static final class CorruptSSTableInExecutorCallable implements SerializableCallable<Boolean[]>

Review comment:
       @dcapwell newly added, the first test uses that.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Jul/20 11:23;githubbot;600","dcapwell commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r457567557



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -58,11 +59,32 @@ private JVMStabilityInspector() {}
      */
     public static void inspectThrowable(Throwable t) throws OutOfMemoryError
     {
-        inspectThrowable(t, true);
+        inspectThrowable(t, null, true, true);
     }
 
+
     public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory) throws OutOfMemoryError
     {
+        inspectThrowable(t, null, propagateOutOfMemory, true);
+    }
+
+    public static void inspectThrowable(Throwable t, Thread thread, boolean propagateOutOfMemory)
+    {
+        inspectThrowable(t, thread, propagateOutOfMemory, true);
+    }
+
+    private static void inspectThrowable(Throwable t, Thread thread, boolean propagateOutOfMemory, boolean shouldLog) throws OutOfMemoryError
+    {
+        if (shouldLog)
+        {
+            if (thread != null)
+                logger.error(String.format(""Uncaught exception in thread %s"", thread), t);

Review comment:
       with logger you can use `logger.error(""Uncaught exception in thread {}"", thread, t);`




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jul/20 17:16;githubbot;600","dcapwell commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r457567557



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -58,11 +59,32 @@ private JVMStabilityInspector() {}
      */
     public static void inspectThrowable(Throwable t) throws OutOfMemoryError
     {
-        inspectThrowable(t, true);
+        inspectThrowable(t, null, true, true);
     }
 
+
     public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory) throws OutOfMemoryError
     {
+        inspectThrowable(t, null, propagateOutOfMemory, true);
+    }
+
+    public static void inspectThrowable(Throwable t, Thread thread, boolean propagateOutOfMemory)
+    {
+        inspectThrowable(t, thread, propagateOutOfMemory, true);
+    }
+
+    private static void inspectThrowable(Throwable t, Thread thread, boolean propagateOutOfMemory, boolean shouldLog) throws OutOfMemoryError
+    {
+        if (shouldLog)
+        {
+            if (thread != null)
+                logger.error(String.format(""Uncaught exception in thread %s"", thread), t);

Review comment:
       with logger you can use `logger.error(""Uncaught exception in thread {}"", thread, t);`.  Don't need String.format




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jul/20 17:16;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r457607363



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -58,11 +59,32 @@ private JVMStabilityInspector() {}
      */
     public static void inspectThrowable(Throwable t) throws OutOfMemoryError
     {
-        inspectThrowable(t, true);
+        inspectThrowable(t, null, true, true);
     }
 
+
     public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory) throws OutOfMemoryError
     {
+        inspectThrowable(t, null, propagateOutOfMemory, true);
+    }
+
+    public static void inspectThrowable(Throwable t, Thread thread, boolean propagateOutOfMemory)
+    {
+        inspectThrowable(t, thread, propagateOutOfMemory, true);
+    }
+
+    private static void inspectThrowable(Throwable t, Thread thread, boolean propagateOutOfMemory, boolean shouldLog) throws OutOfMemoryError
+    {
+        if (shouldLog)
+        {
+            if (thread != null)
+                logger.error(String.format(""Uncaught exception in thread %s"", thread), t);

Review comment:
       thanks, fixed.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jul/20 18:26;githubbot;600","dcapwell commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r457608540



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       sorry that I was not clear.  
   
   What I wanted to say was that these tests test the new behavior but didn't show that the old behavior was problematic.  As the JIRA calls out, the issue is that RuntimeException is used, but this is controlled by `org.apache.cassandra.service.StorageProxy.DroppableRunnable#run`, which isn't leveraged in the test.
   
   To help, here is a patch which rewrites the tests in the model used by FailingRepairTest; they cause corruption at the sstable so a read will fail
   
   ```
   /*
    * Licensed to the Apache Software Foundation (ASF) under one
    * or more contributor license agreements.  See the NOTICE file
    * distributed with this work for additional information
    * regarding copyright ownership.  The ASF licenses this file
    * to you under the Apache License, Version 2.0 (the
    * ""License""); you may not use this file except in compliance
    * with the License.  You may obtain a copy of the License at
    *
    *     http://www.apache.org/licenses/LICENSE-2.0
    *
    * Unless required by applicable law or agreed to in writing, software
    * distributed under the License is distributed on an ""AS IS"" BASIS,
    * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
    * See the License for the specific language governing permissions and
    * limitations under the License.
    */
   
   package org.apache.cassandra.distributed.test;
   
   import java.io.IOException;
   import java.util.HashSet;
   import java.util.Set;
   
   import org.junit.Test;
   
   import org.apache.cassandra.config.Config.DiskFailurePolicy;
   import org.apache.cassandra.db.ColumnFamilyStore;
   import org.apache.cassandra.db.DecoratedKey;
   import org.apache.cassandra.db.Keyspace;
   import org.apache.cassandra.db.RowIndexEntry;
   import org.apache.cassandra.db.Slices;
   import org.apache.cassandra.db.filter.ColumnFilter;
   import org.apache.cassandra.db.rows.UnfilteredRowIterator;
   import org.apache.cassandra.distributed.Cluster;
   import org.apache.cassandra.distributed.api.ConsistencyLevel;
   import org.apache.cassandra.distributed.api.IInvokableInstance;
   import org.apache.cassandra.distributed.shared.AbstractBuilder;
   import org.apache.cassandra.distributed.shared.NetworkTopology;
   import org.apache.cassandra.gms.Gossiper;
   import org.apache.cassandra.io.sstable.CorruptSSTableException;
   import org.apache.cassandra.io.sstable.format.ForwardingSSTableReader;
   import org.apache.cassandra.io.sstable.format.SSTableReader;
   import org.apache.cassandra.io.sstable.format.SSTableReadsListener;
   import org.apache.cassandra.io.util.FileDataInput;
   import org.apache.cassandra.service.CassandraDaemon;
   import org.apache.cassandra.service.StorageService;
   import org.assertj.core.api.Assertions;
   
   import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
   import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
   import static org.apache.cassandra.distributed.api.Feature.NETWORK;
   
   public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl
   {
       @Test
       public void testAbstractLocalAwareExecutorServiceOnIgnoredDiskFailurePolicy() throws IOException
       {
           test(DiskFailurePolicy.ignore, true, true);
       }
   
       @Test
       public void testAbstractLocalAwareExecutorServiceOnStopParanoidDiskFailurePolicy() throws IOException
       {
           test(DiskFailurePolicy.stop_paranoid, false, false);
       }
   
       private static void test(DiskFailurePolicy policy, boolean expectNativeTransportRunning, boolean expectGossiperEnabled) throws IOException
       {
           String table = policy.name();
           try (final Cluster cluster = init(getCluster(policy).start()))
           {
               IInvokableInstance node = cluster.get(1);
               node.runOnInstance(() -> {
                   //TODO why is this not working properly in jvm-dtest?  StorageService.instance.isNativeTransportRunning() never gets set to true even with native enabled
                   StorageService.instance.registerDaemon(CassandraDaemon.getInstanceForTesting());
               });
   
               cluster.schemaChange(""CREATE TABLE "" + KEYSPACE + ""."" + table + "" (id bigint PRIMARY KEY)"");
               node.executeInternal(""INSERT INTO "" + KEYSPACE + ""."" + table + "" (id) VALUES (?)"", 0L);
               corruptTable(node, KEYSPACE, table);
   
               // make sure environment is setup propertly
               Assertions.assertThat(node.callOnInstance(() -> new boolean[]{ StorageService.instance.isNativeTransportRunning(), Gossiper.instance.isEnabled() }))
                         .isEqualTo(new boolean[]{ true, true });
   
               // query should see corrupt sstable and should fail the query
               Assertions.assertThatThrownBy(() -> cluster.coordinator(1).execute(""SELECT * FROM "" + KEYSPACE + ""."" + table + "" WHERE id=?"", ConsistencyLevel.ONE, 0L));
   
               Assertions.assertThat(node.callOnInstance(() -> new boolean[]{ StorageService.instance.isNativeTransportRunning(), Gossiper.instance.isEnabled() }))
                         .isEqualTo(new boolean[]{ expectNativeTransportRunning, expectGossiperEnabled });
           }
       }
   
       private static void corruptTable(IInvokableInstance node, String keyspace, String table)
       {
           node.runOnInstance(() -> {
               ColumnFamilyStore cf = Keyspace.open(keyspace).getColumnFamilyStore(table);
               cf.forceBlockingFlush();
   
               Set<SSTableReader> remove = cf.getLiveSSTables();
               Set<SSTableReader> replace = new HashSet<>();
               for (SSTableReader r : remove)
                   replace.add(new CorruptedSSTableReader(r));
   
               cf.getTracker().removeUnsafe(remove);
               cf.addSSTables(replace);
           });
       }
   
       private static AbstractBuilder<IInvokableInstance, Cluster, Cluster.Builder> getCluster(DiskFailurePolicy diskFailurePolicy)
       {
           return Cluster.build()
                         .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(1, ""dc0"", ""rack0""))
                         .withConfig(config -> config.with(NETWORK, GOSSIP, NATIVE_PROTOCOL)
                                                     .set(""disk_failure_policy"", diskFailurePolicy.name()));
       }
   
       private static final class CorruptedSSTableReader extends ForwardingSSTableReader
       {
           public CorruptedSSTableReader(SSTableReader delegate)
           {
               super(delegate);
           }
   
           public UnfilteredRowIterator iterator(DecoratedKey key, Slices slices, ColumnFilter selectedColumns, boolean reversed, SSTableReadsListener listener)
           {
               throw throwCorrupted();
           }
   
           public UnfilteredRowIterator iterator(FileDataInput file, DecoratedKey key, RowIndexEntry indexEntry, Slices slices, ColumnFilter selectedColumns, boolean reversed)
           {
               throw throwCorrupted();
           }
   
           private CorruptSSTableException throwCorrupted()
           {
               throw new CorruptSSTableException(new IOException(""failed to get position""), descriptor.baseFilename());
           }
       }
   }
   ```
   
   This test replicates the behavior reported in the JIRA and shows that a corrupt sstable on a read will not trigger the disk failure policy and that your patch fixes it.  This problem is localized to the read/write stages as compaction actually handles this behavior (as shown by the FailingRepairTest).




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jul/20 18:28;githubbot;600","dcapwell commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r457611079



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -58,11 +59,32 @@ private JVMStabilityInspector() {}
      */
     public static void inspectThrowable(Throwable t) throws OutOfMemoryError
     {
-        inspectThrowable(t, true);
+        inspectThrowable(t, null, true, true);

Review comment:
       most examples I see calling this method happen in the current thread, so thinking we should use the current thread as the default rather than null.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jul/20 18:32;githubbot;600","dcapwell commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r457611900



##########
File path: src/java/org/apache/cassandra/utils/JVMStabilityInspector.java
##########
@@ -58,11 +59,32 @@ private JVMStabilityInspector() {}
      */
     public static void inspectThrowable(Throwable t) throws OutOfMemoryError
     {
-        inspectThrowable(t, true);
+        inspectThrowable(t, null, true, true);
     }
 
+
     public static void inspectThrowable(Throwable t, boolean propagateOutOfMemory) throws OutOfMemoryError
     {
+        inspectThrowable(t, null, propagateOutOfMemory, true);

Review comment:
       this looks mostly used by networking, and the exception thrown is in a different thread... sadly null makes sense given the current usage =(




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jul/20 18:34;githubbot;600","dcapwell commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r457612698



##########
File path: test/unit/org/apache/cassandra/utils/JVMStabilityInspectorTest.java
##########
@@ -60,6 +62,14 @@ public void testKill() throws Exception
             JVMStabilityInspector.inspectThrowable(new FSReadError(new IOException(), ""blah""));
             assertTrue(killerForTests.wasKilled());
 
+            killerForTests.reset();
+            JVMStabilityInspector.inspectThrowable(new FSWriteError(new IOException(), ""blah""));
+            assertTrue(killerForTests.wasKilled());
+
+            killerForTests.reset();
+            JVMStabilityInspector.inspectThrowable(new CorruptSSTableException(new IOException(), ""blah""));

Review comment:
       can you also add a second one where the corrupt sstable is wrapped in a runnable?




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jul/20 18:35;githubbot;600","smiklosovic commented on a change in pull request #684:
URL: https://github.com/apache/cassandra/pull/684#discussion_r457622425



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,105 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import org.junit.Test;
+
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.JVMStabilityInspector;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl

Review comment:
       @dcapwell good stuff, I wanted to do something similar but I am not so strong in Cassandra test framework itself and I couldnt express that idea into the code as you wrote it. I ll incorporate this into the PR and we should be ready to go I guess ... 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;20/Jul/20 18:55;githubbot;600","dcapwell commented on a change in pull request #681:
URL: https://github.com/apache/cassandra/pull/681#discussion_r458267773



##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,210 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.junit.Test;
+
+import junit.framework.Assert;
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.RowIndexEntry;
+import org.apache.cassandra.db.Slices;
+import org.apache.cassandra.db.filter.ColumnFilter;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.sstable.format.ForwardingSSTableReader;
+import org.apache.cassandra.io.sstable.format.SSTableReader;
+import org.apache.cassandra.io.sstable.format.SSTableReadsListener;
+import org.apache.cassandra.io.util.FileDataInput;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl
+{
+    @Test
+    public void testAbstractLocalAwareExecutorServiceOnIgnoredDiskFailurePolicy() throws Exception
+    {
+        test(DiskFailurePolicy.ignore, true, true);
+    }
+
+    @Test
+    public void testAbstractLocalAwareExecutorServiceOnStopParanoidDiskFailurePolicy() throws Exception
+    {
+        test(DiskFailurePolicy.stop_paranoid, false, false);
+    }
+
+    private static void test(DiskFailurePolicy policy, boolean expectNativeTransportRunning, boolean expectGossiperEnabled) throws Exception
+    {
+        String table = policy.name();
+        try (final Cluster cluster = init(getCluster(policy).start()))
+        {
+            IInvokableInstance node = cluster.get(1);
+
+            Boolean[] setup = node.callOnInstance((SerializableCallable<Boolean[]>) () -> {
+                CassandraDaemon instanceForTesting = CassandraDaemon.getInstanceForTesting();
+                instanceForTesting.completeSetup();
+                StorageService.instance.registerDaemon(instanceForTesting);
+                FileUtils.setFSErrorHandler(new DefaultFSErrorHandler());
+
+                return new Boolean[]{ StorageService.instance.isNativeTransportRunning(), Gossiper.instance.isEnabled() };
+            });
+
+            // make sure environment is setup propertly
+            Assert.assertTrue(""Native support is not running, test is not ready!"", setup[0]);
+            Assert.assertTrue(""Gossiper is not running, test is not ready!"", setup[1]);
+
+            cluster.schemaChange(""CREATE TABLE "" + KEYSPACE + '.' + table + "" (id bigint PRIMARY KEY)"");
+            node.executeInternal(""INSERT INTO "" + KEYSPACE + '.' + table + "" (id) VALUES (?)"", 0L);
+
+            corruptTable(node, KEYSPACE, table);
+
+            try
+            {
+                cluster.coordinator(1).execute(""SELECT * FROM "" + KEYSPACE + '.' + table + "" WHERE id=?"", ConsistencyLevel.ONE, 0L);
+                Assert.fail(""Select should fail as we corrupted SSTable on purpose."");
+            }
+            catch (final Exception ex)
+            {
+                // we expect that above query fails as we corrupted an sstable
+            }
+
+            waitForStop(!expectGossiperEnabled, node, new SerializableCallable<Boolean>()
+            {
+                public Boolean call()
+                {
+                    return Gossiper.instance.isEnabled();
+                }
+            });
+
+            waitForStop(!expectNativeTransportRunning, node, new SerializableCallable<Boolean>()
+            {
+                public Boolean call()
+                {
+                    return StorageService.instance.isNativeTransportRunning();
+                }
+            });
+        }
+    }
+
+    private static void waitForStop(boolean shouldWaitForStop,
+                                    IInvokableInstance node,
+                                    SerializableCallable<Boolean> serializableCallable) throws Exception
+    {
+        int attempts = 3;
+        boolean running = true;
+
+        while (attempts > 0 && running)
+        {
+            try
+            {
+                running = node.callOnInstance(serializableCallable);
+                attempts--;
+            }
+            catch (final NoClassDefFoundError ex)

Review comment:
       these feel like jvm-dtest bugs, can you file a ticket for this?

##########
File path: test/distributed/org/apache/cassandra/distributed/test/JVMStabilityInspectorCorruptSSTableExceptionTest.java
##########
@@ -0,0 +1,210 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.distributed.test;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+
+import org.junit.Test;
+
+import junit.framework.Assert;
+import org.apache.cassandra.config.Config.DiskFailurePolicy;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.RowIndexEntry;
+import org.apache.cassandra.db.Slices;
+import org.apache.cassandra.db.filter.ColumnFilter;
+import org.apache.cassandra.db.rows.UnfilteredRowIterator;
+import org.apache.cassandra.distributed.Cluster;
+import org.apache.cassandra.distributed.api.ConsistencyLevel;
+import org.apache.cassandra.distributed.api.IInvokableInstance;
+import org.apache.cassandra.distributed.api.IIsolatedExecutor.SerializableCallable;
+import org.apache.cassandra.distributed.shared.AbstractBuilder;
+import org.apache.cassandra.distributed.shared.NetworkTopology;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.io.sstable.CorruptSSTableException;
+import org.apache.cassandra.io.sstable.format.ForwardingSSTableReader;
+import org.apache.cassandra.io.sstable.format.SSTableReader;
+import org.apache.cassandra.io.sstable.format.SSTableReadsListener;
+import org.apache.cassandra.io.util.FileDataInput;
+import org.apache.cassandra.io.util.FileUtils;
+import org.apache.cassandra.service.CassandraDaemon;
+import org.apache.cassandra.service.DefaultFSErrorHandler;
+import org.apache.cassandra.service.StorageService;
+
+import static org.apache.cassandra.distributed.api.Feature.GOSSIP;
+import static org.apache.cassandra.distributed.api.Feature.NATIVE_PROTOCOL;
+import static org.apache.cassandra.distributed.api.Feature.NETWORK;
+
+public class JVMStabilityInspectorCorruptSSTableExceptionTest extends TestBaseImpl
+{
+    @Test
+    public void testAbstractLocalAwareExecutorServiceOnIgnoredDiskFailurePolicy() throws Exception
+    {
+        test(DiskFailurePolicy.ignore, true, true);
+    }
+
+    @Test
+    public void testAbstractLocalAwareExecutorServiceOnStopParanoidDiskFailurePolicy() throws Exception
+    {
+        test(DiskFailurePolicy.stop_paranoid, false, false);
+    }
+
+    private static void test(DiskFailurePolicy policy, boolean expectNativeTransportRunning, boolean expectGossiperEnabled) throws Exception
+    {
+        String table = policy.name();
+        try (final Cluster cluster = init(getCluster(policy).start()))
+        {
+            IInvokableInstance node = cluster.get(1);
+
+            Boolean[] setup = node.callOnInstance((SerializableCallable<Boolean[]>) () -> {
+                CassandraDaemon instanceForTesting = CassandraDaemon.getInstanceForTesting();
+                instanceForTesting.completeSetup();
+                StorageService.instance.registerDaemon(instanceForTesting);
+                FileUtils.setFSErrorHandler(new DefaultFSErrorHandler());

Review comment:
       was this needed?  I removed in trunk since jvm dtest adds this already




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;21/Jul/20 17:31;githubbot;600","smiklosovic closed pull request #681:
URL: https://github.com/apache/cassandra/pull/681


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/20 09:01;githubbot;600","smiklosovic commented on pull request #681:
URL: https://github.com/apache/cassandra/pull/681#issuecomment-675986883


   closing as that is already merged


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;19/Aug/20 09:01;githubbot;600","smiklosovic closed pull request #684:
URL: https://github.com/apache/cassandra/pull/684


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Nov/20 21:09;githubbot;600",,,,,,,,,,,,,,0,14400,,,0,14400,,,,,,,,,CASSANDRA-18294,,,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/19 04:44;VincentWhite;log.txt;https://issues.apache.org/jira/secure/attachment/12973303/log.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,stefan.miklosovic,,,,,,,,,,,,Correctness,,,,,,,,Normal,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Jul 30 00:31:04 UTC 2020,,,,,,,All,,,,,"0|z048qg:",9223372036854775807,,,,,,,brandon.williams,dcapwell,,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/db034609554a3185c0808cc67e9f0c148cc912c4,,,,,,,,,jvm dtest,,,,,"16/Jul/20 15:20;stefan.miklosovic;PR for 3.11 [https://github.com/apache/cassandra/pull/681];;;","16/Jul/20 15:25;stefan.miklosovic;Hi [~jeromatron] and [~Bereng], could you review this, please? I ll create patch for trunk if proposed solution is fine here.;;;","17/Jul/20 08:00;stefan.miklosovic;PR for trunk aka 4.0 [https://github.com/apache/cassandra/pull/684];;;","18/Jul/20 00:57;dcapwell;Took a stab at review and left a few comments in the PR.;;;","18/Jul/20 11:22;stefan.miklosovic;[~dcapwell] please review again, I have added a test (hopefully that is something you expect otherwise I am out of ideas here) + I have moved the logging from ALAES to inspector.;;;","20/Jul/20 18:30;dcapwell;Thanks for the changes.  To help show what I was trying (and failing) to say in the PR, I posted different tests that hit the read stage and show this is a problem.  ;;;","20/Jul/20 18:36;dcapwell;Overall the patch LGTM (only reviewed trunk so far), my main comments were in the tests; hope the example given helps.

[~stefan.miklosovic] do you have any CI runs for this patch?;;;","20/Jul/20 22:38;dcapwell;python dtests failed; it looks like exceptions were not logged before and all are logged now.  Simple example is with the auth tests found in https://app.circleci.com/pipelines/github/dcapwell/cassandra/297/workflows/aefdb912-4395-498a-a1b6-b16770d46a45/jobs/1438 

test_udf_permissions_validation - auth_test.TestAuthRoles

{code}
Unexpected error found in node logs (see stdout for full details). Errors: [ERROR [Native-Transport-Requests-11] 2020-07-20 21:51:31,717 JVMStabilityInspector.java:81 - Uncaught exception in thread Thread[Native-Transport-Requests-11,10,main]
org.apache.cassandra.exceptions.UnauthorizedException: User mike has no ALTER permission on <function ks.plus_one(int)> or any of its parents
	at org.apache.cassandra.service.ClientState.ensurePermissionOnResourceChain(ClientState.java:430)
	at org.apache.cassandra.service.ClientState.ensurePermission(ClientState.java:404)
	at org.apache.cassandra.cql3.statements.schema.CreateFunctionStatement.authorize(CreateFunctionStatement.java:177)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:203)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:253)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240)
	at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:108)
	at org.apache.cassandra.transport.Message$Request.execute(Message.java:253)
	at org.apache.cassandra.transport.Message$Dispatcher.processRequest(Message.java:725)
	at org.apache.cassandra.transport.Message$Dispatcher.lambda$channelRead0$0(Message.java:630)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:119)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
{code}

This exception wasn't logged before and was sent back to the user, with this patch we now log all these hidden exceptions.;;;","21/Jul/20 18:50;dcapwell;FYI conversation has been happening in slack: https://the-asf.slack.com/archives/CK23JSY2K/p1595280621333400

Updates:

* the tests are flaky, looks like there is a race condition in the test where the flag isn't updated yet.  A workaround was added to query multiple times with a 5 second sleep in hopes to make the tests stable;;;","24/Jul/20 00:27;dcapwell;CI 3.11 - https://app.circleci.com/pipelines/github/dcapwell/cassandra/309/workflows/b4cbed8d-868f-4640-a697-471fa03fd4bf
CI trunk - https://app.circleci.com/pipelines/github/dcapwell/cassandra/310/workflows/62969c9b-9c65-4558-9ec0-3fcc3f17d79e

Looks like this patch doesn't play nicely with commit log, this breaks the following tests

commitlog_test.py
 - test_ignore_failure_policy
 - test_stop_commit_failure_policy

Here is the log from the ignore policy test https://1573-209217594-gh.circle-artifacts.com/62/dtest_j8_without_vnodes_logs/1595547611103_test_ignore_failure_policy/node1.log

sample that stands out

{code}
ERROR [COMMIT-LOG-ALLOCATOR] 2020-07-23 23:40:08,735 CommitLog.java:499 - Failed managing commit log segments
org.apache.cassandra.io.FSWriteError: java.nio.file.AccessDeniedException: /tmp/dtest-zt17lw0m/test/node1/commitlogs/CommitLog-7-1595547598804.log
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:180)
	at org.apache.cassandra.db.commitlog.MemoryMappedSegment.<init>(MemoryMappedSegment.java:45)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.createSegment(CommitLogSegment.java:137)
	at org.apache.cassandra.db.commitlog.CommitLogSegmentManagerStandard.createSegment(CommitLogSegmentManagerStandard.java:66)
	at org.apache.cassandra.db.commitlog.AbstractCommitLogSegmentManager$1.runMayThrow(AbstractCommitLogSegmentManager.java:114)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.AccessDeniedException: /tmp/dtest-zt17lw0m/test/node1/commitlogs/CommitLog-7-1595547598804.log
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newFileChannel(UnixFileSystemProvider.java:177)
	at java.nio.channels.FileChannel.open(FileChannel.java:287)
	at java.nio.channels.FileChannel.open(FileChannel.java:335)
	at org.apache.cassandra.db.commitlog.CommitLogSegment.<init>(CommitLogSegment.java:175)
	... 7 common frames omitted
ERROR [COMMIT-LOG-ALLOCATOR] 2020-07-23 23:40:09,736 DefaultFSErrorHandler.java:66 - Stopping transports as disk_failure_policy is stop
{code}

Looks like the commit policy isn't respected and instead we fall back to the normal disk policy.

[~stefan.miklosovic] can you look into this?;;;","25/Jul/20 20:50;dcapwell;I took a stab at back porting to 3.0: https://github.com/dcapwell/cassandra/tree/CASSANDRA-15191-3.0

I also made a few small changes from your original patch

1) calls to FSError handler now go through jvm stability
2) Instance adds default fs handler.  this was working on trunk since we fixed it there, but didn't backport; so adding it so we don't need in the test;;;","28/Jul/20 22:19;brandon.williams;3.0 looks good, +1.;;;","29/Jul/20 22:24;dcapwell;+1 from me, getting the commit ready;;;","30/Jul/20 00:31;dcapwell;Thanks [~stefan.miklosovic] for all the hard work!

CI results:

3.0: https://app.circleci.com/pipelines/github/dcapwell/cassandra/373/workflows/359198de-fb02-4a15-95c3-6341492d1f4e
3.11: https://app.circleci.com/pipelines/github/dcapwell/cassandra/374/workflows/a31e1d8a-3fd0-4665-b00e-c62919b816cb
trunk: https://app.circleci.com/pipelines/github/dcapwell/cassandra/377/workflows/dc48ffe6-d168-4cd4-a858-0f6b5e2bc730

3.0 and 3.11 failed with known broken tests or known flaky tests; trunk was green.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InternodeOutboundMetrics overloaded bytes/count mixup,CASSANDRA-15186,13241946,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tdl-jturner,molsson,molsson,27/Jun/19 13:23,15/May/20 08:06,13/Jul/23 08:38,24/Aug/19 04:15,4.0,4.0-alpha1,,,,Observability/Metrics,,,,0,,,,"In [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/metrics/InternodeOutboundMetrics.java] there is a small mixup between overloaded count and bytes, in [LargeMessageDroppedTasksDueToOverload|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/metrics/InternodeOutboundMetrics.java#L129] and [UrgentMessageDroppedTasksDueToOverload|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/metrics/InternodeOutboundMetrics.java#L151].",,jeromatron,jjirsa,molsson,tdl-jturner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/19 21:14;tdl-jturner;15186-trunk.txt;https://issues.apache.org/jira/secure/attachment/12978229/15186-trunk.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,tdl-jturner,,,,,,,,,,,,Code -> Bug - Unclear Impact,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Sat Aug 24 04:15:33 UTC 2019,,,,,,,All,,,,,"0|z045uw:",9223372036854775807,,,,,,,jjirsa,,,,Low,,4.0,,,https://github.com/apache/cassandra/commit/0622288c9b2252e31bba0da73a8608ba2aa13ff3,,,,,,,,,N/A,,,,,"21/Aug/19 21:25;tdl-jturner;Attached a simple patch correcting this mixup.;;;","24/Aug/19 04:15;jjirsa;Thanks! Committed as [0622288c9b2252e31bba0da73a8608ba2aa13ff3|https://github.com/apache/cassandra/commit/0622288c9b2252e31bba0da73a8608ba2aa13ff3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh syntax error output fails with UnicodeEncodeError,CASSANDRA-15182,13241389,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,gloCalHelp.com,gloCalHelp.com,25/Jun/19 05:31,01/Aug/19 22:05,13/Jul/23 08:38,03/Jul/19 05:14,,,,,,CQL/Interpreter,,,,0,,,,"I use cqlsh 5.0.1 with cassandra 3.11.3 with python2.7.13 in Centos 6.9.

when I run this cql command: bin/cqlsh hadoop4 -u dba -p ********** --debug  -e ""INSERT INTO HYGL_JCSJ.hyjg_ods_yy_gps_novar3 (clcph,dwsj,bc,blbs,cjbzh,ckryid,clid,clmc,ddfx,ddrq,fwj,gd,gdjd,gdwd,jsdlc,jszjl,jxzjl,sjid,sjsfzh,sjxm,sssd,xlmc) VALUES ('黑A00888D','2019-06-2509:57:19',0,,'',,,'379-7038',1434,'2019-06-25',275,0,126723690,45726990 ,796.0,2205,746,'null','null','null',0,'379');""

I get the error message as below:

Using CQL driver: <module 'cassandra' from '/home/cassandra/cas3.11.3/bin/../lib/cassandra-driver-internal-only-3.11.0-bb96859b.zip/cassandra-driver-3.11.0-bb96859b/cassandra/__init__.py'>
Using connect timeout: 5 seconds
Using 'utf-8' encoding
Using ssl: False
Traceback (most recent call last):
 File ""/home/cassandra/cas3.11.3/bin/cqlsh.py"", line 926, in onecmd
 self.handle_statement(st, statementtext)
 File ""/home/cassandra/cas3.11.3/bin/cqlsh.py"", line 966, in handle_statement
 return self.perform_statement(cqlruleset.cql_extract_orig(tokens, srcstr))
 File ""/home/cassandra/cas3.11.3/bin/cqlsh.py"", line 1000, in perform_statement
 success, future = self.perform_simple_statement(stmt)
 File ""/home/cassandra/cas3.11.3/bin/cqlsh.py"", line 1053, in perform_simple_statement
 self.printerr(unicode(err.__class__.__name__) + u"": "" + err.message.decode(encoding='utf-8'))
 File ""/usr/local/python27/lib/python2.7/encodings/utf_8.py"", line 16, in decode
 return codecs.utf_8_decode(input, errors, True)
UnicodeEncodeError: 'ascii' codec can't encode character u'\u9ed1' in position 60: ordinal not in range(128)

 

this issue seems different with the select command issue on  https://issues.apache.org/jira/browse/CASSANDRA-10875 

and other method to add ""-*- coding: utf-8 -*- "" in the head of cqlsh.py ,  can anyone hurry up to teach me?

 ",,gloCalHelp.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Jul 03 05:14:13 UTC 2019,,,,,,,All,,,,,"0|z042f4:",9223372036854775807,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"25/Jun/19 15:30;mshuler;(Edited description to remove auth.. (OP should change that))

What is the local {{$LANG}} environment?
Could you post a sanitized and/or simplified schema that reproduces this problem?

{noformat}
(master)mshuler@hana:~/git/ccm$ echo $LANG
en_US.UTF-8

(master)mshuler@hana:~/git/ccm$ ./ccm create test --install-dir=/home/mshuler/git/cassandra
Current cluster is now: test
(master)mshuler@hana:~/git/ccm$ ./ccm populate -n 1
(master)mshuler@hana:~/git/ccm$ ./ccm start

(master)mshuler@hana:~/git/ccm$ ./ccm node1 cqlsh
Connected to test at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.5-SNAPSHOT | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; 
cqlsh> CREATE TABLE test.scratch (a text, b text, PRIMARY KEY (a));
cqlsh> INSERT INTO test.scratch (a, b) VALUES ('黑A00888D', 'something');
cqlsh> SELECT * FROM test.scratch;

 a         | b
-----------+---------------
 黑A00888D | something

(1 rows)
{noformat}

I seem to be able to INSERT and SELECT that character with my local LANG=en_US.UTF-8 env. I believe python-2.7 was not available until RHEL/CentOS 7, so I suspect the local environment and python install may need a little configuration? Just a guess.;;;","26/Jun/19 02:02;gloCalHelp.com;Thanks Sir Michael, because the report error position is not correct, and

I don't know how to santize the insert sql, but the shorter one with a Chinese character can be successfully inserted as below:

INSERT INTO HYGL_JCSJ.hyjg_ods_yy_gps_novar3 (clcph,dwsj,bc) VALUES 
 ('黑A00888D','2019-06-25 09:57:19',0);"", and my LANG environment is really this:

bash-4.1$ echo $LANG
en_US.UTF-8

        Is there anyone who can tell me how to make cqlsh report the correct position of wrong sql except using --debug variable?

 

    ;;;","27/Jun/19 02:03;yukim;Hi [~gloCalHelp.com],

cqlsh is complaining when it is trying to output an error, since your original CQL syntax is wrong (you have to provide some values for columns not like ',,,'.)

'Short' cql did not contain syntax error so it went through.

 ;;;","27/Jun/19 15:46;mshuler;Thanks Yuki, that helps! Simplified reproduction - CQL syntax is wrong, but the syntax error output encoding is problematic:
{noformat}
ccm create --version=git:cassandra-3.11 --nodes=1 --start test
ccm node1 cqlsh --debug -e ""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};""
ccm node1 cqlsh --debug -e ""CREATE TABLE test.scratch (a text, b decimal, PRIMARY KEY (a));""
# show syntax error with INSERT:
ccm node1 cqlsh --debug -e ""INSERT INTO test.scratch (a, b) VALUES ('A00888D',);""
# show unicode problem when syntax error is output:
ccm node1 cqlsh --debug -e ""INSERT INTO test.scratch (a, b) VALUES ('黑A00888D',);""
{noformat}

Full output of above:
{noformat}
mshuler@hana:~$ ccm create --version=git:cassandra-3.11 --nodes=1 --start test
https://gitbox.apache.org/repos/asf/cassandra.git git:cassandra-3.11
10:22:53,842 ccm INFO Cloning Cassandra...
10:23:20,346 ccm INFO Cloning Cassandra (from local cache)
10:23:21,129 ccm INFO Checking out requested branch (cassandra-3.11)
10:23:21,927 ccm INFO Compiling Cassandra cassandra-3.11 ...
Current cluster is now: test
mshuler@hana:~$ ccm node1 cqlsh --debug -e ""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};""Using CQL driver: <module 'cassandra' from '/home/mshuler/.ccm/repository/gitCOLONcassandra-3.11/bin/../lib/cassandra-driver-internal-only-3.11.0-bb96859b.zip/cassandra-driver-3.11.0-bb96859b/cassandra/__init__.py'>
Using connect timeout: 5 seconds
Using 'utf-8' encoding
Using ssl: False
mshuler@hana:~$ ccm node1 cqlsh --debug -e ""CREATE TABLE test.scratch (a text, b decimal, PRIMARY KEY (a));""
Using CQL driver: <module 'cassandra' from '/home/mshuler/.ccm/repository/gitCOLONcassandra-3.11/bin/../lib/cassandra-driver-internal-only-3.11.0-bb96859b.zip/cassandra-driver-3.11.0-bb96859b/cassandra/__init__.py'>
Using connect timeout: 5 seconds
Using 'utf-8' encoding
Using ssl: False
mshuler@hana:~$ ccm node1 cqlsh --debug -e ""INSERT INTO test.scratch (a, b) VALUES ('A00888D',);""
Using CQL driver: <module 'cassandra' from '/home/mshuler/.ccm/repository/gitCOLONcassandra-3.11/bin/../lib/cassandra-driver-internal-only-3.11.0-bb96859b.zip/cassandra-driver-3.11.0-bb96859b/cassandra/__init__.py'>
Using connect timeout: 5 seconds
Using 'utf-8' encoding
Using ssl: False
<stdin>:1:SyntaxException: line 1:50 no viable alternative at input ')' (..., b) VALUES ('A00888D',[)]...)
mshuler@hana:~$ ccm node1 cqlsh --debug -e ""INSERT INTO test.scratch (a, b) VALUES ('黑A00888D',);""
Using CQL driver: <module 'cassandra' from '/home/mshuler/.ccm/repository/gitCOLONcassandra-3.11/bin/../lib/cassandra-driver-internal-only-3.11.0-bb96859b.zip/cassandra-driver-3.11.0-bb96859b/cassandra/__init__.py'>
Using connect timeout: 5 seconds
Using 'utf-8' encoding
Using ssl: False
Traceback (most recent call last):
  File ""/home/mshuler/.ccm/repository/gitCOLONcassandra-3.11/bin/cqlsh.py"", line 925, in onecmd
    self.handle_statement(st, statementtext)
  File ""/home/mshuler/.ccm/repository/gitCOLONcassandra-3.11/bin/cqlsh.py"", line 965, in handle_statement
    return self.perform_statement(cqlruleset.cql_extract_orig(tokens, srcstr))
  File ""/home/mshuler/.ccm/repository/gitCOLONcassandra-3.11/bin/cqlsh.py"", line 999, in perform_statement
    success, future = self.perform_simple_statement(stmt)
  File ""/home/mshuler/.ccm/repository/gitCOLONcassandra-3.11/bin/cqlsh.py"", line 1052, in perform_simple_statement
    self.printerr(unicode(err.__class__.__name__) + u"": "" + err.message.decode(encoding='utf-8'))
  File ""/usr/lib/python2.7/encodings/utf_8.py"", line 16, in decode
    return codecs.utf_8_decode(input, errors, True)
UnicodeEncodeError: 'ascii' codec can't encode character u'\u9ed1' in position 63: ordinal not in range(128)
{noformat};;;","27/Jun/19 16:04;mshuler;Reproduced in cassandra-2.2, cassandra-3.0, cassandra-3.11, and trunk branch HEADs.;;;","29/Jun/19 02:50;gloCalHelp.com;Hi, Yuki SenSei, AriGaTou GouZaiMaSu, you are right, the "",,,,"" is  the problem, because  these columns are decimal without a value, so  I let them blank as traditional DB. Now I put a default value of -20 for these blank decimal , it works now:

bash-4.1$ bin/cqlsh hadoop6 -u  -p  --debug --encoding utf-8  -e ""INSERT INTO HYGL_JCSJ.hyjg_ods_yy_gps_novar3 (clcph,dwsj,bc,blbs,cjbzh,ckryid,clid,clmc,ddfx,ddrq,fwj,gd,gdjd,gdwd,jsdlc,jszjl,jxzjl,sjid,sjsfzh,sjxm,sssd,xlmc) VALUES ('8516178','2019-06-25 09:57:19',0,-20,'',-20,-20,'379-7038',1434,'2019-06-25',275,0,126723690,45726990 ,796.0,2205,746,'null','null','null',0,'379');""

But I find a bug for Cassandra3.11.3, an insert same primary keys doesn't report a error  but work as update.

 ;;;","29/Jun/19 02:51;gloCalHelp.com;so this issue has close, how to close it?;;;","03/Jul/19 05:14;gloCalHelp.com;put a default  num for a non value decimal column, then solve the problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skipping illegal legacy cells can break reverse iteration of indexed partitions,CASSANDRA-15178,13240832,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,21/Jun/19 10:17,27/Jun/19 15:32,13/Jul/23 08:38,27/Jun/19 15:32,3.0.19,3.11.5,,,,Legacy/Local Write-Read Paths,,,,0,,,,"The fix for CASSANDRA-15086 interacts badly with the accounting of bytes read from disk when indexed partitions are read in reverse. The skipped columns can cause the tracking of where CQL rows span index block boundaries to be incorrectly calculated, leading to rows being missing from read results.
",,jasonstack,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Normal,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Jun 27 15:32:54 UTC 2019,,,,,,,All,,,,,"0|z03yzs:",9223372036854775807,,,,,,,marcuse,,,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/f4b6e1d51f683e0c77c6ff7f199373052b082b9e,,,,,,,,,New unit tests & legacy sstables,,,,,"21/Jun/19 10:46;samt;The linked branches revert the original CASSANDRA-15086 patch (but retain the test), and include an alternative fix whereby the illegal legacy cells are read from disk as normal, but get filtered out during the collation of LegacyCells into cql Rows. 

||branch||CI||
|[15178-3.0|https://github.com/beobal/cassandra/tree/15178-3.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F15178-3.0]|
|[15178-3.11|https://github.com/beobal/cassandra/tree/15178-3.11]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F15178-3.11]|
;;;","24/Jun/19 13:34;marcuse;+1;;;","27/Jun/19 15:32;samt;Thanks, committed to 3.0 in {{f4b6e1d51f683e0c77c6ff7f199373052b082b9e}} and merged to 3.11 and trunk (with {{-s ours}});;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reloading of auth caches happens on the calling thread,CASSANDRA-15177,13240706,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,20/Jun/19 18:03,25/Apr/21 11:31,13/Jul/23 08:38,23/Mar/21 10:07,4.0,4.0-rc1,,,,Feature/Authorization,,,,2,,,,"When Guava caches were replaced by their Caffeine equivalents in CASSANDRA-10855, the async reloading of stale AuthCache entries was lost due to the use of {{MoreExecutors.directExecutor()}} to provide the delegate executor. Under normal conditions, we can expect these operations to be relatively expensive, and in failure scenarios where replicas for the auth data are DOWN this will greatly increase latency, so they shouldn’t be done on threads servicing requests.
",,ben.manes,blerer,dnk,eperott,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10855,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 23 10:07:14 UTC 2021,,,,,,,All,,,,,"0|z03y80:",9223372036854775807,,,,,,,blerer,brandon.williams,,,Normal,,4.0-alpha1,,,https://github.com/apache/cassandra/commit/d656f8ac012f4577d22ed7bd3db94c15ae8eb5a9,,,,,,,,,Covered by existing python dtests,,,,,"24/Jun/19 03:49;ben.manes;If you implement CacheLoader, you can override asyncReload to supply the refresh future tied to a different executor.;;;","24/Jun/19 09:19;samt;[~ben.manes] can't we just supply a different executor here: [https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/auth/AuthCache.java#L221] ? ;;;","24/Jun/19 13:20;ben.manes;Oh of course. There was a request at some point to not do that, so I was mentioning the trick in case if you preferred the mixed behavior.;;;","19/Mar/21 17:19;samt;I had forgotten about this, but we ought to fix it before rc. Reloading eligible items on a thread servicing user requests can cause timeouts and unavailables. 

[patch|https://github.com/beobal/cassandra/commits/15177-trunk], [circle|https://app.circleci.com/pipelines/github/beobal/cassandra?branch=cci%2F15177-trunk], [ci-c.a.o|https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/500]
;;;","19/Mar/21 19:45;brandon.williams;+1;;;","23/Mar/21 07:57;blerer;The patch looks good to me.;;;","23/Mar/21 10:07;samt;Thanks, committed to trunk in {{d656f8ac012f4577d22ed7bd3db94c15ae8eb5a9}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix PagingState deserialization when the state was serialized using protocol version different from current session's,CASSANDRA-15176,13240670,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,20/Jun/19 15:06,15/May/20 08:00,13/Jul/23 08:38,26/Jun/19 14:33,3.0.19,3.11.5,4.0,4.0-alpha1,,Messaging/Client,,,,1,,,,"3.0 and native protocol V4 introduced a change to how {{PagingState}} is serialized. Unfortunately that can break requests during upgrades: since paging states are opaque, it's possible for a client to receive a paging state encoded as V3 on a 2.1 node, and then send it to a 3.0 node on a V4 session. The version of the current session will be used to deserialize the paging state, instead of the actual version used to serialize it, and the request will fail.

This is obviously sub-optimal, but also avoidable. This JIRA fixes one half of the problem: 3.0 failing to deserialize 'mislabeled' paging states. We can do this by inspecting the byte buffer to verify if it's been indeed serialized with the protocol version used by the session, and if not, use the other method of deserialization.

It should be noted that we list this as a 'known limitation' somewhere, but really this is an upgrade-blocking bug for some users of C*.",,aleksey,andrew.tolbert,bdeggleston,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Availability -> Response Crash,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Jun 26 14:33:07 UTC 2019,,,,,,,All,,,,,"0|z03y00:",9223372036854775807,,,,,,,bdeggleston,samt,,,Normal,,3.0.0,,,"[dad82fbd30c8b87c4c9fa02abc6796ba8c2bf99a|https://github.com/apache/cassandra/commit/dad82fbd30c8b87c4c9fa02abc6796ba8c2bf99a]",,,,,,,,,Expanded unit test coverage,,,,,"20/Jun/19 15:09;aleksey;3.0: [branch|https://github.com/iamaleksey/cassandra/commits/15176-3.0], [CI|https://circleci.com/workflow-run/1ad6fb56-9c77-48d8-8f1f-0e10a65db708]

3.11: [branch|https://github.com/iamaleksey/cassandra/commits/15176-3.11], [CI|https://circleci.com/workflow-run/53bb7919-9422-4d43-9613-2d42b233721f]

4.0: [branch|https://github.com/iamaleksey/cassandra/commits/15176-4.0], [CI|https://circleci.com/workflow-run/23df2f86-52bb-400e-9fab-83e71122c46b];;;","25/Jun/19 18:33;bdeggleston;+1;;;","26/Jun/19 10:47;samt;+1;;;","26/Jun/19 14:33;aleksey;Cheers, committed to 3.0 as [dad82fbd30c8b87c4c9fa02abc6796ba8c2bf99a|https://github.com/apache/cassandra/commit/dad82fbd30c8b87c4c9fa02abc6796ba8c2bf99a] and merged with 3.11 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LegacyLayout RangeTombstoneList throws IndexOutOfBoundsException,CASSANDRA-15172,13240381,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,Sagges,Sagges,19/Jun/19 09:32,04/Oct/19 16:20,13/Jul/23 08:38,22/Aug/19 11:50,3.0.19,3.11.5,,,,Local/Other,,,,2,,,,"Hi All,

This is the first time I open an issue, so apologies if I'm not following the rules properly.

 

After upgrading a node from version 2.1.21 to 3.11.4, we've started seeing a lot of AbstractLocalAwareExecutorService exceptions. This happened right after the node successfully started up with the new 3.11.4 binaries. 
{noformat}
INFO  [main] 2019-06-05 04:41:37,730 Gossiper.java:1715 - No gossip backlog; proceeding
INFO  [main] 2019-06-05 04:41:38,036 NativeTransportService.java:70 - Netty using native Epoll event loop
INFO  [main] 2019-06-05 04:41:38,117 Server.java:155 - Using Netty Version: [netty-buffer=netty-buffer-4.0.44.Final.452812a, netty-codec=netty-codec-4.0.44.Final.452812a, netty-codec-haproxy=netty-codec-haproxy-4.0.44.Final.452812a, netty-codec-http=netty-codec-http-4.0.44.Final.452812a, netty-codec-socks=netty-codec-socks-4.0.44.Final.452812a, netty-common=netty-common-4.0.44.Final.452812a, netty-handler=netty-handler-4.0.44.Final.452812a, netty-tcnative=netty-tcnative-1.1.33.Fork26.142ecbb, netty-transport=netty-transport-4.0.44.Final.452812a, netty-transport-native-epoll=netty-transport-native-epoll-4.0.44.Final.452812a, netty-transport-rxtx=netty-transport-rxtx-4.0.44.Final.452812a, netty-transport-sctp=netty-transport-sctp-4.0.44.Final.452812a, netty-transport-udt=netty-transport-udt-4.0.44.Final.452812a]
INFO  [main] 2019-06-05 04:41:38,118 Server.java:156 - Starting listening for CQL clients on /0.0.0.0:9042 (unencrypted)...
INFO  [main] 2019-06-05 04:41:38,179 CassandraDaemon.java:556 - Not starting RPC server as requested. Use JMX (StorageService->startRPCServer()) or nodetool (enablethrift) to start it
INFO  [Native-Transport-Requests-21] 2019-06-05 04:41:39,145 AuthCache.java:161 - (Re)initializing PermissionsCache (validity period/update interval/max entries) (2000/2000/1000)
INFO  [OptionalTasks:1] 2019-06-05 04:41:39,729 CassandraAuthorizer.java:409 - Converting legacy permissions data
INFO  [HANDSHAKE-/10.10.10.8] 2019-06-05 04:41:39,808 OutboundTcpConnection.java:561 - Handshaking version with /10.10.10.8
INFO  [HANDSHAKE-/10.10.10.9] 2019-06-05 04:41:39,808 OutboundTcpConnection.java:561 - Handshaking version with /10.10.10.9
INFO  [HANDSHAKE-dc1_02/10.10.10.6] 2019-06-05 04:41:39,809 OutboundTcpConnection.java:561 - Handshaking version with dc1_02/10.10.10.6

WARN  [ReadStage-2] 2019-06-05 04:41:39,857 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: 1
        at org.apache.cassandra.db.AbstractBufferClusteringPrefix.get(AbstractBufferClusteringPrefix.java:55)
        at org.apache.cassandra.db.LegacyLayout$LegacyRangeTombstoneList.serializedSizeCompound(LegacyLayout.java:2545)
        at org.apache.cassandra.db.LegacyLayout$LegacyRangeTombstoneList.serializedSize(LegacyLayout.java:2522)
        at org.apache.cassandra.db.LegacyLayout.serializedSizeAsLegacyPartition(LegacyLayout.java:565)
        at org.apache.cassandra.db.ReadResponse$Serializer.serializedSize(ReadResponse.java:446)
        at org.apache.cassandra.db.ReadResponse$Serializer.serializedSize(ReadResponse.java:352)
        at org.apache.cassandra.net.MessageOut.payloadSize(MessageOut.java:171)
        at org.apache.cassandra.net.OutboundTcpConnectionPool.getConnection(OutboundTcpConnectionPool.java:77)
        at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:802)
        at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:953)
        at org.apache.cassandra.net.MessagingService.sendReply(MessagingService.java:929)
        at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:62)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:114)
        at java.lang.Thread.run(Thread.java:745)
 {noformat}

 

After several of the above warnings, the following warning appeared as well:

 {noformat}
WARN  [ReadStage-9] 2019-06-05 04:42:04,369 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-9,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: null
WARN  [ReadStage-11] 2019-06-05 04:42:04,381 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-11,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: null
WARN  [ReadStage-10] 2019-06-05 04:42:04,396 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-10,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: null
WARN  [ReadStage-2] 2019-06-05 04:42:04,443 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: null

 {noformat}
 

Then suddenly, Validation errors appeared although *no repair was running on any of the nodes*! Checked with {{ps -ef}} command and {{nodetool compactionstats}} on the entire cluster.

 

 {noformat}
ERROR [ValidationExecutor:2] 2019-06-05 04:42:47,979 Validator.java:268 - Failed creating a merkle tree for [repair #e54b4090-876d-11e9-a3f4-c33d22c45471 on ks1/table1, []], /
10.10.10.6 (see log for details)
ERROR [ValidationExecutor:2] 2019-06-05 04:42:47,979 CassandraDaemon.java:228 - Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.NullPointerException: null
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:1363)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager$13.call(CompactionManager.java:977)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81)
        at java.lang.Thread.run(Thread.java:745)
 {noformat}
 

Following those, client requests started to fail and NTR tasks started to pile up and get blocked and GC was impacted.

 {noformat}
INFO  [ScheduledTasks:1] 2019-06-05 04:43:11,660 StatusLogger.java:51 - Native-Transport-Requests       128       197         594810        65              2725
 {noformat}

 

FWIW, these are the warnings I found during startup: 

 {noformat}
-WARN in net.logstash.logback.encoder.LogstashEncoder@140e5a13 - Logback version is prior to 1.2.0.  Enabling backwards compatible encoding.  Logback 1.2.1 or greater is recommended.
 {noformat}

 

 {noformat}
WARN  [main] 2019-06-05 08:44:18,568 NativeLibrary.java:187 - Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
WARN  [main] 2019-06-05 08:44:18,569 StartupChecks.java:136 - jemalloc shared library could not be preloaded to speed up memory allocations

 

WARN  [main] 2019-06-05 08:44:20,225 Optional.java:159 - Legacy auth tables credentials, users, permissions in keyspace system_auth still exist and have not been properly migrated.

WARN  [MessagingService-Outgoing-dc1_03/10.10.10.4-Gossip] 2019-06-05 08:44:49,582 OutboundTcpConnection.java:486 - Seed gossip version is 8; will not connect with that version
WARN  [MessagingService-Outgoing-dc2_02/10.20.20.4-Gossip] 2019-06-05 08:44:49,620 OutboundTcpConnection.java:486 - Seed gossip version is 8; will not connect with that version
WARN  [MessagingService-Outgoing-dc2_01/10.20.20.1-Gossip] 2019-06-05 08:44:49,621 OutboundTcpConnection.java:486 - Seed gossip version is 8; will not connect with that version
WARN  [MessagingService-Outgoing-dc2_03/10.20.20.5-Gossip] 2019-06-05 08:44:49,621 OutboundTcpConnection.java:486 - Seed gossip version is 8; will not connect with that version
WARN  [GossipTasks:1] 2019-06-05 08:44:51,631 FailureDetector.java:278 - Not marking nodes down due to local pause of 30943606906 > 5000000000
 {noformat}

 

We've naturally stopped the upgrade but we still wish to upgrade from 2.1.21 and hopefully find the root cause of this matter. 
I'll be happy to provide additional details if needs be.

 

 ",,Ankitha,benedict,ferozshaik552@gmail.com,jeromatron,mck,Sagges,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Availability -> Response Crash,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Sep 25 16:39:47 UTC 2019,,,,,,,All,,,,,"0|z03w80:",9223372036854775807,,,,,,,mck,,,,Normal,,3.0 alpha 1,,,https://github.com/apache/cassandra/commit/2b10a5f2b5e62f2900119a37e91637916e8b23df,,,,,,,,,unit test included,,,,,"18/Jul/19 14:46;Sagges;Trying to push this up a bit because I still want to upgrade to 3.11.4 but I fear that this issue may recur.

If someone has any idea on what happened here or how to mitigate it, that'd be awesome!

 

Thanks!;;;","06/Aug/19 03:35;ferozshaik552@gmail.com;We have also hit this problem today while upgrading from 2.1.16 to 3.11.4/ 

we encountered this as soon as node started up with 3.11.4 

 

WARN [ReadStage-4] 2019-08-06 02:57:57,408 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-4,5,main]: {}
java.lang.NullPointerException: null

 

ERROR [Native-Transport-Requests-32] 2019-08-06 02:14:20,353 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null

and the below errors continued in the logfile as long as the process was up.

ERROR [Native-Transport-Requests-12] 2019-08-06 03:00:47,135 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-8] 2019-08-06 03:00:48,778 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-13] 2019-08-06 03:00:57,454 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-11] 2019-08-06 03:00:57,482 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-2] 2019-08-06 03:00:58,543 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-8] 2019-08-06 03:00:58,899 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-17] 2019-08-06 03:00:59,074 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-12] 2019-08-06 03:01:08,123 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-17] 2019-08-06 03:01:19,055 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-4] 2019-08-06 03:01:20,880 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 WARN [ReadStage-13] 2019-08-06 03:01:29,983 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-13,5,main]: {}
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-2] 2019-08-06 03:01:31,119 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-6] 2019-08-06 03:01:46,262 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-15] 2019-08-06 03:01:46,520 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 WARN [ReadStage-2] 2019-08-06 03:01:48,842 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]: {}
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-1] 2019-08-06 03:01:50,351 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-5] 2019-08-06 03:02:06,061 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 WARN [ReadStage-8] 2019-08-06 03:02:07,616 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-8,5,main]: {}
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-17] 2019-08-06 03:02:08,384 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null
 ERROR [Native-Transport-Requests-5] 2019-08-06 03:02:10,244 ErrorMessage.java:384 - Unexpected exception during request
 java.lang.NullPointerException: null

 

The nodetool version says 3.11.4 and the no of connections on 9042 was similar to other nodes. The exceptions were scary that we had to call off the change. Any help and insights to this problem from the community is appreciated.;;;","06/Aug/19 10:41;ferozshaik552@gmail.com;Full stack trace is as below:

WARN [ReadStage-4] 2019-08-06 02:57:57,408 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-4,5,main]: {}
java.lang.NullPointerException: null
 at org.apache.cassandra.db.LegacyLayout$LegacyRangeTombstoneList.updateDigest(LegacyLayout.java:2433) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.db.LegacyLayout$LegacyUnfilteredPartition.digest(LegacyLayout.java:1479) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.db.rows.UnfilteredRowIterators.digest(UnfilteredRowIterators.java:182) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.db.partitions.UnfilteredPartitionIterators.digest(UnfilteredPartitionIterators.java:263) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.db.ReadResponse.makeDigest(ReadResponse.java:140) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.db.ReadResponse.createDigestResponse(ReadResponse.java:87) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.db.ReadCommand.createResponse(ReadCommand.java:352) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:50) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) ~[apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134) [apache-cassandra-3.11.4.jar:3.11.4]
 at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:114) [apache-cassandra-3.11.4.jar:3.11.4]
 at java.lang.Thread.run(Thread.java:748) [na:1.8.0_131];;;","06/Aug/19 10:42;ferozshaik552@gmail.com;What we also know is that the nodes suffered some heavy tombstones recently.. could it be under specific condition like ""tombstones"" plus reading legacy version files is hitting this problem. [~Sagges] did you cluster have any tombstone references at all?;;;","06/Aug/19 10:45;ferozshaik552@gmail.com;We have currently isolated this node (cut off thrift , native protocols) and trying to run upgradesstables to see if it can re-write all the files and stop logging the below message.

""WARN [ReadStage-6] 2019-08-06 10:44:09,773 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-6,5,main]: {}
java.lang.NullPointerException: null""

I will keep the thread posted about its outcome, 

 ;;;","06/Aug/19 12:58;benedict;[~Sagges], sorry for the slow response - I missed the original filing of this ticket.

[~ferozshaik552@gmail.com] it looks like your bug, while very similar, presents differently.  It would be great if you could file a separate ticket.

Both of these look to be among the category of 2.1->3.0 upgrade bugs involving range deletions.  To best investigate and diagnose, it would be great to start with information about the affected schema, the kinds of range tombstone deletes you perform, and preferably if you could pin down sstables that are affected and upload them somewhere private for us to access.  This would help us investigate much more readily.

Could you also confirm if you utilise thrift, or CQL schema?  It's possible this is a compatibility issue specific to thrift.;;;","06/Aug/19 13:25;ferozshaik552@gmail.com;Sure, I shall raise a separate request.;;;","12/Aug/19 13:50;benedict;This bug appears to be similar to CASSANDRA-15263, in that a reverse query with the RTBoundCloser is the likely source of asymmetric range tombstone bounds.  However in this case the problem is much easier to solve; we simply have to not assume the bounds have the same length.

I have pushed a patch [here|https://github.com/belliottsmith/cassandra/tree/15172-3.0];;;","21/Aug/19 14:45;mck;[~benedict], we've seen this in the wild as well, with an upgrade from 2.2.14 to 3.11.4.
 I am jumping in to test and review it.;;;","21/Aug/19 21:44;mck;
||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[15172-3.0|https://github.com/apache/cassandra/compare/trunk...belliottsmith:15172-3.0]|[circleci|https://circleci.com/gh/belliottsmith/workflows/cassandra/tree/15172-3.0]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/44//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/44/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/679//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/679]|

;;;","22/Aug/19 11:50;mck;Committed as 2b10a5f2b5e62f2900119a37e91637916e8b23df;;;","25/Aug/19 09:02;Sagges;Hi [~benedict]

Sorry for the late reply...

 

Could you also confirm if you utilise thrift, or CQL schema? It's possible this is a compatibility issue specific to thrift.

I do see there are a few thrift counter tables that were created with the WITH COMPACT STORAGE attribute.

 

I also see that [~ferozshaik552@gmail.com] had seen WARN [ReadStage-4] 2019-08-06 02:57:57,408 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-4,5,main]: {}
 java.lang.NullPointerException: null



What I mainly saw was AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-9,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: null

Does the fix fit the same scenario as Feroz's? Could my issue be thrift related?

 

Sorry again for the late reply.

 ;;;","27/Aug/19 17:40;Sagges;Hi [~benedict] ,

I'm not sure if my comments/tickets are somehow not getting filed, but I hope not :)

Is the above error (previous comment) related to this bug or is it a different one?

 

Thanks!;;;","27/Aug/19 17:50;mck;[~Sagges],

 this bug comes from existing thrift (legacy) tables, where range tombstones were used. 

The NPE [~ferozshaik552@gmail.com] reported is a separate bug, despite it also being coming from legacy thrift tables with range tombstones. Unfortunately though, the fix for this ticket will not solve the NPE bug. ;;;","27/Aug/19 17:56;Sagges;Thanks for clarifying [~mck]!;;;","27/Aug/19 18:47;benedict;Hi [~Sagges], I'm afraid I was on holiday so was unable to respond, and am now otherwise engaged for the next few weeks, but [~mck] is mostly correct.  However, to clarify, the likely cause of this bug that I have established is not related to thrift or legacy tables (though atypical range tombstone use with thrift could cause it), but to communication from a 3.0 node to a 2.2 or 2.1 node, in the face of range tombstones that cover a primary key prefix.

That is to say, a schema of the form (pk, c1, c2, v), with a deletion on (pk, c1);;;","29/Aug/19 12:04;Sagges;Thanks a lot for further clarifying [~benedict]. (I hope you enjoyed your vacation :) )

Just to set my mind straight, the issue is when there are mixed versions in the cluster, so if I upgraded all binaries to 3.11, it won't recur even if I haven't upgraded the SSTables yet. Is my assumption correct?;;;","23/Sep/19 20:23;Sagges;Hi [~benedict]

I tried the patch. It didn't do the trick, however, I was able to fully reproduce the bug.

tl;dr

The bug does occur when running queries on range tombstones, but on top of that, those queries have to specifically be range queries.

 

*+Steps to Reproduce:
+* 

CREATE KEYSPACE ks1 WITH replication = \{'class': 'NetworkTopologyStrategy', 'DC1': '3'} AND durable_writes = true;

+*TABLE:*+ 
CREATE TABLE ks1.table1 (
 col1 text,
 col2 text,
 col3 text,
 col4 text,
 col5 text,
 col6 timestamp,
 data text,
 PRIMARY KEY ((col1, col2, col3), col4, col5, col6)
);

 

Inserted ~4 million rows and created range tombstones by deleting ~1 million rows.

 

+*Create Data*+

_insert into ks1.table1 (col1, col2 , col3 , col4 , col5 , col6 , data ) VALUES ( '1', '11', '21', '1', 'a', 12312312300000, 'data');_
_insert into ks1.table1 (col1, col2 , col3 , col4 , col5 , col6 , data ) VALUES ( '1', '11', '21', '2', 'a', 12312312300000, 'data');_
_insert into ks1.table1 (col1, col2 , col3 , col4 , col5 , col6 , data ) VALUES ( '1', '11', '21', '3', 'a', 12312312300000, 'data');_
_insert into ks1.table1 (col1, col2 , col3 , col4 , col5 , col6 , data ) VALUES ( '1', '11', '21', '4', 'a', 12312312300000, 'data');_
_insert into ks1.table1 (col1, col2 , col3 , col4 , col5 , col6 , data ) VALUES ( '1', '11', '21', '5', 'a', 12312312300000, 'data');_

 

+*Create Range Tombstones*+

delete from ks1.table1 where col1='1' and col2='11' and col3='21' and col4='1';

 

+*Query Live Rows (no tombstones)*+

_select * from ks1.table1 where col1='1' and col2='201' and col3='21' and col4='1' and col5='a' and *col6>12312312300000*;_

No issues found, everything is running properly.

 

+*Query Range Tombstones*+

_select * from ks1.table1 where col1='1' and col2='11' and col3='21' and col4='1' and col5='a' and *col6=12312312300000*;_

No issues found, everything is running properly.

 

+BUT when running range queries:+

_select * from ks1.table1 where col1='1' and col2='11' and col3='21' and col4='1' and col5='a' and *col6>12312312200000;*_

WARN [ReadStage-1] 2019-09-23 14:17:10,281 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-1,5,main]: {}
java.lang.ArrayIndexOutOfBoundsException: 2
 at org.apache.cassandra.db.AbstractBufferClusteringPrefix.get(AbstractBufferClusteringPrefix.java:55)
 at org.apache.cassandra.db.LegacyLayout$LegacyRangeTombstoneList.serializedSizeCompound(LegacyLayout.java:2545)
 at org.apache.cassandra.db.LegacyLayout$LegacyRangeTombstoneList.serializedSize(LegacyLayout.java:2522)
 at org.apache.cassandra.db.LegacyLayout.serializedSizeAsLegacyPartition(LegacyLayout.java:565)
 at org.apache.cassandra.db.ReadResponse$Serializer.serializedSize(ReadResponse.java:446)
 at org.apache.cassandra.db.ReadResponse$Serializer.serializedSize(ReadResponse.java:352)
 at org.apache.cassandra.net.MessageOut.payloadSize(MessageOut.java:171)
 at org.apache.cassandra.net.OutboundTcpConnectionPool.getConnection(OutboundTcpConnectionPool.java:77)
 at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:802)
 at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:953)
 at org.apache.cassandra.net.MessagingService.sendReply(MessagingService.java:929)
 at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:62)
 at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:66)
 at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
 at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
 at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:114)
 at java.lang.Thread.run(Thread.java:745)

 

This WARN is constantly generated until I stop the range queries script.

Hope this helps..

Thanks!

 ;;;","25/Sep/19 08:41;Sagges;For users that happen to stumble upon this use case too, I'd like to emphasize that the bug occurs only during mixed versions.

Once the cluster got fully upgraded (binaries only), the issue was gone.

 ;;;","25/Sep/19 09:19;benedict;Hi [~Sagges], I'm on holiday (again!) but please file a new bug report for this and assign it to me so that I remember when I return, as it is presumably a different (but very similar) bug.;;;","25/Sep/19 16:39;Sagges;Hi [~benedict]

Done and assigned to you! https://issues.apache.org/jira/browse/CASSANDRA-15336

Enjoy your holiday. (the more the merrier) :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SASI does not compare strings correctly,CASSANDRA-15169,13240232,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mazhenlin,mazhenlin,mazhenlin,18/Jun/19 15:58,15/May/20 08:54,13/Jul/23 08:38,17/Oct/19 06:11,3.11.5,4.0,4.0-alpha2,,,Feature/SASI,,,,0,,,,"In our scenario, we need to query with '>' conditions on string columns. So I created index with  is_literal = false. like the following:

 
{code:java}
CREATE TABLE test (id int primary key, t text);

CREATE CUSTOM INDEX ON test (t) USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = {'is_literal': 'false'};
{code}
 I also inserted some records and query:

 
{code:java}
insert into test(id,t) values(1,'abc');
select * from test where t > 'ab';
{code}
At first ,it worked. But after flush, the query returned none record.

I have read the code of SASIIndex and found that it is because in the 
{code:java}
Expression.isLowerSatisfiedBy{code}
function,
{code:java}
term.compareTo{code}
was called with parameter checkFully=false, which cause the string 'abc' was only compared with its first 2 characters( length of expression value).

 

I have wrote a UT for this case and fixed it.",,mazhenlin,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/19 16:00;mazhenlin;CASSANDRA-15169-v1.patch;https://issues.apache.org/jira/secure/attachment/12972113/CASSANDRA-15169-v1.patch","10/Oct/19 15:51;mazhenlin;CASSANDRA-15169-v2.patch;https://issues.apache.org/jira/secure/attachment/12982700/CASSANDRA-15169-v2.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,mazhenlin,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Oct 17 06:11:27 UTC 2019,,,,,,,All,,,,,"0|z03vaw:",9223372036854775807,,,,,,,mck,,,,Normal,,3.4,,,https://github.com/apache/cassandra/commit/b20daee68d96cca7e23f5c2e83f687ba2f3b1852,,,,,,,,,unit test,,,,,"18/Jun/19 16:03;mazhenlin;The isUpperSatisfiedBy function has the same problem. But since column value will be checked later in Operation.localSatisfiedBy , it would not lead to wrong query result. So I did not modify it.

 ;;;","08/Oct/19 14:46;mck;||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[mck/cassandra-3.11_15169|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/cassandra-3.11_15169]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_15169]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/52//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/52/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/687//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/687]|
|[mck/trunk_15169|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_15169]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_15169]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/53//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/53/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/688//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/688]|

;;;","08/Oct/19 15:40;mck;[~mazhenlin], against {{cassandra-3.11}} have you seen the unit test failure in {{OnDiskIndexTest.testNotEqualsQueryForStrings}} ?

ref:
 - https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/52/testReport/junit/org.apache.cassandra.index.sasi.disk/OnDiskIndexTest/testNotEqualsQueryForStrings/
 - https://circleci.com/gh/thelastpickle/cassandra/511#tests/containers/3;;;","09/Oct/19 12:03;mazhenlin;Yes, I have seen the failure . The OnDiskIndexTest.testNotEqualsQueryForStrings test case shows that using '=' for prefix matching seems to be a feature other than a bug .  However,  the inconsistent behavior between MemIndex and OnDiskIndex (when is_literal = true ) still need to be fixed. Comparison for  blob type is also incorrect .

 

We have two choices:

1 Do not use = for prefix match since 'like' is already supported.

2 Use '=' for prefix matching only when is_literal = true.

 

Personally I like 1, but 2 may be  better considering that someone might have already used this feature. Which do you prefer ? [~mck];;;","09/Oct/19 17:07;mck;It makes sense that {{OnDiskIndex}}'s behaviour matches {{TrieMemIndex}}'s ({{is_literal}}) behaviour.
But, if {{checkFully=false}} is also not actually required in {{isUpperSatisfiedBy}}, then why does the `{{Term.compareTo(..,checkFully)}}` method exist at all?;;;","10/Oct/19 10:27;mazhenlin;[~mck] Thank you for your comments. I went deeper and found that I had made some mistakes before.

 

Since isSatisfiedBy will be called for every record in term later, it is acceptable to incorrectly include a term by isUpperSatisfiedBy/isLowerSatisfiedBy, but omitting a term incorrectly is not acceptable.

 

I was wrong about isUpperSatisfiedBy. The checkFully=false is required for the LIKE_PREFIX operator because otherwise term could be omitted by isUpperSatisfiedBy incorrectly. But for the GT operator, checkFully need to be true in isLowerSatisfiedBy and isUpperSatisfiedBy( for ReversedType ). 

 

The OnDiskIndexTest.testNotEqualsQueryForStrings failed because it uses NEQ as ""not like prefix"" semantic.  

 

So I think it should work to set checkFully=true only for RANGE operators and only when is_literal=false( to retain the ""not like prefix"" semantic of NEQ) .;;;","10/Oct/19 15:59;mazhenlin;[~mck] I hava uploaded a new patch that sets checkFully=true only for RANGE operators and only when is_literal=false. Also , new test cases are added for blob comparison and string prefix matching. Please review.;;;","14/Oct/19 20:22;mck;Thanks [~mazhenlin]. I added one test method to your patch, not particularly critical to the fix at hand, but an obvious compliment to the unit testing. LMK WDYT.

||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[mck/cassandra-3.11_15169|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/cassandra-3.11_15169]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_15169]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/55//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/55/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/690//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/690]|
|[mck/trunk_15169|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_15169]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_15169]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/56//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/56/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/691//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/691]|

;;;","15/Oct/19 09:46;mazhenlin;[~mck] , since there are many restrictions in the code for not applying RANGE on literal indexes(e.g. ColumnIndex.supports), I have never considered the operation in your new case. Now if we want to remove these restrictions for PREFIX mode, the followings need to be done:

 

1 The expected results of OnDiskIndexTest.testNotEqualsQueryForStrings need to be changed because it is conflicted between the common '>' semantic and using NEQ as ""not-like-prefix"".

 

2 Make OnDiskIndex behaves correctly  in comparison. This can be done easily.

 

3 Support RANGE for MemIndex. This might be a little complicated. Since the TrieMemIndex does not support RANGE  ,we need to either implement our new radix-tree  class to support RANGE operation , or use SkipListMemIndex for PREFIX mode and treat ""a like b% "" as "" a > b and a < bytes(b)+1 "".

 

Besides,  obviously RANGE operations are meaningful only for untokenized indexes. We need to think about whether these changes are worthwhile, and whether they will confuse users.;;;","15/Oct/19 17:08;mck;[~mazhenlin], if you look through the docs at https://github.com/apache/cassandra/blob/trunk/doc/SASI.md you see a few different use-cases.

I've updated the unit tests to go through more of these use-cases.

bq. there are many restrictions in the code for not applying RANGE on literal indexes(e.g. ColumnIndex.supports)…

thanks for pointing that out. agreed, using ""ALLOW FILTERING"" isn't exactly testing the index, but the at least it's testing that the index doesn't do that query and the results that will be returned otherwise. nonetheless, i've updated the tests (my branches listed above) to verify when the index will work (and won't).


||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[mck/cassandra-3.11_15169|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/cassandra-3.11_15169]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_15169]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/57//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/57/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/692//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/692]|
|[mck/trunk_15169|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_15169]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_15169]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/58//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/58/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/693//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/693]|

;;;","16/Oct/19 16:46;mazhenlin;The failed tests are unrelated. 
{quote} if you look through the docs at [https://github.com/apache/cassandra/blob/trunk/doc/SASI.md] you see a few different use-cases.
{quote}
I am not quite sure what you mean. I have run the demo queries in the doc and found these queries do not work as  described (the ""Text Analysis"" section):
{code:java}
SELECT * FROM sasi WHERE bio LIKE 'they argued';
SELECT * FROM sasi WHERE bio LIKE 'working at the company';
SELECT * FROM sasi WHERE bio LIKE 'soft eng';
{code}
All of them return 0 rows. The same happens at trunk. Maybe we can fix them in a new ticket.;;;","16/Oct/19 17:34;mck;{quote}
bq. if you look through the docs at https://github.com/apache/cassandra/blob/trunk/doc/SASI.md you see a few different use-cases.
I am not quite sure what you mean.{quote}

I was only referring to the fact there was a few more variables that could be considered, and tested, eg the modes: {{PREFIX}}, {{CONTAINS}}, and {{SPARSE}}; as well as the {{is_literal}} option. There's more too, like the tokenisation stuff, but have left that out for now.

bq. I have run the demo queries in the doc and found these queries do not work as described (the ""Text Analysis"" section).

A new ticket indeed, please. This {{SASI.md}} doc page should also be moved into the doc {{source/}} tree so that it becomes part of the docs published on the website (but again a separate ticket).

[~mazhenlin], are you comfortable with the patch now? I will merge it once i get the ok from you.;;;","16/Oct/19 23:03;mazhenlin;Yes , I am ok with that. Thank you again for all the comments . I have learned a lot during the discussion.;;;","17/Oct/19 06:01;mck;bq.  I have learned a lot during the discussion.

Same [~mazhenlin]. Thanks for finding and fixing the bug.;;;","17/Oct/19 06:11;mck;Committed as b20daee68d96cca7e23f5c2e83f687ba2f3b1852;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reference-Reaper detected leak while running FramingTest unit test cases,CASSANDRA-15165,13239894,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,n.v.harikrishna,n.v.harikrishna,n.v.harikrishna,17/Jun/19 09:41,15/May/20 08:00,13/Jul/23 08:38,18/Jun/19 10:16,4.0,4.0-alpha1,,,,Test/unit,,,,0,,,,"Reference-Reaper detected leak while running FramingTest unit test cases. Here are the leak details:

{code}
[junit-timeout] ERROR [Reference-Reaper] 2019-06-17 01:44:53,812 Ref.java:228 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@15460327) to @876994034 was not released before the reference was garbage collected
[junit-timeout] ERROR [Reference-Reaper] 2019-06-17 01:44:53,812 Ref.java:259 - Allocate trace org.apache.cassandra.utils.concurrent.Ref$State@15460327:
[junit-timeout] Thread[main,5,main]
[junit-timeout] 	at java.lang.Thread.getStackTrace(Thread.java:1559)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.Ref$Debug.<init>(Ref.java:249)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.Ref$State.<init>(Ref.java:179)
[junit-timeout] 	at org.apache.cassandra.utils.concurrent.Ref.<init>(Ref.java:101)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$Chunk.setAttachment(BufferPool.java:960)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$Chunk.set(BufferPool.java:1100)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$Chunk.get(BufferPool.java:1090)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGetInternal(BufferPool.java:721)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.tryGet(BufferPool.java:706)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.get(BufferPool.java:656)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool$LocalPool.access$000(BufferPool.java:535)
[junit-timeout] 	at org.apache.cassandra.utils.memory.BufferPool.getAtLeast(BufferPool.java:129)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.sequenceOfMessages(FramingTest.java:413)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.testRandomSequenceOfMessages(FramingTest.java:265)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.testSomeMessages(FramingTest.java:259)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.testRandomLegacy(FramingTest.java:243)
[junit-timeout] 	at org.apache.cassandra.net.FramingTest.testRandomLegacy(FramingTest.java:234)
[junit-timeout] 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[junit-timeout] 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
[junit-timeout] 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[junit-timeout] 	at java.lang.reflect.Method.invoke(Method.java:498)
[junit-timeout] 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
[junit-timeout] 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
[junit-timeout] 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
[junit-timeout] 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
[junit-timeout] 	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
[junit-timeout] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
[junit-timeout] 	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
[junit-timeout] 	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
[junit-timeout] 	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
[junit-timeout] 	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
[junit-timeout] 	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
[junit-timeout] 	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
[junit-timeout] 	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
[junit-timeout] 	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
[junit-timeout] 	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
[junit-timeout] 	at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:38)
[junit-timeout] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:534)
[junit-timeout] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:1196)
[junit-timeout] 	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:1041)
{code}
“sliceAndConsume"" method of ShareableBytes increases reference count. Reference acquired testRandomSequenceOfMessages method is not released which is causing the leak.",,aleksey,jjirsa,n.v.harikrishna,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,n.v.harikrishna,,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Normal,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue Jun 18 10:13:57 UTC 2019,,,,,,,All,,,,,"0|z03t8g:",9223372036854775807,,,,,,,aleksey,,,,Low,,4.0,,,"[abb0e17785b50baee6e53ee02bd367a5ce9455f8|https://github.com/apache/cassandra/commit/abb0e17785b50baee6e53ee02bd367a5ce9455f8]",,,,,,,,,Is a test fix.,,,,,"17/Jun/19 20:06;n.v.harikrishna;Here are the patch details:

[15165-trunk|https://github.com/nvharikrishna/cassandra/tree/15165-trunk] [CircleCi|https://circleci.com/gh/nvharikrishna/cassandra/18]

 ;;;","18/Jun/19 10:13;aleksey;Thanks. I opted to retain the current constructor behaviour, as {{UNSHARED}} isn't necessarily a meaningful value to use here, if we slice again.

Did take in the visibility change, though, and also made the other constructor private, to leave {{wrap()}} as the only way to make an instance.

Committed to trunk as [abb0e17785b50baee6e53ee02bd367a5ce9455f8|https://github.com/apache/cassandra/commit/abb0e17785b50baee6e53ee02bd367a5ce9455f8].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Overflowed Partition Cell Histograms Can Prevent Compactions from Executing,CASSANDRA-15164,13239882,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,maedhroz,ajha,ajha,17/Jun/19 08:47,16/Mar/22 14:17,13/Jul/23 08:38,24/Sep/20 18:08,3.0.23,3.11.9,4.0,4.0-beta3,,CQL/Interpreter,,,,0,compaction,partition,,"Hi, we are running 6 node Cassandra cluster in production with 3 seed node but from last night one of our seed nodes is continuously throwing an error like this;-

cassandra.protocol.ServerError: <Error from server: code=0000 [Server error] message=""java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed"">

For a cluster to be up and running I Drained this node.

Can somebody help me out with this?

 

Any help or lead would be appreciated 

 

Note : We are using Cassandra version 3.7",,ajha,blerer,clohfink,e.dimitrova,jmeredithco,maedhroz,,,,,,,,,,,,,,,,,,,,,,"maedhroz opened a new pull request #750:
URL: https://github.com/apache/cassandra/pull/750


   ...and provide a conservative estimate of the droppable tombstone ratio when a deserialized EstimatedHistogram cannot provide one


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Sep/20 21:15;githubbot;600","maedhroz commented on a change in pull request #750:
URL: https://github.com/apache/cassandra/pull/750#discussion_r487294576



##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Sep/20 21:16;githubbot;600","maedhroz commented on a change in pull request #750:
URL: https://github.com/apache/cassandra/pull/750#discussion_r487294576



##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.

##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Sep/20 20:30;githubbot;600","maedhroz commented on a change in pull request #750:
URL: https://github.com/apache/cassandra/pull/750#discussion_r487294576



##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.

##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.

##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.

##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.

##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.

##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.

##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       I think we should have at least some context for what SSTable a reader is actually being built for to go along with this message.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Sep/20 21:44;githubbot;600","maedhroz opened a new pull request #750:
URL: https://github.com/apache/cassandra/pull/750






----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;12/Sep/20 21:49;githubbot;600","maedhroz commented on a change in pull request #750:
URL: https://github.com/apache/cassandra/pull/750#discussion_r487294576



##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       Existing logging should have at least some context for what SSTable a reader is actually being built for to go along with this message, but `ISerializer` isn't really built around the notion that there are problems we might want to report that don't simply fail serialization/deserialization.




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;14/Sep/20 15:51;githubbot;600","clohfink commented on a change in pull request #750:
URL: https://github.com/apache/cassandra/pull/750#discussion_r492825012



##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       the thread name would give a decent amount of context, but you could dump whole stack trace with message too I suppose but I think its sufficient like this




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Sep/20 15:34;githubbot;600","maedhroz commented on pull request #750:
URL: https://github.com/apache/cassandra/pull/750#issuecomment-696841583


   > when that metric in the sstable is overflowed it will cause errors throughout the rest of the codebase that isn't caught
   
   Yeah, there are a couple other places that might still explode, like GROUP BY queries and range reads if we allow an overflowed histogram. Short of dynamically resizing the number of buckets, I'd propose that for now we sanitize the two estimate histograms in `StatsMetadataSerializer`, so we can at least differentiate the WARN messages in the log.
   
   CC @blerer 


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Sep/20 16:42;githubbot;600","blerer commented on a change in pull request #750:
URL: https://github.com/apache/cassandra/pull/750#discussion_r492875485



##########
File path: src/java/org/apache/cassandra/io/sstable/metadata/StatsMetadata.java
##########
@@ -124,6 +124,16 @@ public MetadataType getType()
      */
     public double getEstimatedDroppableTombstoneRatio(int gcBefore)
     {
+        if (this.estimatedCellPerPartitionCount.isOverflowed())
+        {
+            // If the histogram contains even one value too large for the largest offset, there is no reliable way to
+            // compute an average number of cells per partition. Given this is the case, we conservatively assume a
+            // ratio of zero. This means that the SSTable will participate in normal compactions but will likely not
+            // be compacted in an attempt to physicaly purge tombstones. If we made the opposite assumption, we could
+            // easily compact SSTables that actually have a very low ratio in reality for no reason.
+            return 0.0f;

Review comment:
       Nit: the return type is `double` we should return `0.0d` 

##########
File path: test/unit/org/apache/cassandra/io/sstable/metadata/StatsMetadataTest.java
##########
@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.sstable.metadata;
+
+import org.junit.Test;
+
+import org.apache.cassandra.utils.EstimatedHistogram;
+
+public class StatsMetadataTest
+{
+    @Test
+    public void shouldHandleUnusablePartitionCellHistogram()
+    {
+        EstimatedHistogram histogram = new EstimatedHistogram(3);
+        histogram.add(10);
+        StatsMetadata metadata = new StatsMetadata(null, histogram, null, -1L, -1L, -1, -1, -1, -1, 0.0f, null, -1, 
+                                                   null, null, false, -1L, -1L, -1L, null, false);
+        metadata.getEstimatedDroppableTombstoneRatio(0);

Review comment:
       Nit: I would assert that the `histogram.isOverflowed` is `true` and check that the returned value of `getEstimatedDroppableTombstoneRatio` is the expected one to make the test a bit more robuste. 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Sep/20 16:45;githubbot;600","maedhroz commented on pull request #750:
URL: https://github.com/apache/cassandra/pull/750#issuecomment-696943269


   Tests in progress: https://app.circleci.com/pipelines/github/maedhroz/cassandra?branch=CASSANDRA-15164


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Sep/20 19:51;githubbot;600","clohfink commented on a change in pull request #750:
URL: https://github.com/apache/cassandra/pull/750#discussion_r492999030



##########
File path: src/java/org/apache/cassandra/io/sstable/metadata/StatsMetadata.java
##########
@@ -340,7 +345,27 @@ public void serialize(Version version, StatsMetadata component, DataOutputPlus o
         public StatsMetadata deserialize(Version version, DataInputPlus in) throws IOException
         {
             EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
+            
+            if (partitionSizes.isOverflowed())
+            {
+                logger.warn(""Deserialized partition size histogram with {} values greater than the maximum of {}. "" +

Review comment:
       would this be candidate for no spam logger? I think it will mainly be on open() but might be worth considering (ok to ignore this)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Sep/20 20:03;githubbot;600","clohfink commented on a change in pull request #750:
URL: https://github.com/apache/cassandra/pull/750#discussion_r492825012



##########
File path: src/java/org/apache/cassandra/utils/EstimatedHistogram.java
##########
@@ -390,7 +407,15 @@ public EstimatedHistogram deserialize(DataInputPlus in) throws IOException
                 offsets[i == 0 ? 0 : i - 1] = in.readLong();
                 buckets[i] = in.readLong();
             }
-            return new EstimatedHistogram(offsets, buckets);
+            EstimatedHistogram histogram = new EstimatedHistogram(offsets, buckets);
+
+            if (histogram.isOverflowed())
+            {
+                logger.warn(""Deserialized a histogram with {} values greater than the maximum of {}."",
+                            histogram.overflowCount(), histogram.getLargestBucketOffset());

Review comment:
       the thread name would give a decent amount of context, but you could dump whole stack trace with message too I suppose but I think its sufficient like this

##########
File path: src/java/org/apache/cassandra/io/sstable/metadata/StatsMetadata.java
##########
@@ -340,7 +345,27 @@ public void serialize(Version version, StatsMetadata component, DataOutputPlus o
         public StatsMetadata deserialize(Version version, DataInputPlus in) throws IOException
         {
             EstimatedHistogram partitionSizes = EstimatedHistogram.serializer.deserialize(in);
+            
+            if (partitionSizes.isOverflowed())
+            {
+                logger.warn(""Deserialized partition size histogram with {} values greater than the maximum of {}. "" +

Review comment:
       would this be candidate for no spam logger? I think it will mainly be on open() but might be worth considering (ok to ignore this)




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Sep/20 04:31;githubbot;600","blerer commented on a change in pull request #750:
URL: https://github.com/apache/cassandra/pull/750#discussion_r492875485



##########
File path: src/java/org/apache/cassandra/io/sstable/metadata/StatsMetadata.java
##########
@@ -124,6 +124,16 @@ public MetadataType getType()
      */
     public double getEstimatedDroppableTombstoneRatio(int gcBefore)
     {
+        if (this.estimatedCellPerPartitionCount.isOverflowed())
+        {
+            // If the histogram contains even one value too large for the largest offset, there is no reliable way to
+            // compute an average number of cells per partition. Given this is the case, we conservatively assume a
+            // ratio of zero. This means that the SSTable will participate in normal compactions but will likely not
+            // be compacted in an attempt to physicaly purge tombstones. If we made the opposite assumption, we could
+            // easily compact SSTables that actually have a very low ratio in reality for no reason.
+            return 0.0f;

Review comment:
       Nit: the return type is `double` we should return `0.0d` 

##########
File path: test/unit/org/apache/cassandra/io/sstable/metadata/StatsMetadataTest.java
##########
@@ -0,0 +1,36 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * ""License""); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an ""AS IS"" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io.sstable.metadata;
+
+import org.junit.Test;
+
+import org.apache.cassandra.utils.EstimatedHistogram;
+
+public class StatsMetadataTest
+{
+    @Test
+    public void shouldHandleUnusablePartitionCellHistogram()
+    {
+        EstimatedHistogram histogram = new EstimatedHistogram(3);
+        histogram.add(10);
+        StatsMetadata metadata = new StatsMetadata(null, histogram, null, -1L, -1L, -1, -1, -1, -1, 0.0f, null, -1, 
+                                                   null, null, false, -1L, -1L, -1L, null, false);
+        metadata.getEstimatedDroppableTombstoneRatio(0);

Review comment:
       Nit: I would assert that the `histogram.isOverflowed` is `true` and check that the returned value of `getEstimatedDroppableTombstoneRatio` is the expected one to make the test a bit more robuste. 




----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Sep/20 04:38;githubbot;600","maedhroz commented on pull request #750:
URL: https://github.com/apache/cassandra/pull/750#issuecomment-696841583






----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Sep/20 04:46;githubbot;600","smiklosovic closed pull request #750:
URL: https://github.com/apache/cassandra/pull/750


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 14:17;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,0,9000,,,0,9000,,,,,,CASSANDRA-15325,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,maedhroz,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Challenging,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Sep 24 17:58:56 UTC 2020,,,,,,,All,,,,,"0|z03t5s:",9223372036854775807,,,,,,,blerer,clohfink,,,Critical,,3.0.0,,,https://github.com/apache/cassandra/commit/4782fd399d97be551032576c4d77f079e862fdba,,,,,,,,,new tests to verify EstimatedHistogram overflow clearing and overflowed histograms instats deserialization,,,,,"11/Sep/20 00:46;maedhroz;Let's revisit the stack trace from CASSANDRA-15326, which is almost certainly the same here:

{noformat}
Exception in thread Thread[CompactionExecutor:113041,1,main] 
java.lang.IllegalStateException: Unable to compute ceiling for max when histogram overflowed
at org.apache.cassandra.utils.EstimatedHistogram.rawMean(EstimatedHistogram.java:231) ~[apache-cassandra-3.11.4.jar:3.11.4]
at org.apache.cassandra.utils.EstimatedHistogram.mean(EstimatedHistogram.java:220) ~[apache-cassandra-3.11.4.jar:3.11.4]
at org.apache.cassandra.io.sstable.metadata.StatsMetadata.getEstimatedDroppableTombstoneRatio(StatsMetadata.java:115) ~[apache-cassandra-3.11.4.jar:3.11.4]
at org.apache.cassandra.io.sstable.format.SSTableReader.getEstimatedDroppableTombstoneRatio(SSTableReader.java:1926) ~[apache-cassandra-3.11.4.jar:3.11.4]
at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:424) ~[apache-cassandra-3.11.4.jar:3.11.4]
at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:99) ~[apache-cassandra-3.11.4.jar:3.11.4]
at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:183) ~[apache-cassandra-3.11.4.jar:3.11.4]
at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:153) ~[apache-cassandra-3.11.4.jar:3.11.4]
{noformat}

All our compaction strategies, at some point, want to know what the ratio of droppable tombstones to cells is on average for the partitions in an SSTable. However, if we ever have more than about 1.9 billion cells in a partition, the {{EstimatedHistogram}} that tracks this will overflow. Then, when compaction attempts to get the mean number of cells per partition {{EstimatedHistogram}} throws an {{IllegalStateException}} that aborts the attempt at compaction. This can continue indefinitely.

In C* 4.0, full checksum validation for metadata components exists, but it's also possible that, in previous versions, the serialization/deserialization cycle for {{EstimatedHistogram}} could introduce corruption that breaks the mean calculation.;;;","11/Sep/20 21:20;maedhroz;I've taken a stab at both providing better visibility around overflowed histograms and taking reasonable action to prevent compaction failures when they are present. There are reasonable inline comments and a simple new test.

[trunk|https://github.com/apache/cassandra/pull/750], [j8 tests|https://app.circleci.com/pipelines/github/maedhroz/cassandra/112/workflows/ab37a692-bb8c-40f7-a4f6-d406ddf9f6d5], [j11 tests|https://app.circleci.com/pipelines/github/maedhroz/cassandra/112/workflows/69c9a0ca-f627-4238-b94b-da182f86641b];;;","14/Sep/20 16:15;maedhroz;This also should apply pretty cleanly to 3.0 and 3.11.;;;","22/Sep/20 19:54;maedhroz;[~blerer] [~clohfink] I've reworked the patch a bit to handle both of the stats histograms and take into account the feedback [here|https://github.com/apache/cassandra/pull/750#pullrequestreview-493580468]. Short of dynamically growing the number of buckets, this seems like the most reasonable quick fix to make the things that might depend on an overflowed histogram safe. (The worst case is that our estimates won't be as helpful as they would if we had more buckets, etc.)

Tests in progress [here|https://app.circleci.com/pipelines/github/maedhroz/cassandra?branch=CASSANDRA-15164]...;;;","22/Sep/20 20:33;maedhroz;The tests look pretty clean, outside hitting CASSANDRA-15313 again.;;;","23/Sep/20 16:35;blerer;After thinking about that problem overnight, I realized that a corruption might not necessary be the problem. I had a quick chat with [~slebresne] who believe that it should be possible to create a partitions with more than 2 billions cells.

By consequence, I think that we should change the code to ensure that we do not overflow if a partition has more than 2 billion cells. For that we can either increase the default number of buckets (using {{118}} instead of {{114}} will allow for more than 4 billions cells) or dynamically increasing the number of buckets if needed.

Otherwise, the approach to clear the overflow seems reasonable to me as it will solve the problem of the already corrupted {{Statistics}}. 

wdyt?


 


 

;;;","23/Sep/20 16:59;maedhroz;[~blerer] As long as [~clohfink] doesn't have any objections, I think raising the number of buckets to 118 should be a pretty innocuous change, and we already encode the length, so it won't break anything (...and I'll keep the rest of the latest version of the patch in place to catch existing corruption).;;;","23/Sep/20 17:09;jmeredithco;I'd support increasing the number of buckets and keeping the changes to continue to operate if there is an overflow.  Looks like the histogram #buckets is written by the serializer so no compatibility concerns with the file formats.;;;","23/Sep/20 17:34;maedhroz;Updated version of the patch: [trunk|https://github.com/apache/cassandra/pull/750], [j8 tests|https://app.circleci.com/pipelines/github/maedhroz/cassandra/120/workflows/da3718e8-6cb0-4727-b89d-2c17e14e347f], [j11 tests|https://app.circleci.com/pipelines/github/maedhroz/cassandra/120/workflows/03a81457-1d8a-47a2-a169-779a346053dc]

UPDATE: The only test failures I see are CASSANDRA-15958, CASSANDRA-15892, and CASSANDRA-15997.;;;","23/Sep/20 21:39;clohfink;Committed, thanks;;;","24/Sep/20 08:19;blerer;We should also commit that patch into 3.0 and 3.11.;;;","24/Sep/20 17:58;clohfink;Its been merged back into 3.0 and 3.11 now;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wait for schema agreement rather than in flight schema requests when bootstrapping,CASSANDRA-15158,13239212,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,VincentWhite,VincentWhite,13/Jun/19 09:18,21/Oct/22 14:03,13/Jul/23 08:38,09/Nov/20 20:26,3.0.24,3.11.10,4.0,4.0-beta4,,Cluster/Gossip,Cluster/Schema,,,1,,,,"Currently when a node is bootstrapping we use a set of latches (org.apache.cassandra.service.MigrationTask#inflightTasks) to keep track of in-flight schema pull requests, and we don't proceed with bootstrapping/stream until all the latches are released (or we timeout waiting for each one). One issue with this is that if we have a large schema, or the retrieval of the schema from the other nodes was unexpectedly slow then we have no explicit check in place to ensure we have actually received a schema before we proceed.

While it's possible to increase ""migration_task_wait_in_seconds"" to force the node to wait on each latche longer, there are cases where this doesn't help because the callbacks for the schema pull requests have expired off the messaging service's callback map (org.apache.cassandra.net.MessagingService#callbacks) after request_timeout_in_ms (default 10 seconds) before the other nodes were able to respond to the new node.

This patch checks for schema agreement between the bootstrapping node and the rest of the live nodes before proceeding with bootstrapping. It also adds a check to prevent the new node from flooding existing nodes with simultaneous schema pull requests as can happen in large clusters.

Removing the latch system should also prevent new nodes in large clusters getting stuck for extended amounts of time as they wait `migration_task_wait_in_seconds` on each of the latches left orphaned by the timed out callbacks.

 
||3.11||
|[PoC|https://github.com/apache/cassandra/compare/cassandra-3.11...vincewhite:check_for_schema]|
|[dtest|https://github.com/apache/cassandra-dtest/compare/master...vincewhite:wait_for_schema_agreement]|

 ",,aleksey,bdeggleston,dcapwell,e.dimitrova,icleasby,jasonstack,jay.zhuang,jeromatron,maedhroz,mbyrd,mck,molsson,stefan.miklosovic,tania.engel@quest.com,VincentWhite,,,,,,,,,,,,,"smiklosovic opened a new pull request #628:
URL: https://github.com/apache/cassandra/pull/628


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;11/Jun/20 21:58;githubbot;600","smiklosovic closed pull request #628:
URL: https://github.com/apache/cassandra/pull/628


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Nov/20 21:09;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,CASSANDRA-17972,CASSANDRA-11748,,,,,,,,,,,CASSANDRA-16732,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,stefan.miklosovic,,,,,,,,,,,Correctness -> Consistency,,,,,,,,Challenging,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Nov 13 20:39:30 UTC 2020,,,,,,,All,,,,,"0|z03p14:",9223372036854775807,,,,,,,aleksey,bdeggleston,,,Normal,,3.0.0,,,https://github.com/apache/cassandra/commit/08450080614250a8bfaba23dbca741a4d9315e3c,,,,,,,,,There are unit tests as part of PR testing this feature.,,,,,"23/Apr/20 20:27;bdeggleston;So just skimming this, it looks like it's the right approach. We should add some tests though, think through what we want to do if we can't get the schema to converge, as well as leave an ""escape hatch"" if we need to start up and are willing to skip waiting on schema agreement.;;;","23/Apr/20 20:30;bdeggleston;I also wanted to point out that the underlying issues causing this problem can lead to correctness issues or data loss in some scenarios. Since we don't currently confirm that any of the in flight migration tasks have been completed and applied, it's possible for us to not receive _any_ schema responses, and begin bootstrapping without a schema. In this case, bootstrap would complete immediately, because the node believes there is nothing to stream. When the node later receives the schema, it will begin serving reads and writes with no data. ;;;","29/Apr/20 16:29;stefan.miklosovic;Hi [~bdeggleston],

please review this one [https://github.com/smiklosovic/cassandra/tree/CASSANDRA-15158]

Sorry for the fuss but this seems to be more problematic than I was thinking initially. 

I do not think that we have to _check_ that respective migration request is successful or not, we should just check it all schemas match ...

The logic is based on repeated checking if schemas agree or not and only in case all schemas are equal we proceed, otherwise an exception is thrown (this might be reworked in such sense that we just log this). There is a global timeout for this check, it will be obvious from reading the code.

There is a test added too, because of the nature of the test, I had to ""inject"" these callbacks into respective methods so I could modify their default behaviour and test their state. 

This is against 3.11, we need to backport this to 3.0 for our customer.;;;","29/Apr/20 16:38;bdeggleston;Thanks [~stefan.miklosovic], I'm _should_ have some time to review next week.;;;","04/May/20 13:54;stefan.miklosovic;code for 3.0 [https://github.com/smiklosovic/cassandra/tree/CASSANDRA-15158-cassandra-3];;;","07/May/20 06:28;stefan.miklosovic;for 3.11

 

[https://github.com/smiklosovic/cassandra/tree/CASSANDRA-15158]

 

for 3.0

 

https://github.com/smiklosovic/cassandra/tree/CASSANDRA-15158-cassandra-3;;;","07/May/20 21:17;bdeggleston;It's likely we'll want to fix this in 3.0 and up, so I'll review the 3.0 version to start with and we can go from there.

I haven't completed my review yet, but there are some structural and design issues we should address up front.

*Structural issues*

First, the {{waitForSchema*}} methods should live in the MigrationManager. This will prevent you from setting an updated status if you're sending out additional schema pulls, but we can revisit that later if we think it's neccesary.

Second, instantiating MigrationTaskCallbacks in StorageService/MigrationManager and passing it into MigrationTask is a little awkward. I'd prefer if the callback remained an anonymous class. We can communicate endpoint to send schema pulls to with an inetaddress argument, and we need to rethink what `isRunningForcibly` is doing and why. First, we shouldn't be adding it to {{IAsyncCallback}} for this narrow use case. Next, it's use seems to be changing how schema pulls actually work, but only in a test environment, which is something we should avoid.

*Design issues*

This doesn't deal with multiple schema versions. If a node joins, and there are 2 or more schema versions floating around, it will only wait until it has _some_ schema to begin bootstrapping, not all. Related to this, we also need a plan for unreachable schema versions. For instance, if a single node is reporting a schema version that no one else has, but the node is unreachable, what do we do?

Next, I like how this limits the number of messages sent to a given endpoint, but we should also limit the number of messages we send out for a given schema version. If we have a large cluster, and all nodes are reporting the same version, we don't need to ask every node for it's schema.;;;","08/May/20 08:32;stefan.miklosovic;Hi [~bdeggleston],

commenting on design issues, I am not completely sure if these issues you are talking about are related to this patch or they are already existing? We could indeed focus on the points you raised but it seems to me that the current (comitted) code is worse without this patch than with as I guess these problems are already there?

Isn't the goal here to have all nodes on same versions? Isn't the very fact that there are multiple versions pretty strange to begin with so we should not even try to join a node if they mismatch hence there is nothing to deal with in the first place? 
{quote}It will only wait until it has _some_ schema to begin bootstrapping, not all
{quote}
This is the most likely not true unless I am not getting something. The node to be bootstrapped will never advance in doing so unless all nodes have same versions. 
{quote} For instance, if a single node is reporting a schema version that no one else has, but the node is unreachable, what do we do?
{quote}
We should fail whole bootstrapping and one should go and fix it.
{quote}For instance, if a single node is reporting a schema version that no one else has, but the node is unreachable, what do we do?
{quote}
How can a node report its schema while being unreachable?
{quote}Next, I like how this limits the number of messages sent to a given endpoint, but we should also limit the number of messages we send out for a given schema version. If we have a large cluster, and all nodes are reporting the same version, we don't need to ask every node for it's schema.
{quote}
 

Got you, this might be tracked.

 

When it comes to testing, I admit that adding isRunningForcibly method feels like a hack but I had very hard time to test this stuff out. It was basically the only reasonable way possible at the time I was coding it, if you know of more better version, please tell me otherwise I am not sure what might be better here and we could stick with this for a time being? The whole testing methodology was based on these callbacks and checking their inner state which results into having a methods which are accepting them so we can elaborate on their state. Without ""injecting"" them from outside, I would not be able to do that.;;;","08/May/20 12:48;stefan.miklosovic;It seems to me that one aspect of the PR was overlooked so I just iterate on that one. The mechanim how to not flood nodes with schema pull messages is incorporated in the loop over callbacks. If you notice it, there are sleeps of various lenghts based on a request being already sent or not. This sleep will actually ""delay"" the next schema pull from the other node because during this time of a sleep, some schema could come from the node we just sent a message to so on the next iteration when another node is compared on schema equality, it may happen that there is not any need to pull it anymore because they are on par. Hence we are not blindly sending messages to all nodes.
 If there are some discrepancies, there is the global timeout set after which whole bootstrapping process will be evaluated as errorneous and (in the current code) we throw a ConfigurationException. This behaviour might be relaxed but I consider it more appropriate to just throw it there.;;;","08/May/20 17:03;bdeggleston;{quote}commenting on design issues, I am not completely sure if these issues you are talking about are related to this patch or they are already existing? We could indeed focus on the points you raised but it seems to me that the current (comitted) code is worse without this patch than with as I guess these problems are already there?
Isn't the goal here to have all nodes on same versions? Isn't the very fact that there are multiple versions pretty strange to begin with so we should not even try to join a node if they mismatch hence there is nothing to deal with in the first place?
{quote}
When there are schema changes, it's not strange at all for there to be multiple schema versions in the cluster before they converge. We also don't forbid making schema changes while changing cluster topology, so this would be something we should expect to encounter, although I would expect it to happen infrequently. Since bootstrap doesn't stream keyspaces it doesn't know about, this could create a window of data loss. Since the goal of this ticket is to wait for schema to converge before starting bootstrap, we should deal with edge cases like this. Also, I believe there have been bugs that caused a lot of schema change activity when nodes bootstrap, so depending on what exactly you're doing
{quote}How can a node report its schema while being unreachable?
{quote}
Schema versions are gossiped. So a node might gossip a new schema version then become unreachable. The bootstrapping node would learn about this new version via gossip, but be unable to contact it.
{quote}> admit that adding isRunningForcibly method feels like a hack but I had very hard time to test this stuff out.
{quote}
I'll look into how testing can be improved.
{quote}> This is the most likely not true unless I am not getting something. The node to be bootstrapped will never advance in doing so unless all nodes have same versions.
{quote}
Ah, yes you're right. Althought waiting for all nodes to arrive at the same schema version isn't neccesary, we just need to receive and merge at least one schema pull from every schema version in the cluster.;;;","11/Jun/20 21:44;stefan.miklosovic;Hi Blake,

 

because of your very helpful explanation I was able to put together yet another version of the solution to this problem. You will find it here

[https://github.com/apache/cassandra/pull/628]

Thanks for the review in advance;;;","22/Jul/20 19:07;bdeggleston;I've reworked this a bit more [here|https://github.com/bdeggleston/cassandra/tree/15158-coordinator]. It's now pretty self contained, has some fairly granular unit tests, and fixes a few functional things. Can you take a look [~stefan.miklosovic] and let me know what you think? [~aleksey], can you also take a look / review? The basic idea is that we now track which schema versions exist in gossip, and which endpoints are reporting them, then block bootstrap until we've received a schema for each version. It also tracks how many outstanding migration requests we have per version so we don't send out thousands.

Also, what are your opinions of this going into 3.x? I think I'd lean towards putting it in, since it eliminates a scenario where data loss could occur and will shave a few hours off of adding/replacing nodes in large clusters. On the other hand, since it reduces the amount of migration requests sent out on bootstrap, any bugs determining if we've received sufficient schema data could make data loss _more_ likely.;;;","27/Jul/20 11:00;stefan.miklosovic;In general looks good to me in spite of getting lost a bit on the actual schema migration response:

 

 
{code:java}
Future<Void> response(Collection<Mutation> mutations)
{
    synchronized (info)
    {
        if (shouldApplySchemaFrom(endpoint, info))
            mergeSchemaFrom(endpoint, mutations);
        return pullComplete(endpoint, info, true);  /// why?
    }
}
{code}
 

I am not completely sure why are we pulling again here. I would rewrite the whole solution in a such way that this Callable just does one thing on a successful response (merging of a schema) and the actual ""retry"" would be handled from outside. The reader has to make quite a mental exercise to visualise that this callback might actually call another callback in it until some ""version"" is completed etc ... At least for me, it was quite tedious to track.

I understand the motivation behind that but callback should just do one task and thats it, it shouldnt be responsible for potentially scheduling another callback recursively until some conditions on some signals or what have you are met ... just my 2 cents.

There is also this:

 
{code:java}
synchronized Future<Void> reportEndpointVersion(InetAddress endpoint, UUID version)
{
    UUID current = endpointVersions.get(endpoint);
    if (current != null && current.equals(version))
        return FINISHED_FUTURE;

    VersionInfo info = versionInfo.computeIfAbsent(version, VersionInfo::new);
    if (isLocalVersion(version))
        info.markReceived();
    info.endpoints.add(endpoint);
    info.requestQueue.add(endpoint);
    endpointVersions.put(endpoint, version);

    removeEndpointFromVersion(endpoint, current); ///// why?
    return maybePullSchema(info);
}
{code}
 

TBH that is quite counterintuitive too, maybe renaming of that method would help.

 

The test has failed for me (repeatedly):

 

 
{code:java}
java.lang.AssertionError: java.lang.AssertionError: 
Expected :2
Actual   :0
<Click to see difference> at org.junit.Assert.fail(Assert.java:92) at org.junit.Assert.failNotEquals(Assert.java:689) at org.junit.Assert.assertEquals(Assert.java:127) at org.junit.Assert.assertEquals(Assert.java:514) at org.junit.Assert.assertEquals(Assert.java:498) at org.apache.cassandra.service.MigrationCoordinatorTest.testWeKeepSendingRequests(MigrationCoordinatorTest.java:278) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:44) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:180) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:41) at org.junit.runners.ParentRunner$1.evaluate(ParentRunner.java:173) at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28) at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31) at org.junit.runners.ParentRunner.run(ParentRunner.java:220) at org.junit.runner.JUnitCore.run(JUnitCore.java:159) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33) at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230) at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
{code}
 ;;;","31/Jul/20 19:28;stefan.miklosovic;hi [~bdeggleston], have you had a chance to reflect the above issues?;;;","04/Aug/20 21:29;bdeggleston;{quote}
I am not completely sure why are we pulling again here. I would rewrite the whole solution in a such way that this Callable just does one thing on a successful response (merging of a schema) and the actual ""retry"" would be handled from outside. The reader has to make quite a mental exercise to visualise that this callback might actually call another callback in it until some ""version"" is completed etc ... At least for me, it was quite tedious to track.


{quote}
In the case of a successful pull, we won't pull again. Response and fail both call pullComplete, but an additional pull is only called if it's called from fail.

I get that this can be a bit difficult to follow, but I'm not sure there's a better approach, given the schema pulls are completely event driven during normal runtime. If we miss a schema change during normal runtime (not bootstrap), there's nothing waiting on schema convergence that would enable us to retry from the outside.

There is a periodic task that pulls schema for outstanding versions that don't have any in flight requests^[1]^, but it only runs once a minute, and we need to be more proactive about learning about schema updates since we'll be unable to serve some reads and writes until we're up to date.
{quote}TBH that is quite counterintuitive too
{quote}
Could you expand on what's counterintuitive about it? If the endpoint's schema version has changed, we need to disassociate it with it's previously reported version. I have added a comment saying as much.
{quote}The test has failed for me (repeatedly):
{quote}
Thanks, it should be passing now.

[1] This handles the case where all nodes reporting a given version are on a different version so we can't pull schema from them, and acts as a hedge against any bugs in this implementation that might cause us to not schedule schema pulls as intended;;;","06/Aug/20 11:32;stefan.miklosovic;[~bdeggleston] Thanks for the explanation. Maybe [~aleksey] might take a look before moving this forward?

 ;;;","01/Sep/20 15:48;aleksey;Pushed some minor tweaks [here|https://github.com/iamaleksey/cassandra/commits/15158-review]. Made some bits more idiomatic, and changed the way in-flight requests are being kept track of.

In general, this does the job and solves the problem in the description. It doesn't, however, fully deal with storms in large clusters caused by a sequence of updates in quick succession, but, it's not intended to, either.

EDIT: the amount of synchronisation here bothers me a tiny bit, as all of it will likely have to be eventually gotten rid of, when and if TPC happens, but I can live with it.;;;","02/Sep/20 16:32;bdeggleston;Thanks Aleksey.

Removing requestQueue is going to cause some problems, We need to have some way of cycling requests through the entire set of endpoints reporting a given version. Without one, if the first few endpoints to come out of the iterator are down, we'll get stuck in a loop as we schedule pulls which will immediately fail causing new pulls to be scheduled for the same node.

I've updated my branch with your changes and added the request queue back. Since the storage service doesn't really need to fire off migration requests, I also added a commit making the MigrationCoordinator an endpoint change subscriber. So StorageService is responsible for a little less.;;;","02/Sep/20 18:09;aleksey;Oh, I brainfarted that node liveness check was a part of {{shouldPullFromEndpoint()}}. Could just extend that condition then to add the liveness check in addition to {{shouldPullFromEndpoint()}}?;;;","02/Sep/20 19:16;bdeggleston;Possibly, you still need to check in the submission task in case the node has died in the meantime. There would still be an intersection of node flapping rate and unfortunate scheduling where the lockup could occur though.

The queue, while a little awkward, also makes us a bit more resilient against other unanticipated states and/or bugs.;;;","07/Sep/20 13:16;stefan.miklosovic;I am getting this exception on totally clean node, I am bootstrapping a cluster of 3 nodes:


{code:java}
cassandra_node_1    | INFO  [ScheduledTasks:1] 2020-09-07 15:10:13,037 TokenMetadata.java:517 - Updating topology for all endpoints that have changed
cassandra_node_1    | INFO  [HANDSHAKE-spark-master-1/172.19.0.5] 2020-09-07 15:10:13,311 OutboundTcpConnection.java:561 - Handshaking version with spark-master-1/172.19.0.5
cassandra_node_1    | INFO  [GossipStage:1] 2020-09-07 15:10:13,870 Gossiper.java:1141 - Node /172.19.0.5 is now part of the cluster
cassandra_node_1    | INFO  [GossipStage:1] 2020-09-07 15:10:13,904 TokenMetadata.java:497 - Updating topology for /172.19.0.5
cassandra_node_1    | INFO  [GossipStage:1] 2020-09-07 15:10:13,907 TokenMetadata.java:497 - Updating topology for /172.19.0.5
cassandra_node_1    | INFO  [GossipStage:1] 2020-09-07 15:10:14,052 Gossiper.java:1103 - InetAddress /172.19.0.5 is now UP
cassandra_node_1    | WARN  [MessagingService-Incoming-/172.19.0.5] 2020-09-07 15:10:14,119 IncomingTcpConnection.java:103 - UnknownColumnFamilyException reading from socket; closing
cassandra_node_1    | org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find table for cfId 5bc52802-de25-35ed-aeab-188eecebb090. If a table was just created, this is likely due to the schema not being fully propagated.  Please wait for schema agreement on table creation.
cassandra_node_1    | 	at org.apache.cassandra.config.CFMetaData$Serializer.deserialize(CFMetaData.java:1578) ~[apache-cassandra-3.11.9-SNAPSHOT.jar:3.11.9-SNAPSHOT]
cassandra_node_1    | 	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize30(PartitionUpdate.java:899) ~[apache-cassandra-3.11.9-SNAPSHOT.jar:3.11.9-SNAPSHOT]
cassandra_node_1    | 	at org.apache.cassandra.db.partitions.PartitionUpdate$PartitionUpdateSerializer.deserialize(PartitionUpdate.java:874) ~[apache-cassandra-3.11.9-SNAPSHOT.jar:3.11.9-SNAPSHOT]
cassandra_node_1    | 	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:415) ~[apache-cassandra-3.11.9-SNAPSHOT.jar:3.11.9-SNAPSHOT]
cassandra_node_1    | 	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:434) ~[apache-cassandra-3.11.9-SNAPSHOT.jar:3.11.9-SNAPSHOT]
cassandra_node_1    | 	at org.apache.cassandra.db.Mutation$MutationSerializer.deserialize(Mutation.java:371) ~[apache-cassandra-3.11.9-SNAPSHOT.jar:3.11.9-SNAPSHOT]
cassandra_node_1    | 	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:123) ~[apache-cassandra-3.11.9-SNAPSHOT.jar:3.11.9-SNAPSHOT]
cassandra_node_1    | 	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:195) ~[apache-cassandra-3.11.9-SNAPSHOT.jar:3.11.9-SNAPSHOT]
cassandra_node_1    | 	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessages(IncomingTcpConnection.java:183) ~[apache-cassandra-3.11.9-SNAPSHOT.jar:3.11.9-SNAPSHOT]

{code}

That cfId stands for system_auth/roles. It seems like we are applying changes before schema agreement has occured so that table is not there yet to apply mutations against.

This is the log from the second node. The first one booted fine, the second one throws this, the third one boots fine. It seems like eventually everything is just fine however that exception is ... concerning.
;;;","07/Sep/20 13:36;stefan.miklosovic;There is also a runtime error as that concurrent hash map from that package is not on the class path. I removed it here, I just squashed all changes in Blakes branch + this one fix:

https://github.com/instaclustr/cassandra/commit/e23677deeb7c836b4b7c80f98009353668351620;;;","07/Sep/20 16:58;stefan.miklosovic;These tests are failing

https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/4/#showFailuresLink;;;","09/Sep/20 16:18;stefan.miklosovic;I have improved the original work of mine and I wrote a test for that. Jenkins build does not fail anymore so I believe I have totally on par solution when it comes to dtests as I do not have time to fix dtests which the other solution breaks. While I admit that the improved version is technicaly more superior, the necessity to have clean build and same behaviour when it comes to dtests is more important to me at this moment. It would be awesome if dtests and issues I spotted are resolved though.

The test is here (1), the main logic is that a cluster of two nodes is started, the third node is started afterwards and I am dropping all migration messages to the other two, simulating some communication error between them. After some time, migration messages starts to flow again. So by doing this, I ll test the internals of the logic I wrote and it seems to do its job.

One issue I am little bit concerned of is that StorageService is issuing schema migration requests on ""onAlive, onJoin ..."" in StorageService and these requests are not part of the waitForSchema() logic. It is understandable that it is like that as we need to track migration requests after a node fully bootstraps but we should skip this from happening when a node is under bootstrapping. I wrapped the bodies of these methods into ""if (hasJoined())"" but it was invoked anyway. However, it does not matter too much if this is outside of the logic I did because if schema migration was sucessful, the rewritten logic in waitForSchema does not have anything to deal with so we are done anyway. For skipping this in test, I used ByteBuddy to intercept MigrationManager#scheduleSchemaPull to do nothing hence I effectively skip migration schemas to be sent outside of the change I did.

onChange method in onJoin merges schemas again too, in case state is SCHEMA so I am not completely sure why we are merging schemas on a join anyway?

{code:java}
    public void onJoin(InetAddress endpoint, EndpointState epState)
    {
        for (Map.Entry<ApplicationState, VersionedValue> entry : epState.states())
        {
            onChange(endpoint, entry.getKey(), entry.getValue());
        }

        // this is weird
        MigrationManager.instance.scheduleSchemaPull(endpoint, epState);
    }

    public void onAlive(InetAddress endpoint, EndpointState state)
    {
        // this is weird as well
        MigrationManager.instance.scheduleSchemaPull(endpoint, state);
        if (tokenMetadata.isMember(endpoint))
            notifyUp(endpoint);
    }
{code}


(1) https://github.com/instaclustr/cassandra/blob/15158-original-fix/test/distributed/org/apache/cassandra/distributed/test/BootstrappingSchemaAgreementTest.java

;;;","09/Oct/20 21:11;bdeggleston;Ok, I've fixed all the dtest issues and ported the 3.11 fix to 3.0 and trunk.

The 3.11 branch with misc fixes is here: https://github.com/bdeggleston/cassandra/tree/15158-3.11

Most of the fixes are self explanatory, the less obvious ones are:

* wait for gossip to settle before waiting on schemas. The original patch accidentally removed the wait on the schema version to be non-empty, so this both a fix and a change. Waiting for gossip makes it more likely that we've seen all current schema versions before we begin waiting instead of just waiting for the first schema to be received, which was the effect of waiting on Schema.instance.isEmpty
* don't check liveness in shouldPullFromEndpoint. There were cases where the node wasn't considered alive until after `reportEndpointVersion` had been called (because it happens as part of the current gossip update).
* move migration start inside StorageService.joinRing because in-jvm dtests don't use CassandraDaemon
* don't fail maybePullSchema if version info has no endpoints. We automatically call that after completing a pull, so a version will eventually have no endpoints left on it

The squashed ports are here:
| [3.0|https://github.com/bdeggleston/cassandra/tree/15158-3.0] | [circle|https://app.circleci.com/pipelines/github/bdeggleston/cassandra?branch=15158-3.0] |
| [3.11|https://github.com/bdeggleston/cassandra/tree/15158-3.11-squashed] | [circle|https://app.circleci.com/pipelines/github/bdeggleston/cassandra?branch=15158-3.11-squashed] |
| [trunk|https://github.com/bdeggleston/cassandra/tree/15158-trunk] | [circle|https://app.circleci.com/pipelines/github/bdeggleston/cassandra?branch=15158-trunk] |
;;;","09/Oct/20 23:30;stefan.miklosovic;Thanks for this a lot! I ll review right at the beginning of the next week.;;;","10/Oct/20 08:51;stefan.miklosovic;I have left my comments in 3.0 branch - probably applicable to 3.11 and trunk too.;;;","19/Oct/20 15:27;aleksey;Left a small comment on the 3.0 branch. Also, the following nits for {{MigrationCoordinator}}:
1. A bunch of unused imports
2. {{shouldApplySchemaFrom()}} has an unused argument
3. {{requestQueue}} could be an {{ArrayDequeue}} instead of a {{LinkedList}} - should set a good example for anyone randomly reading this code, even if it's not critical to do the right thing in this context 

EDIT: LGTM, +1, ship it;;;","29/Oct/20 22:09;bdeggleston;Addressed all review comments and rebased onto the latest branches;;;","02/Nov/20 23:21;stefan.miklosovic;I am +1 too (it if counts :) );;;","09/Nov/20 20:26;bdeggleston;[~stefan.miklosovic] it does, thanks to you and [~aleksey] for the reviews. Committed to 3.0 and merged up to trunk.;;;","13/Nov/20 18:52;dcapwell;Found a small set of typos which cause us to wait for schemas for 8h20m rather than 30s, going to submit a patch here and fix in all 3 branches...;;;","13/Nov/20 19:14;dcapwell;Patches:

3.0: https://github.com/dcapwell/cassandra/tree/patchfix/CASSANDRA-15158-3.0
3.11: https://github.com/dcapwell/cassandra/tree/patchfix/CASSANDRA-15158-3.11
trunk: https://github.com/dcapwell/cassandra/tree/patchfix/CASSANDRA-15158-trunk

A test was added in CASSANDRA-16213 which showed this issue, its only stable once this patch is applied (and disable failing);;;","13/Nov/20 19:25;dcapwell;Starting commit

CI Results: Yellow.  3.1 org.apache.cassandra.service.MigrationCoordinatorTest but passes locally, -trunk org.apache.cassandra.distributed.test.ring.BootstrapTest fails frequently due to schemas not present added commit which increases timeout from 30s to 90s-, and other expected issues.
||Branch||Source||Circle CI||Jenkins||
|cassandra-3.0|[branch|https://github.com/dcapwell/cassandra/tree/commit_remote_branch/CASSANDRA-15158-cassandra-3.0-7E401495-E38F-4857-80C1-2C27028F572E]|[build|https://app.circleci.com/pipelines/github/dcapwell/cassandra?branch=commit_remote_branch%2FCASSANDRA-15158-cassandra-3.0-7E401495-E38F-4857-80C1-2C27028F572E]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/200/]|
|cassandra-3.11|[branch|https://github.com/dcapwell/cassandra/tree/commit_remote_branch/CASSANDRA-15158-cassandra-3.11-7E401495-E38F-4857-80C1-2C27028F572E]|[build|https://app.circleci.com/pipelines/github/dcapwell/cassandra?branch=commit_remote_branch%2FCASSANDRA-15158-cassandra-3.11-7E401495-E38F-4857-80C1-2C27028F572E]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/201/]|
|trunk|[branch|https://github.com/dcapwell/cassandra/tree/commit_remote_branch/CASSANDRA-15158-trunk-7E401495-E38F-4857-80C1-2C27028F572E]|[build|https://app.circleci.com/pipelines/github/dcapwell/cassandra?branch=commit_remote_branch%2FCASSANDRA-15158-trunk-7E401495-E38F-4857-80C1-2C27028F572E]|[build|https://ci-cassandra.apache.org/job/Cassandra-devbranch/202/]|
;;;","13/Nov/20 20:26;bdeggleston;Thanks David, +1;;;","13/Nov/20 20:39;dcapwell;Committed https://github.com/apache/cassandra/commit/7d6f9b94dd0d00bfd29374d7a645e650f451023d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure Caffeine cache does not return stale entries,CASSANDRA-15153,13238523,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,azotcsit,eperott,eperott,10/Jun/19 13:12,01/Nov/22 20:09,13/Jul/23 08:38,17/Sep/21 13:55,4.0.2,4.1,4.1-alpha1,,,Feature/Authorization,,,,0,security,,,"Version 2.3.5 of the Caffeine cache that we're using in various places can hand out stale entries in some cases. This seem to happen when an update fails repeatedly, in which case Caffeine may return a previously loaded value. For instance, the AuthCache may hand out permissions even though the reload operation is failing, see CASSANDRA-15041.",,azotcsit,ben.manes,dnk,eperott,jeromatron,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-18002,CASSANDRA-15041,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,azotcsit,,,,,,,,,,,,Security -> Privilege Escalation,,,,,,,,Low Hanging Fruit,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,Security,,Fri Sep 17 19:02:57 UTC 2021,,,,,,,All,,,,,"0|z03ksw:",9223372036854775807,,,,,,,brandon.williams,mck,,,Low,,4.0-alpha,4.0-alpha1,,https://github.com/apache/cassandra/commit/b3af67f0ee950bed75593e0e6ce27547375f4096,,,,,,,,,A unit test was added. No documentation changes are required.,,,,,"26/Mar/20 15:36;eperott;I've created a [unit test|https://github.com/apache/cassandra/compare/trunk...eperott:15153-trunk] to reproduce this [issue|https://circleci.com/gh/eperott/cassandra/215#tests/containers/1].

In short, a {{LoadingCache}} may return a stale entry if its load function at some point start to throw exceptions for a key.

The bug seem to be fixed in Caffeine version 2.5.0 and later.

I guess there are a couple of questions that we need to take a stance on here.
# Should we risk upgrading the Caffeine version because of this? Caffeine is used in 12 different classes in trunk.
# And if so, should we make the minimal change and move to 2.5.0, or something later?

FWIW, I'm not able to create a scenario where I can exploit this. A potential one would've been to login as a user after it was deleted in the database (causing the load function to throw {{AuthenticationException}}). However, other permission checks prevent the login attempt from completing. Then again, this doesn't prove there isn't a way to exploit this!;;;","11/Sep/21 16:10;azotcsit;I looked to the issue a bit further and can confirm there is a bug in the library. Basically the explanation is like that:
 - records are physically removed from the cache asynchronously
 - in order to not return stale records the library relies onto {{writeTime}} field
 - it checks whether a record is expired based on {{writeTime}} on every {{get}} call
 - if the record is expired it tries to load the actual value using {{CacheLoader}}
 - if the value is available, it is cached and returned; if not, the expired data is physically removed and {{null}} is returned; if an exception occurs, there is a problem

Basically it happens that the code updates {{writeTime}} on expiration ([https://github.com/ben-manes/caffeine/blob/v2.3.5/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L2002]) before checking whether it can load the actual value. And if while looking for the actual record ([https://github.com/ben-manes/caffeine/blob/v2.3.5/caffeine/src/main/java/com/github/benmanes/caffeine/cache/BoundedLocalCache.java#L2008]) an exception occurs, the method is interrupted (meaning an exception is re-thrown to the upper level). However, the expired record has the {{writeTime}} updated! Basically the expired record resurrects.

As [~eperott] correctly mentioned, they re-wrote the whole expiration approach in [2.5.0|https://github.com/ben-manes/caffeine/releases/tag/v2.5.0] and the issue is probably fixed (I did not test it though). So the only fix is to update _Caffeine_ to a newer version. As I can see they went ahead and the current version is 3.0.3. I believe it makes sense to move to the latest version because it has a bunch of fixes and perf improvements.

[~blerer] [~mck] 

I have a could of questions:
 # It is clearly a minor bug. Currently expiration logic is used in {{AuthCache}} and {{ActiveRepairService}} classes. There are a few more classes that use _Caffeine_, but they probably won't be changed. In fact, the library upgrade seems to be hard to test since possible issues (in the library itself) can be probably re-produced in a concurrent environment and under a certain load only (aka production). So for me it is hard to asses the risk level of backporting it to the old versions. At the moment all versions starting from 3.0 seem to be affected. With that being said, I'm wondering to what versions we need to apply the fix.
 # Are we good to go with the latest _Caffeine_ version? Alternatively, we can use a hybrid approach - update old versions to 2.5.0 (the first version where the problem seems to be fixed) and 4.1 to 3.0.3 (the latest version) - if that seems to be safer.

Please, share your thoughts.;;;","11/Sep/21 16:18;mck;bq.  Are we good to go with the latest Caffeine version? Alternatively, we can use a hybrid approach - update old versions to 2.5.0 (the first version where the problem seems to be fixed) and 4.1 to 3.0.3 (the latest version) - if that seems to be safer.

This makes sense. In older versions we need to check there's no performance impacts (via testing or code reviewing) .;;;","11/Sep/21 16:33;brandon.williams;I see only bugfixes from 2.5.0 to 2.5.6 (the last of the 2.5 line) so I don't see any reason not to use that.;;;","11/Sep/21 19:58;azotcsit;[~mck] [~brandon.williams]

Thanks for the feedback! I made the changes and here is the summary:
||Branch||Comment||Changes||Source Code||
|3.0|It turned out that 3.0 does not use Caffeine. No changes are required.|No|N/A |
|3.11|{{AuthCache}} uses {{com.google.common.cache.CacheBuilder}} instead {{com.github.benmanes.caffeine.cache.Caffeine}}. I ported the corresponding test only and it passes. |Added a unit test| [https://github.com/alex-ninja/cassandra/tree/cassandra-15153-3.11_caffeine]|
| 4.0|Usage of 2.5.6 version fixes the issue. The corresponding test passes.|Added a unit test and updated _Caffeine_ version to 2.5.6| [https://github.com/alex-ninja/cassandra/tree/cassandra-15153-4.0_caffeine]|
| trunk|It is impossible to use 3.0.3 version because it is not compatible with Java 8 ([release notes|https://github.com/ben-manes/caffeine/releases/tag/v3.0.0]). Therefore, 2.9.2 is used. |Added a unit test and updated _Caffeine_ version to 2.9.2| [https://github.com/alex-ninja/cassandra/tree/cassandra-15153-trunk_caffeine]|

 

Could you please review and (if there are no remarks) start the CI.

 

PS:

If you think that it makes sense to keep versions aligned and stick to 2.5.6 in both 4.0 and trunk, I'm good with that (the patches are interchangeable). My personal attitude is to stick to the newer version for trunk because it may contain bug fixes for issues we have not yet discovered.

 ;;;","11/Sep/21 20:57;mck;CI:
- trunk [!https://ci-cassandra.apache.org/job/Cassandra-devbranch/1111/badge/icon!|https://ci-cassandra.apache.org/job/Cassandra-devbranch/1111/]
- 4.0 [!https://ci-cassandra.apache.org/job/Cassandra-devbranch/1110/badge/icon!|https://ci-cassandra.apache.org/job/Cassandra-devbranch/1110/]
- 3.11 [!https://ci-cassandra.apache.org/job/Cassandra-devbranch-test/793/badge/icon!|https://ci-cassandra.apache.org/job/Cassandra-devbranch-test/793];;;","12/Sep/21 08:06;azotcsit;Thanks [~mck]!

There are a few failures for _trunk_, but they do not seem to be related to this particular change.;;;","12/Sep/21 19:25;mck;All the test results look good [~azotcsit].;;;","17/Sep/21 10:28;azotcsit;[~brandon.williams] [~mck]

Sorry for bugging you, I'm just wondering whether there are any concerns on getting this merged. If yes, please, let me know, so I can do the necessary changes on the weekend.

As far is I understand, there is no need to update CHANGES.txt for bug fixes. However, in this case we updated version of a dependency, does it need to be mentioned somewhere in documentation?;;;","17/Sep/21 13:55;brandon.williams;Looks good to me too.  Committed just the test to 3.11, the test and 2.5.6 to 4.0 and the test and 2.9.2 to trunk.  I broke the test out from the upgrade in separate commits and updated CHANGES where needed. Thanks!;;;","17/Sep/21 14:00;azotcsit;Great! Thanks [~brandon.williams]!

And thanks to [~eperott] for spotting the problem and coming up with the unit test!;;;","17/Sep/21 18:55;ben.manes;Sorry for the bug here. There are so many moving parts that I probably confused myself when writing that original and obviously bad code.. I'll review the test cases to make sure that this metadata is covered and not just fixed. Please do try to keep updated to recent versions for bug fixes.;;;","17/Sep/21 19:02;brandon.williams;Thanks [~ben.manes].  Luckily the impact here remained only theoretical, but we'll try to do a better job of keeping up in the future.  ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transitional TLS server configuration options are overly complex,CASSANDRA-15146,13236564,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,e.dimitrova,jolynch,jolynch,30/May/19 18:39,21/Dec/20 08:04,13/Jul/23 08:38,26/Jun/20 14:33,,,,,,Feature/Encryption,Local/Config,,,0,,,,"It appears as part of the port from transitional client TLS to transitional server TLS in CASSANDRA-10404 (the ability to switch a cluster to using {{internode_encryption}} without listening on two ports and without downtime) we carried the {{enabled}} setting over from the client implementation. I believe that the {{enabled}} option is redundant to {{internode_encryption}} and {{optional}} and it should therefore be removed prior to the 4.0 release where we will have to start respecting that interface. 

Current trunk yaml:
{noformat}
server_encryption_options:                                                      
    # set to true for allowing secure incoming connections                      
    enabled: false                                                              
    # If enabled and optional are both set to true, encrypted and unencrypted connections are handled on the storage_port
    optional: false                                                                                                                                                                                                                                                                                                                             
    # if enabled, will open up an encrypted listening socket on ssl_storage_port. Should be used
    # during upgrade to 4.0; otherwise, set to false.                           
    enable_legacy_ssl_storage_port: false                                       
    # on outbound connections, determine which type of peers to securely connect to. 'enabled' must be set to true.
    internode_encryption: none                                                  
    keystore: conf/.keystore                                                    
    keystore_password: cassandra                                                
    truststore: conf/.truststore                                                
    truststore_password: cassandra            
{noformat}
I propose we eliminate {{enabled}} and just use {{optional}} and {{internode_encryption}} to determine the listener setup. I also propose we change the default of {{optional}} to true. We could also re-name {{optional}} since it's a new option but I think it's good to stay consistent with the client and use {{optional}}.
||optional||internode_encryption||description||
|true|none|(default) No encryption is used but if a server reaches out with it we'll use it|
|false|dc|Encryption is required for inter-dc communication, but not intra-dc|
|false|all|Encryption is required for all communication|
|false|none|We only listen for unencrypted connections|
|true|dc|Encryption is used for inter-dc communication but is not required|
|true|all|Encryption is used for all communication but is not required|

From these states it is clear when we should be accepting TLS connections (all except for false and none) as well as when we must enforce it.

To transition without downtime from an un-encrypted cluster to an encrypted cluster the user would do the following:

1. After adding valid truststores, change {{internode_encryption}} to the desired level of encryption (recommended {{all}}) and restart Cassandra
 2. Change {{optional=false}} and restart Cassandra to enforce #1

If {{optional}} defaulted to {{false}} as it does right now we'd need a third restart to first change {{optional}} to {{true}}, which given my understanding of the OptionalSslHandler isn't really relevant.",,aleksey,benedict,e.dimitrova,jasonstack,jeromatron,jmckenzie,jolynch,mbyrd,PuerTea,rtib,rustyrazorblade,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15234,,,,,,CASSANDRA-15234,,,,,,,,,,,CASSANDRA-15262,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,e.dimitrova,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Jun 26 14:33:35 UTC 2020,,,,,,,All,,,,,"0|z038sg:",9223372036854775807,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,"31/May/19 12:15;benedict;Perhaps we should generally massage this a bit.

Is {{server_encryption_options}} very clear, for instance? {{internode_encryption}} would be clearer to me. With perhaps a {{protect}} field accepting {{none}}, {{inter_dc}}, {{all}}?

Should we consider getting rid of the {{_options}} for both (while still supporting {{_options}} for client indefinitely)? The word is redundant, given the context.;;;","31/May/19 13:34;rustyrazorblade;I like this naming, [~benedict].  I think it improves the clarity a bit by matching the vocabulary we use elsewhere.;;;","31/May/19 17:49;jolynch;Alright, we hashed it out in [asf slack|https://the-asf.slack.com/archives/CK23JSY2K/p1559322494052200] and will take this in two steps:
 # Remove the `enabled` option so we have backwards compatible `yaml` (and probably switch `optional` to true by default)
 # Come up with a proposal for new encryption option names and implement them in a backwards compatible way.;;;","30/Apr/20 02:12;e.dimitrova;+1. I believe this is already part of CASSANDRA-15262.+ *EDIT: * Different optional, my bad
2. I can add these name changes in CASSANDRA-15234. The actual code for name change of parameters with backward compatibility is already implemented there. I have to complete the rest of the ticket this week. (working async on other things too but this is more or less done)

For completeness, I will link CASSANDRA-15262 and CASSANDRA-15234 to this one. 


;;;","19/May/20 22:23;e.dimitrova;[~micarlise], CASSANDRA-15234 is almost done. Please let me know if I can help you with this one. ;;;","30/May/20 16:21;jmckenzie;Going ahead and kicking this over to [~e.dimitrova] as we haven't heard from you [~micarlise] and this is blocking beta.

 

Though I know you have a lot of beta blockers on your plate [~e.dimitrova] - we can raise a flag and find someone else to move this along if you need the assist.

 

Let me or Ekaterina know if you were active on this [~micarlise] - happy to move assignee back over!;;;","31/May/20 00:13;e.dimitrova;I didn't hear anything from [~micarlise] but we had a quick discussion on Slack with Joey. 

Parameters changes will happen after  CASSANDRA-15234 lands and we will close this ticket with that probably. ;;;","05/Jun/20 14:34;e.dimitrova;[~jolynch], just to confirm what we talked and for the record, the final plan is to rename only server_encryption_options to internode_encryption, right? Let me know if there is any additional work that needs attention as part of this ticket. Thanks
;;;","26/Jun/20 14:32;e.dimitrova;Closing this one as duplicate, the annotation for the name change of  server_encryption_options to internode_encryption will be added as part of CASSANDRA-15234.

I take the responsibility to add the commit here later for the record. ;;;","26/Jun/20 14:33;e.dimitrova;All the renaming happens at once in CASSANDRA-15234. This one is considered too. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove assertion on file deletion to trigger failure policy,CASSANDRA-15143,13235976,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,spod,spod,spod,28/May/19 11:07,15/May/20 08:55,13/Jul/23 08:38,03/Jan/20 19:22,4.0,4.0-alpha3,,,,Local/SSTable,,,,0,,,,"Deleting any files by using FileUtils.deleteWithConfirm() will involve checking for file existence using an assertion. If the assertion passes, any errors from the actual Files.delete() operation will get handled as a FSWriteError and invoke the DiskFailurePolicy. This will get us to a situation where only some cases of bad FS or disk issues will be handed by the DFP, while FS issues that may already cause the file.exists() assertion to fail, will only manifest as AssertionErrors. We should invoke DFP for the later as well and remove the assertion altogether.",,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,spod,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Jan 03 19:22:47 UTC 2020,,,,,,,All,,,,,"0|z0355s:",9223372036854775807,,,,,,,mck,,,,Normal,,2.2.0 beta 1,,,https://github.com/apache/cassandra/commit/e597d48bda845815dada7cf53f705ae273d8f457,,,,,,,,,"[Tests|https://circleci.com/gh/spodkowinski/workflows/cassandra/tree/CASSANDRA-15143]",,,,,"28/May/19 11:13;spod;* [CASSANDRA-15143|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-15143]
 * [Tests|https://circleci.com/gh/spodkowinski/workflows/cassandra/tree/CASSANDRA-15143];;;","01/Jan/20 21:45;mck;||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[trunk_15143|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_15143]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_15143]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/48/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/48/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/722/badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/722]|
;;;","03/Jan/20 19:22;mck;Committed as e597d48bda845815dada7cf53f705ae273d8f457;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix errors on repairing empty keyspace,CASSANDRA-15142,13235789,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,spod,spod,spod,27/May/19 11:13,07/Mar/23 11:52,13/Jul/23 08:38,09/Oct/19 09:07,4.0,4.0-alpha2,,,,Consistency/Repair,Tool/nodetool,,,0,,,,"Running repairs on empty keyspaces will produce a rather confusing error in trunk:

{noformat}
ERROR [Repair-Task:1] 2019-05-24 10:36:20,323 RepairRunnable.java:274 - Repair 014607d0-7dff-11e9-9256-158db058ccc5 failed:
java.lang.IllegalArgumentException: repair sessions cannot operate on multiple keyspaces
▸  at com.google.common.base.Preconditions.checkArgument(Preconditions.java:135)
▸  at org.apache.cassandra.service.ActiveRepairService$ParentRepairSession.<init>(ActiveRepairService.java:566)
▸  at org.apache.cassandra.service.ActiveRepairService.registerParentRepairSession(ActiveRepairService.java:484)
▸  at org.apache.cassandra.service.ActiveRepairService.prepareForRepair(ActiveRepairService.java:395)
▸  at org.apache.cassandra.repair.RepairRunnable.runMayThrow(RepairRunnable.java:269)
▸  at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
▸  at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
▸  at java.util.concurrent.FutureTask.run(FutureTask.java:266)
▸  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
▸  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
▸  at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
▸  at java.lang.Thread.run(Thread.java:748)
{noformat}

Let's ignore empty keyspaces and return a success return status instead.",,jeromatron,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,spod,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Oct 09 09:07:32 UTC 2019,,,,,,,All,,,,,"0|z0340o:",9223372036854775807,,,,,,,mck,,,,Low,,5.0,,,https://github.com/apache/cassandra/commit/5bc7a375a3cac85e79543e7e30d620faeb891955,,,,,,,,,"[CircleCI|https://circleci.com/workflow-run/45ecd7af-ec77-4090-bf07-278c78e43e30]",,,,,"27/May/19 14:50;spod;* [CASSANDRA-15142|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-15142]
* [CircleCI|https://circleci.com/workflow-run/45ecd7af-ec77-4090-bf07-278c78e43e30];;;","09/Oct/19 09:00;mck;+1;;;","09/Oct/19 09:07;mck;Committed as 5bc7a37;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect error message in legacy reader,CASSANDRA-15136,13234512,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefan.miklosovic,VincentWhite,VincentWhite,21/May/19 07:17,16/Aug/21 14:53,13/Jul/23 08:38,16/Aug/21 14:53,3.0.26,3.11.12,,,,Observability/Logging,,,,0,,,,"Just fixes the order in the exception message.



||3.0.x||3.11.x||
|[Patch|https://github.com/vincewhite/cassandra/commits/readLegacyAtom30]|[Patch|https://github.com/vincewhite/cassandra/commits/readLegacyAtom]|",,eperott,stefan.miklosovic,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefan.miklosovic,,,,,,,,,,,,Code,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Aug 16 14:51:47 UTC 2021,,,,,,,All,,,,,"0|z02w4w:",9223372036854775807,,,,,,,eperott,stefan.miklosovic,,,Low,,3.0 alpha 1,,,https://github.com/apache/cassandra/commit/ad139da84c600c294e00263bd2c48b40aea2c2cc,,,,,,,,,manual testing,,,,,"22/May/19 18:31;eperott;Thanks for the patch. Issue is present in 3.0 and 3.11 as far as I can tell.

Regarding the patch, you got the parenthesis wrong in the end. Also, you may want to create an entry in the CHANGES.txt file as well.;;;","23/May/19 02:29;VincentWhite;Thanks, should be sorted now.;;;","23/May/19 08:19;eperott;+1;;;","13/Aug/21 13:21;stefan.miklosovic;[~eperott] would you +1 me here? Similar stuff wil be done for 3.11

 

https://github.com/apache/cassandra/pull/1140;;;","13/Aug/21 13:23;stefan.miklosovic;https://github.com/apache/cassandra/pull/1140;;;","13/Aug/21 13:26;brandon.williams;+1;;;","13/Aug/21 13:51;eperott;+1;;;","16/Aug/21 14:51;stefan.miklosovic;https://github.com/apache/cassandra/commit/ad139da84c600c294e00263bd2c48b40aea2c2cc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SASI tokenizer options not validated before being added to schema,CASSANDRA-15135,13234460,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefan.miklosovic,VincentWhite,VincentWhite,21/May/19 04:56,27/May/22 19:25,13/Jul/23 08:38,23/Aug/21 21:27,3.11.12,4.0.1,4.1,4.1-alpha1,,Feature/SASI,,,,0,,,,"If you attempt to create a SASI index with an illegal argument combination the index will be added to the schema tables before trying instantiate the tokenizer which causes a RuntimeException. Since the index was written to the schema tables, cassandra will hit the same exception and fail to start when it tries to load the schema on boot.

 The branch below includes a unit test to reproduce the issue.
||3.11||
|[PoC|https://github.com/vincewhite/cassandra/commit/089547946d284ae3feb0d5620067b85b8fd66ebc]|

 ",,adelapena,jeromatron,stefan.miklosovic,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,"adelapena commented on a change in pull request #1141:
URL: https://github.com/apache/cassandra/pull/1141#discussion_r690552925



##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/AbstractAnalyzer.java
##########
@@ -38,6 +39,10 @@ public void remove()
         throw new UnsupportedOperationException();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException

Review comment:
       Nit: can we use `AbstractType<?>`?

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException

Review comment:
       Nit: can we mark the method with `@Override`?

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/NonTokenizingAnalyzer.java
##########
@@ -56,6 +57,14 @@
     private ByteBuffer input;
     private boolean hasNext = false;
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (options.containsKey(NonTokenizingOptions.CASE_SENSITIVE) && (options.containsKey(NonTokenizingOptions.NORMALIZE_LOWERCASE)
+                                                       || options.containsKey(NonTokenizingOptions.NORMALIZE_UPPERCASE)))
+            throw new ConfigurationException(""case_sensitive option cannot be specified together "" +
+                                               ""with either normalize_lowercase or normalize_uppercase"");

Review comment:
       Nit: maybe we can try to change the original formatting to fit into the line length:
   ```suggestion
                   if (options.containsKey(NonTokenizingOptions.CASE_SENSITIVE) &&
               (options.containsKey(NonTokenizingOptions.NORMALIZE_LOWERCASE) ||
                options.containsKey(NonTokenizingOptions.NORMALIZE_UPPERCASE)))
               throw new ConfigurationException(""case_sensitive option cannot be specified together "" +
                                                ""with either normalize_lowercase or normalize_uppercase"");
   ```

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/NonTokenizingAnalyzer.java
##########
@@ -56,6 +57,14 @@
     private ByteBuffer input;
     private boolean hasNext = false;
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException

Review comment:
       Nit: can we mark the method with `@Override`?

##########
File path: test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java
##########
@@ -2484,6 +2484,40 @@ public void testAnalyzerValidation()
         });
     }
 
+    @Test
+    public void testIllegalArgumentsForAnalyzerShouldFail()
+    {
+        String baseTable = ""illegal_argument_test"";
+        String indexName = ""illegal_index"";
+        QueryProcessor.executeOnceInternal(String.format(""CREATE KEYSPACE IF NOT EXISTS %s WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}"", KS_NAME));
+        QueryProcessor.executeOnceInternal(String.format(""CREATE TABLE IF NOT EXISTS %s.%s (k int primary key, v text);"", KS_NAME, baseTable));
+
+        try
+        {
+            QueryProcessor.executeOnceInternal(String.format(""CREATE CUSTOM INDEX IF NOT EXISTS %s ON %s.%s(v) "" +
+                            ""USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = { 'mode' : 'CONTAINS', "" +
+                            ""'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', "" +
+                            ""'case_sensitive': 'false',"" +
+                            ""'normalize_uppercase': 'true'};"",
+                    indexName, KS_NAME, baseTable));
+
+            Assert.fail(""creation of index analyzer with illegal options should fail"");
+        }
+        catch (ConfigurationException e)
+        {
+            //correct behaviour
+            //confirm that it wasn't written to the schema
+            Assert.assertTrue(QueryProcessor.executeOnceInternal(String.format(""SELECT * FROM system_schema.indexes WHERE keyspace_name = '%s' "" +
+                    ""and table_name = '%s' and index_name = '%s';"", KS_NAME, baseTable, indexName)).isEmpty());

Review comment:
       Nit: not a big deal, but we could prevent the NPE warning for example using `assertThat`:
   ```suggestion
               String query = String.format(""SELECT * FROM system_schema.indexes WHERE keyspace_name = '%s' "" +
                                            ""AND table_name = '%s' AND index_name = '%s'"", 
                                            KS_NAME, baseTable, indexName);
               Assertions.assertThat(QueryProcessor.executeOnceInternal(query)).isEmpty();
   ```

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (!VALID_ANALYZABLE_TYPES.containsKey(validator))
+            throw new ConfigurationException(String.format(""Only text types supported, got %s"", validator));

Review comment:
       I think this error is unreachable. The new `AbstractAnalyzer#validate` method is called exclusively from `IndexMode#validateAnalyzer`, which already validates the type with a call to `AbstractAnalyzer#isCompatibleWith`. 
   
   This makes me thing that probably we don't need the cell type argument on `AbstractAnalyzer#validate`, or alternatively we could consider consider making `AbstractAnalyzer#validate` consistently responsible for checking the cell type so `IndexMode#validateAnalyzer` doesn't have to call `AbstractAnalyzer#isCompatibleWith`. wdyt?




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Aug/21 17:38;githubbot;600","smiklosovic commented on a change in pull request #1141:
URL: https://github.com/apache/cassandra/pull/1141#discussion_r690725961



##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (!VALID_ANALYZABLE_TYPES.containsKey(validator))
+            throw new ConfigurationException(String.format(""Only text types supported, got %s"", validator));

Review comment:
       @adelapena 
   I do not have a problem with the approach you are describing at the end of your response but please look closely here:
   
   ````
                   if (!analyzer.isCompatibleWith(cd.type))
                       throw new ConfigurationException(String.format(""%s does not support type %s"",
                                                                      analyzerClass.getSimpleName(),
                                                                      cd.type.asCQL3Type()));
                   analyzer.validate(indexOptions, cd.cellValueType());
   ````
   
   `cd.type` is not the same thing as `cd.cellValueType()` (go into cellValueType() and read the docs).
   
   isCompatibleWith excepts 'different"" AbstractType than validate method would accept, if I understand that correctly.




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Aug/21 21:11;githubbot;600","adelapena commented on a change in pull request #1141:
URL: https://github.com/apache/cassandra/pull/1141#discussion_r691156299



##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (!VALID_ANALYZABLE_TYPES.containsKey(validator))
+            throw new ConfigurationException(String.format(""Only text types supported, got %s"", validator));

Review comment:
       I missed that one. However, I still don't see a use case where that second configuration exception could be raised, since SASI doesn't support non-frozen collections or UDTs. If the exception where reachable I wonder if we would need a similar check in other text analyzer, and that would make the case to pass the cell value type, or the entire column definition, to `AbstractAnalyzer#isCompatibleWith`.
   I might be missing something obvious, but I think that this check is a remanent from what there was before creating the `isCompatibeWith` method during CASSANDRA-13669. wdyt?




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/21 11:42;githubbot;600","adelapena commented on a change in pull request #1141:
URL: https://github.com/apache/cassandra/pull/1141#discussion_r691157474



##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -100,11 +108,11 @@ protected ByteBuffer computeNext() {
         };
     }
 
-
+    @Override
     public boolean isTokenizing()
     {
         return true;
-    }
+        }

Review comment:
       Nit: misaligned

##########
File path: test/unit/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzerTest.java
##########
@@ -81,10 +83,11 @@ public void testBlankEntries() throws Exception
         Assert.assertFalse(testString.toLowerCase().equals(output.toString()));
     }
 
-    @Test(expected = IllegalArgumentException.class)
+    @Test(expected = ConfigurationException.class)
     public void ensureIncompatibleInputSkipped() throws Exception

Review comment:
       Nit: doesn't need the `throws Exception`

##########
File path: test/unit/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzerTest.java
##########
@@ -81,10 +83,11 @@ public void testBlankEntries() throws Exception
         Assert.assertFalse(testString.toLowerCase().equals(output.toString()));
     }
 
-    @Test(expected = IllegalArgumentException.class)
+    @Test(expected = ConfigurationException.class)
     public void ensureIncompatibleInputSkipped() throws Exception
     {
-        new DelimiterAnalyzer().init(new HashMap(), Int32Type.instance);
+        new DelimiterAnalyzer().validate(new HashMap<>(),

Review comment:
       Nit: we can use `Collections.emptyMap()`

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/AbstractAnalyzer.java
##########
@@ -38,15 +40,23 @@ public void remove()
         throw new UnsupportedOperationException();
     }
 
-    public abstract void init(Map<String, String> options, AbstractType validator);
+    public void validate(Map<String, String> options, ColumnDefinition cd) throws ConfigurationException
+    {
+        if (!isCompatibleWith(cd.type))
+            throw new ConfigurationException(String.format(""%s does not support type %s"",
+                                                           this.getClass().getSimpleName(),
+                                                           cd.type.asCQL3Type()));
+    }
+
+    public abstract void init(Map<String, String> options, AbstractType<?> validator);
 
     public abstract void reset(ByteBuffer input);
 
     /**
      * Test whether the given validator is compatible with the underlying analyzer.
      *
-     * @param validator
-     * @return
+     * @param validator the validator to test the compatibility with
+     * @return true if the give validator is compatible, false otherwise

Review comment:
       I think that `isCompatibleWith` can be `protected`, since now it's only called from `validate`.




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/21 11:46;githubbot;600","adelapena commented on a change in pull request #1141:
URL: https://github.com/apache/cassandra/pull/1141#discussion_r691156299



##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (!VALID_ANALYZABLE_TYPES.containsKey(validator))
+            throw new ConfigurationException(String.format(""Only text types supported, got %s"", validator));

Review comment:
       I missed that one. However, I still don't see a use case where that second configuration exception could be raised, since SASI doesn't support non-frozen collections or UDTs. If the exception where reachable I wonder if we would need a similar check in other analyzers, and that would make the case to pass the cell value type, or the entire column definition, to `AbstractAnalyzer#isCompatibleWith`.
   I might be missing something obvious, but I think that this check is a remanent from what there was before creating the `isCompatibeWith` method during CASSANDRA-13669. wdyt?




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/21 11:53;githubbot;600","adelapena commented on a change in pull request #1141:
URL: https://github.com/apache/cassandra/pull/1141#discussion_r690552925



##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/AbstractAnalyzer.java
##########
@@ -38,6 +39,10 @@ public void remove()
         throw new UnsupportedOperationException();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException

Review comment:
       Nit: can we use `AbstractType<?>`?

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException

Review comment:
       Nit: can we mark the method with `@Override`?

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/NonTokenizingAnalyzer.java
##########
@@ -56,6 +57,14 @@
     private ByteBuffer input;
     private boolean hasNext = false;
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (options.containsKey(NonTokenizingOptions.CASE_SENSITIVE) && (options.containsKey(NonTokenizingOptions.NORMALIZE_LOWERCASE)
+                                                       || options.containsKey(NonTokenizingOptions.NORMALIZE_UPPERCASE)))
+            throw new ConfigurationException(""case_sensitive option cannot be specified together "" +
+                                               ""with either normalize_lowercase or normalize_uppercase"");

Review comment:
       Nit: maybe we can try to change the original formatting to fit into the line length:
   ```suggestion
                   if (options.containsKey(NonTokenizingOptions.CASE_SENSITIVE) &&
               (options.containsKey(NonTokenizingOptions.NORMALIZE_LOWERCASE) ||
                options.containsKey(NonTokenizingOptions.NORMALIZE_UPPERCASE)))
               throw new ConfigurationException(""case_sensitive option cannot be specified together "" +
                                                ""with either normalize_lowercase or normalize_uppercase"");
   ```

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/NonTokenizingAnalyzer.java
##########
@@ -56,6 +57,14 @@
     private ByteBuffer input;
     private boolean hasNext = false;
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException

Review comment:
       Nit: can we mark the method with `@Override`?

##########
File path: test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java
##########
@@ -2484,6 +2484,40 @@ public void testAnalyzerValidation()
         });
     }
 
+    @Test
+    public void testIllegalArgumentsForAnalyzerShouldFail()
+    {
+        String baseTable = ""illegal_argument_test"";
+        String indexName = ""illegal_index"";
+        QueryProcessor.executeOnceInternal(String.format(""CREATE KEYSPACE IF NOT EXISTS %s WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}"", KS_NAME));
+        QueryProcessor.executeOnceInternal(String.format(""CREATE TABLE IF NOT EXISTS %s.%s (k int primary key, v text);"", KS_NAME, baseTable));
+
+        try
+        {
+            QueryProcessor.executeOnceInternal(String.format(""CREATE CUSTOM INDEX IF NOT EXISTS %s ON %s.%s(v) "" +
+                            ""USING 'org.apache.cassandra.index.sasi.SASIIndex' WITH OPTIONS = { 'mode' : 'CONTAINS', "" +
+                            ""'analyzer_class': 'org.apache.cassandra.index.sasi.analyzer.NonTokenizingAnalyzer', "" +
+                            ""'case_sensitive': 'false',"" +
+                            ""'normalize_uppercase': 'true'};"",
+                    indexName, KS_NAME, baseTable));
+
+            Assert.fail(""creation of index analyzer with illegal options should fail"");
+        }
+        catch (ConfigurationException e)
+        {
+            //correct behaviour
+            //confirm that it wasn't written to the schema
+            Assert.assertTrue(QueryProcessor.executeOnceInternal(String.format(""SELECT * FROM system_schema.indexes WHERE keyspace_name = '%s' "" +
+                    ""and table_name = '%s' and index_name = '%s';"", KS_NAME, baseTable, indexName)).isEmpty());

Review comment:
       Nit: not a big deal, but we could prevent the NPE warning for example using `assertThat`:
   ```suggestion
               String query = String.format(""SELECT * FROM system_schema.indexes WHERE keyspace_name = '%s' "" +
                                            ""AND table_name = '%s' AND index_name = '%s'"", 
                                            KS_NAME, baseTable, indexName);
               Assertions.assertThat(QueryProcessor.executeOnceInternal(query)).isEmpty();
   ```

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (!VALID_ANALYZABLE_TYPES.containsKey(validator))
+            throw new ConfigurationException(String.format(""Only text types supported, got %s"", validator));

Review comment:
       I think this error is unreachable. The new `AbstractAnalyzer#validate` method is called exclusively from `IndexMode#validateAnalyzer`, which already validates the type with a call to `AbstractAnalyzer#isCompatibleWith`. 
   
   This makes me thing that probably we don't need the cell type argument on `AbstractAnalyzer#validate`, or alternatively we could consider consider making `AbstractAnalyzer#validate` consistently responsible for checking the cell type so `IndexMode#validateAnalyzer` doesn't have to call `AbstractAnalyzer#isCompatibleWith`. wdyt?

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (!VALID_ANALYZABLE_TYPES.containsKey(validator))
+            throw new ConfigurationException(String.format(""Only text types supported, got %s"", validator));

Review comment:
       I missed that one. However, I still don't see a use case where that second configuration exception could be raised, since SASI doesn't support non-frozen collections or UDTs. If the exception where reachable I wonder if we would need a similar check in other text analyzer, and that would make the case to pass the cell value type, or the entire column definition, to `AbstractAnalyzer#isCompatibleWith`.
   I might be missing something obvious, but I think that this check is a remanent from what there was before creating the `isCompatibeWith` method during CASSANDRA-13669. wdyt?

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -100,11 +108,11 @@ protected ByteBuffer computeNext() {
         };
     }
 
-
+    @Override
     public boolean isTokenizing()
     {
         return true;
-    }
+        }

Review comment:
       Nit: misaligned

##########
File path: test/unit/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzerTest.java
##########
@@ -81,10 +83,11 @@ public void testBlankEntries() throws Exception
         Assert.assertFalse(testString.toLowerCase().equals(output.toString()));
     }
 
-    @Test(expected = IllegalArgumentException.class)
+    @Test(expected = ConfigurationException.class)
     public void ensureIncompatibleInputSkipped() throws Exception

Review comment:
       Nit: doesn't need the `throws Exception`

##########
File path: test/unit/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzerTest.java
##########
@@ -81,10 +83,11 @@ public void testBlankEntries() throws Exception
         Assert.assertFalse(testString.toLowerCase().equals(output.toString()));
     }
 
-    @Test(expected = IllegalArgumentException.class)
+    @Test(expected = ConfigurationException.class)
     public void ensureIncompatibleInputSkipped() throws Exception
     {
-        new DelimiterAnalyzer().init(new HashMap(), Int32Type.instance);
+        new DelimiterAnalyzer().validate(new HashMap<>(),

Review comment:
       Nit: we can use `Collections.emptyMap()`

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/AbstractAnalyzer.java
##########
@@ -38,15 +40,23 @@ public void remove()
         throw new UnsupportedOperationException();
     }
 
-    public abstract void init(Map<String, String> options, AbstractType validator);
+    public void validate(Map<String, String> options, ColumnDefinition cd) throws ConfigurationException
+    {
+        if (!isCompatibleWith(cd.type))
+            throw new ConfigurationException(String.format(""%s does not support type %s"",
+                                                           this.getClass().getSimpleName(),
+                                                           cd.type.asCQL3Type()));
+    }
+
+    public abstract void init(Map<String, String> options, AbstractType<?> validator);
 
     public abstract void reset(ByteBuffer input);
 
     /**
      * Test whether the given validator is compatible with the underlying analyzer.
      *
-     * @param validator
-     * @return
+     * @param validator the validator to test the compatibility with
+     * @return true if the give validator is compatible, false otherwise

Review comment:
       I think that `isCompatibleWith` can be `protected`, since now it's only called from `validate`.

##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (!VALID_ANALYZABLE_TYPES.containsKey(validator))
+            throw new ConfigurationException(String.format(""Only text types supported, got %s"", validator));

Review comment:
       I missed that one. However, I still don't see a use case where that second configuration exception could be raised, since SASI doesn't support non-frozen collections or UDTs. If the exception where reachable I wonder if we would need a similar check in other analyzers, and that would make the case to pass the cell value type, or the entire column definition, to `AbstractAnalyzer#isCompatibleWith`.
   I might be missing something obvious, but I think that this check is a remanent from what there was before creating the `isCompatibeWith` method during CASSANDRA-13669. wdyt?




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/21 16:38;githubbot;600","smiklosovic commented on a change in pull request #1141:
URL: https://github.com/apache/cassandra/pull/1141#discussion_r690725961



##########
File path: src/java/org/apache/cassandra/index/sasi/analyzer/DelimiterAnalyzer.java
##########
@@ -56,14 +57,16 @@ public ByteBuffer next()
         return iter.next();
     }
 
+    public void validate(Map<String, String> options, AbstractType validator) throws ConfigurationException
+    {
+        if (!VALID_ANALYZABLE_TYPES.containsKey(validator))
+            throw new ConfigurationException(String.format(""Only text types supported, got %s"", validator));

Review comment:
       @adelapena 
   I do not have a problem with the approach you are describing at the end of your response but please look closely here:
   
   ````
                   if (!analyzer.isCompatibleWith(cd.type))
                       throw new ConfigurationException(String.format(""%s does not support type %s"",
                                                                      analyzerClass.getSimpleName(),
                                                                      cd.type.asCQL3Type()));
                   analyzer.validate(indexOptions, cd.cellValueType());
   ````
   
   `cd.type` is not the same thing as `cd.cellValueType()` (go into cellValueType() and read the docs).
   
   isCompatibleWith excepts 'different"" AbstractType than validate method would accept, if I understand that correctly.




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;18/Aug/21 16:40;githubbot;600","smiklosovic merged pull request #1141:
URL: https://github.com/apache/cassandra/pull/1141


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;23/Aug/21 21:26;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,4800,,,0,4800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefan.miklosovic,,,,,,,,,,,,Code,,,,,,,,Normal,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Aug 20 17:15:28 UTC 2021,,,,,,,All,,,,,"0|z02vtc:",9223372036854775807,,,,,,,adelapena,,,,Normal,,3.4,,,https://github.com/apache/cassandra/commit/c562f1e78d86cfae6326549d3950f91d054de8ce,,,,,,,,,unit test,,,,,"13/Aug/21 14:23;stefan.miklosovic;Hi [~adelapena]

would you mind to go through this? I think your expertise related to indexes is valuable here.

https://github.com/apache/cassandra/pull/1141/files;;;","16/Aug/21 12:58;adelapena;Sure, I'll take a look;;;","17/Aug/21 17:44;adelapena;Nice catch, overall the approach looks good to me. I have left some comments in the PR. Also, do we have CI results? ;;;","17/Aug/21 20:20;stefan.miklosovic;thanks for the review, ci on the way, i ll ping you once it is prepared to be +1 for you.;;;","20/Aug/21 12:53;stefan.miklosovic;3.11 branch https://github.com/apache/cassandra/pull/1141/files
4.0 branch https://github.com/apache/cassandra/pull/1162

4.0 https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1053/
3.11 https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1052/

https://app.circleci.com/pipelines/github/adelapena/cassandra/789/workflows/81313de5-e1ef-4d07-9dff-c9979c89ccfb
https://app.circleci.com/pipelines/github/adelapena/cassandra/790/workflows/216952c5-1cfe-483c-8c15-ee491186d2e6
;;;","20/Aug/21 14:26;adelapena;Great, last changes look good to me, +1. Could we please have a patch and CI run for trunk?;;;","20/Aug/21 14:57;stefan.miklosovic;https://github.com/apache/cassandra/pull/1163
https://ci-cassandra.apache.org/view/patches/job/Cassandra-devbranch/1055/;;;","20/Aug/21 17:15;adelapena;Thanks, +1 assuming CI looks good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SASI index files not included in snapshots,CASSANDRA-15134,13234457,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,stefan.miklosovic,VincentWhite,VincentWhite,21/May/19 04:26,27/May/22 19:25,13/Jul/23 08:38,16/Sep/21 12:29,3.11.12,4.0.2,4.1,4.1-alpha1,,Feature/SASI,Local/Snapshots,,,0,,,,"Newly written SASI index files are not being included in snapshots. This is because the SASI index files are not added to the components ({{org.apache.cassandra.io.sstable.SSTable#components}}) list of newly written sstables. 

Although I don't believe anything except snapshots ever tries to reference the SASI index files from this location, on startup Cassandra does add the SASI index files (if they are found on disk) of existing sstables in their components list. In that case sstables that existed on startup with SASI index files will have their SASI index files included in any snapshots.

 

This patch updates the components list of newly written sstable once the index is built.
||3.11||4.0||Trunk||
|[https://github.com/apache/cassandra/pull/1150]|[TBD]|[TBD]|

 ",,adelapena,jasonstack,jeromatron,stefan.miklosovic,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,"adelapena commented on a change in pull request #1150:
URL: https://github.com/apache/cassandra/pull/1150#discussion_r706308443



##########
File path: test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java
##########
@@ -137,7 +146,87 @@ public void cleanUp()
     }
 
     @Test
-    public void testSingleExpressionQueries()
+    public void testSASIComponentsAddedToSnapshot() throws Throwable
+    {
+        String snapshotName = ""sasi_test"";
+        Map<String, Pair<String, Integer>> data = new HashMap<>();
+        Random r = new Random();
+
+        for (int i = 0; i < 100; i++)
+        {
+            data.put(UUID.randomUUID().toString(), Pair.create(UUID.randomUUID().toString(), r.nextInt()));
+        }
+
+        ColumnFamilyStore store = loadData(data, true);
+        store.forceMajorCompaction();
+
+        try
+        {
+            // left holds table component sizes, right holds total size of index components (SI_*)
+            Pair<Long, Long> tableIndexSizes = takeSnapshotAndCheckComponents(store, snapshotName);
+            Map<String, Pair<Long, Long>> details = store.getSnapshotDetails();
+
+            // check that SASI components are included in the computation of snapshot size
+            Assert.assertEquals((long) details.get(""sasi_test"").right, tableIndexSizes.left + tableIndexSizes.right);
+        }
+        finally
+        {
+            store.clearSnapshot(""sasi_test"");
+        }
+    }
+
+    private Pair<Long, Long> takeSnapshotAndCheckComponents(ColumnFamilyStore cfs, String snapshotName) throws Throwable
+    {
+        Set<SSTableReader> ssTableReaders = cfs.getLiveSSTables();
+        Set<Component> sasiComponents = new HashSet<>();
+        for (Index index : cfs.indexManager.listIndexes())
+        {
+            if (index instanceof SASIIndex)
+                sasiComponents.add(((SASIIndex) index).getIndex().getComponent());
+        }
+
+        cfs.snapshot(snapshotName);
+        JSONObject manifest = (JSONObject) new JSONParser().parse(new FileReader(cfs.getDirectories().getSnapshotManifestFile(snapshotName)));
+        JSONArray files = (JSONArray) manifest.get(""files"");
+        Assert.assertEquals(ssTableReaders.size(), files.size());
+        Map<Descriptor, Set<Component>> snapshots = cfs.getDirectories().sstableLister(Directories.OnTxnErr.IGNORE).snapshots(snapshotName).list();
+
+        long indexSize = 0;
+        long tableSize = 0;
+
+        for (SSTableReader sstable : ssTableReaders)
+        {
+
+            File snapshotDirectory = Directories.getSnapshotDirectory(sstable.descriptor, snapshotName);
+            Descriptor tmp = new Descriptor(snapshotDirectory,
+                                            sstable.getKeyspaceName(),
+                                            sstable.getColumnFamilyName(),
+                                            sstable.descriptor.generation,
+                                            sstable.descriptor.formatType);
+
+            Set<Component> components = snapshots.get(tmp);
+
+
+            Assert.assertNotNull(components);
+            Assert.assertTrue(components.containsAll(sasiComponents));
+
+            for (Component c : components)
+            {
+                Path p = Paths.get(sstable.descriptor + ""-"" + c.name);
+                long size = Files.size(p);
+                if (c.name.contains(""SI_"")) {
+                    indexSize += size;
+                } else {
+                    tableSize += size;
+                }

Review comment:
       Nit: braces placement should be:
   ```suggestion
                   if (c.name.contains(""SI_""))
                   {
                       indexSize += size;
                   }
                   else 
                   {
                       tableSize += size;
                   }
   ```
   Or without braces, if you prefer so:
   ```suggestion
                   if (c.name.contains(""SI_""))
                       indexSize += size;
                   else
                       tableSize += size;
   ```

##########
File path: src/java/org/apache/cassandra/index/sasi/conf/DataTracker.java
##########
@@ -77,6 +77,11 @@ public View getView()
         }
         while (!view.compareAndSet(currentView, newView));
 
+        for(SSTableReader sstable: indexedSSTables)

Review comment:
       Nit: missed whitespaces:
   ```suggestion
           for (SSTableReader sstable : indexedSSTables)
   ```

##########
File path: test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java
##########
@@ -137,7 +146,87 @@ public void cleanUp()
     }
 
     @Test
-    public void testSingleExpressionQueries()
+    public void testSASIComponentsAddedToSnapshot() throws Throwable
+    {
+        String snapshotName = ""sasi_test"";
+        Map<String, Pair<String, Integer>> data = new HashMap<>();
+        Random r = new Random();
+
+        for (int i = 0; i < 100; i++)
+        {
+            data.put(UUID.randomUUID().toString(), Pair.create(UUID.randomUUID().toString(), r.nextInt()));
+        }
+
+        ColumnFamilyStore store = loadData(data, true);
+        store.forceMajorCompaction();
+
+        try
+        {
+            // left holds table component sizes, right holds total size of index components (SI_*)
+            Pair<Long, Long> tableIndexSizes = takeSnapshotAndCheckComponents(store, snapshotName);
+            Map<String, Pair<Long, Long>> details = store.getSnapshotDetails();
+
+            // check that SASI components are included in the computation of snapshot size
+            Assert.assertEquals((long) details.get(""sasi_test"").right, tableIndexSizes.left + tableIndexSizes.right);
+        }
+        finally
+        {
+            store.clearSnapshot(""sasi_test"");
+        }
+    }
+
+    private Pair<Long, Long> takeSnapshotAndCheckComponents(ColumnFamilyStore cfs, String snapshotName) throws Throwable

Review comment:
       Do we need to have this in a separate method, with a single caller, two separate responsibilities and a composite return type? Also the test doesn't actually check the components, it only collects the sizes. I think that at least the snapshotting (`cfs.snapshot(snapshotName)`) could be done out of the method, and maybe the entire method could be inlined in the caller test. Anyway I don't have a strong opinion about this, just ignore the suggestion if you don't agree.

##########
File path: test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java
##########
@@ -137,7 +146,87 @@ public void cleanUp()
     }
 
     @Test
-    public void testSingleExpressionQueries()
+    public void testSASIComponentsAddedToSnapshot() throws Throwable

Review comment:
       Nice test :)




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Sep/21 16:48;githubbot;600","smiklosovic commented on a change in pull request #1150:
URL: https://github.com/apache/cassandra/pull/1150#discussion_r706380195



##########
File path: test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java
##########
@@ -137,7 +146,87 @@ public void cleanUp()
     }
 
     @Test
-    public void testSingleExpressionQueries()
+    public void testSASIComponentsAddedToSnapshot() throws Throwable
+    {
+        String snapshotName = ""sasi_test"";
+        Map<String, Pair<String, Integer>> data = new HashMap<>();
+        Random r = new Random();
+
+        for (int i = 0; i < 100; i++)
+        {
+            data.put(UUID.randomUUID().toString(), Pair.create(UUID.randomUUID().toString(), r.nextInt()));
+        }
+
+        ColumnFamilyStore store = loadData(data, true);
+        store.forceMajorCompaction();
+
+        try
+        {
+            // left holds table component sizes, right holds total size of index components (SI_*)
+            Pair<Long, Long> tableIndexSizes = takeSnapshotAndCheckComponents(store, snapshotName);
+            Map<String, Pair<Long, Long>> details = store.getSnapshotDetails();
+
+            // check that SASI components are included in the computation of snapshot size
+            Assert.assertEquals((long) details.get(""sasi_test"").right, tableIndexSizes.left + tableIndexSizes.right);
+        }
+        finally
+        {
+            store.clearSnapshot(""sasi_test"");
+        }
+    }
+
+    private Pair<Long, Long> takeSnapshotAndCheckComponents(ColumnFamilyStore cfs, String snapshotName) throws Throwable

Review comment:
       I can definitely inline it, I just think it is more obvious what it does when it is separated to another method. I am not sure about checking the components ... I am already checking that on line 211 that sasi components are all contained in the set of all components. Do you want me to actually check this on a file-name level? I guess that is overkill.




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;10/Sep/21 18:15;githubbot;600","adelapena commented on a change in pull request #1150:
URL: https://github.com/apache/cassandra/pull/1150#discussion_r707223698



##########
File path: test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java
##########
@@ -137,7 +146,80 @@ public void cleanUp()
     }
 
     @Test
-    public void testSingleExpressionQueries()
+    public void testSASIComponentsAddedToSnapshot() throws Throwable
+    {
+        String snapshotName = ""sasi_test"";
+        Map<String, Pair<String, Integer>> data = new HashMap<>();
+        Random r = new Random();
+
+        for (int i = 0; i < 100; i++)
+            data.put(UUID.randomUUID().toString(), Pair.create(UUID.randomUUID().toString(), r.nextInt()));
+
+        ColumnFamilyStore store = loadData(data, true);
+        store.forceMajorCompaction();
+
+        try
+        {
+            Set<SSTableReader> ssTableReaders = store.getLiveSSTables();
+            Set<Component> sasiComponents = new HashSet<>();
+
+            for (Index index : store.indexManager.listIndexes())
+                if (index instanceof SASIIndex)
+                    sasiComponents.add(((SASIIndex) index).getIndex().getComponent());
+
+            Assert.assertFalse(sasiComponents.isEmpty());

Review comment:
       Nit: this could be out of the try-finally block, so the first statement of the block is taking the snapshot




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Sep/21 10:56;githubbot;600","adelapena commented on a change in pull request #1150:
URL: https://github.com/apache/cassandra/pull/1150#discussion_r707227469



##########
File path: test/unit/org/apache/cassandra/index/sasi/SASIIndexTest.java
##########
@@ -137,7 +146,80 @@ public void cleanUp()
     }
 
     @Test
-    public void testSingleExpressionQueries()
+    public void testSASIComponentsAddedToSnapshot() throws Throwable
+    {
+        String snapshotName = ""sasi_test"";
+        Map<String, Pair<String, Integer>> data = new HashMap<>();
+        Random r = new Random();
+
+        for (int i = 0; i < 100; i++)
+            data.put(UUID.randomUUID().toString(), Pair.create(UUID.randomUUID().toString(), r.nextInt()));
+
+        ColumnFamilyStore store = loadData(data, true);
+        store.forceMajorCompaction();
+
+        try
+        {
+            Set<SSTableReader> ssTableReaders = store.getLiveSSTables();
+            Set<Component> sasiComponents = new HashSet<>();
+
+            for (Index index : store.indexManager.listIndexes())
+                if (index instanceof SASIIndex)
+                    sasiComponents.add(((SASIIndex) index).getIndex().getComponent());
+
+            Assert.assertFalse(sasiComponents.isEmpty());
+
+            store.snapshot(snapshotName);
+            JSONObject manifest = (JSONObject) new JSONParser().parse(new FileReader(store.getDirectories().getSnapshotManifestFile(snapshotName)));
+            JSONArray files = (JSONArray) manifest.get(""files"");
+
+            Assert.assertFalse(ssTableReaders.isEmpty());
+            Assert.assertFalse(files.isEmpty());
+            Assert.assertEquals(ssTableReaders.size(), files.size());
+
+            Map<Descriptor, Set<Component>> snapshotSSTables = store.getDirectories().sstableLister(Directories.OnTxnErr.IGNORE).snapshots(snapshotName).list();

Review comment:
       Nit: we could easily break this line to keep it under 120 chars:
   ```suggestion
               Map<Descriptor, Set<Component>> snapshotSSTables = store.getDirectories()
                                                                       .sstableLister(Directories.OnTxnErr.IGNORE)
                                                                       .snapshots(snapshotName).list();
   ```




-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;13/Sep/21 11:01;githubbot;600","smiklosovic closed pull request #1150:
URL: https://github.com/apache/cassandra/pull/1150


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Sep/21 20:31;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,3000,,,0,3000,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,stefan.miklosovic,,,,,,,,,,,,Correctness,,,,,,,,Normal,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Sep 16 12:30:36 UTC 2021,,,,,,,All,,,,,"0|z02vso:",9223372036854775807,,,,,,,adelapena,stefan.miklosovic,,,Normal,,3.4,,,https://github.com/apache/cassandra/commit/24b084fcf8ea64ccf117cd0e98310b1e1b40b6b8,,,,,,,,,unit test,,,,,"17/Aug/21 11:03;stefan.miklosovic;https://github.com/apache/cassandra/pull/1150;;;","17/Aug/21 11:07;stefan.miklosovic;[~adelapena] would you mind to take a look here too?

maybe [~ifesdjeen] or [~xedin] might look at this too as they were touching DataTracker mostly.;;;","26/Aug/21 10:12;stefan.miklosovic;After taking a deeper look, this change is not compatible with the other parts for the code base, for example, when this will be introduced in snaphots, if we ask what size that snapshot is of (on the disk), currently that logic parses ""Component""s but these 2i component files do not follow the naming convention the other components of an sstable do which would throw some exceptions (even I have not tried that empirically, it is quite obvious from the code).  We would have to figure out how to parse these and include them in the size computation.

This work is somehow related to CASSANDRA-16451 where we are refactoring snapshotting logic a lot. More to it, there is a size of a snapshot computed every single time one queries it which is absolutely not necessary and it become a performance issue when there is hundreds of snapshots and somebody lists them frequently via listsnapshots nodetool command. We plan to somehow cache this information and I think that after we do that, this will be the logical extension of that effort.

EDIT: it will not throw but the sizes of these components will be the most probably not included in the size computation which is clearly wrong.

EDIT 2: So it seems that these components are included in size computation after all, I have not dug so deeply initially. There is Component.Type enum on SI_ files ... Anyway, I would wait for 16451 regardless. I am not sure how one affects the other.;;;","08/Sep/21 11:42;stefan.miklosovic;I have checked that the computation of snapshot size takes into account SASI index components.

PR here [https://github.com/apache/cassandra/pull/1150]

On general positive feedback, I will prepare rest of the branches (4.0 and trunk).

Build for above PR is here: https://app.circleci.com/pipelines/github/instaclustr/cassandra/452/workflows/e997d1b1-e221-4a6b-b056-fec218b67eea;;;","10/Sep/21 17:46;adelapena;At first glance the approach looks good to me. There are still things that we have to do as part of CEP-7, such as including the index components in incremental backups and making the snapshot management implementation-agnostic, but I think that the proposed patch would give us reasonable snapshotting functionality for SASI in the meantime.

The method {{SSTable#addComponents}} seems idempotent, so I think that having SASI registering its components won't be a problem if we end up having a more generic mechanism to register index components based on some future {{Index#getComponents()}} method.

[~jasonstack]/[~maedhroz] any thoughts on this?

I have left a few very minor comments on the PR, and I wonder whether we should add a simple dtest taking and restoring an index snapshot, maybe [here|https://github.com/apache/cassandra-dtest/blob/trunk/snapshot_test.py]. That would be our first dtest for SASI ever.;;;","13/Sep/21 08:06;stefan.miklosovic;I reflected review comments in the PR (same branch).

build: [https://app.circleci.com/pipelines/github/instaclustr/cassandra/460/workflows/997e5c2b-1dc3-44a2-adfa-f733beb6db61]

on the final +1 and ideally after commentary from above-mentioned devs I would like to proceed to the actual merging.;;;","16/Sep/21 12:30;stefan.miklosovic;I ve asked [~maedhroz] privately if he is ok with this patch and he was.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource leak when queries apply a RowFilter,CASSANDRA-15126,13233198,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,14/May/19 10:05,15/May/19 14:30,13/Jul/23 08:38,14/May/19 14:49,3.0.19,,,,,CQL/Interpreter,,,,0,,,,"RowFilter.CQLFilter optionally removes those partitions that have no matching results, but fails to close the iterator representing that partition’s unfiltered results, leaking resources when this happens.",,aleksey,benedict,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13050,,,,,,CASSANDRA-6377,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Degradation -> Resource Management,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Tue May 14 14:49:34 UTC 2019,,,,,,,All,,,,,"0|z02o08:",9223372036854775807,,,,,,,aleksey,samt,,,Normal,,3.0.5,,,"[c07f3c88a4ba164bf01b0450b2463746b40c0d48|https://github.com/apache/cassandra/commit/c07f3c88a4ba164bf01b0450b2463746b40c0d48]",,,,,,,,,unit test included in patch,,,,,"14/May/19 10:43;benedict;Patch available [here|https://github.com/belliottsmith/cassandra/tree/15126-3.0], with unit test.

It turns out this was already fixed in 3.11 and trunk by [~Stefania] in CASSANDRA-13050, but I will merge up the unit test anyway.;;;","14/May/19 12:12;samt;+1 LGTM;;;","14/May/19 12:41;aleksey;+1;;;","14/May/19 14:49;benedict;Thanks, committed as [c07f3c88a4ba164bf01b0450b2463746b40c0d48|https://github.com/apache/cassandra/commit/c07f3c88a4ba164bf01b0450b2463746b40c0d48] and merged up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid keeping sstables marked compacting forever when user defined compaction gets interrupted,CASSANDRA-15123,13232653,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,10/May/19 12:14,15/May/20 08:05,13/Jul/23 08:38,07/Aug/19 08:28,3.11.5,4.0,4.0-alpha1,,,Local/Compaction,,,,0,,,,"When we have both repaired + unrepaired data on a node, we create multiple compaction tasks and run them serially. If one of those tasks gets interrupted or throws exception we will keep sstables in the other tasks as compacting forever.",,bdeggleston,jeromatron,marcuse,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Degradation -> Resource Management,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Wed Aug 07 08:28:16 UTC 2019,,,,,,,All,,,,,"0|z02knk:",9223372036854775807,,,,,,,bdeggleston,,,,Normal,,3.11.0,,,https://github.com/apache/cassandra/commit/b773bc7ac51fedc07145017edaefa919fac25696,,,,,,,,,adds unit test,,,,,"10/May/19 12:25;marcuse;this also affects token range compactions

[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...krummas:marcuse/15123-3.11], [circleci|https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15123-3.11]
[trunk|https://github.com/apache/cassandra/compare/trunk...krummas:marcuse/15123-trunk], [circleci|https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15123-trunk];;;","30/Jul/19 22:16;bdeggleston;For trunk, I made a few adjustments to the CompactionTaskCollection class in a branch [here|https://github.com/bdeggleston/cassandra/tree/marcuse/15123-trunk], let me know what you think. The main motivation was improving how empty collections are created / identified to eliminate potential problems with instances created with empty / null collections, but I also renamed the class to be more consistent with our other extended collection classes.

For 3.11, we should either catch Throwable, or always call {{LifecycleTransaction#close}} in a finally block since it’s a noop on committed and aborted txns (my preference).;;;","31/Jul/19 08:37;marcuse;thanks, cherry picked your commit and made the 3.11 change to just close in {{finally}}

tests running here: [3.11|https://circleci.com/workflow-run/157306a3-0874-44bb-889c-fe863be1bb69] [trunk|https://circleci.com/workflow-run/4d492a42-f9ce-4633-8dc0-10acdd004056];;;","31/Jul/19 17:42;bdeggleston;+1;;;","07/Aug/19 08:28;marcuse;and committed as [b773bc7ac51fedc07145017edaefa919fac25696|https://github.com/apache/cassandra/commit/b773bc7ac51fedc07145017edaefa919fac25696] and merged up to trunk, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes that join the ring while another node is MOVING build an invalid view of the token ring,CASSANDRA-15120,13232031,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,07/May/19 14:32,15/May/20 08:00,13/Jul/23 08:38,03/Jun/19 15:07,3.0.19,3.11.5,4.0,4.0-alpha1,,Cluster/Gossip,Cluster/Membership,,,0,,,,"Gossip only updates the token metadata for nodes in the NORMAL, SHUTDOWN or LEAVING* statuses.  MOVING and REMOVING_TOKEN nodes do not have their ring information updated (nor do others, but these other states _should_ only be taken by nodes that are not members of the ring).  

If a node missed the most recent token-modifying events because they were not a member of the ring when they happened (or because Gossip was delayed to them), they will retain an invalid view of the ring until the node enters the one of the NORMAL, SHUTDOWN or LEAVING states.

*LEAVING is populated differently, however, and in a probably unsafe manner that this work will also address.",,bdeggleston,benedict,jay.zhuang,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Correctness -> Consistency,,,,,,,,Challenging,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Mon Jun 03 15:40:01 UTC 2019,,,,,,,All,,,,,"0|z02guo:",9223372036854775807,,,,,,,samt,,,,Critical,,3.0.0,,,https://github.com/apache/cassandra/commit/e4b5d9818f003be2b9091c48f8435d29202ffe2d,,,,,,,,,regression test included,,,,,"07/May/19 14:44;benedict;An initial patch is available [here|https://github.com/belliottsmith/cassandra/tree/15120-3.0] for 3.0.  Some work is needed still to support switching the in-jvm dtest behaviour to disable/enable gossip and networking, as well as to port this capability to other versions of the dtests.

Since this work touches gossip, while the patch appears simple we need to take a great deal of care.  I have attempted to verify the state transitions that may precede one of these presently broken events, so that I have confidence the patch does not degrade gossip's correctness, but this can only be a best effort without a significant investment in evaluating the correctness of gossip more holistically.  I also intend to do another round of analysis before we commit the patch.;;;","14/May/19 17:26;samt;I share your concerns around making changes to this area in 3.0, but I've spent some time working through this and I believe that this change is correct. I haven't really looked at any of non-test changes made to support gossip/networking in the tests yet, but I'll make another pass at it when you're done with those.

 ;;;","15/May/19 12:20;benedict;Ok, I've pushed a version with feature flags introduced to the dtests, so that gossip is only enabled for those tests that need it.  Once you confirm it looks OK to you, I'll port it to the other versions (though I may defer _implementation_ of this capability on trunk until after CASSANDRA-15066, which already implements a lot of this);;;","21/May/19 15:18;samt;So everything looks reasonable to me, except for the changes to {{MessagingService::listen}}. I presume these were not intended to be left in the patch, and it seems to work fine with them reverted.

Nit: the comment on {{SEPW}} line 112 is no longer 100% correct as it isn't explicitly concerned about the shutdown state of the {{SEPE}} any more.

Also, it isn't new but the logging in the catch block of {{SEPW::run}} may not happen depending on the stability inspector.;;;","03/Jun/19 15:07;samt;Committed to 3.0 in {{e4b5d9818f003be2b9091c48f8435d29202ffe2d}} and merged to 3.11 and trunk. The trunk version does not include the dtest, as much of the plumbing to support that is going to land in CASSANDRA-15066. The change to 3.0 also breaks backward compatibility of the cross version dtests with 2.2. 

There are a couple of unrelated, but new-ish test failures.
In the unit tests, {{RowFilterTest}}, which was recently added for CASSANDRA-15126, appears to be failing on 3.11 only. 
In the dtests, {{jmx_auth_test.TestJMXAuth.test_basic_auth}} looks like it was broken by CASSANDRA-14305. 
I'll follow up all of these separately.
;;;","03/Jun/19 15:40;samt;bq. The trunk version does not include the dtest, as much of the plumbing to support that is going to land in CASSANDRA-15066.

CASSANDRA-15148

bq. The change to 3.0 also breaks backward compatibility of the cross version dtests with 2.2.

CASSANDRA-15147

bq. In the unit tests, RowFilterTest, which was recently added for CASSANDRA-15126, appears to be failing on 3.11 only. 

Ninja'd a fix in [7f4ecb0|https://github.com/apache/cassandra/commit/7f4ecb01247088e27ff2ae15aa5375805bada53a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flaky unit test AuditLoggerTest,CASSANDRA-15105,13230748,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,eperott,eperott,eperott,29/Apr/19 18:39,15/May/20 08:03,13/Jul/23 08:38,22/Jun/19 02:22,4.0,4.0-alpha1,,,,Legacy/CQL,,,,0,,,,Depending on execution order some tests will fail in the AuditLoggerTest class. Any test case that happens to execute after testExcludeSystemKeyspaces() will typically fail.,,eperott,sumanth.pasupuleti,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,eperott,sumanth.pasupuleti,,,,,,,,,,,Correctness -> Test Failure,,,,,,,,Low Hanging Fruit,Unit Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Sat Jun 22 02:21:01 UTC 2019,,,,,,,All,,,,,"0|z028xk:",9223372036854775807,,,,,,,sumanth.pasupuleti,vinaykumarcse,,,Low,,4.0,,,https://github.com/apache/cassandra/commit/225fa868884bdda1c20e0fcef61628eb6d941fbe,,,,,,,,,NA,,,,,"29/Apr/19 19:02;eperott;Issue is triggered by audit events collected in {{InMemoryAuditLogger}} which remain after the execution of {{testExcludeSystemKeyspaces()}}.

[Patch|https://github.com/eperott/cassandra/tree/15105-trunk]
[Unit Tests|https://circleci.com/gh/eperott/cassandra/53#queue-placeholder/containers/2];;;","10/May/19 20:34;vinaykumarcse;[~eperott] Thank you for the patch, it is next on my list, will probably get to it next week. ;;;","20/May/19 20:48;sumanth.pasupuleti;I've reviewed the patch. LGTM w.r.t. fix for {{AuditLoggerTest}} class. As I ran UTs multiple times with this patch, I noticed {{testExcludeSystemKeyspaces}} still [fails|https://circleci.com/gh/sumanth-pasupuleti/cassandra/508#tests/containers/14] due to events collected in {{InMemoryAuditLogger}}.
I did a scrub across the UTs to make sure we disable audit logger each time we enable, and consequently made a change to {{StorageServiceServerTest}} on top of [~eperott]'s patch. From my several (10) [runs|https://circleci.com/gh/sumanth-pasupuleti/workflows/cassandra/tree/15105_trunk_UT] of UTs, AuditLogger tests have been passing.

[Patch|https://github.com/apache/cassandra/pull/323]
[Passing Tests|https://circleci.com/workflow-run/7a96f12c-c695-4ca8-8bf6-36108bdaa75c];;;","23/May/19 07:41;eperott;[~sumanth.pasupuleti], thanks for taking the time to review and test.

When looking at the logs of the [failing|https://circleci.com/gh/sumanth-pasupuleti/cassandra/508#tests/containers/14] test I don't think that run was affected by the setup in {{StorageServiceServerTest}} as these were the only test classes executed in that particular container:
{noformat}
org/apache/cassandra/audit/AuditLoggerTest.java
org/apache/cassandra/db/commitlog/CommitLogFailurePolicyTest.java
org/apache/cassandra/dht/RandomPartitionerTest.java
org/apache/cassandra/locator/NetworkTopologyStrategyTest.java
org/apache/cassandra/service/NativeTransportServiceTest.java
org/apache/cassandra/utils/NativeLibraryTest.java
{noformat}
Still, I think that disabling the audit logger whenever it has been enabled is a good strategy.

But regarding the, still flaky, {{testExcludeSystemKeyspaces}}. I was able to reproduce the remaining issue consistently by adding a a few seconds delay in the test case just before checking the size of the {{InMemoryAuditLogger.inMemQueue}}. When printing the content I get this:
{noformat}
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36856|timestamp:1558592418723|type:SELECT|category:QUERY|ks:system_schema|scope:keyspaces|operation:SELECT * FROM system_schema.keyspaces
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36848|timestamp:1558592418722|type:SELECT|category:QUERY|ks:system_schema|scope:keyspaces|operation:SELECT * FROM system_schema.keyspaces
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36852|timestamp:1558592418723|type:SELECT|category:QUERY|ks:system_schema|scope:keyspaces|operation:SELECT * FROM system_schema.keyspaces
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36856|timestamp:1558592418732|type:SELECT|category:QUERY|ks:system_schema|scope:types|operation:SELECT * FROM system_schema.types
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36848|timestamp:1558592418732|type:SELECT|category:QUERY|ks:system_schema|scope:types|operation:SELECT * FROM system_schema.types
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36852|timestamp:1558592418732|type:SELECT|category:QUERY|ks:system_schema|scope:types|operation:SELECT * FROM system_schema.types
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36852|timestamp:1558592418736|type:SELECT|category:QUERY|ks:system_schema|scope:tables|operation:SELECT * FROM system_schema.tables
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36848|timestamp:1558592418736|type:SELECT|category:QUERY|ks:system_schema|scope:tables|operation:SELECT * FROM system_schema.tables
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36856|timestamp:1558592418735|type:SELECT|category:QUERY|ks:system_schema|scope:tables|operation:SELECT * FROM system_schema.tables
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36848|timestamp:1558592418751|type:SELECT|category:QUERY|ks:system_schema|scope:columns|operation:SELECT * FROM system_schema.columns
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36852|timestamp:1558592418750|type:SELECT|category:QUERY|ks:system_schema|scope:columns|operation:SELECT * FROM system_schema.columns
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36856|timestamp:1558592418752|type:SELECT|category:QUERY|ks:system_schema|scope:columns|operation:SELECT * FROM system_schema.columns
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36848|timestamp:1558592418766|type:SELECT|category:QUERY|ks:system_schema|scope:indexes|operation:SELECT * FROM system_schema.indexes
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36852|timestamp:1558592418767|type:SELECT|category:QUERY|ks:system_schema|scope:indexes|operation:SELECT * FROM system_schema.indexes
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36856|timestamp:1558592418767|type:SELECT|category:QUERY|ks:system_schema|scope:indexes|operation:SELECT * FROM system_schema.indexes
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36848|timestamp:1558592418767|type:SELECT|category:QUERY|ks:system_schema|scope:views|operation:SELECT * FROM system_schema.views
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36852|timestamp:1558592418768|type:SELECT|category:QUERY|ks:system_schema|scope:views|operation:SELECT * FROM system_schema.views
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36856|timestamp:1558592418768|type:SELECT|category:QUERY|ks:system_schema|scope:views|operation:SELECT * FROM system_schema.views
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36848|timestamp:1558592418769|type:SELECT|category:QUERY|ks:system_schema|scope:functions|operation:SELECT * FROM system_schema.functions
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36852|timestamp:1558592418769|type:SELECT|category:QUERY|ks:system_schema|scope:functions|operation:SELECT * FROM system_schema.functions
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36848|timestamp:1558592418770|type:SELECT|category:QUERY|ks:system_schema|scope:aggregates|operation:SELECT * FROM system_schema.aggregates
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36856|timestamp:1558592418770|type:SELECT|category:QUERY|ks:system_schema|scope:functions|operation:SELECT * FROM system_schema.functions
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36852|timestamp:1558592418771|type:SELECT|category:QUERY|ks:system_schema|scope:aggregates|operation:SELECT * FROM system_schema.aggregates
user:anonymous|host:127.0.0.1:7010|source:/127.0.0.1|port:36856|timestamp:1558592418771|type:SELECT|category:QUERY|ks:system_schema|scope:aggregates|operation:SELECT * FROM system_schema.aggregates
{noformat}
Some further testing indicates that other test cases were vulnerable to this as well, all depending on execution order and timing. I believe that the queries we see come from the control connection for each of the sessions in {{CQLTester}}, a few for each protocol version.

So, created another patch, making sure to exclude system keyspaces where it makes sense. This is a detail we overlooked when working on CASSANDRA-14498.

Not sure what's the best way to get my latest patch on top of yours. Since you created a PR, perhaps you could merge my latest patch on to your branch?;;;","23/May/19 08:41;sumanth.pasupuleti;[~eperott] Thanks for the latest patch; I have merged your patch on my branch.

[Patch|https://github.com/apache/cassandra/pull/323];;;","01/Jun/19 08:06;vinaykumarcse;Thanks for the patch [~eperott], [~sumanth.pasupuleti] for review and changes.

We might need to exclude {{system_virtual_schema}} also from {{AuditLoggerTest::testIncludeSystemKeyspaces()}} and {{AuditLoggerTest::testExcludeSystemKeyspaces}} methods for future readiness when and if {{system_virtual_schema}} is also quried from control connection, this also keeps us in sync with defaults behavior. I pushed this change in my brnach [here|https://github.com/vinaykumarchella/cassandra/commits/CASSANDRA-15105]. I will commit this as soon [tests|https://circleci.com/gh/vinaykumarchella/workflows/cassandra/tree/CASSANDRA-15105] pass.;;;","02/Jun/19 19:01;eperott;bq. We might need to exclude system_virtual_schema also

+1;;;","22/Jun/19 02:21;vinaykumarcse;Thanks for the patch [~eperott], [~sumanth.pasupuleti] for review and changes.

Following 2 [tests|https://circleci.com/gh/vinaykumarchella/cassandra/414#tests/containers/48] have failed which seems to flaky on the trunk and not related to this change, these tests also passed on subsequent runs ([unit tests|https://circleci.com/gh/vinaykumarchella/cassandra/423#tests/containers/39])
{code:java}
testSendSmall - org.apache.cassandra.net.ConnectionTest
testSendLarge - org.apache.cassandra.net.ConnectionTest
{code}
Committed as [225fa868884bdda1c20e0fcef61628eb6d941fbe|https://github.com/apache/cassandra/commit/225fa868884bdda1c20e0fcef61628eb6d941fbe] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid updating unchanged gossip state,CASSANDRA-15097,13229722,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jay.zhuang,jay.zhuang,jay.zhuang,23/Apr/19 22:12,15/May/20 08:00,13/Jul/23 08:38,19/Jul/19 04:50,3.0.19,3.11.5,4.0,4.0-alpha1,,Cluster/Gossip,,,,0,,,,"The node might get unchanged gossip states, the state might be just updated after sending a GOSSIP_SYN, then it will get the state that is already up to date. If the heartbeat in the GOSSIP_ACK message is updated, it will unnecessary re-apply the same state again, which could be costly like updating token change.
It's very likely to happen for large cluster when a node startup, as the first gossip message will sync all endpoints tokens, it could take some time (in our case about 200 seconds), during that time, it keeps gossip with other node and get the full token states. Which causes lots of pending gossip tasks.",,jay.zhuang,jeromatron,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jay.zhuang,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Fri Jul 19 04:50:43 UTC 2019,,,,,,,All,,,,,"0|z022m0:",9223372036854775807,,,,,,,samt,,,,Low,,3.0.0,,,"[3f70e7c72c703bc323b169a28e8754ce67d4e479|https://github.com/apache/cassandra/commit/3f70e7c72c703bc323b169a28e8754ce67d4e479]",,,,,,,,,Unittest is passed. And the code is committed and running in Instagram production environment.,,,,,"23/Apr/19 23:00;jay.zhuang;Here is a patch to filter out updated states:
| Branch | uTest |
| [15097-3.0|https://github.com/cooldoger/cassandra/tree/15097-3.0] | [!https://circleci.com/gh/cooldoger/cassandra/tree/15097-3.0.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/15097-3.0] |
| [15097-3.11|https://github.com/cooldoger/cassandra/tree/15097-3.11] | [!https://circleci.com/gh/cooldoger/cassandra/tree/15097-3.11.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/15097-3.11] |
| [15097-trunk|https://github.com/cooldoger/cassandra/tree/15097-trunk] | [!https://circleci.com/gh/cooldoger/cassandra/tree/15097-trunk.svg?style=svg!|https://circleci.com/gh/cooldoger/cassandra/tree/15097-trunk] |;;;","12/Jul/19 12:23;samt;Thanks [~jay.zhuang], this looks like a reasonable change to me. It does need rebasing though as a couple of other changes touching {{Gossiper}} have landed recently. If you take care of that I'll re-run the CI with the HIRES config.
;;;","15/Jul/19 17:28;jay.zhuang;Thanks [~samt]. The patch is rebased and tests are passed in circleci:
| Branch | uTest (circleci) |
| [15097-3.0|https://github.com/instagram/cassandra/tree/15097-3.0] | [pass|https://circleci.com/gh/Instagram/cassandra/tree/15097-3.0] |
| [15097-3.11|https://github.com/instagram/cassandra/tree/15097-3.11] | [pass|https://circleci.com/gh/Instagram/cassandra/tree/15097-3.11] |
| [15097-trunk|https://github.com/instagram/cassandra/tree/15097-trunk] | [pass|https://circleci.com/gh/Instagram/cassandra/tree/15097-trunk] |;;;","16/Jul/19 19:30;jay.zhuang;Add dTest results:
| Branch | uTest | jvm-dTest | dTest | dTest vnode |
| [15097-3.0|https://github.com/instagram/cassandra/tree/15097-3.0] | [#66 passed |https://circleci.com/gh/Instagram/cassandra/66] | [#67 passed|https://circleci.com/gh/Instagram/cassandra/67] | [#75 failed|https://circleci.com/gh/Instagram/cassandra/75], passed locally: CASSANDRA-14595 | [#74 failed | https://circleci.com/gh/Instagram/cassandra/74], passed locally: CASSANDRA-14595 |
| [15097-3.11|https://github.com/instagram/cassandra/tree/15097-3.11] | [#69 passed|https://circleci.com/gh/Instagram/cassandra/69] | [#68 passed|https://circleci.com/gh/Instagram/cassandra/68] | [#77 failed|https://circleci.com/gh/Instagram/cassandra/77], passed locally: CASSANDRA-14595 | [#76 failed|https://circleci.com/gh/Instagram/cassandra/76], passed locally: CASSANDRA-14595 |
| [15097-trunk|https://github.com/instagram/cassandra/tree/15097-trunk] | [#72 passed|https://circleci.com/gh/Instagram/cassandra/72] | [#73 passed|https://circleci.com/gh/Instagram/cassandra/73] | [#78 passed|https://circleci.com/gh/Instagram/cassandra/78] | [#79 failed|https://circleci.com/gh/Instagram/cassandra/79], passed locally|
All failed dtests are passed locally with 10x run:
{{$ pytest --count=10 --cassandra-dir=~/cassandra $TESTS}};;;","17/Jul/19 14:19;samt;+1;;;","19/Jul/19 04:50;jay.zhuang;Thanks [~samt] for the review. Committed as [3f70e7c|https://github.com/apache/cassandra/commit/3f70e7c72c703bc323b169a28e8754ce67d4e479].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CassandraNetworkAuthorizer::authorize should get role details from Roles, not directly from IRoleManager",CASSANDRA-15089,13228558,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,17/Apr/19 10:04,15/May/20 08:05,13/Jul/23 08:38,11/Jul/19 16:17,4.0,4.0-alpha1,,,,Feature/Authorization,,,,0,,,,"If the network permissions cache doesn't contain any entry for a role, the authorize method is invoked on the configured INetworkAuthorizer. In the case of CassandraNetworkAuthorizer, this immediately checks whether the role in question has the LOGIN privilege set. It does this using the configured IRoleManager directly, which causes a read from the underlying table in system_auth. It should fetch the flag from Roles::canLogin, which uses the RolesCache, falling back to the IRoleManager if necessary.",,bdeggleston,fengshen,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Low Hanging Fruit,Code Inspection,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu Jul 11 16:17:52 UTC 2019,,,,,,,All,,,,,"0|z01vhc:",9223372036854775807,,,,,,,bdeggleston,,,,Low,,4.0,,,https://github.com/apache/cassandra/commit/149caf01e08f58f306ff51379ab189c7a4b1ca6d,,,,,,,,,Additional unit test added.,,,,,"17/Apr/19 10:08;samt;||branch||CI||
|[15089-trunk|https://github.com/beobal/cassandra/tree/15089-trunk]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F15089-trunk]|;;;","17/Apr/19 11:54;samt;The patch breaks a dtest which was relying on the LOGIN privilege not being cached.

||dtest PR||CI||
|[15089|https://github.com/apache/cassandra-dtest/pull/50]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F15089-trunk];;;","17/Apr/19 21:22;bdeggleston;+1;;;","11/Jul/19 16:17;samt;Thanks, committed to trunk in {{149caf01e08f58f306ff51379ab189c7a4b1ca6d}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Illegal column names make legacy sstables unreadable in 3.0/3.x,CASSANDRA-15086,13228339,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,16/Apr/19 12:59,23/May/19 22:51,13/Jul/23 08:38,08/May/19 09:11,3.0.19,3.11.5,,,,Local/SSTable,,,,0,,,,"CASSANDRA-10608 adds extra validation when decoding a bytebuffer representing a legacy cellname. If the table is not COMPACT and the column name component of the cellname refers to a primary key column, an IllegalArgumentException is thrown. It looks like the original intent of 10608 was to prevent Thrift writes from inserting these invalid cells, but the same code path is exercised on the read path. The problem is that this kind of cells may exist in pre-3.0 sstables, either due to Thrift writes or through side loading of externally generated SSTables. Following an upgrade to 3.0, these partitions become unreadable, breaking both the read and compaction paths (and so also upgradesstables). Scrub in 2.1 does not help here as it blindly reproduces the invalid cells.",,aleksey,cam1982,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,Availability -> Response Crash,,,,,,,,Normal,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,None,,Thu May 23 22:51:51 UTC 2019,,,,,,,All,,,,,"0|z01u4g:",9223372036854775807,,,,,,,aleksey,,,,Normal,,3.0.0,,,,,,,,,,,,Add additional test to LegacySSTableTest. Run upgrade tests and compare against baseline 3.0/3.11 runs,,,,,"16/Apr/19 13:08;samt;This changes the behaviour from throwing IllegalArgumentException to throw a new IllegalLegacyColumnException when such a cell is encountered which allows the exception handling to be dependent on the context. On the read path, including compaction reads of SSTables, we can safely skip the illegal atoms, which is equivalent to the pre-3.0 behaviour. In the thrift layer, we can forcefully reject any writes or reads with filters or predicates containing illegal cells.

The bulk of the patch is mechanical changes to extend either method signatures or catch blocks to accommodate the new exception. 

||branch||CI||
|[15086-3.0|https://github.com/beobal/cassandra/tree/15086-3.0]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F15086-3.0]|
|[15086-3.11|https://github.com/beobal/cassandra/tree/15086-3.11]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F15086-3.11]|;;;","07/May/19 17:01;aleksey;Yup, +1.;;;","08/May/19 09:11;samt;Thanks, committed to 3.0 in {{daf6c85b0530fb45ad1d8433efa0c5e13d7e182c}} and merged to 3.11 and trunk (with {{-s ours}});;;","18/May/19 23:51;cam1982;This appears to be a duplicate of the issue I reported in CASSANDRA-15081 . Since your patch also covers Thrift and is committed can someone triage the issue I reported.;;;","20/May/19 09:35;samt;[~cam1982], it's not really a duplicate, in this issue the columns are illegal in 2.1 as well as 3.0+, but 3.0 has no mechanism to handle them at all (scrub cannot fix the sstables containing these cells). On the other hand, the issue in 15081 can be corrected by adding the missing column to the schema metadata in \{{system_schema.dropped_columns}}. Once this is done, those tables can be read without issue, and even without the correct missing column defs, {{sstablescrub}} can recover the rest of the data.;;;","23/May/19 22:51;cam1982;[~samt] appears I misread the patch on 15086. I thought I detected an overlap of code change. My bad.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LegacyLayout does not have same behavior as 2.x when handling unknown column names,CASSANDRA-15081,13226602,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,High,Fixed,cam1982,cam1982,cam1982,08/Apr/19 02:51,22/Oct/20 04:07,13/Jul/23 08:38,19/Oct/19 14:35,3.11.6,,,,,Legacy/Local Write-Read Paths,,,,0,patch,pull-request-available,,"Due to a bug I haven't been able to reproduce the production cluster had unknown column names. To replicate the issue for this test I did the following:
{noformat}
$ ccm create -v 2.1.19 -n 1 -s bug
$ cat > schema.cql << 'EOF'
CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes = true;
CREATE TABLE test.unknowntest (id int primary key, payload text, ""paylo!d"" text);
EOF
$ ccm node1 cqlsh -f schema.cql
$ export CASSANDRA_INCLUDE=~/.ccm/bug/node1/bin/cassandra.in.sh
$ cat > bug.json << 'EOF'
[
{""key"": ""1"",
""cells"": [["""","""",1554432501209207],
[""paylo!d"",""hello world"",1554432501209207],
[""payload"",""hello world"",1554432501209207]]}
]
EOF
$ ~/.ccm/repository/2.1.19/tools/bin/json2sstable -K test -c unknowntest ~/bug.json ~/.ccm/bug/node1/data0/test/unknowntest-<cfid>/test-unknowntest-ka-1-Data.db{noformat}
Then test the behavior of unknown columns in 2.1:
{noformat}
$ ccm stop
$ ccm create -v 2.1.19 -n 1 -s bug2_1_19
$ cat > schema2.cql << 'EOF'
CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'} AND durable_writes = true;
CREATE TABLE test.unknowntest (id int primary key, payload text);
EOF
$ ccm node1 cqlsh -f schema2.cql
$ ccm stop
$ cp ~/.ccm/bug/node1/data0/test/unknowntest-<cfid>/test-unknowntest-ka-1-* ~/.ccm/bug2_1_19/node1/data0/test/unknowntest-<cfid>/
$ ccm start
$ ccm node1 cqlsh
Connected to bug2_1_19 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 2.1.19 | CQL spec 3.2.1 | Native protocol v3]
Use HELP for help.
cqlsh> select * from test.unknowntest where id = 1;

id | payload
----+-------------
1 | hello world

(1 rows){noformat}
Compared to 3.11.4 which did the following:
{noformat}
$ ccm stop
$ ccm create -v 3.11.4 -n 1 -s bug3_11_4
$ ccm node1 cqlsh -f schema2.cql
$ ccm stop
$ cp ~/.ccm/bug/node1/data0/test/unknowntest-<cfid>/test-unknowntest-ka-1-* ~/.ccm/bug3_11_4/node1/data0/test/unknowntest-<cfid>/
$ ccm start
$ ccm node1 cqlsh
Connected to bug3_11_4 at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.4 | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> select * from test.unknowntest where id = 1;
ReadFailure: Error from server: code=1300 [Replica(s) failed to execute read] message=""Operation failed - received 0 responses and 1 failures"" info={'failures': 1, 'received_responses': 0, 'required_responses': 1, 'consistency': 'ONE'}
{noformat}
In the logs this resulted in an IllegalStateException from LegacyLayout line 1127

The expected behavior would be to ignore the column and return results the same as in 2.1",,baylanger,cam1982,jeromatron,jtgalbraith,mck,,,,,,,,,,,,,,,,,,,,,,,"grom358 commented on pull request #373: CASSANDRA-15081
URL: https://github.com/apache/cassandra/pull/373
 
 
   https://issues.apache.org/jira/browse/CASSANDRA-15081
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;01/Nov/19 05:37;githubbot;600","grom358 closed pull request #373:
URL: https://github.com/apache/cassandra/pull/373


   


----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;22/Oct/20 04:07;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,1200,,,0,1200,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13939,CASSANDRA-11018,,,,,,,,,,,,"08/Apr/19 02:55;cam1982;15081.patch;https://issues.apache.org/jira/secure/attachment/12965151/15081.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,cam1982,,,,,,,,,,,,Degradation -> Performance Bug/Regression,,,,,,,,Low Hanging Fruit,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 22 18:36:25 UTC 2019,,,,,,,,,,,,"0|z01jh4:",9223372036854775807,,,,,,,mck,,,,Normal,,3.11.2,,,https://github.com/apache/cassandra/commit/31a86f891b00ec0db20fcef4919dce63be7bf31d,,,,,,,,,"reproducible manual ccm script
unit test?",,,,,"08/Apr/19 02:54;cam1982;[^15081.patch]

I don't believe this patch causes regression for https://issues.apache.org/jira/browse/CASSANDRA-13939
{noformat}
Connected to bug_patch at 127.0.0.1:9042.
[cqlsh 5.0.1 | Cassandra 3.11.4-SNAPSHOT | CQL spec 3.4.4 | Native protocol v4]
Use HELP for help.
cqlsh> select * from test.unknowntest where id = 1;

id | payload
----+-------------
1 | hello world

(1 rows){noformat}
Can see the patch fixes this bug.;;;","19/Oct/19 19:49;mck;||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[cassandra-3.11_15081|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/cassandra-3.11_15081]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_15081]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/62//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/62/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/700//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/700]|

can we write a unit or dtest for this?;;;","01/Nov/19 05:40;cam1982;Unit test created https://github.com/apache/cassandra/pull/373/commits;;;","03/Nov/19 09:43;mck;Thanks [~cam1982]. 

||branch||circleci||asf jenkins tests||asf jenkins dtests||
|[cassandra-3.11_15081|https://github.com/apache/cassandra/compare/trunk...instaclustr:3.11-15081]|[circleci|https://circleci.com/gh/instaclustr/workflows/cassandra/tree/3.11-15081]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/27//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-pipeline/27/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/700//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/700]|

The tests in the pipeline (non-dtests) look ok compared to the 3.11 branch. (Most of the failures there are in the first ""test"" stage.) The same goes for the dtests compared against 3.11.

I'm double-checking how this effects the scenario described in CASSANDRA-13939 when cells are skipped and the deserializer's correction to the file pointer for reading the next row. As we're now treating legacy unknown columns as such skipped cells as well. Any input on this [~cam1982]?

Also, I'm changing the ""since version"" field, since the bug (in its current form) only existed from 3.11.2, after CASSANDRA-13939, when the AssertionFailedError was changed to an IllegalStateException.;;;","06/Nov/19 04:25;cam1982;[~mck] no input. As far as I could tell CASSANDRA-13939 shouldn't be affected by this, but to be honest I didn't fully understand that issue. I thought I mentioned it in just in case it might and someone more knowledgable might be able to injected if they see an issue. The unit tests passed so hoping that means I haven't broken anything elsewhere.;;;","06/Nov/19 06:36;mck;Thanks [~cam1982]. I believe you're correct. But it needs to be checked. There's an upgrade dtest relevant to this, I will check it out and get back to you.;;;","15/Nov/19 15:40;mck;We have no CI currently that runs [upgrade tests|https://github.com/apache/cassandra-dtest/tree/master/upgrade_tests] (unless you have a paid circleci account).

To run the upgrade test in question i did the following (first time i've ever used the upgrade dtests locally).
{code}
# $cassandra_src needs to be a git clone, checked out to patched branch

cd cassandra-dtest
source ~/dtest/bin/activate
export LOCAL_GIT_REPO=$cassandra_src

python -m pytest --execute-upgrade-tests --cassandra-dir=$cassandra_src upgrade_tests/storage_engine_upgrade_test.py::TestStorageEngineUpgrade::test_update_and_drop_column

python -m pytest  --execute-upgrade-tests --cassandra-dir=$cassandra_src -s --keep-test-dir   upgrade_tests/storage_engine_upgrade_test.py::TestBootstrapAfterUpgrade::test_update_and_drop_column

# when done
deactivate
{code}
;;;","15/Nov/19 18:07;mck;Committed as 31a86f891b00ec0db20fcef4919dce63be7bf31d;;;","22/Nov/19 18:36;baylanger;Title of this issue points to 2.x but comments refer mainly to 2.1.

 

Does this relate or not to 2.2 as well?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SELECT JSON generates invalid JSON for the duration type,CASSANDRA-15075,13225798,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,penberg,penberg,penberg,03/Apr/19 12:38,15/May/20 08:38,13/Jul/23 08:38,28/Nov/19 07:59,3.11.6,4.0,4.0-alpha3,,,CQL/Syntax,,,,0,,,,"Currently, Apache Cassandra generates invalid JSON for the ""duration"" type.

cqlsh> CREATE KEYSPACE ks1 WITH REPLICATION = \{ 'class' : 'SimpleStrategy', 'replication_factor' : 1 };
 cqlsh> CREATE TABLE ks1.data (id int, d duration, PRIMARY KEY (id));

cqlsh> INSERT INTO ks1.data (id, d) VALUES (1, 6h40m);
 cqlsh> SELECT JSON d FROM ks1.data WHERE id = 1;

[json]
 --------------
 \{""d"": 6h40m}

That is, the duration is not quoted and is therefore invalid according to [https://jsonlint.com/,] for example.

 

Fix the problem by quoting the formatted duration type properly:

cqlsh> INSERT INTO ks1.data (id, d) VALUES (1, 6h40m);
 cqlsh> SELECT JSON d FROM ks1.data WHERE id = 1;

[json]
 ----------------
 \{""d"": ""6h40m""}

(1 rows)

 

The problem is fixed by the following patch:

[^0001-Fix-SELECT-JSON-formatting-for-the-duration-type.patch]",,jeromatron,marcuse,penberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/19 12:36;penberg;0001-Fix-SELECT-JSON-formatting-for-the-duration-type.patch;https://issues.apache.org/jira/secure/attachment/12964696/0001-Fix-SELECT-JSON-formatting-for-the-duration-type.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,penberg,,,,,,,,,,,,Correctness,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 28 07:59:34 UTC 2019,,,,,,,,,,,,"0|z01esw:",9223372036854775807,,,,,,,marcuse,,,,Normal,,3.11.0,,,https://github.com/apache/cassandra/commit/5d930cc9db6cdb29c2f7f1dec5a03c5b30ab66a7,,,,,,,,,add new test,,,,,"11/Nov/19 14:03;marcuse;Thanks for the patch and sorry for the delay

* We could probably just remove the {{toJSONString}} override from {{DurationType}} so that the AbstractType method is used?
* The {{json_test.py}} dtest should test {{DurationType}} as well

I totally understand if you have moved on from this - if that is the case I'll make the changes;;;","11/Nov/19 14:12;penberg;Hi Marcus,

I no longer have the development environment, so if you can do the adjustments to the fix yourself, that would be much appreciated!

- Pekka;;;","27/Nov/19 14:45;marcuse;pushed a [small update and a test|https://github.com/krummas/cassandra/commits/marcuse/15075] with tests running [here|https://circleci.com/workflow-run/242ed896-419d-4f61-9585-2c674168422c]

does it look good to you [~penberg]?

It was way too painful to update json_test.py, so I just added a small roundtrip unit test;;;","27/Nov/19 16:16;penberg;Looks good to me!;;;","28/Nov/19 07:59;marcuse;And committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incomplete range results during 2.X -> 3.11.4 upgrade,CASSANDRA-15072,13225337,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,High,Fixed,bdeggleston,muir,muir,01/Apr/19 17:35,24/Apr/19 22:39,13/Jul/23 08:38,24/Apr/19 22:39,3.0.19,3.11.5,,,,Legacy/Coordination,,,,1,,,,"Hello

During an upgrade from 2.1.17 to 3.11.4, our application starting getting back incomplete results for range queries. When all nodes were upgraded (before upgrading sstables), we stopped getting incomplete results. I was able to reproduce it and listed steps below. It seems to require the random partitioner and compact storage to reproduce reliably. It also reproduces coming from 2.1.21 and 2.2.14. You seem to get the bad behavior when an old node is your coordinator and it has to talk to an upgraded replica.
{noformat}
ccm create test -v 2.1.17 -n 3
ccm updateconf 'partitioner: org.apache.cassandra.dht.RandomPartitioner'
ccm node1 updateconf 'initial_token: 0'
ccm node2 updateconf 'initial_token: 56713727820156410577229101238628035242'
ccm node3 updateconf 'initial_token: 113427455640312821154458202477256070484'
ccm start

ccm node1 cqlsh <<SCHEMA
CREATE KEYSPACE test WITH REPLICATION = {'class': 'SimpleStrategy', 'replication_factor': 3};
CREATE COLUMNFAMILY test.test (
  id text,
  foo text,
  bar text,
  PRIMARY KEY (id)
) WITH COMPACT STORAGE;
CONSISTENCY QUORUM;
INSERT INTO test.test (id, foo, bar) values ('1', 'hi', 'there');
INSERT INTO test.test (id, foo, bar) values ('2', 'hi', 'there');
SCHEMA

ccm node1 stop
ccm node1 setdir -v 3.11.4
ccm node1 start

ccm node2 stop
ccm node2 setdir -v 3.11.4
ccm node2 start

# here I use 3.X cqlsh to connect to 2.X node so I can lower the page size (to
# allow for simpler test setup)
cqlsh 127.0.0.3 <<QUERY
CONSISTENCY QUORUM;
PAGING 2;
select * from test.test;
QUERY
{noformat}
This results in:
{noformat}
Page size: 2

 id | bar   | foo
----+-------+-----
  2 | there |  hi

(1 rows)
{noformat}
Running it against the upgraded node (node1):
{noformat}
Page size: 2

 id | bar   | foo
----+-------+-----
  2 | there |  hi
  1 | there |  hi

(2 rows)
{noformat}",,bdeggleston,cnlwsu,cscotta,eriksw,jjirsa,marcuse,mck,muir,psanford,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/19 01:52;eriksw;eriksw-repro.sh;https://issues.apache.org/jira/secure/attachment/12964507/eriksw-repro.sh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,bdeggleston,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 24 22:39:17 UTC 2019,,,,,,,,,,,,"0|z01bzk:",9223372036854775807,,,,,,,samt,,,,Normal,,3.0.0,,,,,,,,,,,,circleci / in jvm upgrade dtests,,,,,"01/Apr/19 20:55;bdeggleston;Are you seeing incomplete results like this in a real cluster? If so, what consistency level are you reading and writing at?

The ccm script you have here _does_ return incomplete results, but it’s also writing and reading at CL ONE (the cqlsh default), so that’s not unexpected. I modified the script here to read and write at QUORUM, and haven't gotten any incomplete results.;;;","01/Apr/19 21:49;muir;Yes, we saw a lot of incomplete results in a real cluster. We read and write at quorum.

Oops, you are right about my repro. I modified the steps to reproduce it at quorum (I upgraded two out of three nodes instead of just one, changed the reads/writes to be quorum, and connected to node 3 to perform the reproduction query).;;;","01/Apr/19 21:54;muir;It seems with my updated steps that only the first query against node3 reproduces it. After that it returns both rows. If you restart node3, it reproduces it again for one query. This is not the behavior we experienced in production (i.e. the problem did not go away). I wonder if I have actually reproduced our issue or not...;;;","02/Apr/19 00:30;eriksw;Muir's colleague here:

I get 100% reproducibility for repeated queries with the following changes:
 # Create the keyspace with replication_factor 2
 # Do the inserts with CONSISTENCY ALL
 # Upgrade the two nodes that contain data (node1, node3); keep the node that does not contain any sstables for test.test (node2) back at 2.1.17

After those steps, I get full results 100% of the time when querying node1 and node3, and truncated results 100% of the time when querying node2.

This is using cqlsh as packaged with 3.11.4.

 ;;;","02/Apr/19 00:47;bdeggleston;Ok, I can repro your issue with the updated script. It looks like you’re hitting a commit log bug that was introduced in 2.1 and fixed in 3.0 (CASSANDRA-13987) If you drain node 1 & 2 before shutting them down, this should stop happening.

I’d also expected putting a sleep larger than the commit log sync interval before shutting down node 1 would fix the problem, but it didn’t. I’m still looking at why that is.

When you say:
{quote}When all nodes were upgraded (before upgrading sstables), we stopped getting incomplete results
{quote}
do you mean data you'd inserted before the upgrade reappeared?;;;","02/Apr/19 02:08;eriksw;Please see the attached [^eriksw-repro.sh], which includes aggressive flushing, draining, and deleting commit logs when stopped to ensure they play no part. With these steps, the truncated results behavior when querying node2 (the un-upgraded node) is 100% reproducible for me for an unlimited number of queries with CONSISTENCY ALL.
{quote}do you mean data you'd inserted before the upgrade reappeared?
{quote}
Yes. After the last node was upgraded to Cassandra 3.11.4, we no longer saw truncated results regardless of which node we queried. All data that should have been in the results was correctly returned for all queries after that point.;;;","02/Apr/19 18:39;bdeggleston;This is a great repro script, thanks. 

A couple of observations:
 * test.test has 2 columns, and uses compact storage, which shouldn’t be possible
 * node1 & node3 are the replicas of the missing partition (we’re querying from the un-upgraded node2, for those following along).
 * doing a point read ({{select * from test.test where id=‘1’;}}) returns the expected partition
 * using LIMIT 2 instead of PAGING 2 has the same problem
 * LIMIT 3 returns a partial row: {{1 | there | null}}
 * LIMIT 4 returns the entire row: {{1 | there |  hi}}

Tables with compact storage can only have a single column, so you shouldn’t be able to create a compact storage table with 2 columns. Instead of throwing an error though, it seems like it just silently treats the table as a normal table. This might be why no one has noticed that our ddl validation is broken.

It looks like the mixed mode read path is treating the table as a proper compact storage table though, and treating each cell as a row, which is why you see partial rows start to appear as you increase the limit. If you remove compact storage from the ddl, or only use a single column, everything works normally.

I'll think on the best way to address this.;;;","02/Apr/19 19:07;psanford;{quote}Tables with compact storage can only have a single column, so you shouldn’t be able to create a compact storage table with 2 columns.
{quote}
According to [http://cassandra.apache.org/doc/latest/cql/ddl.html] that restriction is only for tables with clustering columns:
{quote}if a compact table has at least one clustering column, then it must have exactly one column outside of the primary key ones.
{quote}
We have a lot of tables created from thrift (compact storage) that do not have clustering columns and have > 1 column in the CQL schema.;;;","02/Apr/19 19:20;muir;[https://docs.datastax.com/en/cql/3.3/cql/cql_using/useCompactStorage.html] also explicitly states the implied inverse:
{quote}
A compact table with a primary key that is not compound can have multiple columns that are not part of the primary key.
{quote};;;","02/Apr/19 20:07;bdeggleston;Huh, I did not know that. I guess that makes sense though. So then this is just an upgrade bug.;;;","02/Apr/19 20:40;muir;Thanks for helping us investigate this issue. Do you think you understand the exact cause at this point?

{quote}It looks like the mixed mode read path is treating the table as a proper compact storage table though, and treating each cell as a row
{quote}
Does ""mixed mode"" refer to the mixed 2.X <=> 3.X cassandra versions?

From a high level it sounds like a 2.X coordinator and a 3.X replica have some confusion regarding compact storage cells vs. rows, and how many are needed to satisfy a limit or page quota. Is that still what you think is going on?;;;","02/Apr/19 22:15;bdeggleston;No problem. Yes mixed mode just means you're upgrading your cluster.

I don't know the exact cause, but you've summarized what I think is probably happening. Specifically the legacy read path on the 3.0 nodes is probably always interpreting single cells as rows for compact storage tables, even ones without clustering columns.;;;","04/Apr/19 23:28;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/15072-3.0]|[tests|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15072-3.0]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/15072-3.11]|[tests|https://circleci.com/workflow-run/4567dbed-be97-49e5-8c82-66e320e074ca]|

[~beobal] do you have time to review this? It seems to be related to CASSANDRA-11087.

A few notes:
 * From what I can tell, returning a row per cell is the right thing to do in this case, so I'm using a modified result counter to only going entire partitions in this specific case. However, I'm not familiar enough with all the dark corners of the 2.1 storage engine to be sure that's appropriate, or won't break something else.
 * Doing a point read with the partition key also returns a row per cell, but works correctly because the 2.2 coordinator seems to just discard the limit in that case.
 * If you're not familiar with the in-jvm dtests yet, and want to run the one in this patch, you'll want to run {{ant dtest-jar}} on this branch and [this 2.2 branch|https://github.com/bdeggleston/cassandra/tree/15078-2.2], and put the 2.2 dtest jar in the 3.0 build directory.
 * -CircleCI seems to be behind picking up new branches to test, but I'll update this with links to the workflows once it catches up.-;;;","05/Apr/19 07:34;samt;[~bdeggleston] sure, I'll review asap;;;","05/Apr/19 15:16;samt;This looks safe to me wrt to ""the dark corners"" as the new counter is only used in this very specific use case, so if the CI looks good I'm +1 on the patch.  ;;;","24/Apr/19 22:39;bdeggleston;Committed to 3.0 as [d27c3ad0d2d006a5f156f0a2f2a24286d31c5069|https://github.com/apache/cassandra/commit/d27c3ad0d2d006a5f156f0a2f2a24286d31c5069] and merged up. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper#markAlive can race with Gossiper#markDead,CASSANDRA-15059,13223180,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bdeggleston,bdeggleston,bdeggleston,21/Mar/19 19:59,15/May/20 08:00,13/Jul/23 08:38,25/Apr/19 17:28,3.0.19,3.11.5,4.0,4.0-alpha1,,Cluster/Gossip,,,,0,,,,"The Gossiper class is not threadsafe and assumes all state changes happen in a single thread (the gossip stage). Gossiper#convict, however, can be called from the GossipTasks thread. This creates a race where calls to Gossiper#markAlive and Gossiper#markDead can interleave, corrupting gossip state. Gossiper#assassinateEndpoint has a similar problem, being called from the mbean server thread.",,aleksey,aweisberg,bdeggleston,cscotta,jay.zhuang,jeromatron,n.v.harikrishna,tcooke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,bdeggleston,,,,,,,,,,,,Degradation -> Other Exception,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 25 17:28:26 UTC 2019,,,,,,,,,,,,"0|z00yrs:",9223372036854775807,,,,,,,aweisberg,,,,Normal,,3.0.0,,,,,,,,,,,,circleci runs looks good,,,,,"22/Mar/19 20:51;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/15059-3.0]|[circle|https://circleci.com/workflow-run/a863b2af-db01-4f7a-b059-a0ed2496f286]|
|[3.11|https://github.com/bdeggleston/cassandra/tree/15059-3.11]|[circle|https://circleci.com/workflow-run/c271d33b-92c4-4579-80ce-cab20f4aac6c]|
|[trunk|https://github.com/bdeggleston/cassandra/tree/15059-trunk]|[circle|https://circleci.com/workflow-run/d476de41-c374-40f1-bdae-b464149b703a]|;;;","25/Mar/19 22:49;aweisberg;[You don't need to create a ListenableFutureTask you can just {{submit}} the {{Runnable}} to the {{ExecutorService}} and it will return a {{Future}}|https://github.com/apache/cassandra/compare/cassandra-3.0...bdeggleston:15059-3.0?expand=1#diff-8be666c70553b1f0017a01458c490f47R332].
[This creates significant risk of deadlock if the Gossiper ends up needing to acquire something owned by the thread submitting the task. Document that risk here.|https://github.com/apache/cassandra/compare/cassandra-3.0...bdeggleston:15059-3.0?expand=1#diff-8be666c70553b1f0017a01458c490f47R323]

This is extremely fragile. {{Gossiper.markDead}} calls out to every subscriber. Also we call into {{Gossiper.convict}} from {{FailureDetector}} but then call back out to {{FailureDetector}} from {{Gossiper}}. I am only just beginning to track down where we call in and out.

{{Gossiper.assassinate}} also calls out to various things via {{Gossiper.handleMajorStateChange}}.

I am not 100% on the right way to tease this out to be a good design. If this action is occurring asynchronously then it should be explicit and return a ListenableFuture that is either ignored if it can really occur asynchronously without issue, waited on safely while not holding any locks, or execution resumed later by attaching a listener.

The fan in and fan out is high enough it's going to take some time to for me to figure out if the proposed change is safe.;;;","25/Mar/19 22:50;aweisberg;Also if these methods are supposed to be called from only one thread that should be enforced at compile time? You can also use runtime assertions to validate the correct thread is invoking it.;;;","27/Mar/19 16:13;aweisberg;Finished my review.

[Should this always wrap in AssertionError?|https://github.com/apache/cassandra/compare/cassandra-3.0...bdeggleston:15059-3.0?expand=1#diff-8be666c70553b1f0017a01458c490f47R340]. I get that InterruptedException is probably an Error, but ExecutionException is just a run of the mill exception. It's not super important as it won't have a functional difference.

I think this fixes the problem it sets out to fix. I checked the call hierarchy for {{Gossiper.runInGossipStageBlocking}} and I think that every caller is not holding any locks or resources that might stop the Gossip thread from making progress. All of them don't appear to be holding anything obvious.

Documentation wise definitely document {{runInGossipStageBlocking}}, {{assassinate}}, and {{convict}} warning about the risk of deadlock if the Gossip thread ends up needing to acquire a resource via a listener (or FD's listener if there is a path for that) the calling thread holds.

Future wise this doesn't do anything to address the underlying fragility in how Gossiper doesn't document what is safe to call from outside the Gossip thread and what isn't. It also doesn't validate the correct thread is running a given method.

I think we want to add two (or three?) interfaces that Gossiper can implement. One is for methods that are non-mutating and safe to call from any thread. The other is for methods for that should only be called from the Gossip stage thread. And maybe a third which contains methods that mutate, but block on the Gossip stage. So {{Gossiper.instance}} would go away and you would have a reference to the Gossiper via one of the 2-3 interfaces.

Additionally any method that should only run in Gossip stage interface should have a {{Preconditions.checkState}} checking that it's actually running in the Gossip stage.

WDYT?;;;","27/Mar/19 17:30;bdeggleston;{quote}
Future wise this doesn't do anything to address the underlying fragility in how Gossiper doesn't document what is safe to call from outside the Gossip thread and what isn't. It also doesn't validate the correct thread is running a given method.
I think we want to add two (or three?) interfaces that Gossiper can implement. One is for methods that are non-mutating and safe to call from any thread. The other is for methods for that should only be called from the Gossip stage thread. And maybe a third which contains methods that mutate, but block on the Gossip stage. So Gossiper.instance would go away and you would have a reference to the Gossiper via one of the 2-3 interfaces.
Additionally any method that should only run in Gossip stage interface should have a Preconditions.checkState checking that it's actually running in the Gossip stage.
{quote}

Do you mean as part of this ticket, or as future improvements to gossip?;;;","27/Mar/19 18:53;aweisberg;Well as part of this ticket. There is a question of scope. Yes this fixes the bug, but it arguably doesn't address the reason the bug exists.;;;","08/Apr/19 20:35;bdeggleston;{quote}
Future wise this doesn't do anything to address the underlying fragility in how Gossiper doesn't document what is safe to call from outside the Gossip thread and what isn't. It also doesn't validate the correct thread is running a given method.
{quote}

I’ve been thinking about this a lot, and I think it would be safer if we didn’t do this.

Adding some preconditions isn’t going to fix the underlying fragility of Gossiper. Given the “realities” of the Gossiper class, I think it would end up causing more harm that good. Just starting to pull on that thread reveals at least one situation where we modify gossip state out of the gossip stage that makes sense (on startup). There are probably one or two more (at least), and I’d hate to break a nodetool command or something.;;;","09/Apr/19 03:23;aweisberg;It will and it won't fix it. We will know really quickly if it doesn't work because it's going to fail whatever it was doing. By learning the exceptions we will be able to evaluate whether they are correct.

Major changes like preconditions would be 4.0 only so if we break it we at least won't be breaking it in production anywhere.

Making the API less fragile is also helps reduce the surface area for people to use Gossiper incorrectly. I don't see why we shouldn't do that.;;;","09/Apr/19 16:58;bdeggleston;I think both of these changes are worth considering, but I don’t think would be appropriate to include them as part of this ticket. They are both out of scope and too risky to be putting in 3.x, and I’d argue the same for 4.0 at this point as well.

Gossiper is very brittle, critical to the operation of a cluster, and has little to no test coverage. Any bugs that aren't caught by a manual review or raise any red flags in the dtests will become production issues at some point.;;;","11/Apr/19 16:02;bdeggleston;Maybe we could compromise a bit here. There’s definitely value in rooting out places where we violate the assumptions gossiper makes about concurrency, but I worry that if we don’t catch all of them we'll turn a race that _may_ cause a problem into a hard failure. What if we logged an error by default, but threw an exception if a system property was set? This way we can get feedback and scary messages in the logs, but we haven't made things any less stable, and our tests will fail if we’re doing something we shouldn’t be. I have no problem putting that in 3.x and up.

I would like to punt the refactor to 4.next though. I do think it’s valuable, but I think it’s a bit late in the game to add it to 4.0.

WDYT?;;;","12/Apr/19 21:19;aweisberg;I think that's a good way to resolve the concerns with destabilizing 4.0 while still getting the benefit of doing more runtime checking.

Maybe we want to create a single property (not specific to Gossip?)  for this class of conditional logging or assertion. That way when we invoke the tests we don't have to specify multiple properties to specify whether we want fail fast or logging behavior if we want this elsewhere.

 I would be +1 if we did that.

I agree the refactor is a bit of an ask for 4.0, but I don't see it as a refactor really. It's all type system stuff with no actual changes to what methods are invoked and where. It's really just bucketing every method in Gossiper into a interfaces and instead of having code use Gossiper.instance we would have it use the appropriate interface reference. That is pretty low risk although the LOC churn would not be small.;;;","18/Apr/19 23:59;bdeggleston;I've updated my branches with the runtime checking, could you take a look? The checks discovered a few more places where we were mutating Gossip state from the wrong thread, which I fixed.;;;","22/Apr/19 17:54;bdeggleston;I opened CASSANDRA-15095 as a follow on JIRA.;;;","22/Apr/19 20:10;aweisberg;Great!

{{quarantineEndpoint}} and {{replacementQuarantine}} are private, but maybe check there as well? I don't feel strongly about it, but it's slightly safer when they are called indirectly.

{{assassinateEndpoint}} asserts on the thread inside the lambda for run in Gossip stage. Harmless, but is it too much?

{{notifyFailureDetector}} seems like it could tolerate having this assertion since it is called from VerbHandlers in the gossip stage?;;;","22/Apr/19 20:18;aweisberg;Oops, hit add early.

{{applyNewStates}} is also private, but maybe check the assertion there?;;;","22/Apr/19 21:00;bdeggleston;bq. quarantineEndpoint and replacementQuarantine are private, but maybe check there as well? I don't feel strongly about it, but it's slightly safer when they are called indirectly.

{{quarantineEndpoint}} (and by extension, {{replacementQuarantine}}) modifies the {{justRemovedEndpoints}} map. This map is also modified in the GossipTask thread, and is done correctly as far as I can tell. That's why I didn't add the assertion there.

bq. assassinateEndpoint asserts on the thread inside the lambda for run in Gossip stage. Harmless, but is it too much?

Probably overkill, removed.

bq. notifyFailureDetector seems like it could tolerate having this assertion since it is called from VerbHandlers in the gossip stage?

Nothing happens in that method that we'd need to assert the thread for. I could see adding an assertion with the interface refactor, but I wouldn't want make noise in people's logs if we're not actually doing anything worth complaining about.

bq. applyNewStates is also private, but maybe check the assertion there?

Same as above

I also switched to using the NoSpamLogger for the assertion.;;;","22/Apr/19 21:10;aweisberg;+1 with one nit. For NoSpamLogger use the fully formatted exception as the key so that different traces are rate limited independently.;;;","25/Apr/19 17:28;bdeggleston;Committed to 3.0 as [c3ce32e239b1ba41faf1d58a942465b9bf45b986|https://github.com/apache/cassandra/commit/c3ce32e239b1ba41faf1d58a942465b9bf45b986] and merged up. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid double counting read latencies for digest queries,CASSANDRA-15058,13222622,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,19/Mar/19 15:45,15/May/20 07:59,13/Jul/23 08:38,26/Mar/19 12:26,3.0.19,3.11.5,4.0,4.0-alpha1,,Observability/Metrics,,,,0,,,,We are closing the {{UnfilteredPartitionIterator}} wrapped with {{withMetricsRecording}} twice when we get digest requests - closing it calls {{onClose}} and that makes the metrics update twice.,,jeromatron,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Low Hanging Fruit,Workload Replay,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 26 12:25:57 UTC 2019,,,,,,,,,,,,"0|z00vbs:",9223372036854775807,,,,,,,samt,,,,Normal,,3.0 alpha 1,,,,,,,,,,,,A distributed in-jvm test was added to make sure we don't overcount,,,,,"20/Mar/19 09:31;marcuse;attaching patch to not close the iterator after digesting it;

[15058-3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...krummas:marcuse/15058-3.0]
[15058-3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...krummas:marcuse/15058-3.11]
[15058-trunk|https://github.com/apache/cassandra/compare/trunk...krummas:marcuse/15058-trunk]

https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15058-3.0
https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15058-3.11
https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15058-trunk;;;","20/Mar/19 16:15;samt;+1;;;","25/Mar/19 08:18;marcuse;And committed as {{78776d3534fc9c748ac6999c5b44882fc312a07c}} to 3.0 and merged up, thanks!;;;","26/Mar/19 12:25;marcuse;reopen to make sure this is properly closed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SimpleClient should pass connection properties as options,CASSANDRA-15056,13222115,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sumanth.pasupuleti,sumanth.pasupuleti,sumanth.pasupuleti,17/Mar/19 01:38,16/Mar/22 12:12,13/Jul/23 08:38,15/May/20 13:48,4.0,4.0-alpha1,,,,Messaging/Client,Test/unit,,,0,pull-request-available,,,"As of today. SimpleClient does not pass CHECKSUM or COMPRESSION as options to the server. In order for these parameters to take effect on the server side, they should be passed as options.",,jmckenzie,samt,sumanth.pasupuleti,,,,,,,,,,,,,,,,,,,,,,,,,"sumanth-pasupuleti commented on pull request #305: CASSANDRA-15056: Passing connection properties as options from SimpleClient
URL: https://github.com/apache/cassandra/pull/305
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/19 06:31;githubbot;600","sumanth-pasupuleti commented on pull request #305: CASSANDRA-15056: Passing connection properties as options from SimpleClient
URL: https://github.com/apache/cassandra/pull/305
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/19 06:33;githubbot;600","sumanth-pasupuleti commented on pull request #306: CASSANDRA-15056: Passing checksum and compressor as options from SimpleClient
URL: https://github.com/apache/cassandra/pull/306
 
 
   
 
----------------------------------------------------------------
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.
 
For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;17/Mar/19 06:36;githubbot;600","smiklosovic closed pull request #306:
URL: https://github.com/apache/cassandra/pull/306


   


-- 
This is an automated message from the Apache Git Service.
To respond to the message, please log on to GitHub and use the
URL above to go to the specific comment.

To unsubscribe, e-mail: pr-unsubscribe@cassandra.apache.org

For queries about this service, please contact Infrastructure at:
users@infra.apache.org
;16/Mar/22 12:12;githubbot;600",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0,2400,,,0,2400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,sumanth.pasupuleti,,,,,,,,,,,,Availability,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 15 13:48:23 UTC 2020,,,,,,,,,,,,"0|z00s74:",9223372036854775807,,,,,,,samt,,,,Normal,,4.0,,,,,,,,,,,,No existing usages are affected by this. Unit and dtest runs to confirm no regressions from fix.,,,,,"21/Mar/19 09:28;samt;Thanks [~sumanth.pasupuleti], that's an oversight on my part from CASSANDRA-13304. Fortunately, all the users of {{SimpleClient}} (which are unit tests and stress) are configured to use neither compression nor checksums. The exception to this is the command line client used by the debug-cql script, which is correctly setting the options.

CircleCI run here: https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F15056-trunk (the dtest failures here are unrelated and a PR is up for one of them)
;;;","21/Mar/19 09:29;samt;Committed to trunk in {{e191aff385053bdb5325f15bc6d16d2dc0ee0589}};;;","15/May/20 13:48;jmckenzie;Some how status: resolved w/Resolution: Unresolved. Re-opening to fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix handling FS errors on writing and reading flat files - LogTransaction and hints,CASSANDRA-15053,13221891,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jjirsa,jjirsa,aleksey,15/Mar/19 12:03,15/May/20 08:05,13/Jul/23 08:38,27/Mar/19 14:50,3.0.19,3.11.5,4.0,4.0-alpha1,,Consistency/Hints,,,,0,,,,We currently fail to handle and propagate IO errors when dealing with transaction log and hints.  It's trivial to fix this behaviour to ensure that disk failure policy is properly invoked in error scenarios.,,aleksey,aweisberg,bdeggleston,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13485,,,,,CASSANDRA-15426,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jjirsa,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 14:49:57 UTC 2019,,,,,,,,,,,,"0|z00qtc:",9223372036854775807,,,,,,,aweisberg,bdeggleston,marcuse,,Normal,,3.0.0,,,,,,,,,,,,NA,,,,,"15/Mar/19 13:48;aleksey;Branches: [3.0|https://github.com/iamaleksey/cassandra/tree/15053-3.0], [3.11|https://github.com/iamaleksey/cassandra/tree/15053-3.11], [trunk|https://github.com/iamaleksey/cassandra/tree/15053-4.0]; CI: [3.0|https://circleci.com/workflow-run/60adf75c-2eb8-4ae6-bf6e-28c0ce6b16ea], [3.11|https://circleci.com/workflow-run/911b0a0a-935f-4172-b110-a92a32993906], [trunk|https://circleci.com/workflow-run/04cd6f87-1567-48f0-8bbc-d52029fa4efc].;;;","15/Mar/19 13:59;aweisberg;+1;;;","15/Mar/19 14:08;marcuse;+1;;;","15/Mar/19 16:22;bdeggleston;+1;;;","27/Mar/19 14:49;aleksey;Committed as [ba325955463076987eca476309043fcdd59dad7c|https://github.com/apache/cassandra/commit/ba325955463076987eca476309043fcdd59dad7c] to 3.0 and merged upwards, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JDK7 compatibility broken in cassandra-2.2 by CASSANDRA-14821,CASSANDRA-15050,13221222,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,etedpet,mshuler,mshuler,12/Mar/19 21:05,25/Nov/20 12:08,13/Jul/23 08:38,07/Jun/19 14:09,2.2.15,,,,,Build,,,,0,,,,"CASSANDRA-14821 introduced some errors on JDK7. The build fails as follows:
{noformat}
(cassandra-2.2)mshuler@mana:~/git/cassandra$ java -version
java version ""1.7.0_80""
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)

(cassandra-2.2)mshuler@mana:~/git/cassandra$ ant jar
Buildfile: /home/mshuler/git/cassandra/build.xml

init:
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/main
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/thrift
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/lib
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/classes
   [mkdir] Created dir: /home/mshuler/git/cassandra/src/gen-java
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/lib
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco/partials

maven-ant-tasks-localrepo:
    [copy] Copying 1 file to /home/mshuler/git/cassandra/build

maven-ant-tasks-download:

maven-ant-tasks-init:

maven-declare-dependencies:

maven-ant-tasks-retrieve-build:
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies.xml
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies-sources.xml
    [copy] Copying 65 files to /home/mshuler/git/cassandra/build/lib/jars
    [copy] Copying 41 files to /home/mshuler/git/cassandra/build/lib/sources
    [copy] Copying 25 files to /home/mshuler/git/cassandra/build/lib/jars
   [unzip] Expanding: /home/mshuler/git/cassandra/build/lib/jars/org.jacoco.agent-0.7.5.201505241946.jar into /home/mshu
ler/git/cassandra/build/lib/jars

check-gen-cql3-grammar:

gen-cql3-grammar:
    [echo] Building Grammar /home/mshuler/git/cassandra/src/java/org/apache/cassandra/cql3/Cql.g  ...

generate-cql-html:

build-project:
    [echo] apache-cassandra: /home/mshuler/git/cassandra/build.xml
   [javac] Compiling 45 source files to /home/mshuler/git/cassandra/build/classes/thrift
   [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.openjdk.jmh.generators.Benchmark
Processor' less than -source '1.7'
   [javac] Note: /home/mshuler/git/cassandra/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java uses o
r overrides a deprecated API.
   [javac] Note: Recompile with -Xlint:deprecation for details.
   [javac] Note: Some input files use unchecked or unsafe operations.
   [javac] Note: Recompile with -Xlint:unchecked for details.
   [javac] 1 warning
   [javac] Compiling 1171 source files to /home/mshuler/git/cassandra/build/classes/main
   [javac] Note: Processing compiler hints annotations
   [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.openjdk.jmh.generators.Benchmark
Processor' less than -source '1.7'
   [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/MBeanWrapper.java:22: error: package java.uti
l.function does not exist
   [javac] import java.util.function.Consumer;
   [javac]                          ^
   [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/MBeanWrapper.java:203: error: cannot find sym
bol
   [javac]         private Consumer<Exception> handler;
   [javac]                 ^
   [javac]   symbol:   class Consumer
   [javac]   location: class OnException
   [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/MBeanWrapper.java:204: error: cannot find sym
bol
   [javac]         OnException(Consumer<Exception> handler)
   [javac]                     ^
   [javac]   symbol:   class Consumer
   [javac]   location: class OnException
   [javac] Note: Processing compiler hints annotations
   [javac] Note: Writing compiler command file at META-INF/hotspot_compiler
   [javac] Note: Done processing compiler hints annotations
   [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/MBeanWrapper.java:22: error: package java.uti
l.function does not exist
   [javac] import java.util.function.Consumer;
   [javac]                          ^
   [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/MBeanWrapper.java:203: error: cannot find sym
bol
   [javac]         private Consumer<Exception> handler;
   [javac]                 ^
   [javac]   symbol:   class Consumer
   [javac]   location: class OnException
   [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/MBeanWrapper.java:204: error: cannot find sym
bol
   [javac]         OnException(Consumer<Exception> handler)
   [javac]                     ^
   [javac]   symbol:   class Consumer
   [javac]   location: class OnException
   [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/MBeanWrapper.java:181: error: cannot find sym
bol
   [javac]         THROW(new Consumer<Exception>()
   [javac]                   ^
   [javac]   symbol:   class Consumer
   [javac]   location: class OnException
   [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/MBeanWrapper.java:188: error: cannot find sym
bol
   [javac]         LOG(new Consumer<Exception>()
   [javac]                 ^
   [javac]   symbol:   class Consumer
   [javac]   location: class OnException
   [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/MBeanWrapper.java:195: error: cannot find sym
bol
   [javac]         IGNORE(new Consumer<Exception>()
   [javac]                    ^
   [javac]   symbol:   class Consumer
   [javac]   location: class OnException
   [javac] Note: Some input files use or override a deprecated API.
   [javac] Note: Recompile with -Xlint:deprecation for details.
   [javac] Note: Some input files use unchecked or unsafe operations.
   [javac] Note: Recompile with -Xlint:unchecked for details.
   [javac] 6 errors
   [javac] 1 warning

BUILD FAILED
/home/mshuler/git/cassandra/build.xml:832: Compile failed; see the compiler error output for details.

Total time: 42 seconds
{noformat}
cc: [~ifesdjeen], [~benedict], [~djoshi3]",,benedict,djoshi,eperott,etedpet,jeromatron,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14821,,CASSANDRA-15809,,,,,,,,,,,CASSANDRA-16300,,,,,,,,,,,,,"16/Apr/19 13:51;etedpet;15050-2.2.txt;https://issues.apache.org/jira/secure/attachment/12966089/15050-2.2.txt","06/Jun/19 17:10;mshuler;15050-2.2_v2.txt;https://issues.apache.org/jira/secure/attachment/12971089/15050-2.2_v2.txt","06/Jun/19 14:10;mshuler;cassandra-2.2_ci.png;https://issues.apache.org/jira/secure/attachment/12971068/cassandra-2.2_ci.png",,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,etedpet,,,,,,,,,,,,Correctness,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 07 14:09:26 UTC 2019,,,,,,,,,,,,"0|z00mog:",9223372036854775807,,,,,,,mshuler,,,,Normal,,2.2.14,,,https://github.com/apache/cassandra/commit/a9a4f171be6c7d33ce43d04a9bc2dd20f3fd82a5,,,,,,,,,.,,,,,"12/Mar/19 21:18;djoshi;Can you add the exact steps to reproduce this? {{Consumer}} is available starting Java 8 and looks like your issue is in 2.2 backport.;;;","12/Mar/19 21:30;mshuler;While using JDK7, repro is:
{noformat}
ant jar{noformat}
Alternatively, build the branch on JDK8 and attempt to run on JDK7 (which is where this came from - user of JDK7 (by company policy) emailed me directly, since I'm the packaging guy with my name and email in the deb changelog). Repro is:
{noformat}
./bin/cassandra{noformat}
Foreground log from attempting to run on JDK7 (after build on JDK8):
{noformat}
(cassandra-2.2)mshuler@mana:~/git/cassandra$ java -version
java version ""1.7.0_80""
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)

(cassandra-2.2)mshuler@mana:~/git/cassandra$ ./bin/cassandra -f
Java HotSpot(TM) 64-Bit Server VM warning: Cannot open file ./bin/../logs/gc.log due to No such file or directory

CompilerOracle: inline org/apache/cassandra/db/AbstractNativeCell.compareTo (Lorg/apache/cassandra/db/composites/Composit
e;)I
CompilerOracle: inline org/apache/cassandra/db/composites/AbstractSimpleCellNameType.compareUnsigned (Lorg/apache/cassand
ra/db/composites/Composite;Lorg/apache/cassandra/db/composites/Composite;)I
CompilerOracle: inline org/apache/cassandra/io/util/Memory.checkBounds (JJ)V
CompilerOracle: inline org/apache/cassandra/io/util/SafeMemory.checkBounds (JJ)V
CompilerOracle: inline org/apache/cassandra/utils/AsymmetricOrdering.selectBoundary (Lorg/apache/cassandra/utils/Asymmetr
icOrdering/Op;II)I
CompilerOracle: inline org/apache/cassandra/utils/AsymmetricOrdering.strictnessOfLessThan (Lorg/apache/cassandra/utils/As
ymmetricOrdering/Op;)I
CompilerOracle: inline org/apache/cassandra/utils/ByteBufferUtil.compare (Ljava/nio/ByteBuffer;[B)I
CompilerOracle: inline org/apache/cassandra/utils/ByteBufferUtil.compare ([BLjava/nio/ByteBuffer;)I
CompilerOracle: inline org/apache/cassandra/utils/ByteBufferUtil.compareUnsigned (Ljava/nio/ByteBuffer;Ljava/nio/ByteBuff
er;)I
CompilerOracle: inline org/apache/cassandra/utils/FastByteOperations$UnsafeOperations.compareTo (Ljava/lang/Object;JILjav
a/lang/Object;JI)I
CompilerOracle: inline org/apache/cassandra/utils/FastByteOperations$UnsafeOperations.compareTo (Ljava/lang/Object;JILjav
a/nio/ByteBuffer;)I
CompilerOracle: inline org/apache/cassandra/utils/FastByteOperations$UnsafeOperations.compareTo (Ljava/nio/ByteBuffer;Lja
va/nio/ByteBuffer;)I
INFO  21:25:06 Node configuration:[authenticator=AllowAllAuthenticator; authorizer=AllowAllAuthorizer; auto_bootstrap=tru
e; auto_snapshot=true; batch_size_fail_threshold_in_kb=50; batch_size_warn_threshold_in_kb=5; batchlog_replay_throttle_in
_kb=1024; broadcast_address=null; broadcast_rpc_address=null; cas_contention_timeout_in_ms=1000; client_encryption_option
s=<REDACTED>; cluster_name=Test Cluster; column_index_size_in_kb=64; commit_failure_policy=stop; commitlog_compression=nu
ll; commitlog_directory=null; commitlog_max_compression_buffers_in_pool=3; commitlog_periodic_queue_size=-1; commitlog_se
gment_size_in_mb=32; commitlog_sync=periodic; commitlog_sync_batch_window_in_ms=null; commitlog_sync_period_in_ms=10000;
commitlog_total_space_in_mb=null; compaction_large_partition_warning_threshold_mb=100; compaction_throughput_mb_per_sec=1
6; concurrent_compactors=null; concurrent_counter_writes=32; concurrent_reads=32; concurrent_replicates=null; concurrent_
writes=32; counter_cache_keys_to_save=2147483647; counter_cache_save_period=7200; counter_cache_size_in_mb=null; counter_
write_request_timeout_in_ms=5000; cross_node_timeout=false; data_file_directories=null; disk_access_mode=auto; disk_failu
re_policy=stop; dynamic_snitch=true; dynamic_snitch_badness_threshold=0.1; dynamic_snitch_reset_interval_in_ms=600000; dy
namic_snitch_update_interval_in_ms=100; enable_user_defined_functions=false; encryption_options=null; endpoint_snitch=Sim
pleSnitch; file_cache_size_in_mb=null; gc_log_threshold_in_ms=200; gc_warn_threshold_in_ms=0; hinted_handoff_enabled=true
; hinted_handoff_enabled_by_dc=[]; hinted_handoff_enabled_global=true; hinted_handoff_throttle_in_kb=1024; incremental_ba
ckups=false; index_interval=null; index_summary_capacity_in_mb=null; index_summary_resize_interval_in_minutes=60; initial
_token=null; inter_dc_stream_throughput_outbound_megabits_per_sec=200; inter_dc_tcp_nodelay=false; internode_authenticato
r=null; internode_compression=all; internode_recv_buff_size_in_bytes=null; internode_send_buff_size_in_bytes=null; key_ca
che_keys_to_save=2147483647; key_cache_save_period=14400; key_cache_size_in_mb=null; listen_address=localhost; listen_int
erface=null; listen_interface_prefer_ipv6=false; listen_on_broadcast_address=false; max_hint_window_in_ms=10800000; max_h
ints_delivery_threads=2; max_streaming_retries=3; memory_allocator=null; memtable_allocation_type=heap_buffers; memtable_
cleanup_threshold=null; memtable_flush_writers=null; memtable_heap_space_in_mb=null; memtable_offheap_space_in_mb=null; n
ative_transport_max_concurrent_connections=-1; native_transport_max_concurrent_connections_per_ip=-1; native_transport_ma
x_frame_size_in_mb=256; native_transport_max_threads=128; native_transport_port=9042; num_tokens=256; otc_coalescing_enou
gh_coalesced_messages=8; otc_coalescing_strategy=TIMEHORIZON; otc_coalescing_window_us=200; partitioner=org.apache.cassan
dra.dht.Murmur3Partitioner; permissions_cache_max_entries=1000; permissions_update_interval_in_ms=-1; permissions_validit
y_in_ms=2000; phi_convict_threshold=8.0; range_request_timeout_in_ms=10000; read_request_timeout_in_ms=5000; request_sche
duler=org.apache.cassandra.scheduler.NoScheduler; request_scheduler_id=null; request_scheduler_options=null; request_time
out_in_ms=10000; role_manager=CassandraRoleManager; roles_cache_max_entries=1000; roles_update_interval_in_ms=-1; roles_v
alidity_in_ms=2000; row_cache_class_name=org.apache.cassandra.cache.OHCProvider; row_cache_keys_to_save=2147483647; row_c
ache_save_period=0; row_cache_size_in_mb=0; rpc_address=localhost; rpc_interface=null; rpc_interface_prefer_ipv6=false; r
pc_keepalive=true; rpc_listen_backlog=50; rpc_max_threads=2147483647; rpc_min_threads=16; rpc_port=9160; rpc_recv_buff_si
ze_in_bytes=null; rpc_send_buff_size_in_bytes=null; rpc_server_type=sync; saved_caches_directory=null; seed_provider=org.
apache.cassandra.locator.SimpleSeedProvider{seeds=127.0.0.1}; server_encryption_options=<REDACTED>; snapshot_before_compa
ction=false; ssl_storage_port=7001; sstable_preemptive_open_interval_in_mb=50; start_native_transport=true; start_rpc=fal
se; storage_port=7000; stream_throughput_outbound_megabits_per_sec=200; streaming_socket_timeout_in_ms=86400000; thrift_f
ramed_transport_size_in_mb=15; thrift_max_message_length_in_mb=16; tombstone_failure_threshold=100000; tombstone_warn_thr
eshold=1000; tracetype_query_ttl=86400; tracetype_repair_ttl=604800; trickle_fsync=false; trickle_fsync_interval_in_kb=10
240; truncate_request_timeout_in_ms=60000; unlogged_batch_across_partitions_warn_threshold=10; windows_timer_interval=1;
write_request_timeout_in_ms=2000]
INFO  21:25:06 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
INFO  21:25:06 Global memtable on-heap threshold is enabled at 476MB
INFO  21:25:06 Global memtable off-heap threshold is enabled at 476MB
Exception (java.lang.NoClassDefFoundError) encountered during startup: java/util/function/Consumer
java.lang.NoClassDefFoundError: java/util/function/Consumer
       at org.apache.cassandra.utils.MBeanWrapper$PlatformMBeanWrapper.registerMBean(MBeanWrapper.java:109)
       at org.apache.cassandra.locator.DynamicEndpointSnitch.registerMBean(DynamicEndpointSnitch.java:96)
       at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:91)
       at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:65)
       at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:742)
       at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:465)
       at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133)
       at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:531)
       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:657)
Caused by: java.lang.ClassNotFoundException: java.util.function.Consumer
       at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
       at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
       at java.security.AccessController.doPrivileged(Native Method)
       at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
       at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
       at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
       at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
       ... 9 more
ERROR 21:25:06 Exception encountered during startup
java.lang.NoClassDefFoundError: java/util/function/Consumer
       at org.apache.cassandra.utils.MBeanWrapper$PlatformMBeanWrapper.registerMBean(MBeanWrapper.java:109) ~[main/:na]
       at org.apache.cassandra.locator.DynamicEndpointSnitch.registerMBean(DynamicEndpointSnitch.java:96) ~[main/:na]
       at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:91) ~[main/:na]
       at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:65) ~[main/:na]
       at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:742) ~[main/:na]
       at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:465) ~[main/:na]
       at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:133) ~[main/:na]
       at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:531) [main/:na]
       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:657) [main/:na]
Caused by: java.lang.ClassNotFoundException: java.util.function.Consumer
       at java.net.URLClassLoader$1.run(URLClassLoader.java:366) ~[na:1.7.0_80]
       at java.net.URLClassLoader$1.run(URLClassLoader.java:355) ~[na:1.7.0_80]
       at java.security.AccessController.doPrivileged(Native Method) ~[na:1.7.0_80]
       at java.net.URLClassLoader.findClass(URLClassLoader.java:354) ~[na:1.7.0_80]
       at java.lang.ClassLoader.loadClass(ClassLoader.java:425) ~[na:1.7.0_80]
       at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) ~[na:1.7.0_80]
       at java.lang.ClassLoader.loadClass(ClassLoader.java:358) ~[na:1.7.0_80]
       ... 9 common frames omitted
{noformat};;;","16/Apr/19 13:59;etedpet;Could not find any ""Submit Patch"" button to click so I just attached the patch file.

This is a simple patch that removes the usage of java.util.function.Consumer, which was introduced in Java 8.

One alternative was to introduce a new interface that was similar to Consumer... But I thought it was cleaner to just put the 1-line logic on the Enums instead.;;;","16/Apr/19 15:43;mshuler;Bug needs to be ""opened"", then can be set to ""patch available"".;;;","29/Apr/19 13:55;etedpet;Added another patch on GitHub (simpler to review):
 [https://github.com/etedpet/cassandra/tree/java7_compatibility]

Made the change as small/simple as possible, since it is only needed on cassandra-2.2 (where java 7 should be supported).

Ran the unit tests on CircleCI:
 [https://circleci.com/gh/etedpet/cassandra/tree/java7_compatibility]

Unfortunately the unit tests are not stable and 1 or 2 (different) test cases fail every time... But this change fixes a compilation error (on Java 7) and should not affect any of those tests.;;;","06/Jun/19 01:13;mshuler;{noformat}
(cassandra-2.2)mshuler@mana:~/git/cassandra$ java -version
java version ""1.7.0_80""
Java(TM) SE Runtime Environment (build 1.7.0_80-b15)
Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)
(cassandra-2.2)mshuler@mana:~/git/cassandra$  
(cassandra-2.2)mshuler@mana:~/git/cassandra$ git diff origin/cassandra-2.2
diff --git a/CHANGES.txt b/CHANGES.txt
index 1cc415312d..48bf14f5d4 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -1,4 +1,5 @@
2.2.15
+ * Fix JDK7 compatibility broken in cassandra-2.2 (CASSANDRA-15050)
 * Support cross version messaging in in-jvm upgrade dtests (CASSANDRA-15078)
 * Fix index summary redistribution cancellation (CASSANDRA-15045)
 * Refactor Circle CI configuration (CASSANDRA-14806)
diff --git a/src/java/org/apache/cassandra/utils/MBeanWrapper.java b/src/java/org/apache/cassandra/utils/MBeanWrapper.java
index 3b5c7cb70c..1ee787d8d8 100644
--- a/src/java/org/apache/cassandra/utils/MBeanWrapper.java
+++ b/src/java/org/apache/cassandra/utils/MBeanWrapper.java
@@ -19,7 +19,6 @@
package org.apache.cassandra.utils;
 
import java.lang.management.ManagementFactory;
-import java.util.function.Consumer;
import javax.management.MBeanServer;
import javax.management.MalformedObjectNameException;
import javax.management.ObjectName;
@@ -206,4 +205,10 @@ public interface MBeanWrapper
            this.handler = handler;
        }
    }
+
+    // Locally defined Consumer interface, to be compatible with Java 7. Only needed for cassandra-2.2
+    interface Consumer<T>
+    {
+        void accept(T e);
+    }
}
(cassandra-2.2)mshuler@mana:~/git/cassandra$  
(cassandra-2.2)mshuler@mana:~/git/cassandra$ ant
Buildfile: /home/mshuler/git/cassandra/build.xml

init:
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/main
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/classes/thrift
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/lib
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/classes
   [mkdir] Created dir: /home/mshuler/git/cassandra/src/gen-java
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/lib
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco
   [mkdir] Created dir: /home/mshuler/git/cassandra/build/jacoco/partials

maven-ant-tasks-localrepo:
    [copy] Copying 1 file to /home/mshuler/git/cassandra/build

maven-ant-tasks-download:

maven-ant-tasks-init:

maven-declare-dependencies:

maven-ant-tasks-retrieve-build:
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies.xml
[artifact:dependencies] Building ant file: /home/mshuler/git/cassandra/build/build-dependencies-sources.xml
    [copy] Copying 65 files to /home/mshuler/git/cassandra/build/lib/jars
    [copy] Copying 41 files to /home/mshuler/git/cassandra/build/lib/sources
    [copy] Copying 25 files to /home/mshuler/git/cassandra/build/lib/jars
   [unzip] Expanding: /home/mshuler/git/cassandra/build/lib/jars/org.jacoco.agent-0.7.5.201505241946.jar into /home/mshuler/git/cassandra/b
uild/lib/jars

check-gen-cql3-grammar:

gen-cql3-grammar:
    [echo] Building Grammar /home/mshuler/git/cassandra/src/java/org/apache/cassandra/cql3/Cql.g  ...

generate-cql-html:

build-project:
    [echo] apache-cassandra: /home/mshuler/git/cassandra/build.xml
   [javac] Compiling 45 source files to /home/mshuler/git/cassandra/build/classes/thrift
   [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.openjdk.jmh.generators.BenchmarkProcessor' less tha
n -source '1.7'
   [javac] Note: /home/mshuler/git/cassandra/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java uses or overrides a depre
cated API.
   [javac] Note: Recompile with -Xlint:deprecation for details.
   [javac] Note: Some input files use unchecked or unsafe operations.
   [javac] Note: Recompile with -Xlint:unchecked for details.
   [javac] 1 warning
   [javac] Compiling 1171 source files to /home/mshuler/git/cassandra/build/classes/main
   [javac] Note: Processing compiler hints annotations
   [javac] warning: Supported source version 'RELEASE_6' from annotation processor 'org.openjdk.jmh.generators.BenchmarkProcessor' less tha
n -source '1.7'
   [javac] Note: Processing compiler hints annotations
   [javac] Note: Writing compiler command file at META-INF/hotspot_compiler
   [javac] Note: Done processing compiler hints annotations
   [javac] Note: Some input files use or override a deprecated API.
   [javac] Note: Recompile with -Xlint:deprecation for details.
   [javac] Note: Some input files use unchecked or unsafe operations.
   [javac] Note: Recompile with -Xlint:unchecked for details.
   [javac] 1 warning

createVersionPropFile:
   [mkdir] Created dir: /home/mshuler/git/cassandra/src/resources/org/apache/cassandra/config
[propertyfile] Creating new property file: /home/mshuler/git/cassandra/src/resources/org/apache/cassandra/config/version.properties
    [copy] Copying 2 files to /home/mshuler/git/cassandra/build/classes/main
    [copy] Copying 1 file to /home/mshuler/git/cassandra/conf

build:

build-test:
   [javac] Compiling 384 source files to /home/mshuler/git/cassandra/build/test/classes
   [javac] javac: invalid target release: 1.8
   [javac] Usage: javac <options> <source files>
   [javac] use -help for a list of possible options

BUILD FAILED
/home/mshuler/git/cassandra/build.xml:1204: Compile failed; see the compiler error output for details.

Total time: 49 seconds
{noformat};;;","06/Jun/19 01:31;mshuler;I can build on jdk8 and seem to be able to start {{./bin/cassandra}} after switching back to jdk7, but with the changes from CASSANDRA-14821, I don't understand how we can verify testing on jdk7 and jdk8, as this branch has always done, if build.xml now specifies we must use jdk8 in tests.
{noformat}
     <property name=""source.version"" value=""1.7""/>
+    <property name=""source.test.version"" value=""1.8""/>
     <property name=""target.version"" value=""1.7""/>
+    <property name=""target.test.version"" value=""1.8""/>
{noformat}

I'm unclear as to why such a breaking chnage was committed to the 2.2 branch - can we revert CASSANDRA-14821 [~ifesdjeen] to rework dual-JDK support for this branch or just leave it out entirely, going back to the previous working tests?;;;","06/Jun/19 13:43;benedict;{quote}I'm unclear as to why such a breaking chnage was committed to the 2.2 branch
{quote}
It's unclear to me that this is ""such"" a breaking change; we are still fully compatible with running the project on JDK7.

The project does not - as a matter of course - test on JDK7 anymore, I don't think?  Certainly not in our CircleCI configurations, where we focus on JDK8 and JDK11.  I'm sure somebody would have shouted sooner if we did.

Happy to change things to support it, just not sure it's such a serious loss.  If we really cared about compatibility testing, we would test mixed-JDK-mode clusters, which must occur during fleet upgrades of JDKs.  There are a lot of JDKs, and at the unit test level I'm happy to trust the different JDKs are as compatible as promised for a maintenance-mode branch on an EOL JDK.;;;","06/Jun/19 14:20;mshuler;Mea culpa. Datastax has continued CI on Jenkins internally, since cassci was dropped as a public service, as well as set up on the ASF Jenkins with all it's limitations, namely the inability to run dual-jdk things. It was indeed missed that this change affected test runs. I'm the one that didn't shout sooner...
 !cassandra-2.2_ci.png! 

I will see if I can key off of the new {{""source.test.version"" value=""1.8""}} build.xml values to select & use JDK8 at test cluster creation. The artifacts build is now using JDK8, since we now deviate from ""source.version"" value=""1.7"" with the new values.

The fixver for CASSANDRA-14821 says ""4.0"" - this was another reason for questioning whether this was actually intended for other branches.;;;","06/Jun/19 14:38;mshuler;Actually.. since I typed that out, if we can't build on 1.7, shouldn't we also show that properly in build.xml? This seems appropriate to also include here (or perhaps just one of these? - not sure if that would be source or target, maybe source):
{noformat}
diff --git a/build.xml b/build.xml
index ca06b41e17..d522b595a4 100644
--- a/build.xml
+++ b/build.xml
@@ -71,9 +71,9 @@
     <property name=""dist.dir"" value=""${build.dir}/dist""/>
     <property name=""tmp.dir"" value=""${java.io.tmpdir}""/>
        
-    <property name=""source.version"" value=""1.7""/>
+    <property name=""source.version"" value=""1.8""/>
     <property name=""source.test.version"" value=""1.8""/>
-    <property name=""target.version"" value=""1.7""/>
+    <property name=""target.version"" value=""1.8""/>
     <property name=""target.test.version"" value=""1.8""/>
        
     <condition property=""version"" value=""${base.version}"">
{noformat};;;","06/Jun/19 14:44;benedict;So, it wouldn't actually be very difficult to make it build again with JDK7 if we want this - we just need to exclude the {{distributed}} source folder when running/building with JDK7, and stipulate the test target version dynamically based on this.  Or we could even disable the {{distributed}} sources altogether for unit tests run from {{ant}}.  AFAIR, only this source folder has JDK8 related stuff (and if anything leaked we can squash it);;;","06/Jun/19 16:33;mshuler;From my previous comment, we do need both {{source.version}} and {{target.version}} with {{value=""1.8""}} - I tried just source. and get a build error, since we grok whatever {{source.version}} says to set up scratch testing vms:
{noformat}
16:17:44 build-project:
16:17:44 [echo] apache-cassandra: /var/lib/jenkins/jobs/mshuler-CASSANDRA-15050_2.2-testall/workspace/build.xml
16:17:44 [javac] Compiling 45 source files to /var/lib/jenkins/jobs/mshuler-CASSANDRA-15050_2.2-testall/workspace/build/classes/thrift
16:17:44 [javac] javac: source release 1.8 requires target release 1.8
16:17:44
16:17:44 BUILD FAILED{noformat}
I'm adding those 2 edits to build.xml to the minimal patch Ted submitted and testing it out. I'm fine with build & test on JDK8, as long as we can maintain JDK7 runtime, since we have users that need it. This patch is a small fix to help out those users that can't update JDK versions, yet, since we state compat with Java 7u25+ for this branch. In 3.0, we go to Java 8u40+, so I think we're good here with the JDK8 distributed testing changes for circleCI and spot checking this branch for JDK7 runtime compat, until EOL.;;;","06/Jun/19 22:35;mshuler;dtest breakage from CASSANDRA-14866 fixed for 2.2:
https://github.com/apache/cassandra-dtest/commit/8d2bbf2b5a673fa0d4a52af8dc973897be70924a;;;","07/Jun/19 13:28;mshuler;[https://circleci.com/gh/mshuler/cassandra/tree/CASSANDRA-15050]

Tests look ""good enough"" and the dtest fixes above got rid of ~140 dtest failures on this branch.;;;","07/Jun/19 14:09;mshuler;Committed. Thanks for the patch, Ted!

https://github.com/apache/cassandra/commit/a9a4f171be6c7d33ce43d04a9bc2dd20f3fd82a5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix index summary redistribution compaction cancellation issues,CASSANDRA-15045,13220146,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,07/Mar/19 10:12,15/May/20 08:00,13/Jul/23 08:38,26/Mar/19 10:11,2.2.15,3.0.19,3.11.5,4.0,4.0-alpha1,Local/Compaction,,,,0,,,,"We can't cancel ongoing index summary redistributions currently due to {{CompactionInfo}} returning null for {{getTableMetadata/getCFMetaData}} [here|https://github.com/apache/cassandra/blob/67d613204fa4fb9584f11ec9886a0e7a0d622e92/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1814] for index summary redistributions

CASSANDRA-14935 also introduced a bug where we track the wrong sstables for index summary redistributions",,jwest,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Degradation -> Other Exception,,,,,,,,Challenging,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 27 16:01:16 UTC 2019,,,,,,,,,,,,"0|z00g28:",9223372036854775807,,,,,,,jwest,,,,Normal,,2.2.0 beta 1,,,,,,,,,,,,reopened for jira test,,,,,"07/Mar/19 10:17;marcuse;Attaching patch to just check if the method returns null, if so, cancel

trunk branch also contains a fix for the sstable tracking and a small refactoring of {{IndexSummaryRedistribution}} to make it a bit clearer which sstables are actually being redistributed.

2.2: https://github.com/krummas/cassandra/commits/marcuse/indexsummary
3.0: https://github.com/krummas/cassandra/commits/marcuse/indexsummary-3.0
3.11: https://github.com/krummas/cassandra/commits/marcuse/indexsummary-3.11
trunk: https://github.com/krummas/cassandra/commits/marcuse/indexsummary-trunk

tests:
https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2Findexsummary
https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2Findexsummary-3.0
https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2Findexsummary-3.11
https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2Findexsummary-trunk;;;","18/Mar/19 20:58;jwest;+1 with a few minor comments below. 
 * The change to check for global compactions makes sense. As usual, I want to double check its ok to change the semantics in each of the versions the change is proposed in. Are their any cases where it should be an option whether or not global compactions are cancelled? My initial inclination is no but wanted to bring it up explicitly regardless. 
 * Minor nit: IndexSummaryManager#L225, the variable name “compactingAndNonCompacting” isn’t very descriptive after the changes
 * Good catch re: the exception handling in redistributeSummaries() finally block
 * The refactor to use the pre-calculated size in IndexSummaryRedistribution is much cleaner, thanks;;;","20/Mar/19 10:30;marcuse;Pushed a new commit to the trunk branch to rename the variable

Also pushed the {{closeAll}} change to all branches and made sure we don't error-log {{CompactionInterruptedException}}

bq. As usual, I want to double check its ok to change the semantics in each of the versions the change is proposed in. Are their any cases where it should be an option whether or not global compactions are cancelled? My initial inclination is no but wanted to bring it up explicitly regardless. 
It has been possible to cancel index summary redistributions before as well - but only explicitly (like {{nodetool stop INDEX_SUMMARY}}) but not automatically when starting anticompaction or truncating for example, so I think it should be safe. My biggest worry was that since explicit stops like above are much more rare than the automatic ones we would find some other issue (like transaction not getting closed properly);;;","21/Mar/19 00:40;jwest;+1;;;","26/Mar/19 09:24;marcuse;With CASSANDRA-15067 the tests look OK

Also remembered that 2.2 needs to build using java7, so rewrote the unit test to not use {{Consumer}}

Committed as {{1d615a5ca151b7dbf24a9367f1379247cb90a5b0}} to 2.2 and merged up, thanks!;;;","27/Mar/19 16:01;jwest;Good catch on the Java 7 needs in 2.2. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UncheckedExecutionException if authentication/authorization query fails,CASSANDRA-15041,13219327,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,eperott,eperott,eperott,04/Mar/19 16:32,15/May/20 08:06,13/Jul/23 08:38,05/Jul/19 18:14,2.2.15,3.0.19,3.11.5,4.0,4.0-alpha1,Feature/Authorization,,,,0,,,,"If cache update for permissions/credentials/roles fails with UnavailableException this comes back to client as UncheckedExecutionException.

Stack trace on server side:
{noformat}
ERROR [Native-Transport-Requests-1] 2019-03-04 16:30:51,537 ErrorMessage.java:384 - Unexpected exception during request
com.google.common.util.concurrent.UncheckedExecutionException: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na]
        at org.apache.cassandra.auth.AuthCache.get(AuthCache.java:97) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.PermissionsCache.getPermissions(PermissionsCache.java:45) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthenticatedUser.getPermissions(AuthenticatedUser.java:104) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.authorize(ClientState.java:439) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.checkPermissionOnResourceChain(ClientState.java:368) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.ensureHasPermission(ClientState.java:345) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.hasAccess(ClientState.java:332) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.ClientState.hasColumnFamilyAccess(ClientState.java:310) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.ModificationStatement.checkAccess(ModificationStatement.java:211) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:222) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:532) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:509) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:146) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:566) [apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.4.jar:3.11.4]
        at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_181]
        at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:114) [apache-cassandra-3.11.4.jar:3.11.4]
        at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]
Caused by: com.google.common.util.concurrent.UncheckedExecutionException: java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2203) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache.get(LocalCache.java:3937) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3941) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4824) ~[guava-18.0.jar:na]
        at org.apache.cassandra.auth.AuthCache.get(AuthCache.java:97) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.RolesCache.getRoles(RolesCache.java:44) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.Roles.hasSuperuserStatus(Roles.java:51) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthenticatedUser.isSuper(AuthenticatedUser.java:71) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraAuthorizer.authorize(CassandraAuthorizer.java:81) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.PermissionsCache.lambda$new$0(PermissionsCache.java:37) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthCache$1.load(AuthCache.java:172) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na]
        ... 26 common frames omitted
Caused by: java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:518) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRoles(CassandraRoleManager.java:283) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.RolesCache.lambda$new$0(RolesCache.java:36) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthCache$1.load(AuthCache.java:172) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3527) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2319) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2282) ~[guava-18.0.jar:na]
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2197) ~[guava-18.0.jar:na]
        ... 40 common frames omitted
Caused by: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at org.apache.cassandra.db.ConsistencyLevel.assureSufficientLiveNodes(ConsistencyLevel.java:334) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.AbstractReadExecutor.getReadExecutor(AbstractReadExecutor.java:162) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy$SinglePartitionReadLifecycle.<init>(StorageProxy.java:1766) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1728) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1671) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1586) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.db.SinglePartitionReadCommand$Group.execute(SinglePartitionReadCommand.java:1209) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:315) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:285) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:526) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:508) ~[apache-cassandra-3.11.4.jar:3.11.4]
        ... 47 common frames omitted
{noformat}
Also, if {{x_validity_in_ms}} > {{x_update_interval_in_ms}}, then the background update thread will fail in a similar way:
{noformat}
ERROR [PermissionsCacheRefresh:1] 2019-03-04 16:30:43,541 CassandraDaemon.java:228 - Exception in thread Thread[PermissionsCacheRefresh:1,5,main]
java.lang.RuntimeException: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:518) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.isSuper(CassandraRoleManager.java:307) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.Roles.hasSuperuserStatus(Roles.java:52) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthenticatedUser.isSuper(AuthenticatedUser.java:71) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraAuthorizer.authorize(CassandraAuthorizer.java:81) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.PermissionsCache.lambda$new$0(PermissionsCache.java:37) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.AuthCache$1.lambda$reload$0(AuthCache.java:180) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_181]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_181]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.4.jar:3.11.4]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_181]
Caused by: org.apache.cassandra.exceptions.UnavailableException: Cannot achieve consistency level QUORUM
        at org.apache.cassandra.db.ConsistencyLevel.assureSufficientLiveNodes(ConsistencyLevel.java:334) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.AbstractReadExecutor.getReadExecutor(AbstractReadExecutor.java:162) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy$SinglePartitionReadLifecycle.<init>(StorageProxy.java:1766) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:1728) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.readRegular(StorageProxy.java:1671) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:1586) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.db.SinglePartitionReadCommand$Group.execute(SinglePartitionReadCommand.java:1209) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:315) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:285) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRoleFromTable(CassandraRoleManager.java:526) ~[apache-cassandra-3.11.4.jar:3.11.4]
        at org.apache.cassandra.auth.CassandraRoleManager.getRole(CassandraRoleManager.java:508) ~[apache-cassandra-3.11.4.jar:3.11.4]
        ... 11 common frames omitted
{noformat}
 ",,eperott,jeromatron,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15153,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,eperott,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,Low Hanging Fruit,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 05 18:14:35 UTC 2019,,,,,,,,,,,,"0|z00b1s:",9223372036854775807,3.0.18,3.11.4,,,,,samt,,,,Normal,,2.2.0,,,https://github.com/apache/cassandra/commit/b2f6953addfb1fce111d1b49627285d1a57a7f40,,,,,,,,,New unit and dtests included,,,,,"16/May/19 20:40;eperott;So, this is mostly a cosmetic issue in the sense that there is no way to resolve the underlying problem - that not enough replicas are available to read from the system_auth tables. Still, this generates error messages that cause operators to jump, and Cassandra isn't behaving well towards clients when this happens.

I've updated the auth_test.py to use consistent cache settings and created a few new test cases to reproduce this issue in the scenario where not enough replicas are available. They revile unwanted behavior with small variation on different releases. In short:
 * TC1 will trigger Cassandra to perform a background update of a cached credentials/roles/permissions entries.
 * TC2 will trigger authorization when cached entries have passed both update-interval and validity (blocking update)
 * TC3 will trigger authorization when cache is disabled.

Link to dtest [patch|https://github.com/apache/cassandra-dtest/compare/master...eperott:cassandra-15041].

I'm expecting Cassandra to fail gracefully on TC1, possibly with a warning, but no stack trace.

I'm expecting TC2 and TC3 to reject the request with exception to indicate not-authorized|unavailable|timeout, not sure which makes most sense. In any case, TC2 and TC3 should fail the same way and there should be no errors or stack traces in the log.

Results on different releases:
 * 4.0: TC2 fail
 * 3.11: TC1, TC2, TC3 and existing test_login fail
 * 3.0: TC1, TC2 and TC3 fail
 * 2.2: TC1, TC2 and TC3 fail

4.0 behaves generally better since a similar ticket as this one was fixed in CASSANDRA-13113. The reason TC2 fails is that the request actually will be authorized, even though the cached entries should have timed out. From what I can tell the cache is handing out stale entries.

The reason test_login fail on 3.11 branch is that we're caching credentials since 3.4.

All new test cases fail on 3.11, 3.0 and 2.2 as reported in this ticket.

So far I've made no attempt to work on a fix for this. Before we dive into that, I'd like to get some feedback on the dtests and my findings above.

TC2 and TC3 currently expect an UnavailableException and a message similar to ""Cannot achieve consistency level QUORUM"" at the client side, simply because this is the behavior in 4.0 branch. I feel this might confuse users a bit as the exception and message is related to the internal lookup on the system_auth.* table, rather than the actual query sent to the cluster. Would it make more sense to throw back an UnauthorizedException?

[~beobal] and [~ifesdjeen], you were both much involved in CASSANDRA-13113. What are your thoughts on this?;;;","17/May/19 12:13;samt;Hey [~eperott]. 

I agree that leaking the details about the failure back to the user and spamming the logs are not really desirable and probably quite confusing, so it would be great to fix them both. I'm not totally convinced about changing the response type as an auth failure signifies incorrect credentials or insufficient privileges and so a client should not assume that retrying the request is a valid strategy. On the other hand, a server error indicates an issue outside the client's control and suggests that if the same request is sent again, it may be successful. We could definitely improve the text of the error response though to give a nicer message when this does occur.
{quote}4.0: TC2 fail
{quote}
This appears to be due to a conflict in the Caffeine cache (earlier branches use Guava) between {{refreshAfterWrite}} and {{expireAfterWrite}}. Basically. the refresh keeps trying and failing, which prevents the expiration happening. I haven't been through the changelog to see exactly what the issue was, but upgrading to Caffeine 2.7.0 seems to fix this.;;;","19/May/19 08:21;eperott;{quote}I'm not totally convinced about changing the response type as an auth failure signifies incorrect credentials or insufficient privileges and so a client should not assume that retrying the request is a valid strategy. On the other hand, a server error indicates an issue outside the client's control and suggests that if the same request is sent again, it may be successful.
{quote}
True. If we were to expose the internal RequestExecutionException, then clients might retry and succeed. Less headache for operators.

But this all depends on the retry strategy of the client/driver and whether the actual client request is idempotent. This could also result in a ReadTimeoutException when the client actually is doing an INSERT, which might add to the confusion.

About authentication, both 2.2 and 3.0 convert the internal RequestExecutionException to an AuthenticationException. 3.11 will do the same when the credentials cache is enabled, but otherwise it will not. 4.0 will never convert to AuthenticationException. The behavior in 2.2/3.0 seem quite deliberate.

IMO we should stick with the legacy behavior during authentication and update 3.11/4.0 accordingly. I'm not sure about authorization behavior - RequestExecutionException seem more practical, while an UnauthorizedException seem more correct.

I've added a few more [dtests|https://github.com/apache/cassandra-dtest/compare/master...eperott:cassandra-15041] to cover for authentication as well as valid cases for responding from cached entries. (ATM tests expect AuthenticationException during authentication, and UnavailableException during authorization.)
{quote}We could definitely improve the text of the error response though to give a nicer message when this does occur.
{quote}
Right, so I've incorporated my suggestions in the dtests. The idea is to give an indication of why the request was rejected, and give a hint of the cause for this.
{quote}but upgrading to Caffeine 2.7.0 seems to fix this
{quote}
Indeed it does! For better traceability, and considering that the caffeine library is used on many places in the code base, I suggest to follow up on this issue in a separate ticket.;;;","28/May/19 07:44;eperott;I've been giving this some more thought. My proposal is:
 * Indicate {{AuthenticationException}} to client if system_auth is inaccessible during authentication
 This is in line with existing behavior in 2.2, 3.0 and partially 3.11.

 * Indicate {{UnavailableException}} to client if system_auth is inaccessible during authorization
 This should wrap whatever {{RequestExecutionException}} that was thrown by the authorization backend (perhaps this is what you meant to say from the beginning).

 * In both cases above should the error message clearly state that issue is related to Authenticatoin/Authorization
 The underlying cause should also be included in the message as a hint for troubleshooting.

 * Create a separate ticket for upgrading the Caffein dependency.

[~samt], would be great to get your thoughts on this. Also, would you be able to review this if I work on a patch?;;;","29/May/19 10:17;samt;[~eperott] sure, your proposal does sound entirely reasonable and consistent so let's got for that. I can definitely review when you're ready with a patch.;;;","10/Jun/19 20:24;eperott;Had to make some adjustments while implementing this.

When we fail to perform authorization it is not always possible to convert whatever-exception-we-get into an {{UnavailableException}} since the {{UnavailableException}} constructor requires a bunch of parameters (CL plus required and live nodes). I didn't feel comfortable to change this to achieve our goals here, so I went with the other proposal to convert this into an {{UnauthorizedException}} instead. But I'm happy to discuss options. Worth considering, since IAuthorizer is a public plug-in interface it should define a generic behavior. And, for example, it would be somewhat awkward for an {{LDAPAuthorizer}} to throw an {{UnavailableException}} if it fails to contact the LDAP server, so the {{UnauthorizedException}} may be a better fit anyway.

A side effect of signaling {{UnauthorizedException}} instead of {{UnavailableException}} is that the issue with the stale entries from the Caffeine cache don'ẗ show any more. This is because the driver will not retry on {{UnauthrizedException}}, and the Caffeine issue only shows if it get repeated queries on failing keys. But IMO we should still see to this. I created CASSANDRA-15153 for this.

Also, had a setback with one of the goals of this ticket - to make the background cache reload thread mute if it fails. Turns out the error message is buried deep down in the Guava {{LoadingCache}}. Only option I see for pre-4.0 is to mute this in the logback config.

PR for [dtest|https://github.com/apache/cassandra-dtest/pull/52].

The patches for Cassandra differs a bit on 2.2/3.0 vs. 3.11 vs. trunk. Not sure what's the best way to provide these patches to simplify review and merge into upstream repo. Below are links to the individual branches on my github clone without merge commits, is that OK? Lots if dtests are timing out since I only have the free service, but will try to run failing tests locally

||Patch||CI||
|[15041-cassandra-2.2|https://github.com/eperott/cassandra/tree/15041-cassandra-2.2]|[CircleCI|https://circleci.com/gh/eperott/workflows/cassandra/tree/cci%2F15041-cassandra-2.2]|
|[15041-cassandra-3.0|https://github.com/eperott/cassandra/tree/15041-cassandra-3.0]|[CircleCI|https://circleci.com/gh/eperott/workflows/cassandra/tree/cci%2F15041-cassandra-3.0]|
|[15041-cassandra-3.11|https://github.com/eperott/cassandra/tree/15041-cassandra-3.11]|[CircleCI|https://circleci.com/gh/eperott/workflows/cassandra/tree/cci%2F15041-cassandra-3.11]|
|[15041-trunk|https://github.com/eperott/cassandra/tree/15041-trunk]|[CircleCI|https://circleci.com/gh/eperott/workflows/cassandra/tree/cci%2F15041-trunk]|;;;","15/Jun/19 07:09;eperott;Since dtests on CircleCI are unreliable on the free service, I've run failing j8_dtests-no-vnodes locally and compared with each base branch. There is no regression.;;;","18/Jun/19 16:46;eperott;bq. Also, had a setback with one of the goals of this ticket - to make the background cache reload thread mute if it fails. Turns out the error message is buried deep down in the Guava LoadingCache. Only option I see for pre-4.0 is to mute this in the logback config.

Discovered that this was a false assumption based on comments in the Guava Cache API. The log message is actually coming from the {{DebuggableThreadPoolExecutor}} which is invoking the {{DefaultUncaughtExceptionHandler}} which in turn is installed by {{CassandraDaemon}}. Added a small patch on top of 2.2, 3.0, and 3.11 branches to override this behavior. Also update dtest branch to verify that a failing background update will actually be silent.;;;","18/Jun/19 18:36;samt;Thanks, I should get finished with this soon. I've been running the branches through CircleCI with the HIRES settings and the only failures are for unrelated things (one which has already been fixed in the dtests & one known issue with Thrift tests on 3.11 and lower).;;;","20/Jun/19 18:12;samt;Thanks for this [~eperott], there are definitely valuable improvements here.

Whilst the new dtests for handling unavailability do what they claim, they’re not doing so in the way one *might* expect. First of all, because they use a superuser, table/keyspace level permissions will never be read from system tables and so won’t be vulnerable to unavailablity. The reason the tests are currently passing is that the check that the user has access to the local DC is resulting in a failed read by {{CassandraNetworkAuthorizer}}. So the tests are definitely still valid as they are, but it might be worth adding some commentary just to highlight this and maybe use a non-superuser role that *could* exercise the permissions cache too (even though it won't with the way the current test are organised). Potentially, adding some logging to the new catch blocks in {{CassandraRoleManager}} might be useful, similar to that in {{CassandraAuthorizer::authorize}}. {{CassandraNetworkAuthorizer}} is especially vulnerable here, due to CASSANDRA-15089, which I will commit as soon as I get chance.

Otherwise, this all looks pretty good to me, modulo a typo in {{ClientState::canLogin}} (only in 2.2/3.0/3.11).

I’ve been running the tests with the HIRES circle configuration and everything looks good - all failures are unrelated and/or fixed already by other tickets.
 
So I just have these nits:

* typo in {{ClientState::canLogin}} (2.2/3.0/3.11)
* maybe add some minimal logging in {{CassandraRoleManager::isSuper/canLogin}} or make the exception messages distinguishable
* document the potential sources of the availability failures that the dtests are checking and/or use a non-superuser in tests

One thing that I had missed up to now was that when Guava was replaced with Caffeine in CASSANDRA-10855, the ability to do async reloading in {{AuthCache}} was lost. We should definitely fix that, as having those reads done in a thread servicing client reads is not a good idea. I’ve opened CASSANDRA-15177 for that. 
;;;","27/Jun/19 08:25;eperott;Thanks for taking the time to review.

bq. Whilst the new dtests for handling unavailability do what they claim, they’re not doing so in the way one might expect.

Yeah, had to spend some time to get this sorted in my head. There are certainly a few surprises in how the caches affect the result, and one another. What's more, behavior is a bit different between versions since we're caching the super-user flag in 4.0, while in pre-4.0 we're not. For this reason, a stale entry in the permissions cache on pre-4.0 will result in a query to pull up the role's super-user flag even if the role cache is still up to date. The resulting error message is a bit confusing, but this is more intuitive in 4.0 I think. Also, I believe this makes it hard to use a non-super-user role in the tests and still get clean code and consistent results across versions, so didn't change that. Open to suggestions if you have some ideas...

I've added some comments around assumptions and test strategy in the dtests.

One thing I discovered while working on this is that the assert_exception helper (and friends) don't verify error message properly. I fixed this locally while verifying, but will create a separate ticket for this since it will affect a few places where the helpers are used.

bq. So I just have these nits

Fixed those. Also, since I removed the stack trace dump on error level when background updates fail (pre-4.0), I've re-added the trace log we used to have in the AuthCache.

bq. I’ve been running the tests with the HIRES circle configuration

Thanks. Tried to make it work on free service, but I give up...
;;;","05/Jul/19 18:14;samt;Thanks [~eperott] committed to 2.2 in {{b2f6953addfb1fce111d1b49627285d1a57a7f40}} and merged to 3.0, 3.11 & trunk. Committed the dtest patch in [ef5f2f52f83ae2592555a627ce3534daa8a0a3a5|https://github.com/apache/cassandra-dtest/commit/ef5f2f52f83ae2592555a627ce3534daa8a0a3a5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation claims copyright for future years,CASSANDRA-15039,13219001,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,bmwiedemann,bmwiedemann,bmwiedemann,01/Mar/19 19:58,15/May/20 08:04,13/Jul/23 08:38,08/Mar/19 08:57,3.11.5,4.0,4.0-alpha1,,,Documentation/Javadoc,,,,0,,,,"See attached patch for details and fix.

 
See also on this topic:
[https://stackoverflow.com/questions/2390230/do-copyright-dates-need-to-be-updated]
 ",,bmwiedemann,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1800,1800,,0%,1800,1800,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/19 19:55;bmwiedemann;cassandra.patch;https://issues.apache.org/jira/secure/attachment/12960822/cassandra.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,bmwiedemann,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,,Fri Mar 08 09:32:48 UTC 2019,,,,,,,,,,,,"0|z0091s:",9223372036854775807,3.11.4,,,,mck,,,,,,Low,,,,,,,,,,,,,,,,,,,"08/Mar/19 08:42;mck;It's not just the generated apidocs but the NOTICE file is wrong as well.
I will commit your fix, and correct the NOTICE file (according to https://www.apache.org/legal/src-headers.html );;;","08/Mar/19 09:32;bmwiedemann;Thanks. For later reference: Fix is in commit 84fc68ce3f77e88a542dd2443e560cb291109198;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid CME when cancelling compactions for anticompaction,CASSANDRA-15036,13218143,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,26/Feb/19 17:58,15/May/20 08:04,13/Jul/23 08:38,04/Mar/19 13:10,4.0,4.0-alpha1,,,,Local/Compaction,,,,0,,,,When iterating over a set created with {{Collections.synchronizedSet}} we need to manually synchronize on the set,,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,Correctness -> API / Semantic Implementation,,,,,,,,,Adhoc Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 04 13:10:21 UTC 2019,,,,,,,,,,,,"0|z003s0:",9223372036854775807,,,,,,,samt,,,,Normal,,,,,,,,,,,,,,,,,,,"26/Feb/19 18:02;marcuse;patch to just synchronize on the active compactions;

patch: https://github.com/krummas/cassandra/commits/marcuse/15036
circle: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15036;;;","27/Feb/19 09:13;marcuse;not sure what is going on with circleci - seems to randomly fail, even for trunk: https://circleci.com/workflow-run/1c33a46e-a551-47ef-8872-6a0091041566;;;","04/Mar/19 12:56;samt;+1 LGTM after the [ccm patch|https://github.com/riptano/ccm/pull/694] got circleci looking a bit more healthy.;;;","04/Mar/19 13:10;marcuse;and committed as {{6b3ea1e1a480c9f9a2f6f813443daa4c6866e2ff}} - thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C* 3.0 sstables w/ UDTs are corrupted in 3.11 + 4.0,CASSANDRA-15035,13218085,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,snazy,snazy,snazy,26/Feb/19 13:03,15/May/20 08:41,13/Jul/23 08:38,31/Jan/20 09:15,3.11.6,4.0,4.0-alpha4,,,Feature/UDT,Local/SSTable,,,0,,,,"OSS C* 3.0 writes incorrect type information for UDTs into the serialization-header of each sstable.

In C* 3.0, both UDTs and tuple are always frozen. A frozen type must be enclosed in a {{frozen<...>}} via the {{CQL3Type}} hierarchy (resp {{org.apache.cassandra.db.marshal.FrozenType(...)}} via the {{AbstractType}} hierarchy) “bracket” in the schema and serialization-header.

Since CASSANDRA-7423 (committed to C* 3.6) UDTs can also be non-frozen (= multi-cell).

Unfortunately, C* 3.0 does not write the {{org.apache.cassandra.db.marshal.FrozenType(...)}} “bracket” for UDTs into the {{SerializationHeader.Component}} in the {{-Stats.db}} sstable component.

The order in which columns of a row are serialized depends on the concrete {{AbstractType}}. Columns with variable length types (frozen types belong to this category) are serialized before columns with multi-cell types (non-frozen types belong to that category).

If C* 3.6 (or any newer version) reads an sstable written by C* 3.0 (up to 3.5), it will read the type information “non-frozen UDT” from the serialization header, which is technically correct.

This means, that upgrades from C* 3.0 to C* 3.11 and 4.0, using a schema that uses UDTs, result in inaccessible data in those sstables. Reads against 3.0 sstables as well as attempts to scrub these sstables result in a wide variety of errors/exceptions ({{CorruptSSTableException}}, {{EOFExcepiton}}, {{OutOfMemoryError}}, etc etc), as usual in such cases.

Mitigation strategy in the proposed patch:
* Fix the broken serialization-headers automatically when an upgrade from C* 3.0 is detected.
* Enhance {{sstablescrub}} to verify the serialization-header against the schema and allow {{sstablescrub}} to fix the UDT types according to the information in the schema. This does not apply to ""online scrub"" (e.g. nodetool scrub). The behavior of {{sstablescrub}} has been changed to first inspect the serialization-header and verify the type information against the schema. 

Differences between the schema and the sstable serialization-headers cause {{sstablescrub}} to error out and stop - i.e. safety first (there’s a way to opt-out though).

A new class {{SSTableHeaderFix}} can inspect the serialization-header ({{SerializationHeader.Component}}) in the the {{-Statistics.db}} component and fix the type information in those sstables for UDTs according to the schema information.

This new class could be used during verify and before sstables are imported. But changes to “verify” and “import” are out of the scope of this ticket, as the patch is already bigger than I originally expected.

Another issue not tackled by this ticket is that the wrong ‘kind’ is written to the type information in {{system_schema.dropped_columns}} when a non-frozen UDT column is dropped. When a UDT column is dropped, the type of the dropped column is converted from the UDT definition to its “corresponding” tuple type definition. But all versions currently write {{frozen<tuple<...>>}}, but for non-frozen UDTs it should actually just be {{tuple<...>}}. Unfortunately, there is nothing that could be done in this ticket to fix (or even consider) the type information of a dropped column. But for correctness, the tuple type should be a multi-cell one (only accessible for dropped UDTs though - not as something that a user can create as a type).
",,abdulazizali,aleksey,csplinter,e.dimitrova,jeromatron,jjordan,rtib,snazy,tcooke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,Correctness -> Unrecoverable Corruption / Loss,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 31 09:15:40 UTC 2020,,,,,,,,,,,,"0|z003fc:",9223372036854775807,3.11.4,4.0,,,dimitarndimitrov,,brandon.williams,snazy,,,Critical,,3.6,,,https://github.com/apache/cassandra/commit/ffab2b8dde0c7c40080c1c0b36831edd6965a042,,,,,,,,,,,,,,"26/Feb/19 13:10;snazy;PRs on GitHub:
trunk: https://github.com/apache/cassandra/pull/303
3.11: https://github.com/apache/cassandra/pull/304
dtest: https://github.com/apache/cassandra-dtest/pull/46;;;","16/Jan/20 20:27;brandon.williams;Needs some very minor rebasing now, but +1.;;;","31/Jan/20 09:15;snazy;Thanks for the review!

Committed as [ffab2b8dde0c7c40080c1c0b36831edd6965a042|https://github.com/apache/cassandra/commit/ffab2b8dde0c7c40080c1c0b36831edd6965a042] to [cassandra-3.11|https://github.com/apache/cassandra/tree/cassandra-3.11], [merged|https://github.com/apache/cassandra/commit/7a7eece9578312a2f9d77de6e0755a3c3c542e99] to [trunk|https://github.com/apache/cassandra/tree/trunk].

Committed manually as ffab2b8dde0c7c40080c1c0b36831edd6965a042
 Merged manually as 7a7eece9578312a2f9d77de6e0755a3c3c542e99

[dtest committed|https://github.com/apache/cassandra/commit/ffab2b8dde0c7c40080c1c0b36831edd6965a042] as well;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Handle IR prepare phase failures less race prone by waiting for all results,CASSANDRA-15027,13216314,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,spod,spod,spod,18/Feb/19 07:49,15/May/20 08:01,13/Jul/23 08:38,22/Feb/19 18:57,4.0,4.0-alpha1,,,,Consistency/Repair,Local/Compaction,,,0,,,,"Handling incremental repairs as a coordinator begins by sending a {{PrepareConsistentRequest}} message to all participants, which may also include the coordinator itself. Participants will run anti-compactions upon receiving such a message and report the result of the operation back to the coordinator.

Once we receive a failure response from any of the participants, we fail-fast in {{CoordinatorSession.handlePrepareResponse()}}, which will in turn completes the {{prepareFuture}} that {{RepairRunnable}} is blocking on. Then the repair command will terminate with an error status, as expected.

The issue is that in case the node will both be coordinator and participant, we may end up with a local session and submitted anti-compactions, which will be executed without any coordination with the coordinator session (on same node). This may result in situations where running repair commands right after another, may cause overlapping execution of anti-compactions that will cause the following (misleading) message to show up in the logs and will cause the repair to fail again:
 ""Prepare phase for incremental repair session %s has failed because it encountered intersecting sstables belonging to another incremental repair session (%s). This is by starting an incremental repair session before a previous one has completed. Check nodetool repair_admin for hung sessions and fix them.""",,bdeggleston,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,spod,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 22 18:57:27 UTC 2019,,,,,,,,,,,,"0|yi12wo:",9223372036854775807,,,,,bdeggleston,,,,,,Normal,,,,,,,,,,,,,,,,,,,"18/Feb/19 08:42;spod;* [ [trunk|https://github.com/spodkowinski/cassandra/tree/CASSANDRA-15027] ][ [circleci|https://circleci.com/workflow-run/2b027f87-cf45-48ee-8eae-45a563701bc6] ];;;","21/Feb/19 21:32;bdeggleston;Thanks [~spodxx@gmail.com]. I’ve extended your code so that in addition to waiting for other anti-compactions to complete, the coordinator also pro-actively cancels ongoing anti-compactions on the other participants. This avoids wasting time waiting for anti-compactions on other machines. The code does 3 things:
 * Adds a session state check to the {{isStopRequested}} method in the anti-compaction iterator.
 * The coordinator now sends failure messages to all participants when it receives a failure message from one of them in the prepare phase. It does not mark these participants as having failed internally though, since that would cause the nodetool session to immediately complete. Instead, it waits until it’s received messages from all the other nodes.
 * The participants will now respond with a failed prepare message if the anti-compaction completes, but the session was failed in the mean time. This prevents a dead lock on the coordinator in the case where the participant received a failure message between the time the anti-compaction completes and the callback fires.

Let me know what you think. If everything looks ok to you, I’m +1 on committing.

[trunk|https://github.com/bdeggleston/cassandra/tree/15027-trunk]
 [circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/15027-trunk];;;","22/Feb/19 14:15;spod;Your updates look like valuable improvement over the initial patch. I'm +1 in general as for the changes, but also fixed some additional minor issues and added a new tests:

* [CASSANDRA-15027|https://github.com/spodkowinski/cassandra/commits/CASSANDRA-15027]
* [https://circleci.com/workflow-run/2b444c33-a54c-46b5-9923-bcded8bcf465]

Please see comments with each commit in branch above for details.

Also happy to discuss any of the changes (most likely the last commit) in another jira, if you feel it's out of scope for this ticket.
 ;;;","22/Feb/19 16:43;bdeggleston;Nice. Your follow on changes look good to me, I have 2 nits, but those can just be fixed on commit.

* We should log the session id in compaction manager when an anti-compaction is cancelled (and probably when there's an error as well)
* Some error handling should be added to the commit fixing the race between proposeFuture and hasFailure so nodetool doesn't hang if there's an error in the callback

edit: proposed fixes [here|https://github.com/bdeggleston/cassandra/commit/02d7d9e09983db0d4661486b17adc375e17be24f];;;","22/Feb/19 17:41;spod;LGTM +1;;;","22/Feb/19 18:57;bdeggleston;Committed to trunk as 9bde713ee8883f70d130efb6290ec0e6daea524f, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid NPE in RepairRunnable.recordFailure,CASSANDRA-15025,13216041,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,15/Feb/19 17:27,15/May/20 08:02,13/Jul/23 08:38,18/Feb/19 08:09,4.0,4.0-alpha1,,,,Consistency/Repair,,,,0,,,,"failureMessage parameter in {{RepairRunnable.recordFailure}} can be null, avoid this happening and make sure we log the actual exception",,bdeggleston,cscotta,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 18 08:09:48 UTC 2019,,,,,,,,,,,,"0|yi1188:",9223372036854775807,,,,,bdeggleston,,bdeggleston,,,,Normal,,,,,,,,,,,,,,,,,,,"15/Feb/19 17:30;marcuse;patch: https://github.com/krummas/cassandra/commits/marcuse/avoid_repair_npe
tests: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2Favoid_repair_npe

I'll submit a dtest for this early next week;;;","15/Feb/19 18:32;bdeggleston;+1;;;","18/Feb/19 08:09;marcuse;committed as {{98d81e409a3512fccaeab3ba89f7cf5bfa8f39ae}}, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't try to cancel 2i compactions when starting anticompaction,CASSANDRA-15024,13216038,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,15/Feb/19 17:13,15/May/20 08:03,13/Jul/23 08:38,18/Feb/19 08:09,4.0,4.0-alpha1,,,,Consistency/Repair,Feature/2i Index,,,0,,,,"When we start an anticompaction we cancel ongoing compactions, {{runWithCompactionsDisabled}} always cancels compactions for secondary index cfs:es, this causes problem since CASSANDRA-14935 since we check for range intersection which will fail since 2i sstables are LocalPartitioner.",,bdeggleston,cscotta,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 18 08:09:13 UTC 2019,,,,,,,,,,,,"0|yi117k:",9223372036854775807,,,,,bdeggleston,,bdeggleston,,,,Normal,,,,,,,,,,,,,,,,,,,"15/Feb/19 17:18;marcuse;Attaching patch to add option to simply avoid concatenating the index cfs when calling {{runWithCompactionsDisabled}}.

patch: https://github.com/krummas/cassandra/commits/marcuse/2ianticompaction
tests: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F2ianticompaction;;;","15/Feb/19 18:25;bdeggleston;+1;;;","18/Feb/19 08:09;marcuse;committed as {{0706d32b0bd478160deb0143deb9811d49050b10}} - thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repaired data tracking isn't working for range queries,CASSANDRA-15019,13215274,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,12/Feb/19 15:23,15/May/20 08:00,13/Jul/23 08:38,31/May/19 17:15,4.0,4.0-alpha1,,,,Consistency/Coordination,Consistency/Repair,Test/dtest/python,,0,,,,"CASSANDRA-14145 introduced optional tracking of the repaired dataset used to construct a read response. If enabled, each replica computes a digest for the repaired portion of the data, which the coordinator compares in order to detect divergence between replicas. This isn't working correctly for range reads, as the ReadCommand instance that the DataResolver is intialized with does not have the tracking flag set. This has been undetected up until now as the dtest which should verify it also has a bug in that when the relevant range query is issued the test expectations are being incorrectly set.",,jeromatron,jwest,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/19 21:27;jwest;RepairDigestTrackingTest.java;https://issues.apache.org/jira/secure/attachment/12962544/RepairDigestTrackingTest.java",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,samt,,,,,,,,,,,,Correctness -> Consistency,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 17:15:12 UTC 2019,,,,,,,,,,,,"0|yi0wio:",9223372036854775807,,,,,,,jwest,,,,Normal,,4.0,,,,,,,,,,,,,,,,,"12/Feb/19 15:39;samt;|[trunk|https://github.com/beobal/cassandra/tree/15019-trunk]|[dtest PR|https://github.com/apache/cassandra-dtest/pull/43]|[CI (with patched dtest branch)|https://circleci.com/workflow-run/8dec0886-5b8d-48e3-a4a9-c35723582aeb]|
;;;","14/Mar/19 18:16;jwest;This looks like the simplest patch to fix this but I wonder if we would be better served in the future by fixing the functions that copy {{ReadCommand}} to correctly copy the tracking flag. The patch would be a bit bigger for sure (needing to modify each function and potentially the constructor) but it would be more likely  prevent this mistake in the future and would clarify the disabling of the flag for transient replicas. 

 

Happy to +1 it as is but wanted to toss that out there first. ;;;","14/Mar/19 20:09;jwest;[~beobal] also shouldn't [this|https://github.com/beobal/cassandra/commit/c92b9dea14e128f2063dca01526c3afcd5559d51#diff-71f06c193f5b5e270cf8ac695164f43aR2052] call be to {{DatabaseDescriptor.getRepairedDataTrackingForRangeReadsEnabled}}. Just noticing this now. It existed before the patch. 

 

EDIT: I've uploaded a test that illustrates the bug (uncomment line 51 to make the test pass but it shouldn't be necessary). The test also shows how we could convert the dtest to an in-jvm distributed test. ;;;","20/Mar/19 13:28;samt;Thanks for the review and for adding the in-jvm dtest. I've included that in the final patch along with checking the correct config setting and committed to trunk in {{99ce007c5beb7988ce83fb1443a1e0ca259264cc}}

Re: the copy methods & constructor. I agree that doing something more radical is probably justified now that transient replication has landed (it hadn't when the original patch was written). I believe though that the removal of the {{REPAIRED_DATA_TRACKING}} header is imminent via another in-flight patch so I've made a note to revisit this if/when that lands.
;;;","31/May/19 17:15;jeromatron;Re-resolving as fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up SSL Cert reloading,CASSANDRA-15018,13215160,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,djoshi,djoshi,djoshi,12/Feb/19 06:05,15/May/20 08:03,13/Jul/23 08:38,14/Feb/19 08:10,4.0,4.0-alpha1,,,,Feature/Encryption,,,,0,,,,Minor clean up for SSL Cert reloading code path.,,djoshi,ifesdjeen,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 14 08:10:18 UTC 2019,,,,,,,,,,,,"0|yi0vtk:",9223372036854775807,,,,,ifesdjeen,,ifesdjeen,,,,Low,,,,,,,,,,,,,,,,,,,"12/Feb/19 06:14;djoshi;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/15018-trunk]|
|[utests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/15018-trunk]|;;;","14/Feb/19 08:10;ifesdjeen;+1, thank you for the patch! Great that we're getting rid of sleeps in tests!

Committed to trunk with [cbf4da4397c2cec34d6a240b0e917a847c46b3d0|https://github.com/apache/cassandra/commit/cbf4da4397c2cec34d6a240b0e917a847c46b3d0];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra metrics documentation is not correct for Hint_delays metric,CASSANDRA-15015,13214479,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,anupshirolkar,anupshirolkar,anupshirolkar,07/Feb/19 22:23,15/May/20 08:02,13/Jul/23 08:38,01/Apr/19 09:30,4.0,4.0-alpha1,,,,Documentation/Blog,,,,0,documentation,easyfix,low-hanging-fruit,"The Cassandra metrics for hint delays are not correctly referred on the documentation web page: [http://cassandra.apache.org/doc/latest/operating/metrics.html#hintsservice-metrics] 

The metrics are defined in the [code|https://github.com/apache/cassandra/blob/06209037ea56b5a2a49615a99f1542d6ea1b2947/src/java/org/apache/cassandra/metrics/HintsServiceMetrics.java#L45-L52] as 'Hint_delays' and 'Hint_delays-<ip_of_peer>' but those are listed on the website as 'Hints_delays' and 'Hints_delays-<PeerIP>'.

The documentation should be fixed by removing the extra 's' in Hints to match it with code.

The Jira for adding hint_delays: CASSANDRA-13234 ",,anupshirolkar,djoshi,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13234,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/19 02:06;anupshirolkar;150150-trunk.txt;https://issues.apache.org/jira/secure/attachment/12959539/150150-trunk.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,anupshirolkar,,,,,,,,,,,,,,,,,,,,Low Hanging Fruit,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 01 09:30:23 UTC 2019,,,,,,,,,,,,"0|yi0rmg:",9223372036854775807,,,,,,,djoshi,mck,,,Low,,4.0,,,,,,,,,,,,,,,,,"13/Feb/19 06:46;djoshi;[~anupshirolkar] did you attach the patch or create a branch? I don't see it anywhere on this ticket.;;;","21/Feb/19 02:08;anupshirolkar;[~djoshi3] I have attached a patch now. Can you please review it. Thanks.

Sorry, I had to delete the patch I had submitted last time as I did not follow the process. I think its all good now.

Also created a branch in case required for review:

[https://github.com/instaclustr/cassandra/tree/150150] ;;;","21/Feb/19 08:43;djoshi;Thanks, Anup. At a first glance this looks ok. Let me double check though.;;;","01/Apr/19 09:20;mck;LGTM. Only needs `Fix Version: 4.0`. And no need for the CHANGES entry (it's a small doc fix from the same version changelog).;;;","01/Apr/19 09:30;mck;Committed as e49b25c29faf7b83fe7c566d7b1657d9f599d750;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests failure on trunk,CASSANDRA-15014,13214436,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,ifesdjeen,djoshi,djoshi,07/Feb/19 19:05,01/Aug/21 12:50,13/Jul/23 08:38,25/Feb/19 13:27,2.2.15,3.0.19,3.11.7,4.0,4.0-alpha1,Test/unit,,,,0,,,,"Currently org.apache.cassandra.distributed.test.DistributedReadWritePathTest is failing on trunk with the following error -
{code:java}
[junit-timeout] Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF8
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.DistributedReadWritePathTest
[junit-timeout] Exception in thread ""main"" java.lang.OutOfMemoryError: Metaspace
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.DistributedReadWritePathTest
[junit-timeout] Testsuite: org.apache.cassandra.distributed.test.DistributedReadWritePathTest Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0 sec
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.distributed.test.DistributedReadWritePathTest:readWithSchemaDisagreement: Caused an ERROR
[junit-timeout] Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout] junit.framework.AssertionFailedError: Forked Java VM exited abnormally. Please note the time in the report does not reflect the time until the VM exit.
[junit-timeout] at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout] at java.util.Vector.forEach(Vector.java:1275)
[junit-timeout] at java.lang.Thread.run(Thread.java:748)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Test org.apache.cassandra.distributed.test.DistributedReadWritePathTest FAILED (crashed)
[junitreport] Processing /tmp/cassandra/build/test/TESTS-TestSuites.xml to /tmp/null1041131060
[junitreport] Loading stylesheet jar:file:/usr/share/ant/lib/ant-junit.jar!/org/apache/tools/ant/taskdefs/optional/junit/xsl/junit-frames.xsl
[junitreport] Transform time: 277ms
[junitreport] Deleting: /tmp/null1041131060{code}
I have noticed sporadic failures in the org.apache.cassandra.distributed.test.* suite.",,djoshi,ifesdjeen,jolynch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-15508,,,,,,,,,,,,,"11/Feb/19 13:04;ifesdjeen;Screen Shot 2019-02-11 at 12.30.19.png;https://issues.apache.org/jira/secure/attachment/12958243/Screen+Shot+2019-02-11+at+12.30.19.png","13/Feb/19 14:47;ifesdjeen;Screen Shot 2019-02-13 at 15.46.28.png;https://issues.apache.org/jira/secure/attachment/12958583/Screen+Shot+2019-02-13+at+15.46.28.png","13/Feb/19 02:12;jolynch;threads_stuck_waiting.png;https://issues.apache.org/jira/secure/attachment/12958491/threads_stuck_waiting.png",,,,,,,,,,,,,,,,,,,,,,,,,,,3.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 25 13:26:58 UTC 2019,,,,,,,,,,,,"0|yi0rcw:",9223372036854775807,,,,,jolynch,,jolynch,,,,Normal,,,,,,,,,,,,,,,,,,,"07/Feb/19 19:07;djoshi;[~benedict] [~jolynch] [~ifesdjeen] could you please check whats going on?;;;","11/Feb/19 11:35;ifesdjeen;In summary, there was a problem with retention resulting from schema change: since it was executed on the main thread, we were retaining a bunch of thread locals. Attaching a screenshot showing that we retain mere 12mb after full test run. 

I did test it on several environments and it seems that circleci is still timing out because of GC pressure. I've decided to simplify runs and just have each test executing in its own JVM to avoid GC contention alltogether. 

I've also added an ant task to conveniently run distributed tests without test list and have improved a shutdown process. 

|[2.2|https://github.com/apache/cassandra/compare/cassandra-2.2...ifesdjeen:CASSANDRA-15014-2.2]|[test run|https://circleci.com/workflow-run/8b1aae41-e279-405b-b5e5-12808179daa2]|
|[3.0|https://github.com/apache/cassandra/compare/cassandra-3.0...ifesdjeen:CASSANDRA-15014-3.0]|[test run|https://circleci.com/workflow-run/53ad7477-7d5e-4c23-8634-be4def9e95b9]|
|[3.11|https://github.com/apache/cassandra/compare/cassandra-3.11...ifesdjeen:CASSANDRA-15014-3.11]|[test run|https://circleci.com/workflow-run/e41d800f-e39d-4f1d-a2bd-99e61c8a0675]|
|[trunk|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk]|[test run|https://circleci.com/workflow-run/2cfdb4b1-b52d-4108-b2cb-4899382ac880]|[without high capacity|https://circleci.com/workflow-run/2cfdb4b1-b52d-4108-b2cb-4899382ac880]|;;;","11/Feb/19 23:46;jolynch;Heads up that non trunk links are missing the cassandra before the branch name (e.g. https://github.com/apache/cassandra/compare/2.2...ifesdjeen:CASSANDRA-15014-2.2 instead of https://github.com/apache/cassandra/compare/cassandra-2.2...ifesdjeen:CASSANDRA-15014-2.2). Working on review now.

;;;","13/Feb/19 02:29;jolynch;[~ifesdjeen] I think that this is a pretty reasonable workaround, and from my testing it only appears about 2x slower then running them all in one JVM (My testing indicated about 1 min 10s vs 30s). I think long term we'll need to figure out how to run these either with a fork per method or with a test cluster per test class or something, but I recognize we're trying to mitigate the trunk unit test runs here and we can iterate on making them faster next as we need to.

Feedback:
 * If we go generated script direction can we wrap up the gen list -> chmod -> execute [logic|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk#diff-1d37e48f9ceff6d8030570cd36286a61R194] into an ant target, the only drawback I see is that our stdout will have ""[exec]"" prepended. Maybe something like [this|https://github.com/apache/cassandra/commit/88da841585d4fb310bbac80b03601d74919fa507]
 * Is [test-distributed|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk#diff-2cccd7bf48b7a9cc113ff564acd802a8R1890] dead code?
 * When I run `ant testclasslist -Dtest.classlistfile=/home/josephl/pg/cassandra/testlist.txt -Dtest.classlistprefix=distributed` to try to test if we need to split by method and I attach yourkit it either fails with a class not found exception or if I do attach I see a bunch of threads waiting on ""MigrationManager.announce"" forever (screenshot attached). Can you run the test like that or do you hang as well? I think this might indicate that the wait for logic in AbstractCluster.java may not be working right
 * Unused imports in [Instance.java|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk#diff-7c02c337a482a2dc284c2e67bbb44dc1R28]
 * Calling System.runFinalization() is interesting, did you try doing that without the method separation and it didn't work?

For trunk:
 * Can you run it without the high capacity machines? Usually it's the unit tests that fail;;;","13/Feb/19 14:50;ifesdjeen;[~jolynch] thank you for review

bq. ""MigrationManager.announce"" forever (screenshot attached)

I've just tried running tests myself and for me those threads stop, so I'm not 100% sure what's going on.

bq. Is test-distributed dead code?

It's just a more convenient runner, so that you wouldn't have to create a testiest file.

bq. Unused imports in Instance.java

Fixed.

bq. Calling System.runFinalization() is interesting, did you try doing that without the method separation and it didn't work?

I did try and it sometimes runs fine but isn't stable on non-high-capacity env. I did runs on both environments before pushing. Also your run seems to have passed.

bq. the only drawback I see is that our stdout will have ""[exec]"" prepended.

This is great, let's switch to what you propose.


;;;","25/Feb/19 07:24;jolynch;Alright, finally figured out why attaching a debugger was causing the tests to crash, looks like the {{InstanceClassLoader}} switched from a blacklist to a whitelist where we only use the shared class loader for whitelisted classes, I just added a whitelist for ""com.yourkit"" and I was able to attach again.

Just a few comments, +1 after fixing first one.
 * I don't think right now if a jvm-dtest fails the build actually fails (I inserted an exception into one of the tests and {{ant test-jvm-dtest}} still passed). Perhaps we need {{#!/bin/bash}} and {{set -e}} at the top of the file written by {{TestLocator.java}} and {{failonerror}} set on the ant {{exec}} line.
 * Do you find that [setting|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15014-trunk#diff-2cccd7bf48b7a9cc113ff564acd802a8R1340] {{MetaspaceSize}} or {{MaxMetaspaceExpansion}} does anything? In my experience they just cause a metaspace OOM before the GC can collect classes. Let's leave them out if we don't find they help?
 * After you applied my patch now there is a test-jvm-dtest and a test-distributed target. They do different things so maybe we should have both, but should we unify the names e.g. ""test-jvm-dtest-forking"" and ""test-jvm-dtest"" maybe?;;;","25/Feb/19 13:26;ifesdjeen;Thank you for the review! Fixed tests not showing up, example failure: [here|https://circleci.com/gh/ifesdjeen/cassandra/1366#tests/containers/2].

Committed to [2.2|https://github.com/apache/cassandra/commit/a7d8ba7b10a441f9710724e65a939a46add0ae78] and merged up to [3.0|https://github.com/apache/cassandra/commit/b27cc37abdef959c599440edc9fb85e0bc567249], [3.11|https://github.com/apache/cassandra/commit/7b462ec46753943281b3b4d4b106bcc4625bfae7] and [trunk|https://github.com/apache/cassandra/commit/5c4c75cac70dfb885a9905b7fc33a910f83ac989].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent client requests from blocking on executor task queue,CASSANDRA-15013,13214422,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sumanth.pasupuleti,sumanth.pasupuleti,sumanth.pasupuleti,07/Feb/19 17:22,03/Sep/20 13:16,13/Jul/23 08:38,15/Jul/19 13:55,3.0.19,3.11.5,4.0,4.0-alpha1,,Messaging/Client,,,,0,pull-request-available,,,"This is a follow-up ticket out of CASSANDRA-14855, to make the Flusher queue bounded, since, in the current state, items get added to the queue without any checks on queue size, nor with any checks on netty outbound buffer to check the isWritable state.
We are seeing this issue hit our production 3.0 clusters quite often.",,benedict,cscotta,jasobrown,jeromatron,jjirsa,laxmikant99,n.v.harikrishna,sumanth.pasupuleti,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-16100,,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/19 06:50;sumanth.pasupuleti;15013-3.0.txt;https://issues.apache.org/jira/secure/attachment/12974481/15013-3.0.txt","12/Jul/19 06:50;sumanth.pasupuleti;15013-3.11.txt;https://issues.apache.org/jira/secure/attachment/12974480/15013-3.11.txt","12/Jul/19 06:50;sumanth.pasupuleti;15013-trunk.txt;https://issues.apache.org/jira/secure/attachment/12974482/15013-trunk.txt","15/Feb/19 02:00;sumanth.pasupuleti;BlockedEpollEventLoopFromHeapDump.png;https://issues.apache.org/jira/secure/attachment/12958807/BlockedEpollEventLoopFromHeapDump.png","15/Feb/19 02:00;sumanth.pasupuleti;BlockedEpollEventLoopFromThreadDump.png;https://issues.apache.org/jira/secure/attachment/12958808/BlockedEpollEventLoopFromThreadDump.png","15/Feb/19 02:01;sumanth.pasupuleti;RequestExecutorQueueFull.png;https://issues.apache.org/jira/secure/attachment/12958809/RequestExecutorQueueFull.png","07/Feb/19 17:22;sumanth.pasupuleti;heap dump showing each ImmediateFlusher taking upto 600MB.png;https://issues.apache.org/jira/secure/attachment/12957937/heap+dump+showing+each+ImmediateFlusher+taking+upto+600MB.png","10/Jul/19 13:56;sumanth.pasupuleti;perftest2_15013_base_flamegraph.svg;https://issues.apache.org/jira/secure/attachment/12974207/perftest2_15013_base_flamegraph.svg","10/Jul/19 13:56;sumanth.pasupuleti;perftest2_15013_patch_flamegraph.svg;https://issues.apache.org/jira/secure/attachment/12974208/perftest2_15013_patch_flamegraph.svg","10/Jul/19 12:46;sumanth.pasupuleti;perftest2_blocked_threadpool.png;https://issues.apache.org/jira/secure/attachment/12974189/perftest2_blocked_threadpool.png","10/Jul/19 12:49;sumanth.pasupuleti;perftest2_cpu_usage.png;https://issues.apache.org/jira/secure/attachment/12974194/perftest2_cpu_usage.png","10/Jul/19 12:58;sumanth.pasupuleti;perftest2_heap.png;https://issues.apache.org/jira/secure/attachment/12974197/perftest2_heap.png","10/Jul/19 12:38;sumanth.pasupuleti;perftest2_read_latency_99th.png;https://issues.apache.org/jira/secure/attachment/12974184/perftest2_read_latency_99th.png","10/Jul/19 12:39;sumanth.pasupuleti;perftest2_read_latency_avg.png;https://issues.apache.org/jira/secure/attachment/12974185/perftest2_read_latency_avg.png","10/Jul/19 12:35;sumanth.pasupuleti;perftest2_readops.png;https://issues.apache.org/jira/secure/attachment/12974182/perftest2_readops.png","10/Jul/19 12:43;sumanth.pasupuleti;perftest2_write_latency_99th.png;https://issues.apache.org/jira/secure/attachment/12974188/perftest2_write_latency_99th.png","10/Jul/19 12:41;sumanth.pasupuleti;perftest2_write_latency_avg.png;https://issues.apache.org/jira/secure/attachment/12974187/perftest2_write_latency_avg.png","10/Jul/19 12:40;sumanth.pasupuleti;perftest2_writeops.png;https://issues.apache.org/jira/secure/attachment/12974186/perftest2_writeops.png","10/Jul/19 05:47;sumanth.pasupuleti;perftest_blockedthreads.png;https://issues.apache.org/jira/secure/attachment/12974132/perftest_blockedthreads.png","10/Jul/19 05:58;sumanth.pasupuleti;perftest_connections_count.png;https://issues.apache.org/jira/secure/attachment/12974141/perftest_connections_count.png","10/Jul/19 05:57;sumanth.pasupuleti;perftest_cpu_usage.png;https://issues.apache.org/jira/secure/attachment/12974139/perftest_cpu_usage.png","10/Jul/19 05:58;sumanth.pasupuleti;perftest_heap_usage.png;https://issues.apache.org/jira/secure/attachment/12974140/perftest_heap_usage.png","10/Jul/19 05:55;sumanth.pasupuleti;perftest_readlatency_99th.png;https://issues.apache.org/jira/secure/attachment/12974135/perftest_readlatency_99th.png","10/Jul/19 05:55;sumanth.pasupuleti;perftest_readlatency_avg.png;https://issues.apache.org/jira/secure/attachment/12974136/perftest_readlatency_avg.png","10/Jul/19 05:47;sumanth.pasupuleti;perftest_readops.png;https://issues.apache.org/jira/secure/attachment/12974133/perftest_readops.png","10/Jul/19 05:56;sumanth.pasupuleti;perftest_writelatency_99th.png;https://issues.apache.org/jira/secure/attachment/12974137/perftest_writelatency_99th.png","10/Jul/19 05:56;sumanth.pasupuleti;perftest_writelatency_avg.png;https://issues.apache.org/jira/secure/attachment/12974138/perftest_writelatency_avg.png","10/Jul/19 05:46;sumanth.pasupuleti;perftest_writeops.png;https://issues.apache.org/jira/secure/attachment/12974130/perftest_writeops.png",,28.0,sumanth.pasupuleti,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,Clients,,Thu Jul 25 00:03:17 UTC 2019,,,,,,,,,,,,"0|yi0r9s:",9223372036854775807,,,,,,,benedict,,,,Normal,,2.0.0,,,"[5a03898c680ed6ada63901e8a4b278ccc8070717|https://github.com/apache/cassandra/tree/5a03898c680ed6ada63901e8a4b278ccc8070717]",,,,,,,,,"Passing UTs: https://circleci.com/gh/sumanth-pasupuleti/cassandra/288
Passing jvm dtests: https://circleci.com/gh/sumanth-pasupuleti/cassandra/287
DTests with 2 failures (seems unrelated): https://circleci.com/gh/sumanth-pasupuleti/cassandra/290#tests/containers/81, https://circleci.com/gh/sumanth-pasupuleti/cassandra/289#tests/containers/66",,,,,"08/Feb/19 00:00;sumanth.pasupuleti;Working on a fix to make the Flusher queue bounded.;;;","08/Feb/19 09:50;benedict;Have you tested that this approach resolves your issues?

There's a deadlock that could occur with this change, as the request executor is also blocking, so the Netty event loop could block for room on the request executor, and the request executor could block on queueing to the Flusher (that will be executed on the eventLoop).

Probably we should be disabling reads from the inbound channel during overflow, in both cases, rather than blocking either the eventLoop or the requestExecutor.  The behaviour of blocking the eventLoop could also be the cause of your flusher queue growing so large.;;;","11/Feb/19 20:17;sumanth.pasupuleti;[~benedict] By making the flusher queue bounded, my intention is *not* to make the request executor blocked on enqueuing if the queue is full, rather it would _drop_ the response if the flusher queue is full. This should avoid the deadlock situation you are referring to, I believe.;;;","12/Feb/19 08:20;benedict;There is a potential semantic change here that could have negative consequences for clients, so we need to be careful.  

Presently, clients can expect a response to every message they send to the server while they are connected; the server handles time outs, failures etc. and makes sure to respond to the client in some way (unless the connection is lost).  If we change this, we would need to corroborate client behaviour - do they all gracefully timeout outstanding requests, for instance, or do they kill them on a per-connection basis?

In either case, though, it is a shame to discard work that we've gone to the effort of performing.  From a cluster stability perspective, this is more likely to put the cluster under steadily more pressure, rather than less, as the client will no doubt want to retry this work.  It is better to either discard work that has yet to be initiated, or to try not to discard work at all and provide back pressure signals to the client.

I think it would also be more relevant as an initial step to remove the blocking behaviour on incoming, so that the eventLoop can always service the outgoing queue to prevent this build up.  There's a strong chance the build up of outgoing messages you see is down to the eventLoop that must process it being blocked on offering work to the {{requestExecutor}}, and by removing this block the outgoing queue will not accumulate so readily.

There are two options if we do this: stop reading from incoming channels when the {{requestExecutor}} is full, or throw {{OverloadedException}}.  In my opinion, this is exactly what TCP back pressure is for, but we also have a world where clients have been depending on the server trying its best to never push back, so they have inadequate queueing models internally, with no support for noticing or handling this back pressure.  

This to me is a design flaw that should be addressed in clients, but we could mitigate it for now by increasing the size of our {{requestExecutor}} queue (which is actually unnecessarily small), or even making it unbounded and simply tracking the total number of bytes we have read off the wire but not answered.  Perhaps we could even make the behaviour of {{OverloadedException}} vs back pressure a connection-configurable option, so that clients with poor flow control can utilise {{OverloadedException}} to handle this, and those with better control can use normal TCP flow control mechanisms.

What do you think?;;;","15/Feb/19 02:02;sumanth.pasupuleti;[~benedict] Your theory seems to be spot on (I have all the evidence supporting it from the heap dumps and thread dumps now).
 * Evidence of requestExecutor queue full (indicated by taskPermit), and all 128 workers busy (indicated by workPermit)
 !RequestExecutorQueueFull.png|thumbnail! 
 * Evidence of blocked epollEventLoopGroup threads (from heap)
!BlockedEpollEventLoopFromHeapDump.png|thumbnail!
 * Evidence of blocked epollEventLoopGroup threads (from thread dump)
!BlockedEpollEventLoopFromThreadDump.png|thumbnail!

 ;;;","15/Feb/19 02:10;sumanth.pasupuleti;Regarding the fix,
 * Changes to requestExecutor queue size
a. Making it unbounded - tracking the total number of bytes we have read off the wire but not answered
b. Keep it bounded, by giving a bigger size

I think both options are good, but to keep things simple I am inclined towards (b) - increasing the default queue size
 * When requestExecutor is full, based on the new configuration option, either
a. stop reading from incoming channels (TCP back pressure)
b. Throw OverloadedException

I think we should do both, and have a configurable as you suggest.;;;","15/Feb/19 03:50;jasobrown;I agree with upping the max queue depth (or unbounded plus size monitoring) as well as stop reading from the socket (by setting netty's {{autoRead}} to false). I'm not, however, convinced about adding yet another configuration option; adding more configs options only complicates the lives of operators. How will an operator know how to set it most appropriately to their use case(s)? We should choose the best solution, *document it*, and go with that as a built-in behavior. (Note: I'm amenable to throwing the OverloadedException, as well.);;;","15/Feb/19 07:46;benedict;[~jasobrown]: FWIW, I was proposing a _client configurable_ option.  So operators shouldn't need to do anything - in fact, perhaps only client _authors_ would ever specify this, though some might make this available to the developer using their library if their application semantics prefer one or the other.

I don't mind which we pick as the default, and don't mind if this has a user configurable option, but while tcp back pressure should be the preferred mechanism some clients probably don't behave well in the face of it, and for these clients specifying OverloadedException behaviour is probably useful.

[~sumanth.pasupuleti]: Also, if you do the work of implementing back pressure, I am happy to make the change to monitor bytes instead of queued items.  I don't think it should be significantly more challenging, and it would permit us to more tightly bound system resource consumption.;;;","15/Feb/19 13:02;jasobrown;[~benedict] Ahhhh, I see now that's what you intended by \{{connection-configurable option}}. I'm fine with that.

I'm not sure if specifying the 'backpressure type' would require a change to the native protocol. I think it would be most appropriate in the OPTIONS section (and thus {{OptionasMessage}}), but I might be mistaken.  However, I wonder if we should break that work out into a separate ticket to unblock the other work here, so that it can be backported and fixed in production. wdyt?;;;","15/Feb/19 13:28;benedict;I guess maybe let's wait and see how much more complicated it would be?  You're right that the {{OptionsMessage}} should be sufficient for supplying the option, and I think the complicated bit is going to be negotiating safely with the {{requestExecutor}} when we should start and stop reading. 

If it turns out to be super challenging, by all means let's make it a 4.0-only follow-up, but if (as I suspect) it's nominally extra work, I think it's better to tie up the work while there's pressure to do so. WDYT?;;;","15/Feb/19 13:53;jasobrown;Yup, I agree the harder part, programming wise, is {{requestExecutor}} stuffs, and let's plow through that first. The {{OptionsMessage/client protocol work}} is significantly easier, as I think we agree, but would that qualify as a change to the native protocol, for which we need to wait for a major rev (as in, 4.0)? Or are additive additions ok acceptable for previous native protocol versions? We might have a policy or general advice around this, but I don't know.

 

Either way, [~sumanth.pasupuleti] has enough to work forward for now, and we can figure out the native protocol-impacting stuffs in parallel.;;;","15/Feb/19 13:56;benedict;I would be OK with either approach.  There's no strong reason not to add a feature to the protocol that is optional, though - it's not actually a protocol change, just a behavioural change to a message that is permitted on the protocol today.  Since the riskiest change will be fixing the underlying bug, I'd be in favour of at least supporting this option for clients in 3.0, if we intend to fix this behaviour that far back.

But I'm also comfortable with limiting the client option to 4.0

;;;","15/Feb/19 14:04;jasobrown;Ahh, I just reread the {{doc/native_protocol_v5.spec}}, and the OPTIONS are a semi-defined map, basically. I thought they were a fixed listing (primarily because we only support a fixed set of compression types). OK, so any version works for me :).;;;","01/Apr/19 05:59;sumanth.pasupuleti;[~benedict] 
Pull Request: https://github.com/apache/cassandra/pull/308
Passing UTs: https://circleci.com/gh/sumanth-pasupuleti/cassandra/261
Passing JVM DTests: https://circleci.com/gh/sumanth-pasupuleti/cassandra/260
Passing DTests with vnodes: https://circleci.com/gh/sumanth-pasupuleti/cassandra/262
DTests without vnodes with two failures: https://circleci.com/gh/sumanth-pasupuleti/cassandra/263 (looking into the two failures);;;","01/Apr/19 16:32;benedict;Thanks [~sumanth.pasupuleti] for the patch.  I've only taken a quick glance at it so far, but it looks pretty good, and I'm looking forward to integrating it.

There's only one substantive comment I have for the moment, which is that I think when we {{setAutoread(false)}}, we need to not discard the message.  We should perform only one of the two options; if we ever discard, I think we should always let the user know by throwing {{OverloadedException}}.

If we want to use back pressure without throwing away any messages that are over our limit, I can think of two fairly straight forward mechanisms, with the best being unfortunately quite difficult given our current Netty pipeline.  
 # Like we have done for CASSANDRA-15066, we would ideally leave the message unparsed in the buffer until we have capacity to process it.  
 # Alternatively, as an easier solution, we can unconditionally enqueue the message to the executor, assuming that we must have a fairly limited quantity of bytes we can overshoot by

Also, as a point of consideration only, we _might_ also want to limit the number of bytes we have in-flight per-endpoint, rather than per-channel, to avoid a given host spamming the database with many connections.  It's perhaps not a very good unit to limit by, either, since an IP address may host many applications, but an application can also open an arbitrary number of connections, and crowd out all other hosts...  

Just something to consider, I'm not sure what the best constraints are here.  Perhaps, similar to CASSANDRA-15066, a very small per-connection limit that can always be consumed, to ensure progress on any given channel, with per-endpoint and global limits for when these are exceeded (though this is less obvious than for internode, as there could be many more clients connected, so it would be plausible for these low limits to consume a great deal in combination).

It might be that we want to introduce the concept of an application identifier, so that users who want to run multiple applications per host can do so, while still ensuring QoS to other applications if one goes awry.;;;","02/Apr/19 18:16;sumanth.pasupuleti;Thanks for the feedback [~benedict]

+1 on unconditionally enqueuing the message to the executor when we setAutoRead(false), and throwing OverloadedException each time a message is discarded. In other words, we would never discard a message if the client chose to go with backpressure option, rather we just setAutoRead(false) and process the message.

Regarding in-flight per-endpoint, and having an application identifier, I like the suggestion, as it offers better guarantees on throttling client instances, however, I propose cutting a separate ticket for that work, and keeping the scope limited for this current ticket.;;;","02/Apr/19 18:22;benedict;{quote}In other words, we would never discard a message if the client chose to go with backpressure option
{quote}
+1
{quote}I propose cutting a separate ticket for that work, and keeping the scope limited for this current ticket
{quote}
How about a middle ground: we implement the per-endpoint (IP address) limit (which would be easily generalised to incorporate an application identifier) in this patch, so that the logical behaviour of the message control flow isn't really revisited, we just have to change the inputs and introduce any client API changes in the follow-up patch?

I personally have a preference for trying to get all of the logical semantics settled in the first patch, though I'm not deeply wed to that.;;;","09/Apr/19 23:35;sumanth.pasupuleti;[~benedict] Regarding the per-endpoint limit, we have had multiple back and forth discussions within our team, and since driver offers maxConnectionsPerHost option, it is quite possible for one client instance to have more than one connection to a given C* instance. I will be working on changes to implement the per-endpoint limit, in addition to per-connection and global limit.;;;","16/Apr/19 17:22;sumanth.pasupuleti;[~benedict] here is the PR that includes per-endpoint limit https://github.com/apache/cassandra/pull/311/commits/a03cd0550118e3c1dc98694c0dc0ed84824853d1
This patch has a drawback of a slow leak (we never remove an endpoint from the endpoint inflight map) - looking for advice.;;;","16/Apr/19 17:42;benedict;[~sumanth.pasupuleti] thanks for the update.  As to addressing the slow leak, I would propose the following:

* Instead of a {{Map<InetAddress, AtomicLong>}} in {{Dispatcher}} have a {{Map<InetAddress, Dispatcher>}} in {{Server.Initializer}}
* Add a reference count to the {{Dispatcher}} as well as an atomic {{bytesInFlight}}
* In {{Server.Initializer.initChannel}}, lookup the socket InetAddress:
*# If there is no {{Dispatcher}} create one 
*# If the {{Dispatcher}} cannot increment its reference count, remove it from the map and goto (1)
*# Otherwise we've taken ownership of the {{Dispatcher}} and can use it
* Then, in the {{Dispatcher}}, override {{channelInactive}} to decrement our reference count and remove ourselves from the map if we've been freed

This also marginally reduces the per-message cost of enforcing these constraints.  WDYT?;;;","20/Apr/19 06:05;sumanth.pasupuleti;Updated patch: [https://github.com/apache/cassandra/pull/313]

Passing UTs and DTests: https://circleci.com/workflow-run/31dabaa6-eab8-4f00-a711-f1b210bf7578

Thanks [~benedict]. I learnt from your suggestion, {{Ref}} class is useful for getting around the race conditions I was initially worried about, to evict endpoint from the map.
Attached patch evicts endpoint along the lines of your proposal, except that, I used a new class {{EndpointPayloadTracker}}, in place of suggested class ({{Dispatcher}}). Having Dispatcher mapped against endpoint makes it as 1:1 Dispatcher per endpoint, whereas currently it is one Dispatcher per Channel, and I rely on that association to store channel level inflight payload, which is then useful to turn off backpressure on a channel (one of the conditions I check to {{setAutoRead}}(true) is when channel level inflight payload comes down to zero).

A few other changes I have made as part of this updated patch
 * Removed channel level threshold with the worry of too many config knobs (channel level, endpoint level, global level). So each time endpoint/global thresholds are exceeded, a channel is put backpressure on, or an overloadedexception is thrown.
 * In addition to memory based limit, added another tracker and limit check based on number of requests in flight - this is to keep a check on a situation where there are too many in-coming requests with small enough payload that get around memory limit checks, but result in blocking event loop threads.;;;","01/May/19 15:35;benedict;Hi Sumanth,

Thanks for the updated patch. This is not a full review, just some initial feedback.

First, I think we could do with revisiting the naming of a couple of things. The config parameters should probably be prefixed with native_transport for consistency, and the connection parameter should be shorter (since we encode every byte) and perhaps convey the intent - maybe THROW_ON_OVERLOAD?

Secondly, the patch looks to have some data races when updating shared state. It might be sensible to reuse what we have already produced for this in CASSANDRA-15066 - if you look in [this branch|https://github.com/belliottsmith/cassandra/tree/messaging-improvements] you will find a class called {{ResourceLimits}} that we use to impose per-endpoint and global limits, which is exactly what you’re doing here. There’s not much sense in duplicating the work, so perhaps you can copy the class and use it in this patch; it shouldn’t change before we commit.

Similarly, the accesses of {{requestPayloadInFlightPerEndpoint}} need to be made atomic. In {{initChannel}} this means grabbing the result of {{computeIfAbsent}} and to {{tryRef}} this - if we fail, we need to immediately remove the object you fetched from the map and try again (not just loop, else we may block on another thread removing it). In {{tidy}} we need to remove the (key, value) pair to ensure we do not remove a newer object that has replaced us. Similarly for invocations of {{containsKey}}, we need to instead invoke {{get}} and check the result is not null.

In general, in concurrent operation we need to access the shared state once up front, and only refresh it when necessary, as it can change underneath us at any time.

Finally, we should remove the task queue length limit from {{requestExecutor}} else this patch won’t actually stop us blocking the event loop.

Look forward to giving the final patch a full review in the near future. ;;;","13/May/19 03:18;sumanth.pasupuleti;Thanks for the feedback [~benedict]. I have incorporated it in the branch https://github.com/sumanth-pasupuleti/cassandra/commits/15013_trunk_2

However, with this change, we will lose the ability to change the endpoint/global limits on a node on the fly, without restarting C* process.

Added forceAllocate method to ResourceLimits, to accommodate backpressure scenario.
A few DTest failures, seemingly from race conditions on allocate/release. Working on figuring out where the race is coming from.
Dtest results:
https://circleci.com/gh/sumanth-pasupuleti/cassandra/369#tests
https://circleci.com/gh/sumanth-pasupuleti/cassandra/368#tests;;;","14/May/19 11:25;benedict;Thanks [~sumanth.pasupuleti].  I started reviewing your new changes, and part way through realised we could potentially simplify this all a great deal with a slightly different approach.  Namely, if we were to hash endpoints to a specific eventLoop when accepting the connection.  If we were to do this, we could have very simple per-thread accounting, and we could even aggregate all of the per-endpoint channels into a single flusher for stopping/starting together once they exceed their limits.  Everything would be single threaded, so our logic would be much simpler to reason about.

This isn't without its tradeoffs - potentially users might have a setup with a single application node speaking to the cluster, but this would be a very peculiar system design to pair with Cassandra, and a single dedicated eventLoop for this node would still likely suffice for a majority of workloads.  We also have the potential issue of endpoint collisions, but if we use a cryptographic hash function this should only be a problem for a very small number of nodes (and if we ever find it is a real problem, we can remedy it)

What do you think?  I'm sorry for moving the goal posts suddenly, it just hadn't occurred to me until now.  My goal is only the best patch, so I'm interested to hear your thoughts.;;;","15/May/19 10:08;benedict;So, thinking on it a bit more, I don't think this would actually be a very large change, but it also wouldn't simplify things as much as I might like.  It might only save concurrency for endpoint resource allocation.  So I'll review the patch as it is, and we can consider after that if we want to make any further changes.

It looks like I also made an error in my first skim of the patch, or I was looking at a different version - there's no need to set the queue limit to -1; Integer.MAX_VALUE is fine - if we hit that limit we have bigger problems :) ;;;","15/May/19 11:28;benedict;I've pushed some minor suggestions [here|https://github.com/belliottsmith/cassandra/tree/15013-suggestions] around naming:

# Tried to make the native_transport config parameters have more consistent naming with prior parameters - feel free to modify them further, if you think you can improve them still
# {{forceAllocate}} -> {{allocate}}, which is usually the alternative to {{tryAllocate}}
# Shortened THROW_ON_OVERLOAD parameter

There are three remaining bugs, and I've paused review until they can be addressed:

# {{this::releaseItem}} is unsafe to provide to the {{Flusher}} constructor, since these are unique to a channel, and the {{Flusher}} is per-eventLoop.  If we choose to hash all connections on an endpoint to a single eventLoop this would be easy to accommodate, or otherwise {{FlushItem}} needs to be the implementor of {{release()}}
# I don't think we can use {{Ref}} for management of the {{EndpointPayloadTracker}}.  The {{Tidy}} implementation requires a reference to the object itself, and anyway logically deleting itself after release defeats the point of Ref (which is leak detection).  It's impossible for it to detect a leak and cleanup, if the strong reference is cleaned up by this process (since there will always be a strong reference until it invokes, and it requires there to be no strong references, it will never invoke).  Probably we should use a simple AtomicInteger to manage reference counts.  I think it would be cleanest to encapsulate the map management inside a static method in {{EndpointPayloadTracker}} as well.
# I think we currently have a race condition around the release of a channel (and its {{EndpointPayloadTracker}}) and the attempt to release capacity from the {{EndpointPayloadTracker}} we have requests in flight for.  Channels can be invalidated before we complete requests issued by them, so we must be sure to release from the tracker we allocated from, so that we do not wrap into negative on release.
;;;","17/May/19 00:53;sumanth.pasupuleti;Incorporated the feedback from your branch (naming and TODOs) and from the jira comments.
Here is the updated change: https://github.com/sumanth-pasupuleti/cassandra/commit/45e31829e839d7e74b08566d7e501a46ed818330.

A couple of major changes
* Dispatcher would never query the map for getting EndpointPayloadTracker, rather it uses the reference it already has.
* FlushItem gets a reference to the corresponding Dispatcher, so it calls releaseItem on the right Dispatcher.
* I implemented tryRef and release that manage refCount on EndpointPayloadTracker, which ""should"" be thread safe


All UTs and DTests pass.
https://circleci.com/workflow-run/bb6b2eb6-daa6-41c1-9a3d-44b53bc7fb50
;;;","17/May/19 12:01;benedict;Thanks [~sumanth.pasupuleti], the patch is looking really good.  Some remaining questions:

* Do we need requestsProcessed metric?  We already have {{regularStatementsExecuted}} and {{preparedStatementsExecuted}} which should track closely for the traffic we care about.
* Conversely, do we want some metric to track back pressure being deployed?  It’s not clear exactly what semantics we would want to maintain here, since we don’t _currently_ pause all channels for a given endpoint when the endpoint overflows, and it’s also unclear if we would want to track this per-client (probably not, although it would be really nice to do so)
* I think it would be nice to manage {{requestPayloadInFlightPerEndpoint}} entirely inside {{EndpointPayloadTracker}} ; it's presently only accessed once outside in an adjacent class, but it would be very simple to hide the map entirely, as well as {{tryRef}}, and simply offer a {{public static get}} method in {{EndpointPayloadTracker}}.  WDYT?
* It might also be nice to introduce a new version of {{EndpointAndGlobal.release}} that informs the caller if we are presently above or below the limits.  This would simplify the re-activation of a channel.

What do you also think about starting/stopping all channels for an endpoint at once, when we cross the threshold?  I don't think it is essential, but is probably worth considering, as it makes our limits even less clearly defined (given we're permitted to cross them already, once per channel; it would be nice to tighten that to once per-endpoint);;;","19/May/19 07:02;sumanth.pasupuleti;Thanks for the additional feedback. I've made following further changes to the [patch|https://github.com/sumanth-pasupuleti/cassandra/commit/98126f5d887228f5e88eca66f007873b52a0aacf]:
* Removed {{requestsProcessed}} metric.
* Added {{BackpressureDeployed}} metric to indicate how many times server attempted to apply backpressure.
* {{EndpointPayloadTracker}} manages {{requestPayloadInFlightPerEndpoint}}, and an external consumer of {{EndpointPayloadTracker}} calls a static {{get}} that internally does {{tryRef}}
* {{EndpointAndGlobal.release}} returns {{ABOVE_LIMIT}} or {{BELOW_LIMIT}} Outcome.

[Passing Tests|https://circleci.com/workflow-run/561b655c-c3d8-4ea6-9997-ed24754ad133]

{{EndpointPayloadTracker}} is the only accessor of {{globalRequestPayloadInFlight}}, however, it did not seem to make semantic sense of moving {{globalRequestPayloadInFlight}} as a member of {{EndpointPayloadTracker}}.

Regarding starting/stopping all channels for an endpoint at once, I agree it would make the system better respect the limits than otherwise letting each channel cross once (incase of backpressure mode). However, since each channel's connection properties maybe different (THROW_ON_OVERLOAD vs BACKPRESSURE), we will then have to check the property and make a decision accordingly. I am inclining towards punting this for now. Let me know what you think.

Regarding per-client limit, I see it as a good logical extension to this patch, and I would like to tackle it as a separate ticket. Also, as an additional ticket, I would like to explore extending this concurrency limitting patch to go beyond the current parameter of incoming payload which works well for writes, but not necessarily for reads, maybe considering the response payload and/or no. of concurrent tasks in flight.;;;","26/Jun/19 16:13;benedict;Thanks [~sumanth.pasupuleti].  I think this is very close to commit.

I've pushed a small number of extra suggestions [here|https://github.com/belliottsmith/cassandra/tree/15013-suggestions].  Mostly just minor stylistic simplifications, as well as a modification to of back pressure deployed to simply the number of connections currently experiencing back pressure, since it's not entirely clear how an operator would meaningfully interpret the number of times it was independently applied (since it would be applied more often for small messages than large ones)

Let me know what you think, and we can hopefully see about merging this soon.;;;","27/Jun/19 07:12;sumanth.pasupuleti;+1 on the suggestions [~benedict]. I have applied your commit on my [branch|https://github.com/sumanth-pasupuleti/cassandra/commits/15013_trunk_2] and ran the tests.
UTs and JVM DTests pass. All Dtests pass except for 6 failures which seem unrelated. 
https://circleci.com/workflow-run/04b77dd7-7dca-49d4-8328-e55b357fcca6;;;","27/Jun/19 13:46;benedict;Thanks [~sumanth.pasupuleti].  I realised I had forgotten to handle the case of {{channelInactive}} whilst paused, so I've pushed a tiny follow-up modification.  If it looks good to you, I'll merge the lot into trunk.;;;","27/Jun/19 14:12;sumanth.pasupuleti;Thanks for catching the {{channelInactive}} case. lgtm.

One thing remaining I suppose, is to revisit the default limits [https://github.com/apache/cassandra/commit/98126f5d887228f5e88eca66f007873b52a0aacf#diff-b66584c9ce7b64019b5db5a531deeda1R173]
{code:java}
// TODO: Revisit limit
public volatile long native_transport_max_concurrent_requests_in_bytes_per_ip = 3000000000L;
public volatile long native_transport_max_concurrent_requests_in_bytes = 5000000000L;{code};;;","27/Jun/19 14:14;benedict;Yes, I guess 3GiB per IP is probably too high, as is 5GiB per node.  Not really sure what a good default is - probably it should be a function of heap size like most of our other limits.;;;","27/Jun/19 17:28;sumanth.pasupuleti;[~benedict] Makes sense. I pushed a [change|https://github.com/sumanth-pasupuleti/cassandra/commit/0c75ecf7b6f0824786b840c6cba167eb393b92ce] to [this|https://github.com/sumanth-pasupuleti/cassandra/commits/15013_trunk_2] branch.

Per node defaults to 1/10th of heap size, per IP defaults to 1/40th of heap size.

Similar test [results|https://circleci.com/workflow-run/c61d7df2-c77a-4eab-a954-a59f6165f372] as previous run.;;;","28/Jun/19 13:36;benedict;Thanks [~sumanth.pasupuleti].  I'm happy this resolves the issue, but it occurs to me it would be great to confirm the worst case performance impact of this change is manageable - particularly given your performance testing infrastructure.

The worst case behaviour is probably LOCAL_ONE in-memory reads on a high core count multi-socket machine, serving thousands of TCP connections from the same host at a total rate of hundreds of thousands of QPS.  So a single i3.16xlarge system would be optimal.

Does that sound reasonable to you?;;;","28/Jun/19 14:42;sumanth.pasupuleti;Absolutely. I was thinking along similar lines as well, as to performance test this patch - I was more thinking about having two clusters with the same configuration, one with and one without this patch, and measuring their performance when put under similar load. As you said, we can employ LOCAL_ONE traffic and see that data is small enough to fit into memory. I am thinking about at least a 3-node i3.8xl /i3.16xl cluster setup to even out any instance related issues but will ensure we have thousands of TCP connections per host and ops in the order of ~100k qps per host (or at least as high as we can get). Will start these tests today.;;;","28/Jun/19 14:44;benedict;Fantastic, that sounds great.;;;","10/Jul/19 06:29;sumanth.pasupuleti;Performance tests were run against two C* clusters, one running latest trunk (referred to as _cass_perf_15013_base_), and one running [latest trunk + 15013 patch] (referred to as _cass_perf_15013_patch_). Two NDBench clusters, with similar configuration to emit similar traffic, were setup to throw load at each of the C* clusters. Each of the C* clusters is a single region, six i3.8xl nodes, and each of the NDBench clusters is 450 nodes.

Following is the analysis of the perf run:
# No blocked threadpool in patch, vs blocked threadpool in trunk
 !perftest_blockedthreads.png! 
# Similar writeops
 !perftest_writeops.png! 
# Patch does more readops vs trunk
 !perftest_readops.png! 
# Comparable read and write latencies (99th and avg)
 !perftest_readlatency_99th.png! 
 !perftest_readlatency_avg.png! 
 !perftest_writelatency_99th.png! 
 !perftest_writelatency_avg.png! 
# Comparable CPU usage
 !perftest_cpu_usage.png! 
# Comparable heap usage
 !perftest_heap_usage.png! 
# Connections count (~1000 connections per C* node)
 !perftest_connections_count.png! 
;;;","10/Jul/19 07:01;benedict;Thanks for these [~sumanth.pasupuleti]!

Just to log for watchers, I have had a brief chat with Sumanth, and we intend to capture flame graphs to see if we can explain the 10% (5 percentage point) bump in average CPU utilisation, which may well be down to competition on a single variable for every operation.  This is a worst case cost, given the formulation of this test, which was the whole point - but it's potentially still significant, so we might need to reduce friction by e.g. assigning each connection its own share of the pie at connection, so that we only have to compete for the shared resource infrequently (when we overshot our share, or need to dis/connect).  We'll see what the flame graphs show.

We will also try to explain the different shape of heap utilisation graph - which might be as simple as only one node is coordinating instead of all three, for instance.;;;","10/Jul/19 07:32;benedict;[~jjirsa] has just pointed out that we are seeing _double_ the throughput of reads (I hadn't carefully looked at all the graphs), which very likely explains the increase in CPU

It would be nice to perform a run with fixed throughput at a rate trunk can manage, so we can get directly comparable results.  But this is a _huge_ win, nice work!

(If we can grab flame graphs anyway, there'd be no harm and it would be nice to take a look to confirm nothing unexpected);;;","10/Jul/19 14:18;sumanth.pasupuleti;Thanks [~benedict] and [~jjirsa]. I've re-run the perf test such that throughput is same across both the clusters (I had to tone down the ndbench client pointing to patch version of C* by quite a lot to make it equal to trunk throughput).

I have attached the flamegraphs - CPU usage is a tad lower in patch vs trunk (based on avg).

Also attached all the metrics of this perf run (files starting with perftest2*).

Following is the summary of perf run #2
* Very similar readops and write ops
* Read latency (99th and avg) slightly better for patch vs trunk
* Write latency 99th similar between patch and trunk. Write latency avg is slightly better for patch vs trunk
* No blocked threadpool for patch
* Cpu usage (avg) is slightly better for patch vs trunk
* Heap usage pattern was similar between patch and trunk
;;;","11/Jul/19 13:51;benedict;[~sumanth.pasupuleti]: this looks good to go.  Could you prepare the patch for commit, and submit 3.0, 3.x and trunk versions for me to merge?  Thanks!;;;","12/Jul/19 06:56;sumanth.pasupuleti;Sure [~benedict]. Here are the patches:

*3.0*
Patch:  [^15013-3.0.txt] 
Passing UTs and DTests https://circleci.com/workflow-run/c7889003-9c58-4099-9530-0439bf241238
Github: https://github.com/apache/cassandra/compare/cassandra-3.0...sumanth-pasupuleti:15013_3.0?expand=1

*3.11*
Patch:  [^15013-3.11.txt] 
Passing UTs and DTests https://circleci.com/workflow-run/46de0958-850a-4531-a15f-fd1df0c65aac
Github: https://github.com/apache/cassandra/compare/cassandra-3.11...sumanth-pasupuleti:15013_3.11?expand=1

*trunk*
Patch:  [^15013-trunk.txt] 
Passing UTs and DTests https://circleci.com/workflow-run/67e43b0b-7f13-4de2-8fbd-7cab3d72b607
Github: https://github.com/apache/cassandra/compare/trunk...sumanth-pasupuleti:15013_trunk?expand=1
;;;","15/Jul/19 13:59;benedict;Thanks!

Great work, I've merged it up.  

For future reference, it's super helpful if you can also modify CHANGES.txt, to avoid unnecessary rebasing and merging.  If you can also stick to the commit message format (that I'm unsure if we've codified anywhere, and we should probably rectify), it also makes merging easier:

""
<One sentence description, usually Jira title and CHANGES.txt summary>

<Optional lengthier description>

patch by <Authors>; reviewed by <Reviewers> for CASSANDRA-#####
"";;;","15/Jul/19 15:08;sumanth.pasupuleti;Thanks for the feedback and review [~benedict]. I will incorporate this feedback into my future commits and submit a patch for the documentation if necessary.;;;","25/Jul/19 00:03;sumanth.pasupuleti;[~benedict] Created a task with patch attached to update the documentation w.r.t. suggested commit message format. https://issues.apache.org/jira/browse/CASSANDRA-15246;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NamedThreadLocalFactory unnecessarily wraps runnable into thread local deallocator ,CASSANDRA-15008,13213896,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,05/Feb/19 13:06,31/May/19 17:19,13/Jul/23 08:38,11/Feb/19 17:22,,,,,,,,,,0,,,,"FastThreadLocalThread already does wrapping of runnable by calling [FastThreadLocalRunnable.wrap in constructor in Netty code|https://github.com/netty/netty/blob/netty-4.1.18.Final/common/src/main/java/io/netty/util/concurrent/FastThreadLocalThread.java#L60]. Second call is unnecessary and incurs unnecessary additional wrapping.",,djoshi,ifesdjeen,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 31 17:19:20 UTC 2019,,,,,,,,,,,,"0|yi0o1k:",9223372036854775807,,,,,,,djoshi,,,,Low,,,,,,,,,,,,,,,,,,,"05/Feb/19 13:10;ifesdjeen;|[patch|https://github.com/apache/cassandra/compare/trunk...ifesdjeen:CASSANDRA-15008-trunk]|[tests|https://circleci.com/workflow-run/4a966220-3274-40ea-b543-cf5148cf121d]|

Change was introduced in 4.1.18, so 3.0 and 3.11 are not susceptible.;;;","05/Feb/19 17:50;djoshi;LGTM +1;;;","11/Feb/19 17:21;ifesdjeen;Thank you for the review!

Committed to trunk with [c49d42f318c735676d1cb8984c1dee8ae46b3c0d |https://github.com/apache/cassandra/commit/c49d42f318c735676d1cb8984c1dee8ae46b3c0d].;;;","31/May/19 17:19;jeromatron;Should the fix version be set to 4.0 for this ticket?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect rf validation in SimpleStrategy,CASSANDRA-15007,13213878,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,djoshi,Apozyan,Apozyan,05/Feb/19 11:48,15/May/20 08:05,13/Jul/23 08:38,19/Feb/19 00:30,4.0,4.0-alpha1,,,,CQL/Semantics,,,,0,,,,"Getting uninformative ConfigurationException when trying to create a keyspace with SimpleStrategy and no replication factor.

{{cqlsh> create keyspace test with replication = \{'class': 'SimpleStrategy'};}}
{{ConfigurationException:}}",,Apozyan,bdeggleston,djoshi,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/19 11:49;Apozyan;15007.patch;https://issues.apache.org/jira/secure/attachment/12957626/15007.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 19 00:36:04 UTC 2019,,,,,,,,,,,,"0|yi0nxk:",9223372036854775807,,,,,bdeggleston,,bdeggleston,,,,Low,,,,,,,,,,,,,,,,,,,"05/Feb/19 11:50;Apozyan;Here is a simple fix

{{cqlsh> create keyspace test with replication = \{'class': 'SimpleStrategy'};}}
{{ConfigurationException: SimpleStrategy requires a replication_factor strategy option.}};;;","13/Feb/19 07:08;djoshi;Hi [~Apozyan], thanks for the fix. Will this work in the general case?;;;","13/Feb/19 07:58;djoshi;Nevermind, looking at it more deeply, I noticed this is a recent regression. We already validate options and throw a meaningful message in 3.x. If you notice there is a method called {{validateOptions}} that does exactly what you tried doing in the constructor. It looks like this broke on trunk. So now {{SimpleStrategy}} creation fails with an exception before we have a chance to validate the options.

I have fixed the regression and added a test to avoid future regressions.

||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/15007-trunk]|
|[utests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/15007-trunk]|;;;","13/Feb/19 09:58;Apozyan;Thanks [~djoshi3] for looking into this. You are right, validation broke on trunk, and {{validateOptions}} now is never called if constructor throws an exception.

I compared {{SimpleStrategy.java}} and {{NetworkTopologyStrategy.java}} and I saw that in the latter case validation code is duplicated both in constructor and {{validateOptions}} method. So I think its fine to do the same for SimpleStrategy.;;;","19/Feb/19 00:30;bdeggleston;Looks good to me. Committed to trunk as {{47d4971b56d97ba8a528f7c17bfd6b11f1ababa3}};;;","19/Feb/19 00:36;djoshi;Thanks, [~bdeggleston]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anti-compaction briefly corrupts sstable state for reads,CASSANDRA-15004,13213220,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,benedict,bdeggleston,bdeggleston,01/Feb/19 01:33,01/Aug/21 12:24,13/Jul/23 08:38,06/Feb/19 13:52,3.0.18,3.11.7,4.0,4.0-alpha1,,Local/Compaction,,,,0,,,,"Since we use multiple sstable rewriters in anticompaction, the first call to prepareToCommit will remove the original sstables from the tracker view before the other rewriters add their sstables. This creates a brief window where reads can miss data.",,bdeggleston,benedict,cscotta,eperott,jasonstack,jeromatron,laxmikant99,marcuse,weideng,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,Correctness -> Transient Incorrect Response,,,,,,,,Challenging,Fuzz Test,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 06 13:53:44 UTC 2019,,,,,,,,,,,,"0|yi0jvk:",9223372036854775807,,,,,,,marcuse,,,,Critical,,,,,,,,,,,,,,,,,,,"01/Feb/19 01:42;bdeggleston;|[3.0|https://github.com/bdeggleston/cassandra/tree/15004-3.0]|[3.11|https://github.com/bdeggleston/cassandra/tree/15004-3.11]|[trunk|https://github.com/bdeggleston/cassandra/tree/15004-trunk]|
|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F15004-3.0]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F15004-3.11]|[circle|https://circleci.com/gh/bdeggleston/workflows/cassandra/tree/cci%2F15004-trunk]|;;;","01/Feb/19 09:01;marcuse;+1

pushed a unit test: [3.0|https://github.com/krummas/cassandra/commits/blake/15004-3.0] [3.11|https://github.com/krummas/cassandra/commits/blake/15004-3.11] [trunk|https://github.com/krummas/cassandra/commits/blake/15004-trunk]

edit: not sure what is going on with the dtests though, probably need a restart;;;","01/Feb/19 09:06;benedict;Nice catch, and it looks like a good fix to me.

(+1);;;","04/Feb/19 13:41;benedict;Blake realised there was an issue with the patch he posted, so I have put together an alternative patch with input from [~krummas].

[3.0|https://github.com/belliottsmith/cassandra/tree/15004-3.0] [3.11|https://github.com/belliottsmith/cassandra/tree/15004-3.11] [4.0|https://github.com/belliottsmith/cassandra/tree/15004-4.0]

These patches extract an interface for {{LifecycleTransaction}} and no-op the relevant calls ({{prepareToCommit}} and {{obsoleteOriginals}}) so that {{SSTableRewriter.prepareToCommit}} does not update the tracker - these are then invoked directly once each rewriter has finished its other preparatory work.

It's a bit ugly and still finicky, but probably better/safer than more invasive surgery at this point in time.;;;","04/Feb/19 15:13;marcuse;updated unit tests [3.0|https://github.com/krummas/cassandra/tree/15004-3.0] [3.11|https://github.com/krummas/cassandra/tree/15004-3.11] [trunk|https://github.com/krummas/cassandra/tree/15004-trunk] also adds checks that the files on disk are what we expect;;;","05/Feb/19 15:27;marcuse;lgtm, just need a few comments explaining what is going on and the comment mentioning {{permitRedundantTransitions}} needs to be removed/updated;;;","05/Feb/19 15:54;benedict;Thanks.  I've pushed branches with updated comments.;;;","05/Feb/19 17:08;marcuse;+1;;;","06/Feb/19 13:53;benedict;Thanks, committed to [3.0|https://github.com/apache/cassandra/commit/44785dd2eec5697eec7e496ed3a73d2573f4fe6a], [3.11|https://github.com/apache/cassandra/commit/9199e591c6148d14f3d12784af8ce5342f118161] and [4.0|https://github.com/apache/cassandra/commit/df62169d1b6a5bfff2bc678ffbeb0883a3a576b5];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid leaking threads when anticompaction fails,CASSANDRA-15002,13212868,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,30/Jan/19 17:49,15/May/20 08:04,13/Jul/23 08:38,31/Jan/19 08:16,4.0,4.0-alpha1,,,,Consistency/Repair,,,,0,,,,"If anticompaction fails on a node, a message is sent to all repair participants that this session is failed. If the other participants successfully finish their anticompactions we will not shut down the executor in `LocalSessions` since we can't change the state from ""FAILED"" to ""PREPARED"" and throw exception before calling shutdown.",,bdeggleston,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,marcuse,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 31 08:16:18 UTC 2019,,,,,,,,,,,,"0|yi0hps:",9223372036854775807,,,,,bdeggleston,,bdeggleston,,,,Normal,,,,,,,,,,,,,,,,,,,"30/Jan/19 18:02;marcuse;Patch: https://github.com/krummas/cassandra/commits/marcuse/15002
Tests: https://circleci.com/gh/krummas/workflows/cassandra/tree/marcuse%2F15002

The branch contains 2 commits, first one fixes the thread leak and retries acquiring sstables for 1 minute, the second commit adds rate limiting to anticompaction and makes anticompactions stoppable again (should have been done in CASSANDRA-14935).

The reasoning behind adding retries when trying to acquire sstables is that if there is an sstable [0, 100] and the user submits a repair on [0, 50], we will anticompact the sstable in to two new sstables [0, 50], [51, 100] - if a user submits a repair on [51, 100] while the first anticompaction is executing, we would fail immediately, since the sstable the new repair requires is busy anticompacting. By retrying to acquire sstables we give the first anticompaction a chance to finish and the new repair would be able to grab the new [51, 100] sstable - it does not need to wait for the whole first repair to finish.;;;","31/Jan/19 00:12;bdeggleston;+1;;;","31/Jan/19 08:16;marcuse;And committed as {{7f634feb7cf1fdb135133946ffd75efa681b8cb7}}, thanks

Ran the failing test in a loop locally successfully, don't think this patch caused the test failure;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up javadoc errors,CASSANDRA-14995,13211136,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,djoshi,djoshi,djoshi,23/Jan/19 00:01,15/May/20 08:01,13/Jul/23 08:38,05/Feb/19 03:54,4.0,4.0-alpha1,,,,Documentation/Javadoc,,,,0,,,,There are approximately 100 javadoc errors related to obsolete or incorrect pointers. Let's fix them.,,djoshi,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 05 05:56:52 UTC 2019,,,,,,,,,,,,"0|yi072o:",9223372036854775807,,,,,,,jjirsa,,,,Low,,,,,,,,,,,,,,,,,,,"23/Jan/19 00:08;djoshi;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/14995-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/14995-trunk]|
||;;;","05/Feb/19 03:54;jjirsa;Thanks. Committed as {{ff73c33ab78f70cd0e70280c89e8d8a46f5536d8}}
;;;","05/Feb/19 05:56;djoshi;Thanks, [~jjirsa]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catch CorruptSSTableExceptions and FSErrors in ALAExecutorService,CASSANDRA-14993,13210966,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,spod,spod,spod,22/Jan/19 12:22,15/May/20 08:00,13/Jul/23 08:38,04/Feb/19 15:48,3.0.19,3.11.5,4.0,4.0-alpha1,,,,,,0,,,,"Actively handling CorruptSSTableExceptions and FSErrors currently only happens during opening of sstables and in the default exception handler. What's missing is to catch these in AbstractLocalAwareExecutorService as well. Therefor I propose to add calls to FileUtils.handleCorruptSSTable/handleFSError there, too, so we don't miss invoking the disk failure policy in that case.",,aweisberg,cscotta,jeromatron,jjirsa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,spod,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 04 15:48:24 UTC 2019,,,,,,,,,,,,"0|yi0614:",9223372036854775807,,,,,aweisberg,,aweisberg,,,,Normal,,,,,,,,,,,,,,,,,,,"23/Jan/19 20:44;aweisberg;Do we ever want to skip JVMStability inspector? It checks all the causes so while the top level might be FSError it may contain something else.

Should we be inspecting nested exceptions for FSError?;;;","24/Jan/19 13:30;spod;inspectThrowable is called in any case, either in DefaultFSErrorHandler.handleCorruptSSTable/handleFSError(), or in the else statement.

Should we inspect nested exceptions for FSError? Not 100% sure, but I'd probably first start fixing the logging statement, so we get proper stack traces and see what we get.;;;","31/Jan/19 17:00;aweisberg;+1

I just noticed the switch from WARN to ERROR. Seems appropriate since we have bubbled up to such a high level exception handler that if it wasn't an ERROR it would have been handled already.;;;","04/Feb/19 15:48;spod;Thanks, Ariel!

Merged as c94a6aa7e5dd6d to cassandra-3.0

Tests CircleCI:
[3.0|https://circleci.com/workflow-run/f8154177-162a-416e-ad79-4c3c86f66906]
[3.11|https://circleci.com/workflow-run/eaaf37f8-ea9e-4a0a-b138-c26855e44309]
[trunk|https://circleci.com/workflow-run/76cab84e-33f8-45b8-ba4a-2d5ba8fa37a3]

 
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL Cert Hot Reloading should check for sanity of the new keystore/truststore before loading it,CASSANDRA-14991,13210456,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,djoshi,djoshi,djoshi,18/Jan/19 18:53,14/Jun/20 15:45,13/Jul/23 08:38,08/Feb/19 16:57,4.0,4.0-alpha1,,,,Feature/Encryption,,,,0,security,,,"SSL Cert Hot Reloading assumes that the keystore & truststore are valid. However, a corrupt store or a password mismatch can cause Cassandra to fail accepting new connections as we throw away the old {{SslContext}}. This patch will ensure that we check the sanity of the certificates during startup and during hot reloading. This should protect against bad key/trust stores. As part of this PR, I have cleaned up the code a bit.",,aweisberg,djoshi,jeromatron,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,Availability -> Unavailable,,,,,,,,Normal,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 08 16:57:58 UTC 2019,,,,,,,,,,,,"0|yi02vs:",9223372036854775807,,,,,aweisberg,,aweisberg,,,,Normal,,,,,,,,,,,,,,,,,,,"18/Jan/19 18:57;djoshi;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/14991-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/14991-trunk]|
||;;;","18/Jan/19 21:13;djoshi;dtest with vnodes have some failures but they're unrelated to this change. dtests without vnodes and utests are good.;;;","19/Jan/19 01:11;djoshi;The latest run is clean. I had to rebase the branch on latest trunk.;;;","22/Jan/19 21:58;aweisberg;Looks pretty good.

[This won't return an error to nodetool|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:14991-trunk?expand=1#diff-3514653a59c886f2106d3099124f03bbR315]
[Maybe don't bind the encryption option references in case they ever become hot swappable|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:14991-trunk?expand=1#diff-3514653a59c886f2106d3099124f03bbR355].
[Should buildTrustStore always be true for server options?|https://github.com/apache/cassandra/compare/trunk...dineshjoshi:14991-trunk?expand=1#diff-3514653a59c886f2106d3099124f03bbR374];;;","23/Jan/19 02:26;djoshi;Hi [~aweisberg]. Thanks for looking at the PR. I have addressed comments 1 & 2. Regarding #3, the issue is that when we do optional client auth, we build the truststore based on the value in the configuration so I left it to the value of the client auth variable. When someone sets client auth to off, it is ok to have invalid values for the truststore configuration. If we try to enforce valid values it will deviate from its current behavior. Let me know if you have any other comments.;;;","23/Jan/19 17:18;aweisberg;RE #3, but it's always hardcoded to true when the server actually goes to build the SSL certs?
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/NettyFactory.java#L295
https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/NettyFactory.java#L372
Why should we test with different parameters then are used when we actually go to construct the SSL context? Is the SSL context being constructed with invalid parameters?;;;","23/Jan/19 19:23;djoshi;{{OptionalSecureInitializer}} inherits from {{AbstractSecureInitializer}}. It passes in {{require_client_auth}} which is sourced from the configuration. So we could generate a {{SslContext}} without instantiating the truststore. See [here|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/transport/Server.java#L409];;;","23/Jan/19 19:41;aweisberg;That is for the client -> server not server -> server. Server -> server always hard codes it to true. So why wouldn't we test the server context the way it would be retrieved when we actually need to retrieve them?;;;","23/Jan/19 20:56;djoshi;You're right Ariel. I have pushed an update. Hopefully that fixes the confusion :);;;","31/Jan/19 17:14;aweisberg;+1
Can you add CHANGES.txt, squash, rebase, remove the CircleCI changes? I'll merge it then.;;;","01/Feb/19 01:03;djoshi;Thanks, [~aweisberg]. Here's the commit https://github.com/dineshjoshi/cassandra/commit/307717b307e2f244f6cf5cdb0f4e764e73a72734;;;","08/Feb/19 16:57;aweisberg;Committed as [367cdc95514d4550db57054c90fb794fc29179d1|https://github.com/apache/cassandra/commit/367cdc95514d4550db57054c90fb794fc29179d1]. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException when SELECTing token() on only one part of a two-part partition key,CASSANDRA-14989,13210158,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,djoshi,manuelkiessling,manuelkiessling,17/Jan/19 13:47,08/Oct/21 15:49,13/Jul/23 08:38,28/Jan/19 12:37,4.0,4.0-alpha1,,,,CQL/Interpreter,,,,0,,,,"I have the following schema:

{code}
CREATE TABLE query_tests.cart_snapshots (
    cart_id uuid,
    realm text,
    snapshot_id timeuuid,
    state text,
    PRIMARY KEY ((cart_id, realm), snapshot_id)
) WITH CLUSTERING ORDER BY (snapshot_id DESC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';
{code}

In cqlsh, I try the following query:

{code}select token(cart_id) from cart_snapshots ;{code}

This results in cqlsh returning {{ServerError: java.lang.NullPointerException}}, and the following error in the server log:

{code}
DC1N1_1  | ERROR [Native-Transport-Requests-1] 2019-01-16 12:17:52,075 QueryMessage.java:129 - Unexpected error during query
DC1N1_1  | java.lang.NullPointerException: null
DC1N1_1  |       at org.apache.cassandra.db.marshal.CompositeType.build(CompositeType.java:356) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.db.marshal.CompositeType.build(CompositeType.java:349) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.config.CFMetaData.serializePartitionKey(CFMetaData.java:805) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.functions.TokenFct.execute(TokenFct.java:59) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.ScalarFunctionSelector.getOutput(ScalarFunctionSelector.java:61) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$SelectionWithProcessing$1.getOutputRow(Selection.java:666) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:492) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.newRow(Selection.java:458) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.processPartition(SelectStatement.java:860) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:790) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:438) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:416) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:289) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:117) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_181]
DC1N1_1  |       at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]
DC1N1_1  | ERROR [Native-Transport-Requests-1] 2019-01-16 12:17:52,076 ErrorMessage.java:384 - Unexpected exception during request
DC1N1_1  | java.lang.NullPointerException: null
DC1N1_1  |       at org.apache.cassandra.db.marshal.CompositeType.build(CompositeType.java:356) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.db.marshal.CompositeType.build(CompositeType.java:349) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.config.CFMetaData.serializePartitionKey(CFMetaData.java:805) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.functions.TokenFct.execute(TokenFct.java:59) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.ScalarFunctionSelector.getOutput(ScalarFunctionSelector.java:61) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$SelectionWithProcessing$1.getOutputRow(Selection.java:666) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.getOutputRow(Selection.java:492) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.selection.Selection$ResultSetBuilder.newRow(Selection.java:458) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.processPartition(SelectStatement.java:860) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:790) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:438) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:416) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:289) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:117) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:224) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:255) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:240) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:116) ~[apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:517) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.transport.Message$Dispatcher.channelRead0(Message.java:410) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:35) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:348) [netty-all-4.0.44.Final.jar:4.0.44.Final]
DC1N1_1  |       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [na:1.8.0_181]
DC1N1_1  |       at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:109) [apache-cassandra-3.11.3.jar:3.11.3]
DC1N1_1  |       at java.lang.Thread.run(Thread.java:748) [na:1.8.0_181]
{code}
",Using {{cqlsh 5.0.1}} on a Mac OS X host system with Cassandra 3.11.3 running via Docker for Mac from the official {{cassandra:3.11.3}} image.,aleksey,blerer,djoshi,jmeredithco,manuelkiessling,vinaykumarcse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-13878,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,djoshi,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 28 12:38:14 UTC 2019,,,,,,,,,,,,"0|yi0128:",9223372036854775807,3.0.17,3.11.3,4.0,,,,aleksey,jmeredithco,,,Low,,,,,,,,,,,,,,,,,,,"17/Jan/19 15:38;jmeredithco;Reproducible for me with a smaller reproducer. The token function expects the number of arguments (and probably the argument types) to match the partition key, however in the failing example only a partial key is passed. 
{code:java}
cqlsh:query_tests> DROP TABLE repro14989;
cqlsh:query_tests> CREATE TABLE repro14989(pk1 uuid, pk2 text, PRIMARY KEY ((pk1, pk2)));
cqlsh:query_tests> INSERT INTO repro14989(pk1,pk2) VALUES (uuid(),'pk2');
cqlsh:query_tests> SELECT token(pk1) FROM repro14989;
ServerError: java.lang.NullPointerException
cqlsh:query_tests> SELECT token(pk1,pk2) FROM repro14989;

 system.token(pk1, pk2)
------------------------
    7705645267149106563

(1 rows)
{code}

In the example above, this query should work
{code}
select token(cart_id, realm) from cart_snapshots ;
{code}

As a proposed fix, when preparing the query, Cassandra should check the arguments for {{token}} are suitable for serializing a partition key before executing the function.;;;","22/Jan/19 07:21;djoshi;||trunk||
|[branch|https://github.com/dineshjoshi/cassandra/tree/14989-trunk]|
|[utests &amp; dtests|https://circleci.com/gh/dineshjoshi/workflows/cassandra/tree/14989-trunk]|
||;;;","23/Jan/19 00:18;jmeredithco;Thanks for the patch.  I like the refactor to clean up FunctionResolver.get. The only real comment I have on it is the name for maybeNativeFunction - I think the other functions are native functions too, the thing that's special about token/toJson/fromJson is they support polymorphic types and so don't fit in the Candidate structure something like maybeSpecialFunction or maybePolymorphicFunction would be more descriptive.

After that, +1 from me (not that I can commit it);;;","23/Jan/19 00:30;djoshi;Thanks, [~jmeredithco] I am not too fussy about naming. I can update it if you feel strongly about it. [~iamaleksey] could you please take a look at the trunk patch? I can backport it on 3.0 & 3.11 as it'll not apply cleanly.;;;","23/Jan/19 13:32;aleksey;The patch is fine, but while reviewing it, I noticed a closely related pre-existing bug in the logic that has been preserved by this patch.

Specifically, if there is a UDF that's also named {{token}}, and the correct keyspace is set via {{USE}} command, then invoking {{token()}} function on a table, with all the right arguments, would fail to resolve to the UDF unless fully qualified.

{code}
create function test.token(val double) returns null on null input returns int language java as 'return 0.0;';
create table test(id int primary key, col double);

select token(col) from test;
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Type error: col cannot be passed as argument 0 of function system.token of type int""

select token(1.0) from test;
InvalidRequest: Error from server: code=2200 [Invalid query] message=""Type error: 1.0 cannot be passed as argument 0 of function system.token of type int""
{code}

Using fully-qualified name returns the expected result:
{code}
cqlsh:test> select test.token(1.0) from test;

 test.token(1.0)
-----------------
               0

(1 rows)
cqlsh:test> select test.token(col) from test;

 test.token(col)
-----------------
               0
{code}

However I'm failing to see a reason why non-qualified invocation shouldn't follow the general logic of filtering candidate functions for the best match. Short-circuiting here, and forcing validation that early seems suboptimal to me.

EDIT: I reckon the same applies to {{fromJson}} and {{toJson}} functions.;;;","24/Jan/19 01:46;djoshi;[~iamaleksey] based on your feedback, I have changed the patch. It now not only fixes the NPE but also finds the closest matching function if the same function name exists in current & system keyspace. I've added a test to capture this behavior so we wont regress in the future.;;;","28/Jan/19 12:37;aleksey;The first two unit tests were exact duplicates of each other, so I removed one, and added a missing test for mismatched argument counts.

Committed to trunk as [174cf761f7897443080b8a840b649b7eab17ae25|https://github.com/apache/cassandra/commit/174cf761f7897443080b8a840b649b7eab17ae25], thanks.;;;","28/Jan/19 12:38;aleksey;Didn't commit to 3.0 or 3.11, but don't mind backporting, in principle, despite the issue not being a major one.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Building javadoc with Java11 fails,CASSANDRA-14988,13209854,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,tommy_s,tommy_s,tommy_s,16/Jan/19 10:24,15/May/20 07:59,13/Jul/23 08:38,22/Jan/19 19:14,4.0,4.0-alpha1,,,,Documentation/Javadoc,,,,0,Java11,,,"When building trunk with Java11 building javadoc fails with this error:
{noformat}
[javadoc] /repos/tmp/cassandra/src/java/org/apache/cassandra/hints/HintsBufferPool.java:28: error: package sun.nio.ch is not visible
[javadoc] import sun.nio.ch.DirectBuffer;
[javadoc] ^
[javadoc] (package sun.nio.ch is declared in module java.base, which does not export it to the unnamed module)
[javadoc] 1 error{noformat}
This import is unused and was probably added by mistake, removing it fixes the problem.",,djoshi,jeromatron,jjirsa,tommy_s,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,tommy_s,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 22 19:16:24 UTC 2019,,,,,,,,,,,,"0|y0028o:",9223372036854775807,,,,,,,djoshi,,,,Critical,,,,,,,,,,,,,,,,,,,"16/Jan/19 10:54;tommy_s;I have a branch with a patch to remove the unused import here: [cassandra-14988|https://github.com/tommystendahl/cassandra/tree/cassandra-14988];;;","22/Jan/19 07:47;djoshi;Hi [~tommy_s] I can't repro this issue. My build goes through fine with OpenJDK 11.0.2. See below -

{noformat}
     [exec] copying static files... done
     [exec] copying extra files... done
     [exec] dumping search index in English (code: en) ... done
     [exec] dumping object inventory... done
     [exec] build succeeded, 153 warnings.
     [exec]
     [exec] The HTML pages are in build/html.
     [exec]
     [exec] Build finished. The HTML pages are in build/html.

BUILD SUCCESSFUL
Total time: 1 minute 58 seconds
{noformat}

Could you please add more info here? JDK version, are you using OpenJDK or Oracle JDK? How are you invoking the doc generation?  What are your environment variables set to esp. JAVA_HOME and JAVA8_HOME?;;;","22/Jan/19 09:34;tommy_s;Hi [~djoshi3]

Thanks for looking in to this issue.

I use Oracle jdk 11.0.2 and ant 1.10.5.

 
{noformat}
$ echo $JAVA_HOME
/usr/lib/jvm/jdk-11.0.2/

$ echo $JAVA8_HOME
/usr/lib/jvm/jdk1.8.0_191{noformat}
To reproduce the problem I build with {{ant realclean build javadoc}}, if I just do {{ant realclean javadoc}} I don't get this problem.

I tested with OpenJdk 11.0.2 and got the same issue, I have also tested Oracle jdk 11.0 and 11.0.1.

 ;;;","22/Jan/19 17:39;djoshi;[~tommy_s] I was able to repro it with the steps you provided. Your change fixes it. +1.;;;","22/Jan/19 19:14;jjirsa;Committed as {{4aa022e215bcffbf84becea9253ef07b2e87a19b}}
;;;","22/Jan/19 19:16;djoshi;Thanks, [~jjirsa];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test stdout capture is malfunctioning in 4.0,CASSANDRA-14974,13208989,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benedict,benedict,benedict,11/Jan/19 12:22,27/Aug/20 15:13,13/Jul/23 08:38,15/Jan/19 19:14,4.0,4.0-alpha1,,,,Test/dtest/java,Test/unit,,,0,,,,"In 3.x unit tests we make sure to capture stdout to the unit test files, in case tests log to stdout and not to a logger.  However, in 4.0 due to a configuration parameter that is deprecated in logback, the logic is short-circuited silently.

Once fixed, this affects the cleanup of in-jvm dtests which would register an infinite chain of System.out overrides (one for each node), so we make the functionality robust to multiple instantiations, as well as improve its startup/shutdown sequence guarantees.",,benedict,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14922,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 21 14:20:13 UTC 2019,,,,,,,,,,,,"0|u00qrc:",9223372036854775807,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,,"11/Jan/19 14:58;benedict;[patch|https://github.com/belliottsmith/cassandra/tree/14974], [CI|https://circleci.com/workflow-run/e834041e-6361-4435-a3bc-2720aa5337a9];;;","15/Jan/19 13:50;ifesdjeen;+1, just need to re-run {{DatabaseDescriptorRefTest}} ;;;","15/Jan/19 19:14;benedict;Thanks, committed as [a43b651f8e35dd7081b8593057f118ed0c49cfd6|https://github.com/apache/cassandra/commit/a43b651f8e35dd7081b8593057f118ed0c49cfd6];;;","21/Jan/19 14:20;ifesdjeen;Not sure I should reopen it, but it looks like we might have missed something. Currently, it looks like we double-wrap subsitute logger which results into:

{code}
INFO  [AsyncAppender-Worker-ASYNC] 2019-01-21 14:41:01,193 SubstituteLogger.java:169 - DEBUG [MemtableFlushWriter:1] INSTANCE_127.0.0.3 2019-01-21 14:41:01,193 ColumnFamilyStore.java:1153 - Flushed to [BigTableReader(path='/private/var/folders/d_/t6f7wkp53g7bf1pcnm69ljwc0000gn/T/dtests8830319164832856679/node3/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/na-2-big-Data.db')] (1 sstables, 5.155KiB), biggest 5.155KiB, smallest 5.155KiB

INFO  [AsyncAppender-Worker-ASYNC] 2019-01-21 14:41:01,196 SubstituteLogger.java:169 - DEBUG [MemtableFlushWriter:1] INSTANCE_127.0.0.2 2019-01-21 14:41:01,196 ColumnFamilyStore.java:1153 - Flushed to [BigTableReader(path='/private/var/folders/d_/t6f7wkp53g7bf1pcnm69ljwc0000gn/T/dtests8830319164832856679/node2/data/system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/na-2-big-Data.db')] (1 sstables, 5.155KiB), biggest 5.155KiB, smallest 5.155KiB

INFO  [AsyncAppender-Worker-ASYNC] 2019-01-21 14:41:01,226 SubstituteLogger.java:169 - INFO  [MigrationStage:1] INSTANCE_127.0.0.2 2019-01-21 14:41:01,226 Keyspace.java:368 - Creating replication strategy distributed_test_keyspace params KeyspaceParams{durable_writes=true, replication=ReplicationParams{class=org.apache.cassandra.locator.SimpleStrategy, replication_factor=3}}
{code}

(notice double timestamp, location and log level). If we avoid re-wrapping substitute logger, this won't happen. We need to fix it either here or in multi-JVM patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website documentation search function returns broken links,CASSANDRA-14971,13208446,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,Anthony Grasso,Anthony Grasso,Anthony Grasso,09/Jan/19 01:13,04/Oct/19 16:20,13/Jul/23 08:38,09/Jan/19 05:13,,,,,,Documentation/Website,,,,0,,,,"The search bar on the main page of the [Cassandra Documentation|http://cassandra.apache.org/doc/latest/] returns search [results|http://cassandra.apache.org/doc/latest/search.html?q=cache&check_keywords=yes&area=default] with broken links.

When a link from a returned search is clicked, the site returns a 404 with the message similar to this:
{quote}The requested URL /doc/latest/tools/nodetool/nodetool.rst.html was not found on this server.
{quote}
From the error, it appears that the links are pointing to pages that end in *.rst.html* in their name. The links should point to pages that end in *.html*.",,Anthony Grasso,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/19 01:40;Anthony Grasso;CASSANDRA-14971_v01.patch;https://issues.apache.org/jira/secure/attachment/12954251/CASSANDRA-14971_v01.patch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,Anthony Grasso,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 09 06:17:55 UTC 2019,,,,,,,,,,,,"0|u00nfk:",9223372036854775807,,,,,mck,,mck,,,,Normal,,,,,,,,,,,,,,,,,,,"09/Jan/19 01:28;Anthony Grasso;It looks like the search results are pieced together by the [searchtools.js|https://svn.apache.org/repos/asf/cassandra/site/src/js/searchtools.js] file that lives in the _js_ directory in the SVN [repository|https://svn.apache.org/repos/asf/cassandra/site]. Specifically the {{displayNextItem()}} function walks through the returned results and generates the HTML output. This function generates the filenames using the data in the returned results.

The search results are generated by the {{performObjectSearch}} and {{performTermsSearch}} functions. These functions obtain the file information from the search index. In this case, it is the search index file ([searchindex.js|https://svn.apache.org/repos/asf/cassandra/site/src/doc/4.0/searchindex.js] which is generated by Sphinx.

It appears that we are referencing the documents in the {{filenames}} list property of the search index. These documents contain the *.rst* extension. We should probably be referencing the documents in the {{docnames}} list property of the search index.;;;","09/Jan/19 01:40;Anthony Grasso;Attached {{svn diff}} patch;;;","09/Jan/19 04:42;mck;Solid write up, thanks [~Anthony Grasso].

Patch is +1 from me. Going to test it.;;;","09/Jan/19 05:13;mck;Committed as r1850821;;;","09/Jan/19 06:17;Anthony Grasso;Awesome! Thanks [~mck].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
New releases must supply SHA-256 and/or SHA-512 checksums,CASSANDRA-14970,13208421,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,mck,mshuler,mshuler,08/Jan/19 22:51,15/May/20 08:54,13/Jul/23 08:38,24/Apr/20 13:28,2.2.16,3.0.20,3.11.6,4.0,4.0-beta1,Packaging,,,,0,,,,"Release policy was updated around 9/2018 to state:

""For new releases, PMCs MUST supply SHA-256 and/or SHA-512; and SHOULD NOT supply MD5 or SHA-1. Existing releases do not need to be changed.""

build.xml needs to be updated from MD5 & SHA-1 to, at least, SHA-256 or both. cassandra-builds/cassandra-release scripts need to be updated to work with the new checksum files.

http://www.apache.org/dev/release-distribution#sigs-and-sums",,brooke,mck,mshuler,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14963,,,,,,,,,,,CASSANDRA-15540,CASSANDRA-14962,CASSANDRA-15541,,,,,,,INFRA-14923,,,,"08/Jan/19 23:51;mshuler;0001-Update-downloads-for-sha256-sha512-checksum-files.patch;https://issues.apache.org/jira/secure/attachment/12954240/0001-Update-downloads-for-sha256-sha512-checksum-files.patch","08/Jan/19 23:23;mshuler;0001-Update-release-checksum-algorithms-to-SHA-256-SHA-512.patch;https://issues.apache.org/jira/secure/attachment/12954238/0001-Update-release-checksum-algorithms-to-SHA-256-SHA-512.patch","09/Jan/19 00:56;mshuler;ant-publish-checksum-fail.jpg;https://issues.apache.org/jira/secure/attachment/12954244/ant-publish-checksum-fail.jpg","09/Jan/19 03:21;mshuler;build_cassandra-2.1.png;https://issues.apache.org/jira/secure/attachment/12954260/build_cassandra-2.1.png","09/Jan/19 03:21;mshuler;build_trunk.png;https://issues.apache.org/jira/secure/attachment/12954261/build_trunk.png","27/Jan/20 22:20;mshuler;cassandra-2.1_14970_updated.patch;https://issues.apache.org/jira/secure/attachment/12991954/cassandra-2.1_14970_updated.patch",,,,,,,,,,,,,,,,,,,,,,,,6.0,mck,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 24 13:28:59 UTC 2020,,,,,,,,,,,,"0|u00na0:",9223372036854775807,,,,,,,mck,mshuler,,,Critical,,0.3,,,https://github.com/apache/cassandra/commit/70d8db8558c346603da6e64c035d325834467d3a https://github.com/apache/cassandra-builds/commit/0bdc460cba741a47abb0634905aa7bcd5310f13a,,,,,,,,,`ant artifacts` and cutting 4.0-alpha3,,,,,"08/Jan/19 23:28;mshuler;[^0001-Update-release-checksum-algorithms-to-SHA-256-SHA-512.patch]

Patch against {{cassandra-2.1}} branch. Merges up without conflict.
{noformat}
(cassandra-2.1)mshuler@hana:~/git/cassandra$ ls -l build/*.{gz,sha*}    
-rw-r--r-- 1 mshuler mshuler 25342702 Jan  8 17:04 build/apache-cassandra-2.1.20-SNAPSHOT-bin.tar.gz 
-rw-r--r-- 1 mshuler mshuler       65 Jan  8 17:04 build/apache-cassandra-2.1.20-SNAPSHOT-bin.tar.gz.sha256 
-rw-r--r-- 1 mshuler mshuler      129 Jan  8 17:04 build/apache-cassandra-2.1.20-SNAPSHOT-bin.tar.gz.sha512 
-rw-r--r-- 1 mshuler mshuler 17265833 Jan  8 17:04 build/apache-cassandra-2.1.20-SNAPSHOT-src.tar.gz 
-rw-r--r-- 1 mshuler mshuler       65 Jan  8 17:04 build/apache-cassandra-2.1.20-SNAPSHOT-src.tar.gz.sha256 
-rw-r--r-- 1 mshuler mshuler      129 Jan  8 17:04 build/apache-cassandra-2.1.20-SNAPSHOT-src.tar.gz.sha512
{noformat};;;","08/Jan/19 23:52;mshuler;[^0001-Update-downloads-for-sha256-sha512-checksum-files.patch] attached for the cassandra-builds repo - download the new checksum files for release publication.;;;","09/Jan/19 00:11;brandon.williams;+1;;;","09/Jan/19 00:58;mshuler;I have no idea how the {{ant publish}} task works.. :( I did a staging publish and we still get .md5 and .sha1 checksums.

!ant-publish-checksum-fail.jpg|thumbnail!;;;","09/Jan/19 01:10;mshuler;INFRA-14923 is the issue.;;;","09/Jan/19 01:35;mck;[~mshuler] the asf guidelines applies strictly to the distributed convenience binary artefacts. The asf maven repository doesn't support it yet, that is the nexus repo only keeps sha1 on the jarfiles. (No asf project is using sha-256/512 on maven distributables afaik);;;","09/Jan/19 03:27;mshuler;Our current release process uploads/signs/checksums the tar.gz and maven artifacts to nexus via the 'publish' task, then we vote. After vote, we download the tar.gz/.md5/.sha1 files for final release and promote the staging repo to release. Since the MD5 and SHA files are there in build.xml, I thought the patch for creating the .sha256/.sha512 checksums in the 'release' target were used for release build. They are not. I gave another try at uploading the .sha256/.sha512 files, but realized we never build them due to the target dependencies, so looked a little more.

I created ant target graphs for 2.1 and trunk to get an idea of the target relations. The release task I patched isn't depended on by anything, and currently is completely unused in our release process.

build_cassandra-2.1.png
build_trunk.png

(edit: removed no-thumb images - they are attached..);;;","14/Jan/19 17:50;mshuler;I went ahead and [committed the release target patch to the cassandra-2.1 branch|https://github.com/apache/cassandra/commit/6506684b81a093a329b07cd41f42d858041ba8b7] and merged up. This at least allows us to build the sha256/512 checksum files with ant. This does not fix the maven upload, download, & release problem, yet, so the cassandra-builds patch is incorrect, currently.

Some help fixing the ant mvn-install and publish tasks to push the sha256/512 checksum files to maven as included artifacts would be great! I've tried a couple things, but I'm not getting it right.;;;","22/Jan/19 19:46;spod;Can you add the checksums as part of a comma separate list to an addition [files|http://maven.apache.org/plugins/maven-gpg-plugin/sign-and-deploy-file-mojo.html#files] attribute? What exactly did you try so far to make the sign-and-deploy-file step work?;;;","22/Jan/19 20:17;spod;Also, I think the ultimate goal here should be to provide the sha256/512 files for everything released at [https://dist.apache.org/repos/dist/release/]. Can't we simply copy the checksum files there, as part of uploading other resources to the dist svn tree (finish_release.sh)?;;;","23/Jan/19 16:24;mshuler;I tried switching the dependency from artifacts to release and tried a wildcard in the mvn-install task files, for example \{{file=""${build.dir}/${final.name}-bin.tar.gz*""}}. I don't recall the exact error, but I just don't know ant well enough, so asked for some help.

Yes, we can just do the checksums and toss them in /dist/release/ in finish_release.sh, but that basically flaws the artifacts we're voting on. We should be voting on the entire artifact set, verified by the checksums and gpg signature. I'm just trying to look at the bigger picture and scripting/automating our release process to fix known broken things, as well as make it better/easier for multiple people to contribute to the release process, while trying to keep things stable/simple for user installs.;;;","23/Jan/19 19:45;spod;What you're referring to in the ticket description is the distribution policy, not the release policy. The later doesn't mention any requirement for PMCs to verify checksum, only the detached signature. So I don't see any need to generate any checksums at all, before voting and eventually copying new artifacts into the dist svn tree. I'd also argue that generating checksums locally in finish_release.sh will make things necessarily more complex, compared to generating them via ant and upload+download them from nexus again later and then copy to dist.;;;","05/Feb/19 10:07;mck;[~mshuler],
 since the sha256/512 checksums are only required on the non-maven artefacts, can we not solve this by publishing these pre-vote artefacts to https://dist.apache.org/repos/dist/dev/cassandra/ ?

This also simplifies the post-vote step of publishing these artefacts, as it's simply executing the command:
{{svn mv https://dist.apache.org/repos/dist/dev/cassandra/<version> https://dist.apache.org/repos/dist/release/cassandra/}}

This is also the recommended approach, since people.apache.org as a hosting destination for pre-vote artefacts was deprecated a number of years ago, and why I added it to http://cassandra.apache.org/doc/latest/development/release_process.html#sign-and-upload-distribution-packages-to-bintray
;;;","13/Nov/19 13:54;mck;This is work in progress, but I've pushed the following branches to [cassandra|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_14970] and [cassandra-builds|https://github.com/apache/cassandra-builds/compare/master...thelastpickle:mck/14970_sha512-checksums].

The changes involve…
 - remove the source and binary artefacts from being uploaded to nexus (nexus is meant for maven artefacts)
 - removes the use of people.apache.org for staging test artefacts (this practice was deprecated, with a deadline of 31st December 2012)
 - uses svnpubsub staging of all non-maven artefacts (ie https://dist.apache.org/repos/dist/dev/cassandra/ )
 - adds the rpm docker stuff into the script
 - removes the copy of the source artefact in the debian binary's folder
 - adds a ""test announcement"" email template (it is encouraged to be announcing test builds a few days in advance of starting the vote)

Still to do is…
 - generate the sha512 and gnupg asc signatures on the non-maven artefacts
 - remove the `only_deb` flag (is it really needed?)
 - make corresponding changes to {{finish_release.sh}} and {{upload_bintray.sh}} scripts
 - test rpm docker stuff inside script
 - make patches for 2.2, 3.0, 3.11

[~mshuler], if you agree , shall i continue with this approach?
 ;;;","08/Dec/19 19:51;mck;bq. remove the `only_deb` flag (is it really needed?)

Agreed to keep. ref: https://the-asf.slack.com/archives/CK23JSY2K/p1574199400163100

bq. generate the sha512 and gnupg asc signatures on the non-maven artefacts

This is already done by the {{`ant release`}} task. But I can't see anywhere that is actually calling/using it. I have moved the checksumming into the {{`artifacts`}} tasks (alongside the generation of the original artefacts), and renamed the {{`release}}` task to {{`rat`}}.

The distribution artifacts are no longer getting deployed to the maven staging repository. They don't belong there as they are not maven artefacts. Instead they are just gpg signed, and it is left to the {{prepare_release.sh}} to move them into asf dev dist.;;;","17/Jan/20 23:22;mck;Patches updated at [cassandra|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_14970] and [cassandra-builds|https://github.com/apache/cassandra-builds/compare/master...thelastpickle:mck/14970_sha512-checksums].

Still to do…
* test rpm docker stuff inside script
* test prepare_release.sh
* -make the patches for 2.2, 3.0, 3.11-;;;","25/Jan/20 23:43;mck;The patches against the main repo are ready for review.

||branch||circleci||jenkins pipeline||
|[cassandra_2.1_14970|https://github.com/apache/cassandra/compare/cassandra-2.1...thelastpickle:mck/cassandra-2.1_14970]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-2.1_14970]| |
|[cassandra_2.2_14970|https://github.com/apache/cassandra/compare/cassandra-2.2...thelastpickle:mck/cassandra-2.2_14970]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-2.2_14970]|[!https://builds.apache.org/job/Cassandra-devbranch/16/badge/icon!|https://builds.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/16]|
|[cassandra_3.0_14970|https://github.com/apache/cassandra/compare/cassandra-3.0...thelastpickle:mck/cassandra-3.0_14970]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.0_14970]|[!https://builds.apache.org/job/Cassandra-devbranch/17/badge/icon!|https://builds.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/17]|
|[cassandra_3.11_14970|https://github.com/apache/cassandra/compare/cassandra-3.11...thelastpickle:mck/cassandra-3.11_14970]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Fcassandra-3.11_14970]|[!https://builds.apache.org/job/Cassandra-devbranch/18/badge/icon!|https://builds.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/18]|
|[trunk_14970|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_14970]|[circleci|https://circleci.com/gh/thelastpickle/workflows/cassandra/tree/mck%2Ftrunk_14970]|[!https://builds.apache.org/job/Cassandra-devbranch/19/badge/icon!|https://builds.apache.org/blue/organizations/jenkins/Cassandra-devbranch/detail/Cassandra-devbranch/19]|;;;","27/Jan/20 21:10;mshuler;Set myself as reviewer. First comment: there have been no commits since the cassandra-2.1.21 release (Feb 2019) and this is not a security/critical patch? I suppose if we have a release planned to fix something for 2.1, implement this patch at that time?;;;","27/Jan/20 21:25;mck;bq. Set myself as reviewer. First comment: there have been no commits since the cassandra-2.1.21 release (Feb 2019) and this is not a security/critical patch? I suppose if we have a release planned to fix something for 2.1, implement this patch at that time?

I don't have a problem with that. We will need to remember to do so if the need does arise. Though once the release scripts are also merged (in cassandra-builds) it will become obvious.;;;","27/Jan/20 22:48;mshuler;Patches above for 2.2, 3.0, 3.11, and trunk look good to me on {{ant artifacts}} tests for each branch.
I did also try to build 2.1 and the build currently fails, due to the old http URLs in build.xml and build.properties.default now being disabled. There was someone on slack with the same problem, so I added a 2.1 backport patch to CASSANDRA-15137. With that patch applied to the {{cassandra-2.1}} branch and the 2.1 patch above on top, I did not get the desired .sha256 & .sha512 files created when building the artifacts for some reason. That can probably be worked on later, if needed.;;;","28/Jan/20 06:17;mck;Thanks [~mshuler].

I will commit the 2.2 - trunk changes, but leave the ticket 'in review' as I test the release script  to cut the next releases.;;;","28/Jan/20 07:00;mck;bq. I will commit the 2.2 - trunk changes, but leave the ticket 'in review' as I test the release script to cut the next releases.


Committed as 06a36045fe3dcf07205e2649b2e5eaf0daff5164;;;","30/Jan/20 20:14;mck;The cassandra-builds patch was used to cut and stage the 4.0-alpha3 release.;;;","31/Jan/20 22:45;mck;In addition to the cassandra-builds [patch|https://github.com/apache/cassandra-builds/compare/master...thelastpickle:mck/14970_sha512-checksums], there is an added [patch for updated documentation|https://github.com/apache/cassandra/compare/trunk...thelastpickle:mck/trunk_14970_docs].;;;","03/Feb/20 01:50;brooke;apologies for accidental reassignment. Have fixed. ;;;","19/Mar/20 17:24;mck;Status on this ticket is that [~mshuler] and myself are going to cut one more release with the expectations that no further manual changes or hacks are required during the post-vote actions. The post-vote actions are a bit tricky and can't be tested thoroughly without actually performing the releases.

The next release cut likely is 4.0-alpha4, which is only waiting on CASSANDRA-15358, according to the [dev ML|https://lists.apache.org/thread.html/r2966aa37f42070ed58ed1642eb9f0e24f68a3ecca099d75a840c9ef6%40%3Cdev.cassandra.apache.org%3E].;;;","24/Apr/20 13:28;mck;Committed as [70d8db8558c346603da6e64c035d325834467d3a|https://github.com/apache/cassandra/commit/70d8db8558c346603da6e64c035d325834467d3a] and [0bdc460cba741a47abb0634905aa7bcd5310f13a|https://github.com/apache/cassandra-builds/commit/0bdc460cba741a47abb0634905aa7bcd5310f13a];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReadCommandVerbHandler validateTransientStatus class cast exception ,CASSANDRA-14959,13208068,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aweisberg,aweisberg,aweisberg,07/Jan/19 16:16,15/May/20 08:05,13/Jul/23 08:38,07/Jan/19 22:38,4.0,4.0-alpha1,,,,Consistency/Coordination,,,,0,,,,"Causes a test failure, looks like it should just use instanceof.
{noformat}
Exception in thread Thread-1:
Traceback (most recent call last):
  File ""/usr/local/Cellar/python/3.7.0/Frameworks/Python.framework/Versions/3.7/lib/python3.7/threading.py"", line 917, in _bootstrap_inner
    self.run()
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/src/ccm/ccmlib/cluster.py"", line 189, in run
    self.scan_and_report()
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/src/ccm/ccmlib/cluster.py"", line 182, in scan_and_report
    on_error_call(errordata)
  File ""/Users/aweisberg/repos/cassandra-dtest/dtest_setup.py"", line 137, in _log_error_handler
    pytest.fail(""Error details: \n{message}"".format(message=message))
  File ""/Users/aweisberg/repos/cassandra-dtest/venv/lib/python3.7/site-packages/_pytest/outcomes.py"", line 97, in fail
    raise Failed(msg=msg, pytrace=pytrace)
Failed: Error details: 
Errors seen in logs for: node1
node1: ERROR [ReadStage-2] 2019-01-03 14:02:43,704 AbstractLocalAwareExecutorService.java:167 - Uncaught exception on thread Thread[ReadStage-2,5,main]
java.lang.ClassCastException: org.apache.cassandra.db.PartitionRangeReadCommand cannot be cast to org.apache.cassandra.db.SinglePartitionReadCommand
	at org.apache.cassandra.db.ReadCommandVerbHandler.validateTransientStatus(ReadCommandVerbHandler.java:85)
	at org.apache.cassandra.db.ReadCommandVerbHandler.doVerb(ReadCommandVerbHandler.java:53)
	at org.apache.cassandra.net.MessageDeliveryTask.process(MessageDeliveryTask.java:92)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:54)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$FutureTask.run(AbstractLocalAwareExecutorService.java:162)
	at org.apache.cassandra.concurrent.AbstractLocalAwareExecutorService$LocalSessionFutureTask.run(AbstractLocalAwareExecutorService.java:134)
	at org.apache.cassandra.concurrent.SEPWorker.run(SEPWorker.java:115)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)

FAILED
upgrade_tests/cql_tests.py:2869 (TestCQLNodes3RF3_Upgrade_indev_3_11_x_To_indev_trunk.test_edge_2i_on_complex_pk)
{noformat}",,aweisberg,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14964,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aweisberg,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 07 22:33:30 UTC 2019,,,,,,,,,,,,"0|u00l3k:",9223372036854775807,,,,,ifesdjeen,,ifesdjeen,,,,Normal,,,,,,,,,,,,,,,,,,,"07/Jan/19 16:27;aweisberg;Proposed fix https://github.com/aweisberg/cassandra/commit/290f6dc2e79ff2d265fb2b232799bab0ba3a94cb
CircleCI: https://circleci.com/gh/aweisberg/cassandra/tree/14959-trunk;;;","07/Jan/19 17:12;ifesdjeen;+1, looks good!

Since this is already caught by dtests, we should be good without adding any new tests, too.;;;","07/Jan/19 22:33;aweisberg;Thanks! Committed as [f0494889176873b3f68ae14cc5f1d9dcbc189da9|https://github.com/apache/cassandra/commit/f0494889176873b3f68ae14cc5f1d9dcbc189da9].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counters fail to increment in 2.1/2.2 to 3.X mixed version clusters,CASSANDRA-14958,13208067,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aleksey,aweisberg,aweisberg,07/Jan/19 16:15,02/Aug/19 02:40,13/Jul/23 08:38,14/Jan/19 18:09,3.0.18,3.11.4,,,,Feature/Counters,,,,0,,,,"The upgrade test for this is failing
https://circleci.com/gh/aweisberg/cassandra/2362#tests/containers/1

I confirmed that this is occurring manually using cqlsh against the cluster constructed by the dtest.
{noformat}
cqlsh> describe schema;

CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'}  AND durable_writes = true;

CREATE TABLE ks.clicks (
    userid int,
    url text,
    total counter,
    PRIMARY KEY (userid, url)
) WITH COMPACT STORAGE
    AND CLUSTERING ORDER BY (url ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
    AND comment = ''
    AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
    AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 864000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = '99PERCENTILE';

cqlsh> use ks;
cqlsh:ks> UPDATE clicks SET total = total + 1 WHERE userid = 1 AND url = 'http://foo.com';
cqlsh:ks> SELECT total FROM clicks WHERE userid = 1 AND url = 'http://foo.com'
      ... ;

 total
-------
     0

(1 rows)
{noformat}",,aleksey,aweisberg,benedict,cscotta,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14881,,,,CASSANDRA-13691,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,Correctness -> Recoverable Corruption / Loss,,,,,,,,Challenging,User Report,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 15 20:05:07 UTC 2019,,,,,,,,,,,,"0|u00l3c:",9223372036854775807,,,,,benedict,,benedict,,,,Critical,,,,,,,,,,,,,,,,,,,"11/Jan/19 16:30;aleksey;Unfortunately there is indeed an issue.

In a mixed-version 2.1 + 3.0 cluster, when 3.0 coordinates a counter update request, and chooses to forward the counter mutation to a different leader, such an update would be lost.

For that to happen you need to have a mixed mode cluster with 3.0 node both coordinating and opting to *not* be a leader itself, which is actually a rather likely scenario. I'm (very) surprised that this upgrade bug hasn't been noticed before (before CASSANDRA-14881 that is).;;;","11/Jan/19 18:40;aleksey;Code: [3.0|https://github.com/iamaleksey/cassandra/commits/14958-3.0], [3.11|https://github.com/iamaleksey/cassandra/commits/14958-3.11].
CI: [3.0|https://circleci.com/workflow-run/ad96e947-dff3-4d63-9c34-3e5220e550e4], [3.11|https://circleci.com/workflow-run/8378d5c3-a517-486f-8df0-b8d36c05bd79].

To fix another compatibility issue, CASSANDRA-13691 switched from using local shards to stash counter update values to a special sentinel id. {{LegacyLayout}} code was in one place unfortunately not updated to reflect the change, breaking counter update cell serialisation to legacy nodes, always sending 0-increments.

The linked branches address the issue. There is no new test as this scenario is already covered pretty well by the (now working) upgrade tests.;;;","14/Jan/19 14:04;aleksey;Upgrade test results showing the issue's now fixed: [3.0|https://circleci.com/gh/iamaleksey/cassandra/914], [3.11|https://circleci.com/gh/iamaleksey/cassandra/915].;;;","14/Jan/19 16:57;benedict;+1;;;","14/Jan/19 18:09;aleksey;Cheers. Committed to 3.0 as [ebfa280fac2f43fb88e2e87d81f35b8017222a12|https://github.com/apache/cassandra/commit/ebfa280fac2f43fb88e2e87d81f35b8017222a12] and merged into 3.11, -s ours into trunk.;;;","14/Jan/19 19:33;aweisberg;Thanks! Don't forget to remove the skip annotation on upgrade_tests/cql_tests.py test_counters.;;;","15/Jan/19 20:05;aleksey;[~aweisberg] Would've forgotten if not for your comment - thank you. And just to confirm, two proper upgrade test runs with the annotation removed, with counter tests passing: [3.0|https://circleci.com/gh/iamaleksey/cassandra/921], [3.11|https://circleci.com/gh/iamaleksey/cassandra/923].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paged Range Slice queries with DISTINCT can drop rows from results,CASSANDRA-14956,13208002,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,07/Jan/19 11:45,16/Apr/19 09:29,13/Jul/23 08:38,17/Jan/19 13:12,2.1.21,2.2.14,,,,CQL/Interpreter,,,,0,,,,"If we have a partition where the first CQL row is fully deleted (possibly via TTLs), and that partition happens to fall on the page boundary of a paged range query which is using SELECT DISTINCT, the next live partition *after* it is omitted from the result set. This is due to over fetching of the pages and a bug in trimming those pages where overlap occurs.

This does not affect 3.0+.",,jeromatron,jolynch,marcuse,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,samt,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 04 11:41:19 UTC 2019,,,,,,,,,,,,"0|u00kow:",9223372036854775807,,,,,,,marcuse,,,,Normal,,,,,,,,,,,,,,,,,,,"07/Jan/19 11:59;samt;Patch to change the page trimming for DISTINCT range queries to simply remove the first row from the page.

 
||branch||CI||
|[14956-2.2|https://github.com/beobal/cassandra/tree/14956-2.2]|[circle|https://circleci.com/gh/beobal/workflows/cassandra/tree/cci%2F14956-2.2]|;;;","08/Jan/19 10:39;samt;Pushed a 2.1 branch after discussion on dev@ about one last 2.1 release before EOL. Unfortunately, CircleCI no longer supports v1.0 job configuration so a CI run is going to need the v2.0 config backporting (which we may want to do before a release anyway).

[14956-2.1|https://github.com/beobal/cassandra/tree/14956-2.1];;;","15/Jan/19 14:49;marcuse;+1, just change CASSANDRA-XYZ in the unit test comment;;;","17/Jan/19 13:12;samt;Thanks, committed to 2.1 in {{dd228d4581b020fb2fb788858481c81357d7fa72}} and merged (test only to 3.0+).;;;","31/Jan/19 18:03;jolynch;[~beobal] I think that the merge up to trunk broke trunk test runs (e.g. [7d138e20|https://circleci.com/gh/jolynch/cassandra/415]). I think it has to do with re-using the data directory from {{cassandra-murmur.yaml}}, debugging it now but do you think we need a separate ticket for fixing that test or I can just submit a fixup patch here?

Confirmed it's a pollution issue with the {{EmbeddedCassandraService}} by running two tests that both use it:

{noformat}
$ cat testlist.txt                                                                                                                                                                                                            
org/apache/cassandra/audit/AuditLoggerTest.java
org/apache/cassandra/cql3/PagingTest.java

$ ant testclasslist -Dtest.classlistfile=$(pwd)/testlist.txt
...
[junit-timeout] Testsuite: org.apache.cassandra.cql3.PagingTest
[junit-timeout] Testsuite: org.apache.cassandra.cql3.PagingTest Tests run: 0, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 5.241 sec
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.cql3.PagingTest: Caused an ERROR
[junit-timeout] Unable to gossip with any peers
[junit-timeout] java.lang.RuntimeException: Unable to gossip with any peers
[junit-timeout]         at org.apache.cassandra.gms.Gossiper.doShadowRound(Gossiper.java:1546)
[junit-timeout]         at org.apache.cassandra.service.StorageService.checkForEndpointCollision(StorageService.java:553)
[junit-timeout]         at org.apache.cassandra.service.StorageService.prepareToJoin(StorageService.java:841)
[junit-timeout]         at org.apache.cassandra.service.StorageService.initServer(StorageService.java:699)
[junit-timeout]         at org.apache.cassandra.service.StorageService.initServer(StorageService.java:650)
[junit-timeout]         at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:379)
[junit-timeout]         at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:501)
[junit-timeout]         at org.apache.cassandra.service.EmbeddedCassandraService.start(EmbeddedCassandraService.java:50)
[junit-timeout]         at org.apache.cassandra.cql3.PagingTest.setup(PagingTest.java:63)
[junit-timeout] 
[junit-timeout] 
[junit-timeout] Testcase: org.apache.cassandra.cql3.PagingTest: Caused an ERROR
[junit-timeout] null
[junit-timeout] java.lang.NullPointerException
[junit-timeout]         at org.apache.cassandra.cql3.PagingTest.tearDown(PagingTest.java:81)
[junit-timeout] 
[junit-timeout] 
...
{noformat};;;","31/Jan/19 19:31;jolynch;Turns out it was just that in trunk we don't include the port in the [cassandra-murmur3.yaml|https://github.com/apache/cassandra/blob/7f634feb7cf1fdb135133946ffd75efa681b8cb7/test/conf/cassandra-murmur.yaml#L27] so the check [here|https://github.com/apache/cassandra/blob/7f634feb7cf1fdb135133946ffd75efa681b8cb7/src/java/org/apache/cassandra/service/StorageService.java#L556] fails and we actually try to gossip with peers. A quick patch to just include the port appears to fix the issue. Other tests are broken but PagingTest is now fixed:

 
||trunk||
|[bb4b0626|https://github.com/jolynch/cassandra/commit/bb4b06265cfc7a9cf915e8022feb9ccd83390d5d]|
|[!https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14956-fix-unit.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5!|https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14956-fix-unit]|

Test failures:
{{org.apache.cassandra.distributed.DistributedReadWritePathTest#readRepairTest}}: I believe this is CASSANDRA-14922, debugging that separately;;;","04/Feb/19 11:41;samt;Thanks [~jolynch], good catch. I've pushed your fix to trunk in {{27b35799a46dd5b649c4a172f4f8316b48615304}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Website can be built without nodetool documentation by accident,CASSANDRA-14955,13207936,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jolynch,jolynch,jolynch,07/Jan/19 03:09,24/Sep/20 05:04,13/Jul/23 08:38,07/Jan/19 07:08,4.0,4.0-alpha1,,,,Documentation/Website,,,,0,,,,"While [~mick@thelastpickle.com] was generating docs today we accidentally pushed empty nodetool docs because the {{make website}} target doesn't fail if nodetool fails to run. We believe that this is due to the line in [gen-nodetool-docs.py|https://github.com/apache/cassandra/blob/trunk/doc/gen-nodetool-docs.py#L39] which uses subprocess.call instead of check_call.

Let's make it so that if you try to build docs without nodetool being available the build should fail so that we cant make the same mistake again.",,jolynch,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-16093,CASSANDRA-16066,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/19 05:00;jolynch;14955-cassandra-site-svn.patch;https://issues.apache.org/jira/secure/attachment/12953933/14955-cassandra-site-svn.patch","07/Jan/19 04:59;jolynch;14955-trunk.txt;https://issues.apache.org/jira/secure/attachment/12953932/14955-trunk.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,2.0,jolynch,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 07 07:08:05 UTC 2019,,,,,,,,,,,,"0|u00ka8:",9223372036854775807,,,,,mck,,mck,,,,Low,,,,,,,,,,,,,,,,,,,"07/Jan/19 05:01;jolynch;I attached a patch to trunk ([^14955-trunk.txt] ) which will fail the {{gen-nodetool-docs}} script if the nodetool command fails, tested by running {{ant realclean}} and then:
{noformat}
cassandra-site/src » make add-doc                                                                                                                  2 ↵
make[1]: Entering directory '/home/josephl/pg/cassandra_trunk/doc'
rm -rf build/*
rm -f source/configuration/cassandra_config_file.rst
python convert_yaml_to_rst.py ../conf/cassandra.yaml source/configuration/cassandra_config_file.rst
python gen-nodetool-docs.py
Error: Could not find or load main class org.apache.cassandra.tools.NodeTool
ERROR: Nodetool failed to run, you likely need to build cassandra using ant jar from the top level directory
Traceback (most recent call last):
  File ""gen-nodetool-docs.py"", line 64, in <module>
    create_help_file()
  File ""gen-nodetool-docs.py"", line 48, in create_help_file
    raise cpe
subprocess.CalledProcessError: Command '['../bin/nodetool', 'help']' returned non-zero exit status 1
Makefile:72: recipe for target 'website' failed
make[1]: *** [website] Error 1
make[1]: Leaving directory '/home/josephl/pg/cassandra_trunk/doc'
Makefile:22: recipe for target '.build-doc' failed
make: *** [.build-doc] Error 2
{noformat}
The other changes are fixing the warning that {{make add-doc}} was emitting about the nodetool docs title lengths being wrong, now there are only the TOC errors left.

I've also attached a patch to the svn cassandra-site repo ( [^14955-cassandra-site-svn.patch]) which will run {{ant jar}} for you automatically before building the docs so that the user doesn't have to know they need the main repo built first.;;;","07/Jan/19 07:08;mck;Committed as 1850613 and [c68b0fe|https://github.com/apache/cassandra/commit/c68b0fec6f7034aa74e64abd9859ee1d481b4f62].

Thanks [~jolynch]!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when using allocate_tokens_for_keyspace and add new DC,CASSANDRA-14952,13207771,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,chovatia.jaydeep@gmail.com,04/Jan/19 22:38,15/May/20 08:04,13/Jul/23 08:38,09/Aug/19 17:54,3.0.19,3.11.5,4.0,4.0-alpha1,,Cluster/Gossip,,,,0,,,,"Received following NPE while bootstrapping very first node in the new datacenter with {{allocate_tokens_for_keyspace}} yaml option
{code:java}
INFO  21:44:13 JOINING: getting bootstrap token
Exception (java.lang.NullPointerException) encountered during startup: null
java.lang.NullPointerException
	at org.apache.cassandra.dht.tokenallocator.TokenAllocation.getStrategy(TokenAllocation.java:208)
	at org.apache.cassandra.dht.tokenallocator.TokenAllocation.getStrategy(TokenAllocation.java:170)
	at org.apache.cassandra.dht.tokenallocator.TokenAllocation.allocateTokens(TokenAllocation.java:55)
	at org.apache.cassandra.dht.BootStrapper.allocateTokens(BootStrapper.java:206)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:173)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:854)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:579)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:351)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:586)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:714)

{code}
Please find reproducible steps here:
 1. Set {{allocate_tokens_for_keyspace}} property with {{Networktopologystrategy}} say {{{{Networktopologystrategy, 'dc1' : 1, 'dc2' : 1}}}}
 2. Start first node in {{dc1}}
 3. Now bootstrap second node in {{dc2,}} it will throw above exception.

RCA:
 [doAddEndpoint|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/locator/TokenMetadata.java#L1325] is invoked from the [bootstrap|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L1254] and at this time [local node's rack information|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/locator/TokenMetadata.java#L1276] is available

However with have {{allocate_tokens_for_keyspace}} option, daemon tries to access rack information even before calling [bootstrap|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L1241] function, at [this place|https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L878] which results in NPE

Fix:
 Since this is applicable to only very first node for new dc, we can check for {{null}} as:
{code:java}
diff --git a/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java b/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java
index 8d8a6ffeca..e162757d95 100644
--- a/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java
+++ b/src/java/org/apache/cassandra/dht/tokenallocator/TokenAllocation.java
@@ -205,7 +205,11 @@ public class TokenAllocation
         final int replicas = rs.getReplicationFactor(dc);
 
         Topology topology = tokenMetadata.getTopology();
-        int racks = topology.getDatacenterRacks().get(dc).asMap().size();
+        int racks = 1;
+        if (topology.getDatacenterRacks().get(dc) != null)
+        {
+            racks = topology.getDatacenterRacks().get(dc).asMap().size();
+        }
 
         if (racks >= replicas)
         {
{code}
Let me know your comments.",,chovatia.jaydeep@gmail.com,jay.zhuang,jeromatron,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-12681,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,chovatia.jaydeep@gmail.com,mck,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 09 17:54:04 UTC 2019,,,,,,,,,,,,"0|u00j9k:",9223372036854775807,,,,,,,mck,,,,Low,,3.0 alpha 1,,,https://github.com/apache/cassandra/commit/2374a74eba6a4df84f9bda3fd311916c820e9cd6,,,,,,,,,.,,,,,"10/Jan/19 20:07;chovatia.jaydeep@gmail.com; 

[~blambov] Could you please check this bug and provide your opinion?;;;","01/Aug/19 21:07;mck;There's a few peculiarities in how {{allocate_tokens_for_keyspace}} bootstraps in new datacenters.

For example subsequent nodes in a new datacenter will also fail, unless RF=2, until there is at least one node in each rack up until RF number of racks. That failure is a {{ConfigurationException}} with the message {code}""Token allocation failed: the number of racks %d in datacenter %s is lower than its replication factor %d.""{code}

It is an undocumented requirement that one node in each rack, up until RF number of racks, are bootstrapped with manually calculated tokens, when adding a new datacenter and using {{allocate_tokens_for_keyspace}}.

Do we want to treat the first node added in a new datacenter as a unique unit, which is what we get with {{rack = 1}}?
[~chovatia.jaydeep@gmail.com], unless anyone speaks up, let me do some testing on it and get back to you…;;;","04/Aug/19 21:44;mck;> Do we want to treat the first node added in a new datacenter as a unique unit, which is what we get with rack = 1?

It seems to make sense to treat such a node as its own unique unit (as it's the first in any eventuating unit group). Although seeds (non-autobootstrapping) and non-existant dc names (CASSANDRA-12681) can also prevent that from happening.

A slightly modified version of your fix [~chovatia.jaydeep@gmail.com]
||branch||circleci||asf jenkins testall||asf jenkins dtests||
|[CASSANDRA-14952|https://github.com/thelastpickle/cassandra/commit/3a72a51f9cb06ac85a4c78f3719a598a3a754909]|[circleci|https://circleci.com/workflow-run/b1f8b919-f889-47c5-9019-22a3468a428d]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/41//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-testall/41/]|[!https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/678//badge/icon!|https://builds.apache.org/view/A-D/view/Cassandra/job/Cassandra-devbranch-dtest/678/]|;;;","09/Aug/19 03:34;chovatia.jaydeep@gmail.com;[~mck] Looks good to me. Sorry for the late response.;;;","09/Aug/19 17:54;mck;Committed as 2374a74eba6a4df84f9bda3fd311916c820e9cd6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport dropped column checks to 3.0 and 3.11,CASSANDRA-14948,13207412,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,VincentWhite,VincentWhite,VincentWhite,03/Jan/19 07:27,16/Jul/19 18:00,13/Jul/23 08:38,16/Jul/19 17:55,3.0.19,3.11.5,,,,Cluster/Schema,,,,0,,,,"This is a follow on from CASSANDRA-14913 and CASSANDRA-14843 that introduced some fixes to prevent and mitigate data corruption caused by dropping a column then re-adding it with the same name but an incompatible type (e.g. simple int to a complex map<>) or different kind (regular/static). 

This patch backports the checks that now exist in trunk. This does include adding a column to the dropped_columns table to keep track of static columns like trunk, not sure it we are able to make that change in 3.11.x. 

Also not sure what our stance on backporting just the isValueCompatibleWith check to 3.0 is. I'd be for it since it prevents recreating a simple column as a map (or vice-versa) which will basically always lead to corruption.

||C* 3.11.x||
|[Patch|https://github.com/vincewhite/cassandra/commit/3986b53b8acaf1d3691f9b35fd098a40667c520f]|
",,aleksey,VincentWhite,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-14913,CASSANDRA-14843,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,VincentWhite,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 16 17:55:43 UTC 2019,,,,,,,,,,,,"0|u00h1s:",9223372036854775807,3.11.3,,,,,,aleksey,,,,Low,,3.0.0,,,"[fc862e207b04ed92f15b2129ae7738186ebc6d69|https://github.com/apache/cassandra/commit/fc862e207b04ed92f15b2129ae7738186ebc6d69]",,,,,,,,,,,,,,"16/Jul/19 16:13;aleksey;Hi Vincent.

I rebased the patch and made 3.0 and 3.11 versions, then made some edits. In particular, I decided to make 3.0 checks stricter than the checks that I put in 4.0: if {{kind}} is unknown, don't assume it's {{regular}}, but instead flat out reject any attempt to recreate, as it's an inherent risk. Also added some tests.

3.0: [code|https://github.com/iamaleksey/cassandra/tree/14948-3.0], [CI|https://circleci.com/workflow-run/f3ee4263-5c51-4f83-a80a-e0993689f57f]
3.11: [code|https://github.com/iamaleksey/cassandra/tree/14948-3.11], [CI|https://circleci.com/workflow-run/6aa72012-0d18-4e6d-a943-1d7510ca8a8d]

The change to type validation had already been made in CASSANDRA-15204 independently, so this change only covers {{kind}} validation.;;;","16/Jul/19 17:55;aleksey;Committed to 3.0 as and [fc862e207b04ed92f15b2129ae7738186ebc6d69|https://github.com/apache/cassandra/commit/fc862e207b04ed92f15b2129ae7738186ebc6d69] and merged up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
