Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Completes),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
ParquetIOSuite fails intermittently on master branch,SPARK-39622,13468735,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,kabhwan,kabhwan,27/Jun/22 20:42,12/Dec/22 18:11,13/Jul/23 08:47,21/Jul/22 23:51,3.4.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,"""SPARK-7837 Do not close output writer twice when commitTask() fails"" in ParquetIOSuite fails intermittently with master branch. 

Assertion error follows:

{code}
""Job aborted due to stage failure: Authorized committer (attemptNumber=0, stage=1, partition=0) failed; but task commit success, data duplication may happen."" did not contain ""Intentional exception for testing purposes""
ScalaTestFailureLocation: org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite at (ParquetIOSuite.scala:1216)
org.scalatest.exceptions.TestFailedException: ""Job aborted due to stage failure: Authorized committer (attemptNumber=0, stage=1, partition=0) failed; but task commit success, data duplication may happen."" did not contain ""Intentional exception for testing purposes""
	at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
	at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
	at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
	at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.$anonfun$new$259(ParquetIOSuite.scala:1216)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.$anonfun$new$259$adapted(ParquetIOSuite.scala:1209)
	at org.apache.spark.sql.catalyst.plans.SQLHelper.withTempPath(SQLHelper.scala:69)
	at org.apache.spark.sql.catalyst.plans.SQLHelper.withTempPath$(SQLHelper.scala:66)
	at org.apache.spark.sql.QueryTest.withTempPath(QueryTest.scala:33)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.$anonfun$new$256(ParquetIOSuite.scala:1209)
	at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf(SQLHelper.scala:54)
	at org.apache.spark.sql.catalyst.plans.SQLHelper.withSQLConf$(SQLHelper.scala:38)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.org$apache$spark$sql$test$SQLTestUtilsBase$$super$withSQLConf(ParquetIOSuite.scala:56)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf(SQLTestUtils.scala:247)
	at org.apache.spark.sql.test.SQLTestUtilsBase.withSQLConf$(SQLTestUtils.scala:245)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.withSQLConf(ParquetIOSuite.scala:56)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetIOSuite.$anonfun$new$255(ParquetIOSuite.scala:1190)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:190)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:203)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:64)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:64)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:233)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1563)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:237)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:237)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:236)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:64)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:64)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1320)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1314)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1314)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:993)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:971)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1480)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:971)
	at org.scalatest.tools.Runner$.run(Runner.scala:798)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:38)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:25)
{code}

It seems to produce the expected exception message in first query, but seems no longer produce the expected exception message in second query.",,apachespark,cloud_fan,csun,dongjoon,kabhwan,LuciferYang,panbingkun,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 21 23:51:34 UTC 2022,,,,,,,,,,"0|z16ei0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/22 01:54;cloud_fan;> ParquetIOSuite fails consistently with master branch.

I'm a bit confused. Most PRs still have tests green.;;;","07/Jul/22 01:59;gurwls223;Actually I think it's flaky.;;;","14/Jul/22 12:40;kabhwan;Another failure: https://github.com/HeartSaVioR/spark/runs/7337786485;;;","21/Jul/22 09:21;LuciferYang;Maybe the test suite started flaky after SPARK-39195 was merged. I revert it and ran ""SPARK-7837 Do not close output writer twice when commitTask() fails"" dozens of times without failure. Still  investigate  the root cause.

[~kabhwan] [~hyukjin.kwon] [~cloud_fan] ;;;","21/Jul/22 12:44;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37245;;;","21/Jul/22 12:44;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/37245;;;","21/Jul/22 23:51;dongjoon;Issue resolved by pull request 37245
[https://github.com/apache/spark/pull/37245];;;",,,,,
Make run-tests.py robust by avoiding `rmtree` usage,SPARK-39621,13468660,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,27/Jun/22 17:57,27/Jun/22 19:55,13/Jul/23 08:47,27/Jun/22 19:55,3.1.2,3.2.1,3.3.0,3.4.0,,,,,3.1.3,3.2.2,3.3.1,3.4.0,,PySpark,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 27 19:55:09 UTC 2022,,,,,,,,,,"0|z16efk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/22 18:13;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37010;;;","27/Jun/22 19:55;dongjoon;Issue resolved by pull request 37010
[https://github.com/apache/spark/pull/37010];;;",,,,,,,,,,
History server page and API are using inconsistent conditions to filter running applications,SPARK-39620,13468629,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,wekoms,wekoms,wekoms,27/Jun/22 14:53,08/Jul/22 13:20,13/Jul/23 08:47,08/Jul/22 13:20,3.2.1,3.3.0,,,,,,,3.4.0,,,,,Web UI,,,,0,,,"When opening summary page, history server follows this logic:
* If there's completed/incomplete application, page will add script in response, using AJAX to call the REST API to get the filtered list.
* If there's no such application, page will only return a message telling nothing found.

Issue is that page and REST API are using different conditions to filter applications. In HistoryPage, an application is considered as completed as long as the last attempt is completed. But in ApplicationListResource, all attempts should be completed. This brings inconsistency and will cause issue in a corner case.

In driver, event queues have capacity to protect memory. When there's too many events, some of them will be dropped and the event log file will be incomplete. For an application with multiple attempts, there's possibility that the last attempt is completed, but the previous attempts is considered as incomplete due to loss of application end event.

For this type of application, page thinks it is completed, but the API thinks it is still running. When opening summary page:
* When checking completed applications, page will call script, but API returns nothing.
* When checking incomplete applications, page returns nothing.

So the user won't be able to see this app in history server.",,apachespark,wekoms,,,,,,7200,7200,,0%,7200,7200,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 08 13:20:43 UTC 2022,,,,,,,,,,"0|z16e8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/22 16:13;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/37008;;;","27/Jun/22 16:13;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/37007;;;","08/Jul/22 13:20;srowen;Issue resolved by pull request 37008
[https://github.com/apache/spark/pull/37008];;;",,,,,,,,,
K8s pod name follows `DNS Subdomain Names` rule,SPARK-39614,13468509,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,27/Jun/22 07:28,27/Jun/22 08:33,13/Jul/23 08:47,27/Jun/22 08:33,3.3.0,,,,,,,,3.3.1,3.4.0,,,,Kubernetes,,,,0,,,https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 27 08:33:43 UTC 2022,,,,,,,,,,"0|z16di0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/22 07:32;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36999;;;","27/Jun/22 08:33;dongjoon;Issue resolved by pull request 36999
[https://github.com/apache/spark/pull/36999];;;",,,,,,,,,,
"The dataframe returned by exceptAll() can no longer perform operations such as count() or isEmpty(), or an exception will be thrown.",SPARK-39612,13468478,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,zhujunyong,zhujunyong,27/Jun/22 03:33,12/Dec/22 18:11,13/Jul/23 08:47,05/Jul/22 11:45,3.3.0,,,,,,,,3.3.1,3.4.0,,,,SQL,,,,0,,,"As I said, the dataframe returned by `exceptAll()` can no longer perform operations such as `count()` or `isEmpty()`, or an exception will be thrown.

 

 
{code:java}
>>> d1 = spark.createDataFrame([(""a"")], 'STRING')
>>> d1.show()
+-----+
|value|
+-----+
|    a|
+-----+
>>> d2 = d1.exceptAll(d1)
>>> d2.show()
+-----+
|value|
+-----+
+-----+
>>> d2.count()
22/06/27 11:22:15 ERROR Executor: Exception in task 0.0 in stage 113.0 (TID 525)
java.lang.IllegalStateException: Couldn't find value#465 in [sum#494L]
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
    at scala.collection.immutable.List.map(List.scala:297)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:698)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:528)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
    at org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)
    at org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)
    at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$10(GenerateExec.scala:114)
    at org.apache.spark.sql.execution.LazyIterator.results$lzycompute(GenerateExec.scala:36)
    at org.apache.spark.sql.execution.LazyIterator.results(GenerateExec.scala:36)
    at org.apache.spark.sql.execution.LazyIterator.hasNext(GenerateExec.scala:37)
    at scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)
    at scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.run(Task.scala:136)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)
22/06/27 11:22:15 ERROR TaskSetManager: Task 0 in stage 113.0 failed 1 times; aborting job
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/opt/downloads/spark-3.3.0-bin-hadoop3/python/pyspark/sql/dataframe.py"", line 804, in count
    return int(self._jdf.count())
  File ""/root/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1304, in __call__
    return_value = get_return_value(
  File ""/opt/downloads/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py"", line 190, in deco
    return f(*a, **kw)
  File ""/root/miniconda3/lib/python3.8/site-packages/py4j/protocol.py"", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o253.count.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 113.0 failed 1 times, most recent failure: Lost task 0.0 in stage 113.0 (TID 525) (thomaszhu1.fyre.ibm.com executor driver): java.lang.IllegalStateException: Couldn't find value#465 in [sum#494L]
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
    at scala.collection.immutable.List.map(List.scala:297)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:698)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:528)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
    at org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)
    at org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)
    at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$10(GenerateExec.scala:114)
    at org.apache.spark.sql.execution.LazyIterator.results$lzycompute(GenerateExec.scala:36)
    at org.apache.spark.sql.execution.LazyIterator.results(GenerateExec.scala:36)
    at org.apache.spark.sql.execution.LazyIterator.hasNext(GenerateExec.scala:37)
    at scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)
    at scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.run(Task.scala:136)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)Driver stacktrace:
    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
    at scala.Option.foreach(Option.scala:407)
    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
Caused by: java.lang.IllegalStateException: Couldn't find value#465 in [sum#494L]
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
    at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
    at scala.collection.immutable.List.map(List.scala:297)
    at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:698)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
    at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:528)
    at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
    at org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)
    at org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)
    at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$10(GenerateExec.scala:114)
    at org.apache.spark.sql.execution.LazyIterator.results$lzycompute(GenerateExec.scala:36)
    at org.apache.spark.sql.execution.LazyIterator.results(GenerateExec.scala:36)
    at org.apache.spark.sql.execution.LazyIterator.hasNext(GenerateExec.scala:37)
    at scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)
    at scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
    at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
    at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
    at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
    at org.apache.spark.scheduler.Task.run(Task.scala:136)
    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
    at java.base/java.lang.Thread.run(Thread.java:829)

 {code}
 

 ","OS: centos stream 8
{code:java}
$ uname -a
Linux thomaszhu1.fyre.ibm.com 4.18.0-348.7.1.el8_5.x86_64 #1 SMP Wed Dec 22 13:25:12 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux

$ python --version
Python 3.8.13 

$ pyspark --version
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/
                        
Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.11
Branch HEAD
Compiled by user ubuntu on 2022-06-09T19:58:58Z
Revision f74867bddfbcdd4d08076db36851e88b15e66556
Url https://github.com/apache/spark
Type --help for more information.

$ java --version
openjdk 11.0.11 2021-04-20
OpenJDK Runtime Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9)
OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode) {code}
 ",apachespark,bersprockets,zhujunyong,,,,,,,,,,,,,,,,SPARK-38531,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,Tue Jul 05 11:45:12 UTC 2022,,,,,,,,,,"0|z16db4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/22 02:56;gurwls223;Scala reproducer:

{code}
val d1 = Seq(""a"").toDF
d1.exceptAll(d1).count()
{code};;;","05/Jul/22 02:57;gurwls223;This is a regression from Spark 3.2.0;;;","05/Jul/22 07:14;gurwls223;https://github.com/apache/spark/pull/35864 caused this. I checked that it works if we revert that.;;;","05/Jul/22 09:35;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/37084;;;","05/Jul/22 11:45;gurwls223;Issue resolved by pull request 37084
[https://github.com/apache/spark/pull/37084];;;",,,,,,,
AllocationFailure should not be treated as exitCausedByApp when driver is shutting down,SPARK-39601,13468408,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,,25/Jun/22 14:28,13/Dec/22 14:52,13/Jul/23 08:47,13/Dec/22 14:19,3.3.0,,,,,,,,3.4.0,,,,,YARN,,,,0,,,"I observed some Spark Applications successfully completed all jobs but failed during the shutting down phase w/ reason: Max number of executor failures (16) reached, the timeline is

Driver - Job success, Spark starts shutting down procedure.
{code:java}
2022-06-23 19:50:55 CST AbstractConnector INFO - Stopped Spark@74e9431b{HTTP/1.1, (http/1.1)}
{0.0.0.0:0}
2022-06-23 19:50:55 CST SparkUI INFO - Stopped Spark web UI at http://hadoop2627.xxx.org:28446
2022-06-23 19:50:55 CST YarnClusterSchedulerBackend INFO - Shutting down all executors
{code}
Driver - A container allocate successful during shutting down phase.
{code:java}
2022-06-23 19:52:21 CST YarnAllocator INFO - Launching container container_e94_1649986670278_7743380_02_000025 on host hadoop4388.xxx.org for executor with ID 24 for ResourceProfile Id 0{code}
Executor - The executor can not connect to driver endpoint because driver already stopped the endpoint.
{code:java}
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1911)
  at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:61)
  at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:393)
  at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend$.main(YarnCoarseGrainedExecutorBackend.scala:81)
  at org.apache.spark.executor.YarnCoarseGrainedExecutorBackend.main(YarnCoarseGrainedExecutorBackend.scala)
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: 
  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)
  at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
  at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:101)
  at org.apache.spark.executor.CoarseGrainedExecutorBackend$.$anonfun$run$9(CoarseGrainedExecutorBackend.scala:413)
  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)
  at scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)
  at scala.collection.immutable.Range.foreach(Range.scala:158)
  at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)
  at org.apache.spark.executor.CoarseGrainedExecutorBackend$.$anonfun$run$7(CoarseGrainedExecutorBackend.scala:411)
  at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:62)
  at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:61)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893)
  ... 4 more
Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@hadoop2627.xxx.org:21956
  at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:148)
  at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:144)
  at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
  at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
  at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
  at org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)
  at scala.concurrent.impl.ExecutionContextImpl$$anon$4.execute(ExecutionContextImpl.scala:138)
  at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
  at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
  at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
  at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288){code}
Driver - YarnAllocator received container launch error message and treat it as `exitCausedByApp`
{code:java}
2022-06-23 19:52:27 CST YarnAllocator INFO - Completed container container_e94_1649986670278_7743380_02_000025 on host: hadoop4388.xxx.org (state: COMPLETE, exit status: 1)
2022-06-23 19:52:27 CST YarnAllocator WARN - Container from a bad node: container_e94_1649986670278_7743380_02_000025 on host: hadoop4388.xxx.org. Exit status: 1. Diagnostics: [2022-06-23 19:52:24.932]Exception from container-launch.
Container id: container_e94_1649986670278_7743380_02_000025
Exit code: 1
Shell output: main : command provided 1
main : run as user is bdms_pm
main : requested yarn user is bdms_pm
Getting exit code file...
Creating script paths...
Writing pid file...
Writing to tmp file /mnt/dfs/2/yarn/local/nmPrivate/application_1649986670278_7743380/container_e94_1649986670278_7743380_02_000025/container_e94_1649986670278_7743380_02_000025.pid.tmp
Writing to cgroup task files...
Creating local dirs...
Launching container...
Getting exit code file...
Creating script paths...
[2022-06-23 19:52:24.938]Container exited with a non-zero exit code 1. Error file: prelaunch.err.
Last 4096 bytes of prelaunch.err :
Last 4096 bytes of stderr :
at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:873)
  at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:72)
  at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1(Promise.scala:288)
  at scala.concurrent.impl.Promise$DefaultPromise.$anonfun$tryComplete$1$adapted(Promise.scala:288)
  at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)
  at scala.concurrent.Promise.trySuccess(Promise.scala:94)
  at scala.concurrent.Promise.trySuccess$(Promise.scala:94)
  at scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:187)
  at org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:225)
  at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$7(NettyRpcEnv.scala:246)
  at org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$7$adapted(NettyRpcEnv.scala:246)
  at org.apache.spark.rpc.netty.RpcOutboxMessage.onSuccess(Outbox.scala:90)
  at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:195)
  at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)
  at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
  at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
  at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
{code}
Driver - Eventually application failed because ”failed“ executor reached threshold 
{code:java}
2022-06-23 19:52:30 CST ApplicationMaster INFO - Final app status: FAILED, exitCode: 11, (reason: Max number of executor failures (16) reached){code}",,apachespark,hdaikoku,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 13 14:52:33 UTC 2022,,,,,,,,,,"0|z16cvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/22 15:07;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/36991;;;","25/Jun/22 15:08;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/36991;;;","11/Nov/22 08:31;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/38622;;;","13/Dec/22 14:52;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/39053;;;",,,,,,,,
"Run `Linters, licenses, dependencies and documentation generation ` GitHub Actions failed",SPARK-39596,13468375,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,LuciferYang,LuciferYang,25/Jun/22 01:43,04/Jan/23 06:08,13/Jul/23 08:47,25/Jun/22 07:32,3.4.0,,,,,,,,3.2.4,3.3.1,3.4.0,,,Build,,,,0,,,"Run / Linters, licenses, dependencies and documentation generation

 
{code:java}
x Failed to parse Rd in histogram.Rd
ℹ there is no package called ‘ggplot2’
Caused by error in `loadNamespace()`:
! there is no package called ‘ggplot2’ 
Execution halted
                    ------------------------------------------------
      Jekyll 4.2.1   Please append `--trace` to the `build` command 
                     for any additional information or backtrace. 
                    ------------------------------------------------
/__w/spark/spark/docs/_plugins/copy_api_dirs.rb:147:in `<top (required)>': R doc generation failed (RuntimeError)
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/external.rb:60:in `require'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/external.rb:60:in `block in require_with_graceful_fail'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/external.rb:57:in `each'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/external.rb:57:in `require_with_graceful_fail'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/plugin_manager.rb:89:in `block in require_plugin_files'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/plugin_manager.rb:87:in `each'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/plugin_manager.rb:87:in `require_plugin_files'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/plugin_manager.rb:21:in `conscientious_require'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/site.rb:131:in `setup'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/site.rb:36:in `initialize'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/commands/build.rb:30:in `new'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/commands/build.rb:30:in `process'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/command.rb:91:in `block in process_with_graceful_fail'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/command.rb:91:in `each'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/command.rb:91:in `process_with_graceful_fail'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/commands/build.rb:18:in `block (2 levels) in init_with_program'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/mercenary-0.4.0/lib/mercenary/command.rb:221:in `block in execute'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/mercenary-0.4.0/lib/mercenary/command.rb:221:in `each'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/mercenary-0.4.0/lib/mercenary/command.rb:221:in `execute'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/mercenary-0.4.0/lib/mercenary/program.rb:44:in `go'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/mercenary-0.4.0/lib/mercenary.rb:21:in `program'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/exe/jekyll:15:in `<top (required)>'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/bin/jekyll:23:in `load'
    from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/bin/jekyll:23:in `<main>'
Error: Process completed with exit code 1. {code}
 * [https://github.com/nchammas/spark/runs/7047789435?check_suite_focus=true]
 * [https://github.com/LuciferYang/spark/runs/7050249606?check_suite_focus=true]
 * [https://github.com/apache/spark/runs/7047294196?check_suite_focus=true]
 * https://github.com/amaliujia/spark/runs/7048443511?check_suite_focus=true

 ",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 25 12:07:07 UTC 2022,,,,,,,,,,"0|z16cog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/22 01:57;LuciferYang;cc [HyukjinKwon|https://github.com/HyukjinKwon];;;","25/Jun/22 02:09;LuciferYang;also ping [~dongjoon] ;;;","25/Jun/22 06:15;dongjoon;Thank you for pinging me, [~LuciferYang].;;;","25/Jun/22 07:06;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36987;;;","25/Jun/22 07:32;dongjoon;Issue resolved by pull request 36987
[https://github.com/apache/spark/pull/36987];;;","25/Jun/22 08:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36988;;;","25/Jun/22 12:07;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36989;;;",,,,,
"""Since <version>"" docs on array_agg are incorrect",SPARK-39582,13468344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nchammas,nchammas,nchammas,24/Jun/22 19:31,12/Dec/22 18:10,13/Jul/23 08:47,25/Jun/22 11:42,3.3.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,"[https://spark.apache.org/docs/latest/api/sql/#array_agg]

The docs currently say ""Since: 2.0.0"", but `array_agg` was added in Spark 3.3.0.",,apachespark,nchammas,,,,,,,,,,,,,,,,,,,,SPARK-27974,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 25 11:42:44 UTC 2022,,,,,,,,,,"0|z16chk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/22 20:05;apachespark;User 'nchammas' has created a pull request for this issue:
https://github.com/apache/spark/pull/36982;;;","24/Jun/22 20:05;apachespark;User 'nchammas' has created a pull request for this issue:
https://github.com/apache/spark/pull/36982;;;","25/Jun/22 11:42;gurwls223;Issue resolved by pull request 36982
[https://github.com/apache/spark/pull/36982];;;",,,,,,,,,
ByteBuffer forget to rewind after get in AvroDeserializer,SPARK-39575,13465351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wzx,wzx,wzx,24/Jun/22 05:01,27/Jun/22 02:07,13/Jul/23 08:47,27/Jun/22 02:07,3.3.0,,,,,,,,3.2.2,3.3.1,3.4.0,,,Spark Core,,,,0,,,"{code:java}
case (BYTES, BinaryType) => (updater, ordinal, value) =>
  val bytes = value match {
    case b: ByteBuffer =>
      val bytes = new Array[Byte](b.remaining)
      b.get(bytes)
      // Do not forget to reset the position
      b.rewind()
      bytes
    case b: Array[Byte] => b
    case other => throw new RuntimeException(s""$other is not a valid avro binary."")
  }
  updater.set(ordinal, bytes) {code}
After Avro data is converted to InternalRow, there will be redundant position in ByteBuffer type caused by ByteBuffer#get",,apachespark,wzx,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 27 02:07:05 UTC 2022,,,,,,,,,,"0|z15u0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/22 05:13;apachespark;User 'wzx140' has created a pull request for this issue:
https://github.com/apache/spark/pull/36973;;;","27/Jun/22 02:07;srowen;Issue resolved by pull request 36973
[https://github.com/apache/spark/pull/36973];;;",,,,,,,,,,
inline table should allow expressions with alias,SPARK-39570,13463017,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,23/Jun/22 16:22,28/Jun/22 14:42,13/Jul/23 08:47,28/Jun/22 14:42,3.2.0,,,,,,,,3.2.2,3.3.1,3.4.0,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 28 14:42:13 UTC 2022,,,,,,,,,,"0|z15fls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jun/22 17:01;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36967;;;","28/Jun/22 14:42;cloud_fan;Issue resolved by pull request 36967
[https://github.com/apache/spark/pull/36967];;;",,,,,,,,,,
Add AQE invalid plan check,SPARK-39551,13454983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,22/Jun/22 02:49,02/Dec/22 02:23,13/Jul/23 08:47,22/Jun/22 14:39,3.3.0,,,,,,,,3.2.2,3.3.1,3.4.0,,,SQL,,,,0,,,"AQE logical optimization rules can lead to invalid physical plans as certain physical plan nodes are not compatible with others. E.g., `BroadcastExchangeExec` can only work as a direct child of broadcast join nodes.

Logical optimizations, on the other hand, are not (and should not be) aware of such restrictions. So a general solution here is to check for invalid plans and throw exceptions, which can be caught by AQE replanning process. And if such an exception is captured, AQE can void the current replanning result and keep using the latest valid plan.",,apachespark,cloud_fan,dongjoon,maryannxue,,,,,,,,,,,,SPARK-41336,SPARK-39447,,,,,SPARK-39447,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 07 04:37:44 UTC 2022,,,,,,,,,,"0|z1420o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/22 02:58;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/36953;;;","22/Jun/22 02:59;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/36953;;;","22/Jun/22 14:39;cloud_fan;Issue resolved by pull request 36953
[https://github.com/apache/spark/pull/36953];;;","06/Jul/22 23:40;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37108;;;","07/Jul/22 04:37;dongjoon;This is backported for the bug fix patch, https://github.com/apache/spark/pull/37087 .
- https://github.com/apache/spark/pull/37087#issuecomment-1175840358;;;",,,,,,,
CreateView Command with a window clause query hit a wrong window definition not found issue,SPARK-39548,13454648,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,amaliujia,amaliujia,amaliujia,21/Jun/22 20:47,23/Jun/22 05:33,13/Jul/23 08:47,23/Jun/22 05:33,3.3.0,,,,,,,,3.2.2,3.3.1,3.4.0,,,SQL,,,,0,,,"This query will hit a w2 window definition not found in `WindowSubstitute` rule, however this is a bug since the w2 definition is defined in the query.

```
create or replace temporary view test_temp_view as
with step_1 as (
select * , min(a) over w2 as min_a_over_w2 from (select 1 as a, 2 as b, 3 as c) window w2 as (partition by b order by c)) , step_2 as
(
select *, max(e) over w1 as max_a_over_w1
from (select 1 as e, 2 as f, 3 as g)
join step_1 on true
window w1 as (partition by f order by g)
)
select *
from step_2
```


Also we can move the unresolved window expression check from `WindowSubstitute` rule  to `CheckAnalysis` phrase.
",,amaliujia,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 23 05:33:37 UTC 2022,,,,,,,,,,"0|z13zy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/22 20:55;apachespark;User 'amaliujia' has created a pull request for this issue:
https://github.com/apache/spark/pull/36947;;;","23/Jun/22 05:33;cloud_fan;Issue resolved by pull request 36947
[https://github.com/apache/spark/pull/36947];;;",,,,,,,,,,
V2SessionCatalog should not throw NoSuchDatabaseException in loadNamespaceMetadata,SPARK-39547,13454638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,singhpk234,singhpk234,singhpk234,21/Jun/22 18:39,23/Jun/22 05:27,13/Jul/23 08:47,23/Jun/22 05:27,3.3.0,,,,,,,,3.3.1,3.4.0,,,,SQL,,,,0,,,"DROP NAMESPACE IF EXISTS

{table}

 

if a catalog doesn't overrides `namespaceExists` it by default uses `loadNamespaceMetadata` and in case a `db` not exists loadNamespaceMetadata throws a `NoSuchDatabaseException` which is not catched and we see failures even with `if exists` clause. One such use case we observed was in iceberg table a post test clean up was failing with `NoSuchDatabaseException` now.

 

Found {color:#000000}V2SessionCatalog `{color}{color:#00627a}loadNamespaceMetadata{color}{color:#000000}` {color}was also throwing the same unlike `{color:#000000}JDBCTableCatalog`{color}

ref a stack trace :
{quote}Database 'db' not found
org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'db' not found
at org.apache.spark.sql.catalyst.catalog.SessionCatalog.requireDbExists(SessionCatalog.scala:219)
at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getDatabaseMetadata(SessionCatalog.scala:284)
at org.apache.spark.sql.execution.datasources.v2.V2SessionCatalog.loadNamespaceMetadata(V2SessionCatalog.scala:247)
at org.apache.iceberg.spark.SparkSessionCatalog.loadNamespaceMetadata(SparkSessionCatalog.java:97)
at org.apache.spark.sql.connector.catalog.SupportsNamespaces.namespaceExists(SupportsNamespaces.java:98)
at org.apache.spark.sql.execution.datasources.v2.DropNamespaceExec.run(DropNamespaceExec.scala:40)
at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)
at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)
at org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)
at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)
{quote}",,apachespark,singhpk234,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 21 18:56:57 UTC 2022,,,,,,,,,,"0|z13zw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jun/22 18:39;singhpk234;will post a pr for it shortly.;;;","21/Jun/22 18:56;apachespark;User 'singhpk234' has created a pull request for this issue:
https://github.com/apache/spark/pull/36948;;;","21/Jun/22 18:56;apachespark;User 'singhpk234' has created a pull request for this issue:
https://github.com/apache/spark/pull/36948;;;",,,,,,,,,
The option of DataFrameWriterV2 should be passed to storage properties if fallback to v1,SPARK-39543,13454546,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,kaifeiYi,kaifeiYi,kaifeiYi,21/Jun/22 11:05,23/Jun/22 05:07,13/Jul/23 08:47,23/Jun/22 05:07,3.3.0,,,,,,,,3.2.2,3.3.1,3.4.0,,,SQL,,,,0,,,"The option of DataFrameWriterV2 should be passed to storage properties if fallback to v1, to support something such as compressed formats, example:

spark.range(0, 100).writeTo(""t1"").option(""compression"", ""zstd"").using(""parquet"").create

*before*

gen: part-00000-644a65ed-0e7a-43d5-8d30-b610a0fb19dc-c000.snappy.parquet

*after*

gen: part-00000-6eb9d1ae-8fdb-4428-aea3-bd6553954cdd-c000.zstd.parquet ...

 ",,apachespark,cloud_fan,kaifeiYi,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 23 05:07:57 UTC 2022,,,,,,,,,,"0|z13zbs:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,,"21/Jun/22 11:54;apachespark;User 'Yikf' has created a pull request for this issue:
https://github.com/apache/spark/pull/36941;;;","23/Jun/22 05:07;cloud_fan;Issue resolved by pull request 36941
[https://github.com/apache/spark/pull/36941];;;",,,,,,,,,,
Escape log content rendered in UI,SPARK-39505,13450640,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,dongjoon,17/Jun/22 16:35,17/Jun/22 18:56,13/Jul/23 08:47,17/Jun/22 16:37,3.4.0,,,,,,,,3.1.4,3.2.2,3.3.1,3.4.0,,Spark Core,,,,0,,,,,apachespark,dongjoon,LuciferYang,robreeves,roczei,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 17 16:43:47 UTC 2022,,,,,,,,,,"0|z13c08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/22 16:37;dongjoon;Issue resolved by pull request 36902
[https://github.com/apache/spark/pull/36902];;;","17/Jun/22 16:43;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/36902;;;",,,,,,,,,,
Inline eval path cannot handle null structs,SPARK-39496,13450490,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,16/Jun/22 23:29,12/Dec/22 18:10,13/Jul/23 08:47,18/Jun/22 00:29,3.1.3,3.2.1,3.3.0,3.4.0,,,,,3.1.3,3.2.2,3.3.1,3.4.0,,SQL,,,,0,,,"This issue is somewhat similar to SPARK-39061, but for the eval path rather than the codegen path.

Example:
{noformat}
set spark.sql.codegen.wholeStage=false;
select inline(array(named_struct('a', 1, 'b', 2), null));
{noformat}
This results in a NullPointerException:
{noformat}
22/06/16 15:10:06 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$11(GenerateExec.scala:122)
{noformat}
The next example doesn't require setting {{spark.sql.codegen.wholeStage}} to {{{}false{}}}:
{noformat}
val dfWide = (Seq((1))
  .toDF(""col0"")
  .selectExpr(Seq.tabulate(99)(x => s""$x as col${x + 1}""): _*))

val df = (dfWide
  .selectExpr(""*"", ""array(named_struct('a', 1, 'b', 2), null) as struct_array""))

df.selectExpr(""*"", ""inline(struct_array)"").collect
{noformat}
The result is similar:
{noformat}
22/06/16 15:18:55 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)/ 1]
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.JoinedRow.isNullAt(JoinedRow.scala:80)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.writeFields_0_8$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$11(GenerateExec.scala:122)
{noformat}",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 21 21:01:03 UTC 2022,,,,,,,,,,"0|z13b3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jun/22 19:44;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36903;;;","18/Jun/22 00:29;gurwls223;Fixed in https://github.com/apache/spark/pull/36903;;;","21/Jun/22 21:00;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36949;;;","21/Jun/22 21:01;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36949;;;",,,,,,,,
Update ORC to 1.7.5,SPARK-39493,13450474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,16/Jun/22 20:58,17/Jun/22 00:37,13/Jul/23 08:47,16/Jun/22 23:21,3.3.0,3.4.0,,,,,,,3.3.1,3.4.0,,,,Build,,,,0,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 16 23:21:55 UTC 2022,,,,,,,,,,"0|z13azs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/22 21:01;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36892;;;","16/Jun/22 21:02;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36892;;;","16/Jun/22 23:21;dongjoon;Issue resolved by pull request 36892
[https://github.com/apache/spark/pull/36892];;;",,,,,,,,,
Disable Unwrap cast optimize when casting from Long to Float/ Double or from Integer to Float,SPARK-39476,13450132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,EdisonWang,EdisonWang,EdisonWang,15/Jun/22 05:05,16/Jun/22 01:36,13/Jul/23 08:47,16/Jun/22 01:36,3.1.0,3.1.1,3.1.2,3.2.0,3.2.1,3.3.0,,,3.1.3,3.2.2,3.3.1,3.4.0,,SQL,,,,0,,,,,apachespark,cloud_fan,EdisonWang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 16 01:36:13 UTC 2022,,,,,,,,,,"0|z138vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/22 05:09;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36873;;;","15/Jun/22 05:10;apachespark;User 'WangGuangxin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36873;;;","16/Jun/22 01:36;cloud_fan;Issue resolved by pull request 36873
[https://github.com/apache/spark/pull/36873];;;",,,,,,,,,
Add ReplaceCTERefWithRepartition into nonExcludableRules list,SPARK-39448,13449623,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,11/Jun/22 13:26,14/Jun/22 07:44,13/Jul/23 08:47,14/Jun/22 07:43,3.3.0,3.4.0,,,,,,,3.3.1,3.4.0,,,,SQL,,,,0,,,"A unit test to test excluded rules:
{code:scala}
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package org.apache.spark.sql

import org.apache.spark.internal.config.Tests.IS_TESTING
import org.apache.spark.launcher.SparkLauncher
import org.apache.spark.sql.catalyst.util.resourceToString
import org.apache.spark.sql.internal.SQLConf

// scalastyle:off println
object ExcludedRulesTestSuite extends TPCDSBase {
  override val spark = SparkSession.builder()
    .appName(this.getClass.getSimpleName)
    .master(""local[40]"")
    .config(SparkLauncher.DRIVER_MEMORY, ""4g"")
    .config(SQLConf.SHUFFLE_PARTITIONS.key, 2)
    .getOrCreate()

  override def injectStats: Boolean = true

  createTables()

  private lazy val excludableRules = spark.sessionState.optimizer.batches
    .flatMap(_.rules.map(_.ruleName))
    .distinct
    .filterNot(spark.sessionState.optimizer.nonExcludableRules.contains)

  def main(args: Array[String]): Unit = {
    System.setProperty(IS_TESTING.key, ""true"")

    tpcdsQueries.foreach { name =>
      val queryString = resourceToString(s""tpcds/$name.sql"",
        classLoader = Thread.currentThread().getContextClassLoader)
      excludableRules.foreach { rule =>

        println(name + "": "" + rule)
        try {
          withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> rule) {
            sql(queryString).collect()
          }
        } catch {
          case e: Exception =>
            println(""Exception: "" + e.getMessage)
        }
      }
    }

    tpcdsQueriesV2_7_0.foreach { name =>
      val queryString = resourceToString(s""tpcds-v2.7.0/$name.sql"",
        classLoader = Thread.currentThread().getContextClassLoader)
      excludableRules.foreach { rule =>

        println(""tpcds-v2.7.0 "" + name + "": "" + rule)
        try {
          withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> rule) {
            sql(queryString).collect()
          }
        } catch {
          case e: Exception =>
            println(""Exception: "" + e.getMessage)
        }
      }
    }

    modifiedTPCDSQueries.foreach { name =>
      val queryString = resourceToString(s""tpcds-modifiedQueries/$name.sql"",
        classLoader = Thread.currentThread().getContextClassLoader)
      excludableRules.foreach { rule =>

        println(""tpcds-modifiedQueries "" + name + "": "" + rule)
        try {
          withSQLConf(SQLConf.OPTIMIZER_EXCLUDED_RULES.key -> rule) {
            sql(queryString).collect()
          }
        } catch {
          case e: Exception =>
            println(""Exception: "" + e.getMessage)
        }
      }
    }

    // scalastyle:on println
  }
}

{code}
",,apachespark,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 14 07:43:48 UTC 2022,,,,,,,,,,"0|z135qw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/22 13:40;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36847;;;","11/Jun/22 13:41;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36847;;;","13/Jun/22 01:42;dongjoon;This occurs in the same in 3.3.0 RC6.
{code:java}
scala> spark.version
val res0: String = 3.3.0

scala> sql(""set spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.ReplaceCTERefWithRepartition"")
val res1: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> sql(""SELECT (SELECT avg(id) FROM range(10)),(SELECT sum(id) FROM range(10)),(SELECT count(distinct id) FROM range(10))"").show
warning: 1 deprecation (since 2.13.3); for details, enable `:setting -deprecation` or `:replay -deprecation`
org.apache.spark.SparkException: The Spark SQL phase planning failed with an internal error. Please, fill a bug report in, and provide the full stack trace. {code};;;","14/Jun/22 07:43;dongjoon;Issue resolved by pull request 36847
[https://github.com/apache/spark/pull/36847];;;",,,,,,,,
Only non-broadcast query stage can propagate empty relation,SPARK-39447,13449608,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,yumwang,yumwang,11/Jun/22 11:18,07/Jul/22 14:50,13/Jul/23 08:47,07/Jul/22 14:50,3.4.0,,,,,,,,3.2.2,3.3.1,3.4.0,,,SQL,,,,0,,,,,apachespark,yumwang,,,,,,,,,,,,,,,,SPARK-39551,,,SPARK-39551,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 05 09:40:46 UTC 2022,,,,,,,,,,"0|z135nk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/22 12:26;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36845;;;","24/Jun/22 08:03;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/36974;;;","05/Jul/22 09:40;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/37087;;;",,,,,,,,,
Remove the window if windowExpressions is empty in column pruning,SPARK-39445,13449587,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,11/Jun/22 06:56,20/Jun/22 14:23,13/Jul/23 08:47,20/Jun/22 14:23,3.4.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,,,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 20 14:23:11 UTC 2022,,,,,,,,,,"0|z135iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/22 07:25;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36842;;;","20/Jun/22 14:23;yumwang;Issue resolved by pull request 36842
[https://github.com/apache/spark/pull/36842];;;",,,,,,,,,,
Add OptimizeSubqueries into nonExcludableRules list,SPARK-39444,13449583,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,yumwang,yumwang,11/Jun/22 05:18,12/Dec/22 18:10,13/Jul/23 08:47,15/Jun/22 00:46,3.4.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,,,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 15 00:46:59 UTC 2022,,,,,,,,,,"0|z135i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jun/22 05:48;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36841;;;","15/Jun/22 00:46;gurwls223;Issue resolved by pull request 36841
[https://github.com/apache/spark/pull/36841];;;",,,,,,,,,,
normalize plan id separately in PlanStabilitySuite,SPARK-39437,13449355,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,10/Jun/22 04:59,13/Jun/22 17:32,13/Jul/23 08:47,10/Jun/22 15:19,3.2.0,3.2.1,3.3.0,,,,,,3.2.2,3.3.1,3.4.0,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 13 10:33:22 UTC 2022,,,,,,,,,,"0|z1343c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/22 05:21;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36828;;;","10/Jun/22 05:22;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36827;;;","10/Jun/22 15:19;cloud_fan;Issue resolved by pull request 36828
[https://github.com/apache/spark/pull/36828];;;","10/Jun/22 20:30;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36836;;;","10/Jun/22 22:29;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36838;;;","10/Jun/22 22:30;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36838;;;","13/Jun/22 10:32;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36854;;;","13/Jun/22 10:33;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36854;;;",,,,
Update PySpark dependency in Installation doc,SPARK-39431,13449303,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,09/Jun/22 20:05,09/Jun/22 22:19,13/Jul/23 08:47,09/Jun/22 22:19,3.3.0,,,,,,,,3.3.1,,,,,Documentation,PySpark,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 22:19:39 UTC 2022,,,,,,,,,,"0|z133s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/22 20:21;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36825;;;","09/Jun/22 22:19;dongjoon;Issue resolved by pull request 36825
[https://github.com/apache/spark/pull/36825];;;",,,,,,,,,,
Disable ANSI intervals in the percentile functions,SPARK-39427,13449171,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,09/Jun/22 07:58,23/Jun/22 09:09,13/Jul/23 08:47,09/Jun/22 12:40,3.3.0,,,,,,,,3.3.0,3.4.0,,,,SQL,,,,0,,,,,apachespark,maxgekk,,,,,,,,,,,,,,,,,,SPARK-39567,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 12:40:00 UTC 2022,,,,,,,,,,"0|z132zs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/22 08:18;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/36817;;;","09/Jun/22 12:40;maxgekk;Issue resolved by pull request 36817
[https://github.com/apache/spark/pull/36817];;;",,,,,,,,,,
SHOW CREATE TABLE should suggest 'AS SERDE' for Hive tables with unsupported serde configurations,SPARK-39422,13449103,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,joshrosen,joshrosen,joshrosen,09/Jun/22 01:52,17/Jul/22 06:06,13/Jul/23 08:47,09/Jun/22 19:45,3.0.0,,,,,,,,3.2.2,3.3.1,,,,SQL,,,,0,,,"If you run `SHOW CREATE TABLE` against a Hive table which uses an unsupported Serde configuration, Spark will return an error message like
{code:java}
org.apache.spark.sql.AnalysisException: Failed to execute SHOW CREATE TABLE against table rcFileTable, which is created by Hive and uses the following unsupported serde configuration
 SERDE: org.apache.hadoop.hive.serde2.columnar.LazyBinaryColumnarSerDe INPUTFORMAT: org.apache.hadoop.hive.ql.io.RCFileInputFormat OUTPUTFORMAT: org.apache.hadoop.hive.ql.io.RCFileOutputFormat {code}
which is confusing to end users.

In this situation, I think the error should suggest `SHOW CREATE TABLE ... AS SERDE` to users (similar to other error messages in this code path).",,apachespark,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 19:45:41 UTC 2022,,,,,,,,,,"0|z132ko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/22 02:19;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/36814;;;","09/Jun/22 02:19;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/36814;;;","09/Jun/22 19:45;joshrosen;Issue resolved by pull request 36814
[https://github.com/apache/spark/pull/36814];;;",,,,,,,,,
"Sphinx build fails with ""node class 'meta' is already registered, its visitors will be overridden""",SPARK-39421,13449090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,09/Jun/22 00:53,12/Dec/22 18:10,13/Jul/23 08:47,09/Jun/22 05:30,3.2.1,3.3.0,3.4.0,,,,,,3.2.2,3.3.0,3.4.0,,,Documentation,,,,0,,,"{code}
Moving to python/docs directory and building sphinx.
Running Sphinx v3.0.4
WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.
/__w/spark/spark/python/pyspark/pandas/supported_api_gen.py:101: UserWarning: Warning: Latest version of pandas(>=1.4.0) is required to generate the documentation; however, your version was 1.3.5
  warnings.warn(
Warning, treated as error:
node class 'meta' is already registered, its visitors will be overridden
make: *** [Makefile:35: html] Error 2
                    ------------------------------------------------
      Jekyll 4.2.1   Please append `--trace` to the `build` command 
                     for any additional information or backtrace. 
                    ------------------------------------------------
{code}

Sphinx build fails apparently with the latest docutils (see also https://issues.apache.org/jira/browse/FLINK-24662). we should pin the version.",,apachespark,,,,,,,,,,,,,,,,SPARK-39424,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 05:30:47 UTC 2022,,,,,,,,,,"0|z132hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/22 01:18;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36813;;;","09/Jun/22 05:30;gurwls223;Fixed in https://github.com/apache/spark/pull/36813;;;",,,,,,,,,,
"When the comparator of ArraySort returns null, it should fail.",SPARK-39419,13449085,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,08/Jun/22 23:08,10/Jun/22 23:52,13/Jul/23 08:47,10/Jun/22 23:51,3.3.0,,,,,,,,3.2.2,3.3.1,,,,SQL,,,,0,,,"When the comparator of {{ArraySort}} returns {{null}}, currently it handles it as {{0}} (equal).

According to the doc, 

{quote}
It returns -1, 0, or 1 as the first element is less than, equal to, or greater than the second element. If the comparator function returns other values (including null), the function will fail and raise an error.
{quote}

It's fine to return non -1, 0, 1 integers to follow the Java convention (still need to update the doc, though), but it should throw an exception for {{null}} result.",,apachespark,dongjoon,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 10 23:51:24 UTC 2022,,,,,,,,,,"0|z132go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/22 23:22;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36812;;;","08/Jun/22 23:22;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36812;;;","10/Jun/22 18:22;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36834;;;","10/Jun/22 18:25;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36835;;;","10/Jun/22 23:51;dongjoon;This is resolved via

[https://github.com/apache/spark/pull/36812] (master)

[https://github.com/apache/spark/pull/36834] (branch-3.3)

[https://github.com/apache/spark/pull/36835] (branch-3.2);;;",,,,,,,
Handle Null partition values in PartitioningUtils,SPARK-39417,13449047,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,singhpk234,singhpk234,singhpk234,08/Jun/22 16:34,09/Jun/22 06:45,13/Jul/23 08:47,09/Jun/22 06:45,3.3.0,,,,,,,,3.3.0,3.4.0,,,,SQL,,,,0,,,"partitions with null values we get a NPE on partition discovery, earlier we use to get `DEFAULT_PARTITION_NAME`

 

{quote} [info]   java.lang.NullPointerException:
[info]   at org.apache.spark.sql.execution.datasources.PartitioningUtils$.removeLeadingZerosFromNumberTypePartition(PartitioningUtils.scala:362)
[info]   at org.apache.spark.sql.execution.datasources.PartitioningUtils$.$anonfun$getPathFragment$1(PartitioningUtils.scala:355)
[info]   at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
[info]   at scala.collection.Iterator.foreach(Iterator.scala:943)
[info]   at scala.collection.Iterator.foreach$(Iterator.scala:943){quote}",,apachespark,joshrosen,singhpk234,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 08 17:58:06 UTC 2022,,,,,,,,,,"0|z13288:",9223372036854775807,,,,,,,,,,,,,3.3.0,,,,,,,,,,"08/Jun/22 16:34;singhpk234;adding a PR for this shortly;;;","08/Jun/22 16:47;singhpk234;PR : https://github.com/apache/spark/pull/36810/files;;;","08/Jun/22 17:17;apachespark;User 'singhpk234' has created a pull request for this issue:
https://github.com/apache/spark/pull/36810;;;","08/Jun/22 17:34;joshrosen;I see that the ""affected versions"" is currently set to 3.2.1. Does this problem actually occur in that version or is it a regression in 3.3.0?;;;","08/Jun/22 17:58;singhpk234;Appologies, the problem I think is only in 3.3.0 seems to introduced in https://github.com/apache/spark/commit/fc29c91f27d866502f5b6cc4261d4943b5cccc7e

Let me correct it.;;;",,,,,,,
IllegalStateException from connector does not work well with error class framework,SPARK-39412,13448925,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,maxgekk,kabhwan,kabhwan,08/Jun/22 06:41,10/Jun/22 04:20,13/Jul/23 08:47,08/Jun/22 18:21,3.3.0,,,,,,,,3.3.0,3.4.0,,,,Structured Streaming,,,,0,,,"With SPARK-39346, Spark SQL binds several exceptions to the internal error, and produces different guidance on dealing with the exception. This assumes these exceptions are only used for noticing internal bugs.

This applies to ""connectors"" as well, and introduces side-effect on the error log. For Kafka data source, it is a breaking and unacceptable change, because there is an important use case Kafka data source determines a case of ""dataloss"", and throws IllegalStateException with instruction message on workaround.

I mentioned this as ""important"" use case, because it can even happen with some valid scenarios - streaming query has some maintenance period and Kafka's retention on topic removes some records in the meanwhile.

Two problems arise:

1) This does not mean Spark has a bug and end users have to report, hence the guidance message on internal error is misleading.

2) Most importantly, instruction message is shown after a long stack trace. With the modification of existing test suite, I see the message being appeared in ""line 90"" of the error log.

We should roll the right error message back, at least for Kafka's case.",,apachespark,kabhwan,maxgekk,,,,,,,,,,,,,,,,,,,,,,"08/Jun/22 06:42;kabhwan;kafka-dataloss-error-msg-in-spark-3-2.log;https://issues.apache.org/jira/secure/attachment/13044749/kafka-dataloss-error-msg-in-spark-3-2.log","08/Jun/22 07:38;kabhwan;kafka-dataloss-error-msg-in-spark-3-3-or-master.log;https://issues.apache.org/jira/secure/attachment/13044754/kafka-dataloss-error-msg-in-spark-3-3-or-master.log",,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 12:20:19 UTC 2022,,,,,,,,,,"0|z131hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/22 08:16;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/36804;;;","08/Jun/22 18:21;maxgekk;Issue resolved by pull request 36804
[https://github.com/apache/spark/pull/36804];;;","09/Jun/22 12:20;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/36824;;;",,,,,,,,,
Release candidates do not have the correct version for PySpark,SPARK-39411,13448922,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,08/Jun/22 06:14,12/Dec/22 18:10,13/Jul/23 08:47,08/Jun/22 08:14,3.3.1,,,,,,,,3.3.0,3.4.0,,,,Build,PySpark,,,0,,,https://github.com/apache/spark/blob/v3.3.0-rc5/dev/create-release/release-tag.sh#L88 fails to replace the version in https://github.com/apache/spark/blob/v3.3.0-rc5/python/pyspark/version.py#L19 because now we have {code}: str ={code} hint ...,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 08 08:14:55 UTC 2022,,,,,,,,,,"0|z131go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/22 07:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36803;;;","08/Jun/22 08:14;gurwls223;Issue resolved by pull request 36803
[https://github.com/apache/spark/pull/36803];;;",,,,,,,,,,
Unable to query _metadata in streaming if getBatch returns multiple logical nodes in the DataFrame,SPARK-39404,13448855,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yaohua,yaohua,yaohua,07/Jun/22 18:11,22/Oct/22 10:27,13/Jul/23 08:47,08/Jun/22 08:41,3.2.1,,,,,,,,3.3.2,3.4.0,,,,Structured Streaming,,,,0,,,"Here: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/MicroBatchExecution.scala#L585]

 

We should probably `transform` instead of `match`",,apachespark,kabhwan,yaohua,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 22 00:37:05 UTC 2022,,,,,,,,,,"0|z1311s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jun/22 05:16;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/36801;;;","08/Jun/22 08:41;kabhwan;Issue resolved by pull request 36801
[https://github.com/apache/spark/pull/36801];;;","22/Oct/22 00:36;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/38337;;;","22/Oct/22 00:37;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/38337;;;",,,,,,,,
Replace withView with withTempView in CTEInlineSuite,SPARK-39401,13448835,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,07/Jun/22 15:37,12/Dec/22 18:10,13/Jul/23 08:47,08/Jun/22 02:42,3.2.0,,,,,,,,3.4.0,,,,,SQL,Tests,,,0,,,,,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 08 02:42:31 UTC 2022,,,,,,,,,,"0|z130xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/22 16:14;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36788;;;","08/Jun/22 02:42;gurwls223;Issue resolved by pull request 36788
[https://github.com/apache/spark/pull/36788];;;",,,,,,,,,,
spark-sql remain hive resource download dir after exit,SPARK-39400,13448768,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,07/Jun/22 10:23,09/Jun/22 00:06,13/Jul/23 08:47,09/Jun/22 00:06,3.3.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,"
{code:java}
drwxrwxr-x  2 yi.zhu           yi.zhu                4096 Jun  7 18:06 da92eec4-2db1-4941-9e53-b28c38e25e31_resources
drwxrwxr-x  2 yi.zhu           yi.zhu                4096 Jun  7 18:14 dad364e8-ed1d-4ced-a6df-4897361c69b1_resources
drwxrwxr-x  2 yi.zhu           yi.zhu                4096 Jun  7 18:13 ee0a2ee7-ff3e-4346-9181-e8e491b1ca15_resources
drwxr-xr-x  2 yi.zhu           yi.zhu                4096 Jun  7 18:16 hsperfdata_yi.zhu

{code}
",,angerszhuuu,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 00:06:46 UTC 2022,,,,,,,,,,"0|z130ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/22 11:14;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36786;;;","07/Jun/22 11:14;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36786;;;","09/Jun/22 00:06;yumwang;Issue resolved by pull request 36786
[https://github.com/apache/spark/pull/36786];;;",,,,,,,,,
proxy-user not working for Spark on k8s in cluster deploy mode,SPARK-39399,13448766,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,unamesk15,unamesk15,unamesk15,07/Jun/22 10:15,14/Mar/23 15:51,13/Jul/23 08:47,08/Mar/23 03:35,3.2.0,,,,,,,,3.2.4,3.3.3,3.4.0,,,Kubernetes,Spark Core,,,0,,,"As part of https://issues.apache.org/jira/browse/SPARK-25355 Proxy user support was added for Spark on K8s. But the PR only added proxy user argument on the spark-submit command. The actual functionality of authentication using the proxy user is not working in case of cluster deploy mode.

We get AccessControlException when trying to access the kerberized HDFS through a proxy user. 

Spark-Submit:
$SPARK_HOME/bin/spark-submit \
--master <K8S_APISERVER> \
--deploy-mode cluster \
--name with_proxy_user_di \
--proxy-user <username> \
--class org.apache.spark.examples.SparkPi \
--conf spark.kubernetes.container.image=<SPARK3.2_with_hadoop3.1_image> \
--conf spark.kubernetes.driver.limit.cores=1 \
--conf spark.executor.instances=1 \
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \
--conf spark.kubernetes.namespace=<namespace_name> \
--conf spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf \
--conf spark.eventLog.enabled=true \
--conf spark.eventLog.dir=hdfs://<hdfs_cluster>/scaas/shs_logs \

--conf spark.kubernetes.file.upload.path=hdfs://<hdfs_cluster>/tmp \

--conf spark.kubernetes.container.image.pullPolicy=Always \
$SPARK_HOME/examples/jars/spark-examples_2.12-3.2.0-1.jar 
Driver Logs:
{code:java}
++ id -u
+ myuid=185
++ id -g
+ mygid=0
+ set +e
++ getent passwd 185
+ uidentry=
+ set -e
+ '[' -z '' ']'
+ '[' -w /etc/passwd ']'
+ echo '185:x:185:0:anonymous uid:/opt/spark:/bin/false'
+ SPARK_CLASSPATH=':/opt/spark/jars/*'
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed 's/[^=]*=\(.*\)/\1/g'
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ '[' -n '' ']'
+ '[' -z ']'
+ '[' -z ']'
+ '[' -n '' ']'
+ '[' -z x ']'
+ SPARK_CLASSPATH='/opt/hadoop/conf::/opt/spark/jars/*'
+ '[' -z x ']'
+ SPARK_CLASSPATH='/opt/spark/conf:/opt/hadoop/conf::/opt/spark/jars/*'
+ case ""$1"" in
+ shift 1
+ CMD=(""$SPARK_HOME/bin/spark-submit"" --conf ""spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS"" --deploy-mode client ""$@"")
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=<addr> --deploy-mode client --proxy-user proxy_user --properties-file /opt/spark/conf/spark.properties --class org.apache.spark.examples.SparkPi spark-internal
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0-1.jar) to constructor java.nio.DirectByteBuffer(long,int)
WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Rate of successful kerberos logins and latency (milliseconds)""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Rate of failed kerberos logins and latency (milliseconds)""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""GetGroups""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Renewal failures since startup""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about="""", sampleName=""Ops"", always=false, type=DEFAULT, value={""Renewal failures since last successful login""}, valueName=""Time"")
22/04/26 08:54:38 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
22/04/26 08:54:38 DEBUG SecurityUtil: Setting hadoop.security.token.service.use_ip to true
22/04/26 08:54:38 DEBUG Shell: Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
    at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:469)
    at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:440)
    at org.apache.hadoop.util.Shell.<clinit>(Shell.java:517)
    at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
    at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
    at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:102)
    at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:86)
    at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315)
    at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:303)
    at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1827)
    at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:709)
    at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:659)
    at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:570)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/04/26 08:54:38 DEBUG Shell: setsid exited with exit code 0
22/04/26 08:54:38 DEBUG Groups:  Creating new Groups object
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: backing jks path initialized to file:/etc/security/bind.jceks
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: initialized local file as '/etc/security/bind.jceks'.
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: the local file does not exist.
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Usersearch baseDN: dc=<dc>
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Groupsearch baseDN: dc=<dc>
22/04/26 08:54:38 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.LdapGroupsMapping; cacheTimeout=300000; warningDeltaMs=5000
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login commit
22/04/26 08:54:38 DEBUG UserGroupInformation: using local user:UnixPrincipal: 185
22/04/26 08:54:38 DEBUG UserGroupInformation: Using user: ""UnixPrincipal: 185"" with name 185
22/04/26 08:54:38 DEBUG UserGroupInformation: User entry: ""185""
22/04/26 08:54:38 DEBUG UserGroupInformation: Reading credentials from location set in HADOOP_TOKEN_FILE_LOCATION: /mnt/secrets/hadoop-credentials/..2022_04_26_08_54_34.1262645511/hadoop-tokens
22/04/26 08:54:39 DEBUG UserGroupInformation: Loaded 3 tokens
22/04/26 08:54:39 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE)
22/04/26 08:54:39 DEBUG UserGroupInformation: PrivilegedAction as:proxy_user (auth:PROXY) via 185 (auth:SIMPLE) from:org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163)
22/04/26 08:54:39 DEBUG FileSystem: Loading filesystems
22/04/26 08:54:39 DEBUG FileSystem: file:// = class org.apache.hadoop.fs.LocalFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: har:// = class org.apache.hadoop.fs.HarFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
22/04/26 08:54:39 DEBUG FileSystem: nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar
22/04/26 08:54:39 DEBUG FileSystem: file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar
22/04/26 08:54:39 DEBUG FileSystem: Looking for FS supporting hdfs
22/04/26 08:54:39 DEBUG FileSystem: looking for configuration option fs.hdfs.impl
22/04/26 08:54:39 DEBUG FileSystem: Looking in service filesystems for implementation class
22/04/26 08:54:39 DEBUG FileSystem: FS for hdfs is class org.apache.hadoop.hdfs.DistributedFileSystem
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = true
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
22/04/26 08:54:39 DEBUG DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
22/04/26 08:54:39 DEBUG HAUtilClient: No HA service delegation token found for logical URI hdfs://<hdfs>/tmp/spark-upload-bf713a0c-166b-43fc-a5e6-24957e75b224/spark-examples_2.12-3.0.1.jar
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = false
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = true
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = false
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /var/lib/hadoop-hdfs/dn_socket
22/04/26 08:54:39 DEBUG RetryUtils: multipleLinearRandomRetry = null
22/04/26 08:54:39 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@4a325eb9
22/04/26 08:54:39 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@2577d6c8
22/04/26 08:54:40 DEBUG NativeCodeLoader: Trying to load the custom-built native-hadoop library...
22/04/26 08:54:40 DEBUG NativeCodeLoader: Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path: [/usr/java/packages/lib, /usr/lib64, /lib64, /lib, /usr/lib]
22/04/26 08:54:40 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
22/04/26 08:54:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/04/26 08:54:40 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.
22/04/26 08:54:40 DEBUG DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP dfs.data.transfer.protection = authentication,privacy, configured class dfs.data.transfer.saslproperties.resolver.class = class org.apache.hadoop.security.SaslPropertiesResolver
22/04/26 08:54:40 DEBUG Client: The ping interval is 60000 ms.
22/04/26 08:54:40 DEBUG Client: Connecting to <server>/<ip>:8020
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796)
22/04/26 08:54:40 DEBUG SaslRpcClient: Sending sasl message state: NEGOTIATE22/04/26 08:54:40 DEBUG SaslRpcClient: Get token info proto:interface org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector.class)
22/04/26 08:54:40 DEBUG SaslRpcClient: tokens aren't supported for this protocol or user doesn't have one
22/04/26 08:54:40 DEBUG SaslRpcClient: client isn't using kerberos
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720)
22/04/26 08:54:40 WARN Client: Exception encountered while connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
22/04/26 08:54:40 DEBUG Client: closing ipc connection to <server>/<ip>:8020: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
    at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:757)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
    at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:813)
    at org.apache.hadoop.ipc.Client$Connection.access$3600(Client.java:410)
    at org.apache.hadoop.ipc.Client.getConnection(Client.java:1558)
    at org.apache.hadoop.ipc.Client.call(Client.java:1389)
    at org.apache.hadoop.ipc.Client.call(Client.java:1353)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
    at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:900)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.base/java.lang.reflect.Method.invoke(Unknown Source)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
    at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source)
    at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1654)
    at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)
    at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1576)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1591)
    at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)
    at org.apache.hadoop.fs.Globber.doGlob(Globber.java:270)
    at org.apache.hadoop.fs.Globber.glob(Globber.java:149)
    at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2067)
    at org.apache.spark.util.DependencyUtils$.resolveGlobPath(DependencyUtils.scala:318)
    at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2(DependencyUtils.scala:273)
    at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2$adapted(DependencyUtils.scala:271)
    at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
    at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
    at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
    at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
    at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
    at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
    at org.apache.spark.util.DependencyUtils$.resolveGlobPaths(DependencyUtils.scala:271)
    at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$4(SparkSubmit.scala:364)
    at scala.Option.map(Option.scala:230)
    at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:364)
    at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
    at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:165)
    at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
    at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163)
    at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
    at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
    at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
    at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)
    at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)
    at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:614)
    at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:410)
    at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)
    at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:796)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at java.base/javax.security.auth.Subject.doAs(Unknown Source)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
    at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796)
    ... 53 more  {code}
 

The reason for no delegation token found is that the proxy user UGI doesn't have any credentials/tokens ( tokenSize:: 0 ) 
{code:java}
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:<hdfs>, Ident: (token for proxyUser: HDFS_DELEGATION_TOKEN owner=proxyUser, renewer=proxyUser, realUser=superuser/test@test.com, issueDate=1651165129518, maxDate=1651769929518, sequenceNumber=180516, masterKeyId=601)
22/04/28 16:59:37 DEBUG Token: Cannot find class for token kind HIVE_DELEGATION_TOKEN
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 08 73 68 72 70 72 61 73 61 04 68 69 76 65 1e 6c 69 76 79 2f 6c 69 76 79 2d 69 6e 74 40 43 4f 52 50 44 45 56 2e 56 49 53 41 2e 43 4f 4d 8a 01 80 71 1c 71 b5 8a 01 80 b9 35 79 b5 8e 15 cd 8e 03 6e
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: kms-dt, Service: <ip>:9292, Ident: (kms-dt owner=proxyUser, renewer=proxyUser, realUser=superuser, issueDate=1651165129566, maxDate=1651769929566, sequenceNumber=181197, masterKeyId=1152)
22/04/28 16:59:37 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE)
22/04/28 16:59:37 DEBUG UserGroupInformation: createProxyUser: from:org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
22/04/28 16:59:37 DEBUG UserGroupInformation: proxy user created, ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE)  subject::Subject:
    Principal: proxyUser
    Principal: 185 (auth:SIMPLE)
 tokenSize:: 0 {code}
{code:java}
22/04/28 16:59:38 DEBUG AbstractNNFailoverProxyProvider: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE)  tokensize:: 0
22/04/28 16:59:38 DEBUG HAUtilClient: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE)  tokenSize::0
22/04/28 16:59:38 DEBUG AbstractDelegationTokenSelector: kindName:: HDFS_DELEGATION_TOKEN  service:: ha-hdfs:<hdfs> tokens size:: 0
22/04/28 16:59:38 DEBUG HAUtilClient: No HA service delegation token found for logical URI hdfs://<hdfs>:8020/tmp/spark-upload-10582dde-f07c-4bf7-a611-5afbdd12ff6c/spark-examples_2.12-3.0.1.jar {code}
 

Please refer to the last 4 comments on https://issues.apache.org/jira/browse/SPARK-25355.",,apachespark,jianghuazhu,pralabhkumar,Qin Yao,unamesk15,,,,,,,,,,,,,,,SPARK-42785,,,,,"02/Nov/22 08:31;jianghuazhu;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13051704/screenshot-1.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 03:35:52 UTC 2023,,,,,,,,,,"0|z130i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/22 10:26;unamesk15;I would like to work on this issue. Have a working solution which is in-line with proxy-user implementation for Spark on Yarn and Mesos.;;;","16/Jun/22 04:56;pralabhkumar;ping [~hyukjin.kwon]  , please help us on the same or please provide some reference who can take this forward.  ;;;","20/Jun/22 09:55;pralabhkumar;Gentle ping [~hyukjin.kwon]   [~dongjoon] ;;;","18/Aug/22 19:12;unamesk15;[~dongjoon] [~hyukjin.kwon] Can you please have a look at this issue and let me know if I need to add any more details in order to take this forward.;;;","14/Sep/22 15:42;apachespark;User 'shrprasa' has created a pull request for this issue:
https://github.com/apache/spark/pull/37880;;;","02/Nov/22 08:32;jianghuazhu;It looks like HIVE_DELEGATION_TOKEN is not loaded and populated to Token#tokenKindMap.
Here are some sources of reference:
 !screenshot-1.png! 

We should first check the dependencies related to hive. [~unamesk15];;;","08/Mar/23 03:35;Qin Yao;Issue resolved by pull request 37880
[https://github.com/apache/spark/pull/37880];;;",,,,,
Spark Thriftserver enabled LDAP，Error using beeline connection: error code 49 - invalid credentials,SPARK-39396,13448678,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,haoweiliang,haoweiliang,haoweiliang,07/Jun/22 02:07,25/Jun/22 19:16,13/Jul/23 08:47,25/Jun/22 19:16,2.4.8,,,,,,,,3.4.0,,,,,SQL,,,,0,,,"Spark Thriftserver enabled LDAP，and report an error when logging in with LDAP user through beeline connection：
{code:java}
22/06/06 17:45:29 ERROR transport.TSaslTransport: SASL negotiation failure
javax.security.sasl.SaslException: Error validating the login [Caused by javax.security.sasl.AuthenticationException: Error validating LDAP user [Caused by javax.naming.AuthenticationException: [LDAP: error code 49 - Invalid Credentials]]]
	at org.apache.hive.service.auth.PlainSaslServer.evaluateResponse(PlainSaslServer.java:109)
	at org.apache.thrift.transport.TSaslTransport$SaslParticipant.evaluateChallengeOrResponse(TSaslTransport.java:539)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:283)
	at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
	at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:269)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: javax.security.sasl.AuthenticationException: Error validating LDAP user [Caused by javax.naming.AuthenticationException: [LDAP: error code 49 - Invalid Credentials]]
	at org.apache.hive.service.auth.LdapAuthenticationProviderImpl.Authenticate(LdapAuthenticationProviderImpl.java:77)
	at org.apache.hive.service.auth.PlainSaslHelper$PlainServerCallbackHandler.handle(PlainSaslHelper.java:106)
	at org.apache.hive.service.auth.PlainSaslServer.evaluateResponse(PlainSaslServer.java:102)
	... 8 more
Caused by: javax.naming.AuthenticationException: [LDAP: error code 49 - Invalid Credentials]
	at com.sun.jndi.ldap.LdapCtx.mapErrorCode(LdapCtx.java:3154)
	at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:3100)
	at com.sun.jndi.ldap.LdapCtx.processReturnCode(LdapCtx.java:2886)
	at com.sun.jndi.ldap.LdapCtx.connect(LdapCtx.java:2800)
	at com.sun.jndi.ldap.LdapCtx.<init>(LdapCtx.java:319)
	at com.sun.jndi.ldap.LdapCtxFactory.getUsingURL(LdapCtxFactory.java:192)
	at com.sun.jndi.ldap.LdapCtxFactory.getUsingURLs(LdapCtxFactory.java:210)
	at com.sun.jndi.ldap.LdapCtxFactory.getLdapCtxInstance(LdapCtxFactory.java:153)
	at com.sun.jndi.ldap.LdapCtxFactory.getInitialContext(LdapCtxFactory.java:83)
	at javax.naming.spi.NamingManager.getInitialContext(NamingManager.java:684)
	at javax.naming.InitialContext.getDefaultInitCtx(InitialContext.java:313)
	at javax.naming.InitialContext.init(InitialContext.java:244)
	at javax.naming.InitialContext.<init>(InitialContext.java:216)
	at javax.naming.directory.InitialDirContext.<init>(InitialDirContext.java:101)
	at org.apache.hive.service.auth.LdapAuthenticationProviderImpl.Authenticate(LdapAuthenticationProviderImpl.java:74)
	... 10 more
22/06/06 17:45:29 ERROR server.TThreadPoolServer: Error occurred during processing of message.
java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: Error validating the login
	at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:219)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:269)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.thrift.transport.TTransportException: Error validating the login
	at org.apache.thrift.transport.TSaslTransport.sendAndThrowMessage(TSaslTransport.java:232)
	at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:316)
	at org.apache.thrift.transport.TSaslServerTransport.open(TSaslServerTransport.java:41)
	at org.apache.thrift.transport.TSaslServerTransport$Factory.getTransport(TSaslServerTransport.java:216)
	... 4 more {code}
hive-site.xml:
{code:java}
<?xml version=""1.0"" encoding=""UTF-8"" standalone=""no""?>
<?xml-stylesheet type=""text/xsl"" href=""configuration.xsl""?>
<configuration>
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://metastore_uri:9083</value>
        <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
    </property>


    <property>
        <name>hive.cluster.delegation.token.store.class</name>
        <value>org.apache.hadoop.hive.thrift.MemoryTokenStore</value>
        <description>Hive defaults to MemoryTokenStore, or ZooKeeperTokenStore</description>
    </property>


    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/dtInsight/hive/warehouse</value>
    </property>


    <property>
        <name>hive.exec.scratchdir</name>
        <value>/dtInsight/hive/warehouse</value>
    </property>


    <property>
        <name>hive.server2.thrift.port</name>
        <value>10008</value>
    </property>

    <!-- hive enabled ldap -->
    <property>
        <name>hive.server2.authentication</name>
        <value>LDAP</value>
    </property>
    <property>
        <name>hive.server2.authentication.ldap.baseDN</name>
        <value>ou=People,dc=dtstack,dc=com</value>
    </property>
    <property>
        <name>hive.server2.authentication.ldap.url</name>
        <value>ldap://ldap_ip:389</value>
    </property>
    <property>
        <name>hive.server2.authentication.ldap.userDNPattern</name>
        <value>uid=%s,ou=People,dc=dtstack,dc=com:cn=%s,ou=People,dc=dtstack,dc=com</value>
    </property>

</configuration> {code}",,apachespark,haoweiliang,,,,,,,,,,,,,,,SPARK-39395,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 25 19:16:14 UTC 2022,,,,,,,,,,"0|z12zyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/22 02:20;haoweiliang;I will try to solve this problem;;;","07/Jun/22 03:16;haoweiliang;When a user with DN (cn=user, ou=people, dc=example, dc=com) logs in, it will fail because the DN generated in the class org.apache.hive.service.auth.LdapAuthenticationProviderImpl#Authenticate() is (uid=user, ou=people, dc=example, dc=com);;;","07/Jun/22 06:13;apachespark;User 'xiuzhu9527' has created a pull request for this issue:
https://github.com/apache/spark/pull/36784;;;","07/Jun/22 06:13;apachespark;User 'xiuzhu9527' has created a pull request for this issue:
https://github.com/apache/spark/pull/36783;;;","25/Jun/22 19:16;srowen;Issue resolved by pull request 36784
[https://github.com/apache/spark/pull/36784];;;",,,,,,,
Parquet data source only supports push-down predicate filters for non-repeated primitive types,SPARK-39393,13448637,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,borjianamin,borjianamin,borjianamin,06/Jun/22 19:49,09/Jun/22 08:42,13/Jul/23 08:47,08/Jun/22 21:12,3.1.0,3.1.1,3.1.2,3.2.0,3.2.1,,,,3.1.3,3.2.2,3.3.0,3.4.0,,SQL,,,,0,parquet,,"I use an example to illustrate the problem. The reason for the problem and the problem-solving approach are stated below.

Assume follow Protocol buffer schema:
{code:java}
message Model {
     string name = 1;
     repeated string keywords = 2;
}
{code}
Suppose a parquet file is created from a set of records in the above format with the help of the {{parquet-protobuf}} library.

Using Spark version 3.0.2 or older, we could run the following query using {{{}spark-shell{}}}:
{code:java}
val data = spark.read.parquet(""/path/to/parquet"")
data.registerTempTable(""models"")
spark.sql(""select * from models where array_contains(keywords, 'X')"").show(false)
{code}
But after updating Spark, we get the following error:
{code:java}
Caused by: java.lang.IllegalArgumentException: FilterPredicates do not currently support repeated columns. Column keywords is repeated.
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:176)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:149)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:89)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:56)
  at org.apache.parquet.filter2.predicate.Operators$NotEq.accept(Operators.java:192)
  at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validate(SchemaCompatibilityValidator.java:61)
  at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:95)
  at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:45)
  at org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:149)
  at org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:72)
  at org.apache.parquet.hadoop.ParquetFileReader.filterRowGroups(ParquetFileReader.java:870)
  at org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:789)
  at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:657)
  at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:162)
  at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140)
  at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:373)
  at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:127)
...
{code}
At first it seems the problem is the parquet library. But in fact, our problem is because of this line that has been around since 2014 (based on Git history):

[Parquet Schema Compatibility Validator|https://github.com/apache/parquet-mr/blob/master/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/SchemaCompatibilityValidator.java#L194]

After some check, I notice that the cause of the problem is due to a change in the data filtering conditions:
{code:java}
spark.sql(""select * from log where array_contains(keywords, 'X')"").explain(true);

// Spark 3.0.2 and older
== Physical Plan ==
... 
+- FileScan parquet [link#0,keywords#1]
  DataFilters: [array_contains(keywords#1, Google)]
  PushedFilters: []
  ...

// Spark 3.1.0 and newer
== Physical Plan == ... 
+- FileScan parquet [link#0,keywords#1]
  DataFilters: [isnotnull(keywords#1),  array_contains(keywords#1, Google)]
  PushedFilters: [IsNotNull(keywords)]
  ...{code}
It's good that the filtering section has become smarter. Unfortunately, due to unfamiliarity with code base, I could not find the exact location of the change and related pull request. In general, this change is suitable for non-repeated parquet fields, but in the repeated field, it causes an error from the parquet library. (Like the example given)

The only temporary solution in my opinion to solve the problem is to disable the following setting, which in general greatly reduces performance:
{code:java}
SET spark.sql.parquet.filterPushdown=false {code}
I created a patch for this bug and a pull request will be sent soon.

 

 ",,apachespark,borjianamin,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 06 21:12:28 UTC 2022,,,,,,,,,,"0|z12zpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/22 21:12;apachespark;User 'Borjianamin98' has created a pull request for this issue:
https://github.com/apache/spark/pull/36781;;;",,,,,,,,,,,
Reuse Partitioner Classes,SPARK-39391,13448620,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,hvanhovell,hvanhovell,hvanhovell,06/Jun/22 16:23,06/Jun/22 19:51,13/Jul/23 08:47,06/Jun/22 19:51,3.2.1,,,,,,,,3.4.0,,,,,Spark Core,,,,0,,,There is a bit of duplication in the Partitioner class hierarchy. Let's remove it.,,apachespark,hvanhovell,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 06 17:11:51 UTC 2022,,,,,,,,,,"0|z12zlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jun/22 17:11;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/36779;;;",,,,,,,,,,,
Do not output duplicated columns in star expansion of subquery alias of NATURAL/USING JOIN,SPARK-39376,13448375,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,karenfeng,karenfeng,karenfeng,03/Jun/22 18:16,30/Aug/22 15:49,13/Jul/23 08:47,06/Jun/22 13:00,3.2.0,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"A bug was introduced in https://issues.apache.org/jira/browse/SPARK-34527 such that the duplicated columns within a NATURAL/USING JOIN were output from the qualified star of a subquery alias. For example:

{code:java}
val df1 = Seq((3, 8)).toDF(""a"", ""b"") 
val df2 = Seq((8, 7)).toDF(""b"", ""d"") 
val joinDF = df1.join(df2, ""b"")
joinDF.alias(""r"").select(""r.*"")
{code}

Outputs two duplicate `b` columns, instead of just one.",,apachespark,cloud_fan,karenfeng,,,,,,,,,,,,,,SPARK-38603,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 06 13:00:00 UTC 2022,,,,,,,,,,"0|z12y34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/22 19:04;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/36763;;;","06/Jun/22 13:00;cloud_fan;Issue resolved by pull request 36763
[https://github.com/apache/spark/pull/36763];;;",,,,,,,,,,
Recover spark.kubernetes.memoryOverheadFactor doc and remove deprecation,SPARK-39360,13448016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,02/Jun/22 00:03,02/Jun/22 03:10,13/Jul/23 08:47,02/Jun/22 03:10,3.3.0,,,,,,,,3.3.0,,,,,Kubernetes,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 02 03:10:25 UTC 2022,,,,,,,,,,"0|z12vvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jun/22 00:58;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36744;;;","02/Jun/22 03:10;dongjoon;Issue resolved by pull request 36744
[https://github.com/apache/spark/pull/36744];;;",,,,,,,,,,
Single column uses quoted to construct UnresolvedAttribute,SPARK-39355,13447957,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dzcxzl,dzcxzl,dzcxzl,01/Jun/22 15:19,15/Jun/22 08:06,13/Jul/23 08:47,15/Jun/22 08:06,3.2.0,,,,,,,,3.2.2,3.3.1,3.4.0,,,SQL,,,,0,,," 
{code:java}
select * from (select '2022-06-01' as c1 ) a where c1 in (select date_add('2022-06-01',0)); {code}
{code:java}
Error in query:
mismatched input '(' expecting {<EOF>, '.', '-'}(line 1, pos 8)
== SQL ==
date_add(2022-06-01, 0)
--------^^^ {code}
 ",,apachespark,dzcxzl,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 01 15:36:27 UTC 2022,,,,,,,,,,"0|z12vi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/22 15:36;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/36740;;;",,,,,,,,,,,
The analysis exception is incorrect,SPARK-39354,13447926,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,yumwang,yumwang,01/Jun/22 13:03,12/Dec/22 18:11,13/Jul/23 08:47,02/Jun/22 10:08,3.3.0,,,,,,,,3.3.0,3.4.0,,,,SQL,,,,0,,,"{noformat}
scala> spark.sql(""create table t1(user_id int, auct_end_dt date) using parquet;"")
res0: org.apache.spark.sql.DataFrame = []

scala> spark.sql(""select * from t1 join t2 on t1.user_id = t2.user_id where t1.auct_end_dt >= Date_sub('2020-12-27', 90)"").show
org.apache.spark.sql.AnalysisException: cannot resolve 'date_sub('2020-12-27', 90)' due to data type mismatch: argument 1 requires date type, however, ''2020-12-27'' is of string type.; line 1 pos 76
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.RemoveTempResolvedColumn$.$anonfun$apply$82(Analyzer.scala:4334)
  at org.apache.spark.sql.catalyst.analysis.RemoveTempResolvedColumn$.$anonfun$apply$82$adapted(Analyzer.scala:4327)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:365)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:364)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:364)
{noformat}

The analysis exception should be:
{noformat}
org.apache.spark.sql.AnalysisException: Table or view not found: t2
{noformat}

",,dongjoon,LuciferYang,maxgekk,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 02 10:08:36 UTC 2022,,,,,,,,,,"0|z12vbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/22 13:07;yumwang;[~maxgekk], I think this is a blocker issue for 3.3.0 release.;;;","01/Jun/22 13:16;gurwls223;[~yumwang] is this only exception message related?;;;","01/Jun/22 13:45;yumwang;Yes. It is only exception message related.;;;","01/Jun/22 16:35;LuciferYang;This issue was introduced by SPARK-38118:

 

[https://github.com/apache/spark/blob/5a3ba9b0b301a3b0c43f8d0d88e2b6bdce57d0e6/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L4353-L4372]

 

 
{code:java}
      // HAVING clause will be resolved as a Filter. When having func(column with wrong data type),
      // the column could be wrapped by a TempResolvedColumn, e.g. mean(tempresolvedcolumn(t.c)).
      // Because TempResolvedColumn can still preserve column data type, here is a chance to check
      // if the data type matches with the required data type of the function. We can throw an error
      // when data types mismatches.
      case operator: Filter =>
        operator.expressions.foreach(_.foreachUp {
          case e: Expression if e.childrenResolved && e.checkInputDataTypes().isFailure =>
            e.checkInputDataTypes() match {
              case TypeCheckResult.TypeCheckFailure(message) =>
                e.setTagValue(DATA_TYPE_MISMATCH_ERROR, true)
                e.failAnalysis(
                  s""cannot resolve '${e.sql}' due to data type mismatch: $message"" +
                    extraHintForAnsiTypeCoercionExpression(plan))
            }
          case _ =>
        })
      case _ => {code}
 

`case operator: Filter =>` is too broad, should add some restrictions 

 

 ;;;","01/Jun/22 17:12;maxgekk;Ping [~amaliujia] and [~wenchen] [~dongjoon] as you reviewed the PR which probably introduced the bug.;;;","01/Jun/22 17:17;dongjoon;Thank you for pinging me, [~maxgekk].;;;","01/Jun/22 17:25;dongjoon;Ah, got it. I removed my previous message. Yes, this is a regression.;;;","02/Jun/22 03:03;gurwls223;BTW, I don't think this is a release blocker because it only changes the error messages .. right?;;;","02/Jun/22 10:08;maxgekk;Issue resolved by pull request 36746
[https://github.com/apache/spark/pull/36746];;;",,,
Generate wrong time window when (timestamp-startTime) % slideDuration < 0,SPARK-39347,13447664,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,WweiL,nyingping,nyingping,31/May/22 09:41,06/Feb/23 03:08,13/Jul/23 08:47,06/Feb/23 03:08,3.3.0,,,,,,,,3.4.0,3.5.0,,,,Structured Streaming,,,,0,,,"Since the generation strategy of the sliding window in PR [#35362]([https://github.com/apache/spark/pull/35362]) is changed to the current one, and that leads to a new problem.

A window generation error occurs when the time required to process the recorded data is negative and the modulo value between the time and window length is less than 0. In the current test cases, this bug does not thorw up.

[ test(""negative timestamps"")]([https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/DataFrameTimeWindowingSuite.scala#L299])

 
{code:java}
val df1 = Seq(
  (""1970-01-01 00:00:02"", 1),
  (""1970-01-01 00:00:12"", 2)).toDF(""time"", ""value"")
val df2 = Seq(
  (LocalDateTime.parse(""1970-01-01T00:00:02""), 1),
  (LocalDateTime.parse(""1970-01-01T00:00:12""), 2)).toDF(""time"", ""value"")

Seq(df1, df2).foreach { df =>
  checkAnswer(
    df.select(window($""time"", ""10 seconds"", ""10 seconds"", ""5 seconds""), $""value"")
      .orderBy($""window.start"".asc)
      .select($""window.start"".cast(StringType), $""window.end"".cast(StringType), $""value""),
    Seq(
      Row(""1969-12-31 23:59:55"", ""1970-01-01 00:00:05"", 1),
      Row(""1970-01-01 00:00:05"", ""1970-01-01 00:00:15"", 2))
  )
} {code}
 

 

The timestamp of the above test data is not negative, and the value modulo the window length is not negative, so it can be passes the test case.

An exception occurs when the timestamp becomes something like this.

 
{code:java}
val df3 = Seq(
  (""1969-12-31 00:00:02"", 1),
  (""1969-12-31 00:00:12"", 2)).toDF(""time"", ""value"")
val df4 = Seq(
  (LocalDateTime.parse(""1969-12-31T00:00:02""), 1),
  (LocalDateTime.parse(""1969-12-31T00:00:12""), 2)).toDF(""time"", ""value"")

Seq(df3, df4).foreach { df =>
  checkAnswer(
    df.select(window($""time"", ""10 seconds"", ""10 seconds"", ""5 seconds""), $""value"")
      .orderBy($""window.start"".asc)
      .select($""window.start"".cast(StringType), $""window.end"".cast(StringType), $""value""),
    Seq(
      Row(""1969-12-30 23:59:55"", ""1969-12-31 00:00:05"", 1),
      Row(""1969-12-31 00:00:05"", ""1969-12-31 00:00:15"", 2))
  )
} {code}
 

run and get unexpected result:

 
{code:java}
== Results ==
!== Correct Answer - 2 ==                      == Spark Answer - 2 ==
!struct<>                                      struct<CAST(window.start AS STRING):string,CAST(window.end AS STRING):string,value:int>
![1969-12-30 23:59:55,1969-12-31 00:00:05,1]   [1969-12-31 00:00:05,1969-12-31 00:00:15,1]
![1969-12-31 00:00:05,1969-12-31 00:00:15,2]   [1969-12-31 00:00:15,1969-12-31 00:00:25,2] {code}
 

*benchmark result*

 

oldlogic[#18364]([https://github.com/apache/spark/pull/18364])  VS 【fix version】
{code:java}
Running benchmark: tumbling windows
Running case: old logic
Stopped after 407 iterations, 10012 ms
Running case: new logic
Stopped after 615 iterations, 10007 ms
Java HotSpot(TM) 64-Bit Server VM 1.8.0_181-b13 on Windows 10 10.0
Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
tumbling windows:                         Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
old logic                                            17             25           9        580.1           1.7       1.0X
new logic                                            15             16           2        680.8           1.5       1.2X
Running benchmark: sliding windows
Running case: old logic
Stopped after 10 iterations, 10296 ms
Running case: new logic
Stopped after 15 iterations, 10391 ms
Java HotSpot(TM) 64-Bit Server VM 1.8.0_181-b13 on Windows 10 10.0
Intel64 Family 6 Model 158 Stepping 10, GenuineIntel
sliding windows:                          Best Time(ms)   Avg Time(ms)   Stdev(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------------------------------
old logic                                          1000           1030          19         10.0         100.0       1.0X
new logic                                           668            693          21         15.0          66.8       1.5X
{code}
 

 

Fixed version than PR [#38069]([https://github.com/apache/spark/pull/35362]) lost a bit of the performance.",,apachespark,kabhwan,nyingping,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 03:08:13 UTC 2023,,,,,,,,,,"0|z12tp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jun/22 06:03;apachespark;User 'nyingping' has created a pull request for this issue:
https://github.com/apache/spark/pull/36737;;;","01/Feb/23 08:21;apachespark;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/39843;;;","01/Feb/23 08:22;apachespark;User 'WweiL' has created a pull request for this issue:
https://github.com/apache/spark/pull/39843;;;","06/Feb/23 03:08;kabhwan;Issue resolved via https://github.com/apache/spark/pull/39843;;;",,,,,,,,
KubernetesExecutorBackend should allow IPv6 pod IP,SPARK-39341,13447569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,30/May/22 19:03,30/May/22 20:35,13/Jul/23 08:47,30/May/22 20:07,3.3.0,,,,,,,,3.3.0,,,,,Kubernetes,,,,0,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,SPARK-36058,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 30 20:07:46 UTC 2022,,,,,,,,,,"0|z12t48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/22 19:05;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36728;;;","30/May/22 20:07;dongjoon;Issue resolved by pull request 36728
[https://github.com/apache/spark/pull/36728];;;",,,,,,,,,,
DS v2 agg pushdown should allow dots in the name of top-level columns,SPARK-39340,13447555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,30/May/22 16:30,22/Jun/22 15:01,13/Jul/23 08:47,31/May/22 14:17,3.2.0,,,,,,,,3.2.2,3.3.1,3.4.0,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 21 15:56:48 UTC 2022,,,,,,,,,,"0|z12t14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/22 16:37;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36727;;;","30/May/22 16:37;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36727;;;","31/May/22 14:17;cloud_fan;Issue resolved by pull request 36727
[https://github.com/apache/spark/pull/36727];;;","21/Jun/22 15:56;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36945;;;",,,,,,,,
Remove dynamic pruning subquery if pruningKey's references is empty,SPARK-39338,13447456,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,30/May/22 06:58,01/Jun/22 03:38,13/Jul/23 08:47,01/Jun/22 03:38,3.3.0,3.4.0,,,,,,,3.4.0,,,,,SQL,,,,0,,,,,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 30 07:36:58 UTC 2022,,,,,,,,,,"0|z12sf4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/May/22 07:36;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36724;;;",,,,,,,,,,,
V2ExpressionUtils.toCatalystOrdering should fail if V2Expression can not be translated,SPARK-39313,13447150,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,chengpan,chengpan,,27/May/22 06:50,24/Nov/22 00:38,13/Jul/23 08:47,01/Jun/22 16:50,3.3.0,,,,,,,,3.3.0,3.4.0,,,,SQL,,,,0,,,,,apachespark,csun,,,,,,,,,,,,,,,,,SPARK-37377,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 01 16:50:06 UTC 2022,,,,,,,,,,"0|z12ql4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/May/22 06:57;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/36697;;;","27/May/22 06:58;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/36697;;;","01/Jun/22 16:50;csun;Issue resolved by pull request 36697
[https://github.com/apache/spark/pull/36697];;;",,,,,,,,,
Replcace `Array.toString` with `Array.mkString`,SPARK-39296,13446914,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,26/May/22 02:08,12/Dec/22 18:11,13/Jul/23 08:47,27/May/22 06:43,3.4.0,,,,,,,,3.4.0,,,,,Spark Core,SQL,,,0,,,"Some code in Spark as follows:
{code:java}
val appDirs = workDir.listFiles()
if (appDirs == null) {
  throw new IOException(""ERROR: Failed to list files in "" + appDirs)
} {code}
appDirs is an `Array` and `Array.toString` will print as `className#hashCode`, this result seems meaningless and  Array.mkString should be used.",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 27 06:43:23 UTC 2022,,,,,,,,,,"0|z12p4w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/May/22 02:38;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36677;;;","27/May/22 06:43;gurwls223;Issue resolved by pull request 36677
[https://github.com/apache/spark/pull/36677];;;",,,,,,,,,,
"The accumulator of ArrayAggregate should copy the intermediate result if string, struct, array, or map",SPARK-39293,13446899,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ueshin,ueshin,ueshin,25/May/22 22:27,12/Dec/22 18:11,13/Jul/23 08:47,26/May/22 01:41,3.0.3,3.1.2,3.2.1,3.3.0,,,,,3.0.4,3.1.3,3.2.2,3.3.0,,SQL,,,,0,correctness,,"The accumulator of ArrayAggregate should copy the intermediate result if string, struct, array, or map.

{code:scala}
import org.apache.spark.sql.functions._

val reverse = udf((s: String) => s.reverse)

val df = Seq(Array(""abc"", ""def"")).toDF(""array"")
val testArray = df.withColumn(
  ""agg"",
  aggregate(
    col(""array""),
    array().cast(""array<string>""),
    (acc, s) => concat(acc, array(reverse(s)))))

aggArray.show(truncate=false)
{code}

should be:

{code}
+----------+----------+
|array     |agg       |
+----------+----------+
|[abc, def]|[cba, fed]|
+----------+----------+
{code}

but:

{code}
+----------+----------+
|array     |agg       |
+----------+----------+
|[abc, def]|[fed, fed]|
+----------+----------+
{code}

",,apachespark,dongjoon,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 30 02:15:47 UTC 2022,,,,,,,,,,"0|z12p1k:",9223372036854775807,,,,,,,,,,,,,3.3.0,,,,,,,,,,"25/May/22 22:37;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36674;;;","25/May/22 22:38;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36674;;;","26/May/22 01:41;gurwls223;Fixed in https://github.com/apache/spark/pull/36674;;;","27/May/22 20:33;dongjoon;According to [~ueshin]'s email, I raised the priority as a blocker for Apache Spark 3.3.
This is not a regression, but a correctness issue.;;;","30/May/22 02:15;gurwls223;Yeah I agree.;;;",,,,,,,
Documentation for the decode function has an incorrect reference,SPARK-39286,13446733,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,lucacanali,lucacanali,lucacanali,25/May/22 07:53,10/Jun/22 04:21,13/Jul/23 08:47,07/Jun/22 08:08,3.2.1,,,,,,,,3.3.0,,,,,Documentation,,,,0,,,"The documentation for the decode function introduced in SPARK-33527 refers erroneously to Oracle. It appears that the documentation string has been in large parts copied from [https://docs.oracle.com/en/database/oracle/oracle-database/19/sqlrf/DECODE.html#GUID-39341D91-3442-4730-BD34-D3CF5D4701CE]

This proposes to update the documentation for the decode function to fix the issue.",,apachespark,cloud_fan,lucacanali,,,,,,,,,,,,,,SPARK-39418,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 07 08:08:04 UTC 2022,,,,,,,,,,"0|z12o0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/22 08:11;apachespark;User 'LucaCanali' has created a pull request for this issue:
https://github.com/apache/spark/pull/36662;;;","07/Jun/22 08:08;cloud_fan;Issue resolved by pull request 36662
[https://github.com/apache/spark/pull/36662];;;",,,,,,,,,,
Spark tasks stuck forever due to deadlock between TaskMemoryManager and UnsafeExternalSorter,SPARK-39283,13446673,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,sandeep.pal,sandeep.pal,sandeep.pal,25/May/22 03:05,02/Jun/22 16:31,13/Jul/23 08:47,31/May/22 23:01,3.0.0,3.1.2,,,,,,,3.0.4,3.1.4,3.2.2,3.3.0,,Spark Core,,,,0,Deadlock,spark3.0,"We are seems this deadlock between {{TaskMemoryManager}} and {{UnsafeExternalSorter}} pretty often on our workload. Sometime, the retry is successful but sometimes we have to do hacky ways to break the deadlocks such as turning down the worker machines explicitly. 

Below is the thread dump from the Spark UI showing the deadlock :

!DeadlockSparkTasks.png!

 

I believe there was a related Jira on the similar deadlock between the same threads and it was resolved. 
https://issues.apache.org/jira/browse/SPARK-27338

 

 ",,apachespark,joshrosen,sandeep.pal,xkrogen,,,,,,,,,,,,,,,,,,,,,"25/May/22 03:09;sandeep.pal;DeadlockSparkTasks.png;https://issues.apache.org/jira/secure/attachment/13044155/DeadlockSparkTasks.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 31 23:01:20 UTC 2022,,,,,,,,,,"0|z12nnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/May/22 16:51;sandeep.pal;[~cloud_fan] This seems to occur frequently on spark 3.1.2, let me know if you need more information from my side. ;;;","26/May/22 03:53;apachespark;User 'sandeepvinayak' has created a pull request for this issue:
https://github.com/apache/spark/pull/36680;;;","31/May/22 23:01;joshrosen;Fixed by [https://github.com/apache/spark/pull/36680];;;",,,,,,,,,
Timestamps returned by now() and equivalent functions are not consistent in subqueries,SPARK-39259,13446415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,olaky,olaky,olaky,23/May/22 16:04,12/Jun/22 09:17,13/Jul/23 08:47,02/Jun/22 18:43,3.2.1,,,,,,,,3.1.4,3.2.2,3.3.0,3.4.0,,Optimizer,,,,0,correctness,,"Timestamp evaluation in not consistent across subqueries. As an example for the Spark Shell

 
{code:java}
sql(""SELECT * FROM (SELECT 1) WHERE now() IN (SELECT now())"").collect() {code}
Returns an empty result.

 

The root cause is that [ComputeCurrentTime|https://github.com/apache/spark/blob/c91c2e9afec0d5d5bbbd2e155057fe409c5bb928/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/finishAnalysis.scala#L74] does not iterate into subqueries",Reproduced in the Spark Shell on the current 3.4.0 snapshot,apachespark,joshrosen,maxgekk,olaky,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,Thu Jun 09 11:21:06 UTC 2022,,,,,,,,,,"0|z12m28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/22 16:05;olaky;I will create a PR to fix this in the upcoming days;;;","24/May/22 14:02;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/36654;;;","02/Jun/22 18:43;maxgekk;Issue resolved by pull request 36654
[https://github.com/apache/spark/pull/36654];;;","02/Jun/22 18:59;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/36753;;;","02/Jun/22 19:00;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/36752;;;","03/Jun/22 18:02;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36762;;;","04/Jun/22 00:58;joshrosen;This looks like it might be a fix for a correctness issue? If so, we should probably backport this change to maintenance branches for the other currently-supported Spark versions.;;;","04/Jun/22 02:04;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/36765;;;","04/Jun/22 02:05;apachespark;User 'JoshRosen' has created a pull request for this issue:
https://github.com/apache/spark/pull/36765;;;","09/Jun/22 08:18;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/36818;;;","09/Jun/22 11:20;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/36822;;;","09/Jun/22 11:21;apachespark;User 'olaky' has created a pull request for this issue:
https://github.com/apache/spark/pull/36822;;;"
Fix `Hide credentials in show create table` after SPARK-35378,SPARK-39258,13446409,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,23/May/22 15:41,23/May/22 21:28,13/Jul/23 08:47,23/May/22 21:28,3.4.0,,,,,,,,3.2.2,3.3.0,,,,Tests,,,,0,,,"master UT failed after https://github.com/apache/spark/pull/36632

 
{code:java}
2022-05-23T09:48:27.7668925Z [info] - Hide credentials in show create table *** FAILED *** (43 milliseconds)
2022-05-23T09:48:27.7696056Z [info]   ""[0,10000000d5,5420455441455243,62617420454c4241,414e20200a282031,4e4952545320454d,45485420200a2c47,a29544e49204449,726f20474e495355,6568636170612e67,732e6b726170732e,a6362646a2e6c71,20534e4f4954504f,7462642720200a28,203d2027656c6261,45502e5453455427,200a2c27454c504f,6f77737361702720,2a27203d20276472,2a2a2a2a2a2a2a2a,6574636164657228,2720200a2c272964,27203d20276c7275,2a2a2a2a2a2a2a2a,746361646572282a,20200a2c27296465,3d20277265737527,7355747365742720,a29277265]"" did not contain ""TEST.PEOPLE"" (JDBCSuite.scala:1146)
2022-05-23T09:48:27.7697358Z [info]   org.scalatest.exceptions.TestFailedException:
2022-05-23T09:48:27.7697954Z [info]   at org.scalatest.Assertions.newAssertionFailedException(Assertions.scala:472)
2022-05-23T09:48:27.7698605Z [info]   at org.scalatest.Assertions.newAssertionFailedException$(Assertions.scala:471)
2022-05-23T09:48:27.7699211Z [info]   at org.scalatest.Assertions$.newAssertionFailedException(Assertions.scala:1231)
2022-05-23T09:48:27.7699785Z [info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:1295)
2022-05-23T09:48:27.7700358Z [info]   at org.apache.spark.sql.jdbc.JDBCSuite.$anonfun$new$117(JDBCSuite.scala:1146)
2022-05-23T09:48:27.7700946Z [info]   at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
2022-05-23T09:48:27.7701557Z [info]   at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
2022-05-23T09:48:27.7702126Z [info]   at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
2022-05-23T09:48:27.7702799Z [info]   at org.apache.spark.sql.jdbc.JDBCSuite.$anonfun$new$116(JDBCSuite.scala:1144)
2022-05-23T09:48:27.7703346Z [info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2022-05-23T09:48:27.7703870Z [info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1481)
2022-05-23T09:48:27.7704451Z [info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:306)
2022-05-23T09:48:27.7705082Z [info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:304)
2022-05-23T09:48:27.7705692Z [info]   at org.apache.spark.sql.jdbc.JDBCSuite.withTable(JDBCSuite.scala:48)
2022-05-23T09:48:27.7706491Z [info]   at org.apache.spark.sql.jdbc.JDBCSuite.$anonfun$new$115(JDBCSuite.scala:1131)
2022-05-23T09:48:27.7707032Z [info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
2022-05-23T09:48:27.7707551Z [info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
2022-05-23T09:48:27.7708053Z [info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
2022-05-23T09:48:27.7708544Z [info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
2022-05-23T09:48:27.7709026Z [info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
2022-05-23T09:48:27.7709522Z [info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
2022-05-23T09:48:27.7710081Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:190)
2022-05-23T09:48:27.7710740Z [info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:203)
2022-05-23T09:48:27.7711377Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)
2022-05-23T09:48:27.7712015Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200)
2022-05-23T09:48:27.7712561Z [info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
2022-05-23T09:48:27.7713121Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)
2022-05-23T09:48:27.7713713Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)
2022-05-23T09:48:27.7714354Z [info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:64)
2022-05-23T09:48:27.7714968Z [info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
2022-05-23T09:48:27.7715541Z [info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
2022-05-23T09:48:27.7716158Z [info]   at org.apache.spark.sql.jdbc.JDBCSuite.org$scalatest$BeforeAndAfter$$super$runTest(JDBCSuite.scala:48)
2022-05-23T09:48:27.7716733Z [info]   at org.scalatest.BeforeAndAfter.runTest(BeforeAndAfter.scala:213)
2022-05-23T09:48:27.7717270Z [info]   at org.scalatest.BeforeAndAfter.runTest$(BeforeAndAfter.scala:203)
2022-05-23T09:48:27.7717813Z [info]   at org.apache.spark.sql.jdbc.JDBCSuite.runTest(JDBCSuite.scala:48)
2022-05-23T09:48:27.7718398Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:233)
2022-05-23T09:48:27.7718974Z [info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
2022-05-23T09:48:27.7719487Z [info]   at scala.collection.immutable.List.foreach(List.scala:431)
2022-05-23T09:48:27.7720016Z [info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
2022-05-23T09:48:27.7720554Z [info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
2022-05-23T09:48:27.7721076Z [info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
2022-05-23T09:48:27.7721641Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:233)
2022-05-23T09:48:27.7722297Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:232)
2022-05-23T09:48:27.7722872Z [info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563)
2022-05-23T09:48:27.7723355Z [info]   at org.scalatest.Suite.run(Suite.scala:1112)
2022-05-23T09:48:27.7723787Z [info]   at org.scalatest.Suite.run$(Suite.scala:1094)
2022-05-23T09:48:27.7734912Z [info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1563)
2022-05-23T09:48:27.7735578Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:237)
2022-05-23T09:48:27.7736113Z [info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
2022-05-23T09:48:27.7736675Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:237)
2022-05-23T09:48:27.7737441Z [info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:236)
2022-05-23T09:48:27.7738075Z [info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:64)
2022-05-23T09:48:27.7738690Z [info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
2022-05-23T09:48:27.7739261Z [info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
2022-05-23T09:48:27.7739809Z [info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
2022-05-23T09:48:27.7740411Z [info]   at org.apache.spark.sql.jdbc.JDBCSuite.org$scalatest$BeforeAndAfter$$super$run(JDBCSuite.scala:48)
2022-05-23T09:48:27.7740976Z [info]   at org.scalatest.BeforeAndAfter.run(BeforeAndAfter.scala:273)
2022-05-23T09:48:27.7741492Z [info]   at org.scalatest.BeforeAndAfter.run$(BeforeAndAfter.scala:271)
2022-05-23T09:48:27.7742012Z [info]   at org.apache.spark.sql.jdbc.JDBCSuite.run(JDBCSuite.scala:48)
2022-05-23T09:48:27.7742583Z [info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
2022-05-23T09:48:27.7743347Z [info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
2022-05-23T09:48:27.7743842Z [info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
2022-05-23T09:48:27.7744321Z [info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
2022-05-23T09:48:27.7744909Z [info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
2022-05-23T09:48:27.7745525Z [info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
2022-05-23T09:48:27.7746016Z [info]   at java.lang.Thread.run(Thread.java:750) {code}",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 23 21:28:51 UTC 2022,,,,,,,,,,"0|z12m0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/May/22 15:44;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36637;;;","23/May/22 15:45;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36637;;;","23/May/22 21:28;dongjoon;Issue resolved by pull request 36637
[https://github.com/apache/spark/pull/36637];;;",,,,,,,,,
Upgrade Jackson to 2.13.3,SPARK-39250,13446200,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,22/May/22 02:32,22/May/22 21:14,13/Jul/23 08:47,22/May/22 21:14,3.3.0,,,,,,,,3.3.0,,,,,Build,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 22 21:14:53 UTC 2022,,,,,,,,,,"0|z12kqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/May/22 02:35;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36627;;;","22/May/22 02:35;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36627;;;","22/May/22 21:14;dongjoon;Issue resolved by pull request 36627
[https://github.com/apache/spark/pull/36627];;;",,,,,,,,,
AwaitOffset does not wait correctly for atleast expected offset and RateStreamProvider test is flaky,SPARK-39242,13446097,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,anishshri-db,anishshri-db,anishshri-db,20/May/22 17:29,22/May/22 02:16,13/Jul/23 08:47,22/May/22 02:16,3.2.1,,,,,,,,3.4.0,,,,,Structured Streaming,,,,0,,,AwaitOffset does not wait correctly for atleast expected offset and RateStreamProvider test is flaky,,anishshri-db,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 22 02:16:07 UTC 2022,,,,,,,,,,"0|z12k3k:",9223372036854775807,,,,,kabhwan,,,,,,,,,,,,,,,,,,"20/May/22 17:30;anishshri-db;I have found the root cause for the issue and will submit the PR soon.;;;","20/May/22 18:24;apachespark;User 'anishshri-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36620;;;","20/May/22 18:32;anishshri-db;PR for the change submitted here: [https://github.com/apache/spark/pull/36620]

 

CC - [~kabhwan] - please take a look. Thanks;;;","22/May/22 02:16;kabhwan;Issue resolved by pull request 36620
[https://github.com/apache/spark/pull/36620];;;",,,,,,,,
Remove the check for TimestampNTZ output in Analyzer,SPARK-39233,13445803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,19/May/22 10:16,19/May/22 12:35,13/Jul/23 08:47,19/May/22 12:35,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"In [#36094|https://github.com/apache/spark/pull/36094], a check for failing TimestampNTZ output is added.

However, the check can cause misleading error message.

In 3.3:

 
{code:java}
> sql( ""select date '2018-11-17' > 1"").show()
org.apache.spark.sql.AnalysisException: Invalid call to toAttribute on unresolved object;
'Project [unresolvedalias((2018-11-17 > 1), None)]
+- OneRowRelation   
 
  at org.apache.spark.sql.catalyst.analysis.UnresolvedAlias.toAttribute(unresolved.scala:510)
  at org.apache.spark.sql.catalyst.plans.logical.Project.$anonfun$output$1(basicLogicalOperators.scala:70) {code}
In master or 3.2
{code:java}
> sql( ""select date '2018-11-17' > 1"").show()
org.apache.spark.sql.AnalysisException: cannot resolve '(DATE '2018-11-17' > 1)' due to data type mismatch: differing types in '(DATE '2018-11-17' > 1)' (date and int).; line 1 pos 7;
'Project [unresolvedalias((2018-11-17 > 1), None)]
+- OneRowRelation


  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42) {code}
We should just remove the check

 ",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 19 12:35:07 UTC 2022,,,,,,,,,,"0|z12ia8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/May/22 10:22;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36609;;;","19/May/22 10:22;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36609;;;","19/May/22 12:35;Gengliang.Wang;Issue resolved by pull request 36609
[https://github.com/apache/spark/pull/36609];;;",,,,,,,,,
Fix the precision of the return type of round-like functions,SPARK-39226,13445684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,18/May/22 17:37,09/Jun/22 09:19,13/Jul/23 08:47,19/May/22 05:40,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,,,apachespark,cloud_fan,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 09 09:19:49 UTC 2022,,,,,,,,,,"0|z12hjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/May/22 18:15;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36598;;;","19/May/22 05:40;Gengliang.Wang;Issue resolved by pull request 36598
[https://github.com/apache/spark/pull/36598];;;","09/Jun/22 09:19;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36821;;;",,,,,,,,,
sensitive information is not redacted correctly on thrift job/stage page,SPARK-39221,13445577,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,18/May/22 07:59,20/May/22 02:12,13/Jul/23 08:47,20/May/22 02:12,3.1.2,3.2.1,3.3.0,,,,,,3.4.0,,,,,SQL,,,,0,,,,,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 20 02:12:24 UTC 2022,,,,,,,,,,"0|z12gw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/May/22 08:15;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36592;;;","18/May/22 08:16;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36592;;;","20/May/22 02:12;Qin Yao;Issue resolved by pull request 36592
[https://github.com/apache/spark/pull/36592];;;",,,,,,,,,
Python foreachBatch streaming query cannot be stopped gracefully after pin thread mode is enabled,SPARK-39218,13445560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,18/May/22 06:18,12/Dec/22 18:10,13/Jul/23 08:47,20/May/22 04:04,3.0.3,3.1.2,3.2.1,3.3.0,,,,,3.3.0,,,,,PySpark,Structured Streaming,,,0,,,"For example,

{code}
import time

def func(batch_df, batch_id):
    time.sleep(10)
    print(batch_df.count())

q = spark.readStream.format(""rate"").load().writeStream.foreachBatch(func).start()
time.sleep(5)
q.stop()
{code}

works find with pinned thread mode is disabled. Whe pinned thread mode is enabled:

{code}
22/05/18 15:23:24 ERROR MicroBatchExecution: Query [id = 2538f8a2-c6e4-44c9-bf38-e6dab555267e, runId = 1d500478-1d77-46aa-b35a-585264a809b9] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File ""/.../spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py"", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
  File ""/.../spark/python/pyspark/sql/utils.py"", line 272, in call
    raise e
  File ""/.../spark/python/pyspark/sql/utils.py"", line 269, in call
    self.func(DataFrame(jdf, self.session), batch_id)
  File ""<stdin>"", line 3, in func
  File ""/.../spark/python/pyspark/sql/dataframe.py"", line 804, in count
    return int(self._jdf.count())
  File ""/.../spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py"", line 1321, in __call__
    return_value = get_return_value(
  File ""/.../spark/python/pyspark/sql/utils.py"", line 190, in deco
    return f(*a, **kw)
  File ""/.../spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py"", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o44.count.
: java.lang.InterruptedException
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1302)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)
	at org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:334)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:943)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2227)
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 20 04:04:30 UTC 2022,,,,,,,,,,"0|z12gs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/May/22 07:15;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36589;;;","20/May/22 04:04;gurwls223;Issue resolved by pull request 36589
[https://github.com/apache/spark/pull/36589];;;",,,,,,,,,,
Do not collapse projects in CombineUnions if it hasCorrelatedSubquery,SPARK-39216,13445526,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,allisonwang-db,allisonwang-db,18/May/22 02:15,24/May/22 17:10,13/Jul/23 08:47,19/May/22 06:38,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,," 

SPARK-37915 added CollapseProject in rule CombineUnions, but it shouldn't collapse projects that contain correlated subqueries since haven't been de-correlated (PullupCorrelatedPredicates).

Here is a simple example to reproduce this issue
{code:java}
SELECT (SELECT IF(x, 1, 0)) AS a
FROM (SELECT true) t(x)
UNION 
SELECT 1 AS a {code}
Exception:
{code:java}
java.lang.IllegalStateException: Couldn't find x#4 in [] {code}
 ",,allisonwang-db,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,SPARK-37915,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 19 06:38:05 UTC 2022,,,,,,,,,,"0|z12gko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/May/22 07:55;cloud_fan;[~yumwang] can you take a look?;;;","18/May/22 11:16;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36595;;;","18/May/22 11:17;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36595;;;","19/May/22 06:38;dongjoon;Issue resolved by pull request 36595
[https://github.com/apache/spark/pull/36595];;;",,,,,,,,
Cannot refer to nested CTE within a nested CTE in a subquery.,SPARK-39198,13445231,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,jarraj,jarraj,16/May/22 17:39,22/Feb/23 10:24,13/Jul/23 08:47,22/Feb/23 10:24,3.2.1,3.3.0,,,,,,,,,,,,SQL,,,,1,,,"The following query fails with {color:#ff0000}Table or view not found: cte1;{color}
{code:java}
set spark.sql.legacy.ctePrecedencePolicy=CORRECTED;
with
cte1 as (select 1)
select * from (
  with
    cte2 as (select * from cte1)
    select * from cte2
); {code}
Or Spark 3.1.1 it returns 1 as expected.

This is related to SPARK-38404, but different, since the query fails with Spark built from 'master' (commit 17b85ff9). The [PR #36146|https://github.com/apache/spark/pull/36146] therefore does not fix this issue.","Tested on
 * Databricks runtime 10.4
 * Spark 3.2.1 from [https://spark.apache.org/downloads.html]
 * GitHub apache/spark 'master' commit 17b85ff9",jarraj,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 10:23:57 UTC 2023,,,,,,,,,,"0|z12es0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/23 10:23;jarraj;Seems like the issue has been fixed in 3.3.2.;;;",,,,,,,,,,,
ArrayIndexOutOfBoundsException for some date/time sequences in some time-zones,SPARK-39184,13444915,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,13/May/22 22:05,29/Aug/22 01:45,13/Jul/23 08:47,16/Aug/22 08:57,3.1.3,3.2.1,3.3.0,3.4.0,,,,,3.1.4,3.2.3,3.3.1,3.4.0,,SQL,,,,0,,,"The following query gets an {{ArrayIndexOutOfBoundsException}} when run from the {{America/Los_Angeles}} time-zone:
{noformat}
spark-sql> select sequence(timestamp'2022-03-13 00:00:00', timestamp'2022-03-16 03:00:00', interval 1 day 1 hour) as x;
22/05/13 14:47:27 ERROR SparkSQLDriver: Failed in [select sequence(timestamp'2022-03-13 00:00:00', timestamp'2022-03-16 03:00:00', interval 1 day 1 hour) as x]
java.lang.ArrayIndexOutOfBoundsException: 3
{noformat}
In fact, any such query will get an {{ArrayIndexOutOfBoundsException}} if the start-stop period in your time-zone includes more instances of ""spring forward"" than instances of ""fall back"" and the start-stop period is evenly divisible by the interval.

In the {{America/Los_Angeles}} time-zone, examples include:
{noformat}
-- This query encompasses 2 instances of ""spring forward"" but only one
-- instance of ""fall back"".
select sequence(
  timestamp'2022-03-13',
  timestamp'2022-03-13' + (interval '42' hours * 209),
  interval '42' hours) as x;
{noformat}
{noformat}
select sequence(
  timestamp'2022-03-13',
  timestamp'2022-03-13' + (interval '31' hours * 11),
  interval '31' hours) as x;
{noformat}",,apachespark,bersprockets,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 28 16:21:03 UTC 2022,,,,,,,,,,"0|z12cu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/May/22 22:07;bersprockets;I will take a stab at fixing this when the fix for SPARK-37544 is merged (which is changing code in the same area).;;;","15/May/22 18:59;bersprockets;Some notes on what is happening.

Let's take this small reproduction example (this fails in the {{America/Los_Angeles}} time-zone):
{noformat}
select sequence(
  timestamp'2022-03-13 00:00:00',
  timestamp'2022-03-14 01:00:00',
  interval 1 day 1 hour) as x;
{noformat}
This produces the error:
{noformat}
java.lang.ArrayIndexOutOfBoundsException: 1
	at scala.runtime.ScalaRunTime$.array_update(ScalaRunTime.scala:77) ~[scala-library.jar:?]
{noformat}
The following shows what is happening (essentially), using Scala code that can be pasted into the REPL:
{noformat}
import java.time._
import java.time.temporal.ChronoUnit._

val zid = ZoneId.of(""America/Los_Angeles"")
val startZdt = ZonedDateTime.of(LocalDateTime.of(2022, 3, 13, 0, 0, 0, 0), zid)
val stopZdt = ZonedDateTime.of(LocalDateTime.of(2022, 3, 14, 1, 0, 0, 0), zid)

val interval = Duration.ofDays(1).plusHours(1)
print(interval.toHours)
// prints 25

// the diff between the start and stop is 24 hours, because 2022-03-13 has
// only 23 hours in the America/Los_Angeles time-zone due to ""Spring Forward""
val hours = startZdt.until(stopZdt, HOURS)
println(hours)
// prints 24

// this is how InternalSequenceBase estimates the size of the result array
// (it actually uses micros, but we're just scaling to hours for simplicity here).
// This calculates a single element
print((hours/interval.toHours) + 1)
// prints 1

// InternalSequenceBase thinks it only needs one array element because if you add
// 25 hours to '2022-03-13 00:00:00', you get '2022-03-14 02:00:00', which is greater
// than the stop value.
// To show that, we add 25 hours to the start value
println(startZdt.plusHours(25))
// prints 2022-03-14T02:00-07:00[America/Los_Angeles]

// However, when calculating the value to put in each element, InternalSequenceBase
// doesn't add 25 hours to the previous value (or start value). It instead
// adds 1 day and 1 hour. That gives you '2022-03-14 01:00:00', which is equal to the
// stop value (and thus should be included).
// As a result, we blow past the end of the pre-allocated array.
println(startZdt.plusDays(1).plusHours(1))
// prints 2022-03-14T01:00-07:00[America/Los_Angeles]
{noformat};;;","14/Aug/22 16:24;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/37513;;;","16/Aug/22 08:57;maxgekk;Resolved by https://github.com/apache/spark/pull/37513;;;","17/Aug/22 01:25;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/37542;;;","17/Aug/22 01:26;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/37542;;;","28/Aug/22 16:21;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/37699;;;",,,,,
SHOW DATABASES command should not quote database names under legacy mode,SPARK-39149,13444344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/May/22 08:27,12/May/22 03:19,13/Jul/23 08:47,12/May/22 03:19,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 12 03:19:00 UTC 2022,,,,,,,,,,"0|z129bs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/May/22 08:54;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/36508;;;","12/May/22 03:19;cloud_fan;Issue resolved by pull request 36508
[https://github.com/apache/spark/pull/36508];;;",,,,,,,,,,
Nested subquery expressions deduplicate relations should be done bottom up,SPARK-39144,13444254,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,amaliujia,amaliujia,amaliujia,10/May/22 20:52,24/May/22 05:07,13/Jul/23 08:47,24/May/22 05:07,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"When we have nested subquery expressions, there is a chance that deduplicate relations could replace an attributes with a wrong one. This is because the attributes replacement is done by top down than bottom up. This could happen if the subplan gets deduplicate relations first (thus two same relation with different attributes id), then a more complex plan built on top of the subplan (e.g. a UNION of queries with nested subquery expressions) can trigger this wrong attribute replacement error.",,amaliujia,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 24 05:07:00 UTC 2022,,,,,,,,,,"0|z128s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/May/22 20:53;amaliujia;Testing in https://github.com/apache/spark/pull/36503. Will come up with an example.;;;","10/May/22 20:54;apachespark;User 'amaliujia' has created a pull request for this issue:
https://github.com/apache/spark/pull/36503;;;","10/May/22 20:55;apachespark;User 'amaliujia' has created a pull request for this issue:
https://github.com/apache/spark/pull/36503;;;","24/May/22 05:07;cloud_fan;Issue resolved by pull request 36503
[https://github.com/apache/spark/pull/36503];;;",,,,,,,,
spark3.2.1 cache throw NPE,SPARK-39132,13444033,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,cxb,cxb,09/May/22 17:43,24/May/22 06:50,13/Jul/23 08:47,24/May/22 06:50,3.2.1,,,,,,,,,,,,,Structured Streaming,,,,0,,,"a job running some time about 1 day will throw the exception when i upgrade spark version to 3.2.1

gc log: 
{code:java}
Heap
 par new generation   total 307840K, used 239453K [0x0000000080000000, 0x0000000094e00000, 0x00000000aaaa0000)
  eden space 273664K,  81% used [0x0000000080000000, 0x000000008da4bdd0, 0x0000000090b40000)
  from space 34176K,  46% used [0x0000000092ca0000, 0x0000000093c2b6b8, 0x0000000094e00000)
  to   space 34176K,   0% used [0x0000000090b40000, 0x0000000090b40000, 0x0000000092ca0000)
 concurrent mark-sweep generation total 811300K, used 451940K [0x00000000aaaa0000, 0x00000000dc2e9000, 0x0000000100000000)
 Metaspace       used 102593K, capacity 110232K, committed 121000K, reserved 1155072K
  class space    used 12473K, capacity 13482K, committed 15584K, reserved 1048576K {code}
code:

{{}}{{}}

 
{code:java}
sparkSession
.readStream
.format('kafka')
.load
.repartition(4)
...project
.watermark
.groupby(k1, k2)
.agg(size(collect_set('xxx')))
.writeStream 
.foreachBatch(function test)
.start

def test:(Dataset[Row], Long) => Unit = (ds: Dataset[Row], _: Long) => {
      ds.persist(StorageLevel.MEMORY_AND_DISK_SER)
      ds.write
        .option(""collection"", s""col_1"")
        .option(""maxBatchSize"", ""2048"")
        .mode(""append"")
        .mongo()
      ds..write
        .option(""collection"", s""col_2"")
        .option(""maxBatchSize"", ""2048"")
        .mode(""append"")
        .mongo()
      ds.unpersist()
}{code}
 

 

exception log

 
{code:java}

{code}
22/05/09 21:11:28 ERROR streaming.MicroBatchExecution: Query rydts_regist_gp [id = 669c2031-71b2-422b-859d-336722d289e9, runId = 049de32c-e6ff-48f1-8742-bb95122a36ea] terminated with error
java.lang.NullPointerException
    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.$anonfun$isCachedRDDLoaded$1(InMemoryRelation.scala:248)
    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.$anonfun$isCachedRDDLoaded$1$adapted(InMemoryRelation.scala:247)
    at scala.collection.IndexedSeqOptimized.prefixLengthImpl(IndexedSeqOptimized.scala:41)
    at scala.collection.IndexedSeqOptimized.forall(IndexedSeqOptimized.scala:46)
    at scala.collection.IndexedSeqOptimized.forall$(IndexedSeqOptimized.scala:46)
    at scala.collection.mutable.ArrayOps$ofRef.forall(ArrayOps.scala:198)
    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.isCachedRDDLoaded(InMemoryRelation.scala:247)
    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.isCachedColumnBuffersLoaded(InMemoryRelation.scala:241)
    at org.apache.spark.sql.execution.CacheManager.$anonfun$uncacheQuery$8(CacheManager.scala:189)
    at org.apache.spark.sql.execution.CacheManager.$anonfun$uncacheQuery$8$adapted(CacheManager.scala:176)
    at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:304)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)
    at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)
    at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)
    at scala.collection.TraversableLike.filter(TraversableLike.scala:395)
    at scala.collection.TraversableLike.filter$(TraversableLike.scala:395)
    at scala.collection.AbstractTraversable.filter(Traversable.scala:108)
    at org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:219)
    at org.apache.spark.sql.execution.CacheManager.uncacheQuery(CacheManager.scala:176)
    at org.apache.spark.sql.Dataset.unpersist(Dataset.scala:3220)
    at org.apache.spark.sql.Dataset.unpersist(Dataset.scala:3231)
    at common.job.xxx$.$anonfun$main$3(xxx.scala:117)
    at common.job.xxx$.$anonfun$main$3$adapted(xxx.scala:103)
    at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:35)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:600)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
    at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
    at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
    at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:598)
    at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
    at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
    at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:598)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:228)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)
    at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)
    at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:193)
    at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)
    at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:187)
    at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:303)
    at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
    at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
    at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:286)
    at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:209) 

 

 ","i set it a driver and 2 executors executor allocate 2g memory and old generation usage rate about 50%, i think it is health",cxb,neilagupta,ulysses,,,,,259200,259200,,0%,259200,259200,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 10 04:35:35 UTC 2022,,,,,,,,,,"0|z127g8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/May/22 04:35;ulysses;same bug with SPARK-39104;;;",,,,,,,,,,,
UnsupportedOperationException if spark.sql.ui.explainMode is set to cost,SPARK-39112,13443530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,yumwang,yumwang,06/May/22 06:36,11/May/22 06:33,13/Jul/23 08:47,11/May/22 06:33,3.3.0,3.4.0,,,,,,,3.3.0,,,,,SQL,,,,0,,,"How to reproduce this issue:
{noformat}
yumwang@LM-SHC-16508156 spark-3.3.0-bin-hadoop3 % bin/spark-shell
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http://localhost:4040
Spark context available as 'sc' (master = local[*], app id = local-1651818680247).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.3.0
      /_/

Using Scala version 2.12.15 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_251)
Type in expressions to have them evaluated.
Type :help for more information.

scala> spark.sql(""set spark.sql.ui.explainMode=cost"")
res0: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala> spark.sql(""use default"")
java.lang.UnsupportedOperationException
  at org.apache.spark.sql.catalyst.plans.logical.LeafNode.computeStats(LogicalPlan.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.LeafNode.computeStats$(LogicalPlan.scala:171)
  at org.apache.spark.sql.catalyst.analysis.ResolvedDBObjectName.computeStats(v2ResolutionPlans.scala:219)
  at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.default(SizeInBytesOnlyStatsPlanVisitor.scala:55)
  at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.default(SizeInBytesOnlyStatsPlanVisitor.scala:27)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit(LogicalPlanVisitor.scala:48)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlanVisitor.visit$(LogicalPlanVisitor.scala:25)
  at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.SizeInBytesOnlyStatsPlanVisitor$.visit(SizeInBytesOnlyStatsPlanVisitor.scala:27)
  at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.$anonfun$stats$1(LogicalPlanStats.scala:37)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats(LogicalPlanStats.scala:33)
  at org.apache.spark.sql.catalyst.plans.logical.statsEstimation.LogicalPlanStats.stats$(LogicalPlanStats.scala:33)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.stats(LogicalPlan.scala:30)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$stringWithStats$2.applyOrElse(QueryExecution.scala:300)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$stringWithStats$2.applyOrElse(QueryExecution.scala:299)
  at scala.PartialFunction$Lifted.apply(PartialFunction.scala:228)
  at scala.PartialFunction$Lifted.apply(PartialFunction.scala:224)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$collect$1(TreeNode.scala:396)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$collect$1$adapted(TreeNode.scala:396)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:355)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreach$1(TreeNode.scala:356)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreach$1$adapted(TreeNode.scala:356)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:356)
  at org.apache.spark.sql.catalyst.trees.TreeNode.collect(TreeNode.scala:396)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$collectWithSubqueries$1(QueryPlan.scala:504)
  at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
  at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
  at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.collectWithSubqueries(QueryPlan.scala:504)
  at org.apache.spark.sql.execution.QueryExecution.stringWithStats(QueryExecution.scala:299)
  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:244)
  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:215)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:103)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)
  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:93)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:582)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:174)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:582)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:558)
  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:93)
  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:80)
  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:78)
  at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
  at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
  ... 47 elided
{noformat}
",,apachespark,cloud_fan,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 11 06:33:16 UTC 2022,,,,,,,,,,"0|z124co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/22 11:59;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/36488;;;","11/May/22 06:33;cloud_fan;Issue resolved by pull request 36488
[https://github.com/apache/spark/pull/36488];;;",,,,,,,,,,
Silent change in regexp_replace's handling of empty strings,SPARK-39107,13443410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LorenzoMartini94,rshkv,rshkv,05/May/22 15:24,03/Jan/23 15:21,13/Jul/23 08:47,10/May/22 00:46,3.1.2,,,,,,,,3.1.4,3.2.2,3.3.0,,,SQL,,,,0,correctness,release-notes,"Hi, we just upgraded from 3.0.2 to 3.1.2 and noticed a silent behavior change that a) seems incorrect, and b) is undocumented in the [migration guide|https://spark.apache.org/docs/latest/sql-migration-guide.html]:

{code:title=3.0.2}
scala> val df = spark.sql(""SELECT '' AS col"")
df: org.apache.spark.sql.DataFrame = [col: string]

scala> df.withColumn(""replaced"", regexp_replace(col(""col""), ""^$"", ""<empty>"")).show
+---+--------+
|col|replaced|
+---+--------+
|   | <empty>|
+---+--------+
{code}

{code:title=3.1.2}
scala> val df = spark.sql(""SELECT '' AS col"")
df: org.apache.spark.sql.DataFrame = [col: string]

scala> df.withColumn(""replaced"", regexp_replace(col(""col""), ""^$"", ""<empty>"")).show
+---+--------+
|col|replaced|
+---+--------+
|   |        |
+---+--------+
{code}

Note, the regular expression {{^$}} should match the empty string, but doesn't in version 3.1. E.g. this is the Java behavior:

{code}
scala> """".replaceAll(""^$"", ""<empty>"");
res1: String = <empty>
{code}",,apachespark,LorenzoMartini94,rshkv,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 16 15:03:35 UTC 2022,,,,,,,,,,"0|z123mo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/May/22 18:02;apachespark;User 'LorenzoMartini' has created a pull request for this issue:
https://github.com/apache/spark/pull/36457;;;","05/May/22 18:03;apachespark;User 'LorenzoMartini' has created a pull request for this issue:
https://github.com/apache/spark/pull/36457;;;","05/May/22 18:03;LorenzoMartini94;I looked into the code and I believe [https://github.com/apache/spark/pull/36457] should fix this, can we have a review please? Thanks!;;;","10/May/22 00:46;srowen;Issue resolved by pull request 36457
[https://github.com/apache/spark/pull/36457];;;","16/Jun/22 14:48;tgraves;[~srowen]   I think this actually went into 3.1.4,  not 3.1.3, could you confirm before I update Fixed versions? ;;;","16/Jun/22 15:03;srowen;I agree, oops. I'll fix it.
https://github.com/apache/spark/commit/d557a56b956545806019ee7f13f41955d8cb107f;;;",,,,,,
Null Pointer Exeption on unpersist call,SPARK-39104,13443274,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,Goihburg,Goihburg,05/May/22 07:09,24/Nov/22 00:38,13/Jul/23 08:47,17/May/22 23:29,3.2.1,,,,,,,,3.2.2,3.3.0,3.4.0,,,Spark Core,,,,0,,,"DataFrame.unpesist call fails wth NPE

 
{code:java}
java.lang.NullPointerException
    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.isCachedRDDLoaded(InMemoryRelation.scala:247)
    at org.apache.spark.sql.execution.columnar.CachedRDDBuilder.isCachedColumnBuffersLoaded(InMemoryRelation.scala:241)
    at org.apache.spark.sql.execution.CacheManager.$anonfun$uncacheQuery$8(CacheManager.scala:189)
    at org.apache.spark.sql.execution.CacheManager.$anonfun$uncacheQuery$8$adapted(CacheManager.scala:176)
    at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:304)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)
    at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)
    at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)
    at scala.collection.TraversableLike.filter(TraversableLike.scala:395)
    at scala.collection.TraversableLike.filter$(TraversableLike.scala:395)
    at scala.collection.AbstractTraversable.filter(Traversable.scala:108)
    at org.apache.spark.sql.execution.CacheManager.recacheByCondition(CacheManager.scala:219)
    at org.apache.spark.sql.execution.CacheManager.uncacheQuery(CacheManager.scala:176)
    at org.apache.spark.sql.Dataset.unpersist(Dataset.scala:3220)
    at org.apache.spark.sql.Dataset.unpersist(Dataset.scala:3231){code}
Looks like syncronization in required for org.apache.spark.sql.execution.columnar.CachedRDDBuilder#isCachedColumnBuffersLoaded

 
{code:java}
def isCachedColumnBuffersLoaded: Boolean = {
  _cachedColumnBuffers != null && isCachedRDDLoaded
}

def isCachedRDDLoaded: Boolean = {
    _cachedColumnBuffersAreLoaded || {
      val bmMaster = SparkEnv.get.blockManager.master
      val rddLoaded = _cachedColumnBuffers.partitions.forall { partition =>
        bmMaster.getBlockStatus(RDDBlockId(_cachedColumnBuffers.id, partition.index), false)
          .exists { case(_, blockStatus) => blockStatus.isCached }
      }
      if (rddLoaded) {
        _cachedColumnBuffersAreLoaded = rddLoaded
      }
      rddLoaded
  }
} {code}
isCachedRDDLoaded relies on _cachedColumnBuffers != null check while it can be changed concurrently from other thread. ",,apachespark,Goihburg,LuciferYang,neilagupta,ulysses,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 17 23:29:30 UTC 2022,,,,,,,,,,"0|z122sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/May/22 00:06;neilagupta;Hi Denis, do you have reproduction steps? ;;;","10/May/22 04:37;ulysses;it seems this bug also exists at 3.3.0 branch;;;","10/May/22 10:40;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/36496;;;","17/May/22 23:29;srowen;Issue resolved by pull request 36496
[https://github.com/apache/spark/pull/36496];;;",,,,,,,,
Dividing interval by integral can result in codegen compilation error,SPARK-39093,13442997,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,03/May/22 16:13,16/May/22 12:35,13/Jul/23 08:47,04/May/22 09:22,3.2.1,3.3.0,3.4.0,,,,,,3.3.0,,,,,SQL,,,,0,,,"Assume this data:
{noformat}
create or replace temp view v1 as
select * FROM VALUES
(interval '10' months, interval '10' day, 2)
as v1(period, duration, num);

cache table v1;
{noformat}
These two queries work:
{noformat}
spark-sql> select period/num from v1;
0-5
Time taken: 0.143 seconds, Fetched 1 row(s)
{noformat}
{noformat}
spark-sql> select duration/num from v1;
5 00:00:00.000000000
Time taken: 0.094 seconds, Fetched 1 row(s)
{noformat}
However, these two queries get a codegen compilation error:
{noformat}
spark-sql> select period/(num + 3) from v1;
22/05/03 08:56:37 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 44: Expression ""project_value_2"" is not an rvalue
...
22/05/03 08:56:37 WARN UnsafeProjection: Expr codegen error and falling back to interpreter mode
...
0-2
Time taken: 0.149 seconds, Fetched 1 row(s)
{noformat}
{noformat}
spark-sql> select duration/(num + 3) from v1;
22/05/03 08:57:29 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 54: Expression ""project_value_2"" is not an rvalue
...
22/05/03 08:57:29 WARN UnsafeProjection: Expr codegen error and falling back to interpreter mode
...
2 00:00:00.000000000
Time taken: 0.089 seconds, Fetched 1 row(s)
{noformat}
Even the first two queries will get a compilation error if you turn off whole-stage codegen:
{noformat}
spark-sql> set spark.sql.codegen.wholeStage=false;
spark.sql.codegen.wholeStage	false
Time taken: 0.055 seconds, Fetched 1 row(s)
spark-sql> select period/num from v1;
22/05/03 09:16:42 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 37, Column 5: Expression ""value_1"" is not an rvalue
....
0-5
Time taken: 0.175 seconds, Fetched 1 row(s)
spark-sql> select duration/num from v1;
22/05/03 09:17:41 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 37, Column 5: Expression ""value_1"" is not an rvalue
...
5 00:00:00.000000000
Time taken: 0.104 seconds, Fetched 1 row(s)
{noformat}
Note that in the error cases, the queries still return a result because Spark falls back on interpreting the divide expression (so I marked this as ""minor"").",,apachespark,bersprockets,dongjoon,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 08 05:19:10 UTC 2022,,,,,,,,,,"0|z1212w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/May/22 20:39;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36442;;;","03/May/22 20:39;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36442;;;","04/May/22 09:22;Gengliang.Wang;Issue resolved by pull request 36442
[https://github.com/apache/spark/pull/36442];;;","06/May/22 15:41;dongjoon;I updated the fixed version from 3.3.0 to 3.3.1 because this is not in RC1 vote.;;;","08/May/22 05:18;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36481;;;","08/May/22 05:19;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36481;;;",,,,,,
df.rdd.isEmpty() results in unexpected executor failure and JVM crash,SPARK-39084,13442742,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,02/May/22 05:38,12/Dec/22 18:10,13/Jul/23 08:47,02/May/22 23:33,3.2.0,3.2.1,,,,,,,3.1.3,3.2.2,3.3.0,,,PySpark,,,,0,,,"It was discovered that a particular data distribution in a DataFrame with groupBy clause could result in a JVM crash when calling {{{}df.rdd.isEmpty{}}}.

For example,
{code:java}
data = []
for t in range(0, 10000):
    id = str(uuid.uuid4())
    if t == 0:
        for i in range(0, 99):
            data.append((id,))
    elif t < 10:
        for i in range(0, 75):
            data.append((id,))
    elif t < 100:
        for i in range(0, 50):
            data.append((id,))
    elif t < 1000:
        for i in range(0, 25):
            data.append((id,))
    else:
        for i in range(0, 10):
            data.append((id,))

df = self.spark.createDataFrame(data, [""col""])
df.coalesce(1).write.parquet(tmpPath)

res = self.spark.read.parquet(tmpPath).groupBy(""col"").count()
print(res.rdd.isEmpty()) # crashes JVM{code}
Reproducible 100% on this dataset.

The ticket is related to (can be thought of as a follow-up for) https://issues.apache.org/jira/browse/SPARK-33277. We need to patch one more place to make sure Python iterator is in sync with Java iterator and is terminated whenever the task is marked as completed.

Note that all other operations appear to work fine: {{{}count{}}}, {{{}collect{}}}.",,apachespark,ivan.sadikov,,,,,,,,,,,,,,,,,,,,SPARK-33277,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 02 23:33:01 UTC 2022,,,,,,,,,,"0|z11zig:",9223372036854775807,,,,,,,,,,,,,3.3.0,,,,,,,,,,"02/May/22 05:44;ivan.sadikov;I am going to open a PR to fix this shortly.;;;","02/May/22 05:58;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/36425;;;","02/May/22 23:33;gurwls223;Issue resolved by pull request 36425
[https://github.com/apache/spark/pull/36425];;;",,,,,,,,,
Fix FsHistoryProvider race condition between update and clean app data,SPARK-39083,13442741,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tanvu,tanvu,tanvu,02/May/22 05:22,08/May/22 13:10,13/Jul/23 08:47,08/May/22 13:10,3.2.1,,,,,,,,3.2.2,3.3.0,,,,Web UI,,,,0,,,"After SPARK-29043, FsHistoryProvider will list the log info without waiting all `mergeApplicationListing` task finished.

However, the method `cleanAppData` for stale data is not thread safe and it could lead to a scenario when `cleanAppData` delete the entry of 

ApplicationInfoWrapper for an application right after it has been updated by `mergeApplicationListing`.

So there will be cases when the HS Web UI displays `Application not found` for applications whose logs does exist.","CentOS Linux release 7.7.1908

openjdk version ""1.8.0_262""
OpenJDK Runtime Environment (build 1.8.0_262-b10)
OpenJDK 64-Bit Server VM (build 25.262-b10, mixed mode)",apachespark,tanvu,,,,,,,,,,,,,,,SPARK-39082,,,,,SPARK-29043,SPARK-37659,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun May 08 13:10:26 UTC 2022,,,,,,,,,,"0|z11zi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/22 05:55;apachespark;User 'tanvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36424;;;","08/May/22 13:10;srowen;Issue resolved by pull request 36424
[https://github.com/apache/spark/pull/36424];;;",,,,,,,,,,
Catalog name should not contain dot,SPARK-39079,13442634,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,,30/Apr/22 12:03,24/Nov/22 00:38,13/Jul/23 08:47,11/May/22 15:42,3.0.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 11 15:42:57 UTC 2022,,,,,,,,,,"0|z11yug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Apr/22 12:14;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/36418;;;","11/May/22 15:42;cloud_fan;Issue resolved by pull request 36418
[https://github.com/apache/spark/pull/36418];;;",,,,,,,,,,
Incorrect results or NPE when using Inline function against an array of dynamically created structs,SPARK-39061,13442410,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,29/Apr/22 00:24,12/Dec/22 18:10,13/Jul/23 08:47,16/Jun/22 00:40,3.2.1,3.3.0,3.4.0,,,,,,3.2.2,3.3.1,,,,SQL,,,,0,correctness,,"The following query returns incorrect results:
{noformat}
spark-sql> select inline(array(named_struct('a', 1, 'b', 2), null));
1	2
-1	-1
Time taken: 4.053 seconds, Fetched 2 row(s)
spark-sql>
{noformat}
In Hive, the last row is {{NULL, NULL}}:
{noformat}
Beeline version 2.3.9 by Apache Hive
0: jdbc:hive2://localhost:10000> select inline(array(named_struct('a', 1, 'b', 2), null));
+-------+-------+
|   a   |   b   |
+-------+-------+
| 1     | 2     |
| NULL  | NULL  |
+-------+-------+
2 rows selected (1.355 seconds)
0: jdbc:hive2://localhost:10000> 
{noformat}
If the struct has string fields, you get a {{NullPointerException}}:
{noformat}
spark-sql> select inline(array(named_struct('a', '1', 'b', '2'), null));
22/04/28 16:51:54 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.NullPointerException: null
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeWriter.write(UnsafeWriter.java:110) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.generate_doConsume_0$(Unknown Source) ~[?:?]
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
{noformat}

You can work around the issue by casting the null entry of the array:
{noformat}
spark-sql> select inline(array(named_struct('a', 1, 'b', 2), cast(null as struct<a:int, b:int>)));
1	2
NULL	NULL
Time taken: 0.068 seconds, Fetched 2 row(s)
spark-sql>
{noformat}

As far as I can tell, this issue only happens with arrays of structs where the structs are created in an inline table or in a projection.

The fields of the struct are not getting set to {{nullable = true}} when there is no example in the array where the field is set to {{null}}. As a result, {{GenerateUnsafeProjection.createCode}} generates bad code: it has no code to create a row of null columns, so it just creates a row from variables set with default values.",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 16 00:40:08 UTC 2022,,,,,,,,,,"0|z11xh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/22 00:32;bersprockets;Btw, dataframe example:
{noformat}
scala> val df = Seq((1)).toDF.withColumn(""c1"", array(struct(lit(1).alias(""a""), lit(2).alias(""b"")), lit(null)))
df: org.apache.spark.sql.DataFrame = [value: int, c1: array<struct<a:int,b:int>>]

scala> df.selectExpr(""inline(c1)"").collect
res3: Array[org.apache.spark.sql.Row] = Array([1,2], [-1,-1])
{noformat};;;","15/Jun/22 21:48;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36883;;;","16/Jun/22 00:40;gurwls223;Issue resolved by pull request 36883
[https://github.com/apache/spark/pull/36883];;;",,,,,,,,,
Typo in error messages of decimal overflow,SPARK-39060,13442388,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vli-databricks,vli-databricks,vli-databricks,28/Apr/22 19:50,16/May/22 12:37,13/Jul/23 08:47,04/May/22 06:44,3.2.1,,,,,,,,3.0.4,3.1.3,3.2.2,3.3.0,3.4.0,SQL,,,,0,,,"   org.apache.spark.SparkArithmeticException 

   Decimal(expanded,10000000000000000000000000000000000000.1,39,1}) cannot be represented as Decimal(38, 1). If necessary set spark.sql.ansi.enabled to false to bypass this error.
 

As shown in {{decimalArithmeticOperations.sql.out}}

Notice the extra {{}}} before ‘cannot’


 
 
 
 ",,apachespark,dongjoon,maxgekk,vli-databricks,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 06 15:42:46 UTC 2022,,,,,,,,,,"0|z11xc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/22 20:01;apachespark;User 'vli-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36397;;;","28/Apr/22 20:01;apachespark;User 'vli-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36397;;;","04/May/22 06:44;maxgekk;Issue resolved by pull request 36397
[https://github.com/apache/spark/pull/36397];;;","04/May/22 20:46;apachespark;User 'vli-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36450;;;","05/May/22 18:08;apachespark;User 'vli-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36458;;;","05/May/22 18:59;apachespark;User 'vli-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36459;;;","05/May/22 18:59;apachespark;User 'vli-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36459;;;","05/May/22 19:05;apachespark;User 'vli-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36460;;;","05/May/22 19:06;apachespark;User 'vli-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36460;;;","06/May/22 15:42;dongjoon;I updated the fixed version 3.3.0 to 3.3.1 because this is not in RC1.;;;",,
Fix documentation 404 page,SPARK-39055,13442245,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,28/Apr/22 07:59,12/Dec/22 18:11,13/Jul/23 08:47,28/Apr/22 10:58,3.4.0,,,,,,,,3.3.0,,,,,Documentation,,,,0,,,404 page is currently not working,,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 28 10:58:11 UTC 2022,,,,,,,,,,"0|z11wgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/22 08:15;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36392;;;","28/Apr/22 10:58;gurwls223;Issue resolved by pull request 36392
[https://github.com/apache/spark/pull/36392];;;",,,,,,,,,,
Mapping Spark Query ResultSet/Schema to TRowSet/TTableSchema directly,SPARK-39041,13442016,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,27/Apr/22 08:41,13/May/22 02:36,13/Jul/23 08:47,13/May/22 02:36,3.4.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,,,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 13 02:36:48 UTC 2022,,,,,,,,,,"0|z11v28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/22 09:19;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36373;;;","27/Apr/22 09:19;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36373;;;","13/May/22 02:36;Qin Yao;Issue resolved by pull request 36373
[https://github.com/apache/spark/pull/36373];;;",,,,,,,,,
SparkRuntimeException when trying to get non-existent key in a map,SPARK-39015,13441677,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,razajafri,razajafri,26/Apr/22 01:28,12/Dec/22 17:34,13/Jul/23 08:47,27/Apr/22 08:53,3.3.0,,,,,,,,3.3.0,3.4.0,,,,SQL,,,,0,,,"[~maxgekk] submitted a [commit|https://github.com/apache/spark/commit/bc8c264851457d8ef59f5b332c79296651ec5d1e] that tries to convert the key to SQL but that part of the code is blowing up. 


{code:java}
scala> :pa
// Entering paste mode (ctrl-D to finish)

import org.apache.spark.sql.Row
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StringType
import org.apache.spark.sql.types.DataTypes

val arrayStructureData = Seq(
Row(Map(""hair""->""black"", ""eye""->""brown"")),
Row(Map(""hair""->""blond"", ""eye""->""blue"")),
Row(Map()))

val mapType  = DataTypes.createMapType(StringType,StringType)

val arrayStructureSchema = new StructType()
.add(""properties"", mapType)



val mapTypeDF = spark.createDataFrame(
    spark.sparkContext.parallelize(arrayStructureData),arrayStructureSchema)

mapTypeDF.selectExpr(""element_at(properties, 'hair')"").show

// Exiting paste mode, now interpreting.

+----------------------------+
|element_at(properties, hair)|
+----------------------------+
|                       black|
|                       blond|
|                        null|
+----------------------------+

scala>     spark.conf.set(""spark.sql.ansi.enabled"", true)

scala> mapTypeDF.selectExpr(""element_at(properties, 'hair')"").show
22/04/25 18:26:01 ERROR Executor: Exception in task 6.0 in stage 5.0 (TID 23)
org.apache.spark.SparkRuntimeException: The feature is not supported: literal for 'hair' of class org.apache.spark.unsafe.types.UTF8String.
	at org.apache.spark.sql.errors.QueryExecutionErrors$.literalTypeUnsupportedError(QueryExecutionErrors.scala:240) ~[spark-catalyst_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:101) ~[spark-catalyst_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]
	at org.apache.spark.sql.errors.QueryErrorsBase.toSQLValue(QueryErrorsBase.scala:44) ~[spark-catalyst_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]
	at org.apache.spark.sql.errors.QueryErrorsBase.toSQLValue$(QueryErrorsBase.scala:43) ~[spark-catalyst_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]
	at org.apache.spark.sql.errors.QueryExecutionErrors$.toSQLValue(QueryExecutionErrors.scala:69) ~[spark-catalyst_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]

{code}

Seems like it's trying to convert UTF8String to a sql literal",,apachespark,maxgekk,razajafri,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 27 10:20:22 UTC 2022,,,,,,,,,,"0|z11syw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Apr/22 05:25;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36351;;;","26/Apr/22 05:26;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36351;;;","27/Apr/22 08:53;maxgekk;Issue resolved by pull request 36351
[https://github.com/apache/spark/pull/36351];;;","27/Apr/22 10:19;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36375;;;","27/Apr/22 10:20;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36375;;;",,,,,,,
SparkSQL parse partition value does not support all data types,SPARK-39012,13441652,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,amaliujia,amaliujia,amaliujia,25/Apr/22 21:26,16/May/22 12:37,13/Jul/23 08:47,06/May/22 09:39,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"When Spark needs to infer schema, it needs to parse string to a type. Not all data types are supported so far in this path. For example, binary is known to not be supported. If a user uses binary column, and if the user does not use a metastore, then SparkSQL could fall back to schema inference thus fail to execute during table scan. This should be a bug as schema inference is supported but some types are missing.

string might be converted to all types except ARRAY, MAP, STRUCT, etc. Also because when converting from a string, small scale type won't be identified if there is a larger scale type. For example, short and long 

Based on Spark SQL data types: https://spark.apache.org/docs/latest/sql-ref-datatypes.html, we can support the following types:

BINARY
BOOLEAN

And there are two types that I am not sure if SparkSQL is supporting:
YearMonthIntervalType
DayTimeIntervalType
",,amaliujia,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 06 15:44:14 UTC 2022,,,,,,,,,,"0|z11stc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/22 21:29;amaliujia;PR is ready to support binary type https://github.com/apache/spark/pull/36344;;;","25/Apr/22 21:30;apachespark;User 'amaliujia' has created a pull request for this issue:
https://github.com/apache/spark/pull/36344;;;","25/Apr/22 21:31;apachespark;User 'amaliujia' has created a pull request for this issue:
https://github.com/apache/spark/pull/36344;;;","06/May/22 09:39;cloud_fan;Issue resolved by pull request 36344
[https://github.com/apache/spark/pull/36344];;;","06/May/22 15:44;dongjoon;I updated the fixed version 3.3.0 to 3.3.1 because this is not in RC1.;;;",,,,,,,
Add an Python example of StreamingQueryListener,SPARK-38994,13441055,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,22/Apr/22 03:52,12/Dec/22 18:11,13/Jul/23 08:47,22/Apr/22 10:00,3.4.0,,,,,,,,3.4.0,,,,,PySpark,Structured Streaming,,,0,,,SPARK-38759 added the {{StreamingQueryListener}} support in Python. We should also add an example at https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reporting-metrics-programmatically-using-asynchronous-apis,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 22 10:00:27 UTC 2022,,,,,,,,,,"0|z11p6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/22 04:43;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36318;;;","22/Apr/22 10:00;gurwls223;Issue resolved by pull request 36318
[https://github.com/apache/spark/pull/36318];;;",,,,,,,,,,
Avoid using bash -c in ShellBasedGroupsMappingProvider,SPARK-38992,13441050,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,22/Apr/22 03:03,13/Mar/23 14:23,13/Jul/23 08:47,22/Apr/22 10:03,3.0.3,3.1.2,3.2.1,3.3.0,,,,,3.0.4,3.2.2,3.3.0,,,Spark Core,,,,0,,,Using bash -c can allow arbitrary shall execution from the end user.,,apachespark,wypoon,zhangdonglin,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 22 11:41:52 UTC 2022,,,,,,,,,,"0|z11p5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/22 03:13;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36315;;;","22/Apr/22 03:14;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36315;;;","22/Apr/22 10:03;gurwls223;Issue resolved by pull request 36315
[https://github.com/apache/spark/pull/36315];;;","19/Jul/22 17:41;wypoon;Does this vulnerability not affect Spark 2.4?;;;","20/Jul/22 08:43;zhangdonglin; [~wypoon]   spark 2.4 also affected.

but there are ways to avoid it, the config 'spark.acls.enable' is false by default,  it you don't set 'spark.acls.enable=true' or 'spark.ui.acls.enable=true', it'll not trigger;;;","22/Aug/22 11:41;apachespark;User 'leoluan2009' has created a pull request for this issue:
https://github.com/apache/spark/pull/37614;;;",,,,,,
date_trunc and trunc both fail with format from column in inline table,SPARK-38990,13441020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,21/Apr/22 22:27,12/Dec/22 18:10,13/Jul/23 08:47,22/Apr/22 03:42,3.1.3,3.2.1,3.3.0,3.4.0,,,,,3.0.4,3.1.3,3.2.2,3.3.0,,SQL,,,,0,,,"This fails:
{noformat}
create or replace temp view v1 as
select * from values ('week', timestamp'2012-01-01') as data(col1, col2);

select date_trunc(col1, col2) from v1;
{noformat}
It fails with a {{NullPointerException}}:
{noformat}
java.lang.NullPointerException: null
	at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$8(InternalRow.scala:141) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.InternalRow$.$anonfun$getAccessor$8$adapted(InternalRow.scala:141) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.BoundReference.eval(BoundAttribute.scala:40) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.TruncInstant.evalHelper(datetimeExpressions.scala:2117) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.TruncInstant.evalHelper$(datetimeExpressions.scala:2112) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.TruncTimestamp.evalHelper(datetimeExpressions.scala:2277) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.TruncTimestamp.eval(datetimeExpressions.scala:2295) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:157) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(InterpretedMutableProjection.scala:97) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.optimizer.ConvertToLocalRelation$$anonfun$apply$47.$anonfun$applyOrElse$77(Optimizer.scala:1987) ~[spark-catalyst_2.12-3
{noformat}
 However, if you cache {{v1}}, then it works:
{noformat}
spark-sql> cache table v1;
Time taken: 2.086 seconds
spark-sql> select date_trunc(col1, col2) from v1;
2011-12-26 00:00:00
Time taken: 0.172 seconds, Fetched 1 row(s)
spark-sql> 
{noformat}
Both {{date_trunc}} and {{trunc}} exhibit this behavior.",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 22 03:42:35 UTC 2022,,,,,,,,,,"0|z11oyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/22 01:41;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36312;;;","22/Apr/22 03:42;gurwls223;Issue resolved by pull request 36312
[https://github.com/apache/spark/pull/36312];;;",,,,,,,,,,
"Pandas API - ""PerformanceWarning: DataFrame is highly fragmented."" get printed many times. ",SPARK-38988,13440980,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XinrongM,bjornjorgensen,bjornjorgensen,21/Apr/22 17:59,12/Dec/22 18:11,13/Jul/23 08:47,28/Apr/22 00:26,3.3.0,3.4.0,,,,,,,3.3.0,,,,,PySpark,,,,0,,,"I add a file and a notebook with the info msg I get when I run df.info()

Spark master build from 13.04.22.

df.shape
(763300, 224)

",,apachespark,bjornjorgensen,ueshin,XinrongM,,,,,,,,,,,,,,,,,,,,,"21/Apr/22 19:35;bjornjorgensen;Untitled.html;https://issues.apache.org/jira/secure/attachment/13042760/Untitled.html","21/Apr/22 17:59;bjornjorgensen;info.txt;https://issues.apache.org/jira/secure/attachment/13042758/info.txt","25/Apr/22 11:17;bjornjorgensen;warning printed.txt;https://issues.apache.org/jira/secure/attachment/13042883/warning+printed.txt",,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 28 00:26:27 UTC 2022,,,,,,,,,,"0|z11ops:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/22 00:44;gurwls223;cc [~XinrongM]and [~itholic] FYI;;;","25/Apr/22 11:22;bjornjorgensen;I add a new fil ""warning printed.txt"" it show that it depends one the dataframe size. 

So if you have a dataframe 
Int64Index: 34 entries, 0 to 33
Data columns (total 37 columns):

The warning won`t get printed.

If the datafreme is 
Int64Index: 109 entries, 0 to 108
Data columns (total 112 columns):

Then the warning is printed 13 times.



;;;","26/Apr/22 23:55;XinrongM;Thank you for raising that!

I will try muting the warnings for now.;;;","27/Apr/22 17:47;apachespark;User 'xinrong-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/36367;;;","28/Apr/22 00:26;gurwls223;Issue resolved by pull request 36367
[https://github.com/apache/spark/pull/36367];;;",,,,,,,
Fix schema pruning with correlated subqueries,SPARK-38977,13440827,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,aokolnychyi,aokolnychyi,aokolnychyi,21/Apr/22 04:35,22/Apr/22 21:12,13/Jul/23 08:47,22/Apr/22 21:12,3.2.0,3.2.1,3.3.0,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"Schema pruning fails for some queries with correlated subqueries.


{noformat}
sql(
  s""""""SELECT name FROM contacts c
     |WHERE
     | EXISTS (SELECT 1 FROM ids i WHERE i.value = c.id)
     | AND
     | EXISTS (SELECT 1 FROM first_names n WHERE c.name.first = n.value)
     |"""""".stripMargin)
{noformat}

{noformat}
[info]   org.apache.spark.SparkException: Failed to merge fields 'value' and 'value'. Failed to merge incompatible data types int and string
[info]   at org.apache.spark.sql.errors.QueryExecutionErrors$.failedMergingFieldsError(QueryExecutionErrors.scala:936)
{noformat}




",,aokolnychyi,apachespark,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 22 21:12:47 UTC 2022,,,,,,,,,,"0|z11nrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/22 16:43;apachespark;User 'aokolnychyi' has created a pull request for this issue:
https://github.com/apache/spark/pull/36303;;;","22/Apr/22 21:12;viirya;Issue resolved by pull request 36303
[https://github.com/apache/spark/pull/36303];;;",,,,,,,,,,
List functions should only list registered functions in the specified database,SPARK-38974,13440790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,20/Apr/22 23:19,22/Apr/22 03:25,13/Jul/23 08:47,22/Apr/22 03:25,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"Currently, `SHOW FUNCTIONS IN db` will show all external functions in the database and all registered functions regardless of whether they belong to the specified database or not. This should be fixed.",,allisonwang-db,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 22 03:25:10 UTC 2022,,,,,,,,,,"0|z11njk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/22 00:01;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36291;;;","22/Apr/22 03:25;cloud_fan;Issue resolved by pull request 36291
[https://github.com/apache/spark/pull/36291];;;",,,,,,,,,,
"When push-based shuffle is enabled, a stage may not complete when retried",SPARK-38973,13440778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csingh,csingh,csingh,20/Apr/22 21:28,20/Mar/23 09:47,13/Jul/23 08:47,22/Apr/22 19:39,3.2.0,,,,,,,,3.3.0,,,,,Shuffle,Spark Core,,,0,,,"With push-based shuffle enabled and adaptive merge finalization, there are scenarios where a re-attempt of ShuffleMapStage may not complete. 

With Adaptive Merge Finalization, a stage may be triggered for finalization when it is in the below state:
 # The stage is *not* running ({*}not{*} in the _running_ set of the DAGScheduler) - had failed or canceled or waiting, and
 # The stage has no pending partitions (all the tasks completed at-least once).

For such a stage when the finalization completes, the stage will still not be marked as {_}mergeFinalized{_}. 

The stage of the stage will be: 
 * _stage.shuffleDependency.mergeFinalized = false_
 * _stage.shuffleDependency.getFinalizeTask = finalizeTask_
 * Merged statuses of the state are unregistered

 

When the stage is resubmitted, the newer attempt of the stage will never complete even though its tasks may be completed. This is because the newer attempt of the stage will have {_}shuffleMergeEnabled = true{_}, since with the previous attempt the stage was never marked as {_}mergedFinalized{_}, and the _finalizeTask_ is present (from finalization attempt for previous stage attempt).

 

So, when all the tasks of the newer attempt complete, then these conditions will be true:
 * stage will be running
 * There will be no pending partitions since all the tasks completed

 * _stage.shuffleDependency.shuffleMergeEnabled = true_

 * _stage.shuffleDependency.shuffleMergeFinalized = false_

 * _stage.shuffleDependency.getFinalizeTask_ is not empty

This leads the DAGScheduler to try scheduling finalization and not trigger the completion of the Stage. However because of the last condition it never even schedules the finalization and the stage never completes.",,apachespark,csingh,lyee,mridulm80,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 09:47:15 UTC 2023,,,,,,,,,,"0|z11ngw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Apr/22 03:40;apachespark;User 'otterc' has created a pull request for this issue:
https://github.com/apache/spark/pull/36293;;;","22/Apr/22 19:39;mridulm80;Issue resolved by pull request 36293
[https://github.com/apache/spark/pull/36293];;;","20/Mar/23 09:47;lyee;[~csingh] Should this bugfix be merged into 3.2.x branches?;;;",,,,,,,,,
Graceful decomissionning on Kubernetes fails / decom script error,SPARK-38969,13440716,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,holden,yeachan153,yeachan153,20/Apr/22 15:16,12/Aug/22 23:34,13/Jul/23 08:47,12/Aug/22 23:34,3.2.0,,,,,,,,3.4.0,,,,,Spark Core,,,,0,,,"Hello, we are running into some issue while attempting graceful decommissioning of executors. We enabled:
 * spark.decommission.enabled 
 * spark.storage.decommission.rddBlocks.enabled
 * spark.storage.decommission.shuffleBlocks.enabled
 * spark.storage.decommission.enabled

and set spark.storage.decommission.fallbackStorage.path to a path in our bucket.
 
The logs from the driver seems to suggest the decommissioning process started but then unexpectedly exited and failed:
 
```
22/04/20 15:09:09 WARN KubernetesClusterSchedulerBackend$KubernetesDriverEndpoint: Received executor 3 decommissioned message
22/04/20 15:09:09 INFO KubernetesClusterSchedulerBackend: Decommission executors: 3
22/04/20 15:09:09 INFO BlockManagerMasterEndpoint: Mark BlockManagers (BlockManagerId(3, 100.96.1.130, 44789, None)) as being decommissioning.
22/04/20 15:09:10 ERROR TaskSchedulerImpl: Lost executor 3 on 100.96.1.130: Executor decommission.
22/04/20 15:09:10 INFO DAGScheduler: Executor lost: 3 (epoch 2)
22/04/20 15:09:10 INFO ExecutorMonitor: Executor 3 is removed. Remove reason statistics: (gracefully decommissioned: 0, decommision unfinished: 0, driver killed: 0, unexpectedly exited: 3).
22/04/20 15:09:10 INFO BlockManagerMasterEndpoint: Trying to remove executor 3 from BlockManagerMaster.
22/04/20 15:09:10 INFO BlockManagerMasterEndpoint: Removing block manager BlockManagerId(3, 100.96.1.130, 44789, None)
22/04/20 15:09:10 INFO BlockManagerMaster: Removed 3 successfully in removeExecutor
22/04/20 15:09:10 INFO DAGScheduler: Shuffle files lost for executor: 3 (epoch 2)
```
 
However, the executor logs seem to suggest that decommissioning was successful:
 
```
22/04/20 15:09:09 INFO CoarseGrainedExecutorBackend: Decommission executor 3.
22/04/20 15:09:09 INFO CoarseGrainedExecutorBackend: Will exit when finished decommissioning
22/04/20 15:09:09 INFO BlockManager: Starting block manager decommissioning process...
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Starting block migration
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Attempting to migrate all RDD blocks
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Attempting to migrate all shuffle blocks
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Start refreshing migratable shuffle blocks
22/04/20 15:09:10 INFO BlockManagerDecommissioner: 0 of 0 local shuffles are added. In total, 0 shuffles are remained.
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Attempting to migrate all cached RDD blocks
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Starting shuffle block migration thread for BlockManagerId(4, 100.96.1.131, 35607, None)
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Starting shuffle block migration thread for BlockManagerId(fallback, remote, 7337, None)
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Finished current round refreshing migratable shuffle blocks, waiting for 30000ms before the next round refreshing.
22/04/20 15:09:10 WARN BlockManagerDecommissioner: Asked to decommission RDD cache blocks, but no blocks to migrate
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Finished current round RDD blocks migration, waiting for 30000ms before the next round migration.
22/04/20 15:09:10 INFO CoarseGrainedExecutorBackend: Checking to see if we can shutdown.
22/04/20 15:09:10 INFO CoarseGrainedExecutorBackend: No running tasks, checking migrations
22/04/20 15:09:10 INFO CoarseGrainedExecutorBackend: No running tasks, all blocks migrated, stopping.
22/04/20 15:09:10 ERROR CoarseGrainedExecutorBackend: Executor self-exiting due to : Finished decommissioning
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Stop RDD blocks migration().
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Stop refreshing migratable shuffle blocks.
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Stopping migrating shuffle blocks.
22/04/20 15:09:10 INFO CoarseGrainedExecutorBackend: Driver commanded a shutdown
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Stopped block migration
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Stop shuffle block migration().
22/04/20 15:09:10 INFO BlockManagerDecommissioner: Stop shuffle block migration().
22/04/20 15:09:10 INFO MemoryStore: MemoryStore cleared
22/04/20 15:09:10 INFO BlockManager: BlockManager stopped
22/04/20 15:09:10 INFO ShutdownHookManager: Shutdown hook called
```
 
The decommissioning script `/opt/decom.sh` also always terminates with exit code 137, not really sure why that is.
 
 
 ","Running spark-thriftserver (3.2.0) on Kubernetes (GKE 1.20.15-gke.2500). 

 ",apachespark,holden,yeachan153,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 12 23:34:12 UTC 2022,,,,,,,,,,"0|z11n3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/May/22 19:34;holden;Most likely it's from the `timeout 60` in there (at least the exit code of 137). I'll do some poking into reasoning for `unexpectedly exited` too.;;;","02/May/22 20:11;yeachan153;Hi Holden, thanks for responding. Triggering the decom script manually from within a pod still made it exit with code 137, even though the whole execution took much less than 60 seconds.;;;","02/May/22 21:37;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/36434;;;","12/Aug/22 23:34;holden;Updated decommissioning script to be more resilent and block as long as it takes on the executor to exit. K8s will still kill the pod if it exceeds the graceful shutdown time-limit so we don't have to worry too much about blocking forever there.

 

Also updated how we tag executor loss reasons for executors which decommission too ""quickly""

 

See https://github.com/apache/spark/pull/36434/files;;;",,,,,,,,
Fix wrong computeStats at DataSourceV2Relation,SPARK-38962,13440569,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,apachespark,ulysses,ulysses,20/Apr/22 04:05,20/Apr/22 06:09,13/Jul/23 08:47,20/Apr/22 06:09,3.4.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,The interface `SupportsReportStatistics` should be mixed in `Scan` rather than `ScanBuilder`,,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 20 06:09:29 UTC 2022,,,,,,,,,,"0|z11m7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/22 04:11;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/36276;;;","20/Apr/22 04:11;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/36276;;;","20/Apr/22 06:09;cloud_fan;Issue resolved by pull request 36276
[https://github.com/apache/spark/pull/36276];;;",,,,,,,,,
Fix FAILED_EXECUTE_UDF test case on Java 17,SPARK-38956,13440544,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,19/Apr/22 23:04,12/Dec/22 18:10,13/Jul/23 08:47,20/Apr/22 02:04,3.4.0,,,,,,,,3.4.0,,,,,Tests,,,,0,,,,,apachespark,william,,,,,,,,,,,,,,,,,SPARK-38727,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 20 02:04:07 UTC 2022,,,,,,,,,,"0|z11m1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/22 23:07;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36270;;;","19/Apr/22 23:08;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36270;;;","20/Apr/22 02:04;gurwls223;Issue resolved by pull request 36270
[https://github.com/apache/spark/pull/36270];;;",,,,,,,,,
Disable lineSep option in 'from_csv' and 'schema_of_csv',SPARK-38955,13440533,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,revans2,revans2,19/Apr/22 21:55,12/Dec/22 18:10,13/Jul/23 08:47,22/Apr/22 02:44,3.2.0,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,correctness,,"I don't know how critical this is. I was doing some general testing to understand {{from_csv}} and found that if I happen to have a {{lineSep}} in the input data and I noticed that the next row appears to be corrupted. {{multiLine}} does not appear to fix it. Because this is data corruption I am inclined to mark this as CRITICAL or BLOCKER, but it is an odd corner case so I m not going to set it myself.

{code}
Seq[String](""1,\n2,3,4,5"",""6,7,8,9,10"", ""11,12,13,14,15"", null).toDF.select(col(""value""), from_csv(col(""value""), StructType(Seq(StructField(""a"", LongType), StructField(""b"", StringType))), Map[String,String]())).show()
+--------------+---------------+
|         value|from_csv(value)|
+--------------+---------------+
|   1,\n2,3,4,5|      {1, null}|
|    6,7,8,9,10|      {null, 8}|
|11,12,13,14,15|       {11, 12}|
|          null|           null|
+--------------+---------------+
{code}

{code}
Seq[String](""1,:2,3,4,5"",""6,7,8,9,10"", ""11,12,13,14,15"", null).toDF.select(col(""value""), from_csv(col(""value""), StructType(Seq(StructField(""a"", LongType), StructField(""b"", StringType))), Map[String,String](""lineSep"" -> "":""))).show()
+--------------+---------------+
|         value|from_csv(value)|
+--------------+---------------+
|    1,:2,3,4,5|      {1, null}|
|    6,7,8,9,10|      {null, 8}|
|11,12,13,14,15|       {11, 12}|
|          null|           null|
+--------------+---------------+
{code}

{code}
Seq[String](""1,\n2,3,4,5"",""6,7,8,9,10"", ""11,12,13,14,15"", null).toDF.select(col(""value""), from_csv(col(""value""), StructType(Seq(StructField(""a"", LongType), StructField(""b"", StringType))), Map[String,String](""lineSep"" -> "":""))).show()
+--------------+---------------+
|         value|from_csv(value)|
+--------------+---------------+
|   1,\n2,3,4,5|       {1, \n2}|
|    6,7,8,9,10|         {6, 7}|
|11,12,13,14,15|       {11, 12}|
|          null|           null|
+--------------+---------------+
{code}
",,apachespark,bersprockets,revans2,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 22 02:44:31 UTC 2022,,,,,,,,,,"0|z11lzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Apr/22 02:48;gurwls223;lineSep doesn't work with from_csv. We should probably ban this option within this expression.;;;","20/Apr/22 13:14;tgraves;the from_csv docs point to the data source options which contain the lineSep so it seems like we should update documentation and then like you said don't permit it to be specified. since its a corruption seems bad, so marking as blocker to atleast get more visibility and input.;;;","20/Apr/22 13:22;gurwls223;The main problem is that from_csv assumes each record is one record in CSV file while lineSep is whole CSV file wise option. To allow n to m relation, we should make it like a generator expression but it becomes a bit odd as other columns would have to be duplicated. For the same reason, It explicitly unsupports the option like drop-malformed mode at parseMode option.

Yeah so we should throw an exception, and update the docs to explicitly note supported options. ;;;","20/Apr/22 13:30;gurwls223;For extra clarification, from_csv is an expression whereas reading CSV is a (scan) plan. So it becomes tricky when the option has an impact on the number of rows or row-wise transform.

Similar stuffs were actuay already discussed with multiple committers here and there if I am not mistaken. But I'm open to other suggestions.;;;","20/Apr/22 21:39;revans2;Conceptually I am fine if we want to remove all line separators in from_csv. That is what I would expect to happen, and that is what happens with from_json.

{code}
Seq[String](""{'a': 1\n}"",""{'a': \n3, 'b': 'test\n3'}"", ""{'a'\n: 4}"", null).toDF.select(col(""value""), from_json(col(""value""), StructType(Seq(StructField(""a"", LongType), StructField(""b"", StringType))), Map[String,String](""allowUnquotedControlChars"" -> ""true""))).show(truncate=false)
+--------------------------+----------------+
|value                     |from_json(value)|
+--------------------------+----------------+
|{'a': 1\n}                |{1, null}       |
|{'a': \n3, 'b': 'test\n3'}|{3, test\n3}    |
|{'a'\n: 4}                |{4, null}       |
|null                      |null            |
+--------------------------+----------------+
{code}


But there is no way to turn off line separators in the CSV parser.

https://github.com/uniVocity/univocity-parsers/blob/7e7d1b3c0a3dceaed4a8413875eb1500f2a028ec/src/main/java/com/univocity/parsers/common/Format.java#L54-L65

So implementing the proposed fix may be difficult.  Replacing the default separator '\n' with another like '\0' might be okay, but I do know people with '\0' in their data so it is not truly fixing the problem.

An alternative might be to clear the state of the CSV parser after each row of input. i.e. read all of the tokens out of the parser after each row.  The '\n' is still parsed so the output of a single row is still not ideal if it has the line separator in it, but at least it does not corrupt the output of a good row after it.;;;","21/Apr/22 03:08;gurwls223;Oh okay, I got what you mean.

{code}
1,\n2,3,4,5|      {1, null}
{code}

will actually have to be:

{code}
1,\n2,3,4,5|      {1, \n2}
{code};;;","21/Apr/22 03:16;gurwls223;Yeah, I think {{lineSep}} shouldn't respected here. Let me take a quick look.;;;","21/Apr/22 05:16;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36294;;;","22/Apr/22 02:44;gurwls223;Issue resolved by pull request 36294
[https://github.com/apache/spark/pull/36294];;;",,,
Skip RocksDB-based test case in FlatMapGroupsWithStateSuite on Apple Silicon,SPARK-38942,13440296,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,19/Apr/22 06:12,19/Apr/22 06:50,13/Jul/23 08:47,19/Apr/22 06:50,3.3.0,,,,,,,,3.3.0,,,,,SQL,Tests,,,0,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 19 06:50:23 UTC 2022,,,,,,,,,,"0|z11kuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/22 06:18;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36256;;;","19/Apr/22 06:19;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36256;;;","19/Apr/22 06:50;dongjoon;Issue resolved by pull request 36256
[https://github.com/apache/spark/pull/36256];;;",,,,,,,,,
Skip RocksDB-based test case in StreamingJoinSuite on Apple Silicon ,SPARK-38941,13440294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,19/Apr/22 05:31,19/Apr/22 05:58,13/Jul/23 08:47,19/Apr/22 05:57,3.3.0,,,,,,,,3.3.0,,,,,SQL,Tests,,,0,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 19 05:57:58 UTC 2022,,,,,,,,,,"0|z11ku8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Apr/22 05:34;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36254;;;","19/Apr/22 05:57;dongjoon;Issue resolved by pull request 36254
[https://github.com/apache/spark/pull/36254];;;",,,,,,,,,,
RocksDB File manager would not create initial dfs directory with unknown number of keys on 1st empty checkpoint,SPARK-38931,13440139,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yunta,yunta,yunta,18/Apr/22 08:57,21/Apr/22 03:56,13/Jul/23 08:47,19/Apr/22 21:30,3.3.0,,,,,,,,3.2.2,3.3.0,,,,Structured Streaming,,,,0,,,"Currently, we could disable to track the number of keys for performance when using RocksDB state store. However, if the 1st checkpoint is empty, it will not create the root dfs directory, which leads to exception below:


{code:java}
File /private/var/folders/rk/wyr101_562ngn8lp7tbqt7_00000gp/T/spark-ce4a0607-b1d8-43b8-becd-638c6b030019/state/1/1 does not exist
java.io.FileNotFoundException: File /private/var/folders/rk/wyr101_562ngn8lp7tbqt7_00000gp/T/spark-ce4a0607-b1d8-43b8-becd-638c6b030019/state/1/1 does not exist
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:779)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1100)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:769)
	at org.apache.hadoop.fs.DelegateToFileSystem.getFileStatus(DelegateToFileSystem.java:128)
	at org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:93)
	at org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer.<init>(ChecksumFs.java:353)
	at org.apache.hadoop.fs.ChecksumFs.createInternal(ChecksumFs.java:400)
	at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:626)
	at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:701)
	at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:697)
	at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)
	at org.apache.hadoop.fs.FileContext.create(FileContext.java:703)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:327)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:140)
	at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.<init>(CheckpointFileManager.scala:143)
	at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:333)
	at org.apache.spark.sql.execution.streaming.state.RocksDBFileManager.zipToDfsFile(RocksDBFileManager.scala:438)
	at org.apache.spark.sql.execution.streaming.state.RocksDBFileManager.saveCheckpointToDfs(RocksDBFileManager.scala:174)
	at org.apache.spark.sql.execution.streaming.state.RocksDBSuite.saveCheckpointFiles(RocksDBSuite.scala:566)
	at org.apache.spark.sql.execution.streaming.state.RocksDBSuite.$anonfun$new$35(RocksDBSuite.scala:179)
	at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:190)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:203)
	at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:64)
	at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
	at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
	at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:64)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:233)
	at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:233)
	at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:232)
	at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563)
	at org.scalatest.Suite.run(Suite.scala:1112)
	at org.scalatest.Suite.run$(Suite.scala:1094)
	at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1563)
	at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:237)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
	at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:237)
	at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:236)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:64)
	at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
	at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
	at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:64)
	at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1320)
	at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1314)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1314)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:993)
	at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:971)
	at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1480)
	at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:971)
	at org.scalatest.tools.Runner$.run(Runner.scala:798)
	at org.scalatest.tools.Runner.run(Runner.scala)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2or3(ScalaTestRunner.java:38)
	at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:25)
{code}
",,apachespark,kabhwan,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 21 03:40:49 UTC 2022,,,,,,,,,,"0|z11jx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/22 09:08;apachespark;User 'Myasuka' has created a pull request for this issue:
https://github.com/apache/spark/pull/36242;;;","18/Apr/22 09:09;apachespark;User 'Myasuka' has created a pull request for this issue:
https://github.com/apache/spark/pull/36242;;;","19/Apr/22 21:30;kabhwan;Issue resolved via [https://github.com/apache/spark/pull/36242]

 ;;;","21/Apr/22 03:40;kabhwan;(Leaving the history; SPARK-37224 will be introduced in Spark 3.3.0 which is not yet released - I fixed the affected version. The fix doesn't harm to 3.2 version line indeed - I'll leave the commit as it is.);;;",,,,,,,,
TaskLocation.apply throw NullPointerException,SPARK-38922,13439952,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,16/Apr/22 13:19,20/Apr/22 06:44,13/Jul/23 08:47,20/Apr/22 06:44,3.2.1,3.3.0,3.4.0,,,,,,3.0.4,3.1.3,3.2.2,3.3.0,,Spark Core,,,,0,,," 
{code:java}
//Caused by: java.lang.NullPointerException
    at scala.collection.immutable.StringLike$class.stripPrefix(StringLike.scala:155)
    at scala.collection.immutable.StringOps.stripPrefix(StringOps.scala:29)
    at org.apache.spark.scheduler.TaskLocation$.apply(TaskLocation.scala:71)
    at org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$getPreferredLocsInternal {code}",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 20 06:44:02 UTC 2022,,,,,,,,,,"0|z11irs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/22 13:38;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36222;;;","16/Apr/22 13:39;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36222;;;","20/Apr/22 06:44;Qin Yao;fixed by 
[https://github.com/apache/spark/pull/36222]
 ;;;",,,,,,,,,
Nested column pruning should filter out attributes that do not belong to the current relation,SPARK-38918,13439891,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,15/Apr/22 20:47,06/Dec/22 21:46,13/Jul/23 08:47,27/Apr/22 06:50,3.3.0,,,,,,,,3.0.4,3.1.3,3.3.0,3.4.0,,SQL,,,,0,,,"`SchemaPruning` currently does not check if the root field of a nested column belongs to the current relation. This can happen when the filter contains correlated subqueries, where the children field can contain attributes from both the inner and the outer query.",,allisonwang-db,apachespark,wypoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 06 21:46:42 UTC 2022,,,,,,,,,,"0|z11ie8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/22 00:42;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36216;;;","16/Apr/22 00:43;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36216;;;","27/Apr/22 22:50;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36386;;;","27/Apr/22 22:51;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36387;;;","27/Apr/22 22:51;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36386;;;","27/Apr/22 22:52;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36387;;;","27/Apr/22 22:53;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36388;;;","27/Apr/22 22:54;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36388;;;","06/Dec/22 21:46;wypoon;It seems that this is fixed in 3.2.2 ([7c0b9e6e|https://github.com/apache/spark/commit/7c0b9e6e6f680db45c1e2602b85753d9b521bb58]), but for some reason, 3.2.2 is not in the Fixed Version/s. Can we please correct this?
Probably because of this, this issue does not appear in https://spark.apache.org/releases/spark-release-3-2-2.html.;;;",,,
Tasks not killed caused by race conditions between killTask() and launchTask(),SPARK-38916,13439861,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,maryannxue,maryannxue,maryannxue,15/Apr/22 16:33,21/Jul/22 09:08,13/Jul/23 08:47,21/Apr/22 08:33,3.2.1,,,,,,,,3.3.0,,,,,Spark Core,,,,0,,,"Sometimes when the scheduler tries to cancel a task right after it launches that task on the executor, the KillTask and LaunchTask events can come in a reversed order, causing the task to escape the kill-task signal and finish ""secretly"". And those tasks even show as an un-launched task in Spark UI.",,apachespark,maryannxue,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 18 02:59:49 UTC 2022,,,,,,,,,,"0|z11i7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/22 19:02;srowen;OK, how would we resolve this though?;;;","16/Apr/22 19:18;maryannxue;With a PR, I suppose?

Will push it out soon, but its weekend now :)


;;;","18/Apr/22 02:59;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/36238;;;",,,,,,,,,
Upgrade ORC to 1.6.14,SPARK-38905,13439714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Apr/22 19:32,15/Apr/22 00:41,13/Jul/23 08:47,15/Apr/22 00:41,3.2.1,,,,,,,,3.2.2,,,,,Build,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 15 00:41:28 UTC 2022,,,,,,,,,,"0|z11hb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/22 19:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36204;;;","14/Apr/22 19:34;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36204;;;","15/Apr/22 00:41;dongjoon;Issue resolved by pull request 36204
[https://github.com/apache/spark/pull/36204];;;",,,,,,,,,
Failed to build python docker images due to .cache not found,SPARK-38898,13439622,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,14/Apr/22 09:40,12/Dec/22 18:10,13/Jul/23 08:47,14/Apr/22 23:36,3.4.0,,,,,,,,3.4.0,,,,,Kubernetes,,,,0,,,"rm: cannot remove '/root/.cache': No such file or directory

Related:

[https://github.com/volcano-sh/volcano/runs/6020604500?check_suite_focus=true#step:10:2381]",,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 14 23:36:55 UTC 2022,,,,,,,,,,"0|z11gqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/22 09:49;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36198;;;","14/Apr/22 09:49;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36198;;;","14/Apr/22 23:36;gurwls223;Issue resolved by pull request 36198
[https://github.com/apache/spark/pull/36198];;;",,,,,,,,,
Invalid column name while querying bit type column in MSSQL,SPARK-38889,13439515,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,13/Apr/22 17:58,12/Dec/22 18:10,13/Jul/23 08:47,14/Apr/22 04:11,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"After https://issues.apache.org/jira/browse/SPARK-36644 boolean column filters can be pushed to data sources. However, MSSQL only accepts bit type columns, and the current JDBC dialect for MSSQL does not compile the boolean type values in the pushed predicates into the bit type.",,allisonwang-db,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 14 04:11:28 UTC 2022,,,,,,,,,,"0|z11g2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/22 18:37;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/36182;;;","14/Apr/22 04:11;gurwls223;Issue resolved by pull request 36182
[https://github.com/apache/spark/pull/36182];;;",,,,,,,,,,
The usage logger attachment logic should handle static methods properly.,SPARK-38882,13439294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ueshin,ueshin,12/Apr/22 21:55,12/Dec/22 18:10,13/Jul/23 08:47,13/Apr/22 00:22,3.2.1,3.3.0,,,,,,,3.3.0,,,,,PySpark,,,,0,,,"The usage logger attachment logic has an issue when handling static methods.

For example,

{code}
$ PYSPARK_PANDAS_USAGE_LOGGER=pyspark.pandas.usage_logging.usage_logger ./bin/pyspark
{code}

{code:python}
>>> import pyspark.pandas as ps
>>> psdf = ps.DataFrame({""a"": [1,2,3], ""b"": [4,5,6]})
>>> psdf.from_records([(1, 2), (3, 4)])
A function `DataFrame.from_records(data, index, exclude, columns, coerce_float, nrows)` was failed after 2007.430 ms: 0
Traceback (most recent call last):
...
{code}

without usage logger:

{code:python}
>>> import pyspark.pandas as ps
>>> psdf = ps.DataFrame({""a"": [1,2,3], ""b"": [4,5,6]})
>>> psdf.from_records([(1, 2), (3, 4)])
   0  1
0  1  2
1  3  4
{code}",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 13 00:22:59 UTC 2022,,,,,,,,,,"0|z11eps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/22 22:06;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36167;;;","13/Apr/22 00:22;gurwls223;Fixed in https://github.com/apache/spark/pull/36167;;;",,,,,,,,,,
"SparkSession.builder returns a new builder in Scala, but not in Python",SPARK-38870,13439150,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fpin,fpin,fpin,12/Apr/22 09:53,12/Dec/22 18:11,13/Jul/23 08:47,28/Apr/22 10:56,3.2.1,,,,,,,,3.4.0,,,,,PySpark,SQL,,,0,,,"In pyspark, _SparkSession.builder_ always returns the same static builder, while the expected behaviour should be the same as in Scala, where it returns a new builder each time.

*How to reproduce*

When we run the following code in Scala :
{code:java}
import org.apache.spark.sql.SparkSession

val s1 = SparkSession.builder.master(""local[2]"").config(""key"", ""value"").getOrCreate()
println(""A : "" + s1.conf.get(""key"")) // value
s1.conf.set(""key"", ""new_value"")
println(""B : "" + s1.conf.get(""key"")) // new_value

val s2 = SparkSession.builder.getOrCreate()
println(""C : "" + s1.conf.get(""key"")) // new_value{code}
The output is :
{code:java}
A : value
B : new_value
C : new_value   <<<<<<<<<<<{code}
 

But when we run the following (supposedly equivalent) code in Python:
{code:java}
from pyspark.sql import SparkSession

s1 = SparkSession.builder.master(""local[2]"").config(""key"", ""value"").getOrCreate()
print(""A : "" + s1.conf.get(""key""))
s1.conf.set(""key"", ""new_value"")
print(""B : "" + s1.conf.get(""key""))

s2 = SparkSession.builder.getOrCreate()
print(""C : "" + s1.conf.get(""key"")){code}
The output is : 
{code:java}
A : value
B : new_value
C : value  <<<<<<<<<<<
{code}
 

 

*Root cause analysis*

This comes from the fact that _SparkSession.builder_ behaves differently in Python than in Scala. In Scala, it returns a *new builder* each time, in Python it returns *the same builder* every time, and the SparkSession.Builder._options are static, too.

Because of this, whenever _SparkSession.builder.getOrCreate()_ is called, the options passed to the very first builder are re-applied every time, and overrides the option that were set afterwards. 
This leads to very awkward behavior in every Spark version up to 3.2.1 included

{*}Example{*}:

This example crashes, but was fixed by SPARK-37638

 
{code:java}
from pyspark.sql import SparkSession

spark = SparkSession.builder.config(""spark.sql.sources.partitionOverwriteMode"", ""DYNAMIC"").getOrCreate()

assert spark.conf.get(""spark.sql.sources.partitionOverwriteMode"") == ""DYNAMIC"" # OK

spark.conf.set(""spark.sql.sources.partitionOverwriteMode"", ""STATIC"")
assert spark.conf.get(""spark.sql.sources.partitionOverwriteMode"") == ""STATIC"" # OK

from pyspark.sql import functions as f
from pyspark.sql.types import StringType
f.col(""a"").cast(StringType()) 

assert spark.conf.get(""spark.sql.sources.partitionOverwriteMode"") == ""STATIC"" 
# This fails in all versions until the SPARK-37638 fix
# because before that fix, Column.cast() calle SparkSession.builder.getOrCreate(){code}
 

But this example still crashes in the current version on the master branch
{code:java}
from pyspark.sql import SparkSession

spark = SparkSession.builder.config(""spark.sql.sources.partitionOverwriteMode"", ""DYNAMIC"").getOrCreate()

assert spark.conf.get(""spark.sql.sources.partitionOverwriteMode"") == ""DYNAMIC"" # OK

spark.conf.set(""spark.sql.sources.partitionOverwriteMode"", ""STATIC"")
assert spark.conf.get(""spark.sql.sources.partitionOverwriteMode"") == ""STATIC"" # OK

SparkSession.builder.getOrCreate() 

assert spark.conf.get(""spark.sql.sources.partitionOverwriteMode"") == ""STATIC"" 
# This assert fails in master{code}
 

I made a Pull Request to fix this bug : https://github.com/apache/spark/pull/36161",,apachespark,fpin,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 28 10:56:41 UTC 2022,,,,,,,,,,"0|z11dts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/22 10:25;apachespark;User 'FurcyPin' has created a pull request for this issue:
https://github.com/apache/spark/pull/36161;;;","28/Apr/22 10:56;gurwls223;Issue resolved by pull request 36161
[https://github.com/apache/spark/pull/36161];;;",,,,,,,,,,
`assert_true` fails unconditionnaly after `left_outer` joins,SPARK-38868,13439132,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,StreakyCobra,StreakyCobra,12/Apr/22 08:27,27/Apr/22 07:44,13/Jul/23 08:47,27/Apr/22 07:44,3.1.1,3.1.2,3.2.0,3.2.1,3.3.0,3.4.0,,,3.1.3,3.2.2,3.3.0,,,PySpark,SQL,,,0,,,"When `assert_true` is used after a `left_outer` join the assert exception is raised even though all the rows meet the condition. Using an `inner` join does not expose this issue.

 
{code:java}
from pyspark.sql import SparkSession
from pyspark.sql import functions as sf

session = SparkSession.builder.getOrCreate()

entries = session.createDataFrame(
    [
        (""a"", 1),
        (""b"", 2),
        (""c"", 3),
    ],
    [""id"", ""outcome_id""],
)

outcomes = session.createDataFrame(
    [
        (1, 12),
        (2, 34),
        (3, 32),
    ],
    [""outcome_id"", ""outcome_value""],
)

# Inner join works as expected
(
    entries.join(outcomes, on=""outcome_id"", how=""inner"")
    .withColumn(""valid"", sf.assert_true(sf.col(""outcome_value"") > 10))
    .filter(sf.col(""valid"").isNull())
    .show()
)

# Left join fails with «'('outcome_value > 10)' is not true!» even though it is the case
(
    entries.join(outcomes, on=""outcome_id"", how=""left_outer"")
    .withColumn(""valid"", sf.assert_true(sf.col(""outcome_value"") > 10))
    .filter(sf.col(""valid"").isNull())
    .show()
){code}
Reproduced on `pyspark` versions: `3.2.1`, `3.2.0`, `3.1.2` and `3.1.1`. I am not sure if ""native"" Spark exposes this issue as well or not, I don't have the knowledge/setup to test that.",,apachespark,bersprockets,StreakyCobra,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 25 15:46:52 UTC 2022,,,,,,,,,,"0|z11dps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/22 19:11;srowen;That does seem like a legit bug; I don't know how to debug or address it myself. The results of the two queries are identical.;;;","16/Apr/22 19:52;bersprockets;The issue is in {{EliminateOuterJoin}}. That rule wants to see if it can convert the outer join to a inner join based on the where clause, which in this case is
{noformat}
where assert_true(outcome_value > 10) is null
{noformat}
So the rule evaluates that expression, forcing {{outcome_value}} to be {{null}} to see if the result of the expression is {{null}} or {{false}}. That rule doesn't ever expect the result to be an {{RuntimeException}}, which it is in this case.

I think the easy fix is to add this to {{EliminateOuterJoin#canFilterOutNull}}
{noformat}
if (boundE.exists(_.isInstanceOf[RaiseError])) return false
{noformat};;;","17/Apr/22 00:40;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36230;;;","25/Apr/22 15:46;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36341;;;",,,,,,,,
Update ORC to 1.7.4,SPARK-38866,13439064,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,12/Apr/22 01:49,16/Apr/22 07:32,13/Jul/23 08:47,16/Apr/22 07:32,3.3.0,,,,,,,,3.3.0,,,,,Build,,,,0,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 16 07:32:11 UTC 2022,,,,,,,,,,"0|z11db4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Apr/22 01:53;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36153;;;","16/Apr/22 07:32;dongjoon;Issue resolved by pull request 36153
[https://github.com/apache/spark/pull/36153];;;",,,,,,,,,,
Fix a rejectedExecutionException error when push-based shuffle is enabled,SPARK-38856,13438902,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weixiuli,weixiuli,weixiuli,11/Apr/22 08:59,19/Apr/22 01:51,13/Jul/23 08:47,18/Apr/22 20:03,3.2.0,3.2.1,,,,,,,,,,,,Shuffle,,,,0,,,"When enabled push-based shuffle in our production, there will be a rejectedExecutionException error, this is because that the shuffle pusher pool has been shutdowned before using it.

This is the rejectedExecutionException error :

{{FetchFailed(BlockManagerId(26,xxxxx.hadoop.jd.local, 7337, None), shuffleId=0, mapIndex=6424, mapId=4177, reduceId=1031, message=
org.apache.spark.shuffle.FetchFailedException
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:1181)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:919)
	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:81)
	at org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:29)
	at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)
	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.agg_doAggregateWithKeys_0$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$2.hasNext(WholeStageCodegenExec.scala:815)
	at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:133)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.RejectedExecutionException: Task org.apache.spark.shuffle.ShuffleBlockPusher$$anon$2$$Lambda$1045/583658475@3492bd6f rejected from java.util.concurrent.ThreadPoolExecutor@2e63bad5[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 243134]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2063)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:830)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1379)
	at org.apache.spark.shuffle.ShuffleBlockPusher.submitTask(ShuffleBlockPusher.scala:147)
	at org.apache.spark.shuffle.ShuffleBlockPusher$$anon$2.handleResult(ShuffleBlockPusher.scala:235)
	at org.apache.spark.shuffle.ShuffleBlockPusher$$anon$2.onBlockPushSuccess(ShuffleBlockPusher.scala:245)
	at org.apache.spark.network.shuffle.BlockPushingListener.onBlockTransferSuccess(BlockPushingListener.java:42)
	at org.apache.spark.shuffle.ShuffleBlockPusher$$anon$2.onBlockTransferSuccess(ShuffleBlockPusher.scala:224)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.handleBlockTransferSuccess(RetryingBlockTransferor.java:258)
	at org.apache.spark.network.shuffle.RetryingBlockTransferor$RetryingBlockTransferListener.onBlockPushSuccess(RetryingBlockTransferor.java:304)
	at org.apache.spark.network.shuffle.OneForOneBlockPusher$BlockPushCallback.onSuccess(OneForOneBlockPusher.java:97)
	at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:197)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:142)
	at org.apache.spark.network.server.TransportChannelHandler.channelRead0(TransportChannelHandler.java:53)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:102)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:655)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:581)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	... 1 more

)}}",,apachespark,weixiuli,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 19 01:51:01 UTC 2022,,,,,,,,,,"0|z11cbc:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,,"11/Apr/22 09:23;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/36141;;;","16/Apr/22 19:14;srowen;[~weixiuli] There's no detail here, come on :);;;","19/Apr/22 01:51;weixiuli;OK, done.  [~srowen] ;;;",,,,,,,,,
Teradata's Number is either converted to its floor value or ceiling value despite its fractional part.,SPARK-38846,13438787,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eugeneple,eugeneple,eugeneple,10/Apr/22 13:33,12/Dec/22 18:10,13/Jul/23 08:47,20/Jun/22 23:11,2.3.0,3.2.1,,,,,,,3.4.0,,,,,SQL,,,,0,,,"I'm trying to load data from Teradata, the code using is:
    

    sparkSession.read
          .format(""jdbc"")
          .options(
            Map(
              ""url"" -> ""jdbc:teradata://hostname, user=$username, password=$password"",
              ""MAYBENULL"" -> ""ON"",
              ""SIP_SUPPORT"" -> ""ON"",
              ""driver"" -> ""com.teradata.jdbc.TeraDriver"",
              ""dbtable"" -> $table_name
            )
          )
          .load()

However, some data lost its fractional part after loading. To be more concise, the column in Teradata is in the type of [Number][1] and after loading, the data type in Spark is `DecimalType(38,0)`, the scale value is 0 which means no digits after decimal point. 

Data in Teradata is something like,

    id column1 column2
    1   50.23    100.23
    2   25.8     20.669
    3   30.2     19.23

The `dataframe` of Spark is like,

    id column1 column2
    1   50     100
    2   26     21
    3   30     19

The meta data of the table in Teradata is like:

    CREATE SET TABLE table_name (id BIGINT, column1 NUMBER, column2 NUMBER) PRIMARY INDEX (id);

The Spark version is 2.3.0/3.2.1 and Teradata is 16.20.32.59. 

 ","Spark2.3.0/Spark3.2.1 on Yarn

Teradata 16.20.32.59",apachespark,eugeneple,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,Mon Jun 20 23:11:36 UTC 2022,,,,,,,,,,"0|z11bm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/22 00:11;gurwls223;Spark 2.x is EOL. Mind trying Spark 3+ out?;;;","11/Apr/22 04:36;eugeneple;[~hyukjin.kwon]  Thanks, just tried with latest Spark version (Spark 3.2.1) , the issue is still there.;;;","16/Apr/22 19:22;srowen;Do you know what JDBC type the teradata driver returns for this? it looks like ""DECIMAL"" but Spark is going by what the JDBC driver says;;;","10/May/22 16:15;apachespark;User 'Eugene-Mark' has created a pull request for this issue:
https://github.com/apache/spark/pull/36499;;;","10/May/22 16:20;eugeneple;The JDBC type read from Teradata Numeric is `java.math.BigDecimal`. I thought we might need to implement `getCatalystType` method in TeradataDialect to correctly handle Numeric type. I did some research and managed to keep the fractional part by add getCatalystType method in TeradactDialect. I have submitted a PR-36499 related to the issue for your reference.;;;","20/Jun/22 23:11;srowen;Issue resolved by pull request 36499
[https://github.com/apache/spark/pull/36499];;;",,,,,,
Warn on corrupted block messages,SPARK-38830,13438561,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,08/Apr/22 10:20,11/Apr/22 00:45,13/Jul/23 08:47,11/Apr/22 00:45,3.2.1,3.3.0,,,,,,,3.2.2,3.3.0,,,,Spark Core,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 11 00:45:17 UTC 2022,,,,,,,,,,"0|z11a8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/22 10:26;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36116;;;","08/Apr/22 10:26;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36116;;;","11/Apr/22 00:45;dongjoon;Issue resolved by pull request 36116
[https://github.com/apache/spark/pull/36116];;;",,,,,,,,,
Incorrect result of dataset reduceGroups in java,SPARK-38823,13438444,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,ikozar22,ikozar22,07/Apr/22 17:50,12/Dec/22 18:10,13/Jul/23 08:47,14/Apr/22 23:38,3.3.0,3.4.0,,,,,,,3.3.0,,,,,Java API,,,,0,correctness,,"{code:java}
  @Data
  @NoArgsConstructor
  @AllArgsConstructor
  public static class Item implements Serializable {
    private String x;
    private String y;
    private int z;

    public Item addZ(int z) {
      return new Item(x, y, this.z + z);
    }
  } {code}
{code:java}
List<Item> items = List.of(
 new Item(""X1"", ""Y1"", 1),
 new Item(""X2"", ""Y1"", 1),
 new Item(""X1"", ""Y1"", 1),
 new Item(""X2"", ""Y1"", 1),
 new Item(""X3"", ""Y1"", 1),
 new Item(""X1"", ""Y1"", 1),
 new Item(""X1"", ""Y2"", 1),
 new Item(""X2"", ""Y1"", 1)); 

Dataset<Item> ds = spark.createDataFrame(items, Item.class).as(Encoders.bean(Item.class)); 
ds.groupByKey((MapFunction<Item, Tuple2<String, String>>) item -> Tuple2.apply(item.getX(), item.getY()),
    Encoders.tuple(Encoders.STRING(), Encoders.STRING())) 
.reduceGroups((ReduceFunction<Item>) (item1, item2) -> 
  item1.addZ(item2.getZ()))
 .show(10);
{code}
result is
{noformat}
+--------+----------------------------------------------+
|     key|ReduceAggregator(poc.job.JavaSparkReduce$Item)|
+--------+----------------------------------------------+
|{X1, Y1}|                                   {X2, Y1, 2}|-- expected 3
|{X2, Y1}|                                   {X2, Y1, 2}|-- expected 3
|{X1, Y2}|                                   {X2, Y1, 1}|
|{X3, Y1}|                                   {X2, Y1, 1}|
+--------+----------------------------------------------+{noformat}
pay attention that key doesn't mach with value",,apachespark,bersprockets,ikozar22,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 14 23:38:46 UTC 2022,,,,,,,,,,"0|z119ig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/22 01:57;bersprockets;This appears to be an optimization bug that results in corruption of the buffers in {{AggregationIterator}}.

On master and 3.3, {{NewInstance}} with no arguments is considered foldable. As a result, the {{ConstantFolding}} rule turns NewInstance into a Literal holding an instance of the user's specified Java bean. The instance becomes a singleton that gets reused for each input record (although its fields get updated by {{InitializeJavaBean}}).

Because the instance gets reused, sometimes multiple buffers in {{AggregationIterator}} are actually referring to the same Java bean instance.

Take, for example, the test I added [here|https://github.com/bersprockets/spark/blob/17a8ad64f5bc39cb26d25b63f3692e7b8632baf8/sql/core/src/test/java/test/org/apache/spark/sql/JavaBeanDeserializationSuite.java#L560].

The input is:
{noformat}
List<Item> items = Arrays.asList(
    new Item(""a"", 1),
    new Item(""b"", 3),
    new Item(""c"", 2),
    new Item(""a"", 7));
{noformat}
As {{ObjectAggregationIterator}} reads the input, the buffers get set up as follows (note that the first field of Item should be the same as the key):
{noformat}
- Read Item(""a"", 1)

- Buffers are now:
  Key ""a"" --> Item(""a"", 1)

- Read Item(""b"", 3)

- Buffers are now:
  Key ""a"" -> Item(""b"", 3)
  Key ""b"" -> Item(""b"", 3)
{noformat}
The buffer for key ""a"" now contains Item(""b"", 3). That's because both buffers contain a reference to the same Item instance, and that Item instance's fields were updated when {{Item(""b"", 3)}} was read.

When {{AggregateIterator}} finally calls the test's reduce function, it will pass the same Item instance ({{Item(""a"", 7)}}) as both the buffer and the input record. At that point, the buffers for ""a"", ""b"", and ""c"" will all contain {{Item(""a"", 7)}}.

I _think_ the fix for this is to make {{NewInstance}} non-foldable. My linked test passes with that change (and fails without it). I will run the unit tests and hopefully make a PR tomorrow, assuming the proposed fix doesn't break something else besides {{ConstantFoldingSuite}}.
;;;","13/Apr/22 19:17;bersprockets;By the way, here is some code that demos the issue in spark-shell:
{noformat}
// repro in scala using Java APIs
import org.apache.spark.api.java.function.{MapFunction, ReduceFunction}
import org.apache.spark.sql.Encoders;
import collection.JavaConverters._

class Item (var k: String, var v: Int) extends java.io.Serializable {  
  def setK(value: String): Unit = {
    k = value
  }
  def setV(value: Int): Unit = {
    v = value
  }
  def getK: String = {
    k
  }
  def getV: Int = {
    v
  }
  def this() {
    this("""", 0)
  }

  def addValue(inc: Int): Item = {
    new Item(k, v + inc)
  }

  override def toString: String = {
    s""Item($k,$v)""
  }
}

val items = Seq(
  new Item(""a"", 1),
  new Item(""b"", 3),
  new Item(""c"", 2),
  new Item(""a"", 7)
)

val ds = spark.createDataFrame(items.asJava, classOf[Item]).as(Encoders.bean(classOf[Item])).coalesce(1)

val mf = new MapFunction[Item, String] {
  override def call(item: Item): String = {
    println(s""Key is ${item.k} for item $item"")
    item.k
  }
}

val kvgd1 = ds.groupByKey(mf, Encoders.STRING)

val rf = new ReduceFunction[Item] {
  override def call(item1: Item, item2: Item): Item = {
    val sameRef = item1 eq item2
    val msg = s""item1 $item1; item2 $item2""
    val newItem = item1.addValue(item2.v)
    println(s""$msg; new item is $newItem; sameRef is $sameRef"")
    newItem
  }
}
 
kvgd1.reduceGroups(rf).show(10)
{noformat}
This will return
{noformat}
+---+----------------------------------------------------------------------------+
|key|ReduceAggregator($line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Item)|
+---+----------------------------------------------------------------------------+
|  a|                                                                      {a, 7}|
|  b|                                                                      {a, 7}|
|  c|                                                                      {a, 7}|
+---+----------------------------------------------------------------------------+
{noformat}
However, it should return
{noformat}
+---+----------------------------------------------------------------------------+
|key|ReduceAggregator($line20.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$Item)|
+---+----------------------------------------------------------------------------+
|  a|                                                                      {a, 8}|
|  b|                                                                      {b, 3}|
|  c|                                                                      {c, 2}|
+---+----------------------------------------------------------------------------+
{noformat};;;","13/Apr/22 19:40;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36183;;;","13/Apr/22 19:41;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36183;;;","14/Apr/22 23:38;gurwls223;Issue resolved by pull request 36183
[https://github.com/apache/spark/pull/36183];;;",,,,,,,
Fix the docs of try_multiply/try_subtract/ANSI cast,SPARK-38818,13438397,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,07/Apr/22 12:52,07/Apr/22 17:10,13/Jul/23 08:47,07/Apr/22 17:10,3.3.0,,,,,,,,3.3.0,,,,,Documentation,,,,0,,,"* Fix the valid combinations of ANSI CAST. 
 * Fix the usage of try_multiply/try_subtract, from ""expr1 _FUNC_ expr2"" to ""_FUNC_(expr1<  expr2)""",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 07 17:10:44 UTC 2022,,,,,,,,,,"0|z11980:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/22 13:12;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36099;;;","07/Apr/22 13:13;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/36099;;;","07/Apr/22 17:10;Gengliang.Wang;Issue resolved by pull request 36099
[https://github.com/apache/spark/pull/36099];;;",,,,,,,,,
Error when starting spark shell on Windows system,SPARK-38807,13438189,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huibai,huibai,huibai,06/Apr/22 13:59,03/Jun/22 14:52,13/Jul/23 08:47,02/Jun/22 12:45,3.2.1,,,,,,,,3.2.2,3.3.0,,,,Spark Core,,,,0,,,"Using the release version of spark-3.2.1  and the default configuration, starting spark shell on Windows system fails. (spark 3.1.2 doesn't show this issue)

Here is the stack trace of the exception:
{code:java}
22/04/06 21:47:45 ERROR SparkContext: Error initializing SparkContext.
java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
        ...
        at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.net.URISyntaxException: Illegal character in path at index 30: spark://192.168.X.X:56964/F:\classes
        at java.net.URI$Parser.fail(URI.java:2845)
        at java.net.URI$Parser.checkChars(URI.java:3018)
        at java.net.URI$Parser.parseHierarchical(URI.java:3102)
        at java.net.URI$Parser.parse(URI.java:3050)
        at java.net.URI.<init>(URI.java:588)
        at org.apache.spark.repl.ExecutorClassLoader.<init>(ExecutorClassLoader.scala:57)
        ... 70 more
22/04/06 21:47:45 ERROR Utils: Uncaught exception in thread main
java.lang.NullPointerException
            ... {code}",,apachespark,huibai,,,,,,,,,,,,,,,SPARK-38808,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 02 12:45:36 UTC 2022,,,,,,,,,,"0|z117y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/22 16:42;apachespark;User '1104056452' has created a pull request for this issue:
https://github.com/apache/spark/pull/36089;;;","10/Apr/22 15:21;apachespark;User '1104056452' has created a pull request for this issue:
https://github.com/apache/spark/pull/36134;;;","10/Apr/22 15:22;apachespark;User '1104056452' has created a pull request for this issue:
https://github.com/apache/spark/pull/36134;;;","04/May/22 04:11;apachespark;User '1104056452' has created a pull request for this issue:
https://github.com/apache/spark/pull/36446;;;","04/May/22 04:12;apachespark;User '1104056452' has created a pull request for this issue:
https://github.com/apache/spark/pull/36446;;;","04/May/22 08:59;apachespark;User '1104056452' has created a pull request for this issue:
https://github.com/apache/spark/pull/36447;;;","02/Jun/22 12:45;srowen;Issue resolved by pull request 36447
[https://github.com/apache/spark/pull/36447];;;",,,,,
"Support spark.kubernetes.test.(driver|executor)RequestCores",SPARK-38802,13438112,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,06/Apr/22 08:23,28/Sep/22 03:34,13/Jul/23 08:47,07/Apr/22 22:31,3.4.0,,,,,,,,3.3.1,3.4.0,,,,Kubernetes,Tests,,,0,,,"[https://github.com/apache/spark/pull/35830#pullrequestreview-929597027]

 

Support spark.kubernetes.test.(driver|executor)RequestCores to allow devs setting specific cpu for driver/executor.",,apachespark,dongjoon,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 07 22:31:31 UTC 2022,,,,,,,,,,"0|z117gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/22 09:08;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36087;;;","07/Apr/22 22:31;dongjoon;Issue resolved by pull request 36087
[https://github.com/apache/spark/pull/36087];;;",,,,,,,,,,
Fix scala license declaration,SPARK-38799,13438045,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,06/Apr/22 03:58,06/Apr/22 12:28,13/Jul/23 08:47,06/Apr/22 12:28,3.1.2,3.2.1,3.3.0,3.4.0,,,,,3.4.0,,,,,Project Infra,,,,0,,,"ASF License v2

[https://www.scala-lang.org/download/2.13.8.html]

[https://www.scala-lang.org/download/2.12.15.html] 

[3-clause BSD license|https://opensource.org/licenses/BSD-3-Clause]

[https://www.scala-lang.org/download/2.11.12.html] ",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 06 12:28:49 UTC 2022,,,,,,,,,,"0|z1172g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/22 04:08;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36081;;;","06/Apr/22 04:09;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/36081;;;","06/Apr/22 12:28;srowen;Issue resolved by pull request 36081
[https://github.com/apache/spark/pull/36081];;;",,,,,,,,,
Flaky test: ALSSuite.'ALS validate input dataset',SPARK-38776,13437486,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,03/Apr/22 06:56,03/Apr/22 20:09,13/Jul/23 08:47,03/Apr/22 17:21,3.3.0,,,,,,,,3.3.0,,,,,MLlib,,,,0,,,"- https://github.com/apache/spark/runs/5803714260?check_suite_focus=true

{code}
[info] ALSSuite:
...
[info] - ALS validate input dataset *** FAILED *** (2 seconds, 449 milliseconds)
[info]   Invalid Long: out of range ""Job aborted due to stage failure: Task 0 in stage 100.0 failed 1 times, most recent failure: Lost task 0.0 in stage 100.0 (TID 348) (localhost executor driver): org.apache.spark.SparkArithmeticException: Casting 1231000000000 to int causes overflow. To return NULL instead, use 'try_cast'. If necessary set spark.sql.ansi.enabled to false to bypass this error.
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,SPARK-38490,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 03 20:09:50 UTC 2022,,,,,,,,,,"0|z113oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/22 07:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36051;;;","03/Apr/22 07:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36051;;;","03/Apr/22 17:21;dongjoon;Issue resolved by pull request 36051
[https://github.com/apache/spark/pull/36051];;;","03/Apr/22 20:09;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36054;;;","03/Apr/22 20:09;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36054;;;",,,,,,,
Handle Hive's bucket spec case preserving behaviour,SPARK-38717,13436952,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,petertoth,petertoth,31/Mar/22 16:23,16/Oct/22 08:50,13/Jul/23 08:47,26/Sep/22 05:10,3.3.0,,,,,,,,3.3.1,3.4.0,,,,SQL,,,,0,,,"{code}
CREATE TABLE t(
 c STRING,
 B_C STRING
)
PARTITIONED BY (p_c STRING)
CLUSTERED BY (B_C) INTO 4 BUCKETS
STORED AS PARQUET
{code}
then
{code}
SELECT * FROM t
{code}
fails with:
{code}
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Bucket columns B_C is not part of the table columns ([FieldSchema(name:c, type:string, comment:null), FieldSchema(name:b_c, type:string, comment:null)]
	at org.apache.hadoop.hive.ql.metadata.Table.setBucketCols(Table.java:552)
	at org.apache.spark.sql.hive.client.HiveClientImpl$.toHiveTable(HiveClientImpl.scala:1098)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getPartitionsByFilter$1(HiveClientImpl.scala:764)
	at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:294)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:225)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:224)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:274)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionsByFilter(HiveClientImpl.scala:763)
	at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$listPartitionsByFilter$1(HiveExternalCatalog.scala:1287)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:101)
	... 110 more
{code}
 ",,apachespark,cloud_fan,dongjoon,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 28 20:17:16 UTC 2022,,,,,,,,,,"0|z110s0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/22 16:52;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/36027;;;","23/Sep/22 12:56;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37982;;;","23/Sep/22 12:57;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37982;;;","26/Sep/22 05:10;cloud_fan;Issue resolved by pull request 37982
[https://github.com/apache/spark/pull/37982];;;","28/Sep/22 20:17;dongjoon;I updated the Fix Version from 3.3.1 to 3.3.2 because 3.3.1 RC2 vote start without this.;;;",,,,,,,
Upgrade Hive Metastore Client to the 3.1.3 for Hive 3.1,SPARK-38708,13436786,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,31/Mar/22 02:52,11/Apr/22 01:35,13/Jul/23 08:47,11/Apr/22 01:35,3.4.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,,,apachespark,dongjoon,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 11 01:35:06 UTC 2022,,,,,,,,,,"0|z10zrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/22 02:59;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36018;;;","31/Mar/22 02:59;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36018;;;","11/Apr/22 01:35;dongjoon;Issue resolved by pull request 36018
[https://github.com/apache/spark/pull/36018];;;",,,,,,,,,
Use URI in FallbackStorage.copy,SPARK-38706,13436777,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,31/Mar/22 01:57,31/Mar/22 04:19,13/Jul/23 08:47,31/Mar/22 04:18,3.3.0,,,,,,,,3.3.0,,,,,Spark Core,,,,0,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,HADOOP-17139,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 31 04:18:06 UTC 2022,,,,,,,,,,"0|z10zpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Mar/22 02:50;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36017;;;","31/Mar/22 02:51;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36017;;;","31/Mar/22 04:18;dongjoon;This is resolved via https://github.com/apache/spark/pull/36017;;;",,,,,,,,,
Add `commons-collections` back,SPARK-38696,13436610,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,30/Mar/22 08:51,30/Mar/22 14:27,13/Jul/23 08:47,30/Mar/22 14:27,3.3.0,,,,,,,,3.3.0,,,,,Build,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 30 14:27:19 UTC 2022,,,,,,,,,,"0|z10yog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Mar/22 08:56;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36009;;;","30/Mar/22 14:27;dongjoon;Issue resolved by pull request 36009
[https://github.com/apache/spark/pull/36009];;;",,,,,,,,,,
Stream-stream outer join has a possible correctness issue due to weakly read consistent on outer iterators,SPARK-38684,13436379,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,kabhwan,kabhwan,29/Mar/22 09:22,05/Apr/22 06:29,13/Jul/23 08:47,01/Apr/22 10:09,3.2.1,3.3.0,,,,,,,3.2.2,3.3.0,,,,Structured Streaming,,,,0,correctness,releasenotes,"We figured out stream-stream join has the same issue with SPARK-38320 on the appended iterators. Since the root cause is same as SPARK-38320, this is only reproducible with RocksDB state store provider, but even with HDFS backed state store provider, it is not guaranteed by interface contract hence may depend on the JVM vendor, version, etc.

I can easily construct the scenario of “data loss” in state store.

Condition follows:
 * Use stream-stream time interval outer join

 ** left outer join has an issue on left side, right outer join has an issue on right side, full outer join has an issue on both sides

 * At batch N, produce row(s) on the problematic side which are non-late

 * At the same batch (batch N), some row(s) on the problematic side should be evicted by watermark condition

When the condition is fulfilled, out of sync happens with keyToNumValues between state and the iterator in evict phase. If eviction of the row happens for the grouping key (updating keyToNumValues), the eviction phase “overwrites” keyToNumValues in the state as the value it calculates.

Given that the eviction phase “do not know” about the new rows (keyToNumValues is out of sync), effectively discarding all rows from the state being added in the batch N.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 01 10:09:09 UTC 2022,,,,,,,,,,"0|z10x9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/22 09:23;kabhwan;Will submit a PR sooner.;;;","29/Mar/22 10:47;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/36002;;;","01/Apr/22 10:09;kabhwan;Issue resolved by pull request 36002
[https://github.com/apache/spark/pull/36002];;;",,,,,,,,,
Support nested generic case classes,SPARK-38681,13436325,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,eejbyfeldt,eejbyfeldt,eejbyfeldt,29/Mar/22 07:03,24/May/22 17:11,13/Jul/23 08:47,20/May/22 00:13,3.3.0,3.4.0,,,,,,,3.3.0,,,,,SQL,,,,0,,,"Spark fail to derive schemas when using nested case class with generic parameters. 

Example

{code:java}
case class GenericData[A](
    genericField: A)
{code}

Will derive a correct schema for `GenericData[Int]` but if the classes are nested e.g.

{code:java}
case class NestedGeneric[T](
  generic: GenericData[T])
{code}

it will fail to derive a schema for `NestedGeneric[Int]`.",,apachespark,eejbyfeldt,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 20 00:13:08 UTC 2022,,,,,,,,,,"0|z10wxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/22 12:47;apachespark;User 'eejbyfeldt' has created a pull request for this issue:
https://github.com/apache/spark/pull/36004;;;","19/May/22 10:05;eejbyfeldt;While testing the spark 3.3.0 release candidate I noticed that this is actually a regression from 3.2 that was introduced in #33205;;;","20/May/22 00:13;srowen;Issue resolved by pull request 36004
[https://github.com/apache/spark/pull/36004];;;",,,,,,,,,
Set upperbound for pandas-stubs in CI,SPARK-38680,13436281,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,29/Mar/22 00:27,12/Dec/22 18:11,13/Jul/23 08:47,29/Mar/22 02:08,3.3.0,,,,,,,,3.3.0,,,,,Project Infra,,,,0,,,"e.g., ) https://github.com/apache/spark/runs/5729037000?check_suite_focus=true

{code}
starting mypy annotations test...
annotations failed mypy checks:
python/pyspark/pandas/ml.py:62: error: Incompatible types in assignment (expression has type ""Index"", variable has type ""MultiIndex"")  [assignment]
python/pyspark/pandas/frame.py:6133: error: unused ""type: ignore"" comment
python/pyspark/pandas/frame.py:6292: error: Incompatible types in assignment (expression has type ""Index"", variable has type ""MultiIndex"")  [assignment]
Found 3 errors in 2 files (checked 325 source files)
{code}


Seems like the new release of pandas-stubs has a breaking change.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 29 02:08:34 UTC 2022,,,,,,,,,,"0|z10wo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Mar/22 00:44;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35996;;;","29/Mar/22 02:08;gurwls223;Issue resolved by pull request 35996
[https://github.com/apache/spark/pull/35996];;;",,,,,,,,,,
pyspark hangs in local mode running rdd map operation,SPARK-38677,13436224,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,ankurd,tgraves,tgraves,28/Mar/22 17:47,12/Dec/22 18:11,13/Jul/23 08:47,12/Apr/22 03:06,3.2.1,3.3.0,,,,,,,3.2.2,3.3.0,,,,PySpark,,,,0,,,"In spark 3.2.1 (spark 3.2.0 doesn't show this issue), pyspark will hang when running and RDD map operations and converting to a dataframe.  Code is below to reproduce.  

Env:
spark 3.2.1 local mode, just run {{./bin/pyspark --driver-memory XXXXG --driver-cores XXXX}}

{{download dataset from here [https://rapidsai-data.s3.us-east-2.amazonaws.com/spark/mortgage.zip]}}
{{just 200000 rows could reproduce the issue }}{{head -n 200000 mortgage_eval_merged.csv > mortgage_eval_merged-small.csv}}{{{} but if the input dataset is small, such 50000 rows, it works well.{}}}{{{{}}{}}}run codes below:
{code:java}
path = ""/XXXX/mortgage_eval_merged-small.csv""
src_data = sc.textFile(path).map(lambda x:x.split("",""))
column_list =['c1','c2','c3','c4','c5','c6','c7','c8','c9','c10','c11','c12','c13','c14','c15','c16','c17','c18','c19','c20','c21','c22','c23','c24','c25','c26','c27','c28']
df = spark.createDataFrame(src_data,column_list)
print(df.show(1)){code}",,ankurd,apachespark,cloud_fan,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 13 04:54:35 UTC 2022,,,,,,,,,,"0|z10wbs:",9223372036854775807,,,,,,,,,,,,,3.2.2,3.3.0,,,,,,,,,"28/Mar/22 17:53;tgraves;Note, if you kill the python.daemon process while its hung, it will return to pyspark console and have the right results.

I looked through commits in 3.2.1 and it appears that this was introduced by https://issues.apache.org/jira/browse/SPARK-33277

Specifically commit [https://github.com/apache/spark/commit/243c321db2f02f6b4d926114bd37a6e74c2be185] 

At least I revert that commit and rebuilt and it then works.  Also this did not reproduce in standalone mode so it might just be a local mode issue.

 

[~ueshin] [~ankurdave] [~hyukjin.kwon] ;;;","28/Mar/22 23:59;gurwls223;Thanks for the analysis, [~tgraves]. taking a look now.;;;","04/Apr/22 19:34;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/36065;;;","12/Apr/22 03:06;gurwls223;Fixed in https://github.com/apache/spark/pull/36065;;;","13/Apr/22 04:54;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36172;;;",,,,,,,
Race condition in BlockInfoManager during unlock,SPARK-38675,13436136,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,hvanhovell,hvanhovell,hvanhovell,28/Mar/22 11:00,02/Jun/22 08:48,13/Jul/23 08:47,02/Jun/22 08:48,3.3.0,,,,,,,,3.3.0,,,,,Spark Core,,,,0,,,There is a race condition between unlock and releaseAllLocksForTask in the block manager. This can lead to negative reader counts (which trip an assertion).,,apachespark,cloud_fan,hvanhovell,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 02 08:48:48 UTC 2022,,,,,,,,,,"0|z10vs8:",9223372036854775807,,,,,,,,,,,,,3.3.0,,,,,,,,,,"28/Mar/22 13:42;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/35991;;;","02/Jun/22 08:48;cloud_fan;Issue resolved by pull request 35991
[https://github.com/apache/spark/pull/35991];;;",,,,,,,,,,
Missing aggregate filter checks,SPARK-38666,13435978,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,27/Mar/22 00:31,22/Apr/22 03:14,13/Jul/23 08:47,22/Apr/22 03:14,3.4.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"h3. Window function in filter
{noformat}
select sum(a) filter (where nth_value(a, 2) over (order by b) > 1)
from (select 1 a, '2' b);
{noformat}
This query should produce an analysis error, but instead produces a stack overflow:
{noformat}
java.lang.StackOverflowError: null
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$collect$1(TreeNode.scala:305) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$collect$1$adapted(TreeNode.scala:305) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreach(TreeNode.scala:264) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreach$1(TreeNode.scala:265) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreach$1$adapted(TreeNode.scala:265) ~[spark-catalyst_2.12-3.4.0-SNAPSHOT.jar:3.4.0-SNAPSHOT]
	at scala.collection.Iterator.foreach(Iterator.scala:943) ~[scala-library.jar:?]
...
{noformat}
h3. Non-boolean filter expression
{noformat}
select sum(a) filter (where a) from (select 1 a, '2' b);
{noformat}
This query should produce an analysis error, but instead causes a projection compilation error or whole-stage codegen error (depending on the datatype of the expression):
{noformat}
22/03/26 17:19:03 ERROR CodeGenerator: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 50, Column 6: Not a boolean expression
org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 50, Column 6: Not a boolean expression
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:12021) ~[janino-3.0.16.jar:?]
	at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:4049) ~[janino-3.0.16.jar:?]
	at org.codehaus.janino.UnitCompiler.access$6300(UnitCompiler.java:226) ~[janino-3.0.16.jar:?]
	at org.codehaus.janino.UnitCompiler$14.visitIntegerLiteral(UnitCompiler.java:4016) ~[janino-3.0.16.jar:?]
...
22/03/26 17:19:05 WARN MutableProjection: Expr codegen error and falling back to interpreter mode
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 15: failed to compile: org.codehaus.commons.compiler.CompileException: File 'generated.java', Line 40, Column 15: Not a boolean expression
	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306) ~[guava-14.0.1.jar:?]
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293) ~[guava-14.0.1.jar:?]
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116) ~[guava-14.0.1.jar:?]
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135) ~[guava-14.0.1.jar:?]
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410) ~[guava-14.0.1.jar:?]
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380) ~[guava-14.0.1.jar:?]
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342) ~[guava-14.0.1.jar:?]
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257) ~[guava-14.0.1.jar:?]
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000) ~[guava-14.0.1.jar:?]
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004) ~[guava-14.0.1.jar:?]
...
NULL
Time taken: 5.397 seconds, Fetched 1 row(s)
{noformat}
Interestingly, it also returns a result (NULL).
h3. Aggregate expression in filter expression
{noformat}
select max(b) filter (where max(a) > 1) from (select 1 a, '2' b);
{noformat}
This query should produce an analysis error, but instead causes a projection compilation error or whole-stage codegen error (depending on the datatype of the expression being aggregated):
{noformat}
22/03/26 17:26:38 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 2) (10.0.0.106 executor driver): org.apache.spark.SparkUnsupportedOperationException: Cannot evaluate expression: max(1)
	at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotEvaluateExpressionError(QueryExecutionErrors.scala:79)
	at org.apache.spark.sql.catalyst.expressions.Unevaluable.eval(Expression.scala:344)
	at org.apache.spark.sql.catalyst.expressions.Unevaluable.eval$(Expression.scala:343)
	at org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression.eval(interfaces.scala:99)
	at org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:593)
	at org.apache.spark.sql.catalyst.expressions.If.eval(conditionalExpressions.scala:68)
...
{noformat}",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 05 17:27:41 UTC 2022,,,,,,,,,,"0|z10ut4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/22 17:27;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36072;;;",,,,,,,,,,,
upgrade jackson due to CVE-2020-36518,SPARK-38665,13435965,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,pj.fanning,pj.fanning,26/Mar/22 14:52,24/Nov/22 00:38,13/Jul/23 08:47,27/Mar/22 17:03,3.2.1,,,,,,,,3.3.0,,,,,Spark Core,,,,0,,,"* https://github.com/FasterXML/jackson-databind/issues/2816
* only jackson-databind has a 2.13.2.1 release
* other jackson jars should stay at 2.13.2",,apachespark,pj.fanning,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 27 17:03:54 UTC 2022,,,,,,,,,,"0|z10uq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/22 17:36;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/35981;;;","27/Mar/22 17:03;yumwang;Issue resolved by pull request 35981
https://github.com/apache/spark/pull/35981;;;",,,,,,,,,,
Remove inaccessible repository: https://dl.bintray.com,SPARK-38663,13435932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,26/Mar/22 06:43,12/Dec/22 18:11,13/Jul/23 08:47,27/Mar/22 00:45,3.0.4,,,,,,,,3.0.4,,,,,Tests,,,,0,,,,,apachespark,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 27 00:45:54 UTC 2022,,,,,,,,,,"0|z10uiw:",9223372036854775807,,,,,,,,,,,,,3.0.4,,,,,,,,,,"26/Mar/22 07:36;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/35978;;;","27/Mar/22 00:45;gurwls223;Issue resolved by pull request 35978
[https://github.com/apache/spark/pull/35978];;;",,,,,,,,,,
OffsetWindowFunctionFrameBase cannot find the offset row whose input is not null,SPARK-38655,13435756,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,25/Mar/22 09:09,07/Jun/22 02:14,13/Jul/23 08:47,28/Mar/22 06:18,3.3.0,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,correctness,,"select x, nth_value(x, 5) IGNORE NULLS over (order by x rows between unbounded preceding and current row)
from (select explode(sequence(1, 3)) x)

returns
null
null
3

But it should returns
null
null
null",,apachespark,beliefer,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 28 06:18:21 UTC 2022,,,,,,,,,,"0|z10tgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/22 09:33;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/35971;;;","28/Mar/22 06:18;cloud_fan;Issue resolved by pull request 35971
[https://github.com/apache/spark/pull/35971];;;",,,,,,,,,,
uploadFileUri should preserve file scheme,SPARK-38652,13435698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dcoliversun,dcoliversun,25/Mar/22 02:28,30/Mar/22 15:28,13/Jul/23 08:47,30/Mar/22 15:28,3.3.0,,,,,,,,3.0.4,3.1.3,3.2.2,3.3.0,,Kubernetes,,,,0,,,"DepsTestsSuite in k8s IT test is blocked with PathIOException in hadoop-aws-3.3.2. Exception Message is as follow
{code:java}
Exception in thread ""main"" org.apache.spark.SparkException: Uploading file /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar failed...        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:332)        
at org.apache.spark.deploy.k8s.KubernetesUtils$.$anonfun$uploadAndTransformFileUris$1(KubernetesUtils.scala:277)        
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)        
at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)        
at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)        
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)        
at scala.collection.TraversableLike.map(TraversableLike.scala:286)        
at scala.collection.TraversableLike.map$(TraversableLike.scala:279)        
at scala.collection.AbstractTraversable.map(Traversable.scala:108)        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadAndTransformFileUris(KubernetesUtils.scala:275)        
at org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.$anonfun$getAdditionalPodSystemProperties$1(BasicDriverFeatureStep.scala:187)       
at scala.collection.immutable.List.foreach(List.scala:431)        
at org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.getAdditionalPodSystemProperties(BasicDriverFeatureStep.scala:178)        
at org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.$anonfun$buildFromFeatures$5(KubernetesDriverBuilder.scala:86)        at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)        
at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)        
at scala.collection.immutable.List.foldLeft(List.scala:91)        
at org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.buildFromFeatures(KubernetesDriverBuilder.scala:84)        
at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:104)        
at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5(KubernetesClientApplication.scala:248)        
at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5$adapted(KubernetesClientApplication.scala:242)
at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2738)        
at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:242)        
at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:214)        
at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)        
at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)        
at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)        
at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)        
at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)        
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)        
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)Caused by: org.apache.spark.SparkException: Error uploading file spark-examples_2.12-3.4.0-SNAPSHOT.jar        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileToHadoopCompatibleFS(KubernetesUtils.scala:355)        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:328)        
... 30 more
Caused by: org.apache.hadoop.fs.PathIOException: `Cannot get relative path for URI:file:///Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar': Input/output error
at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.getFinalPath(CopyFromLocalOperation.java:365)        
at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.uploadSourceFromFS(CopyFromLocalOperation.java:226)        
at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.execute(CopyFromLocalOperation.java:170)        
at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$copyFromLocalFile$25(S3AFileSystem.java:3920)        
at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)        
at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)        
at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)        
at org.apache.hadoop.fs.s3a.S3AFileSystem.copyFromLocalFile(S3AFileSystem.java:3913)        
at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileToHadoopCompatibleFS(KubernetesUtils.scala:352)        
... 31 more  {code}
For more information please refer to HADOOP-18173.

 

But, DepsTestsSuite with hadoop-aws-3.3.1 works normally.
{noformat}
hengzhen.sq@b-q922md6r-0237 ~/Desktop$ /Users/hengzhen.sq/IdeaProjects/spark/bin/spark-submit --deploy-mode cluster --class org.apache.spark.examples.SparkRemoteFileTest --master k8s://https://192.168.64.86:8443/ --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.testing=false  --conf spark.hadoop.fs.s3a.access.key=minio --conf spark.kubernetes.driver.label.spark-app-locator=a8937b5fdf6a444a806ee1c3ecac37fc --conf spark.kubernetes.file.upload.path=s3a://spark --conf spark.authenticate=true --conf spark.executor.instances=1 --conf spark.kubernetes.submission.waitAppCompletion=false --conf spark.kubernetes.executor.label.spark-app-locator=a8937b5fdf6a444a806ee1c3ecac37fc --conf spark.kubernetes.namespace=spark-job --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark --conf spark.hadoop.fs.s3a.secret.key=miniostorage --conf spark.executor.extraJavaOptions=-Dlog4j2.debug --conf spark.hadoop.fs.s3a.endpoint=192.168.64.86:32681 --conf spark.app.name=spark-test-app --conf spark.files=/tmp/tmp7013228683780235449.txt --conf spark.ui.enabled=true --conf spark.driver.extraJavaOptions=-Dlog4j2.debug --conf spark.kubernetes.container.image=registry.cn-hangzhou.aliyuncs.com/smart-spark/spark:test --conf spark.executor.cores=1 --conf spark.jars.packages=org.apache.hadoop:hadoop-aws:3.3.1 --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar tmp7013228683780235449.txt
22/03/25 10:24:10 WARN Utils: Your hostname, B-Q922MD6R-0237.local resolves to a loopback address: 127.0.0.1; using 30.25.86.17 instead (on interface en0)
22/03/25 10:24:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address
:: loading settings :: url = jar:file:/Users/hengzhen.sq/IdeaProjects/spark/assembly/target/scala-2.12/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
Ivy Default Cache set to: /Users/hengzhen.sq/.ivy2/cache
The jars for the packages stored in: /Users/hengzhen.sq/.ivy2/jars
org.apache.hadoop#hadoop-aws added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent-8220baa6-0490-4484-9779-945d4cf69df4;1.0
	confs: [default]
	found org.apache.hadoop#hadoop-aws;3.3.1 in central
	found com.amazonaws#aws-java-sdk-bundle;1.11.901 in central
	found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
:: resolution report :: resolve 224ms :: artifacts dl 6ms
	:: modules in use:
	com.amazonaws#aws-java-sdk-bundle;1.11.901 from central in [default]
	org.apache.hadoop#hadoop-aws;3.3.1 from central in [default]
	org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |
	---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent-8220baa6-0490-4484-9779-945d4cf69df4
	confs: [default]
	0 artifacts copied, 3 already retrieved (0kB/6ms)
22/03/25 10:24:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/03/25 10:24:11 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
22/03/25 10:24:12 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
22/03/25 10:24:12 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
22/03/25 10:24:12 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
22/03/25 10:24:12 INFO MetricsSystemImpl: s3a-file-system metrics system started
22/03/25 10:24:13 INFO KubernetesUtils: Uploading file: /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar to dest: s3a://spark/spark-upload-f49ee7fc-182d-499a-b073-40b298c55e8b/spark-examples_2.12-3.4.0-SNAPSHOT.jar...
22/03/25 10:24:13 INFO KubernetesUtils: Uploading file: /private/tmp/tmp7013228683780235449.txt to dest: s3a://spark/spark-upload-906c6d35-6aa7-4ee8-8c37-434f547c6087/tmp7013228683780235449.txt...
22/03/25 10:24:14 INFO ShutdownHookManager: Shutdown hook called
22/03/25 10:24:14 INFO ShutdownHookManager: Deleting directory /private/var/folders/3t/v_td68551s78mq4c1cpk86gc0000gn/T/spark-e89a395e-c3b4-4619-8c9b-e60310af6503
22/03/25 10:24:14 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
22/03/25 10:24:14 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
22/03/25 10:24:14 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.{noformat}
CopyFromLocalFile action in hadoop-aws is different between 3.3.1 and 3.3.2, because 3.3.2 introuduces CopyFromLocalFileOperation.",,apachespark,dannycjones,dcoliversun,dongjoon,stevel@apache.org,,,,,,,,,,,,,,,,,HADOOP-18173,HADOOP-17139,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 30 15:28:25 UTC 2022,,,,,,,,,,"0|z10t3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/22 02:31;dcoliversun;I am working on it.

cc [~chaosun]  & [~dongjoon] ;;;","25/Mar/22 16:03;stevel@apache.org;have you tried running the same suite against an aws s3 endpoint?;;;","26/Mar/22 01:51;dcoliversun;[~stevel@apache.org] No. I can do it, which help us to confirm whether the cause of the problem is minio or hadoop-aws-3.3.2. And, I share result about test here.;;;","27/Mar/22 07:21;dcoliversun;[~stevel@apache.org] Hi, I run the same suite against an real aws s3 endpoint, and have same exception. I think we could exclude reason about minio deployment.
{noformat}
$ bin/spark-submit --deploy-mode cluster --class org.apache.spark.examples.SparkRemoteFileTest --master k8s://https://192.168.64.87:8443/ --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem --conf spark.testing=false  --conf spark.hadoop.fs.s3a.access.key=XXXXXXX --conf spark.kubernetes.driver.label.spark-app-locator=a8937b5fdf6a444a806ee1c3ecac37fc --conf spark.kubernetes.file.upload.path=s3a://dcoliversun --conf spark.authenticate=true --conf spark.executor.instances=1 --conf spark.kubernetes.submission.waitAppCompletion=false --conf spark.kubernetes.executor.label.spark-app-locator=a8937b5fdf6a444a806ee1c3ecac37fc --conf spark.kubernetes.namespace=spark-job --conf spark.kubernetes.authenticate.driver.serviceAccountName=spark --conf spark.hadoop.fs.s3a.secret.key=XXXXXXX --conf spark.executor.extraJavaOptions=-Dlog4j2.debug --conf spark.hadoop.fs.s3a.endpoint=https://s3.ap-southeast-1.amazonaws.com --conf spark.app.name=spark-test-app --conf spark.files=/tmp/tmp7013228683780235449.txt --conf spark.ui.enabled=true --conf spark.driver.extraJavaOptions=-Dlog4j2.debug --conf spark.kubernetes.container.image=registry.cn-hangzhou.aliyuncs.com/smart-spark/spark:test --conf spark.executor.cores=1 --conf spark.hadoop.fs.s3a.connection.ssl.enabled=false /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar tmp7013228683780235449.txt


22/03/27 15:16:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22/03/27 15:16:28 INFO SparkKubernetesClientFactory: Auto-configuring K8S client using current context from users K8S config file
22/03/27 15:16:28 INFO KerberosConfDriverFeatureStep: You have not specified a krb5.conf file locally or via a ConfigMap. Make sure that you have the krb5.conf locally on the driver image.
22/03/27 15:16:29 INFO KubernetesUtils: sq-isLocalAndResolvable => resource is file:/Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar
22/03/27 15:16:29 INFO KubernetesUtils: sq => uri is file:/Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar, uri scheme is file
22/03/27 15:16:29 INFO KubernetesUtils: sq-uploadAndTransformFileUris, uri is file:/Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar
22/03/27 15:16:29 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
22/03/27 15:16:29 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
22/03/27 15:16:29 INFO MetricsSystemImpl: s3a-file-system metrics system started
22/03/27 15:16:31 INFO KubernetesUtils: Uploading file: /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar to dest: s3a://dcoliversun/spark-upload-eb20e2da-17b6-4dcd-b4f1-8e47bc80c1e9/spark-examples_2.12-3.4.0-SNAPSHOT.jar...
22/03/27 15:16:31 INFO S3AFileSystem: Copying local file from /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar to s3a://dcoliversun/spark-upload-eb20e2da-17b6-4dcd-b4f1-8e47bc80c1e9/spark-examples_2.12-3.4.0-SNAPSHOT.jar
22/03/27 15:16:31 INFO CopyFromLocalOperation: Copying local file from /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar to s3a://dcoliversun/spark-upload-eb20e2da-17b6-4dcd-b4f1-8e47bc80c1e9/spark-examples_2.12-3.4.0-SNAPSHOT.jar
22/03/27 15:16:31 INFO CopyFromLocalOperation: execute#CopyFromLocalOperation, sourceFile is /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar
22/03/27 15:16:31 INFO CopyFromLocalOperation: uploadSourceFromFS#CopyFromLocalOperation, localFile 1: path is LocatedFileStatus{path=file:/Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar; isDirectory=false; length=1567474; replication=1; blocksize=33554432; modification_time=1647874074000; access_time=1647874074000; owner=hengzhen.sq; group=staff; permission=rw-r--r--; isSymlink=false; hasAcl=false; isEncrypted=false; isErasureCoded=false}
22/03/27 15:16:31 INFO CopyFromLocalOperation: getFinalPath#CopyFromLocalOperation, src is file:/Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar, source is /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar
Exception in thread ""main"" org.apache.spark.SparkException: Uploading file /Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar failed...
        at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:332)
        at org.apache.spark.deploy.k8s.KubernetesUtils$.$anonfun$uploadAndTransformFileUris$1(KubernetesUtils.scala:277)
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at scala.collection.TraversableLike.map(TraversableLike.scala:286)
        at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
        at scala.collection.AbstractTraversable.map(Traversable.scala:108)
        at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadAndTransformFileUris(KubernetesUtils.scala:275)
        at org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.$anonfun$getAdditionalPodSystemProperties$1(BasicDriverFeatureStep.scala:187)
        at scala.collection.immutable.List.foreach(List.scala:431)
        at org.apache.spark.deploy.k8s.features.BasicDriverFeatureStep.getAdditionalPodSystemProperties(BasicDriverFeatureStep.scala:178)
        at org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.$anonfun$buildFromFeatures$5(KubernetesDriverBuilder.scala:86)
        at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
        at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
        at scala.collection.immutable.List.foldLeft(List.scala:91)
        at org.apache.spark.deploy.k8s.submit.KubernetesDriverBuilder.buildFromFeatures(KubernetesDriverBuilder.scala:84)
        at org.apache.spark.deploy.k8s.submit.Client.run(KubernetesClientApplication.scala:104)
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5(KubernetesClientApplication.scala:248)
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.$anonfun$run$5$adapted(KubernetesClientApplication.scala:242)
        at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2738)
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.run(KubernetesClientApplication.scala:242)
        at org.apache.spark.deploy.k8s.submit.KubernetesClientApplication.start(KubernetesClientApplication.scala:214)
        at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
        at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
        at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
        at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.spark.SparkException: Error uploading file spark-examples_2.12-3.4.0-SNAPSHOT.jar
        at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileToHadoopCompatibleFS(KubernetesUtils.scala:355)
        at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileUri(KubernetesUtils.scala:328)
        ... 30 more
Caused by: org.apache.hadoop.fs.PathIOException: `Cannot get relative path for URI:file:///Users/hengzhen.sq/IdeaProjects/spark/dist/examples/jars/spark-examples_2.12-3.4.0-SNAPSHOT.jar': Input/output error
        at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.getFinalPath(CopyFromLocalOperation.java:365)
        at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.uploadSourceFromFS(CopyFromLocalOperation.java:226)
        at org.apache.hadoop.fs.s3a.impl.CopyFromLocalOperation.execute(CopyFromLocalOperation.java:170)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$copyFromLocalFile$25(S3AFileSystem.java:3920)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)
        at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.copyFromLocalFile(S3AFileSystem.java:3913)
        at org.apache.spark.deploy.k8s.KubernetesUtils$.uploadFileToHadoopCompatibleFS(KubernetesUtils.scala:352)
        ... 31 more
22/03/27 15:16:31 INFO ShutdownHookManager: Shutdown hook called
22/03/27 15:16:31 INFO ShutdownHookManager: Deleting directory /private/var/folders/3t/v_td68551s78mq4c1cpk86gc0000gn/T/spark-c7717437-e729-47d8-aa13-4bdd716beb1d
22/03/27 15:16:32 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
22/03/27 15:16:32 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
22/03/27 15:16:32 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.{noformat};;;","29/Mar/22 22:16;dongjoon;Any update, [~dcoliversun]?;;;","30/Mar/22 00:37;dongjoon;BTW, [~dcoliversun]. K8s IT itself doesn't fail in both Apache Spark `master` branch and `branch-3.3` in my environment. Do you mean the test case fails when you do `spark-submit`?
{code}
$ build/sbt -Psparkr -Pkubernetes -Pvolcano -Pkubernetes-integration-tests -Dtest.exclude.tags=minikube -Dspark.kubernetes.test.deployMode=docker-for-desktop ""kubernetes-integration-tests/test""
...
[info] KubernetesSuite:
[info] - Run SparkPi with no resources (8 seconds, 527 milliseconds)
[info] - Run SparkPi with no resources & statefulset allocation (8 seconds, 323 milliseconds)
[info] - Run SparkPi with a very long application name. (8 seconds, 386 milliseconds)
[info] - Use SparkLauncher.NO_RESOURCE (8 seconds, 425 milliseconds)
[info] - Run SparkPi with a master URL without a scheme. (8 seconds, 385 milliseconds)
[info] - Run SparkPi with an argument. (8 seconds, 328 milliseconds)
[info] - Run SparkPi with custom labels, annotations, and environment variables. (8 seconds, 384 milliseconds)
[info] - All pods have the same service account by default (8 seconds, 342 milliseconds)
[info] - Run extraJVMOptions check on driver (4 seconds, 327 milliseconds)
[info] - Run SparkRemoteFileTest using a remote data file (8 seconds, 429 milliseconds)
...
{code};;;","30/Mar/22 01:26;dcoliversun;[~dongjoon] Hi. DepsTestsSuite has tests as follow
 * Launcher client dependencies
 * SPARK-33615: Launcher client archives
 * SPARK-33748: Launcher python client respecting PYSPARK_PYTHON
 * ...

spark-submit command is used by these tests. So, I think DepsTestsSuite blocks.

Could you please check these tests run? Maybe `-Dtest.exclude.tags` option doesn't need `minikube` value.;;;","30/Mar/22 02:09;dongjoon;Got it, [~dcoliversun].;;;","30/Mar/22 04:23;dongjoon;I also confirmed this regression and raise this issue as a blocker. Thank you, [~dcoliversun].;;;","30/Mar/22 09:41;dongjoon;I'm going to make a PR for this.;;;","30/Mar/22 09:47;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/36010;;;","30/Mar/22 15:28;dongjoon;Issue resolved by pull request 36010
[https://github.com/apache/spark/pull/36010];;;"
NPE with unpersisting memory-only RDD with RDD fetching from shuffle service enabled,SPARK-38640,13435452,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kimahriman,kimahriman,kimahriman,24/Mar/22 00:00,17/Apr/22 13:40,13/Jul/23 08:47,17/Apr/22 13:40,3.2.1,,,,,,,,3.3.0,,,,,Spark Core,,,,0,,,"If you have RDD fetching from shuffle service enabled, memory-only cached RDDs will fail to unpersist.

 

 
{code:java}
// spark.shuffle.service.fetch.rdd.enabled=true
val df = spark.range(5)
  .persist(StorageLevel.MEMORY_ONLY)
df.count()
df.unpersist(true)
{code}
 ",,apachespark,fchen,kimahriman,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 17 13:40:44 UTC 2022,,,,,,,,,,"0|z10rlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/22 11:40;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/35959;;;","24/Mar/22 11:40;apachespark;User 'Kimahriman' has created a pull request for this issue:
https://github.com/apache/spark/pull/35959;;;","17/Apr/22 13:40;srowen;Resolved by https://github.com/apache/spark/pull/35959;;;",,,,,,,,,
Arbitrary shell command injection via Utils.unpack(),SPARK-38631,13435244,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,23/Mar/22 03:53,12/Dec/22 18:11,13/Jul/23 08:47,24/Mar/22 03:15,3.1.2,3.2.1,3.3.0,,,,,,3.1.3,3.2.2,3.3.0,,,Spark Core,,,,0,,,"There is a risk for arbitrary shell command injection via {{Utils.unpack}} when the filename is controlled by a malicious user. This is due to an issue in Hadoop's {{unTar}}, that is not properly escaping the filename before passing to a shell command:https://github.com/apache/hadoop/blob/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileUtil.java#L904",,apachespark,Qin Yao,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 25 10:16:58 UTC 2022,,,,,,,,,,"0|z10qbc:",9223372036854775807,,,,,,,,,,,,,3.1.2,3.2.1,3.3.0,,,,,,,,"23/Mar/22 04:02;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35946;;;","23/Mar/22 04:03;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35946;;;","24/Mar/22 03:15;gurwls223;Fixed in https://github.com/apache/spark/pull/35946;;;","25/Mar/22 10:15;Qin Yao;[~hyukjin.kwon] Hi, shall we update the fixed versions?;;;","25/Mar/22 10:16;gurwls223;Thanks for pointing this out, [~Qin Yao];;;",,,,,,,
K8s app name label should start and end with alphanumeric char,SPARK-38630,13435229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,23/Mar/22 01:45,23/Mar/22 04:12,13/Jul/23 08:47,23/Mar/22 04:12,3.3.0,,,,,,,,3.3.0,,,,,Kubernetes,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,SPARK-36566,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 23 04:12:53 UTC 2022,,,,,,,,,,"0|z10q80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Mar/22 01:48;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35943;;;","23/Mar/22 01:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35943;;;","23/Mar/22 04:12;dongjoon;Issue resolved by pull request 35943
[https://github.com/apache/spark/pull/35943];;;",,,,,,,,,
Don't push down limit through window that's using percent_rank,SPARK-38614,13434897,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,ZygD,ZygD,21/Mar/22 12:29,08/Aug/22 01:30,13/Jul/23 08:47,23/Jun/22 00:53,3.2.0,3.2.1,3.3.0,,,,,,3.2.2,3.3.1,3.4.0,,,PySpark,SQL,,,0,correctness,,"Expected result is obtained using Spark 3.1.2, but not 3.2.0, 3.2.1 or 3.3.0.

*Minimal reproducible example*
{code:java}
from pyspark.sql import SparkSession, functions as F, Window as W
spark = SparkSession.builder.getOrCreate()
 
df = spark.range(101).withColumn('pr', F.percent_rank().over(W.orderBy('id')))
df.show(3)
df.show(5) {code}
*Expected result*
{code:java}
+---+----+
| id|  pr|
+---+----+
|  0| 0.0|
|  1|0.01|
|  2|0.02|
+---+----+
only showing top 3 rows

+---+----+
| id|  pr|
+---+----+
|  0| 0.0|
|  1|0.01|
|  2|0.02|
|  3|0.03|
|  4|0.04|
+---+----+
only showing top 5 rows{code}
*Actual result*
{code:java}
+---+------------------+
| id|                pr|
+---+------------------+
|  0|               0.0|
|  1|0.3333333333333333|
|  2|0.6666666666666666|
+---+------------------+
only showing top 3 rows

+---+---+
| id| pr|
+---+---+
|  0|0.0|
|  1|0.2|
|  2|0.4|
|  3|0.6|
|  4|0.8|
+---+---+
only showing top 5 rows{code}",,apachespark,bersprockets,yumwang,ZygD,,,,,,,,,,,,,,,,,,SPARK-40002,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,Thu Jun 23 00:53:55 UTC 2022,,,,,,,,,,"0|z10o6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jun/22 15:10;ZygD;Just checked with the new 3.3.0 release. The error still persists.;;;","19/Jun/22 20:40;bersprockets;Likely caused by [this|https://github.com/apache/spark/commit/c234c5b5f1676fbb9a79dc865534fec566425326].

I think there should be an exemption for {{{}percent_rank{}}}. I'll make a PR in the next week and see what others think.;;;","21/Jun/22 23:47;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36951;;;","21/Jun/22 23:47;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36951;;;","22/Jun/22 01:52;yumwang;Thank you for reporting this issue. Workaround:
{code:sql}
set spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.LimitPushDownThroughWindow;
{code}
;;;","23/Jun/22 00:53;yumwang;Issue resolved by pull request 36951
[https://github.com/apache/spark/pull/36951];;;",,,,,,
Fix RemoteBlockPushResolverSuite#testWritingPendingBufsIsAbortedImmediatelyDuringComplete,SPARK-38613,13434895,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,21/Mar/22 12:28,23/Mar/22 14:59,13/Jul/23 08:47,23/Mar/22 14:59,3.4.0,,,,,,,,3.4.0,,,,,Tests,,,,0,,,"`testWritingPendingBufsIsAbortedImmediatelyDuringComplete`  throw an

RuntimeException, but the Exception is not thrown by the expected statement.

 ",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 23 14:59:40 UTC 2022,,,,,,,,,,"0|z10o68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/22 12:41;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35923;;;","21/Mar/22 12:42;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35923;;;","23/Mar/22 14:59;srowen;Issue resolved by pull request 35923
[https://github.com/apache/spark/pull/35923];;;",,,,,,,,,
Fix Inline type hint for duplicated.keep,SPARK-38612,13434861,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,21/Mar/22 09:40,12/Dec/22 18:10,13/Jul/23 08:47,21/Mar/22 12:03,3.4.0,,,,,,,,3.3.0,,,,,PySpark,,,,0,,,,,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 21 12:03:26 UTC 2022,,,,,,,,,,"0|z10nyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/22 09:45;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35920;;;","21/Mar/22 09:46;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35920;;;","21/Mar/22 12:03;gurwls223;Issue resolved by pull request 35920
[https://github.com/apache/spark/pull/35920];;;",,,,,,,,,
ceil and floor return different types when called from scala than sql,SPARK-38604,13434685,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,revans2,revans2,revans2,19/Mar/22 13:03,12/Dec/22 18:11,13/Jul/23 08:47,22/Mar/22 00:48,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"In Spark 3.3.0  SPARK-37475 [PR|http://example.com/][https://github.com/apache/spark/pull/34729] went in and added support for a scale parameter to floor and ceil.  There was [discussion|https://github.com/apache/spark/pull/34729#discussion_r761157050] about potential incompatibilities, specifically with respect to the return types. It looks like it was [decided|https://github.com/apache/spark/pull/34729#discussion_r767446855 to keep the old behavior if no scale parameter was passed in, but use the new functionality if a scale is passed in.

 

But the scala API didn't get updated to do the same thing as the SQL API.
{code:scala}
scala> spark.range(1).selectExpr(""id"", ""ceil(id) as one_arg_sql"", ""ceil(id, 0) as two_arg_sql"").select(col(""*""), ceil(col(""id"")).alias(""one_arg_func""), ceil(col(""id""), lit(0)).alias(""two_arg_func"")).printSchema
root
 |-- id: long (nullable = false)
 |-- one_arg_sql: long (nullable = true)
 |-- two_arg_sql: decimal(20,0) (nullable = true)
 |-- one_arg_func: decimal(20,0) (nullable = true)
 |-- two_arg_func: decimal(20,0) (nullable = true)
 

scala> spark.range(1).selectExpr(""cast(id as double) as id"").selectExpr(""id"", ""ceil(id) as one_arg_sql"", ""ceil(id, 0) as two_arg_sql"").select(col(""*""), ceil(col(""id"")).alias(""one_arg_func""), ceil(col(""id""), lit(0)).alias(""two_arg_func"")).printSchema
root
 |-- id: double (nullable = false)
 |-- one_arg_sql: long (nullable = true)
 |-- two_arg_sql: decimal(30,0) (nullable = true)
 |-- one_arg_func: decimal(30,0) (nullable = true)
 |-- two_arg_func: decimal(30,0) (nullable = true) {code}
And because the python code call into this too it also has the same problem. I suspect that the java and R code also expose it too, but I didn't check.

 

 ",,apachespark,revans2,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 22 00:48:09 UTC 2022,,,,,,,,,,"0|z10mvk:",9223372036854775807,,,,,,,,,,,,,3.3.0,,,,,,,,,,"19/Mar/22 13:07;revans2;I marked this as a critical because it technically is a breaking change for all but the SQL APIs. I could see blocker because it could be considered data corruption, but only in a really loose definition of that.  Could someone assign this to me? I have a patch that fixes it. It is a very simple patch. I don't appear to have permission to assign to myself.;;;","19/Mar/22 13:17;apachespark;User 'revans2' has created a pull request for this issue:
https://github.com/apache/spark/pull/35913;;;","19/Mar/22 13:17;apachespark;User 'revans2' has created a pull request for this issue:
https://github.com/apache/spark/pull/35913;;;","21/Mar/22 14:33;apachespark;User 'revans2' has created a pull request for this issue:
https://github.com/apache/spark/pull/35925;;;","21/Mar/22 14:33;apachespark;User 'revans2' has created a pull request for this issue:
https://github.com/apache/spark/pull/35925;;;","22/Mar/22 00:48;gurwls223;Fixed in https://github.com/apache/spark/pull/35913;;;",,,,,,
Include unit into the sql string of TIMESTAMPADD/DIFF ,SPARK-38600,13434570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,18/Mar/22 12:01,18/Mar/22 18:02,13/Jul/23 08:47,18/Mar/22 18:02,3.3.0,3.4.0,,,,,,,3.3.0,,,,,SQL,,,,0,,,"After https://github.com/apache/spark/pull/35805, the sql method doesn't include unit anymore. The ticket aims to override the sql method in the TIMESTAMPADD and TIMESTAMPDIFF expressions, and prepend unit. ",,apachespark,maxgekk,,,,,,,,,,,,,,,,,SPARK-38509,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 18 18:02:01 UTC 2022,,,,,,,,,,"0|z10m68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/22 12:15;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/35911;;;","18/Mar/22 18:02;maxgekk;Issue resolved by pull request 35911
[https://github.com/apache/spark/pull/35911];;;",,,,,,,,,,
Enable Spark on K8S integration tests,SPARK-38597,13434539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,18/Mar/22 09:31,22/Jul/22 16:22,13/Jul/23 08:47,22/Jul/22 16:21,3.4.0,,,,,,,,3.4.0,,,,,Kubernetes,Project Infra,,,0,,,,,apachespark,dongjoon,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 22 16:21:28 UTC 2022,,,,,,,,,,"0|z10lzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/22 14:38;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35830;;;","18/Mar/22 14:38;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35830;;;","21/Jul/22 12:09;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37244;;;","22/Jul/22 16:21;dongjoon;Issue resolved by pull request 37244
[https://github.com/apache/spark/pull/37244];;;",,,,,,,,
Validating new location for rename command should use formatted names,SPARK-38587,13434326,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,17/Mar/22 10:17,23/Mar/22 09:39,13/Jul/23 08:47,23/Mar/22 09:39,3.0.3,3.1.2,3.2.1,3.3.0,,,,,3.0.4,3.1.3,3.2.2,3.3.0,,SQL,,,,0,,,"```
{code:java}
[info] - ALTER TABLE .. RENAME using V1 catalog V1 command: newName *** FAILED *** (61 milliseconds)
[info]   org.apache.spark.sql.catalyst.analysis.NoSuchDatabaseException: Database 'CaseUpperCaseLower' not found
[info]   at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.requireDbExists(ExternalCatalog.scala:42)
[info]   at org.apache.spark.sql.catalyst.catalog.ExternalCatalog.requireDbExists$(ExternalCatalog.scala:40)
[info]   at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.requireDbExists(InMemoryCatalog.scala:47)
[info]   at org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.getDatabase(InMemoryCatalog.scala:171)
[info]   at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getDatabase(ExternalCatalogWithListener.scala:65)
[info]   at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateNewLocationOfRename(SessionCatalog.scala:1863)
[info]   at org.apache.spark.sql.catalyst.catalog.SessionCatalog.renameTable(SessionCatalog.scala:739)
[info]   at org.apache.spark.sql.execution.command.AlterTableRenameCommand.run(tables.scala:209
[info]   at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
[info]   at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
[info]   at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
[info]   at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)
[info]   at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
[info]   at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
[info]   at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[info]   at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
[info]   at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)
[info]   at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:491)
[info]   at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:83)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:491)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[info]   at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[info]   at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
[info]   at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:467)
[info]   at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)
[info]   at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)
[info]   at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)
[info]   at org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)
[info]   at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[info]   at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)
[info]   at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)
[info]   at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
[info]   at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.$anonfun$sql$1(SQLTestUtils.scala:232)
[info]   at org.apache.spark.sql.execution.command.AlterTableRenameSuiteBase.$anonfun$$init$$19(AlterTableRenameSuiteBase.scala:143)
[info]   at org.apache.spark.sql.execution.command.AlterTableRenameSuiteBase.$anonfun$$init$$19$adapted(AlterTableRenameSuiteBase.scala:141)
[info]   at org.apache.spark.sql.execution.command.DDLCommandTestUtils.$anonfun$withNamespaceAndTable$2(DDLCommandTestUtils.scala:67)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1478)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withTable(SQLTestUtils.scala:306)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withTable$(SQLTestUtils.scala:304)
[info]   at org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite.withTable(AlterTableRenameSuite.scala:83)
[info]   at org.apache.spark.sql.execution.command.DDLCommandTestUtils.$anonfun$withNamespaceAndTable$1(DDLCommandTestUtils.scala:67)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1478)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withNamespace(SQLTestUtils.scala:389)
[info]   at org.apache.spark.sql.test.SQLTestUtilsBase.withNamespace$(SQLTestUtils.scala:387)
[info]   at org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite.withNamespace(AlterTableRenameSuite.scala:83)
[info]   at org.apache.spark.sql.execution.command.DDLCommandTestUtils.withNamespaceAndTable(DDLCommandTestUtils.scala:63)
[info]   at org.apache.spark.sql.execution.command.DDLCommandTestUtils.withNamespaceAndTable$(DDLCommandTestUtils.scala:60)
[info]   at org.apache.spark.sql.execution.command.v1.AlterTableRenameSuite.withNamespaceAndTable(AlterTableRenameSuite.scala:83)
[info]   at org.apache.spark.sql.execution.command.AlterTableRenameSuiteBase.$anonfun$$init$$18(AlterTableRenameSuiteBase.scala:141)
[info]   at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:190)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:203)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:188)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:200
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:200)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:182)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:64)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:64)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:233)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:233)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:232)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563)
[info]   at org.scalatest.Suite.run(Suite.scala:1112)
[info]   at org.scalatest.Suite.run$(Suite.scala:1094)
[info]   at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1563)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:237)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:535)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:237)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:236)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:64)
[info]   at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213)
[info]   at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210)
[info]   at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:64)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:318)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:513)
[info]   at sbt.ForkMain$Run.lambda$runTest$1(ForkMain.java:413)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
[info]   at java.lang.Thread.run(Thread.java:748)
{code}
",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 23 09:39:45 UTC 2022,,,,,,,,,,"0|z10ko0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/22 10:57;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/35895;;;","17/Mar/22 10:58;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/35895;;;","23/Mar/22 09:39;Qin Yao;resolved by https://github.com/apache/spark/pull/35895;;;",,,,,,,,,
Trigger notifying workflow in branch-3.3 and other future branches,SPARK-38586,13434317,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,17/Mar/22 09:36,12/Dec/22 18:10,13/Jul/23 08:47,17/Mar/22 09:45,3.3.0,,,,,,,,3.3.0,,,,,Project Infra,,,,0,,,See https://github.com/apache/spark/pull/35891,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 17 09:45:58 UTC 2022,,,,,,,,,,"0|z10km0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/22 09:38;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35891;;;","17/Mar/22 09:45;gurwls223;Fixed in https://github.com/apache/spark/pull/35891;;;",,,,,,,,,,
to_timestamp should allow numeric types,SPARK-38583,13434278,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,17/Mar/22 06:30,12/Dec/22 18:10,13/Jul/23 08:47,18/Mar/22 07:44,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"SPARK-38240 mistakenly disallowed numeric type at to_timestamp. We should allow it back:

{code}
spark.range(1).selectExpr(""to_timestamp(id)"").show()
{code}


*Before*

{code}
+-------------------+
|   to_timestamp(id)|
+-------------------+
|1970-01-01 09:00:00|
+-------------------+
{code}


*After*

{code}
+-----------------+
| to_timestamp(id)|
+-----------------+
|             null|
+-----------------+
{code}
",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 18 07:44:38 UTC 2022,,,,,,,,,,"0|z10kdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/22 07:02;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35887;;;","18/Mar/22 07:44;gurwls223;Issue resolved by pull request 35887
[https://github.com/apache/spark/pull/35887];;;",,,,,,,,,,
Requesting Restful API can cause NullPointerException,SPARK-38579,13434247,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yangyimin,yangyimin,yangyimin,17/Mar/22 02:37,22/Mar/22 10:26,13/Jul/23 08:47,22/Mar/22 10:26,3.1.0,3.1.1,3.1.2,3.2.0,3.2.1,,,,3.1.3,3.2.2,3.3.0,,,Web UI,,,,0,,,"When requesting Restful API  \{baseURL}/api/v1/applications/$appId/sql/$executionId which is introduced by this PR [https://github.com/apache/spark/pull/28208,] it can cause NullPointerException. The root cause is, when calling method doUpdate() of `LiveExecutionData`, `metricsValues` can be null. Then, when statement `printableMetrics(graph.allNodes, exec.metricValues)` is executed, it will throw  NullPointerException.",,apachespark,yangyimin,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 22 10:26:43 UTC 2022,,,,,,,,,,"0|z10k6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Mar/22 02:50;apachespark;User 'yym1995' has created a pull request for this issue:
https://github.com/apache/spark/pull/35884;;;","22/Mar/22 10:26;yumwang;Issue resolved by pull request 35884
[https://github.com/apache/spark/pull/35884];;;",,,,,,,,,,
Enable GitHub Action build_and_test on branch-3.3,SPARK-38567,13434090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,gurwls223,,16/Mar/22 11:24,12/Dec/22 18:10,13/Jul/23 08:47,16/Mar/22 11:50,3.3.0,,,,,,,,3.3.0,,,,,Project Infra,,,,0,,,See https://issues.apache.org/jira/browse/SPARK-35995,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 16 11:50:40 UTC 2022,,,,,,,,,,"0|z10j7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/22 11:38;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/35876;;;","16/Mar/22 11:50;gurwls223;Issue resolved by pull request 35876
[https://github.com/apache/spark/pull/35876];;;",,,,,,,,,,
Upgrade to Py4J 0.10.9.5,SPARK-38563,13434020,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,16/Mar/22 05:24,12/Dec/22 18:10,13/Jul/23 08:47,18/Mar/22 05:04,3.2.1,3.3.0,,,,,,,3.2.2,3.3.0,,,,PySpark,,,,0,,,"There is a resource leak bug, see https://github.com/py4j/py4j/pull/471. We should upgrade Py4J to 0.10.9.4 to fix this",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 18 05:04:38 UTC 2022,,,,,,,,,,"0|z10is0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/22 05:49;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35871;;;","16/Mar/22 05:50;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35871;;;","16/Mar/22 09:25;gurwls223;Issue resolved by pull request 35871
[https://github.com/apache/spark/pull/35871];;;","17/Mar/22 21:14;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35903;;;","17/Mar/22 21:14;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35903;;;","17/Mar/22 21:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35904;;;","18/Mar/22 01:00;dongjoon;This is reverted from `master` and `branch-3.3`.;;;","18/Mar/22 02:45;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35907;;;","18/Mar/22 05:04;gurwls223;Issue resolved by pull request 35907
[https://github.com/apache/spark/pull/35907];;;",,,
UnsafeHashedRelation should serialize numKeys out,SPARK-38542,13433511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,mcdull_zhang,mcdull_zhang,13/Mar/22 12:18,02/Sep/22 21:41,13/Jul/23 08:47,16/Mar/22 06:18,3.2.0,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,correctness,,"At present, UnsafeHashedRelation does not write out numKeys during serialization, so the numKeys of UnsafeHashedRelation obtained by deserialization is equal to 0. The numFields of UnsafeRows returned by UnsafeHashedRelation.keys() are all 0, which can lead to missing or incorrect data.

 

For example, in SubqueryBroadcastExec, the HashedRelation.keys() function is called.
{code:java}
val broadcastRelation = child.executeBroadcast[HashedRelation]().value
val (iter, expr) = if (broadcastRelation.isInstanceOf[LongHashedRelation]) {
  (broadcastRelation.keys(), HashJoin.extractKeyExprAt(buildKeys, index))
} else {
  (broadcastRelation.keys(),
    BoundReference(index, buildKeys(index).dataType, buildKeys(index).nullable))
}{code}
 ",,apachespark,cloud_fan,mcdull_zhang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 16 06:18:12 UTC 2022,,,,,,,,,,"0|z10fo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/22 13:00;apachespark;User 'mcdull-zhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35836;;;","16/Mar/22 06:18;cloud_fan;Issue resolved by pull request 35836
[https://github.com/apache/spark/pull/35836];;;",,,,,,,,,,
GeneratorNestedColumnAliasing does not work correctly for some expressions,SPARK-38530,13433426,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,miny,miny,miny,12/Mar/22 01:33,12/Dec/22 18:10,13/Jul/23 08:47,13/Apr/22 06:02,3.2.1,,,,,,,,3.3.0,,,,,Optimizer,,,,0,,,"[https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/NestedColumnAliasing.scala#L226]

The code to collect ExtractValue expressions is wrong. We should do it in a bottom up way instead of only check 2 levels. It can cause incorrect result if the expression looks like ExtractValue(ExtractValue(some_other_expr)).

 

An example to trigger the bug is:

 

input: <col1: array<struct<a: int, b: struct<a: struct<a: int, b: int>, b: int>>>>

 

Project(ExtractValue(ExtractValue(CaseWhen([col.a == 1, col.b]), ""a""), ""a"")

- Generate(Explode(col1))

 

We will try to incorrectly push down the whole expression into the input of the Explode, now the input of CaseWhen has array<...> as input so we will get wrong result.",,apachespark,cloud_fan,miny,viirya,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 13 06:02:03 UTC 2022,,,,,,,,,,"0|z10f5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/22 01:48;gurwls223;cc [~viirya] FYI. [~miny]mind providing a self-contained reproducer?;;;","14/Mar/22 15:45;miny;I have a fix for it, will put the test in the PR.;;;","16/Mar/22 01:07;apachespark;User 'minyyy' has created a pull request for this issue:
https://github.com/apache/spark/pull/35866;;;","13/Apr/22 06:02;cloud_fan;Issue resolved by pull request 35866
[https://github.com/apache/spark/pull/35866];;;",,,,,,,,
NullPointerException when selecting a generator in a Stream of aggregate expressions,SPARK-38528,13433419,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,12/Mar/22 00:11,12/Dec/22 18:10,13/Jul/23 08:47,14/Mar/22 02:03,3.1.3,3.2.1,3.3.0,,,,,,3.1.3,3.2.2,3.3.0,,,SQL,,,,0,,,"Assume this dataframe:
{noformat}
val df = Seq(1, 2, 3).toDF(""v"")
{noformat}
This works:
{noformat}
df.select(Seq(explode(array(min($""v""), max($""v""))), sum($""v"")): _*).collect
{noformat}
However, this doesn't:
{noformat}
df.select(Stream(explode(array(min($""v""), max($""v""))), sum($""v"")): _*).collect
{noformat}
It throws this error:
{noformat}
java.lang.NullPointerException
  at org.apache.spark.sql.catalyst.analysis.Analyzer$GlobalAggregates$.$anonfun$containsAggregates$1(Analyzer.scala:2516)
  at scala.collection.immutable.List.flatMap(List.scala:366)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$GlobalAggregates$.containsAggregates(Analyzer.scala:2515)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$GlobalAggregates$$anonfun$apply$31.applyOrElse(Analyzer.scala:2509)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$GlobalAggregates$$anonfun$apply$31.applyOrElse(Analyzer.scala:2508)
{noformat}
The only difference between the two queries is that the first one uses {{Seq}} to specify the varargs, whereas the second one uses {{Stream}}.",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 14 17:08:16 UTC 2022,,,,,,,,,,"0|z10f3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/22 00:24;bersprockets;This is a bug in {{ExtractGenerator}} in which an array ({{{}projectExprs{}}}) is updated from within a closure passed to a map operation (the array is external to the closure). If the sequence of expressions on which the map operation is called is a {{{}Stream{}}}, the map operation is evaluated lazily, so the array is not fully updated before the rule completes.;;;","13/Mar/22 23:40;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35837;;;","14/Mar/22 02:03;gurwls223;Fixed in https://github.com/apache/spark/pull/35837;;;","14/Mar/22 17:08;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35851;;;",,,,,,,,
fix misleading function alias name for RuntimeReplaceable,SPARK-38526,13433361,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,11/Mar/22 14:26,12/Mar/22 05:38,13/Jul/23 08:47,12/Mar/22 05:38,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,,,apachespark,cloud_fan,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 12 05:38:39 UTC 2022,,,,,,,,,,"0|z10eqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/22 14:53;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/35821;;;","12/Mar/22 05:38;dongjoon;Issue resolved by pull request 35821
[https://github.com/apache/spark/pull/35821];;;",,,,,,,,,,
Failure on referring to the corrupt record from CSV,SPARK-38523,13433347,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,maxgekk,maxgekk,11/Mar/22 12:41,14/Mar/22 06:36,13/Jul/23 08:47,14/Mar/22 06:11,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"The file below has a invalid value in a field:
{code:java}
0,2013-111_11 12:13:14
1,1983-08-04 {code}
where the timestamp 2013-111_11 12:13:14 is incorrect.

The query fails when it refers to the corrupt record column:
{code:java}
spark.read.format(""csv"")
 .option(""header"", ""true"")
 .schema(schema)
 .load(""csv_corrupt_record.csv"")
 .filter($""_corrupt_record"".isNotNull) {code}
with the exception:
{code:java}
org.apache.spark.sql.AnalysisException: 
Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the
referenced columns only include the internal corrupt record column
(named _corrupt_record by default). For example:
spark.read.schema(schema).csv(file).filter($""_corrupt_record"".isNotNull).count()
and spark.read.schema(schema).csv(file).select(""_corrupt_record"").show().
Instead, you can cache or save the parsed results and then send the same query.
For example, val df = spark.read.schema(schema).csv(file).cache() and then
df.filter($""_corrupt_record"".isNotNull).count().
      
    at org.apache.spark.sql.errors.QueryCompilationErrors$.queryFromRawFilesIncludeCorruptRecordColumnError(QueryCompilationErrors.scala:2047)
    at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:116) {code}",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 14 06:36:00 UTC 2022,,,,,,,,,,"0|z10ens:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/22 12:57;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/35817;;;","14/Mar/22 06:11;cloud_fan;Issue resolved by pull request 35817
[https://github.com/apache/spark/pull/35817];;;","14/Mar/22 06:36;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/35844;;;",,,,,,,,,
Throw Exception if overwriting hive partition table with dynamic and staticPartitionOverwriteMode,SPARK-38521,13433294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jackey Lee,Jackey Lee,Jackey Lee,11/Mar/22 07:37,18/Mar/22 05:18,13/Jul/23 08:47,15/Mar/22 03:17,3.4.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,"The `spark.sql.sources.partitionOverwriteMode` allows us to overwrite the existing data of the table through staticmode, but for hive table, it is disastrous. It may deleting all data in hive partitioned table while writing with dynamic overwrite and `partitionOverwriteMode=STATIC`.

Here we add a check for this and throw Exception if this happends.",,apachespark,dongjoon,Jackey Lee,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 15 16:05:29 UTC 2022,,,,,,,,,,"0|z10ec0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/22 07:54;apachespark;User 'jackylee-ch' has created a pull request for this issue:
https://github.com/apache/spark/pull/35815;;;","14/Mar/22 04:07;apachespark;User 'jackylee-ch' has created a pull request for this issue:
https://github.com/apache/spark/pull/35843;;;","15/Mar/22 03:17;dongjoon;Issue resolved by pull request 35843
[https://github.com/apache/spark/pull/35843];;;","15/Mar/22 16:05;apachespark;User 'jackylee-ch' has created a pull request for this issue:
https://github.com/apache/spark/pull/35862;;;",,,,,,,,
Fix PySpark documentation generation (missing ipython_genutils),SPARK-38517,13433244,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,11/Mar/22 01:53,12/Dec/22 18:10,13/Jul/23 08:47,11/Mar/22 02:36,3.2.1,3.3.0,,,,,,,3.2.2,3.3.0,,,,Project Infra,,,,0,,,"{code}
Extension error:
Could not import extension nbsphinx (exception: No module named 'ipython_genutils')
make: *** [Makefile:35: html] Error 2
                    ------------------------------------------------
      Jekyll 4.2.1   Please append `--trace` to the `build` command 
                     for any additional information or backtrace. 
                    ------------------------------------------------
/__w/spark/spark/docs/_plugins/copy_api_dirs.rb:130:in `<top (required)>': Python doc generation failed (RuntimeError)
	from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/external.rb:60:in `require'
	from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/external.rb:60:in `block in require_with_graceful_fail'
	from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/external.rb:57:in `each'
	from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/external.rb:57:in `require_with_graceful_fail'
	from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/plugin_manager.rb:89:in `block in require_plugin_files'
	from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/plugin_manager.rb:87:in `each'
	from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/plugin_manager.rb:87:in `require_plugin_files'
	from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/plugin_manager.rb:21:in `conscientious_require'
	from /__w/spark/spark/docs/.local_ruby_bundle/ruby/2.7.0/gems/jekyll-4.2.1/lib/jekyll/site.rb:131:in `setup'
{code}

https://github.com/apache/spark/runs/5504729423?check_suite_focus=true",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 11 02:36:17 UTC 2022,,,,,,,,,,"0|z10e14:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/22 02:00;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35812;;;","11/Mar/22 02:01;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35812;;;","11/Mar/22 02:36;gurwls223;Issue resolved by pull request 35812
[https://github.com/apache/spark/pull/35812];;;",,,,,,,,,
"Add log4j-core, log4j-api and log4j-slf4j-impl to classpath if active hadoop-provided",SPARK-38516,13433243,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,yumwang,yumwang,11/Mar/22 01:45,12/Mar/22 05:36,13/Jul/23 08:47,12/Mar/22 05:36,3.3.0,,,,,,,,3.3.0,,,,,Build,,,,0,,,"{noformat}
Error: A JNI error has occurred, please check your installation and try again
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/core/Filter
    at java.lang.Class.getDeclaredMethods0(Native Method)
    at java.lang.Class.privateGetDeclaredMethods(Class.java:2701)
    at java.lang.Class.privateGetMethodRecursive(Class.java:3048)
    at java.lang.Class.getMethod0(Class.java:3018)
    at java.lang.Class.getMethod(Class.java:1784)
    at sun.launcher.LauncherHelper.validateMainClass(LauncherHelper.java:544)
    at sun.launcher.LauncherHelper.checkAndLoadMain(LauncherHelper.java:526)
Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.core.Filter
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 7 more{noformat}

{noformat}
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/logging/log4j/LogManager
	at org.apache.spark.deploy.yarn.SparkRackResolver.<init>(SparkRackResolver.scala:42)
	at org.apache.spark.deploy.yarn.SparkRackResolver$.get(SparkRackResolver.scala:114)
	at org.apache.spark.scheduler.cluster.YarnScheduler.<init>(YarnScheduler.scala:31)
	at org.apache.spark.scheduler.cluster.YarnClusterManager.createTaskScheduler(YarnClusterManager.scala:35)
	at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2985)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:563)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2704)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:953)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:947)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:54)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.<init>(SparkSQLCLIDriver.scala:327)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:159)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:958)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1046)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1055)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: org.apache.logging.log4j.LogManager
	at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
	... 26 more
{noformat}",,apachespark,dongjoon,xkrogen,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 12 05:36:33 UTC 2022,,,,,,,,,,"0|z10e0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/22 01:53;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/35811;;;","11/Mar/22 01:53;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/35811;;;","12/Mar/22 05:36;dongjoon;Issue resolved by pull request 35811
[https://github.com/apache/spark/pull/35811];;;",,,,,,,,,
Failure fetching JSON representation of Spark plans with Hive UDFs,SPARK-38510,13433204,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,shardulm,shardulm,shardulm,10/Mar/22 19:54,19/Mar/22 15:13,13/Jul/23 08:47,19/Mar/22 15:13,3.3.0,,,,,,,,3.4.0,,,,,SQL,,,,0,,,"Repro:
{code:java}
scala> spark.sql(""CREATE TEMPORARY FUNCTION test_udf AS 'org.apache.hadoop.hive.ql.udf.generic.GenericUDFAesEncrypt'"")


scala> spark.sql(""SELECT test_udf('a', 'b')"").queryExecution.analyzed.toJSON
scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving class InterfaceAudience
java.lang.RuntimeException: error reading Scala signature of org.apache.spark.sql.hive.HiveGenericUDF: illegal cyclic reference involving class InterfaceAudience
  at scala.reflect.internal.pickling.UnPickler.unpickle(UnPickler.scala:51)
  at scala.reflect.runtime.JavaMirrors$JavaMirror.unpickleClass(JavaMirrors.scala:660)
  at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.$anonfun$complete$2(SymbolLoaders.scala:37)
  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
  at scala.reflect.internal.SymbolTable.slowButSafeEnteringPhaseNotLaterThan(SymbolTable.scala:333)
  at scala.reflect.runtime.SymbolLoaders$TopClassCompleter.complete(SymbolLoaders.scala:34)
  at scala.reflect.internal.Symbols$Symbol.completeInfo(Symbols.scala:1551)
  at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1514)
  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$7.scala$reflect$runtime$SynchronizedSymbols$SynchronizedSymbol$$super$info(SynchronizedSymbols.scala:203)
  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.$anonfun$info$1(SynchronizedSymbols.scala:158)
  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info(SynchronizedSymbols.scala:149)
  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol.info$(SynchronizedSymbols.scala:158)
  at scala.reflect.runtime.SynchronizedSymbols$SynchronizedSymbol$$anon$7.info(SynchronizedSymbols.scala:203)
  at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1698)
  at scala.reflect.internal.Symbols$SymbolContextApiImpl.selfType(Symbols.scala:151)
  at scala.reflect.internal.Symbols$ClassSymbol.selfType(Symbols.scala:3287)
  at org.apache.spark.sql.catalyst.ScalaReflection$.getConstructorParameterNames(ScalaReflection.scala:656)
  at org.apache.spark.sql.catalyst.trees.TreeNode.jsonFields(TreeNode.scala:1019)
  at org.apache.spark.sql.catalyst.trees.TreeNode.collectJsonValue$1(TreeNode.scala:1009)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$jsonValue$1(TreeNode.scala:1011)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$jsonValue$1$adapted(TreeNode.scala:1011)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at org.apache.spark.sql.catalyst.trees.TreeNode.collectJsonValue$1(TreeNode.scala:1011)
  at org.apache.spark.sql.catalyst.trees.TreeNode.jsonValue(TreeNode.scala:1014)
  at org.apache.spark.sql.catalyst.trees.TreeNode.parseToJson(TreeNode.scala:1057)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$parseToJson$11(TreeNode.scala:1063)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode.parseToJson(TreeNode.scala:1063)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$jsonFields$2(TreeNode.scala:1033)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.catalyst.trees.TreeNode.jsonFields(TreeNode.scala:1024)
  at org.apache.spark.sql.catalyst.trees.TreeNode.collectJsonValue$1(TreeNode.scala:1009)
  at org.apache.spark.sql.catalyst.trees.TreeNode.jsonValue(TreeNode.scala:1014)
  at org.apache.spark.sql.catalyst.trees.TreeNode.toJSON(TreeNode.scala:1000)
  ... 47 elided
{code}
This issue is due to [bug#12190 in Scala|https://github.com/scala/bug/issues/12190] which does not handle cyclic references in Java annotations correctly. The cyclic reference in this case comes from InterfaceAudience annotation which [annotates itself|https://github.com/apache/hadoop/blob/db8ae4b65448c506c9234641b2c1f9b8e894dc18/hadoop-common-project/hadoop-annotations/src/main/java/org/apache/hadoop/classification/InterfaceAudience.java#L45]. This annotation class is present in the type hierarchy of {{{}HiveGenericUDF{}}}.

A simple workaround for this issue, is to just retry the operation. It will succeed on the retry probably because the annotation is partially resolved from the previous attempt.",,apachespark,shardulm,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 19 15:13:07 UTC 2022,,,,,,,,,,"0|z10ds8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/22 18:18;apachespark;User 'shardulm94' has created a pull request for this issue:
https://github.com/apache/spark/pull/35852;;;","14/Mar/22 18:18;apachespark;User 'shardulm94' has created a pull request for this issue:
https://github.com/apache/spark/pull/35852;;;","19/Mar/22 15:13;srowen;Issue resolved by pull request 35852
[https://github.com/apache/spark/pull/35852];;;",,,,,,,,,
Add warn for getAdditionalPreKubernetesResources in executor side,SPARK-38503,13433104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,10/Mar/22 13:02,10/Aug/22 17:45,13/Jul/23 08:47,10/Aug/22 17:45,3.3.0,,,,,,,,3.4.0,,,,,Kubernetes,,,,0,,,,,apachespark,dongjoon,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 10 17:45:34 UTC 2022,,,,,,,,,,"0|z10d60:",9223372036854775807,,,,,,,,,,,,,3.4.0,,,,,,,,,,"10/Mar/22 13:04;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35786;;;","10/Mar/22 13:04;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35786;;;","10/Aug/22 17:45;dongjoon;Issue resolved by pull request 35786
[https://github.com/apache/spark/pull/35786];;;",,,,,,,,,
Spark doc build not work on Mac OS M1,SPARK-38488,13432977,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,10/Mar/22 02:26,12/Dec/22 18:11,13/Jul/23 08:47,22/Mar/22 00:46,3.3.0,3.4.0,,,,,,,3.4.0,,,,,Project Infra,,,,0,,," 
{code:java}
diff --git a/docs/.bundle/config b/docs/.bundle/config
index b13821f801..68c1ee493a 100644
--- a/docs/.bundle/config
+++ b/docs/.bundle/config
@@ -1,2 +1,3 @@
 ---
 BUNDLE_PATH: "".local_ruby_bundle""
+BUNDLE_BUILD__FFI: ""--enable-libffi-alloc""
diff --git a/docs/Gemfile b/docs/Gemfile
index f991622708..6c35201296 100644
--- a/docs/Gemfile
+++ b/docs/Gemfile
@@ -17,6 +17,7 @@
 source ""https://rubygems.org""
+gem ""ffi"", ""1.15.5""
 gem ""jekyll"", ""4.2.1""
 gem ""rouge"", ""3.26.0""
 gem ""jekyll-redirect-from"", ""0.16.0""
{code}
After above patch redo `bundle install`, then it works, you could see this as ref if you meet the same issue.

will take a deep look to solve this.

 

related: https://github.com/ffi/ffi/issues/864",,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 22 00:46:57 UTC 2022,,,,,,,,,,"0|z10cds:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/22 02:48;gurwls223;[~yikunkero] please go ahead for a PR.;;;","10/Mar/22 03:30;yikunkero;[~hyukjin.kwon] Sure, let me check and test it both in x86/arm64;;;","21/Mar/22 03:53;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35918;;;","22/Mar/22 00:46;gurwls223;Issue resolved by pull request 35918
[https://github.com/apache/spark/pull/35918];;;",,,,,,,,
Fix always false condition in LogDivertAppender#initLayout ,SPARK-38458,13432737,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,09/Mar/22 03:11,09/Mar/22 06:14,13/Jul/23 08:47,09/Mar/22 06:14,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,," 
{code:java}
  private static StringLayout initLayout(OperationLog.LoggingLevel loggingMode) {
   ...
    for (Map.Entry<String, Appender> entry : appenders.entrySet()) {
      Appender ap = entry.getValue();
      if (ap.getClass().equals(ConsoleAppender.class)) {
        Layout l = ap.getLayout();
        if (l.getClass().equals(StringLayout.class)) {
          layout = (StringLayout) l;
          break;
        }
      }
    }
    return getLayout(isVerbose, layout);
  } {code}
 

`l.getClass().equals(StringLayout.class)` is always return false because `StringLayout` is a interface

 

 

 ",,apachespark,LuciferYang,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 09 06:14:28 UTC 2022,,,,,,,,,,"0|z10awo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/22 03:47;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35780;;;","09/Mar/22 06:14;viirya;Issue resolved by pull request 35780
[https://github.com/apache/spark/pull/35780];;;",,,,,,,,,,
Deadlock between ExecutorClassLoader and FileDownloadCallback caused by Log4j,SPARK-38446,13432599,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,08/Mar/22 09:37,27/Apr/22 08:01,13/Jul/23 08:47,03/Apr/22 17:31,3.0.3,3.1.2,3.2.1,,,,,,3.1.3,3.2.2,3.3.0,,,Spark Core,,,,0,,,"
{code:java}
files-client-8-1
PRIORITY : 5

THREAD ID : 0X00007FBFFC5EE000

NATIVE ID : 0X14903

NATIVE ID (DECIMAL) : 84227

STATE : BLOCKED


stackTrace:
java.lang.Thread.State: BLOCKED (on object monitor)
at java.lang.ClassLoader.loadClass(ClassLoader.java:398)
- waiting to lock <0x00000003c0753f88> (a org.apache.spark.repl.ExecutorClassLoader)
at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
at org.apache.logging.log4j.util.LoaderUtil.loadClass(LoaderUtil.java:169)
at org.apache.logging.log4j.core.impl.ThrowableProxyHelper.loadClass(ThrowableProxyHelper.java:214)
at org.apache.logging.log4j.core.impl.ThrowableProxyHelper.toExtendedStackTrace(ThrowableProxyHelper.java:112)
at org.apache.logging.log4j.core.impl.ThrowableProxy.(ThrowableProxy.java:113)
at org.apache.logging.log4j.core.impl.ThrowableProxy.(ThrowableProxy.java:97)
at org.apache.logging.log4j.core.impl.Log4jLogEvent.getThrownProxy(Log4jLogEvent.java:629)
at org.apache.logging.log4j.core.pattern.ExtendedThrowablePatternConverter.format(ExtendedThrowablePatternConverter.java:63)
at org.apache.logging.log4j.core.layout.PatternLayout$NoFormatPatternSerializer.toSerializable(PatternLayout.java:342)
at org.apache.logging.log4j.core.layout.PatternLayout.toText(PatternLayout.java:240)
at org.apache.logging.log4j.core.layout.PatternLayout.encode(PatternLayout.java:225)
at org.apache.logging.log4j.core.layout.PatternLayout.encode(PatternLayout.java:59)
at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.directEncodeEvent(AbstractOutputStreamAppender.java:215)
at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.tryAppend(AbstractOutputStreamAppender.java:208)
at org.apache.logging.log4j.core.appender.AbstractOutputStreamAppender.append(AbstractOutputStreamAppender.java:199)
at org.apache.logging.log4j.core.config.AppenderControl.tryCallAppender(AppenderControl.java:161)
at org.apache.logging.log4j.core.config.AppenderControl.callAppender0(AppenderControl.java:134)
at org.apache.logging.log4j.core.config.AppenderControl.callAppenderPreventRecursion(AppenderControl.java:125)
at org.apache.logging.log4j.core.config.AppenderControl.callAppender(AppenderControl.java:89)
at org.apache.logging.log4j.core.config.LoggerConfig.callAppenders(LoggerConfig.java:675)
at org.apache.logging.log4j.core.config.LoggerConfig.processLogEvent(LoggerConfig.java:633)
at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:616)
at org.apache.logging.log4j.core.config.LoggerConfig.log(LoggerConfig.java:552)
at org.apache.logging.log4j.core.config.AwaitCompletionReliabilityStrategy.log(AwaitCompletionReliabilityStrategy.java:82)
at org.apache.logging.log4j.core.Logger.log(Logger.java:161)
at org.apache.logging.log4j.spi.AbstractLogger.tryLogMessage(AbstractLogger.java:2205)
at org.apache.logging.log4j.spi.AbstractLogger.logMessageTrackRecursion(AbstractLogger.java:2159)
at org.apache.logging.log4j.spi.AbstractLogger.logMessageSafely(AbstractLogger.java:2142)
at org.apache.logging.log4j.spi.AbstractLogger.logMessage(AbstractLogger.java:2017)
at org.apache.logging.log4j.spi.AbstractLogger.logIfEnabled(AbstractLogger.java:1983)
at org.apache.logging.slf4j.Log4jLogger.debug(Log4jLogger.java:139)
at org.apache.spark.internal.Logging.logDebug(Logging.scala:82)
at org.apache.spark.internal.Logging.logDebug$(Logging.scala:81)
at org.apache.spark.rpc.netty.NettyRpcEnv.logDebug(NettyRpcEnv.scala:45)
at org.apache.spark.rpc.netty.NettyRpcEnv$FileDownloadCallback.onFailure(NettyRpcEnv.scala:454)
at org.apache.spark.network.client.TransportResponseHandler.handle(TransportResponseHandler.java:260)
{code}

while the class loading lock 0x00000003c0753f88 is locked by ExecutorClassLoader who‘s downloading remote classes/jars though it.
",,apachespark,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 03 17:31:38 UTC 2022,,,,,,,,,,"0|z10a28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/22 10:00;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/35765;;;","03/Apr/22 17:31;dongjoon;Issue resolved by pull request 35765
[https://github.com/apache/spark/pull/35765];;;",,,,,,,,,,
Fix `test_ceil` to test `ceil`,SPARK-38436,13432453,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,bjornjorgensen,bjornjorgensen,bjornjorgensen,07/Mar/22 14:34,17/Jul/22 05:47,13/Jul/23 08:47,07/Mar/22 19:03,3.3.0,,,,,,,,3.2.2,3.3.0,,,,PySpark,Tests,,,0,,,"In python/pyspark/pandas/tests/test_series_datetime.py line 262 



{code:java}
    def test_floor(self):
        self.check_func(lambda x: x.dt.floor(freq=""min""))
        self.check_func(lambda x: x.dt.floor(freq=""H""))

    def test_ceil(self):
        self.check_func(lambda x: x.dt.floor(freq=""min""))
        self.check_func(lambda x: x.dt.floor(freq=""H""))
{code}

Change  x.dt.floor to  x.dt.ceil 
",,apachespark,bjornjorgensen,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 07 19:03:07 UTC 2022,,,,,,,,,,"0|z1095s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/22 14:43;apachespark;User 'bjornjorgensen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35755;;;","07/Mar/22 14:43;apachespark;User 'bjornjorgensen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35755;;;","07/Mar/22 19:03;dongjoon;Issue resolved by pull request 35755
[https://github.com/apache/spark/pull/35755];;;",,,,,,,,,
Change day to month ,SPARK-38416,13432078,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,bjornjorgensen,bjornjorgensen,bjornjorgensen,04/Mar/22 19:35,12/Dec/22 18:10,13/Jul/23 08:47,07/Mar/22 00:01,3.3.0,,,,,,,,3.2.2,3.3.0,,,,PySpark,Tests,,,0,,,"In spark\python\pyspark\pandas\tests\indexes\test_datetime.py
line 117

{code:java}
   def test_day_name(self):
        for psidx, pidx in self.idx_pairs:
            self.assert_eq(psidx.day_name(), pidx.day_name())

    def test_month_name(self):
        for psidx, pidx in self.idx_pairs:
            self.assert_eq(psidx.day_name(), pidx.day_name())
{code}

Both of this functions are doing the same ting. 

",,apachespark,bjornjorgensen,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 07 00:01:04 UTC 2022,,,,,,,,,,"0|z106v4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/22 08:48;yikunkero;Yep, this might be a bad copy paste, are you going to submit a PR fix it?

[https://github.com/Yikun/spark/commit/c8cec8373fd63ce87293977be8fc3bda3cbb3b92]

I tested it in my local env it passed.;;;","06/Mar/22 14:00;apachespark;User 'bjornjorgensen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35741;;;","07/Mar/22 00:01;gurwls223;Issue resolved by pull request 35741
[https://github.com/apache/spark/pull/35741];;;",,,,,,,,,
Invalid call to exprId,SPARK-38413,13431888,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,liujintao,liujintao,04/Mar/22 08:07,12/Dec/22 18:10,13/Jul/23 08:47,07/Mar/22 10:56,2.4.4,,,,,,,,,,,,,SQL,,,,0,,,"java.sql.SQLException: org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to exprId on unresolved object, tree: 'c_column_2
java.lang.RuntimeException: java.sql.SQLException: org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to exprId on unresolved object, tree: 'c_column_2
    at com.bonc.ds.xquery.explorev2.stephandler.utils.HiveJdbcExecutor.createTableWithSql(HiveJdbcExecutor.java:602)
    at com.bonc.ds.xquery.explorev2.stephandler.operator.AbstractOperator.execute(AbstractOperator.java:183)

I see that this issue is fixed by 2.4.0.
I have no idea, i use hadoop3.1.4 spark 2.4.4",,liujintao,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 07 09:40:55 UTC 2022,,,,,,,,,,"0|z105p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/22 02:06;gurwls223;How do you reproduce this? Also Spark 2.4 is EOL. Can you see if it works w/ Spark 3+?;;;","07/Mar/22 09:34;liujintao;create view step_29100348500  as

            select c_user_id, c_column_1, c_column_2, c_column_3, c_column_4, c_column_5,

                        c_column_6, c_column_7, c_column_8, c_column_9, c_column_10, c_column_11,

                       c_column_12, c_column_13, c_column_14, c_column_15, c_column_16, c_column_17,

                       c_column_18, c_column_19, c_column_20, c_column_21, c_column_22, c_column_23,

                      c_column_24, c_column_25, c_column_26, c_column_27, c_column_28, c_column_29,

                       c_column_30, c_target, cast(abs( c_column_2) as bigint) c_60767

          from step_29100328097.

This is my sql,I only  support  2.4.

[~hyukjin.kwon] ;;;","07/Mar/22 09:40;gurwls223;It would be great to make it self-contained. Also, we won't likely fix it in 2.X because it's EOL. it would be good to check if the same exists in 3+ too.;;;",,,,,,,,,
`from` and `to` is swapped in the StateSchemaCompatibilityChecker,SPARK-38412,13431860,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,04/Mar/22 06:06,09/Mar/22 02:38,13/Jul/23 08:47,09/Mar/22 02:38,3.1.2,3.2.1,3.3.0,,,,,,3.1.4,3.2.2,3.3.0,,,Structured Streaming,,,,0,,,"During the work in SPARK-38204, I figured out the parameter is swapped which led to test failure on new test (I disabled the schema check for now in the PR of SPARK-38204).

That allows nullable column to be stored into non-nullable column, which should be prohibited. This is less likely making runtime problem since state schema is conceptual one and row can be stored even not respecting the state schema.

The worse problem is happening in opposite way, that disallows non-nullable column to be stored into nullable column, which should be allowed. Spark fails the query for this case.

We should fix this to allow the case properly.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 09 02:38:59 UTC 2022,,,,,,,,,,"0|z105iw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/22 06:34;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/35731;;;","09/Mar/22 02:38;kabhwan;Issue resolved via https://github.com/apache/spark/pull/35731;;;",,,,,,,,,,
Use UTF-8 when doMergeApplicationListingInternal reads event logs,SPARK-38411,13431853,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,,04/Mar/22 04:35,24/Nov/22 00:38,13/Jul/23 08:47,06/Mar/22 23:49,3.0.3,3.1.3,3.2.1,3.3.0,,,,,3.0.4,3.1.3,3.2.2,3.3.0,,Spark Core,,,,0,,,"After SPARK-29160, we should always use UTF-8 to read event log",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,SPARK-29160,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 06 23:49:11 UTC 2022,,,,,,,,,,"0|z105hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/22 04:41;apachespark;User 'pan3793' has created a pull request for this issue:
https://github.com/apache/spark/pull/35730;;;","06/Mar/22 23:49;dongjoon;Issue resolved by pull request 35730
[https://github.com/apache/spark/pull/35730];;;",,,,,,,,,,
Spark does not find CTE inside nested CTE,SPARK-38404,13431645,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,petertoth,watxaut,watxaut,03/Mar/22 07:48,06/Sep/22 04:37,13/Jul/23 08:47,20/Apr/22 16:44,3.2.0,3.2.1,,,,,,,3.3.1,3.4.0,,,,SQL,,,,1,,,"Hello! 

Seems that when defining CTEs and using them inside another CTE in Spark SQL, Spark thinks the inner call for the CTE is a table or view, which is not found and then it errors with `Table or view not found: <CTE name>`
h3. Steps to reproduce
 # `pip install pyspark==3.2.0` (also happens with 3.2.1)
 # start pyspark console by typing `pyspark` in the terminal
 # Try to run the following SQL with `spark.sql(sql)`

 
{code:java}
  WITH mock_cte__users    AS (
           SELECT 1 AS id
       ),
       model_under_test          AS (
             WITH users    AS (
                      SELECT *
                        FROM mock_cte__users
                  )
           SELECT *
             FROM users
       )
SELECT *
  FROM model_under_test;{code}
Spark will fail with 

 
{code:java}
pyspark.sql.utils.AnalysisException: Table or view not found: mock_cte__users; line 8 pos 29; {code}
I don't know if this is a regression or an expected behavior of the new 3.2.* versions. This fix introduced in 3.2.0 might be related: https://issues.apache.org/jira/browse/SPARK-36447

 

 ","Tested on:
 * MacOS Monterrey 12.2.1 (21D62)
 * python 3.9.10
 * pip 22.0.3
 * pyspark 3.2.0 & 3.2.1 (SQL query does not work) and pyspark 3.0.1 and 3.1.3 (SQL query works)",apachespark,cloud_fan,jarraj,petertoth,watxaut,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,Thu Sep 01 15:16:47 UTC 2022,,,,,,,,,,"0|z10474:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Apr/22 10:06;jarraj;I've experienced the same bug in Databricks. I've also confirmed that the issue can be reproduced with the _spark-sql_ tool from Spark 3.2.1 (from [https://spark.apache.org/downloads.html]), but the issue does not exist in Spark 3.1.2.

This issue causes serious problems for me, as I'm using _dbt_ macros to augment SQL queries and flattening nested CTEs is not an uncomplicated workaround.

Setting *spark.sql.legacy.ctePrecedencePolicy = CORRECTED* does not seem to affect this behaviour.;;;","11/Apr/22 17:37;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/36146;;;","11/Apr/22 17:38;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/36146;;;","20/Apr/22 16:44;cloud_fan;Issue resolved by pull request 36146
[https://github.com/apache/spark/pull/36146];;;","01/Sep/22 15:16;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37760;;;","01/Sep/22 15:16;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/37760;;;",,,,,,
`running-on-kubernetes` page render bad in v3.2.1(latest) website,SPARK-38403,13431641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,yikunkero,yikunkero,03/Mar/22 07:32,03/Mar/22 09:11,13/Jul/23 08:47,03/Mar/22 09:11,3.2.1,,,,,,,,,,,,,Documentation,,,,0,,,"Looks like the `running-on-kubernetes` page encounterd some problems when published.

[1] https://spark.apache.org/docs/latest/running-on-kubernetes.html#spark-properties

(You can see bad format after #spark-properties)
- I also check the master branch (setup local env) and also v3.2.0 (https://spark.apache.org/docs/3.2.0/running-on-kubernetes.html) it works well, .- But for v3.2.1 tag, I couldn't install doc deps due to deps conflict.
I'm not very familiar with doc infra tool. Can anyone help to take a look?",,Qin Yao,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 03 09:11:48 UTC 2022,,,,,,,,,,"0|z10468:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/22 07:41;Qin Yao;I guess it has been fixed via https://github.com/apache/spark/commit/3a179d762602243497c528bea3b3370e7548fa2d;;;","03/Mar/22 09:10;yikunkero;Ah, yes, it's this fix, I also do a compare validation on my env.;;;","03/Mar/22 09:11;yikunkero;master:

[https://github.com/apache/spark/pull/35572]

3.2 backport:

https://github.com/apache/spark/commit/bd2851b809f3037c29a955a79445eb409dafd9ba;;;",,,,,,,,,
build of spark sql against hadoop-3.4.0-snapshot failing with bouncycastle classpath error,SPARK-38394,13431531,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,stevel@apache.org,stevel@apache.org,stevel@apache.org,02/Mar/22 16:23,07/Mar/22 11:59,13/Jul/23 08:47,07/Mar/22 01:23,3.3.0,,,,,,,,3.3.0,,,,,Build,,,,0,,,"builidng spark master with {{-Dhadoop.version=3.4.0-SNAPSHOT}} and a local hadoop build breaks in the sbt compiler plugin


{code}
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.3.0:testCompile (scala-test-compile-first) on project spark-sql_2.12: Execution scala-test-compile-first of goal net.alchim31.maven:scala-maven-plugin:4.3.0:testCompile failed: A required class was missing while executing net.alchim31.maven:scala-maven-plugin:4.3.0:testCompile: org/bouncycastle/jce/provider/BouncyCastleProvider
[ERROR] -----------------------------------------------------
[ERROR] realm =    plugin>net.alchim31.maven:scala-maven-plugin:4.3.0

{code}

* this is the classpath of the sbt compiler
* hadoop hasn't been doing anything related to bouncy castle.

setting scala-maven-plugin to 3.4.0 makes this go away, i.e. reapplying SPARK-36547

the implication here is that the plugin version is going to have to be configured in different profiles.


",,apachespark,stevel@apache.org,,,,,,,,,,,,,,,,,,,,SPARK-36547,SPARK-33512,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 07 11:59:27 UTC 2022,,,,,,,,,,"0|z103i8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/22 18:32;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/35725;;;","03/Mar/22 18:33;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/35725;;;","07/Mar/22 01:23;srowen;Issue resolved by pull request 35725
[https://github.com/apache/spark/pull/35725];;;","07/Mar/22 11:59;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/35753;;;","07/Mar/22 11:59;apachespark;User 'steveloughran' has created a pull request for this issue:
https://github.com/apache/spark/pull/35753;;;",,,,,,,
Fix Kubernetes Client mode when mounting persistent volume with storage class,SPARK-38379,13431361,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tgraves,tgraves,tgraves,01/Mar/22 20:31,17/Jul/22 05:44,13/Jul/23 08:47,10/Mar/22 05:08,3.2.1,,,,,,,,3.2.2,,,,,Kubernetes,,,,0,,,"I'm using Spark 3.2.1 on a kubernetes cluster and starting a spark-shell in client mode.  I'm using persistent local volumes to mount nvme under /data in the executors and on startup the driver always throws the warning below.

using these options:

--conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.claimName=OnDemand \
     --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.storageClass=fast-disks \
     --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.options.sizeLimit=500Gi \
     --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.path=/data \
     --conf spark.kubernetes.executor.volumes.persistentVolumeClaim.spark-local-dir-1.mount.readOnly=false

 

 
{code:java}
22/03/01 20:21:22 WARN ExecutorPodsSnapshotsStoreImpl: Exception when notifying snapshot subscriber.
java.util.NoSuchElementException: spark.app.id
        at org.apache.spark.SparkConf.$anonfun$get$1(SparkConf.scala:245)
        at scala.Option.getOrElse(Option.scala:189)
        at org.apache.spark.SparkConf.get(SparkConf.scala:245)
        at org.apache.spark.SparkConf.getAppId(SparkConf.scala:450)
        at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.$anonfun$constructVolumes$4(MountVolumesFeatureStep.scala:88)
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
        at scala.collection.Iterator.foreach(Iterator.scala:943)
        at scala.collection.Iterator.foreach$(Iterator.scala:943)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
        at scala.collection.IterableLike.foreach(IterableLike.scala:74)
        at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
        at scala.collection.TraversableLike.map(TraversableLike.scala:286)
        at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
        at scala.collection.AbstractTraversable.map(Traversable.scala:108)
        at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.constructVolumes(MountVolumesFeatureStep.scala:57)
        at org.apache.spark.deploy.k8s.features.MountVolumesFeatureStep.configurePod(MountVolumesFeatureStep.scala:34)
        at org.apache.spark.scheduler.cluster.k8s.KubernetesExecutorBuilder.$anonfun$buildFromFeatures$4(KubernetesExecutorBuilder.scala:64)
        at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
        at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
        at scala.collection.immutable.List.foldLeft(List.scala:91)
        at org.apache.spark.scheduler.cluster.k8s.KubernetesExecutorBuilder.buildFromFeatures(KubernetesExecutorBuilder.scala:63)
        at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$requestNewExecutors$1(ExecutorPodsAllocator.scala:391)
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)
        at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.requestNewExecutors(ExecutorPodsAllocator.scala:382)
        at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36(ExecutorPodsAllocator.scala:346)
        at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$onNewSnapshots$36$adapted(ExecutorPodsAllocator.scala:339)
        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
        at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.onNewSnapshots(ExecutorPodsAllocator.scala:339)
        at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3(ExecutorPodsAllocator.scala:117)
        at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsAllocator.$anonfun$start$3$adapted(ExecutorPodsAllocator.scala:117)
        at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.org$apache$spark$scheduler$cluster$k8s$ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber$$processSnapshotsInternal(ExecutorPodsSnapshotsStoreImpl.scala:138)
       at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl$SnapshotsSubscriber.processSnapshots(ExecutorPodsSnapshotsStoreImpl.scala:126)
        at org.apache.spark.scheduler.cluster.k8s.ExecutorPodsSnapshotsStoreImpl.$anonfun$addSubscriber$1(ExecutorPodsSnapshotsStoreImpl.scala:81)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}
 ",,apachespark,dongjoon,tgraves,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 10 15:01:10 UTC 2022,,,,,,,,,,"0|z102gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/22 19:25;tgraves;just going by the stack trace this looks related to change https://issues.apache.org/jira/browse/SPARK-35182

[~dongjoon] Just curious if you have run into this?;;;","02/Mar/22 22:53;dongjoon;Could you describe your procedure about this, [~tgraves]? I've been never running `spark-shell` on K8s cluster so far.
bq. and starting a spark-shell in client mode;;;","07/Mar/22 21:12;tgraves;so I actually created another pod with Spark client in it and use the spark-shell.

[https://spark.apache.org/docs/3.2.1/running-on-kubernetes.html#client-mode]

Only thing I had to do was make sure ports were available. 

Since you don't run in this mode, I can investigate more.;;;","08/Mar/22 07:19;dongjoon;Thank you for your investigation, [~tgraves].;;;","08/Mar/22 23:36;tgraves;so the issue here is there is a race between when kubernetes call MountVolumesFeatureStep via adding it to the ExecutorPodsLifecycleManager which calls addSubscriber in ExecutorPodsSnapshotsStoreImpl. and when the spark.app.id is actually set in the Spark Context.  Here spark context isn't set until after the scheduler backend has started.    If its not set the only way to get the appId is to get the one generated in KubernetesClusterSchedulerBackend since that is wha tis ultimately used in spark context to set spark.app.id.  I'll investigate a fix.

 ;;;","10/Mar/22 01:56;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/35792;;;","10/Mar/22 05:08;dongjoon;This is resolved via https://github.com/apache/spark/pull/35792;;;","10/Mar/22 15:00;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/35804;;;","10/Mar/22 15:01;apachespark;User 'tgravescs' has created a pull request for this issue:
https://github.com/apache/spark/pull/35804;;;",,,
"StackOverflowError with OR(data filter, partition filter)",SPARK-38357,13431140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,huaxingao,huaxingao,01/Mar/22 04:29,02/Mar/22 04:05,13/Jul/23 08:47,02/Mar/22 04:05,3.2.1,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"If the filter has OR and contains both data filter and partition filter, 
e.g. p is partition col and id is data col

{code:java}
SELECT * FROM tmp WHERE (p = 0 AND id > 0) OR (p = 1 AND id = 2) 
{code}

throws StackOverflowError
",,apachespark,dongjoon,huaxingao,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 02 04:05:38 UTC 2022,,,,,,,,,,"0|z1013k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/22 04:29;huaxingao;I will submit a PR soon.;;;","01/Mar/22 05:25;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/35691;;;","02/Mar/22 01:28;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/35703;;;","02/Mar/22 04:05;dongjoon;Issue resolved by pull request 35703
[https://github.com/apache/spark/pull/35703];;;",,,,,,,,
Change mktemp() to mkstemp(),SPARK-38355,13431091,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bjornjorgensen,bjornjorgensen,bjornjorgensen,28/Feb/22 21:16,12/Dec/22 18:10,13/Jul/23 08:47,10/Mar/22 01:02,3.3.0,,,,,,,,3.3.0,,,,,PySpark,,,,0,,,"In the file pandasutils.py on line 262 yield tempfile.mktemp(dir=tmp)

The mktemp() is [deprecated and is not secure|https://docs.python.org/3/library/tempfile.html#deprecated-functions-and-variables]
",,apachespark,bjornjorgensen,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 10 01:02:42 UTC 2022,,,,,,,,,,"0|z100sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/22 02:37;gurwls223;[~bjornjorgensen] are you interested in creating a PR?;;;","06/Mar/22 13:29;bjornjorgensen;Well I did guess that this one was an easy one. But mkstemp returns a tuple. So I get TypeError: expected str, bytes or os.PathLike object, not tuple   
And NamedTemporaryFile gives TypeError: expected str, bytes or os.PathLike object, not _TemporaryFileWrapper. And I can't figure out what and _TemporaryFileWrapper is.
So I have tried a lot of different things as you can see in https://github.com/bjornjorgensen/spark/commits/Change-mktemp-to-mkstemp Right now I get AttributeError: __enter__ https://stackoverflow.com/questions/51427729/python-error-attributeerror-enter
I don't see the right solution for this problem, so I won`t open a PR for this now.   ;;;","08/Mar/22 21:47;apachespark;User 'bjornjorgensen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35775;;;","08/Mar/22 21:48;apachespark;User 'bjornjorgensen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35775;;;","10/Mar/22 01:02;gurwls223;Issue resolved by pull request 35775
[https://github.com/apache/spark/pull/35775];;;",,,,,,,
Nullability propagation in transformUpWithNewOutput,SPARK-38347,13430949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,buyingyi,buyingyi,buyingyi,28/Feb/22 09:14,01/Mar/22 01:40,13/Jul/23 08:47,01/Mar/22 01:40,3.2.0,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"The nullability of a replaced attribute should be `a.nullable` instead of `b.nullable`:
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/QueryPlan.scala#L357

The scenario is a left outer join where the RHS attributes are replaced bottom up.",,apachespark,buyingyi,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 01 01:40:47 UTC 2022,,,,,,,,,,"0|z0zzxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/22 10:22;apachespark;User 'sigmod' has created a pull request for this issue:
https://github.com/apache/spark/pull/35677;;;","28/Feb/22 17:23;apachespark;User 'sigmod' has created a pull request for this issue:
https://github.com/apache/spark/pull/35685;;;","01/Mar/22 01:40;Gengliang.Wang;Issue resolved by pull request 35685
[https://github.com/apache/spark/pull/35685];;;",,,,,,,,,
Avoid to submit task when there are no requests to push up in push-based shuffle,SPARK-38344,13430920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weixiuli,weixiuli,weixiuli,28/Feb/22 06:14,02/Mar/22 02:26,13/Jul/23 08:47,02/Mar/22 02:26,3.2.0,3.2.1,,,,,,,3.3.0,,,,,Shuffle,Spark Core,,,0,,,,,apachespark,mridulm80,weixiuli,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 02 02:26:10 UTC 2022,,,,,,,,,,"0|z0zzqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/22 06:25;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/35675;;;","28/Feb/22 06:26;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/35675;;;","02/Mar/22 02:26;mridulm80;Issue resolved by pull request 35675
[https://github.com/apache/spark/pull/35675];;;",,,,,,,,,
DPP cause DataSourceScanExec java.lang.NullPointerException,SPARK-38333,13430776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,lijiahong,lijiahong,lijiahong,25/Feb/22 22:34,31/Mar/22 13:38,13/Jul/23 08:47,31/Mar/22 13:38,3.1.2,,,,,,,,3.1.3,3.2.2,3.3.0,,,SQL,,,,0,,,"In DPP,we trigger NPE,like blow:

Caused by: java.lang.NullPointerException
    at org.apache.spark.sql.execution.DataSourceScanExec.$init$(DataSourceScanExec.scala:57)
    at org.apache.spark.sql.execution.FileSourceScanExec.<init>(DataSourceScanExec.scala:172)

...

    at org.apache.spark.sql.catalyst.expressions.CodeGeneratorWithInterpretedFallback.createObject(CodeGeneratorWithInterpretedFallback.scala:56)
    at org.apache.spark.sql.catalyst.expressions.Predicate$.create(predicates.scala:101)
    at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$2(basicPhysicalOperators.scala:246)
    at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$2$adapted(basicPhysicalOperators.scala:245)
    at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)

,the root cause is addExprTree funtion in EquivalentExpressions:

```

def addExprTree(
expr: Expression,
addFunc: Expression => Boolean = addExpr): Unit = {
val skip = expr.isInstanceOf[LeafExpression] ||
// `LambdaVariable` is usually used as a loop variable, which can't be evaluated ahead of the
// loop. So we can't evaluate sub-expressions containing `LambdaVariable` at the beginning.
expr.find(_.isInstanceOf[LambdaVariable]).isDefined ||
// `PlanExpression` wraps query plan. To compare query plans of `PlanExpression` on executor,
// can cause error like NPE.
(expr.isInstanceOf[PlanExpression[_]] && TaskContext.get != null)

if (!skip && !addFunc(expr)) {
childrenToRecurse(expr).foreach(addExprTree(_, addFunc))
commonChildrenToRecurse(expr).filter(_.nonEmpty).foreach(addCommonExprs(_, addFunc))

```

maybe we should change it like this :
```

(expr.find(_.isInstanceOf[PlanExpression[_]]).isDefined && TaskContext.get != null)

```

because, in DPP,the filter expression like this:

DynamicPruningExpression(InSubqueryExec(value, broadcastValues, exprId)

so, we should iterator children, if PlanExpression found, such as  InSubqueryExec, we should skip addExprTree, then NPE will not appears",,apachespark,cloud_fan,lijiahong,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 31 13:38:15 UTC 2022,,,,,,,,,,"0|z0zyuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/22 01:15;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/35662;;;","30/Mar/22 10:55;apachespark;User 'monkeyboy123' has created a pull request for this issue:
https://github.com/apache/spark/pull/36012;;;","31/Mar/22 13:38;cloud_fan;Issue resolved by pull request 36012
[https://github.com/apache/spark/pull/36012];;;",,,,,,,,,
"The second range is not [0, 59] in the day time ANSI interval",SPARK-38324,13430587,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chongg@nvidia,chongg@nvidia,chongg@nvidia,25/Feb/22 04:21,17/Feb/23 06:30,13/Jul/23 08:47,17/Feb/23 06:30,3.3.0,,,,,,,,3.5.0,,,,,Java API,,,,0,,,"[https://spark.apache.org/docs/latest/sql-ref-datatypes.html]
 * SECOND, seconds within minutes and possibly fractions of a second [0..59.999999]{{{{}}{}}}

{{Doc shows SECOND is seconds within minutes, it's range should be [0, 59]}}

 

But testing shows 99 second is valid:

{{>>> spark.sql(""select INTERVAL '10 01:01:99' DAY TO SECOND"")}}
{{{}DataFrame[INTERVAL '10 01:02:39' DAY TO SECOND: interval day to second]{}}}{{{{}}{}}}

 

Meanwhile, minute range check is ok, see below:

>>> spark.sql(""select INTERVAL '10 01:60:01' DAY TO SECOND"")
requirement failed: {color:#de350b}*minute 60 outside range [0, 59]*{color}(line 1, pos 16)

== SQL ==
select INTERVAL '10 01:60:01' DAY TO SECOND
----------------^^^

 ",Spark 3.3.0 snapshot,apachespark,chongg@nvidia,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 06:30:04 UTC 2023,,,,,,,,,,"0|z0zxow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/22 04:48;gurwls223;cc [~Gengliang.Wang] FYI;;;","15/Feb/23 03:54;apachespark;User 'haoyanzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40028;;;","15/Feb/23 03:54;apachespark;User 'haoyanzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40028;;;","15/Feb/23 08:44;apachespark;User 'haoyanzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/40033;;;","17/Feb/23 06:30;maxgekk;Issue resolved by pull request 40033
[https://github.com/apache/spark/pull/40033];;;",,,,,,,
(flat)MapGroupsWithState can timeout groups which just received inputs in the same microbatch,SPARK-38320,13430489,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,alex-balikov,alex-balikov,alex-balikov,24/Feb/22 17:51,05/Apr/22 06:29,13/Jul/23 08:47,13/Mar/22 05:12,3.2.1,,,,,,,,3.2.2,3.3.0,,,,Structured Streaming,,,,0,correctness,releasenotes,"We have identified an issue where the RocksDB state store iterator will not pick up store updates made after its creation. As a result of this, the _timeoutProcessorIter_ in

[https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/FlatMapGroupsWithStateExec.scala]

will not pick up state changes made during _newDataProcessorIter_ input processing. The user observed behavior is that a group state may receive input records and also be called with timeout in the same micro batch. This contradics the public documentation for GroupState -

[https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/streaming/GroupState.html]
 * The timeout is reset every time the function is called on a group, that is, when the group has new data, or the group has timed out. So the user has to set the timeout duration every time the function is called, otherwise, there will not be any timeout set.",,alex-balikov,apachespark,kabhwan,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 13 05:12:28 UTC 2022,,,,,,,,,,"0|z0zx3c:",9223372036854775807,,,,,kabhwan,,,,,,,,,,,,,,,,,,"10/Mar/22 22:53;apachespark;User 'alex-balikov' has created a pull request for this issue:
https://github.com/apache/spark/pull/35810;;;","13/Mar/22 05:12;kabhwan;Issue resolved via https://github.com/apache/spark/pull/35810;;;",,,,,,,,,,
Fail to read parquet files after writing the hidden file metadata in,SPARK-38314,13430401,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yaohua,yaohua,yaohua,24/Feb/22 10:25,28/Feb/22 11:47,13/Jul/23 08:47,28/Feb/22 11:47,3.2.1,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"Selecting and then writing df containing hidden file metadata column `_metadata` into a file format like `parquet`, `delta` will still keep the internal `Attribute` metadata information. Then when reading those `parquet`, `delta` files again, it will actually break the code, because it wrongly thinks user data schema named `_metadata` is a hidden file source metadata column.

 

Reproducible code:
{code:java}
// prepare a file source df
df.select(""*"", ""_metadata"")
  .write.format(""parquet"").save(path)
spark.read.format(""parquet"").load(path)
  .select(""*"").show(){code}",,apachespark,cloud_fan,yaohua,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 28 11:47:36 UTC 2022,,,,,,,,,,"0|z0zwk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/22 14:04;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/35650;;;","24/Feb/22 14:04;apachespark;User 'Yaohua628' has created a pull request for this issue:
https://github.com/apache/spark/pull/35650;;;","28/Feb/22 11:47;cloud_fan;Issue resolved by pull request 35650
[https://github.com/apache/spark/pull/35650];;;",,,,,,,,,
SHS has incorrect percentiles for shuffle read bytes and shuffle total blocks metrics,SPARK-38309,13430301,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,robreeves,robreeves,robreeves,23/Feb/22 22:18,08/Mar/22 18:46,13/Jul/23 08:47,08/Mar/22 18:46,3.1.0,,,,,,,,3.1.3,3.2.2,3.3.0,,,Spark Core,,,,0,correctness,,"*Background*

In [this PR|https://github.com/apache/spark/pull/26508] (SPARK-26260) the SHS stage metric percentiles were updated to only include successful tasks when using disk storage. It did this by making the values for each metric negative when the task is not in a successful state. This approach was chosen to avoid breaking changes to disk storage. See [this comment|https://github.com/apache/spark/pull/26508#issuecomment-554540314] for context.

To get the percentiles, it reads the metric values, starting at 0, in ascending order. This filters out all tasks that are not successful because the values are less than 0. To get the percentile values it scales the percentiles to the list index of successful tasks. For example if there are 200 tasks and you want percentiles [0, 25, 50, 75, 100] the lookup indexes in the task collection are [0, 50, 100, 150, 199].

*Issue*
For metrics 1) shuffle total reads and 2) shuffle total blocks, the above PR incorrectly makes the metric indices positive. This means tasks that are not successful are included in the percentile calculations. The percentile lookup index calculation is still based on the number of successful task so the wrong task metric is returned for a given percentile. This was not caught because the unit test only verified values for one metric, executorRunTime.

*Steps to Reproduce*
_SHS UI_
 # Find a spark application in the SHS that has failed tasks for a stage with shuffle read.
 # Navigate to the stage UI.
 # Look at the max shuffle read size in the summary metrics
 # Sort the tasks by shuffle read size descending. You'll see it doesn't match step 3.

 

!image-2022-02-23-14-19-33-255.png|width=789,height=389!

_API_
 # For the same stage in the above repro steps, make a request to the task summary endpoint (e.g. /api/v1/applications/application_1632281309592_21294517/1/stages/6/0/taskSummary?quantiles=0,0.25,0.5,0.75,1.0)
 # Look at the shuffleReadMetrics.readBytes and shuffleReadMetrics.totalBlocksFetched. You will see -2 for at least some of the lower percentiles and the positive values will also be wrong.",,apachespark,mridulm80,robreeves,xkrogen,,,,,,,,,,,,,,,,,,,,,"23/Feb/22 22:19;robreeves;image-2022-02-23-14-19-33-255.png;https://issues.apache.org/jira/secure/attachment/13040393/image-2022-02-23-14-19-33-255.png",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 08 18:46:06 UTC 2022,,,,,,,,,,"0|z0zvxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/22 23:56;apachespark;User 'robreeves' has created a pull request for this issue:
https://github.com/apache/spark/pull/35637;;;","23/Feb/22 23:57;apachespark;User 'robreeves' has created a pull request for this issue:
https://github.com/apache/spark/pull/35637;;;","08/Mar/22 18:46;mridulm80;Issue resolved by pull request 35637
[https://github.com/apache/spark/pull/35637];;;",,,,,,,,,
Select of a stream of window expressions fails,SPARK-38308,13430292,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,23/Feb/22 20:43,12/Dec/22 18:11,13/Jul/23 08:47,27/Mar/22 00:31,3.1.3,3.2.1,3.3.0,3.4.0,,,,,3.3.0,,,,,SQL,,,,0,,,"The following query fails:
{noformat}
val df = spark.range(0, 20).map { x =>
  (x % 4, x + 1, x + 2)
}.toDF(""a"", ""b"", ""c"")

import org.apache.spark.sql.expressions._

val w = Window.partitionBy(""a"").orderBy(""b"")
val selectExprs = Stream(
  sum(""c"").over(w.rowsBetween(Window.unboundedPreceding, Window.currentRow)).as(""sumc""),
  avg(""c"").over(w.rowsBetween(Window.unboundedPreceding, Window.currentRow)).as(""avgc"")
)

df.select(selectExprs: _*).show(false)
{noformat}
It fails with the following error:
{noformat}
org.apache.spark.sql.AnalysisException: Resolved attribute(s) avgc#23 missing from c#16L,a#14L,b#15L,sumc#21L in operator !Project [c#16L, a#14L, b#15L, sumc#21L, sumc#21L, avgc#23].;
{noformat}
If you change the Stream of window expressions to a Vector or List, the query succeeds.",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 27 00:31:11 UTC 2022,,,,,,,,,,"0|z0zvvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/22 20:45;bersprockets;The cause is similar issue to that of SPARK-38221. The fix should be trivial. I will make a PR sometime soon.;;;","23/Feb/22 21:29;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35635;;;","23/Feb/22 21:30;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35635;;;","27/Mar/22 00:31;gurwls223;Fixed in https://github.com/apache/spark/pull/35635;;;",,,,,,,,
Elt() should return null if index is null under ANSI mode,SPARK-38304,13430191,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,23/Feb/22 11:05,11/Apr/22 12:48,13/Jul/23 08:47,23/Feb/22 15:33,3.3.0,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,,,apachespark,cloud_fan,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-38860,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 23 15:33:24 UTC 2022,,,,,,,,,,"0|z0zv9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Feb/22 11:10;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35629;;;","23/Feb/22 15:33;cloud_fan;Issue resolved by pull request 35629
[https://github.com/apache/spark/pull/35629];;;",,,,,,,,,,
Union's maxRows and maxRowsPerPartition may overflow,SPARK-38286,13429917,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,22/Feb/22 11:41,24/Feb/22 02:55,13/Jul/23 08:47,24/Feb/22 02:55,3.0.3,3.1.2,3.2.1,3.3.0,,,,,3.0.4,3.1.3,3.2.2,3.3.0,,SQL,,,,0,,,"{code:java}
scala> val df1 = spark.range(0, Long.MaxValue, 1, 1)
df1: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> val df2 = spark.range(0, 100, 1, 10)
df2: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> val union = df1.union(df2)
union: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> union.queryExecution.logical.maxRowsPerPartition
res19: Option[Long] = Some(-9223372036854775799)

scala> union.queryExecution.logical.maxRows
res20: Option[Long] = Some(-9223372036854775709)
 {code}",,apachespark,cloud_fan,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 24 02:55:47 UTC 2022,,,,,,,,,,"0|z0ztlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/22 11:45;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/35609;;;","24/Feb/22 02:55;cloud_fan;Issue resolved by pull request 35609
[https://github.com/apache/spark/pull/35609];;;",,,,,,,,,,
ClassCastException: GenericArrayData cannot be cast to InternalRow,SPARK-38285,13429878,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,allebacco,allebacco,22/Feb/22 09:11,07/Mar/22 20:04,13/Jul/23 08:47,07/Mar/22 20:04,3.2.1,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"The following code with Spark 3.2.1 raises an exception:

{code:python}
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StructField, ArrayType, StringType

t = StructType([
    StructField('o', 
        ArrayType(
            StructType([
                StructField('s', StringType(), False),
                StructField('b', ArrayType(
                    StructType([
                        StructField('e', StringType(), False)
                    ]),
                    True),
                False)
            ]), 
        True),
    False)])

value = {
    ""o"": [
        {
            ""s"": ""string1"",
            ""b"": [
                {
                    ""e"": ""string2""
                },
                {
                    ""e"": ""string3""
                }
            ]
        },
        {
            ""s"": ""string4"",
            ""b"": [
                {
                    ""e"": ""string5""
                },
                {
                    ""e"": ""string6""
                },
                {
                    ""e"": ""string7""
                }
            ]
        }
    ]
}

df = (
    spark.createDataFrame([value], schema=t)
    .select(F.explode(""o"").alias(""eo""))
    .select(""eo.b.e"")
)


df.show()
{code}

The exception message is:
{code}
java.lang.ClassCastException: org.apache.spark.sql.catalyst.util.GenericArrayData cannot be cast to org.apache.spark.sql.catalyst.InternalRow
	at org.apache.spark.sql.catalyst.util.GenericArrayData.getStruct(GenericArrayData.scala:76)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:80)
	at org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:155)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)
	at org.apache.spark.scheduler.Task.doRunTask(Task.scala:153)
	at org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:122)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.scheduler.Task.run(Task.scala:93)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:824)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1641)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:827)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:683)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

I am using Spark 3.2.1, but I don't know if even Spark 3.3.0 is affected.

Please note that the issue seems to be related to SPARK-37577: I am using the same DataFrame schema, but this time I have populated it with non empty value.

I think that this is bug because with the following configuration it works as expected:
{code:python}
spark.conf.set(""spark.sql.optimizer.expression.nestedPruning.enabled"", False)
spark.conf.set(""spark.sql.optimizer.nestedSchemaPruning.enabled"", False)
{code}

Update: The provided code is working with Spark 3.1.2 without problems, so it seems an error due to expression pruning.

The expected result is:

{code}
+---------------------------+
|e                          |
+---------------------------+
|[string2, string3]         |
|[string5, string6, string7]|
+---------------------------+
{code}",,allebacco,apachespark,bersprockets,viirya,,,,,,,,,,,,,,,,,,SPARK-37577,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 07 20:04:55 UTC 2022,,,,,,,,,,"0|z0ztcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/22 02:01;bersprockets;Since {{eo.b}} is an array of structs, don't you need to choose an index before accessing field {{{}e{}}}?

For example:
{noformat}
df = (
    spark.createDataFrame([value], schema=t)
    .select(F.explode(""o"").alias(""eo""))
    .select(F.element_at(""eo.b"", 1).alias(""eob"")).select(""eob.e"")
)

df.show()
{noformat}
This displays:
{noformat}
>>> df.show()
+-------+
|      e|
+-------+
|string2|
|string5|
+-------+

>>> 
{noformat};;;","24/Feb/22 08:45;allebacco;I agree with you that it is an uncommon accessing expression. However, the provided code is working with

* Spark 3.1.2
* Spark 3.2.1 with {{spark.sql.optimizer.expression.nestedPruning.enabled=false}} and {{spark.sql.optimizer.nestedSchemaPruning.enabled=false}}

The expected result is:

{code}
+---------------------------+
|e                          |
+---------------------------+
|[string2, string3]         |
|[string5, string6, string7]|
+---------------------------+
{code}

I think that it should work even with Spark 3.2.1 and _nestedPruning_ optimizations enabled. Otherwise, the _nestedPruning_ optimization are not reliable.;;;","24/Feb/22 19:11;bersprockets;I see your point.

It appears to be caused by [this commit|https://github.com/apache/spark/commit/c59988aa79] (for SPARK-34638). cc [~viirya]

Before that commit, this works:
{noformat}
create or replace temp view v1 as
select * from values
(array(
  named_struct('s', 'string1', 'b', array(named_struct('e', 'string2'), named_struct('e', 'string3'))),
  named_struct('s', 'string4', 'b', array(named_struct('e', 'string5'), named_struct('e', 'string6')))
  )
)
v1(o);

select eo.b.e from (select explode(o) as eo from v1);
{noformat}
It produces:
{noformat}
[""string2"",""string3""]
[""string5"",""string6""]
{noformat}
After that commit, you instead get the following error:
{noformat}
java.lang.ClassCastException: org.apache.spark.sql.catalyst.util.GenericArrayData cannot be cast to org.apache.spark.sql.catalyst.InternalRow
{noformat}
You can bypass the error by caching the {{{}explode{}}}. For example, this works even after SPARK-34638:
{noformat}
create or replace temp view v1 as
select * from values
(array(
  named_struct('s', 'string1', 'b', array(named_struct('e', 'string2'), named_struct('e', 'string3'))),
  named_struct('s', 'string4', 'b', array(named_struct('e', 'string5'), named_struct('e', 'string6')))
  )
)
v1(o);

create or replace temporary view v2 as select explode(o) as eo from v1;
cache table v2;
select eo.b.e from v2;
{noformat}
Also you can bypass the error by turning off {{spark.sql.optimizer.expression.nestedPruning.enabled}} and {{{}spark.sql.optimizer.nestedSchemaPruning.enabled{}}}, as [~allebacco] mentioned above.;;;","25/Feb/22 00:34;viirya;Thanks for reporting this. I will take a look.;;;","04/Mar/22 14:47;allebacco;Any news on this issue?
The code freeze for release 3.3 will happen on March 15th. This bug is probably present even in the current master branch (I have not tested it on master), so there is the risk that version 3.3 could be released without the fix.;;;","07/Mar/22 08:17;viirya;No worries as bug fix is not blocked by code freeze. I've submitted a PR for this.;;;","07/Mar/22 08:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/35749;;;","07/Mar/22 20:04;viirya;Issue resolved by pull request 35749
[https://github.com/apache/spark/pull/35749];;;",,,,
Consider to include WriteBatch's memory in the memory usage of RocksDB state store,SPARK-38275,13429698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yunta,yunta,yunta,21/Feb/22 11:16,25/Feb/22 03:08,13/Jul/23 08:47,25/Feb/22 03:08,3.2.1,,,,,,,,3.3.0,,,,,Structured Streaming,,,,0,,,"Current RocksDB state store actually use a unlimited {{WriteBatch}} with a DB, the {{WriteBatch}} would not be cleared until the micro-batch data committed, which results that the memoy usage of {{WriteBatch}} could be very huge.

We should consider to add the approximate memory usgae of WriteBatch as the totdal memory usage and also print it separately.",,apachespark,kabhwan,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 25 03:08:08 UTC 2022,,,,,,,,,,"0|z0zs9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/22 11:42;apachespark;User 'Myasuka' has created a pull request for this issue:
https://github.com/apache/spark/pull/35600;;;","25/Feb/22 03:08;kabhwan;Issue resolved by pull request 35600
[https://github.com/apache/spark/pull/35600];;;",,,,,,,,,,
decodeUnsafeRows's iterators should close underlying input streams,SPARK-38273,13429695,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kevin.w.sewell,kevin.w.sewell,kevin.w.sewell,21/Feb/22 10:46,12/Dec/22 18:11,13/Jul/23 08:47,24/Feb/22 16:15,3.2.0,3.2.1,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"SPARK-34647 replaced the ZstdInputStream with ZstdInputStreamNoFinalizer. This meant that all usages of `CompressionCodec.compressedInputStream` would need to manually close the stream as this would no longer be handled by GC finaliser mechanism.

In SparkPlan, the result of `CompressionCodec.compressedInputStream` is wrapped in an Iterator which never calls close. This implementation needs to make use of NextIterator which allows for the closing of underlying streams.",,apachespark,dongjoon,kevin.w.sewell,,,,,,,,,,,,,,,,,,,,,SPARK-34647,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 24 16:15:10 UTC 2022,,,,,,,,,,"0|z0zs8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/22 10:47;kevin.w.sewell;I am happy to submit a PR for this change.;;;","22/Feb/22 00:57;gurwls223;[~kevin.w.sewell] please go ahead for a PR. cc [~dongjoon] FYI;;;","22/Feb/22 09:06;dongjoon;Yep, I'm working with [~kevin.w.sewell] in order to make a valid patch.;;;","22/Feb/22 16:05;apachespark;User 'kevins-29' has created a pull request for this issue:
https://github.com/apache/spark/pull/35613;;;","22/Feb/22 16:05;apachespark;User 'kevins-29' has created a pull request for this issue:
https://github.com/apache/spark/pull/35613;;;","24/Feb/22 16:15;dongjoon;Issue resolved by pull request 35613
[https://github.com/apache/spark/pull/35613];;;",,,,,,
PoissonSampler may output more rows than MaxRows,SPARK-38271,13429647,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,podongfeng,podongfeng,21/Feb/22 08:03,17/Jul/22 05:38,13/Jul/23 08:47,22/Feb/22 13:05,3.0.3,3.1.2,3.2.1,3.3.0,,,,,3.2.2,3.3.0,,,,SQL,,,,0,correctness,,"{code:java}
scala> val df = spark.range(0, 1000)
df: org.apache.spark.sql.Dataset[Long] = [id: bigint]

scala> df.count
res0: Long = 1000

scala> df.sample(true, 0.999999, 10).count
res1: Long = 1004
 {code}",,apachespark,cloud_fan,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 22 13:05:37 UTC 2022,,,,,,,,,,"0|z0zry0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/22 08:12;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/35593;;;","21/Feb/22 08:12;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/35593;;;","22/Feb/22 13:05;cloud_fan;Issue resolved by pull request 35593
[https://github.com/apache/spark/pull/35593];;;",,,,,,,,,
UnresolvedException: Invalid call to dataType on unresolved object caused by GetDateFieldOperations,SPARK-38266,13429596,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,21/Feb/22 02:39,22/Feb/22 07:53,13/Jul/23 08:47,21/Feb/22 02:41,3.2.1,3.3.0,,,,,,,,,,,,SQL,,,,0,,,"{code:java}
test(""GetDateFieldOperations should skip unresolved nodes"") {
  withSQLConf(SQLConf.ANSI_ENABLED.key -> ""true"") {
    val df = Seq(""1644821603"").map(i => (i.toInt, i)).toDF(""tsInt"", ""tsStr"")
    val df1 = df.select(df(""tsStr"").cast(""timestamp"")).as(""df1"")
    val df2 = df.select(df(""tsStr"").cast(""timestamp"")).as(""df2"")
    df1.join(df2, $""df1.tsStr"" === $""df2.tsStr"", ""left_outer"")
    val df3 = df1.join(df2, $""df1.tsStr"" === $""df2.tsStr"", ""left_outer"")
      .select($""df1.tsStr"".as(""timeStr"")).as(""df3"")
    // This throws ""UnresolvedException: Invalid call to
    // dataType on unresolved object"" instead of ""AnalysisException: Column 'df1.timeStr' does not exist.""
    df3.join(df1, year($""df1.timeStr"") === year($""df3.tsStr""))
  }
} {code}",,apachespark,Ngone51,,,,,,,,,,,,,,,,,SPARK-35937,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 22 03:25:12 UTC 2022,,,,,,,,,,"0|z0zrmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/22 02:41;Ngone51;Issue resolved by https://github.com/apache/spark/pull/35568;;;","21/Feb/22 02:42;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/35568;;;","22/Feb/22 03:24;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/35601;;;","22/Feb/22 03:25;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/35601;;;",,,,,,,,
Unintended exception thrown in pyspark.ml.LogisticRegression.getThreshold,SPARK-38243,13429137,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,zero323,zero323,zero323,17/Feb/22 15:48,18/Feb/22 17:09,13/Jul/23 08:47,18/Feb/22 17:08,2.4.0,3.1.0,3.2.0,3.3.0,,,,,3.3.0,,,,,ML,PySpark,,,0,,,"If {{LogisticRegression.getThreshold}} is called with model having multiple thresholds we suppose to raise an exception,
{code:python}
ValueError: Logistic Regression getThreshold only applies to binary classification ...
{code}
However, {{thresholds}} ({{{}List[float]{}}}) are incorrectly passed to {{{}str.join{}}}, resulting in unintended {{TypeError}}


{code:python}
>>> from pyspark.ml.classification import LogisticRegression
... 
... model = LogisticRegression(thresholds=[1.0, 2.0, 3.0])
>>> model.getThreshold()
Traceback (most recent call last):
  Input In [7] in <module>
    model.getThreshold()
  File /path/to/spark/python/pyspark/ml/classification.py:1003 in getThreshold
    + "","".join(ts)
Type Error: sequence item 0: expected str instance, float found

{code}",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 18 17:08:48 UTC 2022,,,,,,,,,,"0|z0zotc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/22 15:58;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/35558;;;","17/Feb/22 15:59;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/35558;;;","18/Feb/22 17:08;srowen;Issue resolved by pull request 35558
[https://github.com/apache/spark/pull/35558];;;",,,,,,,,,
AttributeError: 'LogisticRegressionModel' object has no attribute '_call_java',SPARK-38239,13429094,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,17/Feb/22 12:44,06/Mar/22 16:56,13/Jul/23 08:47,06/Mar/22 16:56,2.4.0,3.0.0,3.1.0,3.2.0,3.3.0,,,,3.3.0,,,,,MLlib,PySpark,,,0,,,"Trying to invoke {{\_\_repr\_\_}} on {{pyspark.mllib.classification.LogisticRegressionModel}} leads to {{AttributeError}}:

{code:python}
>>> type(model)
<class 'pyspark.mllib.classification.LogisticRegressionModel'>
>>> model
Traceback (most recent call last):
  File /path/to/python3.9/site-packages/IPython/core/formatters.py:698 in __call__
    return repr(obj)
  File /path/to/spark/python/pyspark/mllib/classification.py:281 in __repr__
    return self._call_java(""toString"")
AttributeError: 'LogisticRegressionModel' object has no attribute '_call_java'
{code}

This problem was introduced SPARK-14712, where the method was added, with the same implementation, for both {{ml}} and {{mllib}}.",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 06 16:56:12 UTC 2022,,,,,,,,,,"0|z0zojs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/22 12:52;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/35554;;;","17/Feb/22 12:53;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/35554;;;","06/Mar/22 16:56;srowen;Issue resolved by pull request 35554
[https://github.com/apache/spark/pull/35554];;;",,,,,,,,,
Absolute file paths specified in create/alter table are treated as relative,SPARK-38236,13429026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bozhang,bozhang,bozhang,17/Feb/22 08:25,24/Feb/22 07:09,13/Jul/23 08:47,24/Feb/22 07:08,3.1.1,3.1.2,3.2.0,3.2.1,,,,,3.1.3,3.2.2,3.3.0,,,SQL,,,,0,,,"After https://github.com/apache/spark/pull/28527 we change to create table under the database location when the table location specified is relative. However the criteria to determine if a table location is relative/absolute is URI.isAbsolute, which basically checks if the table location URI has a scheme defined. So table URIs like /table/path are treated as relative and the scheme and authority of the database location URI are used to create the table. For example, when the database location URI is s3a://bucket/db, the table will be created at s3a://bucket/table/path, while it should be created under the file system defined in SessionCatalog.hadoopConf instead.

This also applies to alter table.",,apachespark,bozhang,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 24 07:08:36 UTC 2022,,,,,,,,,,"0|z0zo54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Feb/22 08:27;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/35462;;;","17/Feb/22 08:28;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/35462;;;","21/Feb/22 05:16;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/35591;;;","21/Feb/22 05:17;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/35591;;;","24/Feb/22 07:08;cloud_fan;Issue resolved by pull request 35591
[https://github.com/apache/spark/pull/35591];;;",,,,,,,
Apply strict nullability of nested column in time window / session window,SPARK-38227,13428724,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,16/Feb/22 07:29,21/Feb/22 05:45,13/Jul/23 08:47,21/Feb/22 05:45,3.2.1,3.3.0,,,,,,,3.3.0,,,,,Structured Streaming,,,,0,,,"In TimeWindow and SessionWindow, we define dataType of these function expressions as StructType having two nested columns ""start"" and ""end"", which is ""nullable"".

And we replace these expressions in the analyzer via corresponding rules, TimeWindowing for TimeWindow, and SessionWindowing for SessionWindow.

The rules replace the function expressions with Alias, referring CreateNamedStruct. For the value side of CreateNamedStruct, we don't specify anything about nullability, which leads to a risk the value side may be interpreted (or optimized) as non-nullable, which would make inconsistency.

We should make sure the nullability of columns in CreateNamedStruct remains the same with dataType definition on these function expressions.",,apachespark,kabhwan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 21 05:45:14 UTC 2022,,,,,,,,,,"0|z0zmag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Feb/22 09:25;kabhwan;Will submit a PR sooner.;;;","16/Feb/22 11:55;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/35543;;;","21/Feb/22 05:45;viirya;Issue resolved by pull request 35543
[https://github.com/apache/spark/pull/35543];;;",,,,,,,,,
Group by a stream of complex expressions fails,SPARK-38221,13428649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,bersprockets,bersprockets,15/Feb/22 21:02,12/Dec/22 18:10,13/Jul/23 08:47,16/Feb/22 02:13,3.2.1,3.3.0,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"This query fails:
{noformat}
scala> Seq(1).toDF(""id"").groupBy(Stream($""id"" + 1, $""id"" + 2): _*).sum(""id"").show(false)
java.lang.IllegalStateException: Couldn't find _groupingexpression#24 in [id#4,_groupingexpression#23]
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:83)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:425)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:94)
  at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:418)
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
  at scala.collection.immutable.Stream.$anonfun$map$1(Stream.scala:418)
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1173)
  at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1163)
  at scala.collection.immutable.Stream.foreach(Stream.scala:534)
  at scala.collection.TraversableOnce.count(TraversableOnce.scala:152)
  at scala.collection.TraversableOnce.count$(TraversableOnce.scala:145)
  at scala.collection.AbstractTraversable.count(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.createCode(GenerateUnsafeProjection.scala:293)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doConsumeWithKeys(HashAggregateExec.scala:623)
{noformat}
However, replace {{Stream}} with {{Seq}} and it works:
{noformat}
scala> Seq(1).toDF(""id"").groupBy(Seq($""id"" + 1, $""id"" + 2): _*).sum(""id"").show(false)
+--------+--------+-------+
|(id + 1)|(id + 2)|sum(id)|
+--------+--------+-------+
|2       |3       |1      |
+--------+--------+-------+

scala> 
{noformat}
 ",,apachespark,bersprockets,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 16 02:13:47 UTC 2022,,,,,,,,,,"0|z0zlts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/22 21:04;bersprockets;I think I have an idea what's going on. I will submit a PR soon.;;;","16/Feb/22 01:45;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35537;;;","16/Feb/22 02:13;gurwls223;Fixed in https://github.com/apache/spark/pull/35537;;;",,,,,,,,,
'Column' object is not callable,SPARK-38208,13428356,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,joycerecacho,joycerecacho,14/Feb/22 15:23,14/Feb/22 17:21,13/Jul/23 08:47,14/Feb/22 17:21,3.1.2,3.2.1,,,,,,,3.1.2,,,,,Deploy,,,,0,,,"Hi guys, I have such simple dataframe and am trying to create one new column.

That its schema:

 

>>>> df_operation_event_sellers.schema 

Out[69]: StructType(List(StructField(id,StringType,true),StructField(account_id,StringType,true),StructField(p_tenant_id,StringType,true),StructField(vendor_id,StringType,true),StructField(amount,DecimalType(38,18),true),StructField(operation_type,StringType,true),StructField(reference_id,StringType,true),StructField(date,TimestampType,true),StructField(carrier_id,StringType,true),StructField(account_number,StringType,true),StructField(data_source,StringType,true),StructField(entity,StringType,true),StructField(ingestion_date,DateType,true),StructField(event_type,StringType,false),StructField(amount_new,DecimalType(38,18),true),StructField(date_new,IntegerType,true),StructField(row_num,IntegerType,true)))

 

>>> command to create the new column

df_operation_event_sellers= df_operation_event_sellers.withColumn('flag_first_selling',when(col('row_num') == 1,'YES').instead('NO'))



ISSUE >>>>>>>>>>>> TypeError: 'Column' object is not callable

 

What is happing?

ps. I created other columns the same way successfully

 

 

 

 ",,joycerecacho,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 14 17:20:36 UTC 2022,,,,,,,,,,"0|z0zk0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/22 17:20;joycerecacho;the correct is 'otherwise' and not 'instead' for the 'else' condition.;;;",,,,,,,,,,,
Relax the requirement of data type comparison for keys in stream-stream join,SPARK-38206,13428314,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,14/Feb/22 11:22,22/Feb/22 07:00,13/Jul/23 08:47,22/Feb/22 07:00,3.1.2,3.2.1,3.3.0,,,,,,3.3.0,,,,,Structured Streaming,,,,0,,,"Currently, stream-stream join checks for the data type compatible between left keys and right keys. It is done as ""strict"" checking, requiring nullability as same for both sides. This leads to throw assertion error if optimizer turns some columns in one side from nullable to non-nullable but not touching opposite side.

If it is logically correct to relax the nullability check (with deciding proper type on output schema), we should do it to avoid any possible issue from optimization.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 22 07:00:48 UTC 2022,,,,,,,,,,"0|z0zjrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/22 11:36;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/35599;;;","22/Feb/22 07:00;kabhwan;Issue resolved by pull request 35599
[[https://github.com/apache/spark/pull/35599]];;;",,,,,,,,,,
All state operators are at a risk of inconsistency between state partitioning and operator partitioning,SPARK-38204,13428304,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,kabhwan,kabhwan,14/Feb/22 10:31,23/Mar/22 01:12,13/Jul/23 08:47,15/Mar/22 20:30,2.2.3,2.3.4,2.4.8,3.0.3,3.1.2,3.2.1,3.3.0,,3.2.2,3.3.0,,,,Structured Streaming,,,,0,correctness,releasenotes,"Except stream-stream join, all stateful operators use ClusteredDistribution as a requirement of child distribution.

ClusteredDistribution is very relaxed one - any output partitioning can satisfy the distribution if the partitioning can ensure all tuples having same grouping keys are placed in same partition.

To illustrate an example, support we do streaming aggregation like below code:
{code:java}
df
  .withWatermark(""timestamp"", ""30 minutes"")
  .groupBy(""group1"", ""group2"", window(""timestamp"", ""10 minutes""))
  .agg(count(""*"")) {code}
In the code, streaming aggregation operator will be involved in physical plan, which would have ClusteredDistribution(""group1"", ""group2"", ""window"").

The problem is, various output partitionings can satisfy this distribution:
 * RangePartitioning

 ** This accepts exact and subset of the grouping key, with any order of keys (combination), with any sort order (asc/desc)

 * HashPartitioning

 ** This accepts exact and subset of the grouping key, with any order of keys (combination)

 * (upcoming Spark 3.3.0+) DataSourcePartitioning

 ** output partitioning provided by data source will be able to satisfy ClusteredDistribution, which will make things worse (assuming data source can provide different output partitioning relatively easier)

e.g. even we only consider HashPartitioning, HashPartitioning(""group1""), HashPartitioning(""group2""), HashPartitioning(""group1"", ""group2""), HashPartitioning(""group2"", ""group1""), HashPartitioning(""group1"", ""group2"", ""window""), etc.

The requirement of state partitioning is much more strict, since we should not change the partitioning once it is partitioned and built. *It should ensure that all tuples having same grouping keys are placed in same partition (same partition ID) across query lifetime.*

*The impedance of distribution requirement between ClusteredDistribution and state partitioning leads correctness issue silently.*

For example, let's assume we have a streaming query like below:
{code:java}
df
  .withWatermark(""timestamp"", ""30 minutes"")
  .repartition(""group2"")
  .groupBy(""group1"", ""group2"", window(""timestamp"", ""10 minutes""))
  .agg(count(""*"")) {code}
repartition(""group2"") satisfies ClusteredDistribution(""group1"", ""group2"", ""window""), so Spark won't introduce additional shuffle there, and state partitioning would be HashPartitioning(""group2"").

we run this query for a while, and stop the query, and change the manual partitioning like below:
{code:java}
df
  .withWatermark(""timestamp"", ""30 minutes"")
  .repartition(""group1"")
  .groupBy(""group1"", ""group2"", window(""timestamp"", ""10 minutes""))
  .agg(count(""*"")) {code}
repartition(""group1"") also satisfies ClusteredDistribution(""group1"", ""group2"", ""window""), so Spark won't introduce additional shuffle there. That said, child output partitioning of streaming aggregation operator would be HashPartitioning(""group1""), whereas state partitioning is HashPartitioning(""group2"").

[https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovery-semantics-after-changes-in-a-streaming-query]

In SS guide doc we enumerate the unsupported modifications of the query during the lifetime of streaming query, but there is no notion of this.

Making this worse, Spark doesn't store any information on state partitioning (that said, there is no way to validate), so *Spark simply allows this change and brings up correctness issue while the streaming query runs like no problem at all.* The only way to indicate the correctness is from the result of the query.

We have no idea whether end users already suffer from this in their queries or not. *The only way to look into is to list up all state rows and apply hash function with expected grouping keys, and confirm all rows provide the exact partition ID where they are in.* If it turns out as broken, we will have to have a tool to “re”partition the state correctly, or in worst case, have to ask throwing out checkpoint and reprocess.

{*}This issue has been laid from the introduction of stateful operators (Spark 2.2+){*}, since HashClusteredDistribution (strict requirement) had introduced in Spark 2.3 and we didn't change stateful operators to use this distribution. stream-stream join hopefully used HashClusteredDistribution from Spark 2.3, so it seems to be safe.",,apachespark,kabhwan,viirya,XuanYuan,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 18 03:06:55 UTC 2022,,,,,,,,,,"0|z0zjpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/22 10:33;kabhwan;Since this is not a regression, we could change the priority to critical if we can't make it in the release period of Spark 3.3.;;;","22/Feb/22 07:33;kabhwan;Let’s have a short-term fix first to make sure we no longer open the chance to mess up with new streaming query first. We can craft a long-term fix accounting existing queries on top of short-term fix.
 ;;;","28/Feb/22 04:21;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/35673;;;","15/Mar/22 20:30;XuanYuan;Issue resolved by pull request 35673
[https://github.com/apache/spark/pull/35673];;;","18/Mar/22 03:06;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/35908;;;","18/Mar/22 03:06;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/35908;;;",,,,,,
Fix KubernetesUtils#uploadFileToHadoopCompatibleFS use passed in `delSrc` and `overwrite`,SPARK-38201,13428266,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,LuciferYang,LuciferYang,LuciferYang,14/Feb/22 08:28,16/Feb/22 03:50,13/Jul/23 08:47,16/Feb/22 03:50,3.3.0,,,,,,,,3.3.0,,,,,Kubernetes,,,,0,,,"KubernetesUtils#uploadFileToHadoopCompatibleFS defines the input parameters `

delSrc` and `overwrite`,  but constants(false and true) are used when call `

FileSystem.copyFromLocalFile(boolean delSrc, boolean overwrite, Path src, Path dst) ` method.

`

 ",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 16 03:50:57 UTC 2022,,,,,,,,,,"0|z0zjgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/22 08:41;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35509;;;","14/Feb/22 08:42;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35509;;;","16/Feb/22 03:50;dongjoon;Issue resolved by pull request 35509
[https://github.com/apache/spark/pull/35509];;;",,,,,,,,,
Fix `QueryExecution.debug#toFile` use the passed in `maxFields` when `explainMode` is `CodegenMode`,SPARK-38198,13428249,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,14/Feb/22 07:24,14/Feb/22 16:47,13/Jul/23 08:47,14/Feb/22 10:28,3.3.0,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"`QueryExecution.debug#toFile` method supports passing in `maxFields` and this parameter will be passed down when `explainMode` is SimpleMode, ExtendedMode, or CostMode, but the passed down `maxFields` was ignored because `QueryExecution#stringWithStats` overrides it with `SQLConf.get.maxToStringFields` at present

 ",,apachespark,LuciferYang,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 14 13:09:18 UTC 2022,,,,,,,,,,"0|z0zjd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/22 07:40;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35506;;;","14/Feb/22 10:28;maxgekk;Issue resolved by pull request 35506
[https://github.com/apache/spark/pull/35506];;;","14/Feb/22 13:08;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35515;;;","14/Feb/22 13:09;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35515;;;",,,,,,,,
Use try-with-resources in Level/RocksDBSuite.java,SPARK-38192,13428037,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,11/Feb/22 13:56,14/Feb/22 02:00,13/Jul/23 08:47,14/Feb/22 01:59,3.3.0,,,,,,,,3.3.0,,,,,Tests,,,,0,,,"Manually compiled rocksdbjni with rocksdb 6.29.fb on Apple Silicon/M1, then test `org.apache.spark.util.kvstore.RocksDBSuite` using the generated jar, there is a VM crash caused by resource leak

 

 

 ",,apachespark,dongjoon,LuciferYang,viirya,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 14 01:59:41 UTC 2022,,,,,,,,,,"0|z0zi20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/22 14:39;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35493;;;","14/Feb/22 01:59;dongjoon;Issue resolved by pull request 35493
[https://github.com/apache/spark/pull/35493];;;",,,,,,,,,,
Fix data incorrect if aggregate function is empty,SPARK-38185,13427939,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,11/Feb/22 06:58,11/Feb/22 13:15,13/Jul/23 08:47,11/Feb/22 13:15,3.2.1,3.3.0,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"The group only condition should check if the aggregate expression is empty.

In DataFrame api, it is allowed to make a empty aggregations.

So the following query should return 1 rather than 0 because it's a global aggregate.
{code:java}
val emptyAgg = Map.empty[String, String]
spark.range(2).where(""id > 2"").agg(emptyAgg).limit(1).count
{code}
",,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 11 13:15:05 UTC 2022,,,,,,,,,,"0|z0zhg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/22 07:15;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35490;;;","11/Feb/22 07:15;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35490;;;","11/Feb/22 13:15;cloud_fan;Issue resolved by pull request 35490
[https://github.com/apache/spark/pull/35490];;;",,,,,,,,,
Fix NoSuchElementException if pushed filter does not contain any references,SPARK-38182,13427920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,11/Feb/22 04:51,17/Feb/22 21:12,13/Jul/23 08:47,17/Feb/22 21:11,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"reproduce:

{code:java}
CREATE TABLE t (c1 int) USING PARQUET;

SET spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.BooleanSimplification;

SELECT * FROM t WHERE c1 = 1 AND 2 > 1;
{code}

and the error msg:

{code:java}
java.util.NoSuchElementException: next on empty iterator
	at scala.collection.Iterator$$anon$2.next(Iterator.scala:41)
	at scala.collection.Iterator$$anon$2.next(Iterator.scala:39)
	at scala.collection.mutable.LinkedHashSet$$anon$1.next(LinkedHashSet.scala:89)
	at scala.collection.IterableLike.head(IterableLike.scala:109)
	at scala.collection.IterableLike.head$(IterableLike.scala:108)
	at org.apache.spark.sql.catalyst.expressions.AttributeSet.head(AttributeSet.scala:69)
	at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.$anonfun$listFiles$3(PartitioningAwareFileIndex.scala:85)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.execution.datasources.PartitioningAwareFileIndex.listFiles(PartitioningAwareFileIndex.scala:84)
	at org.apache.spark.sql.execution.FileSourceScanExec.selectedPartitions$lzycompute(DataSourceScanExec.scala:249)
{code}

",,apachespark,dongjoon,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 17 21:11:40 UTC 2022,,,,,,,,,,"0|z0zhc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/22 05:51;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35487;;;","11/Feb/22 05:52;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35487;;;","17/Feb/22 21:11;dongjoon;Issue resolved by pull request 35487
[https://github.com/apache/spark/pull/35487];;;",,,,,,,,,
Correct the logic to measure the memory usage of RocksDB,SPARK-38178,13427784,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yunta,yunta,yunta,10/Feb/22 15:20,11/Feb/22 03:38,13/Jul/23 08:47,11/Feb/22 03:38,3.2.1,,,,,,,,3.2.2,3.3.0,,,,Structured Streaming,,,,0,,,"Currently, structed streaming would report the memory usage of RocksDB as state store's metrics. However, it only sums from {{rocksdb.estimate-table-readers-mem}} and {{rocksdb.size-all-mem-tables}}. However, this lacks of the memory usage of block cache. (You can refer to [Memory-usage-in-RocksDB](https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB) for more details.)

BTW, as the ""block-cache-pinned-usage"" is included in ""block-cache-usage"", we don't need to include the pinned usage.",,apachespark,dongjoon,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 11 03:38:06 UTC 2022,,,,,,,,,,"0|z0zghs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/22 15:34;apachespark;User 'Myasuka' has created a pull request for this issue:
https://github.com/apache/spark/pull/35480;;;","10/Feb/22 15:35;apachespark;User 'Myasuka' has created a pull request for this issue:
https://github.com/apache/spark/pull/35480;;;","11/Feb/22 03:38;dongjoon;This is resolved via https://github.com/apache/spark/pull/35480;;;",,,,,,,,,
Quoted column cannot be recognized correctly when quotedRegexColumnNames is true,SPARK-38173,13427728,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,blackpig,blackpig,blackpig,10/Feb/22 10:56,10/Jan/23 20:59,13/Jul/23 08:47,16/Feb/22 04:41,3.1.2,3.2.0,,,,,,,3.2.4,3.3.0,,,,SQL,,,,0,,,"When spark.sql.parser.quotedRegexColumnNames=true
{code:java}
 SELECT `(C3)?+.+`,`C1` * C2 FROM (SELECT 3 AS C1,2 AS C2,1 AS C3) T;{code}
The above query will throw an exception
{code:java}
Error: org.apache.hive.service.cli.HiveSQLException: Error running query: org.apache.spark.sql.AnalysisException: Invalid usage of '*' in expression 'multiply'
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:370)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:266)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:78)
        at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:62)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:44)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:266)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:261)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:422)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
        at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:275)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.spark.sql.AnalysisException: Invalid usage of '*' in expression 'multiply'
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:50)
        at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:49)
        at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:155)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$expandStarExpression$1.applyOrElse(Analyzer.scala:1700)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$expandStarExpression$1.applyOrElse(Analyzer.scala:1671)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:342)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:342)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:339)
        at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
        at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:339)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.expandStarExpression(Analyzer.scala:1671)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.$anonfun$buildExpandedProjectList$1(Analyzer.scala:1656) {code}
It works fine in hive
{code:java}
0: jdbc:hive2://hiveserver-inc.> set hive.support.quoted.identifiers=none;
No rows affected (0.003 seconds)
0: jdbc:hive2://hiveserver-inc.> SELECT `(C3)?+.+`,`C1` * C2 FROM (SELECT 3 AS C1,2 AS C2,1 AS C3) T;
22/02/10 19:01:43 INFO ql.Driver: OK
+-------+-------+------+
| t.c1  | t.c2  | _c1  |
+-------+-------+------+
| 3     | 2     | 6    |
+-------+-------+------+
1 row selected (0.136 seconds){code}
 ",,apachespark,blackpig,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 02:41:39 UTC 2023,,,,,,,,,,"0|z0zg5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/22 11:30;apachespark;User 'TongWei1105' has created a pull request for this issue:
https://github.com/apache/spark/pull/35476;;;","10/Feb/22 11:31;apachespark;User 'TongWei1105' has created a pull request for this issue:
https://github.com/apache/spark/pull/35476;;;","16/Feb/22 04:41;cloud_fan;Issue resolved by pull request 35476
[https://github.com/apache/spark/pull/35476];;;","10/Jan/23 02:41;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39473;;;","10/Jan/23 02:41;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/39473;;;",,,,,,,
Upgrade ORC to 1.7.3,SPARK-38171,13427677,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,10/Feb/22 08:30,10/Feb/22 16:42,13/Jul/23 08:47,10/Feb/22 16:42,3.3.0,,,,,,,,3.3.0,,,,,Build,SQL,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 10 16:42:17 UTC 2022,,,,,,,,,,"0|z0zfu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Feb/22 08:31;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35474;;;","10/Feb/22 16:42;dongjoon;Issue resolved by pull request 35474
[https://github.com/apache/spark/pull/35474];;;",,,,,,,,,,
Handle `Pacific/Kanton` in DateTimeUtilsSuite,SPARK-38151,13427340,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,09/Feb/22 03:14,14/Feb/22 21:46,13/Jul/23 08:47,09/Feb/22 21:21,3.1.3,3.2.2,3.3.0,,,,,,3.1.4,3.2.2,3.3.0,,,SQL,Tests,,,0,,,"This issue aims to fix the flaky UT failures due to https://bugs.openjdk.java.net/browse/JDK-8274407 (Update Timezone Data to 2021c) and its backport commits that renamed 'Pacific/Enderbury' to 'Pacific/Kanton' in the latest Java 17.0.2, 11.0.14, and 8u311.

Rename Pacific/Enderbury to Pacific/Kanton.

**MASTER**
- https://github.com/dongjoon-hyun/spark/runs/5119322349?check_suite_focus=true
{code}
[info] - daysToMicros and microsToDays *** FAILED *** (620 milliseconds)
[info]   9131 did not equal 9130 Round trip of 9130 did not work in tz Pacific/Kanton (DateTimeUtilsSuite.scala:783)
{code}

**BRANCH-3.2**
- https://github.com/apache/spark/runs/5122380604?check_suite_focus=true
{code}
[info] - daysToMicros and microsToDays *** FAILED *** (643 milliseconds)
[info]   9131 did not equal 9130 Round trip of 9130 did not work in tz Pacific/Kanton (DateTimeUtilsSuite.scala:771)
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 09 21:21:26 UTC 2022,,,,,,,,,,"0|z0zdrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/22 18:28;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35468;;;","09/Feb/22 18:29;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35468;;;","09/Feb/22 21:21;dongjoon;Issue resolved by pull request 35468
[https://github.com/apache/spark/pull/35468];;;",,,,,,,,,
UDAF fails to aggregate TIMESTAMP_NTZ column,SPARK-38146,13427329,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,09/Feb/22 02:12,10/Feb/22 04:01,13/Jul/23 08:47,10/Feb/22 04:01,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"When using a UDAF against unsafe rows containing a TIMESTAMP_NTZ column, Spark throws the error:
{noformat}
22/02/08 18:05:12 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.UnsupportedOperationException: null
	at org.apache.spark.sql.catalyst.expressions.UnsafeRow.update(UnsafeRow.java:218) ~[spark-catalyst_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]
	at org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils.$anonfun$createSetters$15(udaf.scala:217) ~[spark-sql_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]
	at org.apache.spark.sql.execution.aggregate.BufferSetterGetterUtils.$anonfun$createSetters$15$adapted(udaf.scala:215) ~[spark-sql_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]
	at org.apache.spark.sql.execution.aggregate.MutableAggregationBufferImpl.update(udaf.scala:272) ~[spark-sql_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]
	at $line17.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$ScalaAggregateFunction.$anonfun$update$1(<console>:46) ~[scala-library.jar:?]
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158) ~[scala-library.jar:?]
	at $line17.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$ScalaAggregateFunction.update(<console>:45) ~[scala-library.jar:?]
	at org.apache.spark.sql.execution.aggregate.ScalaUDAF.update(udaf.scala:458) ~[spark-sql_2.12-3.3.0-SNAPSHOT.jar:3.3.0-SNAPSHOT]
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$1.$anonfun$applyOrElse$2(AggregationIterator.scala:197) ~[spark-sql_2.12-3.3.0-SNAPSHO
{noformat}
This  is because {{BufferSetterGetterUtils#createSetters}} does not have a case statement for {{TimestampNTZType}}, so it generates a function that tries to call {{UnsafeRow.update}}, which throws an {{UnsupportedOperationException}}.

This reproduction example is mostly taken from {{AggregationQuerySuite}}:
{noformat}
import org.apache.spark.sql.expressions.{MutableAggregationBuffer, UserDefinedAggregateFunction}
import org.apache.spark.sql.types._
import org.apache.spark.sql.Row

class ScalaAggregateFunction(schema: StructType) extends UserDefinedAggregateFunction {

  def inputSchema: StructType = schema

  def bufferSchema: StructType = schema

  def dataType: DataType = schema

  def deterministic: Boolean = true

  def initialize(buffer: MutableAggregationBuffer): Unit = {
    (0 until schema.length).foreach { i =>
      buffer.update(i, null)
    }
  }

  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    if (!input.isNullAt(0) && input.getInt(0) == 50) {
      (0 until schema.length).foreach { i =>
        buffer.update(i, input.get(i))
      }
    }
  }

  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    if (!buffer2.isNullAt(0) && buffer2.getInt(0) == 50) {
      (0 until schema.length).foreach { i =>
        buffer1.update(i, buffer2.get(i))
      }
    }
  }

  def evaluate(buffer: Row): Any = {
    Row.fromSeq(buffer.toSeq)
  }
}

import scala.util.Random
import java.time.LocalDateTime

val r = new Random(65676563L)
val data = Seq.tabulate(50) { x =>
  Row((x + 1).toInt, (x + 2).toDouble, (x + 2).toLong, LocalDateTime.parse(""2100-01-01T01:33:33.123"").minusDays(x + 1))
}
val schema = StructType.fromDDL(""id int, col1 double, col2 bigint, col3 timestamp_ntz"")
val rdd = spark.sparkContext.parallelize(data, 1)
val df = spark.createDataFrame(rdd, schema)

val udaf = new ScalaAggregateFunction(df.schema)

val allColumns = df.schema.fields.map(f => col(f.name))

df.groupBy().agg(udaf(allColumns: _*)).show(false)
{noformat}",,apachespark,bersprockets,Gengliang.Wang,,,,,,,,,,,,,,,,,,SPARK-38133,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 10 04:01:49 UTC 2022,,,,,,,,,,"0|z0zdp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/22 02:17;bersprockets;This affects master only and has a simple fix: {{BufferSetterGetterUtils}} needs case statements for {{TimestampNTZType}} that replicates what is done for {{{}TimestampType{}}}.

Edit: Also, the filtering of {{TimestampNTZType}} [here in AggregationQuerySuite|https://github.com/apache/spark/blob/master/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/AggregationQuerySuite.scala#L901] should be removed so that {{TimestampNTZType}} gets tested.;;;","09/Feb/22 23:49;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35470;;;","10/Feb/22 04:01;Gengliang.Wang;Issue resolved by pull request 35470
[https://github.com/apache/spark/pull/35470];;;",,,,,,,,,
"Desc column stats (min, max) for timestamp type is not consistent with the value due to time zone difference",SPARK-38140,13427185,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhenhuawang,zhenhuawang,zhenhuawang,08/Feb/22 12:48,21/Feb/22 07:40,13/Jul/23 08:47,21/Feb/22 07:40,3.1.2,3.2.1,,,,,,,3.3.0,,,,,SQL,,,,0,,,"Currently timestamp column's stats (min/max) are stored in UTC in metastore, and when desc its min/max column stats, they are also shown in UTC.

As a result, for users not in UTC, the column stats (shown to users) are not consistent with the actual value, which causes confusion.

For example:

{noformat}
spark-sql> create table tab_ts_master (ts timestamp) using parquet;

spark-sql> insert into tab_ts_master values make_timestamp(2022, 1, 1, 0, 0, 1.123456), make_timestamp(2022, 1, 3, 0, 0, 2.987654);

spark-sql> select * from tab_ts_master;
2022-01-01 00:00:01.123456
2022-01-03 00:00:02.987654

spark-sql> set spark.sql.session.timeZone;
spark.sql.session.timeZone	Asia/Shanghai

spark-sql> analyze table tab_ts_master compute statistics for all columns;

spark-sql> desc formatted tab_ts_master ts;
col_name	ts
data_type	timestamp
comment	NULL
min	2021-12-31 16:00:01.123456
max	2022-01-02 16:00:02.987654
num_nulls	0
distinct_count	2
avg_col_len	8
max_col_len	8
histogram	NULL
{noformat}
",,apachespark,cloud_fan,zhenhuawang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 21 07:40:43 UTC 2022,,,,,,,,,,"0|z0zcug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/22 13:24;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/35440;;;","21/Feb/22 07:40;cloud_fan;Issue resolved by pull request 35440
[https://github.com/apache/spark/pull/35440];;;",,,,,,,,,,
ml.recommendation.ALS doctests failures,SPARK-38139,13427182,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,08/Feb/22 12:26,12/Dec/22 18:10,13/Jul/23 08:47,13/Feb/22 23:31,3.3.0,,,,,,,,3.3.0,,,,,ML,PySpark,,,0,,,"In my dev setups, ml.recommendation:ALS test consistently converges to value lower than expected and fails with:

{code:python}
File ""/path/to/spark/python/pyspark/ml/recommendation.py"", line 322, in __main__.ALS
Failed example:
    predictions[0]
Expected:
    Row(user=0, item=2, newPrediction=0.69291...)
Got:
    Row(user=0, item=2, newPrediction=0.6929099559783936)
{code}

In can correct for that, but it creates some noise, so if anyone else experiences this, we could drop  a digit from the results

{code}
diff --git a/python/pyspark/ml/recommendation.py b/python/pyspark/ml/recommendation.py
index f0628fb922..b8e2a6097d 100644
--- a/python/pyspark/ml/recommendation.py
+++ b/python/pyspark/ml/recommendation.py
@@ -320,7 +320,7 @@ class ALS(JavaEstimator, _ALSParams, JavaMLWritable, JavaMLReadable):
     >>> test = spark.createDataFrame([(0, 2), (1, 0), (2, 0)], [""user"", ""item""])
     >>> predictions = sorted(model.transform(test).collect(), key=lambda r: r[0])
     >>> predictions[0]
-    Row(user=0, item=2, newPrediction=0.69291...)
+    Row(user=0, item=2, newPrediction=0.6929...)
     >>> predictions[1]
     Row(user=1, item=0, newPrediction=3.47356...)
     >>> predictions[2]

{code}",,apachespark,podongfeng,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 13 23:31:14 UTC 2022,,,,,,,,,,"0|z0zcts:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/22 01:35;gurwls223;Yeah, I think we should fix as so!;;;","09/Feb/22 12:45;podongfeng;I think it is ok to adjust the tol in this case;;;","10/Feb/22 00:11;gurwls223;yup agree;;;","13/Feb/22 16:39;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/35503;;;","13/Feb/22 23:31;gurwls223;Issue resolved by pull request 35503
[https://github.com/apache/spark/pull/35503];;;",,,,,,,
Grouping by timestamp_ntz will sometimes corrupt the results,SPARK-38133,13427039,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,07/Feb/22 22:00,09/Feb/22 23:46,13/Jul/23 08:47,08/Feb/22 03:36,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,correctness,,"Assume this data:
{noformat}
create or replace temp view v1 as
select * from values
  (1, timestamp_ntz'2012-01-01 00:00:00', 10000),
  (2, timestamp_ntz'2012-01-01 00:00:00', 20000),
  (1, timestamp_ntz'2012-01-01 00:00:00', 5000),
  (1, timestamp_ntz'2013-01-01 00:00:00', 48000),
  (2, timestamp_ntz'2013-01-01 00:00:00', 30000)
  as data(a, b, c);
{noformat}
Run the following query:
{noformat}
select *
from v1
pivot (
  sum(c)
  for a in (1, 2)
);
{noformat}
You get incorrect results for the group-by column:
{noformat}
2012-01-01 19:05:19.476736	15000	20000
2013-01-01 19:05:19.476736	48000	30000
Time taken: 2.65 seconds, Fetched 2 row(s)
{noformat}
Actually, _whenever_ the TungstenAggregationIterator is used to group by a timestamp_ntz column, you get incorrect results:
{noformat}
set spark.sql.codegen.wholeStage=false;
select a, b, sum(c) from v1 group by a, b;
{noformat}
This query produces
{noformat}
2	2012-01-01 09:32:39.738368	20000
1	2013-01-01 09:32:39.738368	48000
2	2013-01-01 09:32:39.738368	30000
Time taken: 1.927 seconds, Fetched 4 row(s)
{noformat}",,apachespark,bersprockets,dongjoon,,,,,,,,,,,,,,,,,,,SPARK-38146,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 08 03:36:52 UTC 2022,,,,,,,,,,"0|z0zbyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/22 22:01;bersprockets;I think I have a handle on what is causing this, and will make a PR shortly.;;;","08/Feb/22 00:23;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35430;;;","08/Feb/22 00:33;dongjoon;Does this happen on master branch only, [~bersprockets]?;;;","08/Feb/22 00:37;bersprockets;[~dongjoon] 

>Does this happen on master branch only

As far as I know, TIMESTAMP_NTZ exists only on master branch.;;;","08/Feb/22 03:36;dongjoon;Issue resolved by pull request 35430
[https://github.com/apache/spark/pull/35430];;;",,,,,,,
Remove NotPropagation,SPARK-38132,13427025,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kazuyukitanimura,kazuyukitanimura,kazuyukitanimura,07/Feb/22 20:11,08/Feb/22 00:18,13/Jul/23 08:47,08/Feb/22 00:18,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"To mitigate the bug introduced by SPARK-36665. Remove {{NotPropagation}} optimization for now until we find a better approach.

{{NotPropagation}} optimization previously broke {{RewritePredicateSubquery}} so that it does not properly rewrite the predicate to a NULL-aware left anti join anymore.",,apachespark,dongjoon,kazuyukitanimura,,,,,,,,,,,,,,,,,,SPARK-36665,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 08 00:18:26 UTC 2022,,,,,,,,,,"0|z0zbvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/22 20:23;apachespark;User 'kazuyukitanimura' has created a pull request for this issue:
https://github.com/apache/spark/pull/35428;;;","07/Feb/22 20:23;apachespark;User 'kazuyukitanimura' has created a pull request for this issue:
https://github.com/apache/spark/pull/35428;;;","08/Feb/22 00:18;dongjoon;Issue resolved by pull request 35428
[https://github.com/apache/spark/pull/35428];;;",,,,,,,,,
array_sort does not allow non-orderable datatypes,SPARK-38130,13426909,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,steven.aerts,steven.aerts,steven.aerts,07/Feb/22 12:29,15/Feb/22 17:43,13/Jul/23 08:47,15/Feb/22 17:43,3.2.1,,,,,,,,3.3.0,,,,,SQL,,,,0,,," {{array_sort}} has check to see if the entries it has to sort are orderable.

I think this check should be removed.  Because even entries which are not orderable can have a lambda function which makes them orderable.
{code:java}
Seq((Array[Map[String, Int]](Map(""a"" -> 1), Map()), ""x"")).toDF(""a"", ""b"").selectExpr(""array_sort(a, (x,y) -> cardinality(x) - cardinality(y))""){code}
fails with:
{code:java}
org.apache.spark.sql.AnalysisException: cannot resolve 'array_sort(`a`, lambdafunction((cardinality(namedlambdavariable()) - cardinality(namedlambdavariable())), namedlambdavariable(), namedlambdavariable()))' due to data type mismatch: array_sort does not support sorting array of type map<string,int> which is not orderable {code}
While the case where this check is relevant, fails with a different error which is triggered earlier in the code path:
{code:java}
> Seq((Array[Map[String, Int]](Map(""a"" -> 1), Map()), ""x"")).toDF(""a"", ""b"").selectExpr(""array_sort(a)""){code}
Fails with:
{code:java}
org.apache.spark.sql.AnalysisException: cannot resolve '(namedlambdavariable() < namedlambdavariable())' due to data type mismatch: LessThan does not support ordering on type map<string,int>; line 1 pos 0;
{code}", ,apachespark,cloud_fan,steven.aerts,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 15 17:43:11 UTC 2022,,,,,,,,,,"0|z0zb5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/22 13:20;apachespark;User 'steven-aerts' has created a pull request for this issue:
https://github.com/apache/spark/pull/35426;;;","15/Feb/22 17:43;cloud_fan;Issue resolved by pull request 35426
[https://github.com/apache/spark/pull/35426];;;",,,,,,,,,,
Revive HashClusteredDistribution and apply to stream-stream join,SPARK-38124,13426857,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,kabhwan,kabhwan,07/Feb/22 07:04,16/Mar/22 07:24,13/Jul/23 08:47,08/Feb/22 23:32,3.3.0,,,,,,,,3.3.0,,,,,SQL,Structured Streaming,,,0,,,"SPARK-35703 removed HashClusteredDistribution and replaced its usages with ClusteredDistribution.

While this works great for non stateful operators, we still need to have a separate requirement of distribution for stateful operator, because the requirement of ClusteredDistribution is too relaxed while the requirement of physical partitioning on stateful operator is quite strict.

In most cases, stateful operators must require child distribution as HashClusteredDistribution, with below major assumptions:
 # HashClusteredDistribution creates HashPartitioning and we will never ever change it for the future.
 # We will never ever change the implementation of {{partitionIdExpression}} in HashPartitioning for the future, so that Partitioner will behave consistently across Spark versions.
 # No partitioning except HashPartitioning can satisfy HashClusteredDistribution.

 

We should revive HashClusteredDistribution (with probably renaming specifically with stateful operator) and apply the distribution to the all stateful operators.

SPARK-35703 only touched stream-stream join, which means stream-stream join hasn't been broken in actual releases. Let's aim the partial revert of SPARK-35703 in this ticket, and have another ticket to deal with other stateful operators, which have been broken for their introduction (2.2+).",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 15 10:25:02 UTC 2022,,,,,,,,,,"0|z0zau8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Feb/22 08:29;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/35419;;;","08/Feb/22 23:32;kabhwan;Issue resolved by pull request 35419
[https://github.com/apache/spark/pull/35419];;;","14/Feb/22 10:53;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/35512;;;","15/Feb/22 10:24;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/35529;;;","15/Feb/22 10:25;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/35529;;;",,,,,,,
HiveExternalCatalog.listPartitions is failing when partition column name is upper case and dot in partition value,SPARK-38120,13426813,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,khalidmammadov9@gmail.com,khalidmammadov9@gmail.com,khalidmammadov9@gmail.com,06/Feb/22 23:19,14/Feb/22 21:47,13/Jul/23 08:47,09/Feb/22 08:55,3.0.3,3.1.2,3.2.1,,,,,,3.1.4,3.2.2,3.3.0,,,SQL,,,,0,,,"HiveExternalCatalog.listPartitions method call is failing when a partition column name is upper case and partition value contains dot. It's related to this change [https://github.com/apache/spark/commit/f18b905f6cace7686ef169fda7de474079d0af23]

The test casein that PR does not produce the issue as partition column name is lower case.

 

Below how to reproduce the issue:

scala> import org.apache.spark.sql.catalyst.TableIdentifier
import org.apache.spark.sql.catalyst.TableIdentifier

scala> spark.sql(""CREATE TABLE customer(id INT, name STRING) PARTITIONED BY (partCol1 STRING, partCol2 STRING)"")
scala> spark.sql(""INSERT INTO customer PARTITION (partCol1 = 'CA', partCol2 = 'i.j') VALUES (100, 'John')"")                               

scala> spark.sessionState.catalog.listPartitions(TableIdentifier(""customer""), Some(Map(""partCol2"" -> ""i.j""))).foreach(println)
java.util.NoSuchElementException: key not found: partcol2
  at scala.collection.immutable.Map$Map2.apply(Map.scala:227)
  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogUtils$.$anonfun$isPartialPartitionSpec$1(ExternalCatalogUtils.scala:205)
  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogUtils$.$anonfun$isPartialPartitionSpec$1$adapted(ExternalCatalogUtils.scala:202)
  at scala.collection.immutable.Map$Map1.forall(Map.scala:196)
  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogUtils$.isPartialPartitionSpec(ExternalCatalogUtils.scala:202)
  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$listPartitions$6(HiveExternalCatalog.scala:1312)
  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$listPartitions$6$adapted(HiveExternalCatalog.scala:1312)
  at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:304)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:303)
  at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:297)
  at scala.collection.AbstractTraversable.filterImpl(Traversable.scala:108)
  at scala.collection.TraversableLike.filter(TraversableLike.scala:395)
  at scala.collection.TraversableLike.filter$(TraversableLike.scala:395)
  at scala.collection.AbstractTraversable.filter(Traversable.scala:108)
  at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$listPartitions$1(HiveExternalCatalog.scala:1312)
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClientWrappingException(HiveExternalCatalog.scala:114)
  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:103)
  at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitions(HiveExternalCatalog.scala:1296)
  at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.listPartitions(ExternalCatalogWithListener.scala:254)
  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitions(SessionCatalog.scala:1251)
  ... 47 elided",,apachespark,cloud_fan,khalidmammadov9@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 09 08:55:08 UTC 2022,,,,,,,,,,"0|z0zakg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/22 23:50;apachespark;User 'khalidmammadov' has created a pull request for this issue:
https://github.com/apache/spark/pull/35409;;;","06/Feb/22 23:50;apachespark;User 'khalidmammadov' has created a pull request for this issue:
https://github.com/apache/spark/pull/35409;;;","09/Feb/22 08:55;cloud_fan;Issue resolved by pull request 35409
[https://github.com/apache/spark/pull/35409];;;",,,,,,,,,
Func(wrong data type) in HAVING clause should throw data mismatch error,SPARK-38118,13426776,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,amaliujia,amaliujia,amaliujia,06/Feb/22 07:09,18/Feb/22 00:02,13/Jul/23 08:47,17/Feb/22 23:59,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"{code:java}
with t as (select true c)
3select t.c
4from t
5group by t.c
6having mean(t.c) > 0 {code}
This query throws ""Column 't.c' does not exist. Did you mean one of the following? [t.c]""

However, mean(boolean) is not a supported function signature, thus error result should be  ""cannot resolve 'mean(t.c)' due to data type mismatch: function average requires numeric or interval types, not boolean""

 

This is because
 # The mean(boolean) in HAVING was not marked as resolved in {{ResolveFunctions}} rule.

 # Thus in {{{}ResolveAggregationFunctions{}}}, the {{TempResolvedColumn}} as a wrapper in mean({{{}TempResolvedColumn{}}}(t.c)) cannot be removed (only resolved AGG can remove its’s TempResolvedColumn).

 # Thus in a later batch rule applying,  {{TempResolvedColumn}} was reverted and it becomes mean(`t.c`), so mean loses the information about t.c.

 # Thus at the last step, the analyzer can only report t.c not found.

 

mean(boolean) in HAVING is not marked as resolved in {{ResolveFunctions}} rule because 
 # It uses Expression default `resolved` field population code 
{code:java}
lazy val resolved: Boolean = childrenResolved && checkInputDataTypes().isSuccess {code}
 
 #  During the analyzing,  mean(boolean) is mean(TempResolveColumn(boolean), thus childrenResolved is true.
 # however checkInputDataTypes() will be false ([Average.scala#L55|[https://github.com/apache/spark/blob/74ebef243c18e7a8f32bf90ea75ab6afed9e3132/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregate/Average.scala#L55])]
 # Thus eventually Average's `resolved`  will be false, but it leads to wrong error message.

 

 ",,amaliujia,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 17 23:59:48 UTC 2022,,,,,,,,,,"0|z0zac8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/22 07:17;apachespark;User 'amaliujia' has created a pull request for this issue:
https://github.com/apache/spark/pull/35404;;;","17/Feb/22 23:59;dongjoon;Issue resolved by pull request 35404
[https://github.com/apache/spark/pull/35404];;;",,,,,,,,,,
Hive script transform with order by and limit will return fake rows,SPARK-38075,13425684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,bersprockets,bersprockets,30/Jan/22 22:58,01/Feb/22 21:04,13/Jul/23 08:47,31/Jan/22 18:50,3.1.2,3.1.3,3.2.0,3.2.1,3.3.0,,,,3.1.3,3.2.2,3.3.0,,,SQL,,,,0,correctness,,"For example:

{noformat}
create or replace temp view t as
select * from values
(1),
(2),
(3)
as t(a);

select transform(a)
USING 'cat' AS (a int)
FROM t order by a limit 10;
{noformat}
This returns:
{noformat}
NULL
NULL
NULL
1
2
3
{noformat}
Without {{order by}} and {{limit}}, the query returns:
{noformat}
1
2
3
{noformat}
Spark script transform does not have this issue. That is, if {{spark.sql.catalogImplementation=in-memory}}, Spark does not return fake rows.
",,apachespark,bersprockets,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 31 23:29:01 UTC 2022,,,,,,,,,,"0|z0z3ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/22 23:01;bersprockets;It's a small iterator issue. I will make a PR shortly.;;;","30/Jan/22 23:15;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35368;;;","30/Jan/22 23:16;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35368;;;","31/Jan/22 18:50;dongjoon;This is resolved via https://github.com/apache/spark/pull/35368;;;","31/Jan/22 23:29;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35375;;;",,,,,,,
Update atexit function to avoid issues with late binding,SPARK-38073,13425661,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,30/Jan/22 15:25,12/Dec/22 18:10,13/Jul/23 08:47,05/Feb/22 04:21,3.2.0,3.3.0,,,,,,,3.2.2,3.3.0,,,,PySpark,Spark Shell,,,0,,,"When {{PYSPARK_DRIVER_PYTHON=$(which ipython) bin/pyspark}} is executed with Python >= 3.8, function registered wiht atexit seems to be executed in different scope than in Python 3.7.

It result in {{NameError: name 'sc' is not defined}} on exit:

{code:python}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.3.0-SNAPSHOT
      /_/

Using Python version 3.8.12 (default, Oct 12 2021 21:57:06)
Spark context Web UI available at http://192.168.0.198:4040
Spark context available as 'sc' (master = local[*], app id = local-1643555855409).
SparkSession available as 'spark'.

In [1]:                                                                                                                                                                                 
Do you really want to exit ([y]/n)? y
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File ""/path/to/spark/python/pyspark/shell.py"", line 49, in <lambda>
    atexit.register(lambda: sc.stop())
NameError: name 'sc' is not defined
{code}


This could be easily fixed by capturing `sc` instance

{code:none}
diff --git a/python/pyspark/shell.py b/python/pyspark/shell.py
index f0c487877a..4164e3ab0c 100644
--- a/python/pyspark/shell.py
+++ b/python/pyspark/shell.py
@@ -46,7 +46,7 @@ except Exception:
 
 sc = spark.sparkContext
 sql = spark.sql
-atexit.register(lambda: sc.stop())
+atexit.register((lambda sc: lambda: sc.stop())(sc))
 
 # for compatibility
 sqlContext = spark._wrapped
{code}",,apachespark,dongjoon,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 05 04:21:32 UTC 2022,,,,,,,,,,"0|z0z3j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/22 15:29;zero323;[~holden] FYI, since you questioned usage of {{ipython}} as {{PYSPARK_DRIVER_PYTHON}}.;;;","03/Feb/22 05:05;gurwls223;I think we should fix this .. ;;;","04/Feb/22 11:59;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/35396;;;","05/Feb/22 04:21;dongjoon;Issue resolved by pull request 35396
[https://github.com/apache/spark/pull/35396];;;",,,,,,,,
Inconsistent missing values handling in Pandas on Spark to_json,SPARK-38067,13425576,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bjornjorgensen,bjornjorgensen,bjornjorgensen,29/Jan/22 20:37,01/Feb/22 11:59,13/Jul/23 08:47,01/Feb/22 11:59,3.2.1,,,,,,,,3.3.0,,,,,PySpark,,,,1,,,"If {{ps.DataFrame.to_json}} is called without {{path}} argument, missing values are written explicitly 

{code:python}
import pandas as pd
import pyspark.pandas as ps

pdf = pd.DataFrame({""id"": [1, 2, 3], ""value"": [None, 3, None]})
psf = ps.from_pandas(pdf)
psf.to_json()
## '[{""id"":1,""value"":null},{""id"":2,""value"":3.0},{""id"":3,""value"":null}]'
{code:python}

This behavior is consistent with Pandas:

{code:python}
pdf.to_json()
## '{""id"":{""0"":1,""1"":2,""2"":3},""value"":{""0"":null,""1"":3.0,""2"":null}}'
{code}

However, if {{path}} is provided, missing values are omitted by default:


{code:python}
import tempfile

path = tempfile.mktemp()
psf.to_json(path)

spark.read.text(path).show()
## +--------------------+
## |               value|
## +--------------------+
## |{""id"":2,""value"":3.0}|
## |            {""id"":3}|
## |            {""id"":1}|
## +--------------------+
{code}


We should set {{ignoreNullFields}} for Pandas API, to be `False` by default, so both cases handle missing values in the same way.


{code:python}
psf.to_json(path, ignoreNullFields=False)
spark.read.text(path).show(truncate=False)


## +---------------------+
## |value                |
## +---------------------+
## |{""id"":3,""value"":null}|
## |{""id"":1,""value"":null}|
## |{""id"":2,""value"":3.0} |
## +---------------------+
{code}


",,apachespark,bjornjorgensen,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 01 11:59:28 UTC 2022,,,,,,,,,,"0|z0z308:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jan/22 20:39;apachespark;User 'bjornjorgensen' has created a pull request for this issue:
https://github.com/apache/spark/pull/35296;;;","01/Feb/22 11:59;zero323;Issue resolved by pull request 35296
[https://github.com/apache/spark/pull/35296];;;",,,,,,,,,,
Inconsistent behavior from JSON option allowNonNumericNumbers,SPARK-38060,13425453,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,andygrove,andygrove,andygrove,28/Jan/22 17:20,24/Feb/22 01:29,13/Jul/23 08:47,22/Feb/22 14:43,3.2.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"The behavior of the JSON option allowNonNumericNumbers is not consistent:

1. Some NaN and Infinity values are still parsed when the option is set to false

2. Some values are parsed differently depending on whether they are quoted or not (see results for positive and negative Infinity)
h2. Input data
{code:java}
{ ""number"": ""NaN"" }
{ ""number"": NaN }
{ ""number"": ""+INF"" }
{ ""number"": +INF }
{ ""number"": ""-INF"" }
{ ""number"": -INF }
{ ""number"": ""INF"" }
{ ""number"": INF }
{ ""number"": Infinity }
{ ""number"": +Infinity }
{ ""number"": -Infinity }
{ ""number"": ""Infinity"" }
{ ""number"": ""+Infinity"" }
{ ""number"": ""-Infinity"" }
{code}
h2. Setup
{code:java}
import org.apache.spark.sql.types._

val schema = StructType(Seq(StructField(""number"", DataTypes.FloatType, false))) {code}
h2. allowNonNumericNumbers = false
{code:java}
spark.read.format(""json"").schema(schema).option(""allowNonNumericNumbers"", ""false"").json(""nan_valid.json"")

df.show

+---------+
|   number|
+---------+
|      NaN|
|     null|
|     null|
|     null|
|     null|
|     null|
|     null|
|     null|
|     null|
|     null|
|     null|
| Infinity|
|     null|
|-Infinity|
+---------+ {code}
h2. allowNonNumericNumbers = true
{code:java}
val df = spark.read.format(""json"").schema(schema).option(""allowNonNumericNumbers"", ""true"").json(""nan_valid.json"") 

df.show

+---------+
|   number|
+---------+
|      NaN|
|      NaN|
|     null|
| Infinity|
|     null|
|-Infinity|
|     null|
|     null|
| Infinity|
| Infinity|
|-Infinity|
| Infinity|
|     null|
|-Infinity|
+---------+{code}",Running Spark 3.2.0 in local mode on Ubuntu 20.04.3 LTS,andygrove,apachespark,nartal1,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 24 01:29:34 UTC 2022,,,,,,,,,,"0|z0z28w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/22 23:15;apachespark;User 'andygrove' has created a pull request for this issue:
https://github.com/apache/spark/pull/35573;;;","18/Feb/22 23:16;apachespark;User 'andygrove' has created a pull request for this issue:
https://github.com/apache/spark/pull/35573;;;","22/Feb/22 14:43;srowen;Issue resolved by pull request 35573
[https://github.com/apache/spark/pull/35573];;;","24/Feb/22 01:28;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35641;;;","24/Feb/22 01:29;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35641;;;",,,,,,,
Structured streaming not working in history server when using LevelDB,SPARK-38056,13425399,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wekoms,wekoms,wekoms,28/Jan/22 12:43,27/Jun/22 15:21,13/Jul/23 08:47,09/Feb/22 08:01,3.1.2,3.2.0,,,,,,,3.1.4,3.2.2,3.3.0,,,Structured Streaming,Web UI,,,0,,,"In [SPARK-31953|https://github.com/apache/spark/commit/4f9667035886a67e6c9a4e8fad2efa390e87ca68], structured streaming support is added to history server. However this does not work when spark.history.store.path is set to save app info using LevelDB.

This is because one of the keys of StreamingQueryData, runId,  is UUID type, which is not supported by LevelDB. When replaying event log file in history server, StreamingQueryStatusListener will throw an exception when writing info to the store, saying ""java.lang.IllegalArgumentException: Type java.util.UUID not allowed as key."".

Example event log is provided in attachments. When opening it in history server with spark.history.store.path set to somewhere, no structured streaming info is available.",,apachespark,kabhwan,wekoms,,,,,,,,,,,,,,,,,,,,,,"28/Jan/22 12:43;wekoms;local-1643373518829;https://issues.apache.org/jira/secure/attachment/13039471/local-1643373518829",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 27 15:21:43 UTC 2022,,,,,,,,,,"0|z0z1ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/22 13:37;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/35356;;;","28/Jan/22 13:37;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/35356;;;","09/Feb/22 08:01;kabhwan;Issue resolved by pull request 35356
[https://github.com/apache/spark/pull/35356];;;","09/Feb/22 14:37;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/35463;;;","09/Feb/22 14:37;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/35463;;;","09/Feb/22 14:38;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/35464;;;","27/Jun/22 15:21;apachespark;User 'kuwii' has created a pull request for this issue:
https://github.com/apache/spark/pull/37007;;;",,,,,
Encoder cannot be found when a tuple component is a type alias for an Array,SPARK-38042,13425099,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jtnystrom,jtnystrom,jtnystrom,27/Jan/22 08:23,28/Feb/22 11:35,13/Jul/23 08:47,28/Feb/22 11:34,3.1.2,3.2.0,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"ScalaReflection.dataTypeFor fails when Array[T] has been aliased for some T, and then the alias is being used as a component of e.g. a product.

Minimal example, tested in version 3.1.2:
{code:java}
type Data = Array[Long]
val xs:List[(Data,Int)] = List((Array(1),1), (Array(2),2))
sc.parallelize(xs).toDF(""a"", ""b""){code}
This gives the following exception:
{code:java}
scala.MatchError: Data (of class scala.reflect.internal.Types$AliasNoArgsTypeRef) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$dataTypeFor$1(ScalaReflection.scala:104) 
 at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.dataTypeFor(ScalaReflection.scala:88) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$6(ScalaReflection.scala:573) 
 at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238) 
 at scala.collection.immutable.List.foreach(List.scala:392) 
 at scala.collection.TraversableLike.map(TraversableLike.scala:238) 
 at scala.collection.TraversableLike.map$(TraversableLike.scala:231) 
 at scala.collection.immutable.List.map(List.scala:298) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerFor$1(ScalaReflection.scala:562) 
 at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:432) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.$anonfun$serializerForType$1(ScalaReflection.scala:421) 
 at scala.reflect.internal.tpe.TypeConstraints$UndoLog.undo(TypeConstraints.scala:69) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects(ScalaReflection.scala:904) 
 at org.apache.spark.sql.catalyst.ScalaReflection.cleanUpReflectionObjects$(ScalaReflection.scala:903) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.cleanUpReflectionObjects(ScalaReflection.scala:49) 
 at org.apache.spark.sql.catalyst.ScalaReflection$.serializerForType(ScalaReflection.scala:413) 
 at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:55) 
 at org.apache.spark.sql.Encoders$.product(Encoders.scala:285) 
 at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder(SQLImplicits.scala:251) 
 at org.apache.spark.sql.LowPrioritySQLImplicits.newProductEncoder$(SQLImplicits.scala:251) 
 at org.apache.spark.sql.SQLImplicits.newProductEncoder(SQLImplicits.scala:32) 
 ... 48 elided{code}
At first glance, I think this could be fixed by changing e.g.
{code:java}
getClassNameFromType(tpe) to 
getClassNameFromType(tpe.dealias)
{code}
in ScalaReflection.dataTypeFor. I will try to test that and submit a pull request shortly.

 

 ",,apachespark,cloud_fan,jtnystrom,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 28 11:34:11 UTC 2022,,,,,,,,,,"0|z0z02g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/22 08:28;jtnystrom;A similar issue was solved in SPARK-21567.

 

 ;;;","31/Jan/22 05:15;apachespark;User 'jtnystrom' has created a pull request for this issue:
https://github.com/apache/spark/pull/35370;;;","08/Feb/22 23:49;jtnystrom;My initial idea above was wrong. I ended up changing
{code:java}
val TypeRef(_, _, Seq(elementType)) = tpe{code}
to
{code:java}
val TypeRef(_, _, Seq(elementType)) = tpe.dealias{code}
and this seems to work.;;;","28/Feb/22 11:34;cloud_fan;Issue resolved by pull request 35370
[https://github.com/apache/spark/pull/35370];;;",,,,,,,,
Fix ColumnVectorUtils.populate to handle CalendarIntervalType correctly,SPARK-38018,13424602,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengsu,chengsu,chengsu,25/Jan/22 07:12,07/Jul/22 17:13,13/Jul/23 08:47,25/Jan/22 11:25,3.3.0,,,,,,,,3.2.2,3.3.0,,,,Spark Core,SQL,,,0,correctness,,"`ColumnVectorUtils.populate()` does not handle CalendarInterval type correctly - [https://github.com/apache/spark/blob/master/sql/core/src/main/java/org/apache/spark/sql/execution/vectorized/ColumnVectorUtils.java#L93-L94] . The CalendarInterval type is in the format of (months: int, days: int, microseconds: long) ([https://github.com/apache/spark/blob/master/common/unsafe/src/main/java/org/apache/spark/unsafe/types/CalendarInterval.java#L58] ). However, the function above misses `days` field, and sets `microseconds` field in wrong position.

 

`ColumnVectorUtils.populate()` is used by Parquet ([https://github.com/apache/spark/blob/master/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/parquet/VectorizedParquetRecordReader.java#L258] ) and ORC ([https://github.com/apache/spark/blob/master/sql/core/src/main/java/org/apache/spark/sql/execution/datasources/orc/OrcColumnarBatchReader.java#L171] )vectorized reader to read partition column. So technically Spark can potentially produce wrong result if reading table with CalendarInterval partition column. However I also notice Spark explicitly disallows writing data with CalendarInterval type ([https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L586] ), so it might not be a big deal for users. But it's worth to fix anyway.

 

Caveat: I found the bug when reading through the related code path, and I don't have experience in production for partition column with CalendarInterval type. I think it should be an obvious fix unless anyone more experienced could find some historical context. The code was introduced a long time ago where I couldn't find any more info why it was implemented as it is ([https://github.com/apache/spark/pull/11435] )",,apachespark,chengsu,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 07 17:13:21 UTC 2022,,,,,,,,,,"0|z0yx08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/22 07:21;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/35314;;;","07/Jul/22 07:14;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/37114;;;","07/Jul/22 07:14;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/37114;;;","07/Jul/22 17:13;dongjoon;I added `correctness` label.;;;",,,,,,,,
Fix the API doc for window to say it supports TimestampNTZType too as timeColumn,SPARK-38017,13424598,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,25/Jan/22 06:57,28/Sep/22 03:32,13/Jul/23 08:47,25/Jan/22 11:45,3.2.0,,,,,,,,3.2.2,3.3.1,,,,Documentation,SQL,,,0,,,"window function supports not only TimestampType but also TimestampNTZType but the API docs doesn't mention TimestampNTZType.

This issue is similar to SPARK-38016, but this issue affects 3.2.0 too, so I separate the tickets.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 14 17:05:59 UTC 2022,,,,,,,,,,"0|z0ywzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/22 07:06;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/35313;;;","25/Jan/22 07:06;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/35313;;;","25/Jan/22 11:45;sarutak;Issue resolved in https://github.com/apache/spark/pull/35313.;;;","14/Sep/22 17:02;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/37882;;;","14/Sep/22 17:03;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/37882;;;","14/Sep/22 17:05;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/37883;;;",,,,,,
Fix the API doc for session_window to say it supports TimestampNTZType too as timeColumn,SPARK-38016,13424594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,25/Jan/22 06:40,25/Jan/22 11:42,13/Jul/23 08:47,25/Jan/22 11:42,3.3.0,,,,,,,,3.3.0,,,,,Documentation,SQL,,,0,,,"As of Spark 3.3.0, session_window supports not only TimestampType but also TimestampNTZType but the API docs doesn't mention TimestampNTZType.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 25 11:42:16 UTC 2022,,,,,,,,,,"0|z0ywyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/22 06:49;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/35312;;;","25/Jan/22 06:50;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/35312;;;","25/Jan/22 11:42;sarutak;Issue resolved in https://github.com/apache/spark/pull/35312.;;;",,,,,,,,,
Upgrade ORC to 1.6.13,SPARK-37977,13423896,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,21/Jan/22 04:15,12/Dec/22 18:11,13/Jul/23 08:47,21/Jan/22 06:49,3.2.2,,,,,,,,3.2.2,,,,,Build,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 21 06:49:21 UTC 2022,,,,,,,,,,"0|z0yso8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/22 04:32;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35266;;;","21/Jan/22 06:49;gurwls223;Issue resolved by pull request 35266
[https://github.com/apache/spark/pull/35266];;;",,,,,,,,,,
Typing incompatibilities with numpy==1.22.x,SPARK-37972,13423775,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,20/Jan/22 15:18,21/Jan/22 11:00,13/Jul/23 08:47,21/Jan/22 11:00,3.3.0,,,,,,,,3.3.0,,,,,MLlib,PySpark,,,0,,,"When type checked against {{numpy==1.22}} mypy detects following issues:


{code:python}
python/pyspark/mllib/linalg/__init__.py:412: error: Argument 2 to ""norm"" has incompatible type ""Union[float, str]""; expected ""Union[None, float, Literal['fro'], Literal['nuc']]""  [arg-type]
python/pyspark/mllib/linalg/__init__.py:457: error: No overload variant of ""dot"" matches argument types ""ndarray[Any, Any]"", ""Iterable[float]""  [call-overload]
python/pyspark/mllib/linalg/__init__.py:457: note: Possible overload variant:
python/pyspark/mllib/linalg/__init__.py:457: note:     def dot(a: Union[_SupportsArray[dtype[Any]], _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, float, complex, str, bytes, _NestedSequence[Union[bool, int, float, complex, str, bytes]]], b: Union[_SupportsArray[dtype[Any]], _NestedSequence[_SupportsArray[dtype[Any]]], bool, int, float, complex, str, bytes, _NestedSequence[Union[bool, int, float, complex, str, bytes]]], out: None = ...) -> Any
python/pyspark/mllib/linalg/__init__.py:457: note:     <1 more non-matching overload not shown>
python/pyspark/mllib/linalg/__init__.py:707: error: Argument 2 to ""norm"" has incompatible type ""Union[float, str]""; expected ""Union[None, float, Literal['fro'], Literal['nuc']]""  [arg-type]
{code}",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 21 11:00:42 UTC 2022,,,,,,,,,,"0|z0yrxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/22 15:29;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/35261;;;","21/Jan/22 11:00;zero323;Issue resolved by pull request 35261
[https://github.com/apache/spark/pull/35261];;;",,,,,,,,,,
Need to update Partition URI after renaming table in InMemoryCatalog,SPARK-37963,13423497,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,19/Jan/22 12:09,17/Jul/22 05:16,13/Jul/23 08:47,20/Jan/22 13:40,3.3.0,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,correctness,,"After renaming a partitioned table, select from the new table from InMemoryCatalog will get an empty result.

The following checkAnswer will fail as the result is empty.
{code:java}
sql(s""create table foo(i int, j int) using PARQUET partitioned by (j)"")
sql(""insert into table foo partition(j=2) values (1)"")
sql(s""alter table foo rename to bar"")
checkAnswer(spark.table(""bar""), Row(1, 2)) {code}
To fix the bug, we need to update Partition URI after renaming a table in InMemoryCatalog

 ",,apachespark,Gengliang.Wang,huaxingao,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 21 04:09:42 UTC 2022,,,,,,,,,,"0|z0yq7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/22 12:25;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35251;;;","20/Jan/22 13:40;Gengliang.Wang;Issue resolved by pull request 35251
[https://github.com/apache/spark/pull/35251];;;","21/Jan/22 04:09;huaxingao;Changed the fix version to 3.2.2 for now. Will change back if RC2 fails.;;;",,,,,,,,,
Pyspark SparkContext.AddFile() does not respect spark.files.overwrite,SPARK-37958,13423351,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yoda-mon,tschneider_live,tschneider_live,18/Jan/22 19:45,12/Dec/22 18:10,13/Jul/23 08:47,03/Feb/22 00:42,3.1.1,,,,,,,,3.3.0,,,,,Documentation,Input/Output,Java API,,0,,,"I am currently running apache spark 3.1.1. on kubernetes.

When I try to re-add a file that has already been added I see that the updated file is not actually loaded into the cluster. I see the following warning when calling the addFile() function.
{code:java}
22/01/18 19:05:50 WARN SparkContext: The path http://15.4.12.12:80/demo_data.csv has been added already. Overwriting of added paths is not supported in the current version. {code}
When I display the dataframe that was loaded I see that the old data is loaded. If I log into the worker pods and delete the file, the same results or observed.

My SparkConf has the following configurations
{code:java}
('spark.master', 'k8s://https://15.4.7.11:6443')
('spark.app.name', 'spark-jupyter-mlib')
('spark.submit.deploy.mode', 'cluster')
('spark.kubernetes.container.image', 'tschneider/apache-spark-k8:v7')
('spark.kubernetes.namespace', 'spark')
('spark.kubernetes.pyspark.pythonVersion', '3')
('spark.kubernetes.authenticate.driver.serviceAccountName', 'spark-sa')
('spark.kubernetes.authenticate.serviceAccountName', 'spark-sa')
('spark.executor.instances', '3')
('spark.executor.cores', '2')
('spark.executor.memory', '4096m')
('spark.executor.memoryOverhead', '1024m')
('spark.driver.memory', '1024m')
('spark.driver.host', '15.4.12.12')
('spark.files.overwrite', 'true')
('spark.files.useFetchCache', 'false') {code}
According to the documentation for 3.1.1. The spark.files.overwrite parameter should in fact load the updated files. The documentation can be found here: [https://spark.apache.org/docs/3.1.1/configuration.html]

The only workaround is to use a python function to manually delete and re-download the file. Calling addFile still shows the warning in this case. My code for the delete and redownload is as follows:
{code:java}
def os_remove(file_path):
    import socket
    hostname = socket.gethostname()    action = None
    import os
    if os.path.exists(file_path):
        action = ""delete""
        os.remove(file_path)
        
    return (hostname, action)worker_file_path = u""file:///{0}"".format(csv_file_name)


worker_count = int(spark_session.conf.get('spark.executor.instances'))
rdd = sc.parallelize(range(worker_count)).map(lambda var: os_remove(worker_file_path))
rdd.collect()


def download_updated_file(file_url):
    import urllib.parse as parse
    file_name = os.path.basename(parse.urlparse(csv_file_url).path)
    local_file_path = ""/{0}"".format(file_name)
    
    import urllib.request as urllib
    urllib.urlretrieve(file_url, local_file_path)
    

rdd = sc.parallelize(range(worker_count)).map(lambda var: download_updated_file(csv_file_url))
rdd.collect(){code}
I believe this is either a bug or a documentation mistake. Perhaps the configuration parameter has a misleading description?

 

 

 

 

 ",,apachespark,tschneider_live,vs1004,yoda-mon,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,9223372036854775807,,,,Thu Feb 03 00:42:02 UTC 2022,,,,,,,,,,"0|z0ypb4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/22 03:10;yoda-mon;This is probably default behavior not only on k8s or pyspark environment.

{{spark.files.overwrite}} was introduced in 1.0. Then at the PR [https://github.com/apache/spark/pull/14396] , calling addFile twice on the same name file is not allowed on drivers side.

By setting this true users can overwrite the files that already existed at startup by calling addFile or addJar, which is prohibited at default configuration.

 
{code:java}
// code placeholder
$ export K8S_ENDPOINT=""https://192.168.49.2:8443"" # minikube
$ export SPARK_IMAGE=""spark:add-file-in-advance"" # containes /opt/spark/work-dir/test.text
$ docker run -i $SPARK_IMAGE cat /opt/spark/work-dir/test.txt
...
This is original file

# put a new file on driver's side
$ cat work-dir/test.txt
Hello
spark
on
k8


# spark.files.overwrite=false
$ ./bin/spark-shell \
  --master k8s://${K8S_ENDPOINT} \
  --deploy-mode client \
  --name spark-shell \
  --conf spark.executor.instances=1 \
  --conf spark.kubernetes.container.image=${SPARK_IMAGE} \
  --conf spark.kubernetes.container.image.pullPolicy=Never \
  --conf spark.files.overwrite=false
scala> spark.read.format(""text"").load(""work-dir/test.txt"").show()
[Stage 0:>                                                                                                         +--------------------+
|               value|
+--------------------+
|This is original ...|
+--------------------+
scala> sc.addFile(""work-dir/test.txt"")
scala> spark.read.format(""text"").load(""work-dir/test.txt"").show()
22/02/01 02:48:02 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (172.17.0.4 executor 1): org.apache.spark.SparkException: File ./test.txt exists and does not match contents of spark://...


# spark.files.overwrite=true
$ ./bin/spark-shell \
  --master k8s://${K8S_ENDPOINT} \
  --deploy-mode client \
  --name spark-shell \
  --conf spark.executor.instances=1 \
  --conf spark.kubernetes.container.image=${SPARK_IMAGE} \
  --conf spark.kubernetes.container.image.pullPolicy=Never \
  --conf spark.files.overwrite=true

scala> sc.addFile(""work-dir/test.txt"")
scala> spark.read.format(""text"").load(""work-dir/test.txt"").show() 
[Stage 0:>                                                                                                         +-----+
|value|
+-----+
|Hello|
|spark|
|   on|
|  k8s|
+-----+
scala> sc.addFile(""work-dir/test.txt"")
22/02/01 02:54:08 WARN SparkContext: The path work-dir/test.txt has been added already. Overwriting of added paths is not supported in the current version{code}
 

We might be better to consider deleting this option because the PR aimed to avoid concurrency problem, but the behavior above might cause the problem.

 

Anyway, it is misleading description so I would like to open update PR.;;;","01/Feb/22 05:35;apachespark;User 'yoda-mon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35377;;;","01/Feb/22 05:35;apachespark;User 'yoda-mon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35377;;;","03/Feb/22 00:42;gurwls223;Issue resolved by pull request 35377
[https://github.com/apache/spark/pull/35377];;;",,,,,,,,
Cannot use <func>_outer generators in a lateral view,SPARK-37947,13423134,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,bersprockets,bersprockets,bersprockets,17/Jan/22 20:54,09/Mar/22 15:25,13/Jul/23 08:47,09/Mar/22 15:25,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"This works:
{noformat}
select * from values 1, 2 lateral view outer explode(array()) as b;
{noformat}
But this does not work:
{noformat}
select * from values 1, 2 lateral view explode_outer(array()) as b;
{noformat}
It produces the error:
{noformat}
Error in query: Column 'b' does not exist. Did you mean one of the following? [col1]; line 1 pos 26;
{noformat}
Similarly, this works:
{noformat}
select * from values 1, 2
lateral view outer inline(array(struct(1, 2, 3))) as b, c, d;
{noformat}
But this does not:
{noformat}
select * from values 1, 2
lateral view inline_outer(array(struct(1, 2, 3))) as b, c, d;
{noformat}
It produces the error:
{noformat}
Error in query: Column 'b' does not exist. Did you mean one of the following? [col1]; line 2 pos 0;
{noformat}",,apachespark,bersprockets,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 09 15:25:25 UTC 2022,,,,,,,,,,"0|z0ynz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/22 20:56;bersprockets;While a minor issue (you could always use ""{{outer <func>}}"" rather than ""{{<func>_outer}}"", it's a small fix, so I will make a PR.;;;","17/Jan/22 21:21;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/35232;;;","09/Mar/22 15:25;cloud_fan;Issue resolved by pull request 35232
[https://github.com/apache/spark/pull/35232];;;",,,,,,,,,
Analyzer can fail when join left side and right side are the same view,SPARK-37932,13423013,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chen,fishcus,fishcus,17/Jan/22 10:49,01/Mar/22 17:26,13/Jul/23 08:47,01/Mar/22 17:25,3.2.0,,,,,,,,3.2.2,3.3.0,,,,SQL,,,,0,,,"See the attachment for details, including SQL and the exception information.
 * sql1, there is a normal filter (LO_SUPPKEY > 10) in the right side subquery, Analyzer works as expected;
 * sql2, there is a HAVING filter(HAVING COUNT(DISTINCT LO_SUPPKEY) > 1) in the right side subquery, Analyzer failed with ""Resolved attribute(s) LO_SUPPKEY#337 missing ..."".

      From the debug info, the problem seems to be occurred after the rule DeduplicateRelations is applied.",,apachespark,cloud_fan,fishcus,yumwang,Zhixiong Chen,,,,,,,,,,,,,,,,,,,,"17/Jan/22 12:43;fishcus;sql_and_exception;https://issues.apache.org/jira/secure/attachment/13038943/sql_and_exception",,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 01 17:25:25 UTC 2022,,,,,,,,,,"0|z0yn88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/22 10:29;fishcus;test 
{code:scala}
test(""SPARK-37932: view join view self with having filter"") {
  withTable(""t"") {
    withView(""v1"") {
      Seq((2, ""test2""), (3, ""test3""), (1, ""test1"")).toDF(""id"", ""name"")
        .write.format(""parquet"").saveAsTable(""t"")
      sql(""CREATE VIEW v1 (id, name) AS SELECT id, name FROM t"")

      sql(""""""
        |SELECT l1.id
        | FROM v1 l1
        | INNER JOIN (
        | SELECT id
        | FROM v1
        | GROUP BY id
        | HAVING COUNT(DISTINCT name) > 1
        | ) l2
        | ON l1.id = l2.id
        | GROUP BY l1.name, l1.id;
        """""".stripMargin)

    }
  }
}

{code}
 

exception
{code:java}
org.apache.spark.sql.AnalysisException: Resolved attribute(s) name#25 missing from id#29,name#30 in operator !Aggregate [id#29], [id#29, count(distinct name#25) AS count(distinct name#25)#31L]. Attribute(s) with the same name appear in the operation: name. Please check if the right attribute(s) are used.;
Aggregate [name#25, id#24], [id#24]
+- Join Inner, (id#24 = id#29)
   :- SubqueryAlias l1
   :  +- SubqueryAlias spark_catalog.default.v1
   :     +- View (`default`.`v1`, [id#24,name#25])
   :        +- Project [cast(id#20 as int) AS id#24, cast(name#21 as string) AS name#25]
   :           +- Project [id#20, name#21]
   :              +- SubqueryAlias spark_catalog.default.t
   :                 +- Relation default.t[id#20,name#21] parquet
   +- SubqueryAlias l2
      +- Project [id#29]
         +- Filter (count(distinct name#25)#31L > cast(1 as bigint))
            +- !Aggregate [id#29], [id#29, count(distinct name#25) AS count(distinct name#25)#31L]
               +- SubqueryAlias spark_catalog.default.v1
                  +- View (`default`.`v1`, [id#29,name#30])
                     +- Project [cast(id#26 as int) AS id#29, cast(name#27 as string) AS name#30]
                        +- Project [id#26, name#27]
                           +- SubqueryAlias spark_catalog.default.t
                              +- Relation default.t[id#26,name#27] parquet

    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis(CheckAnalysis.scala:51)
    at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.failAnalysis$(CheckAnalysis.scala:50)

{code}
 ;;;","19/Jan/22 11:23;Zhixiong Chen;I'm working on;;;","29/Jan/22 05:15;apachespark;User 'chenzhx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35361;;;","24/Feb/22 10:33;apachespark;User 'chenzhx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35649;;;","24/Feb/22 10:34;apachespark;User 'chenzhx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35649;;;","25/Feb/22 09:52;apachespark;User 'chenzhx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35660;;;","28/Feb/22 16:29;apachespark;User 'chenzhx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35684;;;","28/Feb/22 16:30;apachespark;User 'chenzhx' has created a pull request for this issue:
https://github.com/apache/spark/pull/35684;;;","01/Mar/22 17:25;cloud_fan;Issue resolved by pull request 35684
[https://github.com/apache/spark/pull/35684];;;",,,
Remove tab character and trailing space in pom.xml,SPARK-37920,13422873,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jackey Lee,Jackey Lee,Jackey Lee,15/Jan/22 18:11,15/Jan/22 23:59,13/Jul/23 08:47,15/Jan/22 23:59,3.3.0,,,,,,,,3.3.0,,,,,Build,,,,0,,,"There are some tabs in pom.xml, which don't seem to be standardized. This pr tries to modify this problem.",,apachespark,dongjoon,Jackey Lee,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 15 23:59:20 UTC 2022,,,,,,,,,,"0|z0ymdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/22 18:22;apachespark;User 'stczwd' has created a pull request for this issue:
https://github.com/apache/spark/pull/35218;;;","15/Jan/22 23:59;dongjoon;Issue resolved by pull request 35218
[https://github.com/apache/spark/pull/35218];;;",,,,,,,,,,
Make `merge_spark_pr.py` set primary author from the first commit in case of ties,SPARK-37905,13422655,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Jan/22 03:26,14/Jan/22 04:33,13/Jul/23 08:47,14/Jan/22 04:33,3.3.0,,,,,,,,3.3.0,,,,,Project Infra,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 14 04:33:07 UTC 2022,,,,,,,,,,"0|z0yl0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/22 03:29;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35205;;;","14/Jan/22 04:33;dongjoon;Issue resolved by pull request 35205
[https://github.com/apache/spark/pull/35205];;;",,,,,,,,,,
Error while joining two tables with non-english field names,SPARK-37895,13422474,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,planga82,mkrasilnikova,mkrasilnikova,13/Jan/22 09:53,08/Mar/22 02:53,13/Jul/23 08:47,08/Mar/22 02:53,3.2.0,3.3.0,,,,,,,3.3.0,,,,,SQL,,,,0,,,"While trying to join two tables with non-english field names in postgresql with query like
""select view1.`Имя1` , view1.`Имя2`, view2.`Имя3` from view1 left join  view2 on view1.`Имя2`=view2.`Имя4`""
we get an error which says that there is no field ""`Имя4`"" (field name is surrounded by backticks).
It appears that to get the data from the second table it constructs query like
SELECT ""Имя3"",""Имя4"" FROM ""public"".""tab2""  WHERE (""`Имя4`"" IS NOT NULL) 
and these backticks are redundant in WHERE clause.",,apachespark,cloud_fan,huaxingao,mkrasilnikova,planga82,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 08 02:53:10 UTC 2022,,,,,,,,,,"0|z0yjwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/22 13:31;cloud_fan;This bug is only in JDBC v2. In the v2 code path, we always enable nested column in filter pushdown, and the column name in the predicate follows SQL style, which may have quotes.

In the long term, this problem can be fixed by using v2 filters, which has native support for nested columns, so that we don't need to encode nested column into a single string and introduce quotes. For now, I think we should fix the v1 filter pushdown code path in JDBC v2, which is `JDBCScanBuilder.pushFilters`.;;;","13/Jan/22 13:32;cloud_fan;[~beliefer]  can you help to fix it? also cc [~huaxingao] ;;;","02/Mar/22 23:18;planga82;I'm working on a fix;;;","03/Mar/22 19:31;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/35726;;;","08/Mar/22 02:53;cloud_fan;Issue resolved by pull request 35726
[https://github.com/apache/spark/pull/35726];;;",,,,,,,
Fix flaky test: AdaptiveQueryExecSuite with Scala 2.13,SPARK-37893,13422436,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,13/Jan/22 08:19,14/Jan/22 02:57,13/Jul/23 08:47,14/Jan/22 02:57,3.3.0,,,,,,,,3.3.0,,,,,Tests,,,,0,,,"Use maven test `AdaptiveQueryExecSuite` with Scala-2.13, the following exceptions will occur with a very small probability:
{code:java}
AdaptiveQueryExecSuite

- Logging plan changes for AQE *** FAILED ***
  java.util.ConcurrentModificationException: mutation occurred during iteration
  at scala.collection.mutable.MutationTracker$.checkMutations(MutationTracker.scala:43)
  at scala.collection.mutable.CheckedIndexedSeqView$CheckedIterator.hasNext(CheckedIndexedSeqView.scala:47)
  at scala.collection.StrictOptimizedIterableOps.filterImpl(StrictOptimizedIterableOps.scala:225)
  at scala.collection.StrictOptimizedIterableOps.filterImpl$(StrictOptimizedIterableOps.scala:222)
  at scala.collection.mutable.ArrayBuffer.filterImpl(ArrayBuffer.scala:43)
  at scala.collection.StrictOptimizedIterableOps.filterNot(StrictOptimizedIterableOps.scala:220)
  at scala.collection.StrictOptimizedIterableOps.filterNot$(StrictOptimizedIterableOps.scala:220)
  at scala.collection.mutable.ArrayBuffer.filterNot(ArrayBuffer.scala:43)
  at org.apache.spark.SparkFunSuite$LogAppender.loggingEvents(SparkFunSuite.scala:288)
  at org.apache.spark.sql.execution.adaptive.AdaptiveQueryExecSuite.$anonfun$new$152(AdaptiveQueryExecSuite.scal{code}
 

 ",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 14 02:57:17 UTC 2022,,,,,,,,,,"0|z0yjo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/22 09:38;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35190;;;","14/Jan/22 02:57;dongjoon;This is resolved via https://github.com/apache/spark/pull/35190;;;",,,,,,,,,,
Upgrade kubernetes-client to 5.10.2,SPARK-37884,13422313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,12/Jan/22 18:27,12/Jan/22 21:00,13/Jul/23 08:47,12/Jan/22 21:00,3.3.0,,,,,,,,3.3.0,,,,,Kubernetes,,,,0,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 12 21:00:11 UTC 2022,,,,,,,,,,"0|z0yiww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/22 18:31;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35182;;;","12/Jan/22 18:32;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35182;;;","12/Jan/22 21:00;dongjoon;Issue resolved by pull request 35182
[https://github.com/apache/spark/pull/35182];;;",,,,,,,,,
Link to Pandas UDF documentation is broken,SPARK-37874,13422140,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,alexott,alexott,12/Jan/22 07:37,12/Dec/22 18:10,13/Jul/23 08:47,14/Jan/22 06:02,3.2.0,,,,,,,,3.2.1,3.3.0,,,,Documentation,,,,0,,,Link at [https://spark.apache.org/docs/latest/api/python/user_guide/arrow_pandas.html] is broken,,alexott,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 14 06:02:20 UTC 2022,,,,,,,,,,"0|z0yhuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/22 06:02;gurwls223;Fixed in https://github.com/apache/spark/pull/34475;;;",,,,,,,,,,,
Spark should not dedup the groupingExpressions when the first child of Union has duplicate columns,SPARK-37865,13421938,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,karenfeng,chasingegg,chasingegg,11/Jan/22 09:16,06/Jul/22 20:43,13/Jul/23 08:47,09/Mar/22 01:38,3.3.0,,,,,,,,3.0.4,3.1.3,3.2.2,3.3.0,,SQL,,,,0,correctness,,"When the first child of Union has duplicate columns like select a, a from t1 union select a, b from t2, spark only use the first column to aggregate the results, which would make the results incorrect, and this behavior is inconsistent with other engines like PostgreSQL, MySQL. We could alias the attribute of the first child of union to resolve this, or you could argue that this is the feature of Spark SQL.

sample query:
select
a,
a
from values (1, 1), (1, 2) as t1(a, b)
UNION
SELECT
a,
b
from values (1, 1), (1, 2) as t2(a, b)

result is
(1,1)

result from PostgreSQL and MySQL
(1,1)
(1,2)",,apachespark,chasingegg,cloud_fan,joshrosen,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 14 04:06:02 UTC 2022,,,,,,,,,,"0|z0ygm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/22 13:53;apachespark;User 'chasingegg' has created a pull request for this issue:
https://github.com/apache/spark/pull/35168;;;","11/Jan/22 13:53;apachespark;User 'chasingegg' has created a pull request for this issue:
https://github.com/apache/spark/pull/35168;;;","23/Jan/22 14:07;apachespark;User 'chasingegg' has created a pull request for this issue:
https://github.com/apache/spark/pull/35290;;;","23/Jan/22 14:08;apachespark;User 'chasingegg' has created a pull request for this issue:
https://github.com/apache/spark/pull/35290;;;","08/Mar/22 05:44;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/35760;;;","09/Mar/22 01:38;cloud_fan;Issue resolved by pull request 35760
[https://github.com/apache/spark/pull/35760];;;","14/Mar/22 04:06;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/35842;;;",,,,,
[BUG] Revert: Fix taskid in the stage page task event timeline,SPARK-37860,13421841,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Jackey Lee,Jackey Lee,Jackey Lee,11/Jan/22 01:20,11/Jan/22 06:30,13/Jul/23 08:47,11/Jan/22 06:29,3.2.1,,,,,,,,3.0.4,3.1.3,3.2.1,3.3.0,,Web UI,,,,0,,,"In [#32888|https://github.com/apache/spark/pull/32888], [@shahidki31|https://github.com/shahidki31] change taskInfo.index to taskInfo.taskId. However, we generally use {{index.attempt}} or {{taskId}} to distinguish tasks within a stage, not {{{}taskId.attempt{}}}.
Thus [#32888|https://github.com/apache/spark/pull/32888] was a wrong fix issue, we should revert it.",,apachespark,Jackey Lee,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 11 06:30:51 UTC 2022,,,,,,,,,,"0|z0yg0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/22 02:47;apachespark;User 'stczwd' has created a pull request for this issue:
https://github.com/apache/spark/pull/35160;;;","11/Jan/22 02:48;apachespark;User 'stczwd' has created a pull request for this issue:
https://github.com/apache/spark/pull/35160;;;","11/Jan/22 06:29;sarutak;Issue resolved in https://github.com/apache/spark/pull/35160;;;","11/Jan/22 06:30;sarutak;Note: If the vote of Spark 3.2.1 RC1 passes, replace the fix version of 3.2.1 with 3.2.2.;;;",,,,,,,,
SQL tables created with JDBC with Spark 3.1 are not readable with 3.2,SPARK-37859,13421829,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,karenfeng,karenfeng,karenfeng,10/Jan/22 22:47,14/Jan/22 02:55,13/Jul/23 08:47,14/Jan/22 02:55,3.2.0,,,,,,,,3.2.1,3.3.0,,,,SQL,,,,0,,,"In https://github.com/apache/spark/blob/bd24b4884b804fc85a083f82b864823851d5980c/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JdbcUtils.scala#L312, a new metadata field is added during reading. As we do a full comparison of the user-provided schema and the actual schema in https://github.com/apache/spark/blob/bd24b4884b804fc85a083f82b864823851d5980c/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSource.scala#L356, resolution fails if a table created with Spark 3.1 is read with Spark 3.2.",,apachespark,cloud_fan,karenfeng,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 14 02:55:28 UTC 2022,,,,,,,,,,"0|z0yfxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/22 23:02;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/35158;;;","10/Jan/22 23:02;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/35158;;;","14/Jan/22 02:55;cloud_fan;Issue resolved by pull request 35158
[https://github.com/apache/spark/pull/35158];;;",,,,,,,,,
IllegalStateException when transforming an array inside a nested struct,SPARK-37855,13421736,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,gmuciaccia,gmuciaccia,10/Jan/22 14:49,12/Jan/22 19:45,13/Jul/23 08:47,12/Jan/22 19:45,3.2.0,,,,,,,,3.2.1,3.3.0,,,,Spark Core,,,,0,,,"*NOTE*: this bug is only present in version {{3.2.0}}. Downgrading to {{3.1.2}} solves the problem.

h3. Prerequisites to reproduce the bug

# use Spark version 3.2.0
# create a DataFrame with an array field, which contains a struct field with a nested array field
# *apply a limit* to the DataFrame
# transform the outer array, renaming one of its fields
# transform the inner array too, which requires two {{getField}} in sequence

h3. Example that reproduces the bug

This is a minimal example (as minimal as I could make it) to reproduce the bug:

{code}
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._
import org.apache.spark.sql.{DataFrame, Row}

def makeInput(): DataFrame = {
    val innerElement1 = Row(3, 3.12)
    val innerElement2 = Row(4, 2.1)
    val innerElement3 = Row(1, 985.2)
    val innerElement4 = Row(10, 757548.0)
    val innerElement5 = Row(1223, 0.665)

    val outerElement1 = Row(1, Row(List(innerElement1, innerElement2)))
    val outerElement2 = Row(2, Row(List(innerElement3)))
    val outerElement3 = Row(3, Row(List(innerElement4, innerElement5)))

    val data = Seq(
        Row(""row1"", List(outerElement1)),
        Row(""row2"", List(outerElement2, outerElement3)),
    )

    val schema = new StructType()
        .add(""name"", StringType)
        .add(""outer_array"", ArrayType(new StructType()
            .add(""id"", IntegerType)
            .add(""inner_array_struct"", new StructType()
                .add(""inner_array"", ArrayType(new StructType()
                    .add(""id"", IntegerType)
                    .add(""value"", DoubleType)
                ))
            )
        ))

    spark.createDataFrame(spark.sparkContext
        .parallelize(data),schema)
}

// val df = makeInput()
val df = makeInput().limit(2)
// val df = makeInput().limit(2).cache()

val res = df.withColumn(""extracted"", transform(
    col(""outer_array""),
    c1 => {
        struct(
            c1.getField(""id"").alias(""outer_id""),
            transform(
                c1.getField(""inner_array_struct"").getField(""inner_array""),
                c2 => {
                    struct(
                        c2.getField(""value"").alias(""inner_value"")
                    )
                }
            )
        )
    }
))

res.printSchema()
res.show(false)
{code}

h4. Executing the example code

When executing it as-is, the execution will fail on the {{show}} statement, with

{code}
java.lang.IllegalStateException Couldn't find _extract_inner_array#23 in [name#2,outer_array#3]
{code}

However, *if the limit is not applied, or if the DataFrame is cached after the limit, everything works* (you can uncomment the corresponding lines in the example to try it).","OS: Ubuntu 20.04.3 LTS

Scala version: 2.12.12

 ",allebacco,apachespark,gmuciaccia,ulysses,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 12 05:31:00 UTC 2022,,,,,,,,,,"0|z0yfd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/22 17:06;ulysses;The regression seems from SPARK-35636, for the quick work around, you can set config

{code:java}
set spark.sql.optimizer.nestedSchemaPruning.enabled=false;
{code}
;;;","11/Jan/22 17:15;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35170;;;","11/Jan/22 17:16;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35170;;;","12/Jan/22 05:31;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35175;;;",,,,,,,,
TaskContext is used at wrong place in BlockManagerDecommissionIntegrationSuite,SPARK-37846,13421515,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zhenhuawang,zhenhuawang,zhenhuawang,09/Jan/22 05:05,12/Dec/22 18:10,13/Jul/23 08:47,09/Jan/22 06:30,3.1.0,3.2.0,,,,,,,3.3.0,,,,,Tests,,,,0,,,,,apachespark,zhenhuawang,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 09 06:30:52 UTC 2022,,,,,,,,,,"0|z0ye08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/22 05:08;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/35137;;;","09/Jan/22 05:09;apachespark;User 'wzhfy' has created a pull request for this issue:
https://github.com/apache/spark/pull/35137;;;","09/Jan/22 06:30;gurwls223;Issue resolved by pull request 35137
[https://github.com/apache/spark/pull/35137];;;",,,,,,,,,
BasicWriteTaskStatsTracker should not try get status for a skipped file,SPARK-37841,13421261,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,07/Jan/22 09:25,11/Jan/22 09:01,13/Jul/23 08:47,11/Jan/22 09:01,3.2.0,3.3.0,,,,,,,3.3.0,,,,,SQL,,,,0,,,"https://github.com/apache/spark/pull/35117#issuecomment-1007171965

",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 11 09:01:47 UTC 2022,,,,,,,,,,"0|z0ycfs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/22 09:44;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/35132;;;","11/Jan/22 09:01;Qin Yao;Issue resolved by pull request 35132
[https://github.com/apache/spark/pull/35132];;;",,,,,,,,,,
Reenable length check in Python linter,SPARK-37834,13421193,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,07/Jan/22 02:35,12/Dec/22 18:10,13/Jul/23 08:47,07/Jan/22 06:03,3.3.0,,,,,,,,3.3.0,,,,,Project Infra,PySpark,,,0,,,SPARK-37380 mistakenly removed the length check if PySpark in the codebase. We should reenable it.,,apachespark,,,,,,,,,,,,,,,,,,,,SPARK-37850,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 07 06:03:49 UTC 2022,,,,,,,,,,"0|z0yc0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/22 03:14;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35123;;;","07/Jan/22 06:03;gurwls223;Issue resolved by pull request 35123
[https://github.com/apache/spark/pull/35123];;;",,,,,,,,,,
An outer-join using joinWith on DataFrames returns Rows with null fields instead of null values,SPARK-37829,13421101,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kings129,cdegroc,cdegroc,06/Jan/22 16:03,08/Jul/23 16:19,13/Jul/23 08:47,19/Apr/23 14:05,3.0.0,3.0.1,3.0.2,3.0.3,3.1.0,3.1.1,3.1.2,3.2.0,3.3.3,3.4.1,3.5.0,,,SQL,,,,0,,,"Doing an outer-join using {{joinWith}} on {{{}DataFrame{}}}s used to return missing values as {{null}} in Spark 2.4.8, but returns them as {{Rows}} with {{null}} values in Spark 3+.

The issue can be reproduced with [the following test|https://github.com/cdegroc/spark/commit/79f4d6a1ec6c69b10b72dbc8f92ab6490d5ef5e5] that succeeds on Spark 2.4.8 but fails starting from Spark 3.0.0.

The problem only arises when working with DataFrames: Datasets of case classes work as expected as demonstrated by [this other test|https://github.com/apache/spark/blob/v3.0.0/sql/core/src/test/scala/org/apache/spark/sql/DatasetSuite.scala#L1200-L1223].

I couldn't find an explanation for this change in the Migration guide so I'm assuming this is a bug.

A {{git bisect}} pointed me to [that commit|https://github.com/apache/spark/commit/cd92f25be5a221e0d4618925f7bc9dfd3bb8cb59].

Reverting the commit solves the problem.

A similar solution,  but without reverting, is shown [here|https://github.com/cdegroc/spark/commit/684c675bf070876a475a9b225f6c2f92edce4c8a].

Happy to help if you think of another approach / can provide some guidance.",,apachespark,cdegroc,cloud_fan,kings129,koert,xkrogen,,,,,,,,,,,,,,SPARK-44323,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 17:20:53 UTC 2023,,,,,,,,,,"0|z0ybg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/22 18:53;apachespark;User 'cdegroc' has created a pull request for this issue:
https://github.com/apache/spark/pull/35139;;;","07/Jan/22 20:14;cdegroc;CC [~cloud_fan]. Tagging you as you're the original author and you might have more context and/or ideas on how to best solve this.;;;","07/Jan/22 20:16;cdegroc;Opened two equivalent PRs: one with a fix and one with a revert.;;;","07/Jan/22 20:18;apachespark;User 'cdegroc' has created a pull request for this issue:
https://github.com/apache/spark/pull/35140;;;","29/Mar/23 20:16;kings129;
Our company encountered this issue during our migration from Spark 2.4 to 3. This issue may cause data correctness issues in our pipeline, as null is used to determine whether there is a matching row in a DataFrame outer join.

To unblock the migration, we would like to backport the fixing patch from upstream. However, I noticed that the pull requests above have been closed due to inactivity. 
[~cdegroc], are you planning to resume this work? 

By the way, I'm happy to help in any way!;;;","01/Apr/23 05:27;cdegroc;I'm not planning to resume. I don't know that part of the codebase well enough to submit a better fix other than the one I already submitted in my PR.;;;","05/Apr/23 16:52;kings129;Got it, thank you [~cdegroc]!
The codebase is kinda complicated, but I'll give it a shot.;;;","13/Apr/23 17:49;kings129;I created a pull request for this issue: (somehow it's not automatically linked here, so I manually comment here)
[https://github.com/apache/spark/pull/40755];;;","19/Apr/23 14:05;cloud_fan;Issue resolved by pull request 40755
[https://github.com/apache/spark/pull/40755];;;","06/Jul/23 17:20;koert;since this (admittedly somewhat weird) behavior of returning a Row with null values has been present since spark 3.0.x (a major breaking release, and 3 years ago) i would argue this is the default behavior and this jira introduces a breaking change.

basically i am saying if one argues this was a breaking change in going from spark 2.x to 3.x then i agree but a major version can make a breaking change. introducing a fix in 3.4.1 that reverts that breaking change is basically introducing a breaking change going from 3.4.0 to 3.4.1 which is worse in my opinion.

also expressionencoders are used for other purposes than dataset joins and now we find nulls popping up in places they should not. this is how i ran into this issue.;;;",,
Replace ApacheCommonBase64 with JavaBase64 for string fucntions,SPARK-37820,13420958,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,06/Jan/22 02:52,07/Jan/22 05:32,13/Jul/23 08:47,07/Jan/22 05:32,3.3.0,,,,,,,,3.3.0,,,,,SQL,,,,0,,,"Replace dependency on third-party libraries with native support(https://docs.oracle.com/javase/8/docs/api/java/util/Base64.html
) for Base-64 encode/decode.


1. Performace gain

http://java-performance.info/base64-encoding-and-decoding-performance/

2. reduce dependencies afterward




 


 

",,apachespark,dongjoon,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 07 05:32:56 UTC 2022,,,,,,,,,,"0|z0yakg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/22 02:58;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/35110;;;","07/Jan/22 05:32;dongjoon;Issue resolved by pull request 35110
[https://github.com/apache/spark/pull/35110];;;",,,,,,,,,,
Fix a typo in HttpAuthenticationException message,SPARK-37807,13420560,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,rkchilaka,rkchilaka,rkchilaka,04/Jan/22 10:24,05/Jan/22 07:33,13/Jul/23 08:47,05/Jan/22 07:32,3.2.0,,,,,,,,3.1.3,3.2.1,3.3.0,,,SQL,,,,0,,,Please find the PR (https://github.com/apache/spark/pull/35097),,apachespark,dongjoon,rkchilaka,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 05 07:32:08 UTC 2022,,,,,,,,,,"0|z0y848:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/22 10:26;apachespark;User 'RamakrishnaChilaka' has created a pull request for this issue:
https://github.com/apache/spark/pull/35097;;;","05/Jan/22 07:32;dongjoon;Issue resolved by pull request 35097
[https://github.com/apache/spark/pull/35097];;;",,,,,,,,,,
composite field name like `field name` doesn't work with Aggregate push down,SPARK-37802,13420313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,huaxingao,huaxingao,huaxingao,03/Jan/22 06:00,10/Jan/22 08:02,13/Jul/23 08:47,07/Jan/22 03:37,3.2.0,3.3.0,,,,,,,3.2.1,3.3.0,,,,SQL,,,,0,,,"
{code:java}
sql(""SELECT SUM(`field name`) FROM h2.test.table"")

org.apache.spark.sql.catalyst.parser.ParseException: 
extraneous input 'name' expecting <EOF>(line 1, pos 9)

	at org.apache.spark.sql.catalyst.parser.ParseErrorListener$.syntaxError(ParseDriver.scala:212)
	at org.antlr.v4.runtime.ProxyErrorListener.syntaxError(ProxyErrorListener.java:41)
	at org.antlr.v4.runtime.Parser.notifyErrorListeners(Parser.java:544)
	at org.antlr.v4.runtime.DefaultErrorStrategy.reportUnwantedToken(DefaultErrorStrategy.java:377)
	at org.antlr.v4.runtime.DefaultErrorStrategy.singleTokenDeletion(DefaultErrorStrategy.java:548)
	at org.antlr.v4.runtime.DefaultErrorStrategy.recoverInline(DefaultErrorStrategy.java:467)
	at org.antlr.v4.runtime.Parser.match(Parser.java:206)
	at org.apache.spark.sql.catalyst.parser.SqlBaseParser.singleMultipartIdentifier(SqlBaseParser.java:519)
{code}
",,apachespark,cloud_fan,huaxingao,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 07 05:50:02 UTC 2022,,,,,,,,,,"0|z0y6lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/22 19:33;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/35108;;;","05/Jan/22 19:33;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/35108;;;","07/Jan/22 03:37;cloud_fan;Issue resolved by pull request 35108
[https://github.com/apache/spark/pull/35108];;;","07/Jan/22 05:50;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/35125;;;",,,,,,,,
TreeNode.argString incorrectly formats arguments of type Set[_],SPARK-37800,13420233,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,simeons,simeons,simeons,01/Jan/22 23:43,12/Dec/22 18:10,13/Jul/23 08:47,04/Jan/22 04:39,3.2.0,,,,,,,,3.2.1,3.3.0,,,,SQL,,,,0,,,"The implementation of {{argString}} uses the following pattern for sets:

 
{code:java}
case set: Set[_] =>
  // Sort elements for deterministic behaviours
  val sortedSeq = set.toSeq.map(formatArg(_, maxFields).sorted)                                                              
  truncatedString(sortedSeq, ""{"", "", "", ""}"", maxFields) :: Nil {code}
Instead of sorting the elements of the set, the implementation sorts the characters of the strings that {{formatArg}} returns. 

The fix is simply to move the closing parenthesis to the correct location:
{code:java}
  val sortedSeq = set.toSeq.map(formatArg(_, maxFields)).sorted
{code}
 ",,apachespark,cloud_fan,simeons,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 05 09:11:28 UTC 2022,,,,,,,,,,"0|z0y63s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/22 00:48;gurwls223;Yeah, from a cursory look, what you said here sounds sane. interested in creating a PR?;;;","02/Jan/22 01:12;simeons;[~hyukjin.kwon] Done: https://github.com/apache/spark/pull/35084;;;","02/Jan/22 01:12;apachespark;User 'ssimeonov' has created a pull request for this issue:
https://github.com/apache/spark/pull/35084;;;","04/Jan/22 04:39;cloud_fan;Issue resolved by pull request 35084
[https://github.com/apache/spark/pull/35084];;;","05/Jan/22 09:11;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35105;;;",,,,,,,
