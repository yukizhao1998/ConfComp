Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Inward issue link (Completes),Outward issue link (Completes),Outward issue link (Completes),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Supercedes),Inward issue link (Testing),Outward issue link (Testing),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Release Note),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy constantly fails,FLINK-32490,13541970,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,29/Jun/23 16:04,29/Jun/23 16:11,13/Jul/23 08:29,29/Jun/23 16:11,1.18.0,,,,,,1.18.0,,,,,,,,Table SQL / API,,,,0,test-stability,,,"ArrayElementOutputTypeStrategyTest fails with
{noformat}
[ERROR] Failures: 
[ERROR]   ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy:58 
expected: ""INT NOT NULL (AtomicDataType@26e4eacd)""
 but was: ""INT NOT NULL (AtomicDataType@1716b369)""
[ERROR]   ArrayElementOutputTypeStrategyTest>TypeStrategiesTestBase.testTypeStrategy:58 
expected: ""INT (AtomicDataType@18ab74e7)""
 but was: ""INT (AtomicDataType@f0704a2)""
{noformat}
 

also could be reproduced locally",,mapohl,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32257,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 16:11:52 UTC 2023,,,,,,,,,,"0|z1ivn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 16:11;mapohl;master: 5ad86c2f01bea141ca76250ae4035d4e6403c8ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceCoordinatorAlignmentTest.testAnnounceCombinedWatermarkWithoutStart fails,FLINK-32478,13541888,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,fanrui,fanrui,29/Jun/23 06:14,30/Jun/23 12:16,13/Jul/23 08:29,30/Jun/23 12:15,1.16.3,1.17.2,1.18.0,,,,1.16.3,1.17.2,1.18.0,,,,,,Connectors / Common,,,,1,pull-request-available,test-stability,,"SourceCoordinatorAlignmentTest.testAnnounceCombinedWatermarkWithoutStart fails

 

Root cause: multiple sources share the same thread pool, and the second source cannot start due to the first source closes the shared thread pool.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50611&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8613",,fanrui,Feifan Wang,mapohl,martijnvisser,RocMarshal,Sergey Nuyanzin,taoran,,,,,,,,,,,,,,,,,,,,FLINK-32487,,,FLINK-32411,FLINK-32316,FLINK-32495,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 12:16:05 UTC 2023,,,,,,,,,,"0|z1iv5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jun/23 06:18;Feifan Wang;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50623&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8168;;;","29/Jun/23 06:36;fanrui;All of Flink master, release-1.16 and release-1.17 have this bug, sorry for causing this problem.

I'm fixing it with high priority.;;;","29/Jun/23 10:41;Sergey Nuyanzin;I'm going to upgrade the priority to blocker since all builds on master mirror started to fail now...;;;","29/Jun/23 10:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50611&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8613;;;","29/Jun/23 10:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50612&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8389;;;","29/Jun/23 10:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50613&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8357;;;","29/Jun/23 10:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50617&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8541;;;","29/Jun/23 10:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50619&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8542;;;","29/Jun/23 10:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50621&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8617;;;","29/Jun/23 10:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50627&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8368;;;","29/Jun/23 10:44;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50643&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8617;;;","29/Jun/23 13:06;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50646&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8095;;;","29/Jun/23 14:38;fanrui;Sorry again for this bug, merged.

<master: 1.18> 210dc81612ddd80abbe46c66322a21c1b38f2000

1.17: a26dfdd44b37a719a2fba2ac8a8a53074332ea74

1.16: 2327a320fd63e3c49838da998e9154a8ac571855;;;","29/Jun/23 17:20;martijnvisser;Looking at https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50667&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8 I don’t think this is yet fixed [~fanrui];;;","29/Jun/23 20:09;Sergey Nuyanzin;yep, there is another case for 1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50668&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=9089;;;","30/Jun/23 03:24;fanrui;Hi [~martijnvisser]  [~Sergey Nuyanzin] [~mapohl] 

Sorry it wasn't fully fixed, SourceCoordinatorAlignmentTest.testWatermarkAlignmentWithTwoGroups fails now. I have analyzed it, and created FLINK-32495. Let's follow it there.

BTW, FLINK-32495 doesn't happen every time, I just run CI at master branch before merging. I merged them for master, release-1.16 and release-1.17 after the master CI passed.

I will submit multiple PRs in the future for each release branch, and check all CIs. Try to avoid it in the future.

Sorry again for this mistake.;;;","30/Jun/23 05:34;Sergey Nuyanzin;[~fanrui] thanks for working on this

I wonder whether it is possible to retry ci (for the same commit in PR) 2-3 times in a row to be a bit more sure that this or new related issue doesn't appear?;;;","30/Jun/23 05:56;fanrui;I have submit 3 PRs(master, 1.16, 1.17) for FLINK-32495, their changed are same, and they will run CI 3 times, do you think it's ok? Or do you want to run 3 times for master PR?

BTW, I have run `SourceCoordinatorAlignmentTest.testWatermarkAlignmentWithTwoGroups` on my Local more than 1K times. They are fine now with FLINK-32495.

You can check the test at this [comment|https://issues.apache.org/jira/browse/FLINK-32495?focusedCommentId=17738873&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17738873].;;;","30/Jun/23 06:05;Sergey Nuyanzin;yes, my initial idea was about master

thanks, i will have a look

UPD:

{quote}
 I have submit 3 PRs(master, 1.16, 1.17) for FLINK-32495, their changed are same, and they will run CI 3 times
{quote}
IMHO I don't think we can treat them as same ci runs at least because it's not cherry pick (changes are not identical)

 ;;;","30/Jun/23 06:21;taoran;[~fanrui] hi. ci got this problem again.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50662&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8617;;;","30/Jun/23 06:45;martijnvisser;You can always check https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary for the overview of all PRs that have been merged into master/release branches for an indication if the problem has been resolved. ;;;","30/Jun/23 08:19;fanrui;{quote} I don't think we can treat them as same ci runs at least because it's not cherry pick (changes are not identical)
{quote}
Actually, the first commit of them are cherry pick. I developed the master commit, and cherry pick to 1.16 and 1.17.

Also, I added a limitation for master PR based on this bug.

Why do I prefer to run only 1 time?
 * It blocks all CIs, I want to fix it asap. Each separate CI run takes 4 hours and I'm afraid we won't be able to fix it today.
 * They are cherry pick, 3 different PRs can show whether the problem has been fixed.
 * I run the failed test on my Mac more than 1K times.

Of course, running CI of each PR three times is more robust. If all of you think it's necessary, I will follow it.

 ;;;","30/Jun/23 12:16;martijnvisser;I re-opened this before FLINK-32495 was filed. With that ticket now closed, it looks like all issues are resolved so closing this ticket too. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid input strategy for many functions which allows BINARY strings,FLINK-32466,13541748,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,twalthr,twalthr,28/Jun/23 09:35,10/Jul/23 17:12,13/Jul/23 08:29,10/Jul/23 17:12,,,,,,,1.18.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"""string"" in SQL terms covers both character strings and binary strings. The author of CONCAT might not have known this. In any case, the code gen instead of the validator fails when executing:

{code}
TableEnvironment t = TableEnvironment.create(EnvironmentSettings.inStreamingMode());
t.createTemporaryView(""t"", t.fromValues(lit(new byte[] {97})));
t.executeSql(""SELECT CONCAT(f0, '-magic') FROM t"").print();
{code}

As future work, we should also allow binary strings.",,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 10 17:12:15 UTC 2023,,,,,,,,,,"0|z1iua8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 11:49;twalthr;This bugs affects many functions not only CONCAT. In the PR I went through all functions and synchronized the strategy with the runtime implementation.;;;","10/Jul/23 17:12;twalthr;Merged to master: 4cf2124d71a8dd0595e40f07c2dbcc4c85883b82;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KerberosLoginProvider.isLoginPossible does accidental login with keytab,FLINK-32465,13541745,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,28/Jun/23 09:30,28/Jun/23 15:51,13/Jul/23 08:29,28/Jun/23 15:51,1.17.2,1.18.0,,,,,1.17.2,1.18.0,,,,,,,API / Core,,,,0,pull-request-available,,,"In KerberosLoginProvider.isLoginPossible there is a call to UserGroupInformation.getCurrentUser() before principal check (keytab usage). This triggers an accidental login with either kerberos credentials if available, or as the local OS user, based on security settings. This is not problematic most of the time since KerberosLoginProvider.doLogin overwrites the credentials with keytab. The problem hurts however when login fails for whatever reason. Such case the workload is just not starting.",,gaborgsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 13:39:12 UTC 2023,,,,,,,,,,"0|z1iu9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 13:39;gaborgsomogyi;e33875e on master
0472cc1 on release-1.17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
manage  union operator state increase very large in Jobmanager ,FLINK-32461,13541685,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,1026688210,1026688210,28/Jun/23 08:06,02/Jul/23 02:51,13/Jul/23 08:29,02/Jul/23 02:51,1.17.1,,,,,,,,,,,,,,,,,,0,,,,"This issue doesn't usually occur, but it happens during busy nights when the machines are more active. The ""manage operator state"" will increase significantly, and I found the number of  operator union state object is 128 ,same with the parallelism .Whether the union state only needs to be loaded once?

 !screenshot-1.png! 
 !image-2023-06-28-16-24-11-538.png! 
",,1026688210,fanrui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/23 08:24;1026688210;image-2023-06-28-16-24-11-538.png;https://issues.apache.org/jira/secure/attachment/13060920/image-2023-06-28-16-24-11-538.png","28/Jun/23 08:08;1026688210;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13060919/screenshot-1.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-28 08:06:56.0,,,,,,,,,,"0|z1itw8:",9223372036854775807,it's a usage problem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-kafka does not build against Flink 1.18-SNAPSHOT,FLINK-32453,13541653,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,tzulitai,tzulitai,28/Jun/23 02:25,12/Jul/23 12:51,13/Jul/23 08:29,12/Jul/23 12:51,kafka-3.0.0,,,,,,kafka-3.0.1,kafka-3.1.0,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"There are a few breaking changes in test utility code that prevents {{apache/flink-connector-kafka}} from building against Flink 1.18-SNAPSHOT. This umbrella ticket captures all breaking changes, and should only be closed once we make things build again.",,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 02:27:45 UTC 2023,,,,,,,,,,"0|z1itp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 02:27;tzulitai;cc 1.18.0 release managers [~knaufk] [~martijnvisser] [~renqs] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector Shared Utils checks out wrong branch when running CI for PRs,FLINK-32448,13541612,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,27/Jun/23 17:58,27/Jun/23 18:53,13/Jul/23 08:29,27/Jun/23 18:52,,,,,,,,,,,,,,,Build System,,,,0,pull-request-available,,,"Since FLINK-31923, when a branch is not specified, all CI runs use {{main}} as the default branch when none is specified. This doesn't work when submitting a PR, since it shouldn't use {{main}} but it should use the specific ref that triggered that workflow. ",,martijnvisser,mason6345,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31923,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 18:52:51 UTC 2023,,,,,,,,,,"0|z1itg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 18:52;martijnvisser;PR also tests this behavior now, then it was tried on https://github.com/apache/flink-connector-gcp-pubsub/pull/11, https://github.com/apache/flink-connector-gcp-pubsub/pull/12 (which was broken first) and lastly with a manually triggered nightly build at https://github.com/apache/flink-connector-gcp-pubsub/actions/runs/5393387648

Fixed in apache/flink-connector-shared-utils:ci_utils - edaacb2b7fb7d19b9aa79b144eb40bca2c447d51;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
table hints lost when they inside a view referenced by an external query,FLINK-32447,13541582,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,27/Jun/23 14:23,29/Jun/23 04:26,13/Jul/23 08:29,28/Jun/23 04:27,1.17.1,,,,,,1.17.2,1.18.0,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Table hints will lost when they inside a view referenced by an external query, this is due to the upgrading of calcite-1.28 (affected by CALCITE-4640 which changed the default implementation of SqlDialect suppresses all table hints).
This can be reproduced by adding a new case to current {code}OptionsHintTest{code}:
{code}
+
+  @Test
+  def testOptionsHintInsideView(): Unit = {
+    util.tableEnv.executeSql(
+      ""create view v1 as select * from t1 /*+ OPTIONS(k1='#v111', k4='#v444')*/"")
+    util.verifyExecPlan(s""""""
+                           |select * from t2 join v1 on v1.a = t2.d
+                           |"""""".stripMargin)
+  }
{code}
wrong plan which lost table hints(dynamic options):
{code}
Join(joinType=[InnerJoin], where=[(a = d)], select=[d, e, f, a, b, c], leftInputSpec=[NoUniqueKey], rightInputSpec=[NoUniqueKey])
:- Exchange(distribution=[hash[d]])
:  +- LegacyTableSourceScan(table=[[default_catalog, default_database, t2, source: [OptionsTableSource(props={k3=v3, k4=v4})]]], fields=[d, e, f])
+- Exchange(distribution=[hash[a]])
   +- Calc(select=[a, b, (a + 1) AS c])
      +- LegacyTableSourceScan(table=[[default_catalog, default_database, t1, source: [OptionsTableSource(props={k1=v1, k2=v2})]]], fields=[a, b])
{code}

We should use {code}AnsiSqlDialect{code} instead to reserve table hints.",,lincoln.86xy,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 04:26:06 UTC 2023,,,,,,,,,,"0|z1it9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jun/23 04:27;lincoln.86xy;fixed in master: e35f33f539b09b2c18fb826a144c08541e148dd0;;;","29/Jun/23 04:26;lincoln.86xy;fixed in 1.17:  8da3785d798cc6111f463ee1665ff62a42098274;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MongoWriter should regularly check whether the last write time is greater than the specified time.,FLINK-32446,13541560,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,27/Jun/23 12:09,28/Jun/23 06:08,13/Jul/23 08:29,28/Jun/23 04:07,mongodb-1.0.0,mongodb-1.0.1,,,,,mongodb-1.1.0,,,,,,,,Connectors / MongoDB,,,,0,pull-request-available,,,"Mongo sink waits for new record to write previous records. I have a upsert-kafka topic filled that has already some events. I start a new upsert-kafka to mongo db sink job. I expect all the data from the topic to be loaded to mongodb right away. But instead, only the first record is written to mongo db. The rest of the records don’t arrive in mongodb until a new event is written to kafka topic. The new event that was written is delayed until the next event arrives. 

To prevent this problem, the MongoWriter should regularly check whether the last write time is greater than the specified time.",,jiabao.sun,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,https://lists.apache.org/thread/15mmltprxdb7hyjv0syok6fzcbfk9coj,,,,,,,,,,9223372036854775807,,,,Wed Jun 28 04:07:19 UTC 2023,,,,,,,,,,"0|z1it4g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 12:10;jiabao.sun;[~Leonard] Can you help assign this ticket to me?;;;","27/Jun/23 13:24;leonard;Thanks  [~jiabao.sun] for reporting this issue, assigned to you.;;;","28/Jun/23 04:07;leonard;Fixed in flink-connector-mongodb(main): 49b7550fbc0285e1c605c5b0efdae762cd9b144b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DefaultSchedulerTest#testTriggerCheckpointAndCompletedAfterStore fails with timeout on AZP,FLINK-32441,13541480,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,srichter,Sergey Nuyanzin,Sergey Nuyanzin,26/Jun/23 22:21,27/Jun/23 13:08,13/Jul/23 08:29,27/Jun/23 12:50,1.18.0,,,,,,1.18.0,,,,,,,,API / Core,Tests,,,0,pull-request-available,test-stability,,"This build https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50461&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9274

fails with timeout on {{DefaultSchedulerTest#testTriggerCheckpointAndCompletedAfterStore}}

",,martijnvisser,Sergey Nuyanzin,srichter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 12:50:26 UTC 2023,,,,,,,,,,"0|z1isn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jun/23 06:30;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50467&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8576;;;","27/Jun/23 07:49;Sergey Nuyanzin;[~srichter] it looks it starts appearing after merging of FLINK-32347
could you please have a look?;;;","27/Jun/23 11:41;martijnvisser;Other occurrences: 
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50480&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9212

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50489&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9195

Depending if this will occur more, we'll have to bump this one to a blocker;;;","27/Jun/23 12:50;srichter;Fixed in master 0c787f5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix adaptive local hash agg can't work when auxGrouping exist,FLINK-32426,13541296,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,lsy,lsy,lsy,25/Jun/23 09:59,29/Jun/23 12:27,13/Jul/23 08:29,29/Jun/23 12:27,1.17.1,1.18.0,,,,,1.18.0,,,,,,,,,,,,0,pull-request-available,,,"For the following case, the field `a` is primary key,  we select from `AuxGroupingTable` and group by a, b. Since a is primary key, it also guarantee the unique, so planner will extract b as auxGrouping field.
{code:java}
registerCollection(
  ""AuxGroupingTable"",
  data2,
  type2,
  ""a, b, c, d, e"",
  nullablesOfData2,
  FlinkStatistic.builder().uniqueKeys(Set(Set(""a"").asJava).asJava).build())

checkResult(
  ""SELECT a, b, COUNT(c) FROM AuxGroupingTable GROUP BY a, b"",
  Seq(
    row(1, 1, 1),
    row(2, 3, 2),
    row(3, 4, 3),
    row(4, 10, 4),
    row(5, 11, 5)
  )
) {code}
 

Due to the generated code doesn't get auxGrouping fields from input RowData and then setting it to aggBuffer, the aggBuffer RowData loses some fields, and it will throw an index Exception when get the field from it. As following:
{code:java}
Caused by: java.lang.AssertionError: index (1) should < 1
    at org.apache.flink.table.data.binary.BinaryRowData.assertIndexIsValid(BinaryRowData.java:127)
    at org.apache.flink.table.data.binary.BinaryRowData.isNullAt(BinaryRowData.java:156)
    at org.apache.flink.table.data.utils.JoinedRowData.isNullAt(JoinedRowData.java:113)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.toBinaryRow(RowDataSerializer.java:201)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:103)
    at org.apache.flink.table.runtime.typeutils.RowDataSerializer.serialize(RowDataSerializer.java:48)
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:165)
    at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.serialize(StreamElementSerializer.java:43)
    at org.apache.flink.runtime.plugable.SerializationDelegate.write(SerializationDelegate.java:54)
    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.serializeRecord(RecordWriter.java:141)
    at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)
    at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:134)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collectAndCheckIfChained(RecordWriterOutput.java:114)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:95)
    at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:48)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)
    at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)
    at LocalHashAggregateWithKeys$39.processElement_split2(Unknown Source)
    at LocalHashAggregateWithKeys$39.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at BatchExecCalc$10.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at SourceConversion$6.processElement(Unknown Source)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.pushToOperator(ChainingOutput.java:108)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:77)
    at org.apache.flink.streaming.runtime.tasks.ChainingOutput.collect(ChainingOutput.java:39)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
    at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
    at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333) {code}",,jark,lsy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 12:27:54 UTC 2023,,,,,,,,,,"0|z1iri8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 02:07;jark;Could you add more description about what error will be thrown in which case? That would be helpful for SEO for users searching similar problems. ;;;","29/Jun/23 12:27;jark;Fixed in master: 3f485162a372818c1402d78bf9fb25e06ca1cdf7;;;","29/Jun/23 12:27;jark;[~lsy], do we need to fix it in 1.17.2 as well? If yes, could you help to create a PR for release-1.17 branch? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Opensearch Connector wrong empty doc link,FLINK-32425,13541294,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,tanyuxin,tanyuxin,25/Jun/23 09:21,26/Jun/23 09:21,13/Jul/23 08:29,26/Jun/23 01:56,,,,,,,,,,,,,,,Connectors / Opensearch,,,,0,pull-request-available,,,"There is an empty link(""see here for further information"") in https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/opensearch/. And we should fix this.

The link should be like this (""See how to link with it for cluster execution here."") in https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/opensearch/
",,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 09:21:26 UTC 2023,,,,,,,,,,"0|z1irhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jun/23 09:21;Weijie Guo;merged to main: 502274a308bedd1735d496099927031411482ad9.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmbeddedLeaderServiceTest.testConcurrentRevokeLeadershipAndShutdown is not properly implemented,FLINK-32421,13541194,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,23/Jun/23 15:51,05/Jul/23 15:52,13/Jul/23 08:29,05/Jul/23 15:52,1.16.2,1.17.1,1.18.0,,,,1.16.3,1.17.2,1.18.0,,,,,,Runtime / Coordination,Tests,,,0,pull-request-available,,,"The purpose of {{EmbeddedLeaderServiceTest.testConcurrentRevokeLeadershipAndShutdown}} is to check that there is no {{NullPointerException}} happening if the event processing happens after the shutdown of the {{EmbeddedExecutorService}} (see FLINK-11855).

But the concurrent execution is not handled properly. The test also succeeds if the close call happened before the shutdown (due to the multi-threaded nature of the test) which leaves us without the actual test scenario being tested.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 05 15:52:41 UTC 2023,,,,,,,,,,"0|z1iqvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jul/23 15:52;mapohl;master: 5dbbc695ed90241bc22d01d05f19e2bdfb4b1e0b
1.17: 067df7b9ce14067375adffcc05bab5513592d646
1.16: 56cb5442333413e7617b12e5796c806d64de62e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobID collisions in FlinkSessionJob,FLINK-32412,13541002,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fabiowanner,fabiowanner,fabiowanner,22/Jun/23 09:30,26/Jun/23 14:17,13/Jul/23 08:29,26/Jun/23 14:17,kubernetes-operator-1.5.0,,,,,,kubernetes-operator-1.6.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"From time to time we see {{JobId}} collisions in our deployments due to the low entropy of the generated {{{}JobId{}}}. The problem is that, although the {{uid}} from the k8s-resource (which is a UUID V4), only the {{hashCode}} of it will be used for the {{{}JobId{}}}. The {{hashCode}} is an integer, thus 32 bits. If we look at the birthday problem theorem we can expect a collision with a 50% chance with only 77000 random integers. 

In reality we seem to see the problem more often, but this could be because the {{uid}} might not be completely random, therefore increasing the chances if we just use parts of it.

We propose to at least use the complete 64 bits of the upper part of the {{{}JobId{}}}, where 5.1×10{^}9{^} IDs are needed for a collision chance of 50%. We could even argue that most probably 64 bit for the generation number is not needed and another 32 bit could be spent on the uid to increase the entropy of the {{JobId}} even further (This would mean the max generation would be 4,294,967,295).

Our suggestion for using 64 bits would be:
{code:java}
new JobID(
    UUID.fromString(Preconditions.checkNotNull(uid)).getMostSignificantBits(), 
    Preconditions.checkNotNull(generation)
);
{code}
Any thoughts on this? I would create a PR once we know how to proceed.",,fabiowanner,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 26 14:17:38 UTC 2023,,,,,,,,,,"0|z1ippc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Jun/23 10:04;gyfora;I think this is a very good improvement. We just have to make sure to not break the existing jobs but since the JobId is recorded in the status I think we are good.;;;","22/Jun/23 10:14;fabiowanner;Ohh that's a very good point, I did not think about! I will try this out to be extra sure and then open a PR with the suggested solution! Thanks for the fast reply!;;;","26/Jun/23 14:17;gyfora;merged to main abf9d040ae58caf8313ca7b71049d6709fa26ea3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager HA configuration update needed in Flink k8s Operator ,FLINK-32408,13540894,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,dongwoo.kim,dongwoo.kim,21/Jun/23 13:09,21/Jun/23 14:54,25/Jun/23 09:53,21/Jun/23 14:54,kubernetes-operator-1.5.0,,,,,,kubernetes-operator-1.6.0,,,,,,,,Kubernetes Operator,,,,0,,,,"In flink 1.17 documentation it says, to configure job manger ha we have to configure *high-availability.type* key not *high-availability* key{*}.{*} (It seems to be changed from 1.17)

And currently kubernetes-operator-1.5.0 says it supports flink 1.17 version. 
So I expected that configuring job manager ha with *high-availability.type* should work but it didn't, only *high-availability* works

*ref*
[https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/config/#high-availability] 
[https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-release-1.5/docs/concepts/overview/#core] ",,dongwoo.kim,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 21 13:26:18 UTC 2023,,,,,,,,,,"0|z1ip1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Jun/23 13:20;dongwoo.kim;I think this is fixed in new version because in main branch's pom.xml, flink version is updated from *1.16.1* to *1.17.1.*
But then I'm curious whether we can say that kubernetes-operator-1.5.0 supports flink 1.17 version, since they had flink version configured to 1.16.1;;;","21/Jun/23 13:26;gyfora;If you are using Operator 1.5.0 with Flink 1.17.1 you need to use the old config key. But it will work;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several jobs failed on AZP with No space left on device,FLINK-32392,13540732,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,renqs,Sergey Nuyanzin,Sergey Nuyanzin,20/Jun/23 10:00,28/Jun/23 06:10,13/Jul/23 08:29,28/Jun/23 06:10,1.16.3,1.17.2,1.18.0,,,,1.16.3,1.17.2,1.18.0,,,,,,Test Infrastructure,,,,0,pull-request-available,test-stability,,"This Build failed with no space left https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50162&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b
{noformat}
##[error]Unhandled exception. System.IO.IOException: No space left on device : '/home/vsts/agents/3.220.5/_diag/Worker_20230619-021757-utc.log'
   at System.IO.RandomAccess.WriteAtOffset(SafeFileHandle handle, ReadOnlySpan`1 buffer, Int64 fileOffset)
   at System.IO.Strategies.BufferedFileStreamStrategy.FlushWrite()
   at System.IO.StreamWriter.Flush(Boolean flushStream, Boolean flushEncoder)
   at System.Diagnostics.TextWriterTraceListener.Flush()
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.WriteHeader(String source, TraceEventType eventType, Int32 id) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 151
   at Microsoft.VisualStudio.Services.Agent.HostTraceListener.TraceEvent(TraceEventCache eventCache, String source, TraceEventType eventType, Int32 id, String message) in /home/vsts/work/1/s/src/Microsoft.VisualStudio.Services.Agent/HostTraceListener.cs:line 81
   at System.Diagnostics.TraceSource.TraceEvent(TraceEventType eventType, Int32 id, String message)
   at Microsoft.VisualStudio.Services.Agent.Util.ProcessInvoker.ProcessExitedHandler(Object sender, EventArgs e) in /home/vsts/work/1/s/src/Agent.Sdk/ProcessInvoker.cs:line 496
   at System.Diagnostics.Process.OnExited()
   at System.Diagnostics.Process.RaiseOnExited()
   at System.Diagnostics.Process.CompletionCallback(Object waitHandleContext, Boolean wasSignaled)
   at System.Threading._ThreadPoolWaitOrTimerCallback.WaitOrTimerCallback_Context_f(Object state)
   at System.Threading.ExecutionContext.RunInternal(ExecutionContext executionContext, ContextCallback callback, Object state)
--- End of stack trace from previous location ---
   at System.Threading._ThreadPoolWaitOrTimerCallback.PerformWaitOrTimerCallback(_ThreadPoolWaitOrTimerCallback helper, Boolean timedOut)
   at System.Threading.PortableThreadPool.CompleteWait(RegisteredWaitHandle handle, Boolean timedOut)
   at System.Threading.ThreadPoolWorkQueue.Dispatch()
   at System.Threading.PortableThreadPool.WorkerThread.WorkerThreadStart()
,##[error]The hosted runner encountered an error while running your job. (Error Type: Failure).
{noformat}
for 1.16, 1.17 it happens while   'Upload artifacts to S3'
for 1.18 while 'Deploy maven snapshot'",,renqs,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 09:13:04 UTC 2023,,,,,,,,,,"0|z1io1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Jun/23 10:11;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50150&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff;;;","20/Jun/23 10:12;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50151&view=logs&s=9fca669f-5c5f-59c7-4118-e31c641064f0&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b;;;","20/Jun/23 10:17;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50149&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f&l=23712;;;","20/Jun/23 10:32;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50130&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f&l=23711;;;","20/Jun/23 10:34;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50071&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b;;;","20/Jun/23 10:40;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50070&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=7b3c1df5-9194-5183-5ebd-5567f52d5f8f;;;","26/Jun/23 04:39;renqs;Some investigations:
 * The case only happens on Microsoft-hosted agents, whose limit on disk space is 10GB.
 * The local Maven repo (.m2) is cached across pipeline runs, which is ~15GB currently, and it kept growing in the past year. 

I think the root cause is that the cached local Maven repo has a lot of outdated dependencies and never get cleared. I created a PR to invalidate the cache per year so that the size of local Maven repo won't grow indefinitely. ;;;","26/Jun/23 09:38;renqs;Fixed on master: 9b63099964b36ad9d78649bb6f5b39473e0031bd;;;","26/Jun/23 22:23;Sergey Nuyanzin;1.17.x: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=50397&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b&t=68e20e55-906c-5c49-157c-3005667723c9;;;","27/Jun/23 09:13;renqs;Backport to 1.17: 0740649c714b50b83572f14300132fd1a1c41ea8

1.16: 020990a86e78f6ee12f23ea6fb152baf6226947a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecNodeGraphInternalPlan#writeToFile should support TRUNCATE_EXISTING for overwriting,FLINK-32374,13540540,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,19/Jun/23 01:25,20/Jun/23 01:49,13/Jul/23 08:29,20/Jun/23 01:49,1.16.0,1.16.1,1.16.2,1.17.0,1.17.1,1.18.0,1.18.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"If the existing JSON plan is not truncated when overwriting, and the newly generated JSON plan contents are shorter than the previous JSON plan content, the plan be an invalid JSON.
h4. How to reproduce
{code:sql}
Flink SQL> create table debug_sink (f0 int, f1 string) with ('connector' = 'blackhole');
[INFO] Execute statement succeed.

Flink SQL> create table dummy_source (f0 int, f1 int, f2 string, f3 string) with ('connector' = 'datagen');
[INFO] Execute statement succeed.

Flink SQL> compile plan '/foo/bar/debug.json' for insert into debug_sink select if(f0 > f1, f0, f1) as f0, concat(f2, f3) as f1 from dummy_source;
[INFO] Execute statement succeed.

Flink SQL> set 'table.plan.force-recompile' = 'true';
[INFO] Execute statement succeed.

Flink SQL> compile plan '/foo/bar/debug.json' for insert into debug_sink select * from (values (2, 'bye')) T (id, message);
[INFO] Execute statement succeed.
{code}
cat -n debug.json, and check L#67
{code:json}
     1	{
     2	  ""flinkVersion"" : ""1.17"",
     3	  ""nodes"" : [ {
     4	    ""id"" : 15,
     5	    ""type"" : ""stream-exec-values_1"",
     6	    ""tuples"" : [ [ {
     7	      ""kind"" : ""LITERAL"",
     8	      ""value"" : ""2"",
     9	      ""type"" : ""INT NOT NULL""
    10	    }, {
    11	      ""kind"" : ""LITERAL"",
    12	      ""value"" : ""bye"",
    13	      ""type"" : ""CHAR(3) NOT NULL""
    14	    } ] ],
    15	    ""outputType"" : ""ROW<`id` INT NOT NULL, `message` CHAR(3) NOT NULL>"",
    16	    ""description"" : ""Values(tuples=[[{ 2, _UTF-16LE'bye' }]])"",
    17	    ""inputProperties"" : [ ]
    18	  }, {
    19	    ""id"" : 16,
    20	    ""type"" : ""stream-exec-sink_1"",
    21	    ""configuration"" : {
    22	      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
    23	      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
    24	      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
    25	      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    26	    },
    27	    ""dynamicTableSink"" : {
    28	      ""table"" : {
    29	        ""identifier"" : ""`default_catalog`.`default_database`.`debug_sink`"",
    30	        ""resolvedTable"" : {
    31	          ""schema"" : {
    32	            ""columns"" : [ {
    33	              ""name"" : ""f0"",
    34	              ""dataType"" : ""INT""
    35	            }, {
    36	              ""name"" : ""f1"",
    37	              ""dataType"" : ""VARCHAR(2147483647)""
    38	            } ],
    39	            ""watermarkSpecs"" : [ ]
    40	          },
    41	          ""partitionKeys"" : [ ],
    42	          ""options"" : {
    43	            ""connector"" : ""blackhole""
    44	          }
    45	        }
    46	      }
    47	    },
    48	    ""inputChangelogMode"" : [ ""INSERT"" ],
    49	    ""inputProperties"" : [ {
    50	      ""requiredDistribution"" : {
    51	        ""type"" : ""UNKNOWN""
    52	      },
    53	      ""damBehavior"" : ""PIPELINED"",
    54	      ""priority"" : 0
    55	    } ],
    56	    ""outputType"" : ""ROW<`id` INT NOT NULL, `message` CHAR(3) NOT NULL>"",
    57	    ""description"" : ""Sink(table=[default_catalog.default_database.debug_sink], fields=[id, message])""
    58	  } ],
    59	  ""edges"" : [ {
    60	    ""source"" : 15,
    61	    ""target"" : 16,
    62	    ""shuffle"" : {
    63	      ""type"" : ""FORWARD""
    64	    },
    65	    ""shuffleMode"" : ""PIPELINED""
    66	  } ]
    67	} ""$CONCAT$1"",
    68	      ""operands"" : [ {
    69	        ""kind"" : ""INPUT_REF"",
    70	        ""inputIndex"" : 2,
    71	        ""type"" : ""VARCHAR(2147483647)""
    72	      }, {
    73	        ""kind"" : ""INPUT_REF"",
    74	        ""inputIndex"" : 3,
    75	        ""type"" : ""VARCHAR(2147483647)""
    76	      } ],
    77	      ""type"" : ""VARCHAR(2147483647)""
    78	    } ],
    79	    ""condition"" : null,
    80	    ""inputProperties"" : [ {
    81	      ""requiredDistribution"" : {
    82	        ""type"" : ""UNKNOWN""
    83	      },
    84	      ""damBehavior"" : ""PIPELINED"",
    85	      ""priority"" : 0
    86	    } ],
    87	    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    88	    ""description"" : ""Calc(select=[IF((f0 > f1), f0, f1) AS f0, CONCAT(f2, f3) AS f1])""
    89	  }, {
    90	    ""id"" : 14,
    91	    ""type"" : ""stream-exec-sink_1"",
    92	    ""configuration"" : {
    93	      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
    94	      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
    95	      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
    96	      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    97	    },
    98	    ""dynamicTableSink"" : {
    99	      ""table"" : {
   100	        ""identifier"" : ""`default_catalog`.`default_database`.`debug_sink`"",
   101	        ""resolvedTable"" : {
   102	          ""schema"" : {
   103	            ""columns"" : [ {
   104	              ""name"" : ""f0"",
   105	              ""dataType"" : ""INT""
   106	            }, {
   107	              ""name"" : ""f1"",
   108	              ""dataType"" : ""VARCHAR(2147483647)""
   109	            } ],
   110	            ""watermarkSpecs"" : [ ]
   111	          },
   112	          ""partitionKeys"" : [ ],
   113	          ""options"" : {
   114	            ""connector"" : ""blackhole""
   115	          }
   116	        }
   117	      }
   118	    },
   119	    ""inputChangelogMode"" : [ ""INSERT"" ],
   120	    ""inputProperties"" : [ {
   121	      ""requiredDistribution"" : {
   122	        ""type"" : ""UNKNOWN""
   123	      },
   124	      ""damBehavior"" : ""PIPELINED"",
   125	      ""priority"" : 0
   126	    } ],
   127	    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
   128	    ""description"" : ""Sink(table=[default_catalog.default_database.debug_sink], fields=[f0, f1])""
   129	  } ],
   130	  ""edges"" : [ {
   131	    ""source"" : 12,
   132	    ""target"" : 13,
   133	    ""shuffle"" : {
   134	      ""type"" : ""FORWARD""
   135	    },
   136	    ""shuffleMode"" : ""PIPELINED""
   137	  }, {
   138	    ""source"" : 13,
   139	    ""target"" : 14,
   140	    ""shuffle"" : {
   141	      ""type"" : ""FORWARD""
   142	    },
   143	    ""shuffleMode"" : ""PIPELINED""
   144	  } ]
   145	}
{code}",,luoyuxia,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31956,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 09:23:08 UTC 2023,,,,,,,,,,"0|z1imuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jun/23 09:23;luoyuxia;master: f69ed3454f2ab200310edee230da292ee2408503

1.17: 3b5c1f7915ecfe0f7e353fd342f8d22df5bcd7c4

1.16: 08bced4646c4bef9aca7089d0764426d78a89b0a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesTestFixture doesn't implement the checkAndUpdateConfigMapFunction properly,FLINK-32368,13540393,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,16/Jun/23 12:31,19/Jun/23 06:46,13/Jul/23 08:29,19/Jun/23 06:46,1.16.2,1.17.1,1.18.0,,,,1.16.3,1.17.2,1.18.0,,,,,,Deployment / Kubernetes,Tests,,,0,pull-request-available,,,"[FlinkKubeClient.checkAndUpdateConfigMap|https://github.com/apache/flink/blob/ab3eb40d920fa609f49164a0bbb5fcbb3004a808/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/FlinkKubeClient.java#L163] expects an error to be forwarded through the {{CompletableFuture}} instead of throwing a {{RuntimeException}}.

The actual implementation implements it accordingly in [Fabric8FlinkKubeClient:313|https://github.com/apache/flink/blob/025a95b627faf8ec8b725a7784d1279b41e10ba7/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/Fabric8FlinkKubeClient.java#L313] where a {{CompletionException}} is thrown within the {{CompletableFuture}}'s {{supplyAsync}} call resulting the future to fail.

{{KubernetesTestFixture}} doesn't make the returned future complete exceptionally but throws a {{CompletionException}} (see [KubernetesTestFixture:172|https://github.com/apache/flink/blob/6fc5f789869433688eb5f62494f1a4404e0dd11b/flink-kubernetes/src/test/java/org/apache/flink/kubernetes/highavailability/KubernetesTestFixture.java#L172]).

This results in inconsistent test behavior.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 16 15:41:24 UTC 2023,,,,,,,,,,"0|z1ilz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jun/23 15:41;mapohl;master: ed854c0240cf6f53fa6e784c8267e69ff88a06e6
1.17: c359fe884b6e23f47f7904c8be0d3081b05ebd48
1.16: d4bdca1f76ece7e73549b3379cbb8b81c8f5728f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Elasticsearch v3.0 won't compile when testing against Flink 1.17.1,FLINK-32357,13540212,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,reta,martijnvisser,martijnvisser,15/Jun/23 11:48,19/Jun/23 11:58,13/Jul/23 08:29,19/Jun/23 11:58,,,,,,,elasticsearch-3.0.2,,,,,,,,Connectors / ElasticSearch,,,,0,pull-request-available,,,"{code:java}
[INFO] ------------------------------------------------------------------------
Error:  Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test (default-test) on project flink-connector-elasticsearch-base: Execution default-test of goal org.apache.maven.plugins:maven-surefire-plugin:3.0.0-M5:test failed: org.junit.platform.commons.JUnitException: TestEngine with ID 'archunit' failed to discover tests: com.tngtech.archunit.lang.syntax.elements.MethodsThat.areAnnotatedWith(Ljava/lang/Class;)Ljava/lang/Object; -> [Help 1]
{code}

https://github.com/apache/flink-connector-elasticsearch/actions/runs/5277721611/jobs/9546112876#step:13:159

",,martijnvisser,reta,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 11:58:34 UTC 2023,,,,,,,,,,"0|z1ikuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 11:49;martijnvisser;[~snuyanzin] [~reta] Haven't we seen this issue before with Opensearch as well? ;;;","15/Jun/23 11:54;Sergey Nuyanzin;I remember something like that, I will have a look;;;","15/Jun/23 12:32;reta;Yes, we certainly had it !;;;","19/Jun/23 11:55;Sergey Nuyanzin;Merged to v3.0 as [9876d39269b36b2a47f819541fb10e774e573e09|https://github.com/apache/flink-connector-elasticsearch/commit/9876d39269b36b2a47f819541fb10e774e573e09]
;;;","19/Jun/23 11:58;Sergey Nuyanzin;besides mentioned in description there was another issue fixed with 
[36409d2683de67558e5977b29dc21bd359a5afec|https://github.com/apache/flink-connector-elasticsearch/commit/36409d2683de67558e5977b29dc21bd359a5afec]
now nightly is green https://github.com/apache/flink-connector-elasticsearch/actions/runs/5311373807;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MongoDB tests are flaky and time out,FLINK-32348,13540181,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,jiabao.sun,martijnvisser,martijnvisser,15/Jun/23 09:47,12/Jul/23 09:13,13/Jul/23 08:29,12/Jul/23 09:13,,,,,,,mongodb-1.1.0,,,,,,,,Connectors / MongoDB,,,,0,pull-request-available,test-stability,,https://github.com/apache/flink-connector-mongodb/actions/runs/5232649632/jobs/9447519651#step:13:39307,,jiabao.sun,leonard,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 12 09:13:20 UTC 2023,,,,,,,,,,"0|z1iko0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 09:49;martijnvisser;A full run with thread dumps can be found at https://github.com/apache/flink-connector-mongodb/actions/runs/5276796512/jobs/9543998611?pr=10#step:15:48;;;","15/Jun/23 09:52;martijnvisser;[~jiabao.sun] Could you take a look?;;;","15/Jun/23 10:10;jiabao.sun;Sure.;;;","30/Jun/23 02:29;jiabao.sun;Hi [~martijnvisser]

I tried to investigate and reproduce the issue, and found that when the `AsyncCheckpointRunnable` meets `CancellationException`, the task never stops as expected.

I think this problem may relate to [FLINK-25902|https://issues.apache.org/jira/browse/FLINK-25902].
The root cause of this problem remains to be further investigated.


{code:sh}
00:30:26,533 [flink-akka.actor.default-dispatcher-10] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering Checkpoint 4 for job 79765d8c304b804a1adbd3677bc39708 failed due to org.apache.flink.runtime.checkpoint.CheckpointException: TaskManager received a checkpoint request for unknown task 6585a08f46e2d380ebe0ac7fde3739a7_cbc357ccb763df2852fee8c4fc7d55f2_1_1. Failure reason: Task local checkpoint failure.
00:30:26,533 [    Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 4 for job 79765d8c304b804a1adbd3677bc39708. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: TaskManager received a checkpoint request for unknown task 6585a08f46e2d380ebe0ac7fde3739a7_cbc357ccb763df2852fee8c4fc7d55f2_1_1. Failure reason: Task local checkpoint failure.
	at org.apache.flink.runtime.taskexecutor.TaskExecutor.triggerCheckpoint(TaskExecutor.java:1025) ~[flink-runtime-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at sun.reflect.GeneratedMethodAccessor34.invoke(Unknown Source) ~[?:?]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_372]
	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_372]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[?:?]
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[?:?]
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) ~[?:?]
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) ~[?:?]
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[?:?]
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) ~[?:?]
	at akka.actor.Actor.aroundReceive(Actor.scala:537) ~[?:?]
	at akka.actor.Actor.aroundReceive$(Actor.scala:535) ~[?:?]
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) ~[?:?]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) ~[?:?]
	at akka.actor.ActorCell.invoke(ActorCell.scala:547) ~[?:?]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) ~[?:?]
	at akka.dispatch.Mailbox.run(Mailbox.scala:231) ~[?:?]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243) ~[?:?]
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) ~[?:1.8.0_372]
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) ~[?:1.8.0_372]
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) ~[?:1.8.0_372]
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) ~[?:1.8.0_372]
00:30:26,554 [AsyncOperations-thread-1] INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Sink: Data stream collect sink (1/1)#1 - asynchronous part of checkpoint 4 could not be completed.
java.util.concurrent.CancellationException: null
	at java.util.concurrent.FutureTask.report(FutureTask.java:121) ~[?:1.8.0_372]
	at java.util.concurrent.FutureTask.get(FutureTask.java:192) ~[?:1.8.0_372]
	at org.apache.flink.util.concurrent.FutureUtils.runIfNotDoneAndGet(FutureUtils.java:544) ~[flink-core-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:60) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) ~[flink-streaming-java-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_372]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_372]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_372]
00:30:26,559 [jobmanager-io-thread-2] WARN  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Received late message for now expired checkpoint attempt 4 from task 6585a08f46e2d380ebe0ac7fde3739a7_cbc357ccb763df2852fee8c4fc7d55f2_0_1 of job 79765d8c304b804a1adbd3677bc39708 at c5c049a8-08be-4625-9431-5e9d1f75ba01 @ localhost (dataPort=41367).
00:30:26,609 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 5 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1686443426609 for job 79765d8c304b804a1adbd3677bc39708.
00:30:26,899 [jobmanager-io-thread-2] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 5 for job 79765d8c304b804a1adbd3677bc39708 (2327692 bytes, checkpointDuration=290 ms, finalizationTime=0 ms).
00:30:26,906 [SourceCoordinator-Source: MongoDB-Source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 5 as completed for source Source: MongoDB-Source.
00:30:26,906 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 6 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1686443426906 for job 79765d8c304b804a1adbd3677bc39708.
00:30:27,170 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 6 for job 79765d8c304b804a1adbd3677bc39708 (2327692 bytes, checkpointDuration=263 ms, finalizationTime=1 ms).
00:30:27,170 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointRequestDecider [] - checkpoint request time in queue: 161
00:30:27,171 [SourceCoordinator-Source: MongoDB-Source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 6 as completed for source Source: MongoDB-Source.
00:30:27,171 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 7 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1686443427170 for job 79765d8c304b804a1adbd3677bc39708.
00:30:27,424 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 7 for job 79765d8c304b804a1adbd3677bc39708 (2327692 bytes, checkpointDuration=254 ms, finalizationTime=0 ms).
00:30:27,427 [SourceCoordinator-Source: MongoDB-Source] INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Marking checkpoint 7 as completed for source Source: MongoDB-Source.
{code}

;;;","30/Jun/23 12:26;martijnvisser;[~jiabao.sun] That problem was fixed with Flink 1.15, while this issue occurs with later Flink versions. ;;;","03/Jul/23 07:26;martijnvisser;This week's run also failed: https://github.com/apache/flink-connector-mongodb/actions/runs/5433810366/jobs/9881741648#step:13:39415;;;","03/Jul/23 07:28;martijnvisser;[~jiabao.sun] It looks like MongoSourceITCase#testRecovery hangs:

{code:java}
""main"" #1 prio=5 os_prio=0 tid=0x00007fedb800a800 nid=0xc75 sleeping[0x00007fedbefd2000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:245)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:114)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at java.util.Iterator.forEachRemaining(Iterator.java:115)
	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:122)
	at org.apache.flink.connector.mongodb.source.MongoSourceITCase.testRecovery(MongoSourceITCase.java:264)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
{code};;;","11/Jul/23 17:07;jiabao.sun;[~martijnvisser] Sorry for the late reply.

The root cause of this error is not removing it from readersAwaitingSplit when closing an idle reader.
This resulted in splits being incorrectly assigned to readers that did not complete when resuming tasks from checkpoints.

1. readersAwaitingSplit: [0]
2. signalNoMoreSplits but not remove 0 from readersAwaitingSplit
3. TaskManager failover
4. split request from reader 1 -> readersAwaitingSplit: [0, 1]
5. but actually assigns split to reader 0.

The PR is ready, could you help review it?;;;","12/Jul/23 09:13;leonard;Fixed by flink-connector-mongodb(main): e2babc9bcfa501a3f6727f28c677c199f7bfcad5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions from the CompletedCheckpointStore are not registered by the CheckpointFailureManager ,FLINK-32347,13540172,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,srichter,vivacell,vivacell,15/Jun/23 09:05,27/Jun/23 13:12,13/Jul/23 08:29,27/Jun/23 13:10,1.15.3,1.16.2,1.17.1,,,,1.18.0,,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"Currently if an error occurs while saving a completed checkpoint in the {_}CompletedCheckpointStore{_}, _CheckpointCoordinator_ doesn't call _CheckpointFailureManager_ to handle the error. Such behavior leads to the fact, that errors from _CompletedCheckpointStore_ don't increase the failed checkpoints count and _'execution.checkpointing.tolerable-failed-checkpoints'_ option does not limit the number of errors of this kind in any way.

Possible solution may be to move the notification of _CheckpointFailureManager_ about successful checkpoint after storing completed checkpoint in the _CompletedCheckpointStore_ and providing the exception to the _CheckpointFailureManager_ in the {_}CheckpointCoordinator#{_}{_}[addCompletedCheckpointToStoreAndSubsumeOldest()|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1440]{_} method.",,srichter,Thesharing,vivacell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 13:23:06 UTC 2023,,,,,,,,,,"0|z1ikm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 12:16;vivacell;Hi, [~srichter] ! I've already fixed this issue in our local flink fork, so I can share it if you haven't started working on the task yet.;;;","15/Jun/23 13:23;srichter;Hey, I've already opened a PR. The issue was still unassigned, so I thought I can still work on it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL array_union could return wrong result,FLINK-32337,13540035,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,14/Jun/23 11:11,15/Jun/23 22:43,13/Jul/23 08:29,15/Jun/23 22:39,1.18.0,,,,,,1.18.0,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,"This is was mentioned at [https://github.com/apache/flink/pull/22717#issuecomment-1587333488]

 how to reproduce
{code:sql}
SELECT array_union(ARRAY[CAST(NULL AS INT)], ARRAY[1]); -- returns [NULL, 1], this is OK
SELECT array_union(ARRAY[1], ARRAY[CAST(NULL AS INT)]); -- returns [1, 0], this is NOT OK
{code}",,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 22:39:07 UTC 2023,,,,,,,,,,"0|z1ijrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 22:39;Sergey Nuyanzin;Merged as [9bcebe7db3abfae6474e9dc4f1180bc5b63f2a75|https://github.com/apache/flink/commit/9bcebe7db3abfae6474e9dc4f1180bc5b63f2a75];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator failed to create taskmanager deployment because it already exist,FLINK-32334,13540002,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nfraison.datadog,nfraison.datadog,nfraison.datadog,14/Jun/23 07:57,19/Jun/23 09:58,13/Jul/23 08:29,19/Jun/23 09:58,kubernetes-operator-1.5.0,,,,,,kubernetes-operator-1.6.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"During a job upgrade the operator has failed to start the new job because it has failed to create the taskmanager deployment:

 
{code:java}
Jun 12 19:45:28.115 >>> Status | Error | UPGRADING | {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster \""flink-metering\""."",""throwableList"":[{""type"":""org.apache.flink.client.deployment.ClusterDeploymentException"",""message"":""Could not create Kubernetes cluster \""flink-metering\"".""},{""type"":""org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException"",""message"":""Failure executing: POST at: https://10.129.144.1/apis/apps/v1/namespaces/metering/deployments. Message: object is being deleted: deployments.apps \""flink-metering-taskmanager\"" already exists. Received status: Status(apiVersion=v1, code=409, details=StatusDetails(causes=[], group=apps, kind=deployments, name=flink-metering-taskmanager, retryAfterSeconds=null, uid=null, additionalProperties={}), kind=Status, message=object is being deleted: deployments.apps \""flink-metering-taskmanager\"" already exists, metadata=ListMeta(_continue=null, remainingItemCount=null, resourceVersion=null, selfLink=null, additionalProperties={}), reason=AlreadyExists, status=Failure, additionalProperties={}).""}]} {code}
As indicated in the error log this is due to taskmanger deployment still existing while it is under deletion.

Looking at the [source code|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java#L150] we are well relying on FOREGROUND policy by default.

Still it seems that the REST API call to delete only wait until the resource has been modified and the {{deletionTimestamp}} has been added to the metadata: [ensure delete returns only when the delete operation is fully finished -  Issue #3246 -  fabric8io/kubernetes-client|https://github.com/fabric8io/kubernetes-client/issues/3246#issuecomment-874019899]

So we could face this issue if the k8s cluster is slow to ""really"" delete the deployment

 ",,gyfora,nfraison.datadog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 19 09:58:35 UTC 2023,,,,,,,,,,"0|z1ijk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Jun/23 08:04;nfraison.datadog;Adding a check that object has been well deleted should be enough for this issue
{code:java}
kubernetesClient
        .apps()
        .deployments()
        .inNamespace(namespace)
        .withName(StandaloneKubernetesUtils.getTaskManagerDeploymentName(clusterId))
        .waitUntilCondition(Objects::isNull, 30, TimeUnit.SECONDS); {code};;;","14/Jun/23 09:29;gyfora;We should not add waiting unnecessarily to all deletions, because we simply don't need to wait in most cases synchronously. 
There is a waitForClusterShutdown(...) in the flink service that is called when we actually need to wait. Maybe the implementation doesn't work correctly for standalone clusters.;;;","14/Jun/23 09:45;nfraison.datadog;I can see in the log that we have wait once and then ""found"" the cluster to be shutdown
{code:java}
Jun 12 19:45:25.133 for cluster shutdown...
Jun 12 19:45:27.192 shutdown completed. {code}
I think the issue is due to the fact that we only look for JM pods not for TM one

I will see to add a check for the TM in it (and only for standalone clusters);;;","19/Jun/23 09:58;gyfora;merged to main 4d9615f5c76672c9b324ed8f4876d62af7fef60e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlServerDynamicTableSourceITCase is flaky,FLINK-32325,13539817,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,martijnvisser,martijnvisser,13/Jun/23 09:40,15/Jun/23 11:37,13/Jul/23 08:29,15/Jun/23 09:03,jdbc-3.1.1,jdbc-3.2.0,,,,,jdbc-3.1.1,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,,,"{code:java}
[INFO] Running org.apache.flink.connector.jdbc.databases.sqlserver.table.SqlServerDynamicTableSourceITCase
[INFO] Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 12.533 s - in org.apache.flink.connector.jdbc.JdbcITCase
[INFO] Running org.apache.flink.connector.jdbc.databases.sqlserver.table.SqlServerTableSourceITCase
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 39249b3b-40c2-4f71-9598-20abe5e93d2d Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 2c0e4870-7284-4022-b97d-7f441fc834dd Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 34e9eff4-445c-477e-8975-d23180897ff8 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:50 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 0d4fe549-66e7-4354-b7c5-ed7ee66527d2 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:51 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 4e98d176-2f1f-4dec-af3e-798ecb536c39 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:52 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 32ba6716-772b-42f3-b27c-9ec1593adcd7 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:32ba6716-772b-42f3-b27c-9ec1593adcd7
Jun 13, 2023 8:49:53 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: fe5e363f-fade-48b8-beb1-9f2e3a524282 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:54 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: b454a476-5f05-4cd7-bb43-f62e1f7e030e Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:55 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 50282ce3-1fdc-4fa5-8467-4cd4867a8395 Prelogin error: host localhost port 32783 Unexpected end of prelogin response after 0 bytes read
Jun 13, 2023 8:49:56 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 837d01bc-7b0c-4532-88d2-1d91671d74f3 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:837d01bc-7b0c-4532-88d2-1d91671d74f3
Jun 13, 2023 8:49:57 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 43ed7181-5b5d-46e3-b7d2-f3cd2decb043 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:43ed7181-5b5d-46e3-b7d2-f3cd2decb043
Jun 13, 2023 8:49:58 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: f5a54844-ef86-4675-9b39-ede75733686b Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:f5a54844-ef86-4675-9b39-ede75733686b
Jun 13, 2023 8:49:59 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: 82da197b-0c48-4cb1-9a0b-e5dbfa27c616 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:82da197b-0c48-4cb1-9a0b-e5dbfa27c616
Jun 13, 2023 8:50:00 AM com.microsoft.sqlserver.jdbc.SQLServerConnection Prelogin
WARNING: ConnectionID:1 ClientConnectionId: df9a8372-08cb-4686-85de-2a5ca5aabe52 Prelogin error: host localhost port 32783 Error reading prelogin response: Connection reset ClientConnectionId:df9a8372-08cb-4686-85de-2a5ca5aabe52
{code}

https://github.com/apache/flink-connector-jdbc/actions/runs/5253247136/jobs/9490321045#step:13:322",,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32342,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-06-13 09:40:59.0,,,,,,,,,,"0|z1iif4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CI mirror service on Azure stops responding to commits to master,FLINK-32322,13539788,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,Sergey Nuyanzin,Sergey Nuyanzin,13/Jun/23 06:52,13/Jun/23 12:33,13/Jul/23 08:29,13/Jun/23 12:33,1.18.0,,,,,,1.18.0,,,,,,,,Build System / Azure Pipelines,,,,0,,,,Last time it ran it was on Saturday(11.06.2023),,jingge,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 12:32:45 UTC 2023,,,,,,,,,,"0|z1ii8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jun/23 12:32;jingge;The disk was full and has been cleaned up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CrateDB relies on flink-shaded in flink-connector-jdbc,FLINK-32313,13539653,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,snuyanzin,martijnvisser,martijnvisser,12/Jun/23 11:47,12/Jun/23 18:50,13/Jul/23 08:29,12/Jun/23 18:50,,,,,,,jdbc-3.2.0,,,,,,,,Connectors / JDBC,,,,0,pull-request-available,,,See https://github.com/apache/flink-connector-jdbc/blob/main/flink-connector-jdbc/src/main/java/org/apache/flink/connector/jdbc/databases/cratedb/catalog/CrateDBCatalog.java#L27 - JDBC shouldn't rely on flink-shaded. ,,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 18:50:52 UTC 2023,,,,,,,,,,"0|z1iheo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 11:47;martijnvisser;[~matriv] This is a blocker for JDBC, can you make sure that no Flink-Shaded is used by CrateDB? ;;;","12/Jun/23 18:50;martijnvisser;Fixed in main: 28016fdec16621e37b0e42322b1a542ca79343f8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement and DefaultLeaderElectionService.onGrantLeadership fell into dead lock,FLINK-32311,13539651,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,Sergey Nuyanzin,Sergey Nuyanzin,12/Jun/23 11:17,15/Jun/23 12:00,13/Jul/23 08:29,15/Jun/23 12:00,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49750&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8]

 

there are 2 threads one locked {{0x00000000e3a8a1e8}} and waiting for {{0x00000000e3a89c18}}

{noformat}

2023-06-08T01:18:54.5609123Z Jun 08 01:18:54 ""ForkJoinPool-50-worker-25-EventThread"" #956 daemon prio=5 os_prio=0 tid=0x00007f9374253800 nid=0x6a4e waiting for monitor entry [0x00007f94b63e1000]
2023-06-08T01:18:54.5609820Z Jun 08 01:18:54    java.lang.Thread.State: BLOCKED (on object monitor)
2023-06-08T01:18:54.5610557Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.runInLeaderEventThread(DefaultLeaderElectionService.java:425)
2023-06-08T01:18:54.5611459Z Jun 08 01:18:54 	- waiting to lock <0x00000000e3a89c18> (a java.lang.Object)
2023-06-08T01:18:54.5612198Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onGrantLeadership(DefaultLeaderElectionService.java:300)
2023-06-08T01:18:54.5613110Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.isLeader(ZooKeeperLeaderElectionDriver.java:153)
2023-06-08T01:18:54.5614070Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch$$Lambda$1649/586959400.accept(Unknown Source)
2023-06-08T01:18:54.5615014Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.lambda$forEach$0(MappingListenerManager.java:92)
2023-06-08T01:18:54.5616259Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager$$Lambda$1640/1393625763.run(Unknown Source)
2023-06-08T01:18:54.5617137Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager$$Lambda$1633/2012730699.execute(Unknown Source)
2023-06-08T01:18:54.5618047Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.MappingListenerManager.forEach(MappingListenerManager.java:89)
2023-06-08T01:18:54.5618994Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.listen.StandardListenerManager.forEach(StandardListenerManager.java:89)
2023-06-08T01:18:54.5620071Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.setLeadership(LeaderLatch.java:711)
2023-06-08T01:18:54.5621198Z Jun 08 01:18:54 	- locked <0x00000000e3a8a1e8> (a org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch)
2023-06-08T01:18:54.5622072Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.checkLeadership(LeaderLatch.java:597)
2023-06-08T01:18:54.5622991Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.access$600(LeaderLatch.java:64)
2023-06-08T01:18:54.5623988Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch$7.processResult(LeaderLatch.java:648)
2023-06-08T01:18:54.5624965Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926)
2023-06-08T01:18:54.5626218Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683)
2023-06-08T01:18:54.5627369Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
2023-06-08T01:18:54.5628353Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetChildrenBuilderImpl$2.processResult(GetChildrenBuilderImpl.java:187)
2023-06-08T01:18:54.5629281Z Jun 08 01:18:54 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:666)
2023-06-08T01:18:54.5630124Z Jun 08 01:18:54 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553)
{noformat}
and another locked {{0x00000000e3a89c18}} and waits for {{0x00000000e3a8a1e8}}
{noformat}
2023-06-08T01:18:54.5738286Z Jun 08 01:18:54 ""ForkJoinPool-50-worker-25"" #620 daemon prio=5 os_prio=0 tid=0x00007f953874f000 nid=0x682e waiting for monitor entry [0x00007f95461d4000]
2023-06-08T01:18:54.5738959Z Jun 08 01:18:54    java.lang.Thread.State: BLOCKED (on object monitor)
2023-06-08T01:18:54.5739645Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:203)
2023-06-08T01:18:54.5740731Z Jun 08 01:18:54 	- waiting to lock <0x00000000e3a8a1e8> (a org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch)
2023-06-08T01:18:54.5741591Z Jun 08 01:18:54 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.leader.LeaderLatch.close(LeaderLatch.java:190)
2023-06-08T01:18:54.5742609Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionDriver.close(ZooKeeperLeaderElectionDriver.java:135)
2023-06-08T01:18:54.5743491Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.close(DefaultLeaderElectionService.java:217)
2023-06-08T01:18:54.5744427Z Jun 08 01:18:54 	- locked <0x00000000e3a89c18> (a java.lang.Object)
2023-06-08T01:18:54.5745200Z Jun 08 01:18:54 	at org.apache.flink.runtime.leaderelection.ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement(ZooKeeperLeaderElectionTest.java:346)
2023-06-08T01:18:54.5746206Z Jun 08 01:18:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
2023-06-08T01:18:54.5746829Z Jun 08 01:18:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
2023-06-08T01:18:54.5747552Z Jun 08 01:18:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
2023-06-08T01:18:54.5748207Z Jun 08 01:18:54 	at java.lang.reflect.Method.invoke(Method.java:498)
...
{noformat}",,mapohl,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31773,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 12:00:48 UTC 2023,,,,,,,,,,"0|z1ihe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jun/23 13:44;mapohl;This can, indeed, happen in the new implementation (even with the {{MultipleComponentLeaderElectionDriver}} implementation which is not used in the test run right now) because we call close on the driver within the lock. The old {{DefaultLeaderElectionService}} implementation didn't do that (see [DefaultLeaderElectionService:113|https://github.com/apache/flink/blob/release-1.17/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L113] in {{release-1.17}}).

The {{DefaultMultipleComponentLeaderElectionService}} implementation does close the driver in a lock, though. But it doesn't rely on the lock when processing the event (e.g. in [DefaultMultipleComponentLeaderElectionService:152|https://github.com/apache/flink/blob/e3cd3b311c1c8a6a0e0cdc849d7c951ef8beea5c/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultMultipleComponentLeaderElectionService.java#L152]). I considered this a bug in the implementation {{MultipleComponent*}} implementation initially: The event handling processing is done in a single thread to avoid locking. But the close method can be still called from another thread. ;;;","13/Jun/23 09:41;mapohl;Ok, I did go through the code comparing the {{DefaultLeaderElectionService}} with the {{DefaultMultipleComponentLeaderElectionService}} in that regards: The actual deadlock in the test happens between the ZK LeaderLatch lock in the curator's event thread and the test code's thread calling the {{DefaultLeaderElectionService#close()}} which is guarded by the {{DefaultLeaderElectionService}}'s lock. With FLINK-31733, we split up the driver lifecycle and the contender lifecycle and moved the driver's closing into the lock's monitor region (because we wanted the driver to be shutdown along the {{leaderOperationExecutor}} which processes the leader events to avoid {{RejectedExecutionException}}s from happening). This caused the concurrent nested lock scenario for the legacy driver to happen (as we see it being documented in the Jira issue).

Now the question is whether or why it didn't appear in the production code where we use the FLINK-24038 classes, i.e. {{MultipleComponentLeaderElectionDriverAdapter}} and {{DefaultMultipleComponentLeaderElectionService}}. The latter one does also close its driver implementation within the locks (analogously to how it's done in the current {{DefaultLeaderElectionService}} implementation). The difference is that the {{DefaultMultipleComponentLeaderElectionService}} owns its own lock. This prevents both threads from trying to own the same two locks.

Therefore, my conclusion is that it's a test code issue right now. But we have to address if we want to continue with FLINK-26522.;;;","15/Jun/23 12:00;mapohl;master: fdfff096a3a513979fe7676ab2e3ab1100468494;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The CI fails due to HiveITCase,FLINK-32294,13539354,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yuxia,fanrui,fanrui,09/Jun/23 02:28,09/Jun/23 03:14,13/Jul/23 08:29,09/Jun/23 02:52,1.18.0,,,,,,1.18.0,,,,,,,,Connectors / Hive,,,,0,pull-request-available,,,"2 ITCases fail:
 * HiveITCase.testHiveDialect
 * HiveITCase.testReadWriteHive

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&view=logs&j=ef799394-2d67-5ff4-b2e5-410b80c9c0af&t=9e5768bc-daae-5f5f-1861-e58617922c7a&l=14346]

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49766&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&s=ae4f8708-9994-57d3-c2d7-b892156e7812&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=14652]

 

 ",,fanrui,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32291,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 03:04:51 UTC 2023,,,,,,,,,,"0|z1ifk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 02:31;fanrui;Hi [~yuxia] , I'm working on FLINK-30585[1], it's related to flame graph, and shouldn't affect the HiveITCase. However, these 2 ITCases fail twice.
 * HiveITCase.testHiveDialect
 * HiveITCase.testReadWriteHive

Would you mind help take a look in your free time? thanks~

  [1] https://github.com/apache/flink/pull/22552;;;","09/Jun/23 02:50;luoyuxia;[~fanrui] Thanks for reporting. ;;;","09/Jun/23 02:52;luoyuxia;master: eacd5c1d90c98349a35c25dc420eb59eec7bf698

[~fanrui] Should be fixed. You can try to rebase master again. Sorry for that.;;;","09/Jun/23 03:04;fanrui;Thanks a lot for your quick feedback, let me try it now.:);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TableUtils.getRowTypeInfo fails to get type information of Tuple,FLINK-32292,13539349,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,09/Jun/23 01:28,09/Jun/23 02:17,13/Jul/23 08:29,09/Jun/23 02:17,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,,,lindong,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 09 02:17:29 UTC 2023,,,,,,,,,,"0|z1ifj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jun/23 02:17;lindong;Merged to apache/flink-ml master branch 0b6b7f70e45bebfa5f66e9405f152031607bc45a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The metadata column type is incorrect in Kafka table connector example,FLINK-32289,13539226,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiqian_yu,leonard,leonard,08/Jun/23 09:10,13/Jun/23 14:11,13/Jul/23 08:29,13/Jun/23 14:11,1.15.4,1.16.2,1.17.1,,,,1.18.0,kafka-4.0.0,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"The example[1] defined ts column with TIMESTAMP type

 
{code:java}
  `ts` TIMESTAMP(3) METADATA FROM 'timestamp'
{code}
the correct column type should be TIMESTAMP_LTZ type.

 
{code:java}
 `ts` TIMESTAMP_LTZ(3) METADATA FROM 'timestamp'  {code}
 

[1] https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/connectors/table/kafka/#how-to-create-a-kafka-table",,leonard,xiqian_yu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 14:10:43 UTC 2023,,,,,,,,,,"0|z1iers:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Jun/23 09:46;xiqian_yu;I'm glad to take this ticket.;;;","13/Jun/23 14:10;leonard;Fixed in：

flink(master)： 5326cb4528a02ecbe8f68ff1ece31e9305050162
flink-connector-kafka(main): aee4e82397af082e0463ffc156ae215132aa2b57

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManager may not allocate enough taskmangers if maxSlotNum is configured,FLINK-32254,13538769,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,05/Jun/23 13:04,07/Jun/23 09:01,13/Jul/23 08:29,07/Jun/23 09:01,1.16.0,,,,,,1.16.3,1.17.2,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"We ran a job with {{slotmanager.number-of-slots.max = 10}}, {{taskmanager.numberOfTaskSlots = 10}} and {{taskmanager.memory.process.size: 24000m}}. The resources of the cluster are sufficient, but no TaskManager can be allocated. It seems that there is a problem with the calculation logic of {{SlotManagerConfiguration#getMaxTotalMem}}. Due to the rounding down of division, the calculated {{MemorySize}} is too small.",,huwh,Thesharing,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 07 09:01:14 UTC 2023,,,,,,,,,,"0|z1iby8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 05:38;huwh;Thanks for reporting this bug. This is definitely a bug. But in this case, 24000/10*10 is also 24000. This does not lose any precision.  Please correct me if I am wrong.;;;","06/Jun/23 05:52;Weijie Guo;[~huwh] In fact, the {{defaultWorkerResourceSpec.getTotalMemSize()}} is used to calculate {{maxTotalMem}}, which is not exactly equal to {{taskmanager.memory.process.size}}.;;;","06/Jun/23 07:30;huwh;Thanks for your clarification. ;;;","07/Jun/23 09:01;Weijie Guo;master(1.18) via a008f25b87130a4320f0c8a46f3f9cd1ad09f503.
release-1.17 via ff52b475d14abbc8b7e6dd1fc19500a18491e78d.
release-1.16 via b5bffa89f4a85ddc561d18452c58dad763bb4035.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert the Field name of BUFFER_TIMEOUT to improve compatibility,FLINK-32250,13538729,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,05/Jun/23 05:07,06/Jun/23 02:24,13/Jul/23 08:29,06/Jun/23 02:24,1.18.0,,,,,,1.18.0,,,,,,,,API / DataStream,,,,0,pull-request-available,,,"FLINK-32023 changed the `ExecutionOptions.BUFFER_TIMEOUT` to `ExecutionOptions.BUFFER_TIMEOUT_INTERVAL`, the filed name should be reverted.

Because the `ExecutionOptions` is a public evolving API, some flink users are using the `ExecutionOptions.BUFFER_TIMEOUT` in their code. If we update it, the code cannot upgrade to 1.18 directly.

 

BTW, the option name is changed from `execution.buffer-timeout` to `execution.buffer-timeout.interval`. However, we marked the `execution.buffer-timeout` as `DeprecatedKeys`. So 1.18 is compatible with the old option name.

 ",,fanrui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32023,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 02:24:05 UTC 2023,,,,,,,,,,"0|z1ibpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jun/23 02:24;fanrui;<master:1.18> 7403e2efea273c307e52475d56c10910ca940430;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A Java string should be used instead of a Calcite NlsString to construct the column comment of CatalogTable,FLINK-32249,13538706,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,04/Jun/23 15:38,06/Jun/23 00:53,13/Jul/23 08:29,06/Jun/23 00:53,1.17.1,,,,,,1.17.2,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"when Flink interacts with CatalogTable, it directly passes the Calcite's NlsString comment as a string to the comment attribute of the schema and column. In theory, a Java string should be passed here, otherwise the CatalogTable implementers may encounter special character encoding issues, e.g., an issue in apache paimon: [https://github.com/apache/incubator-paimon/issues/1262]

also tested in sql-client:

{code}

Flink SQL> CREATE TABLE s1 (
>     order_id    STRING comment '测试中文',
>     price       DECIMAL(32,2) comment _utf8'测试_utf8中文',
>     currency    STRING,
>     order_time  TIMESTAMP(3)
> ) comment '测试中文table comment' WITH ('connector'='dategen');
[INFO] Execute statement succeed.

Flink SQL> show tables;
+------------+
| table name |
+------------+
|         s1 |
+------------+
1 row in set

Flink SQL> desc s1;
+------------+----------------+------+-----+--------+-----------+-------------------------+
|       name |           type | null | key | extras | watermark |                 comment |
+------------+----------------+------+-----+--------+-----------+-------------------------+
|   order_id |         STRING | TRUE |     |        |           | u&'\6d4b\8bd5\4e2d\6587 |
|      price | DECIMAL(32, 2) | TRUE |     |        |           |     _UTF8'测试_utf8中文 |
|   currency |         STRING | TRUE |     |        |           |                         |
| order_time |   TIMESTAMP(3) | TRUE |     |        |           |                         |
+------------+----------------+------+-----+--------+-----------+-------------------------+
4 rows in set

Flink SQL> show create table s1;
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                   result |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| CREATE TABLE `default_catalog`.`default_database`.`s1` (
  `order_id` VARCHAR(2147483647),
  `price` DECIMAL(32, 2),
  `currency` VARCHAR(2147483647),
  `order_time` TIMESTAMP(3)
) COMMENT '测试中文table comment'
WITH (
  'connector' = 'dategen'
)
 |
+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set

{code}",,aitozi,hackergin,jark,libenchao,lincoln.86xy,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/23 13:21;lincoln.86xy;1.17-problematic-to-string.png;https://issues.apache.org/jira/secure/attachment/13058785/1.17-problematic-to-string.png","05/Jun/23 13:22;lincoln.86xy;1.18-proper-to-string.png;https://issues.apache.org/jira/secure/attachment/13058786/1.18-proper-to-string.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 00:52:55 UTC 2023,,,,,,,,,,"0|z1ibk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 05:10;aitozi;It does not reproduce in my local sql client


{code:java}
Flink SQL> CREATE TABLE s1 (
>      order_id    STRING comment '测试中文',
>      price       DECIMAL(32,2) comment _utf8'测试_utf8中文',
>      currency    STRING,
>      order_time  TIMESTAMP(3)
>  ) comment '测试中文table comment' WITH ('connector'='dategen');
[INFO] Execute statement succeed.

Flink SQL> 
> show create table s1;
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
|                                                                                                                                                                                                                                                                                              result |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| CREATE TABLE `default_catalog`.`default_database`.`s1` (
  `order_id` VARCHAR(2147483647) COMMENT '测试中文',
  `price` DECIMAL(32, 2) COMMENT '测试_utf8中文',
  `currency` VARCHAR(2147483647),
  `order_time` TIMESTAMP(3)
) COMMENT '测试中文table comment'
WITH (
  'connector' = 'dategen'
)
 |
+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set

Flink SQL> desc s1;
+------------+----------------+------+-----+--------+-----------+---------------+
|       name |           type | null | key | extras | watermark |       comment |
+------------+----------------+------+-----+--------+-----------+---------------+
|   order_id |         STRING | TRUE |     |        |           |      测试中文 |
|      price | DECIMAL(32, 2) | TRUE |     |        |           | 测试_utf8中文 |
|   currency |         STRING | TRUE |     |        |           |               |
| order_time |   TIMESTAMP(3) | TRUE |     |        |           |               |
+------------+----------------+------+-----+--------+-----------+---------------+
4 rows in set

Flink SQL> 

{code}


;;;","05/Jun/23 11:39;libenchao;[~aitozi] This should only affect 1.7.x, you can try with 1.7.1;;;","05/Jun/23 12:28;lincoln.86xy;[~aitozi] it can be reproduced in 1.17.0 and 1.17.1, but works in current master branch, so the fix version might be 1.17.2;;;","05/Jun/23 13:29;lincoln.86xy;The `col.getComment().get().toString()` causes this problem:  
!1.17-problematic-to-string.png! 

while in master branch, it was fixed by removing the related code, and the `MergeTableLikeUtil#appendDerivedColumns` works fine: 
 !1.18-proper-to-string.png! 

we can have a quick fix on 1.17 that just change the problematic toString logic which will minimize the change.;;;","06/Jun/23 00:52;lincoln.86xy;fixed in 1.17: 7c2631b8ba8c935be03c91fd44b7aa42937a9698;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NonDeterministicTests #testTemporalFunctionsInBatchMode failure masked due to incorrect test initialization,FLINK-32245,13538496,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,02/Jun/23 02:13,02/Jun/23 13:05,13/Jul/23 08:29,02/Jun/23 13:05,1.17.1,1.18.0,,,,,1.18.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"The test case NonDeterministicTests #testTemporalFunctionsInBatchMode has been consistently failing due to incorrect test initialization.

 

However, this failure has been masked because the test class name ends with ""Tests"", causing the CI to skip the test case, which has been further validated by searching through the historical logs of the CI.

This issue needs to be addressed, and the test case should be executed to ensure proper testing. ",,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 02 13:05:42 UTC 2023,,,,,,,,,,"0|z1ia9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 13:05;lincoln.86xy;fixed in master: bfe49b2973d4ffc8f7404a376cab1e419b53406a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-master-benchmarks-regression-check always fails since 2023.05.30,FLINK-32244,13538495,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,wanglijie,wanglijie,02/Jun/23 02:09,07/Jun/23 08:16,13/Jul/23 08:29,05/Jun/23 13:05,1.18.0,,,,,,,,,,,,,,Benchmarks,,,,0,pull-request-available,,,"Since 2023.05.30, the flink-master-benchmarks-regression-check always fail:

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1631|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1631/]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1632|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1631/]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1633] ",,martijnvisser,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 13:05:00 UTC 2023,,,,,,,,,,"0|z1ia9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Jun/23 02:42;wanglijie;{code:java}
[2023-06-01T11:38:20.703Z] Traceback (most recent call last):
[2023-06-01T11:38:20.703Z]   File ""./regression_report_v2.py"", line 110, in <module>
[2023-06-01T11:38:20.703Z]     checkBenchmark(args, exe, benchmark)
[2023-06-01T11:38:20.703Z]   File ""./regression_report_v2.py"", line 84, in checkBenchmark
[2023-06-01T11:38:20.703Z]     args.minInstabilityMultiplier, direction)
[2023-06-01T11:38:20.703Z]   File ""./regression_report_v2.py"", line 54, in detectRegression
[2023-06-01T11:38:20.703Z]     sustainable_x = [min(scores[i - 3: i]) for i in range(3, baselineSize)]
[2023-06-01T11:38:20.703Z] ValueError: min() arg is an empty sequence
{code};;;","02/Jun/23 05:44;wanglijie;cc [~Yanfei Lei];;;","05/Jun/23 13:05;martijnvisser;Fixed in flink-benchmarks:master

7fae796e18f40e9fae5f53b16d763dfbdcba37e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestClusterClient leaks jobgraph file if submission fails,FLINK-32226,13538254,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,31/May/23 11:32,01/Jun/23 09:06,13/Jul/23 08:29,01/Jun/23 09:06,1.17.0,,,,,,1.17.2,1.18.0,,,,,,,Client / Job Submission,,,,0,pull-request-available,,,"{code:java}
        submissionFuture
                .thenCompose(ignored -> jobGraphFileFuture)
                .thenAccept(
                        jobGraphFile -> {
                            try {
                                Files.delete(jobGraphFile);
                            } catch (IOException e) {
                                LOG.warn(""Could not delete temporary file {}."", jobGraphFile, e);
                            }
                        });
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 09:06:21 UTC 2023,,,,,,,,,,"0|z1i8sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Jun/23 09:06;chesnay;master: d90a72da2fd601ca4e2a46700e91ec5b348de2ad
1.17: e5fb97e305d6f23bdd0fae6484cb2f6c5c2dcd1d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL client hangs when executing EXECUTE PLAN,FLINK-32219,13538176,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,xu_shuai_,xu_shuai_,31/May/23 03:01,13/Jun/23 07:26,13/Jul/23 08:29,08/Jun/23 02:12,1.17.1,,,,,,1.17.2,1.18.0,,,,,,,Table SQL / Client,,,,0,pull-request-available,,,"I compiled a plan for an INSERT statement and executed the plan, but the SQL client became unresponsive when executing the EXECUTE PLAN statement. I confirmed that the Flink job is running normally by checking the Flink dashboard. The only issue is that the SQL client becomes stuck and cannot accept new commands. I printed the stack trace of the SQL client process, and here is a part of it for reference.
{code:java}
""pool-2-thread-1"" #30 prio=5 os_prio=31 tid=0x00000001172e5000 nid=0x6d03 waiting on condition [0x0000000173e01000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x000000076e72af20> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.api.internal.InsertResultProvider.hasNext(InsertResultProvider.java:83)
	at org.apache.flink.table.api.internal.InsertResultProvider.access$200(InsertResultProvider.java:37)
	at org.apache.flink.table.api.internal.InsertResultProvider$Iterator.hasNext(InsertResultProvider.java:106)
	at java.util.Iterator.forEachRemaining(Iterator.java:115)
	at org.apache.flink.util.CollectionUtil.iteratorToList(CollectionUtil.java:115)
	at org.apache.flink.table.gateway.service.result.ResultFetcher.fromTableResult(ResultFetcher.java:163)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.callOperation(OperationExecutor.java:542)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:440)
	at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:195)
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
	at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl$$Lambda$389/1391083077.apply(Unknown Source)
	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
	at org.apache.flink.table.gateway.service.operation.OperationManager$$Lambda$390/208625838.call(Unknown Source)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)
	at org.apache.flink.table.gateway.service.operation.OperationManager$Operation$$Lambda$392/670621032.run(Unknown Source)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)
{code}",,fsk119,jark,qingyue,xu_shuai_,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31956,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 07 02:02:54 UTC 2023,,,,,,,,,,"0|z1i8b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 01:52;qingyue;Hi, [~xu_shuai_] Thanks for reporting this issue.

OperationExecutor failed to consider the types of CompileAndExecutePlanOperation and ExecutePlanOperation and instead used callOperation by default, resulting in no job ID being returned. I'd like to fix this issue. cc [~shengkai] ;;;","07/Jun/23 02:02;fsk119;Merged into master: 83ba6b5348cbffb26e8d1d5ce6e8d6bb1994e3bc
Merged into release-1.17: be53133546d054e282c0a24bfe722d0d276f9a8f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Retain metric store can cause NPE ,FLINK-32217,13538092,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,30/May/23 09:23,05/Jun/23 08:20,13/Jul/23 08:08,05/Jun/23 08:20,1.17.0,,,,,,1.16.3,1.17.2,1.18.0,,,,,,Runtime / Metrics,Runtime / REST,,,0,pull-request-available,,,"When metricsFetcher fetches metrics, it will update the metricsStore ([here|https://github.com/apache/flink/blob/d6c3d332340922c24d1af9dd8835d0bf790184b5/flink-runtime/src/main/java/org/apache/flink/runtime/rest/handler/legacy/metrics/MetricStore.java#LL91C44-L91C44]). But in this method, it can get null metricStore and cause NPE, which will lead to incorrect results of metrics retain, and we should also fix it from the perspective of stability.",,JunRuiLi,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 08:20:30 UTC 2023,,,,,,,,,,"0|z1i7sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 08:20;wanglijie;Fix via:
master(1.18): 3245e0443b2a4663552a5b707c5c8c46876c1f6d
release-1.17: dfe2874b8757f942d0143e6e5ea6ed3cc13a21d6
release-1.16: bb1ae0559bd7ae24bb3da553a681d07938673c98
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZooKeeperLeaderElectionTest.testZooKeeperReelectionWithReplacement fails with The ExecutorService is shut down already. No Callables can be executed on AZP,FLINK-32204,13537766,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,Sergey Nuyanzin,Sergey Nuyanzin,26/May/23 08:03,13/Jun/23 07:16,13/Jul/23 08:29,13/Jun/23 07:16,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"[This build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49386&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=7095] fails as
{noformat}

May 25 18:45:50 Caused by: java.util.concurrent.RejectedExecutionException: The ExecutorService is shut down already. No Callables can be executed.
May 25 18:45:50 	at org.apache.flink.util.concurrent.DirectExecutorService.throwRejectedExecutionExceptionIfShutdown(DirectExecutorService.java:237)
May 25 18:45:50 	at org.apache.flink.util.concurrent.DirectExecutorService.submit(DirectExecutorService.java:100)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:902)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:894)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.access$1200(TreeCache.java:79)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache$TreeNode.processResult(TreeCache.java:489)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.sendToBackgroundCallback(CuratorFrameworkImpl.java:926)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.CuratorFrameworkImpl.processBackgroundOperation(CuratorFrameworkImpl.java:683)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.WatcherRemovalFacade.processBackgroundOperation(WatcherRemovalFacade.java:152)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.imps.GetDataBuilderImpl$3.processResult(GetDataBuilderImpl.java:272)
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:634)
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553)
May 25 18:45:50
{noformat}",,jark,mapohl,Sergey Nuyanzin,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31995,,,,,,FLINK-29813,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 07:16:09 UTC 2023,,,,,,,,,,"0|z1i5sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 09:10;mapohl;Thanks for reporting the issue. I'm going to have a look. -But I suspect it not being related to the FLINK-26522 changes because the test still relies on the legacy {{LeaderElectionDriver}} implementation of ZooKeeper.- This specific test utilizes the {{DefaultLeaderElectionService}} which was touched in FLINK-31838 and FLINK-31773;;;","26/May/23 09:15;mapohl;FYI: CI failed on [a4de8945|https://github.com/apache/flink/commit/a4de8945]. That version didn't include FLINK-31776, yet.;;;","26/May/23 10:49;mapohl;It's most likely being caused by FLINK-31995. The {{ZooKeeperLeaderElectionDriver}} uses a {{DirectExecutorService}} in for the {{TreeCache}}. The {{RejectedExecutionException}} handling was added in FLINK-31995. So, it could be that it reveals a bug in some other code which was just not visible before.;;;","26/May/23 12:38;mapohl;The curator's {{TreeCache}} isn't thread-safe: The event processing within the cache happens is triggered from within the client's EventThread and calls {{TreeCache#publishEvent}}:
{code}
[...]
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:902)
May 25 18:45:50 	at org.apache.flink.shaded.curator5.org.apache.curator.framework.recipes.cache.TreeCache.publishEvent(TreeCache.java:894)
[...]
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:634)
May 25 18:45:50 	at org.apache.flink.shaded.zookeeper3.org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:553)
[...]
{code}
{{TreeCache#publishEvent}} will submit a new task to the cache's {{executorService}} (see [TreeCache:901|https://github.com/apache/curator/blob/844c0ad36340b695b2784489c078cfd78522143c/curator-recipes/src/main/java/org/apache/curator/framework/recipes/cache/TreeCache.java#L901]) which is a {{directExecutorService}} in the case of Flink's ZooKeeper LeaderElectionDriver implementations (see [ZooKeeperUtils:764|https://github.com/apache/flink/blob/4576e4384ff36623712043564039f654c3b44a30/flink-runtime/src/main/java/org/apache/flink/runtime/util/ZooKeeperUtils.java#L764] for the legacy {{ZooKeeperLeaderElectionDriver}} and [ZooKeeperMultipleComponentLeaderElectionDriver:76|https://github.com/apache/flink/blob/8ddfd590ebba7fc727e79db41b82d3d40a02b56a/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/ZooKeeperMultipleComponentLeaderElectionDriver.java#L76]). The close call happens in the test's main thread.

The {{TreeCache#close}} call sets the cache's state to {{CLOSED}} in [TreeCache:628|https://github.com/apache/curator/blob/844c0ad36340b695b2784489c078cfd78522143c/curator-recipes/src/main/java/org/apache/curator/framework/recipes/cache/TreeCache.java#L628]. {{TreeCache#publishEvent}} checks this state in [TreeCache:898|https://github.com/apache/curator/blob/844c0ad36340b695b2784489c078cfd78522143c/curator-recipes/src/main/java/org/apache/curator/framework/recipes/cache/TreeCache.java#L898]. The latter one doesn't use a lock. This can cause a race condition where the {{publishEvent}} method is called and passes the if condition before the test's main thread can trigger the close method but after the task is actually submitted causing the {{RejectedExecutionException}} which we're observing right now.

[~dmvk] may you verify my finding? I would suggest adding a less restrictive version of the {{DirectExecutorService}} that we could use in the production code to avoid running into this bug. We could continue to use the more restrictive version (which was introduced in FLINK-31995) in the test code. WDYT David?;;;","26/May/23 12:47;mapohl;FYI: I couldn't reproduce the error locally with 10000 test executions.;;;","30/May/23 08:04;mapohl;Looking into the code once more: FLINK-31995 in general is problematic because we use {{DirectExecutorService.INSTANCE}} in several places as a singleton. We should through a {{RejectedExecutionException}} in these cases because that might influence other classes which use the very same executor.;;;","03/Jun/23 09:49;yunta;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49593&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","06/Jun/23 14:49;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49606&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=7108;;;","12/Jun/23 09:51;jark;ZooKeeperLeaderElectionTest.testZooKeeperReelection
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49893&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8;;;","13/Jun/23 07:16;mapohl;master: df2b9f8d4722c8fc9533656340b9162e0199fb08;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MetricStore does not remove metrics of nonexistent parallelism in TaskMetricStore when scale down job parallelism,FLINK-32199,13537747,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,26/May/23 05:03,05/Jun/23 07:54,13/Jul/23 08:29,05/Jun/23 07:54,1.17.0,,,,,,1.16.3,1.17.2,1.18.0,,,,,,Runtime / Metrics,Runtime / REST,,,0,pull-request-available,,,"After FLINK-29615, FLINK will update the subtask metrics store when scaling down parallelism. However, task metrics are added in the form of ""subtaskIndex + metric.name"" or ""subtaskIndex + operatorName + metric.name"". Users will be able to find many redundant metrics through JobVertexMetricsHandler, which will be very troublesome for users.",,JunRuiLi,tanyuxin,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 07:54:03 UTC 2023,,,,,,,,,,"0|z1i5o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 05:06;JunRuiLi;I've prepared a quick fix for it. Can you assign this ticket for me?[~wanglijie] :);;;","26/May/23 05:43;wanglijie;Thanks [~JunRuiLi] , assigned to you :D;;;","05/Jun/23 07:54;wanglijie;Fix via:
master(1.18): f3ab9626bf18cad993f7cecba23a7bce6e14407b
release-1.17: 07959a4141ba599194e1e0a8b6da163793d53d0d
release-1.16: e90b2e01fd42330e1115a96e2045d0b3189ba321;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad link in Flink page,FLINK-32190,13537651,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,JunRuiLi,claude,claude,25/May/23 11:50,01/Jun/23 06:09,13/Jul/23 08:29,01/Jun/23 06:09,,,,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,"on the page: [https://flink.apache.org/use-cases/]
in the section: What are typical data analytics applications?

The first link: [Quality monitoring of Telco networks|http://2016.flink-forward.org/kb_sessions/a-brief-history-of-time-with-apache-flink-real-time-monitoring-and-analysis-with-flink-kafka-hb/]

returns a 404 error: 
h1. This site can’t be reached

Check if there is a typo in 2016.flink-forward.org.
DNS_PROBE_FINISHED_NXDOMAIN",,claude,JunRuiLi,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 06:09:05 UTC 2023,,,,,,,,,,"0|z1i52w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/May/23 03:08;JunRuiLi;It seems that the 404 error was caused by an expired external link. May be we can replace other links or remove this use case. [~wanglijie] What do you think? I'd like to take this ticket.:);;;","26/May/23 03:10;wanglijie;Thanks [~JunRuiLi], assigned to you :);;;","31/May/23 03:27;JunRuiLi;The user cases page has some bad links, all because the linked site has expired. I plan to replace these bad links with the video page posted on youtube by the flink-forward official account.;;;","01/Jun/23 06:09;wanglijie;Fix via
asf-site(flink-web): f9e8540d629b220f0432033507b9b8ecfff5613f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration tests fail due to Process Exit Code: 239 and NoClassDefFound in logs ,FLINK-32189,13537639,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,25/May/23 10:55,13/Jun/23 21:36,13/Jul/23 08:29,13/Jun/23 21:32,,,,,,,,,,,,,,,,,,,0,test-stability,,,Since there are multiple similar cases with different classes mentioned in {{NoClassDefFound}} this is an umbrella for such cases,,renqs,Sergey Nuyanzin,Thesharing,,,,,,,,,,,,,,,,,,,FLINK-32314,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 21:36:29 UTC 2023,,,,,,,,,,"0|z1i508:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/May/23 08:09;renqs;[~chesnay] Could you take a look at this umbrella issue? Thanks;;;","13/Jun/23 21:36;chesnay;With FLINK-32314 we now ignore classloading errors in the rpc system after it has been shut down. I believe that the actor system termination future isn't telling us the whole story and threads can linger around still doing _something_. we don't have a handle on these threads so there's little we can do in terms of waiting for longer.
It is a bit of a band-aid, but I'm not sure where else to go; digging into Akka won't get us anywhere because this won't get fixed in the Apache licensed version.

Since this issue only occurs if the rpc system was closed, which only happens after our own business logic has already concluded (by virtue of all rpc endpoints being shut down) I don't see a risk in this approach.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaExample can not run with args,FLINK-32172,13537394,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,Weijie Guo,xulongfeng2018,xulongfeng2018,23/May/23 23:16,25/May/23 13:05,13/Jul/23 08:29,24/May/23 10:54,1.16.0,,,,,,1.16.3,1.17.2,1.18.0,,,,,,Connectors / Kafka,,,,0,,,,"i fork and clone flink-connector-kafka repo. after build and package, i run org/apache/flink/streaming/kafka/test/KafkaExample.java main() but failed,

comment say:
Example usage: --input-topic test-input --output-topic test-output --bootstrap.servers
* localhost:9092 --group.id myconsumer
 
but console print: Missing parameters!  from KafkaExampleUtil where need 5 paramters but we have 4
 
thank you for your attention to this matter","* win11
 * Git
 * Maven (we recommend version 3.8.6)
 * Java 11",martijnvisser,Weijie Guo,xulongfeng2018,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/23 23:16;xulongfeng2018;args.png;https://issues.apache.org/jira/secure/attachment/13058460/args.png","23/May/23 23:16;xulongfeng2018;kafkaexample.png;https://issues.apache.org/jira/secure/attachment/13058459/kafkaexample.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 08:38:37 UTC 2023,,,,,,,,,,"0|z1i3i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/May/23 03:31;Weijie Guo;Thanks for the report, this should be an accidental mistake(the minimum parameter count should be 4 instead of 5). It will be fixed asap.;;;","24/May/23 10:58;xulongfeng2018;thanks for your help;;;","25/May/23 08:38;Weijie Guo;master(1.18) via 366d01d08601d8cc42fd3ba74aa754a63503ed34.
release-1.17 via 83da3560584f8d95de0acbe2e922b7f327fec7ec.
release-1.16 via a5851984e9f282897c627a668d1b3d1acf48e50d.
flink-connector-kafka via 224468804f4cfb4a293102b8b596a299463dc077.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Misleading log message due to missing null check,FLINK-32162,13537321,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,chesnay,chesnay,chesnay,23/May/23 11:27,23/May/23 14:55,13/Jul/23 08:29,23/May/23 14:53,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Updating the job requirements always logs ""Failed to update requirements for job {}."" because we don't check whether the error is not null.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 14:53:58 UTC 2023,,,,,,,,,,"0|z1i31s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 14:53;chesnay;master: fadde2a378aac4293676944dd513291919a481e3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix incorrect docker image links of specific versions in flink-web site,FLINK-32145,13537064,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,22/May/23 07:53,22/May/23 09:19,13/Jul/23 08:29,22/May/23 09:19,,,,,,,,,,,,,,,Documentation,,,,0,pull-request-available,,,"Current links of docker images in many release announcements point to an incorrect place, we should fix them to the correct locations.",,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 09:19:32 UTC 2023,,,,,,,,,,"0|z1i1go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 09:19;yunta;resolved in apache/flink-web: 69c1dd54672dbbe4015c7bbcbb54971518d2e55b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SharedStateRegistry print too much info log,FLINK-32141,13536955,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,20/May/23 16:35,22/May/23 15:22,13/Jul/23 08:29,22/May/23 15:22,1.17.0,,,,,,1.17.2,1.18.0,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"FLINK-29095 added some log to SharedStateRegistry for trouble shooting. Among them, a info log be added when newHandle is equal to the registered one:

[https://github.com/apache/flink/blob/release-1.17.0/flink-runtime/src/main/java/org/apache/flink/runtime/state/SharedStateRegistryImpl.java#L117]

!image-2023-05-21-00-26-20-026.png|width=775,height=126!

But this case cannot be considered as a potential bug, because FsStateChangelogStorage will directly use the FileStateHandle of the previous checkpoint instead of PlaceholderStreamStateHandle.

In our tests, JobManager printed so much of this log that useful information was overwhelmed.

So I suggest change this log level to trace, WDYT [~Yanfei Lei], [~klion26] ?",,Feifan Wang,Weijie Guo,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/May/23 16:26;Feifan Wang;image-2023-05-21-00-26-20-026.png;https://issues.apache.org/jira/secure/attachment/13058385/image-2023-05-21-00-26-20-026.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 15:22:05 UTC 2023,,,,,,,,,,"0|z1i0sg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 12:35;Yanfei Lei;Thanks for reporting this.

+1 for ""FsStateChangelogStorage will directly use the FileStateHandle of the previous checkpoint instead of PlaceholderStreamStateHandle."";;;","22/May/23 15:22;Weijie Guo;master(1.18) via d94818e5f2301200f3f1bd6bd094a79f4545bd84.
release-1.17 via f0adce77f261627a651dd231189c9222e407c9de.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Data accidentally deleted and not deleted when upsert sink to hbase,FLINK-32139,13536951,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,LiuZeshan,LiuZeshan,LiuZeshan,20/May/23 14:49,30/Jun/23 12:41,13/Jul/23 08:29,30/Jun/23 12:32,,,,,,,1.18.0,,,,,,,,Connectors / HBase,,,,0,pull-request-available,,,"h4. *Problem background*

We meet data accidental deletion and non deletion issues when synchronizing MySQL cdc data to HBase using HBase connectors.
h3. Reproduction steps

1、The Flink job with 1 parallelism synchronize a MySQL table into HBase. SinkUpsertMaterializer is tunned off by setting {{{}table.exec.sink.upsert-materialize = 'NONE'{}}}。

MySQL table schema is as follows。
{code:java}
CREATE TABLE `source_sample_1001` (
`id` int(11) NOT NULL AUTO_INCREMENT,
`name` varchar(200) DEFAULT NULL,
`age` int(11) DEFAULT NULL,
`weight` float DEFAULT NULL,
PRIMARY KEY (`id`)
);{code}
The source table definition in Flink is as follows.
{code:java}
CREATE TABLE `source_sample_1001` (
    `id` bigint,
    `name` String,
    `age` bigint,
    `weight` float,
  PRIMARY KEY (`id`) NOT ENFORCED
) WITH (
'connector' = 'mysql-cdc' ,
'hostname' = '${ip}',
'port' = '3306',
'username' = '${user}',
'password' = '${password}',
'database-name' = 'testdb_0010',
'table-name' = 'source_sample_1001'
);{code}
HBase sink table are created in {{testdb_0011}} namespace.
{code:java}
CREATE 'testdb_0011:source_sample_1001', 'data'
​
describe 'testdb_0011:source_sample_1001'
 
# describe output
Table testdb_0011:source_sample_1001 is ENABLED                                                                                                                                                         
testdb_0011:source_sample_1001                                                                                                                                                                          
COLUMN FAMILIES DESCRIPTION                                                                                                                                                                             {NAME => 'data', BLOOMFILTER => 'ROW', IN_MEMORY => 'false', VERSIONS => '1', KEEP_DELETED_CELLS => 'FALSE', DATA_BLOCK_ENCODING => 'NONE', COMPRESSION => 'NONE', TTL => 'FOREVER', MIN_VERSIONS => '0' , BLOCKCACHE => 'true', BLOCKSIZE => '65536', REPLICATION_SCOPE => '0'}
{code}
 

                                                                                                                                
The sink table definition in Flink.
{code:java}
CREATE TABLE `hbase_sink1` (
    `id` STRING COMMENT 'unique id',
    `data` ROW<
        `name` string,
        `age` string,
        `weight` string
    >,
    primary key(`id`) not enforced
) WITH (
  'connector' = 'hbase-2.2',
  'table-name' = 'testdb_0011:source_sample_1001',
  'zookeeper.quorum' = '${hbase.zookeeper.quorum}'
);{code}
DML in flink to synchronize data.
{code:java}
INSERT INTO `hbase_sink1` SELECT
    `id`, row(`name`, `age`, `weight`)
FROM (
    SELECT
        REVERSE(CONCAT_WS('', CAST(id AS VARCHAR ))) as id,
        `name`, cast(`age` as varchar) as `age`, cast(`weight` as varchar) as `weight`
    FROM `source_sample_1001`
) t;{code}
2、Another flink job sinks datagen data to the MySQL table {{source_sample_1001}} 。id range from 1 to 10_000， that means source_sample_1001 will have at most 10_000 records。
{code:java}
CREATE TABLE datagen_source (
    `id` int,
    `name` String,
    `age` int,
    `weight` int
) WITH (
   'connector' = 'datagen',
  'fields.id.kind' = 'random',
  'fields.id.min' = '1',
  'fields.id.max' = '10000',
  'fields.name.length' = '20',
  'fields.age.min' = '1',
  'fields.age.max' = '150',
  'fields.weight.min' = '5',
  'fields.weight.max' = '300',
  'rows-per-second' = '5000'
);
​
CREATE TABLE `source_sample_1001` (
    `id` bigint,
    `name` String,
    `age` bigint,
    `weight` float,
  PRIMARY KEY (`id`) NOT ENFORCED
) WITH (
  'connector' = 'jdbc',
  'url' = 'jdbc:mysql://${ip}:3306/testdb_0010?rewriteBatchedStatements=true&serverTimezone=Asia/Shanghai',
  'table-name' = 'source_sample_1001',
  'username' = '${user}',
  'password' = '${password}',
  'sink.buffer-flush.max-rows' = '500',
  'sink.buffer-flush.interval' = '1s'
);
​
-- dml
INSERT INTO `source_sample_1001` SELECT `id`, `name`, `age`, cast(`weight` as float) FROM `datagen_source`;{code}
3、A bash script deletes the MySQL table {{source_sample_1001}} with batch 10.
{code:java}
#!/bin/bash
​
mysql1=""mysql -h${ip} -u${user} -p${password}""
batch=10
​
for ((i=1; ;i++)); do
echo ""iteration $i start""
for ((j=1; j<=10000; j+=10)); do
  $mysql1 -e ""delete from testdb_0010.source_sample_1001 where id >= $j and id < $((j+10))""
done
echo ""iteration $i end""
sleep 10
done{code}
4、Start the above two flink jobs and the bash script. Wait for several minutes, usually 5 minutes is enough. Please note that deleting data bash script is necessary for reproduce the problem.

5、Stop the bash script, and waiting for MySQL table to fill up with 10_000 data by the datagen flink job。And then stop datagen flink job. Waiting for the sink hbase job to read all the binlog of MySQL table {{{}source_sample_1001{}}}.

6、Check the hbase table and reproduce the issue of data loss. As shown below, 67 records were lost in a test.
{code:java}
hbase(main):006:0> count 'testdb_0011:source_sample_1001'                                                   
9933 row(s)
Took 0.8724 seconds                                                                                                                                                                                     
=> 9933{code}
Find out a missing record and check the raw data in HBase.
{code:java}
hbase(main):008:0> get 'testdb_0011:source_sample_1001', '24'
COLUMN                                             CELL                                                                                                                                                
0 row(s)
Took 0.0029 seconds                                                                                                                                                                                     
hbase(main):009:0> scan 'testdb_0011:source_sample_1001', {RAW => true, VERSIONS => 1000, STARTROW => '24', STOPROW => '24'}
ROW                                                 COLUMN+CELL                                                                                                                                         
24                                                 column=data:name, timestamp=2023-05-20T21:17:44.884, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:44.884, value=3a8f571c25a9d9040ef3                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:43.769, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:43.769, value=5aada98281ee0a961841                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:42.902, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:42.902, value=599790a9a641e6121ab3                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:41.614, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:41.614, value=4ece6410d32959457f80                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.885, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.885, value=9edcfcf1c958a7e4ae2a                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.841, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:40.841, value=3d82dcf982d5bcd5b6b7                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:39.788, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:39.788, value=2888a338b65caaf15b30                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.799, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.799, value=a8d7549e18ef0c0e8674                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.688, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.688, value=ada7237e52d030dcef7a                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.650, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:35.650, value=482feed26918dcdc911e                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:34.885, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:34.885, value=36d6bdd585dbb65dedb7                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.905, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.905, value=6e15c4462f8435040700                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.803, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.803, value=d122df5afd4eac32da72                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.693, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:33.693, value=ed603d47fedb3852b520                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:31.784, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:31.784, value=1ebdd5fe6310850b8098                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:30.684, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:30.684, value=cc628ba45d1ad07fce2f                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.812, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.812, value=c1d4df6e987bdb3cd0a3                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.590, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:29.590, value=535557700ca01c6b6b1e                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.876, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.876, value=a63c2ebfefc82eab4bcf                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.565, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:28.565, value=dd2b24ff0dfa672c49ba                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.879, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.879, value=69dbe1287c2bc54781ab                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.699, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:27.699, value=775d06dcbf1148e665ee                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:24.209, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:24.209, value=e23c010ab06125c88870                                                                     
24                                                 column=data:name, timestamp=2023-05-20T21:17:22.480, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:20.716, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:18.678, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:17.720, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:16.858, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:16.682, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:15.753, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:14.571, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:11.572, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:09.681, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:08.792, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:05.888, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:05.754, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:03.626, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:02.652, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:01.790, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:17:00.986, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:59.797, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.982, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.781, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.626, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:58.149, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:56.610, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:51.655, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:51.458, type=Delete                                                                                    
24                                                 column=data:name, timestamp=2023-05-20T21:16:44.860, type=Delete                                                                                    
1 row(s)
Took 0.1466 seconds                                                                                  {code}
7、Start the bash script to delete all data of the MySQL table. Waiting for the sink hbase job to read all the binlog of MySQL table {{{}source_sample_1001{}}}.

6、Check the hbase table and reproduce the issue of data no deletion. As shown below, 6 records were not deleted in the test.
{code:java}
hbase(main):012:0> count 'testdb_0011:source_sample_1001'
6 row(s)
Took 0.5121 seconds                                                                                                                                                                                     
=> 6{code}
Check the raw data of a record in HBase.
{code:java}
hbase(main):013:0> get 'testdb_0011:source_sample_1001', '3668'
COLUMN                                             CELL                                                                                                                                                
data:name                                         timestamp=2023-05-20T21:17:26.714, value=ebb15f905622340d0351                                                                                       
1 row(s)
Took 0.0037 seconds                                                                                                                                                                                     
hbase(main):014:0> scan 'testdb_0011:source_sample_1001', {RAW => true, VERSIONS => 1000, STARTROW => '3668', STOPROW => '3668'}
ROW                                                 COLUMN+CELL                                                                                                                                         
3668                                               column=data:name, timestamp=2023-05-20T21:17:45.728, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:45.728, value=c675a12c7cbed27599c3                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:44.693, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:44.693, value=413921aa1ac44f545954                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:43.854, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:43.854, value=7d44b0efc0923e4035b7                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:41.721, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:41.721, value=60bfaef81bf8efdf781a                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:40.763, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:40.763, value=2c371f9cd3909dd3b3f8                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:37.872, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:37.872, value=9e32087cb39065976e50                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:32.573, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:32.573, value=708364bf84dad4a04170                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:26.811, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:26.811, value=c0e8e11eed3f8410dea9                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:26.714, value=ebb15f905622340d0351                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:24.310, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:24.310, value=21681a161ed2ccbe884e                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:23.508, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:23.508, value=a1ef547a9efd57a7a0e2                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:22.788, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:22.788, value=34e688060e6c40f4f83b                                                                     
3668                                               column=data:name, timestamp=2023-05-20T21:17:21.746, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:17.761, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:12.610, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:11.909, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:07.846, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:06.901, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:06.758, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:06.569, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:02.689, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:17:00.344, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:59.961, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:59.415, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.916, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.781, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.718, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:58.339, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:56.340, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:55.883, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:55.683, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:55.056, type=Delete                                                                                    
3668                                               column=data:name, timestamp=2023-05-20T21:16:46.845, type=Delete                                                                                    
1 row(s)
Took 0.0457 seconds                                                                                          {code}
h4. *Reason for the problem*

The [HBase connector|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L177] use the [Delete key type|https://github.com/apache/hbase/blob/c05ee564d3026688bcfdc456071059c7c8409694/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java#L380] [without timestamp|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L168] to {{{}delete the latest version of the specified column. This is an expensive call in that on the server-side, it first does a get to find the latest versions timestamp. Then it adds a delete using the fetched cells timestamp{}}}. Causing the following issues:

Problem 1: When writing update data, the timestamp of -U and +U added by the hbase server to the update message may be the same, and -U deleted the latest version of +U data, resulting in accidental deletion of the data. The problem was also reported by https://issues.apache.org/jira/browse/FLINK-28910

Problem 2: When there are multiple versions of HBase data, deleting the data will exposes earlier versions of the data, and resulting in the issue of data no deletion.
h4. *Solution proposal* 

Use the [DeleteColumn key type|https://github.com/apache/hbase/blob/c05ee564d3026688bcfdc456071059c7c8409694/hbase-client/src/main/java/org/apache/hadoop/hbase/client/Delete.java#L322] and set strongly increasing timestamp for [put|https://github.com/lzshlzsh/flink/blob/a2341810a244b97a3af32951e17efbc49f570cdd/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L138] and [delete|https://github.com/lzshlzsh/flink/blob/a2341810a244b97a3af32951e17efbc49f570cdd/flink-connectors/flink-connector-hbase-base/src/main/java/org/apache/flink/connector/hbase/util/HBaseSerde.java#L170] mutation. The delete mutation will delete all versions of the specified column with a timestamp less than or equal to the specified.

I have test the proposed solution for several days, and neither the data accidental deletion nor no deletion issues happen.",,kyledong,LiuZeshan,martijnvisser,yesorno,zlzhang0122,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,FLINK-28910,,,"24/May/23 15:12;LiuZeshan;aa.log;https://issues.apache.org/jira/secure/attachment/13058481/aa.log","24/May/23 15:07;LiuZeshan;image-2023-05-24-23-07-23-978.png;https://issues.apache.org/jira/secure/attachment/13058480/image-2023-05-24-23-07-23-978.png","24/May/23 15:17;LiuZeshan;image-2023-05-24-23-16-59-508.png;https://issues.apache.org/jira/secure/attachment/13058482/image-2023-05-24-23-16-59-508.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jun 30 12:32:02 UTC 2023,,,,,,,,,,"0|z1i0rk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/May/23 15:22;LiuZeshan;[~ferenc-csaky] [~martijnvisser] Could you help to take a look at this issue.

cc [~mgergely] [~Leonard] [~jark] ;;;","22/May/23 02:42;LiuZeshan;[~liyu] Would you also help to take a look at this issue?;;;","24/May/23 15:21;LiuZeshan;By adding logs to HBase, the root cause of unexpected data deletion was found. HBase version 2.4.5.

Firstly, writing data to HBase using put, delete, and put with the same timestamp results in the data being deleted.

!image-2023-05-24-23-07-23-978.png|width=309,height=100!

Verify as follows：
{code:java}
hbase:012:0> put 'source_sample_1001', 1, 'data:name', 'MyName', 1684937724000
Took 0.0294 seconds
hbase:013:0> scan 'source_sample_1001'
ROW                                         COLUMN+CELL
 1                                          column=data:name, timestamp=2023-05-24T22:15:24, value=MyName
1 row(s)
Took 0.0610 seconds
hbase:014:0> delete 'source_sample_1001', 1, 'data:name', 1684937724000
Took 0.0248 seconds
hbase:015:0> scan 'source_sample_1001'
ROW                                         COLUMN+CELL
0 row(s)
Took 0.0101 seconds
hbase:016:0> put 'source_sample_1001', 1, 'data:name', 'MyName', 1684937724000
Took 0.0139 seconds
hbase:017:0> scan 'source_sample_1001'
ROW                                         COLUMN+CELL
0 row(s)
Took 0.0164 seconds {code}
We add debug code in HBase as follows:
{code:java}
diff --git a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
index d304fa3c8f..6dd4ef0706 100644
--- a/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
+++ b/hbase-server/src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
@@ -3254,12 +3254,15 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
             if (!coprocessorHost.prePrepareTimeStampForDeleteVersion(mutation, cell,
                 byteNow, get)) {
               updateDeleteLatestVersionTimestamp(cell, get, count, byteNow);
+              LOG.info(""-D[{}][{}]"", mutation, cell.getTimestamp());
             }
           } else {
             updateDeleteLatestVersionTimestamp(cell, get, count, byteNow);
+            LOG.info(""-D[{}][{}]"", mutation, cell.getTimestamp());
           }
         } else {
           PrivateCellUtil.updateLatestStamp(cell, byteNow);
+          LOG.info(""-D[{}][{}]"", mutation, cell.getTimestamp());
         }
       }
     }
@@ -3874,6 +3877,7 @@ public class HRegion implements HeapSize, PropagatingConfigurationObserver, Regi
       visitBatchOperations(true, miniBatchOp.getLastIndexExclusive(), (int index) -> {
         Mutation mutation = getMutation(index);
         if (mutation instanceof Put) {
+          LOG.info(""+I[{}][{}]"", mutation, timestamp);
           HRegion.updateCellTimestamps(familyCellMaps[index].values(), Bytes.toBytes(timestamp));
           miniBatchOp.incrementNumOfPuts();
         } else if (mutation instanceof Delete) { {code}
Obtained the detailed logs of the unexpectedly deleted rowKey. [^aa.log]

After preprocessing the log data, we get:
{code:java}
+I[""ts"":""9223372036854775807""}][1684930529299]
-D[""ts"":""9223372036854775807""}][1684930529299]
+I[""ts"":""9223372036854775807""}][1684930529646]
-D[""ts"":""9223372036854775807""}][1684930529646]
+I[""ts"":""9223372036854775807""}][1684930534294]
-D[""ts"":""9223372036854775807""}][1684930534294]
+I[""ts"":""9223372036854775807""}][1684930536493]
-D[""ts"":""9223372036854775807""}][1684930536493]
+I[""ts"":""9223372036854775807""}][1684930539063]
-D[""ts"":""9223372036854775807""}][1684930539063]
+I[""ts"":""9223372036854775807""}][1684930542714]
-D[""ts"":""9223372036854775807""}][1684930542714]
+I[""ts"":""9223372036854775807""}][1684930545573]
-D[""ts"":""9223372036854775807""}][1684930545573]
+I[""ts"":""9223372036854775807""}][1684930559779]
-D[""ts"":""9223372036854775807""}][1684930559779]
+I[""ts"":""9223372036854775807""}][1684930561168]
-D[""ts"":""9223372036854775807""}][1684930561168]
+I[""ts"":""9223372036854775807""}][1684930561168]
-D[""ts"":""9223372036854775807""}][1684930564624]
+I[""ts"":""9223372036854775807""}][1684930564624]
-D[""ts"":""9223372036854775807""}][1684930565135]
+I[""ts"":""9223372036854775807""}][1684930565135]
-D[""ts"":""9223372036854775807""}][1684930568112]
+I[""ts"":""9223372036854775807""}][1684930568112]
-D[""ts"":""9223372036854775807""}][1684930575200]
+I[""ts"":""9223372036854775807""}][1684930575200]
-D[""ts"":""9223372036854775807""}][1684930577432]
+I[""ts"":""9223372036854775807""}][1684930577432]
-D[""ts"":""9223372036854775807""}][1684930584437]
+I[""ts"":""9223372036854775807""}][1684930584437]
-D[""ts"":""9223372036854775807""}][1684930593148]
+I[""ts"":""9223372036854775807""}][1684930593148]
-D[""ts"":""9223372036854775807""}][1684930599743]
+I[""ts"":""9223372036854775807""}][1684930599743]
-D[""ts"":""9223372036854775807""}][1684930605087]
+I[""ts"":""9223372036854775807""}][1684930605087]
-D[""ts"":""9223372036854775807""}][1684930607430]
+I[""ts"":""9223372036854775807""}][1684930607430]
-D[""ts"":""9223372036854775807""}][1684930609912]
+I[""ts"":""9223372036854775807""}][1684930609912]
-D[""ts"":""9223372036854775807""}][1684930612221]
+I[""ts"":""9223372036854775807""}][1684930612221]
-D[""ts"":""9223372036854775807""}][1684930613310]
+I[""ts"":""9223372036854775807""}][1684930613310]
-D[""ts"":""9223372036854775807""}][1684930616447]
+I[""ts"":""9223372036854775807""}][1684930616447]
-D[""ts"":""9223372036854775807""}][1684930621082]
+I[""ts"":""9223372036854775807""}][1684930621082]
-D[""ts"":""9223372036854775807""}][1684930642614]
+I[""ts"":""9223372036854775807""}][1684930642614] {code}
The HBase RAW data for the unexpectedly deleted rowKey is as follows (obtained from the scan command):
{code:java}
 041                                             column=data:name, timestamp=2023-05-24T20:17:22.614, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:17:22.614, value=7a6820768c7af73ee097
 041                                             column=data:name, timestamp=2023-05-24T20:17:01.082, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:17:01.082, value=2b787b3910a36d3c04f5
 041                                             column=data:name, timestamp=2023-05-24T20:16:56.447, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:56.447, value=999115f88f5fe66b6cca
 041                                             column=data:name, timestamp=2023-05-24T20:16:53.310, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:53.310, value=0302da679440e7743e4a
 041                                             column=data:name, timestamp=2023-05-24T20:16:52.221, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:52.221, value=352b9c11acd86548c126
 041                                             column=data:name, timestamp=2023-05-24T20:16:49.912, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:49.912, value=b254fef72188d54908ae
 041                                             column=data:name, timestamp=2023-05-24T20:16:47.430, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:47.430, value=9c0d6d0faa218ab2acd9
 041                                             column=data:name, timestamp=2023-05-24T20:16:45.087, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:45.087, value=73a40d2892839f657579
 041                                             column=data:name, timestamp=2023-05-24T20:16:39.743, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:33.148, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:24.437, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:17.432, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:15.200, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:08.112, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:05.135, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:04.624, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:16:01.168, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:59.779, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:45.573, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:42.714, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:39.063, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:36.493, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:34.294, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:29.646, type=Delete
 041                                             column=data:name, timestamp=2023-05-24T20:15:29.299, type=Delete

# translate timestamp to microsecond
1684930642614 type=Delete
1684930642614 value=7a6820768c7af73ee097
1684930621082 type=Delete
1684930621082 value=2b787b3910a36d3c04f5
1684930616447 type=Delete
1684930616447 value=999115f88f5fe66b6cca
1684930613310 type=Delete
1684930613310 value=0302da679440e7743e4a
1684930612221 type=Delete
1684930612221 value=352b9c11acd86548c126
1684930609912 type=Delete
1684930609912 value=b254fef72188d54908ae
1684930607430 type=Delete
1684930607430 value=9c0d6d0faa218ab2acd9
1684930605087 type=Delete
1684930605087 value=73a40d2892839f657579
1684930599743 type=Delete
1684930593148 type=Delete
1684930584437 type=Delete
1684930577432 type=Delete
1684930575200 type=Delete
1684930568112 type=Delete
1684930565135 type=Delete
1684930564624 type=Delete
1684930561168 type=Delete
1684930559779 type=Delete
1684930545573 type=Delete
1684930542714 type=Delete
1684930539063 type=Delete
1684930536493 type=Delete
1684930534294 type=Delete
1684930529646 type=Delete
1684930529299 type=Delete {code}
We know that the DELETE key type of delete mutation with timestamp is used to delete the data of the specified timestamp. Analyzing the above two datas, there is no obvious evidence from the scan's RAW data.

We need to analyze the HBase log data. As shown in the following image, the first row of data is the snapshot data in MySQL. The following -D/+I pairs are MySQL's UPDATE (or DELETE/INSERT) events, and for each UPDATE event, the timestamp of -D is the same as the timestamp of the previous +I, that is, delete the data since the last update, and then insert the data of the current UPDATE with HBase's current timestamp. If the timestamp of this UPDATE is the same as the timestamp of the previous UPDATE (lines 17, 18, and 19 in the figure below), the final effect is accidental deletion of the data. Afterwards, all UPDATE's -D/+I have the same timestamp, resulting in the data being deleted.

!image-2023-05-24-23-16-59-508.png|width=756,height=435!;;;","30/Jun/23 12:32;martijnvisser;Fixed in apache/flink:master via:

fe2ef22ff049e88774e57bb9a3ce81a0215ffc52
a7a16484cc678edbd0bb49ab61f111702920ac6c
8d3b74f5588faeb406cad8693398ab914d5ff354;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pyflink gateway server launch fails when purelib != platlib,FLINK-32136,13536902,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,wash,wash,19/May/23 17:34,17/Jun/23 10:03,13/Jul/23 08:29,17/Jun/23 10:03,1.13.3,,,,,,1.16.3,1.17.2,1.18.0,,,,,,API / Python,,,,0,pull-request-available,,,"On distros where python's {{purelib}} is different than {{platlib}} (e.g. Amazon Linux 2, but from my research it's all of the Redhat-based ones), you wind up with components of packages being installed across two different locations (e.g. {{/usr/local/lib/python3.7/site-packages/pyflink}} and {{{}/usr/local/lib64/python3.7/site-packages/pyflink{}}}).

{{_find_flink_home}} [handles|https://github.com/apache/flink/blob/06688f345f6793a8964ec00002175f44cda13c33/flink-python/pyflink/find_flink_home.py#L58C63-L60] this, and in flink releases <= 1.13.2 its setting of the {{FLINK_LIB_DIR}} environment variable was the one being used. However, from 1.13.3, a refactoring of {{launch_gateway_server_process}} ([1.13.2,|https://github.com/apache/flink/blob/release-1.13.2/flink-python/pyflink/pyflink_gateway_server.py#L200] [1.13.3|https://github.com/apache/flink/blob/release-1.13.3/flink-python/pyflink/pyflink_gateway_server.py#L280]) re-ordered some method calls. {{{}prepare_environment_variable{}}}'s [non-awareness|https://github.com/apache/flink/blob/release-1.13.3/flink-python/pyflink/pyflink_gateway_server.py#L94C67-L95] of multiple homes and setting of {{FLINK_LIB_DIR}} now is the one that matters, and it is the incorrect location.

I've confirmed this problem on Amazon Linux 2 and 2023. The problem does not exist on, for example, Ubuntu 20 and 22 (for which {{platlib}} == {{{}purelib{}}}).

Repro steps on Amazon Linux 2
{quote}{{yum -y install python3 java-11}}
{{pip3 install apache-flink==1.13.3}}
{{python3 -c 'from pyflink.table import EnvironmentSettings ; EnvironmentSettings.new_instance()'}}
{quote}
The resulting error is
{quote}{{The flink-python jar is not found in the opt folder of the FLINK_HOME: /usr/local/lib64/python3.7/site-packages/pyflink}}
{{Error: Could not find or load main class org.apache.flink.client.python.PythonGatewayServer}}
{{Caused by: java.lang.ClassNotFoundException: org.apache.flink.client.python.PythonGatewayServer}}
{{Traceback (most recent call last):}}
{{  File ""<string>"", line 1, in <module>}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/table/environment_settings.py"", line 214, in new_instance}}
{{    return EnvironmentSettings.Builder()}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/table/environment_settings.py"", line 48, in {_}{{_}}init{{_}}{_}}}
{{    gateway = get_gateway()}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/java_gateway.py"", line 62, in get_gateway}}
{{    _gateway = launch_gateway()}}
{{  File ""/usr/local/lib64/python3.7/site-packages/pyflink/java_gateway.py"", line 112, in launch_gateway}}
{{    raise Exception(""Java gateway process exited before sending its port number"")}}
{{Exception: Java gateway process exited before sending its port number}}
{quote}
The flink home under /lib64/ does not contain the jar, but it is in the /lib/ location
{quote}{{bash-4.2# find /usr/local/lib64/python3.7/site-packages/pyflink -name ""flink-python*.jar""}}
{{bash-4.2# find /usr/local/lib/python3.7/site-packages/pyflink -name ""flink-python*.jar""}}
{{/usr/local/lib/python3.7/site-packages/pyflink/opt/flink-python_2.11-1.13.3.jar}}
{quote}
 ",,dianfu,wash,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 17 10:03:29 UTC 2023,,,,,,,,,,"0|z1i0go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 11:44;dianfu;[~wash] Good catch! This seems like a critical problem. Would you like to open a PR to fix this issue?;;;","15/Jun/23 13:36;wash;I doubt I'll have the time anytime soon to study the code enough to make such a change.;;;","16/Jun/23 05:25;dianfu;[~wash] I have submitted a PR (https://github.com/apache/flink/pull/22802). Could you help to review? It would be great if you could also help to verify it~;;;","16/Jun/23 18:01;wash;I can confirm it resolves the use-case that was previously failing for me. Thanks for getting that PR up so quickly.;;;","17/Jun/23 10:03;dianfu;Fixed in:
- master via f64563bc1a7ff698acd708b61e9e80ae9c3e848f
- release-1.17 via 1b3f25432a29005370b0f51aaa7d4ee79a5edd58
- release-1.16 via f9394025fb756c844ec5f2615971f227d40b9244;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler min/max parallelism configs should respect the current job parallelism,FLINK-32134,13536868,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,19/May/23 13:19,22/May/23 13:42,13/Jul/23 08:29,22/May/23 13:41,kubernetes-operator-1.5.0,,,,,,kubernetes-operator-1.6.0,,,,,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,The autoscaler should never scale the job purely due to max/min parallelism configs. We should adjust these limits to the current parallelism,,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 13:41:57 UTC 2023,,,,,,,,,,"0|z1i094:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 13:41;gyfora;merged to main 505da97b73798f5f8134f56354ef6e205bddb4e9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Filesystem connector is not compatible with option 'pipeline.generic-types',FLINK-32129,13536800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,19/May/23 02:27,22/May/23 11:47,13/Jul/23 08:29,22/May/23 11:45,1.17.0,,,,,,1.18.0,,,,,,,,Connectors / FileSystem,,,,0,pull-request-available,,,"Filesystem connector always output 'PartitionCommitInfo' message even when there is no partition in the sink table, which will cause exception `java.lang.UnsupportedOperationException: Generic types have been disabled in the ExecutionConfig and type java.util.List is treated as a generic type.` when `pipeline.generic-types` is false",,KristoffSC,libenchao,Thesharing,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 11:45:10 UTC 2023,,,,,,,,,,"0|z1hzu8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/May/23 11:45;libenchao;Fixed via [https://github.com/apache/flink/commit/3cbacbf26f09b5301b280beed4f78fc03d573d76]

[~zjureel] Thanks for the PR!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avro Confluent Schema Registry nightly end-to-end test failed due to timeout,FLINK-32123,13536612,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,pgaref,pgaref,pgaref,17/May/23 19:25,20/May/23 17:38,13/Jul/23 08:29,20/May/23 17:37,1.18.0,,,,,,1.18.0,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,test-stability,,"For the past few hours, E2E tests fail with: 'Avro Confluent Schema Registry nightly end-to-end test' failed after 9 minutes and 53 seconds! Test exited with exit code 1

Looks like [https://archive.apache.org/dist/kafka/]  mirror is overloaded – download locally took more than 30min

Lets switch to  [https://downloads.apache.org|https://downloads.apache.org/] mirror
 ",,dmvk,pgaref,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,FLINK-32121,,,,,INFRA-24607,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 20 17:37:29 UTC 2023,,,,,,,,,,"0|z1hyog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/May/23 02:39;tanyuxin;[~pgaref] Thanks for reporting this, I will take a look at this issue.;;;","18/May/23 03:49;pgaref;[~tanyuxin]  I already opened a  PR and tests passed [https://github.com/apache/flink/pull/22603]  – just need a +1 :) 
Please reassign to me;;;","18/May/23 05:43;tanyuxin;OK, The jira has something wrong and can not link it dynamically. So I didn't notice the PR. I have reassigned it to you.;;;","18/May/23 05:48;pgaref;Thanks much [~tanyuxin] !;;;","18/May/23 06:09;dmvk;> Lets switch to  [https://downloads.apache.org|https://downloads.apache.org/] mirror

The problem with this approach is that it only provides the latest minor version of N major versions, so it will start failing any time Kafka folks do a new minor release.;;;","18/May/23 06:12;dmvk;I'm opening a ticket with INFRA; the proper solution should be fixing the archive on their side;;;","18/May/23 06:51;dmvk;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49102&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d#:~:text=%5BFAIL%5D%20%27Avro%20Confluent%20Schema%20Registry]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49103&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=0f3adb59-eefa-51c6-2858-3654d9e0749d&l=3804;;;","18/May/23 10:27;dmvk;master: 8c3637b47f3dd906694857cd1b808139599126fa

 

The change should be reverted once  INFRA-24607 is addressed.;;;","20/May/23 17:37;Weijie Guo;Reverted in master(1.18) via f32052a12309cfe38f66344cf6d4ab39717e44c8 as INFRA-24607 was addressed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replacing cluster in failed state with a new one failed,FLINK-32111,13536420,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tamirsagi,tamirsagi,tamirsagi,16/May/23 15:07,15/Jun/23 11:40,13/Jul/23 08:29,19/May/23 15:43,kubernetes-operator-1.5.0,kubernetes-operator-1.6.0,,,,,kubernetes-operator-1.6.0,,,,,,,,Kubernetes Operator,,,,0,,,,"I deployed a problematic cluster(HA enabled with 3 JMs) to check cluster updates process. The cluster was in restart loops. Then I provided a newer CRD (Updated several configurations) and expected the cluster to get re-deployed. however the following exception happened

 

Caused by: java.lang.NullPointerException
        at org.apache.flink.kubernetes.operator.service.CheckpointHistoryWrapper.getInProgressCheckpoint(CheckpointHistoryWrapper.java:60) 
        at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.getCheckpointInfo(AbstractFlinkService.java:564) 
        at org.apache.flink.kubernetes.operator.service.AbstractFlinkService.getLastCheckpoint(AbstractFlinkService.java:520) 
        at org.apache.flink.kubernetes.operator.observer.SavepointObserver.observeLatestSavepoint(SavepointObserver.java:209) 
        at org.apache.flink.kubernetes.operator.observer.SavepointObserver.observeSavepointStatus(SavepointObserver.java:73) 
        at org.apache.flink.kubernetes.operator.observer.deployment.ApplicationObserver.observeFlinkCluster(ApplicationObserver.java:61) 
        at org.apache.flink.kubernetes.operator.observer.deployment.AbstractFlinkDeploymentObserver.observeInternal(AbstractFlinkDeploymentObserver.java:73) 
        at org.apache.flink.kubernetes.operator.observer.AbstractFlinkResourceObserver.observe(AbstractFlinkResourceObserver.java:53) 
        at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:134) 

 

upgradeMode was first `last-state` and then I changed it to `stateless` but it still did not deploy the new cluster.",,gyfora,tamirsagi,,,,,,,,,,,,,,,,,,,,,,,FLINK-32340,,,,,,,,,,,,,,,,"16/May/23 15:07;tamirsagi;operator-error.txt;https://issues.apache.org/jira/secure/attachment/13058262/operator-error.txt",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 15:43:43 UTC 2023,,,,,,,,,,"0|z1hxhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 15:10;gyfora;I have seen this issue once in the past, could be that some strange data was returned by the flink rest api resulting in a null somewhere in the response. We should add a null check in the logic.;;;","16/May/23 15:11;gyfora;[~tamirsagi] would you be interested in providing a simple fix for this?;;;","17/May/23 07:58;tamirsagi;sure,

I already created MR.

[https://github.com/apache/flink-kubernetes-operator/pull/603|https://github.com/apache/flink-kubernetes-operator/pull/603/commits]

I tested it locally, it worked.;;;","19/May/23 15:43;gyfora;merged to main c531b3701a5e1e7ea51d37143979c27f1c88f78f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TM native memory leak when using time window in Pyflink ThreadMode,FLINK-32110,13536413,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunjunluo,yunjunluo,yunjunluo,16/May/23 14:06,25/May/23 09:50,13/Jul/23 08:29,25/May/23 09:50,1.17.0,,,,,,1.17.2,,,,,,,,API / Python,,,,0,,,,"If job use time window in Pyflink thread mode, TM native memory will grow slowly during the job running until TM can't allocate memory from operate system.

The leak rate is likely proportional to the number of key.",,begginghard,dianfu,hxb,yunjunluo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 09:50:49 UTC 2023,,,,,,,,,,"0|z1hxg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 09:50;hxb;Merged into master via 2f015eb899feb6e60a379a33baab442f93f17ca2

Merged into release-1.17 11ccc44b1e7beacc198a78cdc75be6294840a74b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable connection to archive.apache.org on AZP,FLINK-32106,13536366,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,16/May/23 07:05,25/May/23 10:46,13/Jul/23 08:29,25/May/23 10:46,1.18.0,,,,,,1.18.0,,,,,,,,Tests,,,,0,test-stability,,,"Azure pipelines fail with 
{noformat}
Using Google mirror

Resolving archive.apache.org (archive.apache.org)... 138.201.131.134, 2a01:4f8:172:2ec5::2
Connecting to archive.apache.org (archive.apache.org)|138.201.131.134|:443... failed: Connection timed out.
Connecting to archive.apache.org (archive.apache.org)|2a01:4f8:172:2ec5::2|:443... failed: Network is unreachable.
##[error]Bash exited with code '4'.

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49020&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=9baa6deb-e632-5387-d76c-cf2ba9138f2e&l=16",,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 10:45:56 UTC 2023,,,,,,,,,,"0|z1hx5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 07:06;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49020&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=e811a31d-7c99-5e74-90b0-fc9fa25fddce&l=16;;;","16/May/23 07:06;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49020&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=a7382ec4-87d2-5a9d-7c53-a2f93e317458&l=16;;;","16/May/23 07:07;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49021&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=d6e79740-7cf7-5407-2e69-ca34c9be0efb&l=16;;;","25/May/23 10:45;Sergey Nuyanzin;url was changed to https://repo.maven.apache.org (same as in maven wrapper) 
;;;","25/May/23 10:45;Sergey Nuyanzin;merged as [d62ad9d24bd00c62956549688b584bddb38abf82|https://github.com/apache/flink/commit/d62ad9d24bd00c62956549688b584bddb38abf82];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RBAC flinkdeployments/finalizers missing for OpenShift Deployment,FLINK-32103,13536319,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jbusche,jbusche,jbusche,15/May/23 21:16,19/May/23 06:33,13/Jul/23 08:29,19/May/23 06:33,kubernetes-operator-1.5.0,,,,,,kubernetes-operator-1.6.0,,,,,,,,Kubernetes Operator,,,,0,,,,"In OpenShift 4.10 and above, I'm noticing with the Flink 1.5.0 RC release that there's an issue with flinkdeployments on OpenShift.  Flinkdeployments are stuck in upgrading:
{quote}oc get flinkdep

NAME                                    JOB STATUS   LIFECYCLE STATE

basic-example                                        UPGRADING
{quote}
 

The error message looks like:
{quote}oc describe flinkdep basic-example

....

Error:                          {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster \""basic-example\""."",""throwableList"":[\{""type"":""org.apache.flink.client.deployment.ClusterDeploymentException"",""message"":""Could not create Kubernetes cluster \""basic-example\"".""},\{""type"":""org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException"",""message"":""Failure executing: POST at: https://172.30.0.1/apis/apps/v1/namespaces/default/deployments. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. deployments.apps \""basic-example\"" is forbidden: cannot set blockOwnerDeletion if an ownerReference refers to a resource you can't set finalizers on: , <nil>.""}]}

 

 Job Manager Deployment Status:  MISSING
{quote}
 

The solution is to fix it in the rbac.yaml of the helm template, adding a ""  - flinkdeployments/finalizers"" line to the flink.apache.org apiGroup.

 

If the Operator is already running and flinkdeployments are having trouble on OpenShift, then someone can manually edit the flink-kubernetes-operator.v1.5.0 clusterrole and add the

""  - flinkdeployments/finalizers"" in the flink.apache.org apiGroup.

 

I'll create a PR that addresses this.",,gyfora,jbusche,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 06:33:58 UTC 2023,,,,,,,,,,"0|z1hwvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/May/23 01:04;jbusche;Created PR [https://github.com/apache/flink-kubernetes-operator/pull/600] to address this;;;","16/May/23 01:06;jbusche;Can fix this on an existing OpenShift install by adding ""- flinkdeployments/finalizers"" to the flink.apache.org resources like this:
{quote}oc edit clusterrole flink-operator
 - apiGroups:

  - flink.apache.org

  resources:

  - flinkdeployments

  - flinkdeployments/status

  - flinkdeployments/finalizers

  - flinksessionjobs

  - flinksessionjobs/status

  verbs:

  - '*'
{quote};;;","19/May/23 06:33;gyfora;merged to main 0edb5443bf38c7a2dd5ada56f10301e4799f9b35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Max parallelism is incorrectly calculated with multiple topics,FLINK-32100,13536214,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,15/May/23 10:57,16/May/23 17:23,13/Jul/23 08:29,16/May/23 17:22,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.5.1,kubernetes-operator-1.6.0,,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,"So far, we've taken the max number partitions we can find. However, the correct way to calculate the
max source parallelism would be to sum the number of partitions of all topis.
",,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-15 10:57:37.0,,,,,,,,,,"0|z1hw8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.17 doc points to non-existant branch in flink-training repo,FLINK-32096,13536175,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,samrat007,samrat007,15/May/23 07:08,16/May/23 07:03,13/Jul/23 08:29,15/May/23 08:36,1.17.0,,,,,,,,,,,,,,Documentation,Documentation / Training / Exercises,,,0,,,,"There is one broken link in Flink documentation which i think needs commiter or PMC privileges.
In [Hands-on|https://nightlies.apache.org/flink/flink-docs-stable/docs/learn-flink/etl/#hands-on] section

>  The hands-on exercise that goes with this section is the Rides and Fares .

[Rides and Fares|https://github.com/apache/flink-training/blob/release-1.17//rides-and-fares] points to non-existent branch in flink-training repo and leads to 404 (Not Found)  [hyper-link|https://github.com/apache/flink-training/blob/release-1.17//rides-and-fares]. This is due to missing `release-1.17` branch in [flink-training|https://github.com/apache/flink-training/] repo.
 ",,leonard,samrat007,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32105,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 08:36:57 UTC 2023,,,,,,,,,,"0|z1hvzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 07:16;yunta;[~samrat007] Thanks for the reporting, I will fix this problem.;;;","15/May/23 07:26;samrat007;[~yunta] :+1;;;","15/May/23 08:36;yunta;Already resolved, merged in master: 983035c1799ae985b6dbcac5320de4b8a866819f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
startScheduling.BATCH performance regression since May 11th,FLINK-32094,13536145,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tanyuxin,martijnvisser,martijnvisser,14/May/23 21:57,15/May/23 13:35,13/Jul/23 08:29,15/May/23 13:34,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Coordination,,,,0,,,,http://codespeed.dak8s.net:8000/timeline/#/?exe=5&ben=startScheduling.BATCH&extr=on&quarts=on&equid=off&env=2&revs=200,,huwh,JunRuiLi,martijnvisser,tanyuxin,Thesharing,wanglijie,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,FLINK-31635,,,,,,,,,,,"14/May/23 21:58;martijnvisser;image-2023-05-14-22-58-00-886.png;https://issues.apache.org/jira/secure/attachment/13058161/image-2023-05-14-22-58-00-886.png","15/May/23 04:33;Thesharing;image-2023-05-15-12-33-56-319.png;https://issues.apache.org/jira/secure/attachment/13058168/image-2023-05-15-12-33-56-319.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 13:35:04 UTC 2023,,,,,,,,,,"0|z1hvtc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/May/23 03:59;Weijie Guo;This may caused by FLINK-31635, [~tanyuxin] would you mind taking a look at this?;;;","15/May/23 04:35;Thesharing;The extra cost may be caused by {{{}TieredStorageIdMappingUtils#convertId(ResultPartitionID resultPartitionId){}}}.

!image-2023-05-15-12-33-56-319.png|height=75%,width=75%!;;;","15/May/23 08:20;tanyuxin;Thanks [~martijnvisser] and [~Thesharing].

I reproduced the issue locally. 
To address the issue, I have submitted a PR(https://github.com/apache/flink/pull/22578) to prevent the creation of the tiered internal shuffle master until tiered storage is enabled. 
Furthermore, to ensure that the issue does not reoccur, we will address the regression issue before proceeding with the enablement of tiered storage.;;;","15/May/23 13:35;Weijie Guo;master(1.18) via 1a3b539fa43b4e1c8f07accf2d4aa352b7f63858.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When no pod template configured, an invalid null pod template is configured ",FLINK-32067,13535943,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,12/May/23 09:37,12/May/23 12:48,13/Jul/23 08:29,12/May/23 12:48,kubernetes-operator-1.5.0,,,,,,kubernetes-operator-1.5.0,kubernetes-operator-1.6.0,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"https://issues.apache.org/jira/browse/FLINK-30609 introduced a bug in the podtemplate logic that breaks deployments when no podtemplates are configured.

The basic example doesnt work anymore for example. The reason is that an invalid null object is set as podtemplate when nothing is configured.",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 12:48:33 UTC 2023,,,,,,,,,,"0|z1huq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/23 12:48;gyfora;main: af8aabf48928804619cfdb6874700b2bf2250a87
release-1.5: f5be73b9e8861fe7134c224d68eabcd10a9ebfd3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink CI service on Azure stops responding to pull requests,FLINK-32066,13535927,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,Wencong Liu,Wencong Liu,12/May/23 07:22,12/May/23 16:52,13/Jul/23 08:29,12/May/23 16:52,1.18.0,,,,,,1.18.0,,,,,,,,Build System / Azure Pipelines,,,,0,,,,"As of the time when this issue was created, Flink's CI service on Azure could no longer be triggered by new pull requests.
!20230512152023.jpg!",,jingge,mapohl,martijnvisser,Sergey Nuyanzin,tanyuxin,Thesharing,Wencong Liu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/23 07:22;Wencong Liu;20230512152023.jpg;https://issues.apache.org/jira/secure/attachment/13058032/20230512152023.jpg",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 16:14:55 UTC 2023,,,,,,,,,,"0|z1humo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/May/23 07:44;mapohl;Thanks for reporting this, [~Wencong Liu]. [~jingge] [~wangyang0918] may someone have a look at this? Looks like there are some problems with the CIBot deployment?

CC: [~snuyanzin] fyi;;;","12/May/23 07:45;martijnvisser;I've raised this to a Blocker. ;;;","12/May/23 15:29;jingge;working on it;;;","12/May/23 16:14;jingge;should work now, please check and share the feedback for confirmation;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Resource metric groups are not cleaned up on removal,FLINK-32061,13535773,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,11/May/23 10:56,11/May/23 11:13,13/Jul/23 08:29,11/May/23 11:11,,,,,,,kubernetes-operator-1.5.0,,,,,,,,Autoscaler,Kubernetes Operator,,,0,,,,Not cleaning up leaks memory.,,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-05-11 10:56:08.0,,,,,,,,,,"0|z1htog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update the used Pulsar connector in flink-python to 4.0.0,FLINK-32056,13535745,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,martijnvisser,martijnvisser,11/May/23 07:44,23/May/23 12:10,13/Jul/23 08:29,23/May/23 12:03,1.17.1,1.18.0,,,,,1.17.2,1.18.0,,,,,,,API / Python,Connectors / Pulsar,,,0,pull-request-available,,,"flink-python still references and tests flink-connector-pulsar:3.0.0, while it should be using flink-connector-pulsar:4.0.0. That's because the newer version is the only version compatible with Flink 1.17 and it doesn't rely on flink-shaded. ",,dianfu,martijnvisser,,,,,,,,,,,,,,,,,,,FLINK-32032,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 23 12:03:17 UTC 2023,,,,,,,,,,"0|z1hti8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/May/23 12:03;dianfu;Fixed in:
- master via fbf7b91424ec626ae56dd2477347a7759db6d5fe
- release-1.17 via d3a3755a7eef5708871580671169fd6bd2babf28;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"DecimalITCase.testAggMinGroupBy fails with ""Insufficient number of network buffers""",FLINK-32048,13535652,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tanyuxin,wanglijie,wanglijie,10/May/23 14:22,11/May/23 11:57,13/Jul/23 08:29,11/May/23 11:57,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Network,Table SQL / Planner,Tests,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48855&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4]
{code:java}
May 10 09:37:41 Caused by: java.io.IOException: Insufficient number of network buffers: required 1, but only 0 available. The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max'.
May 10 09:37:41 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalCreateBufferPool(NetworkBufferPool.java:495)
May 10 09:37:41 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.createBufferPool(NetworkBufferPool.java:456)
May 10 09:37:41 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGateFactory.lambda$createBufferPoolFactory$3(SingleInputGateFactory.java:330)
May 10 09:37:41 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:274)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.setup(InputGateWithMetrics.java:105)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:969)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:654)
May 10 09:37:41 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
May 10 09:37:41 	at java.lang.Thread.run(Thread.java:748)
{code}",,Sergey Nuyanzin,tanyuxin,wanglijie,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30815,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 11 11:57:07 UTC 2023,,,,,,,,,,"0|z1hsxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/23 15:25;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48874&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12367;;;","10/May/23 18:22;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48863&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12370;;;","10/May/23 18:24;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48844&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12658;;;","10/May/23 20:22;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48885&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12654;;;","11/May/23 07:02;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48890&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12654;;;","11/May/23 07:31;Weijie Guo;This maybe caused by FLINK-30815, [~tanyuxin] would you mind taking a look at this?;;;","11/May/23 07:49;tanyuxin;I will take a look. The reason may be that the parent class is migrated to JUnit5 in https://issues.apache.org/jira/browse/FLINK-30815, the subclasses can not be set to DEFAULT_PARALLELISM=3 in BatchAbstractTestBase, because the subclass DecimalITCase is not JUnit5.
I will first update this class to JUnit5 to resolve the issue. And I created a follow-up jira https://issues.apache.org/jira/browse/FLINK-32055 to resolve this.;;;","11/May/23 08:28;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48895&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12869;;;","11/May/23 08:29;Sergey Nuyanzin;Thanks for having a look [~tanyuxin];;;","11/May/23 11:57;Weijie Guo;master(1.18) via 8ef5cc8c3f85b26e713b9a37c02ac54475f493d8.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlClient session unrecoverable once one wrong setting occurred,FLINK-32043,13535608,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,lincoln.86xy,lincoln.86xy,10/May/23 08:33,25/May/23 11:51,13/Jul/23 08:29,25/May/23 11:51,1.17.0,1.18.0,,,,,,,,,,,,,Table SQL / Client,,,,0,,,,"In sql client, it can not work normally once one wrong setting occurred
{code:java}
// wrong setting here

Flink SQL> SET table.sql-dialect = flink;
[INFO] Execute statement succeed.

Flink SQL> select '' AS f1, a from t1;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK

Flink SQL> SET table.sql-dialect = default;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK

Flink SQL> RESET table.sql-dialect;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK

Flink SQL> RESET;
[ERROR] Could not execute SQL statement. Reason:
java.lang.IllegalArgumentException: No enum constant org.apache.flink.table.api.SqlDialect.FLINK 
{code}",,fsk119,jark,leonard,lincoln.86xy,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 25 02:22:10 UTC 2023,,,,,,,,,,"0|z1hsns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/May/23 02:22;fsk119;Merged into release-1.17: 23030f6546a5f5877166ee1dc6f49dd18f4dc188

Merged into master: 1ef847b2c5244de0ba351dff3f21701acb8f3cce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-kubernetes-operator RoleBinding for Leases not created in correct namespace when using watchNamespaces,FLINK-32041,13535530,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tchin,ottomata,ottomata,09/May/23 14:27,02/Jun/23 12:16,13/Jul/23 08:29,01/Jun/23 11:14,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.6.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"When enabling [HA for flink-kubernetes-operator|https://nightlies.apache.org/flink/flink-kubernetes-operator-docs-main/docs/operations/configuration/#leader-election-and-high-availability] RBAC rules must be created to allow the flink-operator to manage k8s Lease resources.  When not using {{{}watchNamespaces{}}}, the RBAC rules are created at the k8s cluster level scope, giving the flink-operator ServiceAccount the ability to manage all needed k8s resources for all namespaces.

However, when using {{{}watchNamespaces{}}}, RBAC rules are only created in the {{{}watchNamepaces{}}}.  For most rules, this is correct, as the operator needs to manage resources like Flink pods and deployments in the {{{}watchNamespaces{}}}.  

However, For flink-kubernetes-operator HA, the Lease resource is managed in the same namespace in which the operator is deployed.  

The Helm chart should be fixed so that the proper RBAC rules for Leases are created to allow the operator's ServiceAccount in the operator's namespace.

Mailing list discussion [here.|https://lists.apache.org/thread/yq89jm0szkcodfocm5x7vqnqdmh0h1l0]",,gyfora,huwh,ottomata,tamirsagi,tchin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 11:14:51 UTC 2023,,,,,,,,,,"0|z1hs6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/May/23 15:10;gyfora;A current workaround for this would be to add the operator's own namespace to the list of watched namespaces. That would set up the roles correctly in that namespace as well :) ;;;","10/May/23 05:42;tamirsagi;Hey Gyula,

I also encountered something similar  (HA is enabled).

 
I checked the rolebinding between the service account `dev-0-flink-clusters:dev-0-xsight-flink-operator-sa` and the corresponded role({*}flink-operator{*}) which has been created by the operator using *{{rbac.nodesRule.create=true, they both look fine.}}*

 

The operator watches 2 namespaces:
 # its own:  dev-0-flink-clusters
 # dev-0-flink-temp-clusters

!https://lists.apache.org/api/email.lua?attachment=true&id=61qtwrnxlh722pvok8dtnzdt7t7k7drb&file=fe69ed8d14240d73b73f68176ee7fa4f13f2b0ee303676f8eea92b7bdee9ceb3!

!https://lists.apache.org/api/email.lua?attachment=true&id=61qtwrnxlh722pvok8dtnzdt7t7k7drb&file=c8a40ca61528174bd1667e3fcf10ba39e2224700198a69e828db80c66315719d!

{{Then the following error is thrown:}}

{{org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException"",""message"":""Failure executing: GET at: [https://172.20.0.1/api/v1/nodes]. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. nodes is forbidden: User ""system:serviceaccount:dev-0-flink-clusters:{*}dev-0-xsight-flink-operator-sa{*}"" cannot list resource ""nodes"" in API group """" at the cluster scope.""}}

{{could it be related to : kubernetes.rest-service.exposed.type? }}

 

EDIT: seems like it resolved when changing {{kubernetes.rest-service.exposed.type from NodePort to ClusterIP.}};;;","14/May/23 16:24;gyfora;[~tamirsagi] does the same error occur without HA enabled?;;;","14/May/23 16:32;tamirsagi;I need to try and get back to you with an answer (probably tomorrow) . But it does seems connected to -k8s HA service &- RestClient & KubeClient. 
 
RestClient uses k8s client internally which needs NodeList permissions but instead of reading from Service account it looks for kube.config file.

[https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#kubernetes-config-file]

 
ClusterIP Service
[https://github.com/apache/flink/blob/release-1.17.0/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/services/ClusterIPService.java#L44-L53]
 
NodePort Service
[https://github.com/apache/flink/blob/release-1.17.0/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/services/NodePortService.java#L62];;;","14/May/23 16:35;gyfora;This may be completely unrelated to the original issue described in the ticket. That is about kubernetes operator HA, not Flink job HA.
Would be good to know whether this error occurs in operator 1.4.0 vs 1.5.0 and Flink version 1.16 vs 1.17;;;","14/May/23 17:11;tamirsagi;-I'm talking about Operator HA.-

-My point was when creating RestClusterClient, its constructor can take the ClientHighAvailabilityServicesFactory, if not provided it will look in classpath for implementation(based on cluster configurations). In my Flink configurations it is set to kubernetes. that's why I thought it might be related. (Noticed that in the operator it does pass the standalone HA service). so I might be wrong here.-

 

Edit: It has nothing to do with HA. rest expose-type should be ClusterIP and not NodePort.;;;","14/May/23 17:26;gyfora;Can you please share both the operator and FlinkDeployment configuration? I am a bit confused ;;;","18/May/23 17:43;ottomata;[~tchin] Is going to submit a PR for this, can you assign this to him?;;;","18/May/23 17:46;gyfora;done :) ;;;","22/May/23 13:29;tchin;Don't know how the GitHub bot works but a pull request has been opened [here|https://github.com/apache/flink-kubernetes-operator/pull/604];;;","01/Jun/23 11:14;gyfora;merged to main 6711bd93e1c8b1496c81117b6147ab6e163551fb;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python's DistUtils is deprecated as of 3.10,FLINK-32034,13535386,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,coltenp,coltenp,coltenp,08/May/23 14:23,15/Jun/23 12:16,13/Jul/23 08:29,15/Jun/23 12:16,1.17.0,,,,,,1.17.2,1.18.0,,,,,,,API / Python,,,,0,pull-request-available,,,"I have recent just went through an upgrade from 1.13 to 1.17, along with that I upgraded the python version on our Flink Session server. Most everything that is part of our workflow works, except for Python Dependency Management.

After doing some digging, I found the reason is due to the DeprecationWarning that is printed when trying to get the site packages path. The script is 

GET_SITE_PACKAGES_PATH_SCRIPT and it is executed in the getSitePackagesPath method in the PythonEnvironmentManagerUtils class. The issue is that the DeprecationWarning is included into the PYTHONPATH environment variable which is passed to the beam runner. The deprecation warning breaks Python's ability to find the site packages due to characters that are not allowed in filesystem paths.

 

Example of the PYTHONPATH environment variable:
PYTHONPATH == <string>:1: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives:/tmp/python-dist-c63e1464-925c-4289-bb71-c6f50e83186f/python-requirements/lib/python3.10/site-packages
HADOOP_CONF_DIR == /opt/flink/conf","Kubernetes

Java 11

Python 3.10.9",coltenp,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/May/23 15:39;coltenp;get_site_packages_path_script.py;https://issues.apache.org/jira/secure/attachment/13057895/get_site_packages_path_script.py","08/May/23 15:39;coltenp;get_site_packages_path_script_shortened.py;https://issues.apache.org/jira/secure/attachment/13057894/get_site_packages_path_script_shortened.py",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Patch,,,,,,,,9223372036854775807,,,Java,Thu Jun 15 12:16:06 UTC 2023,,,,,,,,,,"0|z1hrao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 14:41;coltenp;After digging into the code, and taking a couple of hours to review the alternatives, I believe that I have found a way that would be a good replacement for the GET_SITE_PACKAGES_PATH_SCRIPT code. I have attached the scripts to the ticket. I can also make a branch and commit these.;;;","14/Jun/23 12:43;dianfu;[~coltenp] Thanks for reporting this issue and the finding. Would you like to create a PR for this issue?;;;","14/Jun/23 19:57;coltenp;[~dianfu] - Yep, I sure can do that. PR incoming soon.;;;","14/Jun/23 20:22;coltenp;[~dianfu] - [https://github.com/apache/flink/pull/22782] Here is the link to review. Thanks!;;;","15/Jun/23 12:16;dianfu;Fixed in:
- master via 6ee1912e949cf290e5e1e620b1f4bae6552b428b
- release-1.17 via dd1dde377f775c43fcd95eed1bae91bf5fcfee2e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink GCP Connector having issues with Conscrypt library,FLINK-32031,13535362,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jjayadeep,jjayadeep,jjayadeep,08/May/23 10:31,04/Jul/23 09:08,13/Jul/23 08:29,04/Jul/23 09:08,1.15.4,,,,,,gcp-pubsub-3.0.2,,,,,,,,Connectors / Google Cloud PubSub,,,,0,pubsub,pull-request-available,,"When using the current pubsub connector it is not using the latest libraries bom due to which when the connector is used in Cloud Dataproc it is failing with conscrypt related issues.

 
{code:java}
Caused by: repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannel$AnnotatedSocketException: Network is unreachable: pubsub.googleapis.com/2607:f8b0:4001:c23:0:0:0:5f:443
Caused by: java.net.SocketException: Network is unreachable
    at java.base/sun.nio.ch.Net.connect0(Native Method)
    at java.base/sun.nio.ch.Net.connect(Net.java:483)
    at java.base/sun.nio.ch.Net.connect(Net.java:472)
    at java.base/sun.nio.ch.SocketChannelImpl.connect(SocketChannelImpl.java:692)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:91)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.SocketUtils$3.run(SocketUtils.java:88)
    at java.base/java.security.AccessController.doPrivileged(Native Method)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.SocketUtils.connect(SocketUtils.java:88)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.socket.nio.NioSocketChannel.doConnect(NioSocketChannel.java:315)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.connect(AbstractNioChannel.java:248)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.connect(DefaultChannelPipeline.java:1342)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:548)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.connect(AbstractChannelHandlerContext.java:533)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.ChannelDuplexHandler.connect(ChannelDuplexHandler.java:54)
    at repackaged.io.grpc.netty.shaded.io.grpc.netty.WriteBufferingAndExceptionHandler.connect(WriteBufferingAndExceptionHandler.java:150)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeConnect(AbstractChannelHandlerContext.java:548)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext.access$1000(AbstractChannelHandlerContext.java:61)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.AbstractChannelHandlerContext$9.run(AbstractChannelHandlerContext.java:538)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
    at repackaged.io.grpc.netty.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
    at repackaged.io.grpc.netty.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
    at repackaged.io.grpc.netty.shaded.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
    at java.base/java.lang.Thread.run(Thread.java:829){code}

On disabling ipv6 getting the following error:-
{code:java}
Channel Pipeline: [SslHandler#0, ProtocolNegotiators$ClientTlsHandler#0, WriteBufferingAndExceptionHandler#0, DefaultChannelPipeline$TailContext#0]
    at io.grpc.stub.ClientCalls.toStatusRuntimeException(ClientCalls.java:244)
    at io.grpc.stub.ClientCalls.getUnchecked(ClientCalls.java:225)
    at io.grpc.stub.ClientCalls.blockingUnaryCall(ClientCalls.java:142)
    at com.google.pubsub.v1.SubscriberGrpc$SubscriberBlockingStub.pull(SubscriberGrpc.java:1641)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:73)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:77)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:77)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:77)
    at org.apache.flink.streaming.connectors.gcp.pubsub.BlockingGrpcPubSubSubscriber.pull(BlockingGrpcPubSubSubscriber.java:67)
    at org.apache.flink.streaming.connectors.gcp.pubsub.PubSubSource.run(PubSubSource.java:128)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
    at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
    at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:332)
Caused by: javax.net.ssl.SSLHandshakeException: No subjectAltNames on the certificate match
    at org.conscrypt.SSLUtils.toSSLHandshakeException(SSLUtils.java:361)
    at org.conscrypt.ConscryptEngine.convertException(ConscryptEngine.java:1135)
    at org.conscrypt.ConscryptEngine.readPlaintextData(ConscryptEngine.java:1090)
    at org.conscrypt.ConscryptEngine.unwrap(ConscryptEngine.java:867)
    at org.conscrypt.ConscryptEngine.unwrap(ConscryptEngine.java:738)
    at org.conscrypt.Java8EngineWrapper.unwrap(Java8EngineWrapper.java:252)
    at org.conscrypt.Conscrypt.unwrap(Conscrypt.java:605)    {code}
 ",,jjayadeep,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 03 08:46:48 UTC 2023,,,,,,,,,,"0|z1hr5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 10:32;jjayadeep;The libraries bom needs to be updated, will submit a PR soon.;;;","03/Jul/23 08:46;martijnvisser;Fixed in:

apache/flink-connector-gcp-pubsub:main 4fdaca7b42969d19bd939c0823afb5372c51461b
apache/flink-connector-gcp-pubsub:v3.0 cba42d81fbe41fcf3687bf2bc243d912e9362ef6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FutureUtils.handleUncaughtException swallows exceptions that are caused by the exception handler code,FLINK-32029,13535355,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,08/May/23 09:18,10/May/23 16:15,13/Jul/23 08:29,10/May/23 16:15,1.16.1,1.17.0,1.18.0,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,I ran into an issue in FLINK-31773 where the passed exception handler relies on the {{leaderContender}} field of the {{DefaultLeaderElectionService}}. This field can become {{null}} with the changes of FLINK-31773. But the {{null}} check was missed in the error handling code. This bug wasn't exposed because {{FutureUtils.handleUncaughtException(..)}} expects the passed error handler callback to be bug-free.,,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 09:58:32 UTC 2023,,,,,,,,,,"0|z1hr3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/23 09:58;mapohl;master: 88b3432a2845952543a8396aeb8e2cddab77b509
1.17: 1d85f6ffc00789e0239f8ed7164af03b81e8dfae
1.16: 55c748647fdf68050312c08bfaba574a423a8464;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch jobs could hang at shuffle phase when max parallelism is really large,FLINK-32027,13535320,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,yunta,yunta,08/May/23 03:17,10/May/23 03:00,13/Jul/23 08:29,10/May/23 03:00,1.16.0,1.16.1,1.17.0,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Network,,,,0,pull-request-available,,,"In batch stream mode with adaptive batch schedule mode, If we set the max parallelism large as 32768 (pipeline.max-parallelism), the job could hang at the shuffle phase:

It would hang for a long time and show ""No bytes sent"":
 !image-2023-05-08-11-12-58-361.png! 

After some time to debug, we can see the downstream operator did not receive the end-of-partition event.
",,hackergin,JunRuiLi,tanyuxin,Thesharing,wanglijie,wangm92,Weijie Guo,yunta,zhuzh,,,,,,,,,,,,,,,,,,,,,FLINK-28519,,,,,,,,,,,"08/May/23 03:13;yunta;image-2023-05-08-11-12-58-361.png;https://issues.apache.org/jira/secure/attachment/13057871/image-2023-05-08-11-12-58-361.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 03:00:27 UTC 2023,,,,,,,,,,"0|z1hqw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 03:18;yunta;[~Weijie Guo] [~zhuzh] Please take a look at this issue.;;;","08/May/23 11:52;Weijie Guo;Thanks [~yunta] for reporting this! 

Through some analysis, [~kevin.cyj] and I found that this is indeed a bug caused by multiple threads moving the {{FileChannel}} of index file simultaneously. I will fix this asap.;;;","09/May/23 04:47;Weijie Guo;Marked this as blocker as this has a probability of causing incorrect job results and will not report an error.;;;","10/May/23 03:00;Weijie Guo;master(1.18) via 63443aec09ece8596321328273c1e431e5029c4d.
release-1.17 via 8e5fb18ae5a80c4d0620979a944b017b203cdeac.
release-1.16 via c5a883d3976fc8367eba446790088ff46e59ab79.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
execution.buffer-timeout cannot be set to -1 ms,FLINK-32023,13535251,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiangang,Jiangang,Jiangang,06/May/23 07:13,06/Jun/23 02:20,13/Jul/23 08:29,06/Jun/23 02:20,1.16.1,1.17.0,1.18.0,,,,1.16.3,1.17.2,1.18.0,,,,,,API / DataStream,,,,0,pull-request-available,,,"The desc for execution.buffer-timeout is as following:
{code:java}
public static final ConfigOption<Duration> BUFFER_TIMEOUT =
        ConfigOptions.key(""execution.buffer-timeout"")
                .durationType()
                .defaultValue(Duration.ofMillis(100))
                .withDescription(
                        Description.builder()
                                .text(
                                        ""The maximum time frequency (milliseconds) for the flushing of the output buffers. By default ""
                                                + ""the output buffers flush frequently to provide low latency and to aid smooth developer ""
                                                + ""experience. Setting the parameter can result in three logical modes:"")
                                .list(
                                        text(
                                                ""A positive value triggers flushing periodically by that interval""),
                                        text(
                                                FLUSH_AFTER_EVERY_RECORD
                                                        + "" triggers flushing after every record thus minimizing latency""),
                                        text(
                                                DISABLED_NETWORK_BUFFER_TIMEOUT
                                                        + "" ms triggers flushing only when the output buffer is full thus maximizing ""
                                                        + ""throughput""))
                                .build()); {code}
When we set execution.buffer-timeout to -1 ms, the following error is reported:
{code:java}
Caused by: java.lang.IllegalArgumentException: Could not parse value '-1 ms' for key 'execution.buffer-timeout'.
    at org.apache.flink.configuration.Configuration.getOptional(Configuration.java:856)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.configure(StreamExecutionEnvironment.java:822)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.<init>(StreamExecutionEnvironment.java:224)
    at org.apache.flink.streaming.api.environment.StreamContextEnvironment.<init>(StreamContextEnvironment.java:51)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.createStreamExecutionEnvironment(StreamExecutionEnvironment.java:1996)
    at java.util.Optional.orElseGet(Optional.java:267)
    at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.getExecutionEnvironment(StreamExecutionEnvironment.java:1986)
    at com.kuaishou.flink.examples.api.WordCount.main(WordCount.java:27)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:327)
    ... 11 more
Caused by: java.lang.NumberFormatException: text does not start with a number
    at org.apache.flink.util.TimeUtils.parseDuration(TimeUtils.java:78)
    at org.apache.flink.configuration.Configuration.convertToDuration(Configuration.java:1058)
    at org.apache.flink.configuration.Configuration.convertValue(Configuration.java:996)
    at org.apache.flink.configuration.Configuration.lambda$getOptional$2(Configuration.java:853)
    at java.util.Optional.map(Optional.java:215)
    at org.apache.flink.configuration.Configuration.getOptional(Configuration.java:853)
    ... 23 more {code}
The reason is that the value for Duration can not be negative. We should change the behavior or support to trigger flushing only when the output buffer is full.",,fanrui,huwh,Jiangang,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-14787,,FLINK-32250,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 10:16:06 UTC 2023,,,,,,,,,,"0|z1hqgo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/May/23 02:39;wanglijie;[~Jiangang] Thanks for reporting this. Would you like to prepare a fix ?  I think we should make the {{TimeUtils#parseDuration}} support parsing negative duration.;;;","08/May/23 02:46;fanrui;Hi [~Jiangang] , thanks for your report. It's really a bug. Would you like to fix it?

From the perspective of Duration, that the value for Duration can not be negative is reasonable. So I prefer choose a special time as the flag of the disable buffer timeout mechanism. WDYT?

cc [~dwysakowicz] , maybe you are also interested in this bug. :);;;","08/May/23 02:56;fanrui;Thanks [~wanglijie] 's feedback.

Sorry, after my double check, I see the `java.time.Duration` supports negative value, and it isn't supported in flink side. So it's reasonable making the {{TimeUtils#parseDuration}} support parsing negative duration.

 ;;;","08/May/23 06:20;Jiangang;[~wanglijie] [~fanrui] Thanks for the reply. I would to fix it.;;;","08/May/23 06:22;fanrui;Thanks [~Jiangang] , I have assigned this JIRA to you.:);;;","29/May/23 06:08;fanrui;<master:1.18> 522ff833ad7e3b3c80f4c1ba326cb05fdc4d6a3c

<1.17> 67acbe9c8814dd56262053d443ae2712e03d1cb0

<1.16> 71482596056feaca04389496373fde006ca26803;;;","31/May/23 10:16;fanrui;This Jira isn't closed due to waiting for backport it to 1.16 and 1.17.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Many builds of benchmark have been interrupted since 20230428,FLINK-32018,13535235,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,Yanfei Lei,Yanfei Lei,06/May/23 02:13,16/May/23 08:40,13/Jul/23 08:29,16/May/23 08:40,1.18.0,,,,,,,,,,,,,,Benchmarks,,,,0,pull-request-available,,,"Since 2023.04.28, flink-statebackend-benchmarks-java8, flink-statebackend-benchmarks-java11, flink-master-benchmarks-java8 and  flink-master-benchmarks-java11 all failed.

 

[http://codespeed.dak8s.net:8080/job/flink-statebackend-benchmarks-java8/]

[http://codespeed.dak8s.net:8080/job/flink-statebackend-benchmarks-java11/]

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java8/] 

[http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-java11/] ",,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31925,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat May 06 05:33:14 UTC 2023,,,,,,,,,,"0|z1hqd4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/23 05:33;Yanfei Lei;I suspect it has something to do with the version of JMH, before FLINK-31925 , the benchmark can run normally. [http://codespeed.dak8s.net:8080/job/flink-benchmark-request/219/]

[~MartijnVisser] could you help take a look?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Operator failed to rollback due to missing HA metadata,FLINK-32012,13535161,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,nfraison.datadog,nfraison.datadog,05/May/23 12:07,27/Jun/23 15:39,13/Jul/23 08:29,27/Jun/23 15:39,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.6.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"The operator has well detected that the job was failing and initiate the rollback but this rollback has failed due to `Rollback is not possible due to missing HA metadata`

We are relying on saevpoint upgrade mode and zookeeper HA.

The operator is performing a set of action to also delete this HA data in savepoint upgrade mode:
 * [flink-kubernetes-operator/AbstractFlinkService.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L346] : Suspend job with savepoint and deleteClusterDeployment

 * [flink-kubernetes-operator/StandaloneFlinkService.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/StandaloneFlinkService.java#L158] : Remove JM + TM deployment and delete HA data

 * [flink-kubernetes-operator/AbstractFlinkService.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L1008] : Wait cluster shutdown and delete zookeeper HA data

 * [flink-kubernetes-operator/FlinkUtils.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java#L155] : Remove all child znode

Then when running rollback the operator is looking for HA data even if we rely on sevepoint upgrade mode:
 * [flink-kubernetes-operator/AbstractFlinkResourceReconciler.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L164] Perform reconcile of rollback if it should rollback

 * [flink-kubernetes-operator/AbstractFlinkResourceReconciler.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L387] Rollback failed as HA data is not available

 * [flink-kubernetes-operator/FlinkUtils.java at main · apache/flink-kubernetes-operator|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/utils/FlinkUtils.java#L220] Check if some child znodes are available

For both step the pattern looks to be the same for kubernetes HA so it doesn't looks to be linked to a bug with zookeeper.

 

From https://issues.apache.org/jira/browse/FLINK-30305 it looks to be expected that the HA data has been deleted (as it is also performed by flink when relying on savepoint upgrade mode).

Still the use case seems to differ from https://issues.apache.org/jira/browse/FLINK-30305 as the operator is aware of the failure and treat a specific rollback event.

So I'm wondering why we enforce such a check when performing rollback if we rely on savepoint upgrade mode. Would it be fine to not rely on the HA data and rollback from the last savepoint (the one we used in the deployment step)?",,gyfora,nfraison.datadog,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 15:39:11 UTC 2023,,,,,,,,,,"0|z1hpwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 12:11;nfraison.datadog;[~gyfora] do you think that we can implement this specific case for rollback or do you foresee some potential issues with this?;;;","06/May/23 07:54;gyfora;Hey!
This is an unfortunate limitation of the rollback mechanism currently that it strictly relies on the HA metadata at the time of the rollback. This works in many cases if the upgraded cluster at least starts up (in that case the HA metadata is created with the initial restore savepoint path).

We need to overhaul the rollback mechansim based on how the upgrade works now where we already fixed these issues.;;;","16/May/23 14:55;nfraison.datadog;I'd like to have your feeling on the first approach I'd like to do to merge upgrade/rollback.

 

Currently to do a rollback we
 * First check if spec change
 * If not check if we should rollback
 * If yes initiate rollback where we set status to ROLLING_BACK and wait for next reconcile loop
 * Next reconcile loop will do same checks
 * As it should rollback and state already up to date it will rollback relying on last stable spec but doesn't update the spec

The idea would be to really rollback the spec spec to rely for both mechanism on reconcileSpecChange:
 * Still keep the check if spec change
 * Then check if should rollback
 * In the initiate rollback set status to ROLLING_BACK and restore last stable spec and wait for next reconcile loop
 * Next reconcile loop will check if spec change
 * It will be the case so will run reconcileSpecChange but with status ROLLING_BACK so could then applied specificities of rollback

Would also simplify the operation that need to reapply the working spec like [resubmitJob|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java#L309]

Still I'm wondering if we will then be really able to simplify things as from your [comment|https://github.com/apache/flink-kubernetes-operator/pull/590#discussion_r1189847280] the rollback doesn't seem to be aligned to the upgrade?

For ex for savepoint upgrade mode:
 * upgrade
 ** delete ha metadata
 ** restore from savepoint
 * rollback
 ** keep ha metadata if exist
 ** restore from it if exist
 ** restore from savepoint if not exist and JM pod never started;;;","16/May/23 15:14;gyfora;I agree [~nfraison.datadog] it should be possible to unify upgrades and rollbacks. After checking if we should rollback we could temporarily reset the spec to the latest stable and execute the upgrade as if this was the spec sent by the user, also we should modify the upgradeMode to last-state. 

I think the upgrade logic already covers the necessary HA meta checks so this should be good in theory :) ;;;","23/May/23 13:35;nfraison.datadog;[~gyfora], I started an implementation which do not modify the upgradeMode to last-state as the [restoreJob|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractJobReconciler.java#L143%5D] for LAST_STATE upgrade mode will enforce the requirement for HA metadata which is not what we want when relying on SAVEPOINT.

Also when restoring last stable spec there is a case where the UpgradeMode is set to STATELESS in this spec even if the chosen mode is SAVEPOINT ([updateStatusBeforeFirstDeployment|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/reconciler/deployment/AbstractFlinkResourceReconciler.java#L193])

In order to avoid restoring this bad state leading to rollback not taking in account the savepoint I enforce the upgrade mode of the restored spec to be the one currently set on the job.

But I'm wondering why we have decided to not persist the really used upgrade mode in the last stable spec for first deployment?

Here is the diff of my [WIP|https://github.com/apache/flink-kubernetes-operator/compare/main...ashangit:flink-kubernetes-operator:nfraison/FLINK-32012?expand=1] if approach is not clear (not to review...);;;","23/May/23 14:02;gyfora;[~nfraison.datadog] , I don't think it's possible to use ""SAVEPOINT"" upgrade mode when rolling back. That would imply that we can take a savepoint which means that the job would be running.

Also regarding your comment about what to set in the status for upgradeMode, we have to set what we actually ended up using, there is some logic that relies on this. The first time a job is started from an empty state we record stateless, if you started with initialSavepointPath then it would be savepoint. All these are key pieces of logic that allows us to upgrade safely without accidentally losing state.;;;","23/May/23 15:02;nfraison.datadog;But the whole point of this request was the fact that when JobManager failed to start the HA metadata was not available during RollBack while the savepoint taken for the upgrade is available.

So relying on SAVEPOINT for rollback will ensure that Flink is aware of the availability of a savepoint.

From my understanding of [tryRestoreExecutionGraphFromSavepoint|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultExecutionGraphFactory.java#L198]  Flink will rely on the saveoint if no HA metadata exist otherwise it will load the checkpoint

 

Forgot to mention it but indeed when calling [cancelJob|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/service/AbstractFlinkService.java#L353%5D] it fallback to LAST_STATE in order to ensure no savepoint is created and HA metadata is not deleted

 ;;;","23/May/23 15:06;gyfora;Yes you are right. But this is already handled in the getAvailableUpgradeModes logic during a normal upgrade. If your job failed to start after a savepoint upgrade you can send in a new spec and it will be upgraded using the previous savepoint.;;;","23/May/23 15:26;gyfora;I think you are on the right track. But we should ensure that in the lastReconciledSpec we record the upgradeMode that was used during the last deployment / rollback otherwise subsequent upgrades can be problematic.

If we used a savepoint we should record savepoint if HA metadata was used it should be last-state.;;;","27/Jun/23 15:39;gyfora;merged to main d346ca9c437d20042ed8f4a1954f0f0ed438b3ae;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesLeaderRetrievalDriver always waits for lease update to resolve leadership,FLINK-32010,13535133,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,05/May/23 09:50,06/May/23 06:20,13/Jul/23 08:29,06/May/23 06:20,1.16.1,1.17.0,1.18.0,,,,1.16.2,1.17.1,1.18.0,,,,,,Deployment / Kubernetes,Runtime / Coordination,,,0,pull-request-available,,,"The k8s-based leader retrieval is based on ConfigMap watching. The config map lifecycle (from the consumer point of view) is handled as a series of events with the following types:
 * ADDED -> the first time the consumer has seen the CM
 * UPDATED -> any further changes to the CM
 * DELETED -> ... you get the idea

The implementation assumes that ElectionDriver (the one that creates the CM) and ElectionRetriver are started simultaneously and therefore ignore the ADDED events because the CM is always created as empty and is updated with the leadership information later on.

This assumption is incorrect in the following cases (I might be missing some, but that's not important, the goal is to illustrate the problem):
 * TM joining the cluster later when the leaders are established to discover RM / JM
 * RM tries to discover JM when 
MultipleComponentLeaderElectionDriver is used

This, for example, leads to higher job submission latencies that could be unnecessarily held back for up to the lease retry period [1].

[1] Configured by _high-availability.kubernetes.leader-election.retry-period_",,dmvk,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22054,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 19:03:13 UTC 2023,,,,,,,,,,"0|z1hpqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 19:03;dmvk;master: 026d7ccfe1d6f4cfa26c9038dd05403c889d2e0d

release-1.16: 6aa84630a39fbe33209487fbeac412dc98439b46

release-1.17: 2799038b964e88129545bf4e6a5128c03e3d2f2b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Protobuf format cannot work with FileSystem Connector,FLINK-32008,13535096,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rskraba,xuannan,xuannan,05/May/23 06:22,17/Jun/23 14:55,13/Jul/23 08:29,17/Jun/23 14:55,1.17.0,,,,,,1.18.0,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,"The protobuf format throws exception when working with Map data type. I uploaded a example project to reproduce the problem.

 
{code:java}
Caused by: java.lang.RuntimeException: One or more fetchers have encountered exception
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:261)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169)
    at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:131)
    at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:417)
    at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:165)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:114)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
Caused by: java.io.IOException: Failed to deserialize PB object.
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:75)
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:42)
    at org.apache.flink.api.common.serialization.DeserializationSchema.deserialize(DeserializationSchema.java:82)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.readRecord(DeserializationSchemaAdapter.java:197)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$LineBytesInputFormat.nextRecord(DeserializationSchemaAdapter.java:210)
    at org.apache.flink.connector.file.table.DeserializationSchemaAdapter$Reader.readBatch(DeserializationSchemaAdapter.java:124)
    at org.apache.flink.connector.file.src.util.RecordMapperWrapperRecordIterator$1.readBatch(RecordMapperWrapperRecordIterator.java:82)
    at org.apache.flink.connector.file.src.impl.FileSourceSplitReader.fetch(FileSourceSplitReader.java:67)
    at org.apache.flink.connector.base.source.reader.fetcher.FetchTask.run(FetchTask.java:58)
    at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:162)
    ... 6 more
Caused by: java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.formats.protobuf.deserialize.ProtoToRowConverter.convertProtoBinaryToRow(ProtoToRowConverter.java:129)
    at org.apache.flink.formats.protobuf.deserialize.PbRowDataDeserializationSchema.deserialize(PbRowDataDeserializationSchema.java:70)
    ... 15 more
Caused by: com.google.protobuf.InvalidProtocolBufferException: While parsing a protocol message, the input ended unexpectedly in the middle of a field.  This could mean either that the input has been truncated or that an embedded message misreported its own length.
    at com.google.protobuf.InvalidProtocolBufferException.truncatedMessage(InvalidProtocolBufferException.java:115)
    at com.google.protobuf.CodedInputStream$ArrayDecoder.pushLimit(CodedInputStream.java:1196)
    at com.google.protobuf.CodedInputStream$ArrayDecoder.readMessage(CodedInputStream.java:887)
    at com.example.proto.MapMessage.<init>(MapMessage.java:64)
    at com.example.proto.MapMessage.<init>(MapMessage.java:9)
    at com.example.proto.MapMessage$1.parsePartialFrom(MapMessage.java:756)
    at com.example.proto.MapMessage$1.parsePartialFrom(MapMessage.java:750)
    at com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:158)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:191)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:203)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:208)
    at com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:48)
    at com.example.proto.MapMessage.parseFrom(MapMessage.java:320)
    ... 21 more {code}",,libenchao,maosuhan,rskraba,xuannan,,,,,,,,,,,,,,,,,,,,,FLINK-31944,,,,,,,,,,,,,,,,"05/May/23 06:17;xuannan;flink-protobuf-example.zip;https://issues.apache.org/jira/secure/attachment/13057850/flink-protobuf-example.zip",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jun 17 14:55:45 UTC 2023,,,,,,,,,,"0|z1hpi8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/May/23 07:13;libenchao;CC [~maosuhan] ;;;","23/May/23 04:29;maosuhan;I think the error is related to corrupt data.
 ;;;","25/May/23 16:53;rskraba;Hello, thanks for the example project!  That's so helpful to reproduce and debug.

The *current* file strategy for protobuf in Flink is to write one record serialized as binary per line, adding a *{{0x0a}}*.  Your example message is serialized as:

{code}
12 06 0a 01 61 12 01 62 0a
{code}

The first *{{0a}}* is the protobuf encoding for the key field in the map.  The last *{{0a}}* is a new line (which probably shouldn't be there).

When reading, from a file, splits are calculated and assigned to tasks using the *{{0a}}* as a delimiter, which is very, very likely to fail and a fault in the protobuf file implementation of Flink.

I'm guessing this isn't limited to maps, we can expect this delimiter byte to occur many different ways in the protobuf binary.

If this hasn't been addressed, it's probably because it's pretty rare to store protobuf messages in a file container (as opposed to in a single message packet, or a table cell).  Do you have a good use case that we can use to guide what we expect Flink to do with protobuf files?

For info, nothing in the Protobuf encoding that can be used to [distinguish the start or end|https://protobuf.dev/programming-guides/techniques/#streaming] of a message.  If we want to store multiple messages in the same container (file or any sequence of bytes), we have to manage the indices ourselves  The above link recommends writing the message size followed by the binary (in Java, this is using {{writeDelimitedTo}}/{{parseDelimitedFrom}} instead of {{writeTo}}/{{parseFrom}}, for example).;;;","25/May/23 17:03;rskraba;Oh, just taking a quick look – protobuf isn't supported by the filesystem connector in the [Flink|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/table/filesystem/] docs.  The real bug here might be that filesystem + protobuf doesn't fail immediately as an unsupported option!;;;","26/May/23 02:29;libenchao;[~rskraba] Thanks for the digging. I agree that currently FileSystem's fallback (de)serializer does not fit for all formats, this should be one of them. It might be worth to add support to implement a {{BulkReaderFormatFactory}} for protobuf format.;;;","26/May/23 09:56;rskraba;It's probably worth checking in with the community to see what we expect of a bulk format, or if it would be an interesting thing to add!  [I'll ask on the mailing list|https://lists.apache.org/thread/z9tqdqrhj12c17wqsdbm5fhzonqq5kp0].

I took a quick look but didn't see any related JIRA (outside of FLINK-12149, which proposes using the protobuf API to interact with parquet files).  Can a committer change the title of this JIRA to better reflect the issue?  Something like ""Protobuf format on filesystem is faulty"";;;","26/May/23 10:55;libenchao;I've changed the title.;;;","16/Jun/23 17:53;rskraba;I've created a PR that prevents the protobuf format from being used to write bulk files.  (I used the error message {{""The 'protobuf' format is not supported for the 'filesystem' connector.""}}, but this could be refined.)

In my experience and with the responses on the mailing list, I don't think that create a custom BulkReaderFormatFactory/BulkWriterFormatFactory would be used.  At the minimum, we should fail quickly (instead of generating unreadable files silently).

If there's a future need to create our own protobuf-oriented file format (for temporary storage?) or if someone makes a protobuf file container in the future, we can probably revisit this strategy!  In the meantime, I'd probably recommend using something like parquet (with the protobuf-message-oriented API) to persist bulk records in a file.  What do you think?;;;","17/Jun/23 14:47;libenchao;[~rskraba] Thanks, I agree with your proposal, I'll review it.;;;","17/Jun/23 14:55;libenchao;Fixed via https://github.com/apache/flink/commit/7d4ee28e85aad4abc8ad126c4d953d0e921ea07e (master);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release 3.0.0-1.16 and 1.16.1 doesn't work with OAuth2,FLINK-32003,13535025,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tison,nlu90,nlu90,04/May/23 16:25,24/May/23 08:06,13/Jul/23 08:29,24/May/23 08:06,pulsar-3.0.0,,,,,,pulsar-3.0.1,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"The release for 3.0.0-1.16 and 1.16.1 depends on Pulsar client version 2.10.1.

There is an issue using OAuth2 with this client version which results in the following error.


{code:java}
Exception in thread ""main"" java.lang.RuntimeException: org.apache.pulsar.client.admin.PulsarAdminException: java.util.concurrent.CompletionException: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at me.nlu.pulsar.PulsarAdminTester.main(PulsarAdminTester.java:56)
Caused by: org.apache.pulsar.client.admin.PulsarAdminException: java.util.concurrent.CompletionException: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at org.apache.pulsar.client.admin.internal.BaseResource.getApiException(BaseResource.java:251)
	at org.apache.pulsar.client.admin.internal.TopicsImpl$1.failed(TopicsImpl.java:187)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.JerseyInvocation$1.failed(JerseyInvocation.java:882)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.processFailure(ClientRuntime.java:247)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.processFailure(ClientRuntime.java:242)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime.access$100(ClientRuntime.java:62)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime$2.lambda$failure$1(ClientRuntime.java:178)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors$1.call(Errors.java:248)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors$1.call(Errors.java:244)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:292)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:274)
	at org.apache.pulsar.shade.org.glassfish.jersey.internal.Errors.process(Errors.java:244)
	at org.apache.pulsar.shade.org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:288)
	at org.apache.pulsar.shade.org.glassfish.jersey.client.ClientRuntime$2.failure(ClientRuntime.java:178)
	at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$apply$1(AsyncHttpConnector.java:218)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$retryOperation$4(AsyncHttpConnector.java:277)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.NettyResponseFuture.abort(NettyResponseFuture.java:273)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.NettyConnectListener.onFailure(NettyConnectListener.java:181)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.NettyConnectListener$1.onFailure(NettyConnectListener.java:151)
	at org.apache.pulsar.shade.org.asynchttpclient.netty.SimpleFutureListener.operationComplete(SimpleFutureListener.java:26)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:578)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListeners0(DefaultPromise.java:571)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:550)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:491)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:616)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:609)
	at org.apache.pulsar.shade.io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:117)
	at org.apache.pulsar.shade.io.netty.handler.ssl.SslHandler.setHandshakeFailure(SslHandler.java:1882)
	at org.apache.pulsar.shade.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1067)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
	at org.apache.pulsar.shade.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
	at org.apache.pulsar.shade.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
	at org.apache.pulsar.shade.io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:813)
	at org.apache.pulsar.shade.io.netty.util.concurrent.AbstractEventExecutor.runTask$$$capture(AbstractEventExecutor.java:174)
	at org.apache.pulsar.shade.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java)
	at org.apache.pulsar.shade.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
	at org.apache.pulsar.shade.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
	at org.apache.pulsar.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at org.apache.pulsar.shade.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:995)
	at org.apache.pulsar.shade.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.apache.pulsar.shade.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.util.concurrent.CompletionException: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$OrApply.tryFire(CompletableFuture.java:1503)
	... 37 more
Caused by: org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector$RetryException: Could not complete the operation. Number of retries has been exhausted. Failed reason: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at org.apache.pulsar.client.admin.internal.http.AsyncHttpConnector.lambda$retryOperation$4(AsyncHttpConnector.java:279)
	... 34 more
Caused by: java.net.ConnectException: https://func-test-31a67160-533f-4a5f-81a8-30b6221f34a9.gcp-shared-gcp-usce1-martin.streamnative.g.snio.cloud:443
	at org.apache.pulsar.shade.org.asynchttpclient.netty.channel.NettyConnectListener.onFailure(NettyConnectListener.java:179)
	... 28 more
Caused by: java.nio.channels.ClosedChannelException
	at org.apache.pulsar.shade.io.netty.handler.ssl.SslHandler.channelInactive(SslHandler.java:1064)
	... 17 more{code}

We need to upgrade the pulsar-client-all version to at least 2.10.2 (for which I have verified the client auth issue is fixed) and publish new releases for 1.16 and 3.0.0-1.16.",,leonard,nlu90,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 24 08:06:27 UTC 2023,,,,,,,,,,"0|z1hp2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/May/23 06:07;tison;Thanks for reporting this issue [~nlu90]! It's reasonable to upgrade patch versions as long as it holds the semantic.;;;","24/May/23 07:55;leonard;[~tison] Could you also open a PR for branch main?;;;","24/May/23 07:58;tison;The main branch already has more later version. It's not affected by this issue.;;;","24/May/23 08:06;leonard;Thanks [~tison]  for the clarification.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chaining operators with different max parallelism prevents rescaling,FLINK-31996,13534928,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,04/May/23 08:22,06/Jul/23 13:43,13/Jul/23 08:29,06/Jul/23 13:43,,,,,,,1.18.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"We might chain operators with different max parallelism together if they are set to have the same parallelism initially.

When we decide to rescale the JobGraph vertices (using AdaptiveScheduler), we're gapped by the lowest maxParallelism of the operator chain. This is especially visible with things like CollectSink, TwoPhaseCommitSink, CDC, and a GlobalCommiter with maxParallelism set to 1.

 

An obvious solution would be to prevent the chaining of operators with different maxParallelism, but we need to double-check this doesn't introduce a breaking change.",,dmvk,gyfora,martijnvisser,mxm,pgaref,,,,,,,,,,,,,,,,,,,,FLINK-32022,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 13:43:23 UTC 2023,,,,,,,,,,"0|z1hogw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/23 08:28;gyfora;In theory user can also break the operator chain by redeploying the job with a different parallelism setting and that should be compatible.
So I would expect this to not be a breaking change.;;;","04/May/23 08:44;dmvk;The main question is whether we need an opt-out feature flag to allow people to revert to the old behavior.;;;","04/May/23 08:48;chesnay;??So I would expect this to not be a breaking change.??

We need an opt-in flag because this change may break existing chains, which can result in certain types being serialized that previously weren't. This can affect performance  and savepoint compatibility.
E.g., if you have a job that is one big chain then your super special custom type doesn't go through Kryo. Users may also rely on the same object being passed through the job.;;;","04/May/23 08:50;gyfora;Fair point [~chesnay] , I think you are completely right this cannot be the new default.;;;","06/Jul/23 13:43;chesnay;master: 58a849bcc497e17bf575e49946f106030f3e1a1a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DirectExecutorService doesn't follow the ExecutorService contract throwing a RejectedExecutionException in case it's already shut down,FLINK-31995,13534922,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,04/May/23 07:45,30/May/23 13:31,13/Jul/23 08:29,04/May/23 15:31,1.16.1,1.17.0,1.18.0,,,,1.18.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"We experienced an issue where we tested behavior using the {{DirectExecutorService}} with the {{ExecutorService}} being shutdown already. The tests succeeded. The production code failed, though, because we used a singleThreadExecutor for which the task execution failed after the executor was stopped. We should add this behavior to the {{DirectExecutorService}} as well to make the unit tests be closer to the production code.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32204,,,,FLINK-29813,,,FLINK-31838,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 04 15:31:46 UTC 2023,,,,,,,,,,"0|z1hofk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/23 15:31;mapohl;master: 8d430f51d77cf4f0d3291da6a7333f1aa9a87d22;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update copyright in NOTICE files of flink-shaded ,FLINK-31994,13534906,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,leonard,leonard,04/May/23 04:34,04/May/23 06:45,13/Jul/23 08:29,04/May/23 06:45,1.17.0,,,,,,1.18.0,,,,,,,,BuildSystem / Shaded,,,,0,,,,"The copyright of all flink-shaded dependency NOTICE files are still 2021, we can use tools/update_notice_year.sh to update it.",,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 04 06:45:37 UTC 2023,,,,,,,,,,"0|z1hoc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/May/23 06:45;leonard;Fixed in https://github.com/apache/flink-shaded/commit/f12239b5d5ec865713c00087fa379c3d8305ac1e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobManager crashes after KubernetesClientException exception with FatalExitExceptionHandler,FLINK-31974,13534509,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,sergiosp,sergiosp,28/Apr/23 22:18,01/Jun/23 18:12,13/Jul/23 08:29,01/Jun/23 18:12,1.17.0,1.18.0,,,,,1.17.2,1.18.0,,,,,,,Deployment / Kubernetes,,,,0,pull-request-available,,,"When resource quota limit is reached JobManager will throw

 

org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.

 

In {*}1.16.1 , this is handled gracefully{*}:
{code}
2023-04-28 22:07:24,631 WARN  org.apache.flink.runtime.resourcemanager.active.ActiveResourceManager [] - Failed requesting worker with resource spec WorkerResourceSpec \{cpuCores=1.0, taskHeapSize=25.600mb (26843542 bytes), taskOffHeapSize=0 bytes, networkMemSize=64.000mb (67108864 bytes), managedMemSize=230.400mb (241591914 bytes), numSlots=4}, current pending count: 0
java.util.concurrent.CompletionException: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-138"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
        at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-138"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.16.1.jar:1.16.1]
        at io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.16.1.jar:1.16.1]
        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.16.1.jar:1.16.1]
        ... 4 more
{code}

But , {*}in Flink 1.17.0 , Job Manager crashes{*}:
{code}
2023-04-28 20:50:50,534 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-15' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
        at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.17.0.jar:1.17.0]
        ... 4 more
{code}",,begginghard,gyfora,huwh,mapohl,mbalassi,mxm,sergiosp,tanyuxin,thw,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,FLINK-30908,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 01 13:21:59 UTC 2023,,,,,,,,,,"0|z1hlwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/May/23 15:04;mapohl;Thanks for reporting. This should be caused by the changes that were introduced with FLINK-30908. Could you share the logs that are printed before the fatal error is reported? There should be additional logs related to the error.;;;","03/May/23 15:17;mapohl;I'm still wondering what the desired behavior in that case is. The k8s cluster doesn't provide the resources so that the Flink cluster would be able to handle the parallelism of the submitted job. In my opinion, it feels like the fatal error is correct. [~xtsong] what's your take on that one?;;;","04/May/23 02:03;xtsong;Thanks [~sergiosp] for reporting, and thanks [~mapohl] for looking into this.

bq. The k8s cluster doesn't provide the resources so that the Flink cluster would be able to handle the parallelism of the submitted job.

This is not always true. For streaming workloads in reactive mode, it is expected that not all requested resources can be obtained, and as long as the minimum resource requirements are fulfilled the job can be executed. Also for batch workloads, ideally a job can be executed with a single slot, because tasks don't have to be executed at the same time.

Moreover, there's a timeout at the JobMaster side that will fail the job if resources cannot be fulfilled within a certain time, with the execution mode and minimum resource requirements taken into consideration.

In most cases, the phenomenon for not obtaining a resource is that Flink can create the meta of desired pod at K8s API Server and will keep waiting for the K8s cluster to schedule and bring up the pod. However, in this case it throws an exception, which was not covered by the current implementation.

I think we may identify the specific error and not treat it as fatal error. Instead, we can pass this information to JobMaster via {{JobMasterGateway#notifyNotEnoughResourcesAvailable}} and rely on JobMaster to decide whether should fail the job.

WDYT?;;;","04/May/23 04:30;Weijie Guo;Thanks Xintong for the analysis and proposal, It makes sense to me for relying JobMaster to decide whether to fail the job or not. IMO, The exception mentioned in the ticket should not arbitrarily cause JM to crash, especially for batch workload. If this is reasonable, I'm willing to fix it.;;;","04/May/23 07:06;mapohl;Sounds good to me, too.

Just for me to understand: With ""we can pass this information to JobMaster"" you mean letting the SlotManager implementations deal with the timeout you mentioned that occurs when we fail to create a new worker. That way, we only need to identify the {{KubernetesClientException}} in {{ResourceManagerDriver#requestResource}} and print a warning to make the user aware. I'm asking because I struggled to find a code path between the JobMaster and the ResourceManager that would enable us to inform the JobMaster about this specific error which is how I understood ""passing the information"" in the first place.;;;","04/May/23 07:30;xtsong;[~mapohl],
There're two paths for JobMaster to handle the situation that resources are not obtained.
- With the timeout
- {{JobMasterGateway#notifyNotEnoughResourcesAvailable}}

The second path is for the job to fail earlier rather than waiting for the timeout, if SlotManager knows that the resource cannot be obtained and it makes no sense to wait, e.g., in a standalone cluster. I think the question is, after we identify the specific error that suggest a quota exceeding, how do we pass this information all the way from {{KubernetesResourceManagerDriver}} to {{ResourceManager}} and to {{SlotManager}}. I think it shouldn't be complex to complete the missing part of the path.;;;","04/May/23 08:02;gyfora;Somewhat of a side comment:
I think in native kubernetes integration case we should basically never give these fatal exceptions. Even if there is a missing serviceaccount/permission/timeout we should keep retrying because more often than not these are actually temporary (even if they need some time to be resolved by the operating platform team).

A job fatal error requires a complete redeployment which is not what most users want. For standalone this may be different but there we will get different errors.;;;","04/May/23 08:35;xtsong;Not sure about never giving fatal exceptions. I personally would lean towards a whitelist approach, where Flink only handles a certain set of errors that are known to be non-fatal, and by default fail for whatever errors that it doesn't recognize and doesn't know how to handle. My concern for keeping retrying by default is that, when there's a large Kubernetes cluster with lot's of applications, this approach would exacerbate the burden on the Kubernetes API server and sometimes make the temporary outage even harder to recovery. I've seen that for many times in production.;;;","04/May/23 08:47;gyfora;[~xtsong] what errors would you consider actually fatal in Kubernetes world?

From my perspective I would like to treat almost every kubernetes error non fatal. At least this should be configurable because as you say some may prefer shuttind down the flink jobs (fatal) and some (we for instance) would like to retry everything based on the restart strategy.;;;","04/May/23 09:22;xtsong;[~gyfora],

IMO, errors that Flink cannot recover from by itself should be considered fatal. E.g., for a permission issue, if not provided the details that it's due to reaching the quota limit, I don't see how Flink can fix that by itself. I would be fine with Flink trying to improve how it handles various errors based on understanding of what the errors mean. However, I'd be hesitate about to simply retry for arbitrary errors.

bq. because more often than not these are actually temporary

TBH, my observations are to the contrary. Might because of differences between our production environments.

bq. At least this should be configurable

Normally, I'd avoid introducing new configuration unless absolutely necessary. In this case, if you believe it worths the complexity not to trigger a re-deployment upon arbitrary errors, I'd be fine with making it configurable.

I'm still trying to understand why re-deployment upon API server outage is a big deal. Is it because the outage happens a lot in your production environment?

bq. retry everything based on the restart strategy

I believe restart strategy only controls behaviors upon job failures. An error thrown from the interactions between the resource manager and the kubernetes cluster would not invoke the restart strategy. Unless you mean waiting for the resource allocation timeout.;;;","04/May/23 10:06;gyfora;Flink treats only very few errors fatal. IO errors, connector (source/sink ) errors etc all cause job restarts and in many cases ""Flink cannot recover from by itself"". You actually expect the error to be temporary and hopefully not get it after the restart. So I think it would be generally inconsistent with the current error handling behaviour if resource manager errors would simply let the job die fatally and not retry in the same way.

So I am mostly looking at this from the user perspective. Flink jobs/clusters should be resilient and keep retrying in case of errors and should not give up especially for streaming workloads. This is how it works now and this what most users expect I think.;;;","04/May/23 10:08;gyfora;cc [~mbalassi] [~mxm] [~thw] ;;;","04/May/23 11:17;mbalassi;In the specific case I much prefer the behaviour exhibited by 1.16.1. Resource quota not being available changes dynamically, if the JobManager kept retrying (ideally with a backoff) it is not unreasonable to expect that eventually it could succeed in most real-world scenarios. Adding some guardrails around this (if a minimum parallelism is not satisfied fail instead, if a max timeout is reached fail etc) to avoid ending up with many small jobs competing for insufficient resources and wasting capacity would be acceptable to me, but outright failing on the first try is more a bug than a feature imho. :);;;","04/May/23 13:43;xtsong;[~gyfora],

bq. Flink treats only very few errors fatal. IO errors, connector (source/sink ) errors etc all cause job restarts and in many cases ""Flink cannot recover from by itself"". You actually expect the error to be temporary and hopefully not get it after the restart. So I think it would be generally inconsistent with the current error handling behaviour if resource manager errors would simply let the job die fatally and not retry in the same way.

I think the difference here is that, for IO errors and connector errors, it affects the job but not the Flink cluster / deployment. Thinking of a session cluster, we should not fail the cluster for an error from a single job. But for resource manager interacting with Kubernetes API server, this is a cluster behavior and conceptually we don't distinguish resources for individual jobs until the slots are allocated. Moreover, it's possible that multiple jobs share the same resource (pod). One could argue that in application mode the cluster / deployment is equivalent to the job. However, the cluster mode (session / application) is transparent to the resource manager.

 bq. Flink jobs/clusters should be resilient and keep retrying in case of errors and should not give up especially for streaming workloads.

This is different from the feedback that I get from our production. But I can understand if that's what some of the users want. So I guess maybe it worth a configuration option as you suggested.

[~mbalassi],

+1 to what you said about the specific case. I think there's a consensus on reaching quota limit should not be treated as fatal errors.;;;","04/May/23 14:24;thw;There are many cases where errors are transient. This specific case is actually quite obvious, the resource availability on a large cluster is changing constantly. A pod may not be scheduled now but few seconds later. Other k8s related issues can also be transient, for example a failed request due to rate limiting will likely succeed soon after and we would actually make things worse by not following a backoff/retry strategy and simply letting the job fail. I'm also leaning more towards retry by default strategy and identify the cases that should be fatal error.;;;","04/May/23 20:48;sergiosp;Hi [~mapohl] - let me setup a new cluster later on to get the full logs. Meanwhile, please find the thread dump from the Flink 1.17.0 crash:

 
{code:java}
2023-04-28 20:50:50,305 INFO  org.apache.flink.runtime.resourcemanager.slotmanager.DeclarativeSlotManager [] - Received resource requirements from job 0a97c80a173b7ebb619c5b030b607520: [ResourceRequirement{resourceProfile=ResourceProfile{UNKNOWN}, numberOfRequiredSlots=1}]
...
2023-04-28 20:50:50,534 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'flink-akka.actor.default-dispatcher-15' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/env-my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at java.util.concurrent.CompletableFuture.encodeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture.completeThrowable(Unknown Source) ~[?:?]
        at java.util.concurrent.CompletableFuture$AsyncRun.run(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
        at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.KubernetesClientException: Failure executing: POST at: https://10.96.0.1/api/v1/namespaces/env-my-namespace/pods. Message: Forbidden!Configured service account doesn't have access. Service account may have been revoked. pods ""my-namespace-flink-cluster-taskmanager-1-2"" is forbidden: exceeded quota: my-namespace-resource-quota, requested: limits.cpu=3, used: limits.cpu=12100m, limited: limits.cpu=13.
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:684) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.requestFailure(OperationSupport.java:664) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.assertResponseCode(OperationSupport.java:613) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:558) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleResponse(OperationSupport.java:521) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.OperationSupport.handleCreate(OperationSupport.java:308) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:644) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.BaseOperation.handleCreate(BaseOperation.java:83) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.shaded.io.fabric8.kubernetes.client.dsl.base.CreateOnlyResourceOperation.create(CreateOnlyResourceOperation.java:61) ~[flink-dist-1.17.0.jar:1.17.0]
        at org.apache.flink.kubernetes.kubeclient.Fabric8FlinkKubeClient.lambda$createTaskManagerPod$1(Fabric8FlinkKubeClient.java:163) ~[flink-dist-1.17.0.jar:1.17.0]
        ... 4 more
2023-04-28 20:50:50,602 ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - Thread dump: 
""main"" prio=5 Id=1 WAITING on java.util.concurrent.CompletableFuture$Signaller@2897b146
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.CompletableFuture$Signaller@2897b146
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.CompletableFuture$Signaller.block(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.managedBlock(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.CompletableFuture.waitingGet(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.CompletableFuture.get(Unknown Source)
        at app//org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:741)
        at app//org.apache.flink.kubernetes.entrypoint.KubernetesSessionClusterEntrypoint.main(KubernetesSessionClusterEntrypoint.java:61)""Reference Handler"" daemon prio=10 Id=2 RUNNABLE
        at java.base@11.0.19/java.lang.ref.Reference.waitForReferencePendingList(Native Method)
        at java.base@11.0.19/java.lang.ref.Reference.processPendingReferences(Unknown Source)
        at java.base@11.0.19/java.lang.ref.Reference$ReferenceHandler.run(Unknown Source)""Finalizer"" daemon prio=8 Id=3 WAITING on java.lang.ref.ReferenceQueue$Lock@2b21a3e4
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on java.lang.ref.ReferenceQueue$Lock@2b21a3e4
        at java.base@11.0.19/java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.base@11.0.19/java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.base@11.0.19/java.lang.ref.Finalizer$FinalizerThread.run(Unknown Source)""Signal Dispatcher"" daemon prio=9 Id=4 RUNNABLE""Common-Cleaner"" daemon prio=8 Id=9 TIMED_WAITING on java.lang.ref.ReferenceQueue$Lock@16aab001
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on java.lang.ref.ReferenceQueue$Lock@16aab001
        at java.base@11.0.19/java.lang.ref.ReferenceQueue.remove(Unknown Source)
        at java.base@11.0.19/jdk.internal.ref.CleanerImpl.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)
        at java.base@11.0.19/jdk.internal.misc.InnocuousThread.run(Unknown Source)""Log4j2-TF-3-Scheduled-1"" daemon prio=5 Id=12 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2ee9ffbd
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2ee9ffbd
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""Log4j2-TF-6-Scheduled-2"" daemon prio=5 Id=18 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3b6166a7
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3b6166a7
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-scheduler-1"" prio=5 Id=19 TIMED_WAITING
        at java.base@11.0.19/java.lang.Thread.sleep(Native Method)
        at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
        at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
        at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-akka.actor.internal-dispatcher-2"" prio=5 Id=20 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@1d79ccac
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@1d79ccac
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-akka.actor.default-dispatcher-4"" prio=5 Id=22 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-akka.remote.default-remote-dispatcher-5"" prio=5 Id=23 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@3fe545c2
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@3fe545c2
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-akka.remote.default-remote-dispatcher-6"" prio=5 Id=24 TIMED_WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@3fe545c2
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@3fe545c2
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkUntil(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""New I/O worker #1"" prio=5 Id=25 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@f0148c7
        -  locked sun.nio.ch.EPollSelectorImpl@5c4ee927
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@708d421""New I/O boss #2"" prio=5 Id=27 RUNNABLE
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@6f9289
        -  locked sun.nio.ch.EPollSelectorImpl@15b10c13
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@409550d1""New I/O worker #3"" prio=5 Id=28 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@10733912
        -  locked sun.nio.ch.EPollSelectorImpl@3bdffdea
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@dbc95df""New I/O server boss #4"" prio=5 Id=29 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@4758dbc2
        -  locked sun.nio.ch.EPollSelectorImpl@5a1496ae
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@85887aa""Timer-0"" daemon prio=5 Id=32 TIMED_WAITING on java.util.TaskQueue@53a69f57
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on java.util.TaskQueue@53a69f57
        at java.base@11.0.19/java.util.TimerThread.mainLoop(Unknown Source)
        at java.base@11.0.19/java.util.TimerThread.run(Unknown Source)""BLOB Server listener at 6124"" daemon prio=5 Id=31 RUNNABLE (in native)
        at java.base@11.0.19/java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.base@11.0.19/java.net.AbstractPlainSocketImpl.accept(Unknown Source)
        at java.base@11.0.19/java.net.ServerSocket.implAccept(Unknown Source)
        at java.base@11.0.19/java.net.ServerSocket.accept(Unknown Source)
        at app//org.apache.flink.util.NetUtils.acceptWithoutTimeout(NetUtils.java:143)
        at app//org.apache.flink.runtime.blob.BlobServer.run(BlobServer.java:316)""flink-metrics-scheduler-1"" prio=5 Id=34 TIMED_WAITING
        at java.base@11.0.19/java.lang.Thread.sleep(Native Method)
        at akka.actor.LightArrayRevolverScheduler.waitNanos(LightArrayRevolverScheduler.scala:90)
        at akka.actor.LightArrayRevolverScheduler$$anon$3.nextTick(LightArrayRevolverScheduler.scala:300)
        at akka.actor.LightArrayRevolverScheduler$$anon$3.run(LightArrayRevolverScheduler.scala:270)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-metrics-akka.actor.internal-dispatcher-3"" prio=5 Id=36 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@17521864
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@17521864
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-metrics-akka.remote.default-remote-dispatcher-5"" prio=5 Id=38 TIMED_WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@16555787
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@16555787
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkUntil(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-metrics-akka.remote.default-remote-dispatcher-6"" prio=5 Id=39 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@16555787
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@16555787
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""New I/O worker #5"" prio=5 Id=40 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@688a0205
        -  locked sun.nio.ch.EPollSelectorImpl@f316464
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@7d652799""New I/O boss #6"" prio=5 Id=42 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@827ccde
        -  locked sun.nio.ch.EPollSelectorImpl@34f25b32
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@73a2c645""New I/O worker #7"" prio=5 Id=43 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@cd0fd83
        -  locked sun.nio.ch.EPollSelectorImpl@846aa39
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:68)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:434)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:89)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@779d4ce9""New I/O server boss #8"" prio=5 Id=44 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@3eb1c6d3
        -  locked sun.nio.ch.EPollSelectorImpl@18bc21fc
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
        at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:212)
        at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
        at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@329fcc7e""flink-rest-server-netty-boss-thread-1"" daemon prio=5 Id=48 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet@130bfb8d
        -  locked sun.nio.ch.EPollSelectorImpl@2ea19cb7
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:879)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:526)
        at app//org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        ...""Flink-DispatcherRestEndpoint-thread-1"" daemon prio=5 Id=49 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""cluster-io-thread-1"" daemon prio=5 Id=50 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""pool-2-thread-1"" prio=5 Id=51 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2bc769e4
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@2bc769e4
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""cluster-io-thread-2"" daemon prio=5 Id=52 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-akka.actor.default-dispatcher-13"" prio=5 Id=55 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""flink-akka.actor.default-dispatcher-14"" prio=5 Id=56 WAITING on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool@5e904dba
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinPool.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ForkJoinWorkerThread.run(Unknown Source)""resourcemanager_1-main-scheduler-thread-1"" daemon prio=5 Id=73 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@9a11c0f
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@9a11c0f
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-kubeclient-io-for-resourcemanager-thread-1"" daemon prio=5 Id=74 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-akka.actor.default-dispatcher-15"" prio=5 Id=75 RUNNABLE
        at java.management@11.0.19/sun.management.ThreadImpl.dumpThreads0(Native Method)
        at java.management@11.0.19/sun.management.ThreadImpl.dumpAllThreads(Unknown Source)
        at java.management@11.0.19/sun.management.ThreadImpl.dumpAllThreads(Unknown Source)
        at app//org.apache.flink.util.concurrent.ThreadUtils.errorLogThreadDump(ThreadUtils.java:33)
        at app//org.apache.flink.util.FatalExitExceptionHandler.uncaughtException(FatalExitExceptionHandler.java:47)
        at app//org.apache.flink.util.concurrent.FutureUtils.lambda$handleUncaughtException$20(FutureUtils.java:1216)
        at app//org.apache.flink.util.concurrent.FutureUtils$$Lambda$623/0x00000001006a9840.accept(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.CompletableFuture.uniWhenComplete(Unknown Source)
        ...""OkHttp https://10.96.0.1/..."" prio=5 Id=76 RUNNABLE (in native)
        at java.base@11.0.19/java.net.SocketInputStream.socketRead0(Native Method)
        at java.base@11.0.19/java.net.SocketInputStream.socketRead(Unknown Source)
        at java.base@11.0.19/java.net.SocketInputStream.read(Unknown Source)
        at java.base@11.0.19/java.net.SocketInputStream.read(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.read(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.readHeader(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.bytesInCompletePacket(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketImpl.readApplicationRecord(Unknown Source)
        ...        Number of locked synchronizers = 2
        - java.util.concurrent.locks.ReentrantLock$NonfairSync@234e456e
        - java.util.concurrent.ThreadPoolExecutor$Worker@330607e6""OkHttp ConnectionPool"" daemon prio=5 Id=77 TIMED_WAITING on org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool@335680aa
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool@335680aa
        at java.base@11.0.19/java.lang.Object.wait(Unknown Source)
        at app//org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool.lambda$new$0(RealConnectionPool.java:62)
        at app//org.apache.flink.kubernetes.shaded.okhttp3.internal.connection.RealConnectionPool$$Lambda$709/0x000000010071d040.run(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@457b7a36""Okio Watchdog"" daemon prio=5 Id=78 TIMED_WAITING on java.lang.Class@711498ff
        at java.base@11.0.19/java.lang.Object.wait(Native Method)
        -  waiting on java.lang.Class@711498ff
        at app//org.apache.flink.kubernetes.shaded.okio.AsyncTimeout.awaitTimeout(AsyncTimeout.java:348)
        at app//org.apache.flink.kubernetes.shaded.okio.AsyncTimeout$Watchdog.run(AsyncTimeout.java:313)""flink-kubeclient-io-for-resourcemanager-thread-2"" daemon prio=5 Id=80 RUNNABLE
        at java.base@11.0.19/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base@11.0.19/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at java.base@11.0.19/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.base@11.0.19/java.lang.reflect.Method.invoke(Unknown Source)
        at app//org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:689)
        at app//org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:774)
        at app//org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178)
        at app//org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:119)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.ThreadPoolExecutor$Worker@5fdf1d80""cluster-io-thread-3"" daemon prio=5 Id=84 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""Flink-DispatcherRestEndpoint-thread-2"" daemon prio=5 Id=85 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-rest-server-netty-worker-thread-1"" daemon prio=5 Id=86 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySet@3296b3ab
        -  locked sun.nio.ch.EPollSelectorImpl@72ad4a98
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.SelectedSelectionKeySetSelector.select(SelectedSelectionKeySetSelector.java:68)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:879)
        at app//org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:526)
        at app//org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        ...""Flink-DispatcherRestEndpoint-thread-3"" daemon prio=5 Id=88 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""Flink-DispatcherRestEndpoint-thread-4"" daemon prio=5 Id=89 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1e9640ee
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""OkHttp WebSocket https://10.96.0.1/..."" prio=5 Id=98 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@23991496
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@23991496
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""jobmanager-io-thread-1"" daemon prio=5 Id=99 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@24cd939f
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@24cd939f
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-akka.actor.supervisor-dispatcher-16"" prio=5 Id=100 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@23417746
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@23417746
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.poll(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""flink-metrics-13"" prio=1 Id=102 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3ba0103c
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3ba0103c
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.poll(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""cluster-io-thread-4"" daemon prio=5 Id=108 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67dc2101
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""jobmanager-future-thread-1"" daemon prio=5 Id=109 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@50769e7a
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@50769e7a
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-kubeclient-io-for-resourcemanager-thread-3"" daemon prio=5 Id=112 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""OkHttp 10.96.0.1 Writer"" prio=5 Id=113 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@186fae80
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@186fae80
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""OkHttp 10.96.0.1"" daemon prio=5 Id=114 RUNNABLE (in native)
        at java.base@11.0.19/java.net.SocketInputStream.socketRead0(Native Method)
        at java.base@11.0.19/java.net.SocketInputStream.socketRead(Unknown Source)
        at java.base@11.0.19/java.net.SocketInputStream.read(Unknown Source)
        at java.base@11.0.19/java.net.SocketInputStream.read(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.read(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.readHeader(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketInputRecord.bytesInCompletePacket(Unknown Source)
        at java.base@11.0.19/sun.security.ssl.SSLSocketImpl.readApplicationRecord(Unknown Source)
        ...        Number of locked synchronizers = 1
        - java.util.concurrent.locks.ReentrantLock$NonfairSync@303c118c""OkHttp Http2Connection"" daemon prio=5 Id=115 TIMED_WAITING on java.util.concurrent.SynchronousQueue$TransferStack@ba6d647
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.SynchronousQueue$TransferStack@ba6d647
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.SynchronousQueue$TransferStack.transfer(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.SynchronousQueue.poll(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""FlinkCompletableFutureDelayScheduler-thread-1"" daemon prio=5 Id=117 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6fc71c07
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@6fc71c07
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""flink-kubeclient-io-for-resourcemanager-thread-4"" daemon prio=5 Id=118 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@251cace5
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""AkkaRpcService-Supervisor-Termination-Future-Executor-thread-1"" daemon prio=5 Id=119 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@ab9b4d1
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@ab9b4d1
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.LinkedBlockingQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.base@11.0.19/java.lang.Thread.run(Unknown Source)""jobmanager_3-main-scheduler-thread-1"" daemon prio=5 Id=121 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@5a59647b
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@5a59647b
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""SourceCoordinator-Source: *anonymous_kafka$15*[66]"" prio=5 Id=122 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67708599
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@67708599
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""Checkpoint Timer"" daemon prio=5 Id=123 TIMED_WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30809a54
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@30809a54
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.parkNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...""kafka-admin-client-thread | KafkaSource--5297459524620964018-enumerator-admin-client"" daemon prio=5 Id=125 RUNNABLE (in native)
        at java.base@11.0.19/sun.nio.ch.EPoll.wait(Native Method)
        at java.base@11.0.19/sun.nio.ch.EPollSelectorImpl.doSelect(Unknown Source)
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.lockAndDoSelect(Unknown Source)
        -  locked sun.nio.ch.Util$2@77d8bfa9
        -  locked sun.nio.ch.EPollSelectorImpl@66b37795
        at java.base@11.0.19/sun.nio.ch.SelectorImpl.select(Unknown Source)
        at app//org.apache.kafka.common.network.Selector.select(Selector.java:873)
        at app//org.apache.kafka.common.network.Selector.poll(Selector.java:465)
        at app//org.apache.kafka.clients.NetworkClient.poll(NetworkClient.java:560)
        at app//org.apache.kafka.clients.admin.KafkaAdminClient$AdminClientRunnable.processRequests(KafkaAdminClient.java:1408)
        ...""SourceCoordinator-Source: *anonymous_kafka$15*[66]-worker-thread-1"" daemon prio=5 Id=127 WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@58c6a97
        at java.base@11.0.19/jdk.internal.misc.Unsafe.park(Native Method)
        -  waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@58c6a97
        at java.base@11.0.19/java.util.concurrent.locks.LockSupport.park(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.getTask(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.base@11.0.19/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        ...
2023-04-28 20:50:50,607 INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting KubernetesSessionClusterEntrypoint down with application status UNKNOWN. Diagnostics Cluster entrypoint has been closed externally..
2023-04-28 20:50:50,609 INFO  org.apache.flink.runtime.blob.BlobServer                     [] - Stopped BLOB server at 0.0.0.0:6124
[pod@my-namespace-flink-cluster-59cd575d6b-bll6b log]$ {code}
 

 ;;;","05/May/23 01:25;xtsong;cc [~wangyang0918];;;","05/May/23 08:00;mapohl;[~sergiosp] I guess it's not necessary to provide the logs anymore. The problem is understood and the discussion went on, already.

On the discussion about how to handle errors in this part of the code: tbh, initially I leaned towards [~xtsong]'s proposal where he suggested to make the error handling as strict as possible through a whitelist and avoid adding yet another configuration parameter with the idea in mind that Flink's deployment environment should be in a healthy state without any mis-configuration. But as the discussion moved on, I started to acknowledge that it's too strict in quite a few scenarios. I also get [~gyfora]'s point that we're not that restrictive in other places of the code base, either.

One concern I have with the error whitelisting, though, is that the error classification could become ""complex"". The error [~sergiosp] shared was about hitting quota limits. The error type we're seeing is a Forbidden error (unfortunately, without the error code being logged but I would assume 403 analogously to the HTTP error code). I could imagine this error type also being returned in other cases (e.g. wrong service account being used). The former error is something we want to retry in certain scenarios but the latter one (based on my understanding) would be one that could be considered a general infrastructure issue and, as a consequence, could be treated as a fatal error. It looks like it would require error message parsing to identify the type of error. How confident are we about the stability of those error messages? It looks like they are derived from the k8s HTTP responses and, therefore, might be stable among different Kubernetes versions. But generally, relying on error messages for deriving Flink's behavior feels not right. Is this a valid concern? In this sense, I started to favor what was proposed by [~gyfora] in the discussion.

I might be wrong here because I'm not that familiar with the k8s API. I wanted to share this, anyway.;;;","05/May/23 08:44;xtsong;Thanks all for the explanation and patience. It seems there's a commonly tendency towards the retry-by-default approach. 

I also consulted a few colleagues from our Kubernetes team about this. They also share the opinion that there might be more error types that can be resolved by retrying than a whitelist could possibly handle. The only concern they mentioned is that keeping retrying may make the Kubernetes API Server harder to recover from outages in some specific cases, which I believe can be addressed with backoff and guardrails as [~mbalassi] mentioned.

I'd respect the opinion of the majority, withdraw my proposal, and +1 for [~gyfora]'s proposal.;;;","30/May/23 06:55;gyfora;[~Weijie Guo] are you working on this ticket?;;;","30/May/23 08:00;Weijie Guo;[~gyfora] Sorry, I am quite busy recently, feel free to re-assign this ticket if you want to pick up it. :);;;","30/May/23 12:07;gyfora;No worries, I will assign it to myself and will work on this shortly.;;;","01/Jun/23 13:21;Weijie Guo;master(1.18) via 3b9f7cf8ffcd357f252f62dee62d26dbc6a76e91.
release-1.17 via 07f43a5c68301ce119352d40cbec46b2c52a79a2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,
SQL with LAG function NullPointerException,FLINK-31967,13534418,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,pyro,padavan,padavan,28/Apr/23 08:06,31/May/23 03:07,13/Jul/23 08:29,31/May/23 03:07,,,,,,,1.16.3,1.17.2,1.18.0,,,,,,Table SQL / API,,,,0,pull-request-available,,,"I want to make a query with the LAG function. And got Job Exception without any explanations.

 

*Code:*
{code:java}
private static void t1_LeadLag(DataStream<UserModel> ds, StreamExecutionEnvironment env) {
    StreamTableEnvironment te = StreamTableEnvironment.create(env);
    Table t = te.fromDataStream(ds, Schema.newBuilder().columnByExpression(""proctime"", ""proctime()"").build());

    te.createTemporaryView(""users"", t);

    Table res = te.sqlQuery(""SELECT userId, `count`,\n"" +
            "" LAG(`count`) OVER (PARTITION BY userId ORDER BY proctime) AS prev_quantity\n"" +
            "" FROM users"");

    te.toChangelogStream(res).print();
}{code}
 

*Input:*

{""userId"":3,""count"":0,""dt"":""2023-04-28T07:44:21.551Z""}

 

*Exception:* I remove part about basic JobExecutionException and kept the important(i think)
{code:java}
Caused by: java.lang.NullPointerException
    at org.apache.flink.table.data.GenericRowData.getInt(GenericRowData.java:149)
    at org.apache.flink.table.data.RowData.lambda$createFieldGetter$245ca7d1$6(RowData.java:245)
    at org$apache$flink$table$runtime$functions$aggregate$LagAggFunction$LagAcc$2$Converter.toExternal(Unknown Source)
    at org.apache.flink.table.data.conversion.StructuredObjectConverter.toExternal(StructuredObjectConverter.java:101)
    at UnboundedOverAggregateHelper$15.setAccumulators(Unknown Source)
    at org.apache.flink.table.runtime.operators.over.ProcTimeUnboundedPrecedingFunction.processElement(ProcTimeUnboundedPrecedingFunction.java:92)
    at org.apache.flink.table.runtime.operators.over.ProcTimeUnboundedPrecedingFunction.processElement(ProcTimeUnboundedPrecedingFunction.java:42)
    at org.apache.flink.streaming.api.operators.KeyedProcessOperator.processElement(KeyedProcessOperator.java:83)
    at org.apache.flink.streaming.runtime.io.RecordProcessorUtils.lambda$getRecordProcessor$0(RecordProcessorUtils.java:60)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:237)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:146)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
    at java.base/java.lang.Thread.run(Thread.java:829){code}",,fsk119,jark,martijnvisser,padavan,pyro,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Apr/23 11:46;padavan;image-2023-04-28-14-46-19-736.png;https://issues.apache.org/jira/secure/attachment/13057705/image-2023-04-28-14-46-19-736.png","28/Apr/23 12:06;padavan;image-2023-04-28-15-06-48-184.png;https://issues.apache.org/jira/secure/attachment/13057706/image-2023-04-28-15-06-48-184.png","28/Apr/23 12:14;padavan;image-2023-04-28-15-14-58-788.png;https://issues.apache.org/jira/secure/attachment/13057707/image-2023-04-28-15-14-58-788.png","28/Apr/23 12:17;padavan;image-2023-04-28-15-17-49-144.png;https://issues.apache.org/jira/secure/attachment/13057708/image-2023-04-28-15-17-49-144.png","28/Apr/23 14:06;padavan;image-2023-04-28-17-06-20-737.png;https://issues.apache.org/jira/secure/attachment/13057713/image-2023-04-28-17-06-20-737.png","28/Apr/23 14:05;padavan;simpleFlinkKafkaLag.zip;https://issues.apache.org/jira/secure/attachment/13057712/simpleFlinkKafkaLag.zip",,,6.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 31 03:07:34 UTC 2023,,,,,,,,,,"0|z1hlc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 08:55;martijnvisser;I can't reproduce this. I've created the following table:

{code:sql}
CREATE TABLE users (
    userId STRING,
    `count` INT,
    proctime AS PROCTIME()
) WITH (
  'connector' = 'datagen',
  'fields.count.min' = '0',
  'fields.count.max' = '0'
);
{code}

And then run 

{code:sql}
SELECT userId, `count`, LAG(`count`) OVER (PARTITION BY userId ORDER BY proctime) AS prev_quantity FROM users;
{code}

On Flink 1.17.0. There's no NPE for me. Which Flink version did you use?;;;","28/Apr/23 08:58;martijnvisser;Also, when using:

{code:sql}
CREATE TABLE users (
    userId INT,
    `count` INT,
    proctime AS PROCTIME()
) WITH (
  'connector' = 'datagen',
  'fields.userId.min' = '1',
  'fields.userId.max' = '10',
  'fields.count.min' = '0',
  'fields.count.max' = '10'
);
{code}

I get results as I would expect. ;;;","28/Apr/23 11:44;padavan;[~martijnvisser] 

/usr/lib/jvm/java-1.11.0-openjdk-amd64/

!image-2023-04-28-14-46-19-736.png!

 
{code:java}
 public static void main(String[] args) throws Exception {
  
  final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
 
  KafkaSource<UserModel> source = KafkaSource.<UserModel>builder()
                .setBootstrapServers(kafka)
                .setTopics(""x1"").setGroupId(""flink_group"")
                .setValueOnlyDeserializer(new JsonConverter()).build();
 
        WatermarkStrategy<UserModel> strategy = WatermarkStrategy.               <UserModel>forBoundedOutOfOrderness(Duration.ofSeconds(10))
                .withTimestampAssigner((i, timestamp) -> {
                  long time = i.dt.toInstant(ZoneOffset.UTC).toEpochMilli();
                    return time;
                });
        DataStream<UserModel> ds = env.fromSource(source, strategy, ""Kafka Source"");
        
        t1_LeadLag(ds, env);
        env.execute(""Flink Java API Skeleton"");
}
{code}
 
{code:java}
private static void t1_LeadLag(DataStream<UserModel> ds, StreamExecutionEnvironment env) {
    StreamTableEnvironment te = StreamTableEnvironment.create(env);
    Table t = te.fromDataStream(ds, Schema.newBuilder().columnByExpression(""proctime"", ""proctime()"").build());

    te.createTemporaryView(""users"", t);

    Table res = te.sqlQuery(""SELECT userId, `count`,\n"" +
            "" LAG(`count`) OVER (PARTITION BY userId ORDER BY proctime) AS prev_quantity\n"" +
            "" FROM users"");

    te.toChangelogStream(res).print();
}{code}
 

 

 ;;;","28/Apr/23 11:54;martijnvisser;[~padavan] You are mixing different versions of Flink (1.17.0, 1.16.1). That won't work. Please make sure that you are all using the same versions. ;;;","28/Apr/23 12:15;padavan;[~martijnvisser]  i sync to latest version 

1.17.0

nothing change , same error

!image-2023-04-28-15-17-49-144.png!

 ;;;","28/Apr/23 12:25;martijnvisser;I can't reproduce it. You're using Kafka Source, so it could be your incoming data. Can you create a reproducer with the datagen connector? ;;;","28/Apr/23 13:18;padavan;[~martijnvisser] 

wow , intresting if use DataGen then all work fine: 
{code:java}
  private static void t1_LeadLag(StreamExecutionEnvironment env) {
        StreamTableEnvironment te = StreamTableEnvironment.create(env);

        te.executeSql(
                ""CREATE TABLE users (\n"" +
                        ""    userId INT,\n"" +
                        ""    `count` INT,\n"" +
                        ""    proctime AS PROCTIME()\n"" +
                        "") WITH (\n"" +
                        ""  'connector' = 'datagen',\n"" +
                        ""  'fields.userId.min' = '1',\n"" +
                        ""  'fields.userId.max' = '10',\n"" +
                        ""  'fields.count.min' = '0',\n"" +
                        ""  'fields.count.max' = '10'\n"" +
                        "");"");

/*      
        Table t = te.fromDataStream(ds, Schema.newBuilder().columnByExpression(""proctime"", ""proctime()"").build());
        t.printSchema();
        te.createTemporaryView(""users"", t);
*/

        Table res = te.sqlQuery(""SELECT userId, `count`, proctime,"" +
                "" LAG(`count`, 1, 1) OVER w AS prev_quantity"" +
                "" FROM users"" +
                "" WINDOW w AS (ORDER BY proctime)"");
 
        te.toChangelogStream(res).print();
    }
{code}
 

I work with kafka and did various ETLs with it, Window Agg all worked fine.

But when I started working with the LAG and above code, it gives an exception. 

What can it be?
If need, Maybe I should make a project archive with my example (but you need kafka (for example in docker))? 

 

 ;;;","28/Apr/23 13:34;martijnvisser;That would be great for debugging, yes. ;;;","28/Apr/23 14:07;padavan;[~martijnvisser] 

Add Project

[^simpleFlinkKafkaLag.zip]

with two functions (work datagen and exception kafka)

!image-2023-04-28-17-06-20-737.png!

For kafta input example: 
{code:java}
{""userId"":1,""count"":5,""dt"":""2023-04-28T14:02:23.113Z""}{code}
 

java-1.11.0-openjdk-amd64;;;","07/May/23 12:27;padavan;[~martijnvisser] [~jark] [~lincoln.86xy] 

If you need anything else, let me know, i'll do it. :);;;","11/May/23 12:45;fsk119;[~padavan] after investigation, the type inference is not correct for the lag function. For a quick fix, you can modify the type of the `count` in the UserModel to `Integer`. ;;;","11/May/23 21:50;padavan;[~fsk119] with Integer work. 

I think it is at least unexpected behavior that int != Integer. Will there be any fixes? Or will you just improve the output error with a description?;;;","12/May/23 01:38;fsk119;[~padavan] Sure. There will be a PR to fix this soon. ;;;","15/May/23 06:25;pyro;[~padavan]  This problem is caused by the basic data type of Java, and I will propose a PR to solve this problem in the future;;;","18/May/23 07:49;padavan;[~fsk119] [~pyro] 

This looks like basic functionality that is often used. It's very strange that it still doesn't work. I hope to get this fixed as soon as possible (y);;;","25/May/23 03:07;fsk119;Merged into master: ac6aedbf0f35ba9734108a3c347e649bbf231c62

Merged into release-1.17: 6cac01d827ddb58bd79b5b42e1af4e05fbc45814

Merged into release-1.16: c07b50170d081978a3ca17b968d183ca9f4a57a2;;;","27/May/23 18:18;padavan;[~fsk119] already in master ? 

How can I check what works? Will there be new versions of java packages ? 1.17.2 ? ;;;","31/May/23 03:06;fsk119;[~padavan] Yes we have already merged into the master, 1.17 and 1.16. It will a bug fix version for 1.17 in the future. But right now you should package the latest 1.17 branch. It's welcome you report the new problems here.;;;","31/May/23 03:07;fsk119;I close this right now. If meet a new problem about this, please reopen this again.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ClassNotFoundException in benchmarks,FLINK-31965,13534398,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Paul Lin,Paul Lin,Paul Lin,28/Apr/23 06:29,09/May/23 06:52,13/Jul/23 08:29,09/May/23 06:52,1.18.0,,,,,,,,,,,,,,Benchmarks,,,,0,pull-request-available,,,"The benchmarks rely on the test jar of `flink-streaming-java`. However, the jar is set to test scope, thus not included in the packaged jar. Therefore ClassNotFoundException occurs while running the benchmarks with `java --jar xxx` command.

```
java.lang.NoClassDefFoundError: org/apache/flink/streaming/util/KeyedOneInputStreamOperatorTestHarness
    at org.apache.flink.contrib.streaming.state.benchmark.RescalingBenchmark.prepareState(RescalingBenchmark.java:111)
    at org.apache.flink.contrib.streaming.state.benchmark.RescalingBenchmark.setUp(RescalingBenchmark.java:78)
    at org.apache.flink.state.benchmark.RocksdbStateBackendRescalingBenchmarkExecutor.setUp(RocksdbStateBackendRescalingBenchmarkExecutor.java:66)
    at org.apache.flink.state.benchmark.generated.RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest._jmh_tryInit_f_rocksdbstatebackendrescalingbenchmarkexecutor0_0(RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest.java:370)
    at org.apache.flink.state.benchmark.generated.RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest.rescaleRocksDB_AverageTime(RocksdbStateBackendRescalingBenchmarkExecutor_rescaleRocksDB_jmhTest.java:147)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:453)
    at org.openjdk.jmh.runner.BenchmarkHandler$BenchmarkTask.call(BenchmarkHandler.java:437)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: org.apache.flink.streaming.util.KeyedOneInputStreamOperatorTestHarness
    at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    ... 17 more
```",,martijnvisser,Paul Lin,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 09 06:52:36 UTC 2023,,,,,,,,,,"0|z1hl7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 09:26;Yanfei Lei;Have you tried running it with the exec command?
{code:java}
mvn clean package exec:exec \
 -Dbenchmarks=""org.apache.flink.state.benchmark.*"" {code};;;","28/Apr/23 09:30;Paul Lin;Yes, it runs fine with source codes.;;;","09/May/23 06:52;martijnvisser;Fixed in master: c038bf3f83c56230c6e621373160d0215af0e256;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
java.lang.ArrayIndexOutOfBoundsException when scaling down with unaligned checkpoints,FLINK-31963,13534378,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,srichter,tanee.kim,tanee.kim,28/Apr/23 02:01,19/May/23 04:55,13/Jul/23 08:29,19/May/23 04:53,1.15.4,1.16.1,1.17.0,1.18.0,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Checkpointing,,,,0,stability,,,"I'm testing Autoscaler through Kubernetes Operator and I'm facing the following issue.

As you know, when a job is scaled down through the autoscaler, the job manager and task manager go down and then back up again.

When this happens, an index out of bounds exception is thrown and the state is not restored from a checkpoint.

[~gyfora] told me via the Flink Slack troubleshooting channel that this is likely an issue with Unaligned Checkpoint and not an issue with the autoscaler, but I'm opening a ticket with Gyula for more clarification.

Please see the attached JM and TM error logs.
Thank you.","Flink: 1.17.0
FKO: 1.4.0
StateBackend: RocksDB(Genetic Incremental Checkpoint & Unaligned Checkpoint enabled)",martijnvisser,masteryhx,pnowojski,renqs,rmetzger,samrat007,srichter,tanee.kim,vineethNaroju,Weijie Guo,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,FLINK-27031,,,,,,,"28/Apr/23 17:49;tanee.kim;image-2023-04-29-02-49-05-607.png;https://issues.apache.org/jira/secure/attachment/13057718/image-2023-04-29-02-49-05-607.png","28/Apr/23 01:51;tanee.kim;jobmanager_error.txt;https://issues.apache.org/jira/secure/attachment/13057672/jobmanager_error.txt","28/Apr/23 01:51;tanee.kim;taskmanager_error.txt;https://issues.apache.org/jira/secure/attachment/13057671/taskmanager_error.txt",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 04:55:48 UTC 2023,,,,,,,,,,"0|z1hl3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 03:02;masteryhx;We also saw simliar exception when rescaling down manually with unaligned checkpoint is enabled.
This issue is related to unaligned checkpoint rescaling.
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 54
at org.apache.flink.runtime.io.network.partition.PipelinedResultPartition.getCheckpointedSubpartition(PipelinedResultPartition.java:183)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.getSubpartition(RecoveredChannelStateHandler.java:222)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.lambda$calculateMapping$1(RecoveredChannelStateHandler.java:237)
at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250)
at java.util.Spliterators$IntArraySpliterator.forEachRemaining(Spliterators.java:1032)
at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693)
at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481)
at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471)
at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.calculateMapping(RecoveredChannelStateHandler.java:238)
at java.util.HashMap.computeIfAbsent(HashMap.java:1126)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.getMappedChannels(RecoveredChannelStateHandler.java:227)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.getBuffer(RecoveredChannelStateHandler.java:182)
at org.apache.flink.runtime.checkpoint.channel.ResultSubpartitionRecoveredStateHandler.getBuffer(RecoveredChannelStateHandler.java:157)
at org.apache.flink.runtime.checkpoint.channel.ChannelStateChunkReader.readChunk(SequentialChannelStateReaderImpl.java:198)
at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readSequentially(SequentialChannelStateReaderImpl.java:107)
at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.read(SequentialChannelStateReaderImpl.java:93)
at org.apache.flink.runtime.checkpoint.channel.SequentialChannelStateReaderImpl.readOutputData(SequentialChannelStateReaderImpl.java:79)
at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:704)
at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:683)
at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:650)
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:954)
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:923)
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:746)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:568)
at java.lang.Thread.run(Thread.java:834) {code};;;","28/Apr/23 04:38;tanee.kim;Thanks, Hangxiang.

I've saw unaligned checkpoint have been stable in production but some issues still remain?

If there is a related ticket about rescaling down with unaligned checkpoint, please let me know.

 ;;;","28/Apr/23 09:50;pnowojski;Yes, I agree it looks like a problem with unaligned checkpoints. [~tanee.kim] could you clarify a couple of things?
* Can you reproduce this issue? Or did it happen only once? Or maybe a couple of times, but not always? If you can reproduce, can you post steps to reproduce?
* Could you share a job graph for which this error happened?
* What are the parallelism values before the rescale and after for all of the tasks?
* From which task/subtask this error is being thrown?;;;","28/Apr/23 17:53;tanee.kim;Thank you for responding, [~pnowojski].
 * It happens a couple of times, but not always.
 * Please check below for job graph
!image-2023-04-29-02-49-05-607.png!
 * It's actually scaled down through the autoscaler, so it depends on the processing speed or whatever the relevant metric is at the time, but it's usually something like 10 -> 5, 5 -> 3.
 * The middle of above graph,  process_stream :- Sink: es_error_sink +- Filter +- Filter

 ;;;","28/Apr/23 18:03;tanee.kim;A question unrelated to this ticket, but if the subtasks that exist in the above jobgraph all appear to be one, why is that?
In order to do source scaling, the outputRecords value needs to be non-zero, but since the downstream after the kafka source stream is not separated on the jobgraph, the outputRecords is getting zero, so we explicitly added a keyBy operator to the kafka source stream so that we can intentionally separate them and then calculate the outputRecords value.
(I don't think this is very good for performance) Is there any other way to ensure that the streams are separated into two at the desired location in the jobgraph?;;;","02/May/23 15:26;pnowojski;Thanks for the answers [~tanee.kim]

{quote}
It happens a couple of times, but not always.
{quote}
But once it happened once during a recovery from an unaligned checkpoint, does it happen always for that same checkpoint? Or even that is indeterministic and retrying recovery from the same checkpoint can sucede? 
{quote}
A question unrelated to this ticket, but if the subtasks that exist in the above jobgraph all appear to be one, why is that?
In order to do source scaling, the outputRecords value needs to be non-zero, but since the downstream after the kafka source stream is not separated on the jobgraph, the outputRecords is getting zero, so we explicitly added a keyBy operator to the kafka source stream so that we can intentionally separate them and then calculate the outputRecords value.
(I don't think this is very good for performance) Is there any other way to ensure that the streams are separated into two at the desired location in the jobgraph?
{quote}
You can brake chains via {{startNewChain}} or {{disableChaining}} https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/#task-chaining-and-resource-groups . However this doesn't seem like the right think to do. What do you mean by {{outputRecords}}? {{numRecordsOut}} metric should be available for all operators, including chained source operators.;;;","06/May/23 01:37;tanee.kim;Q1

If it happens once during recovery from an unaligned checkpoint, it will always happen from the same checkpoint.

Q2

If the numRecordsOut metric applies to all operators, including chaining, then I may have jumped the gun.
Since scaling usually takes time and source & downstream scaling conditions are different, I guess I should have monitored it more closely.

Can you explain the difference between Vertex and Operator?
Scaling is done on a per-Vertex basis in the JobGraph, but if chaining is applied, are multiple Operators considered as one Vertex and therefore not subject to Source & Downstream scaling?

Thanks for answering my question.;;;","08/May/23 16:52;srichter;Hi, just to clarify: when you say a checkpoint that fails once fails always - does this only apply for restore with rescaling or can you also not recover from the CP when the parallelism remains unchanged? If it only happens with rescaling, can you at least recover for some parallelism values or for no change at all?;;;","08/May/23 17:03;pnowojski;So far I was not able to reproduce this :(

Additionally to what [~srichter] asked. [~tanee.kim], would it be possible for you to provide the checkpoint files from when the failure was happening, so that we could reproduce it more easily? 

Secondly, a random guess. Can someone verify if setting {{execution.checkpointing.unaligned.max-subtasks-per-channel-state-file}} to 1 stops this issue from reoccurring? [1]

[1] https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#execution-checkpointing-unaligned-max-subtasks-per-channel-state;;;","10/May/23 04:08;masteryhx;Hi, [~pnowojski]. I am a bit sure that it may not be related to unified file mergeing of unaligned checkpoints because I meet above exception in 1.15.
My job is a bit complicated so I tried to simplify it to reproduce it. But I haven't currently. 
I will share more if I can reproduce it by a simple job or an ITCase.;;;","10/May/23 14:45;srichter;[~masteryhx] Did your job also make use of side-outputs? Just fishing among things that are potentially ""unusual"" about the jobs.;;;","11/May/23 02:45;masteryhx;[~srichter] No, I haven't used side-outputs.
The problematic nodes / connection in my job: keyedProcessFunction -> sink (The partitioner type is rebalance).

The exception is thrown while scaling down sink node.;;;","11/May/23 13:36;pnowojski;We have managed to reproduce and find the bug. Thank you for reporting the issue and help with analysing [~tanee.kim] and [~masteryhx]. We are now working on fixing it.;;;","11/May/23 13:59;srichter;I have a local reproducer as well as a fix, will open a PR once I have written the tests.;;;","12/May/23 11:14;srichter;Seems that this is similar to the problem described in FLINK-27031.;;;","16/May/23 08:11;renqs;[~srichter] Is there any updates on this issue? Thanks;;;","16/May/23 08:41;srichter;Yes, PR is currently in review here: https://github.com/apache/flink/pull/22584;;;","16/May/23 12:57;pnowojski;To clarify impact of this bug. This is a rare issue that can happen in every not backpressured job. The problem is that if the input buffers of a downstream subtask are empty AND the output buffers of the upstream subtask are not empty, then in-flight data are incorrectly restored from such checkpoint during a recovery attempt combined with rescaling. This can lead to variety of issues:
* ArrayIndexOutOfBoundException when downscaling (as reported here)
* in-flight records sent to incorrect downstream subtasks during scaling up or down. This for keyed exchanges will cause an immediate failure when trying to match key group on the downstream subtask. For non keyed exchanges the misalignment can remain undetected, causing incorrect results. 

Checkpoint itself is not corrupted, so recovery attempt without rescaling would work without without problems. Also recovery and rescaling from such checkpoint using a Flink version that has this bug fixed will also work correctly.;;;","19/May/23 04:53;Weijie Guo;master(1.18) via 354c0f455b92c083299d8028f161f0dd113ab614.
release-1.17 via 8d8a486aaa8360d6beabacc6980280c96bf900ea.
release-1.16 via 2203bc3bdc9f963e9bddf305714adae379b268bc.;;;","19/May/23 04:55;Weijie Guo;It seems that all pull request has been merged but this ticket not closed. Include this fix in rc1 of 1.16.2 and close it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,
libssl not found when running CI,FLINK-31962,13534357,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,27/Apr/23 19:57,04/May/23 11:44,13/Jul/23 08:29,27/Apr/23 20:03,1.16.2,1.17.1,1.18.0,,,,1.16.2,1.17.1,1.18.0,,,,,,Build System,,,,0,,,,"{code:java}
Installed Maven 3.2.5 to /home/vsts/maven_cache/apache-maven-3.2.5
Installing required software
Reading package lists...
Building dependency tree...
Reading state information...
bc is already the newest version (1.07.1-2build1).
bc set to manually installed.
libapr1 is already the newest version (1.6.5-1ubuntu1).
libapr1 set to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.
--2023-04-27 11:42:53--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.11_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-04-27 11:42:53 ERROR 404: Not Found.
{code}

",,martijnvisser,pgaref,qingyue,,,,,,,,,,,,,,,,,,,,,,FLINK-31999,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 27 20:03:22 UTC 2023,,,,,,,,,,"0|z1hkyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Apr/23 20:03;martijnvisser;Fixed in

master: ed7ca22efc68203d4882887a3856434137e6980f
release-1.17: a6e4652b071196c66f525a487baaae076ea9d64f
release-1.16: 4944df6aad2b46a864aa153303b9b4a49fedea36;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the unaligned checkpoint type at checkpoint level,FLINK-31959,13534303,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,27/Apr/23 10:43,10/May/23 08:42,13/Jul/23 08:29,10/May/23 08:41,1.16.1,1.17.0,1.18.0,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"FLINK-18851 added the checkpoint type in web UI to distinguish aligned checkpoint, unaligned checkpoint, savepoint and savepoint on cancel in {*}Flink 1.12{*}.

It distinguishes between UC and AC based on whether UC is enabled or disabled.[1]

However, FLINK-19680, FLINK-19681 and FLINK-19682 introduced the alignment-timeout in {*}Flink 1.13{*}, and it has been changed to {{aligned-checkpoint-timeout.}}
{code:java}
When activated, each checkpoint will still begin as an aligned checkpoint, but when the global checkpoint duration exceeds the aligned-checkpoint-timeout, if the aligned checkpoint has not completed, then the checkpoint will proceed as an unaligned checkpoint.{code}
If UC and AC-timeout is enabled and the checkpoint is completed as aligned checkpoint. It should show the aligned checkpoint instead of unaligned checkpoint.

 

[1] [https://github.com/apache/flink/blob/a3368635e3d06f764d144f8c8e2e06e499e79665/flink-runtime-web/web-dashboard/src/app/pages/job/checkpoints/detail/job-checkpoints-detail.component.ts#L118]

 ",,fanrui,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 08:41:45 UTC 2023,,,,,,,,,,"0|z1hkn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/May/23 07:42;pnowojski;merged as 35254cbdc9a^,35254cbdc9a to master;;;","10/May/23 08:41;fanrui;Merged 01ed7dbbde9f9bc8c3dc54e063c81fbe7de7f86b to 1.17-release

Merged 538edbdc11962a436134fc99e9cdbbc821341c2d to 1.16-release;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The new resource requirements REST API is only available for session clusters,FLINK-31935,13533975,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dmvk,dmvk,dmvk,25/Apr/23 09:45,26/Apr/23 07:43,13/Jul/23 08:29,26/Apr/23 07:43,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / REST,,,,0,pull-request-available,,,"We need to register both `JobResourceRequirementsHandler` and `
JobResourceRequirementsUpdateHandler` for application / per-job clusters as well.
 
These handlers have been introduced as part of FLINK-31316.",,dmvk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 26 07:43:23 UTC 2023,,,,,,,,,,"0|z1him8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 09:55;chesnay;Whoops; just need to move them up into the WebMonitorEndpoint I guess.;;;","26/Apr/23 07:43;chesnay;master: 3acb76dcf450f9136dec150124db901d1a9aa47b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra source raises an exception on Flink 1.16.0 if the user enables the metrics in the cassandra driver,FLINK-31927,13533886,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,echauchot,echauchot,echauchot,24/Apr/23 18:26,05/May/23 09:23,13/Jul/23 08:29,04/May/23 08:42,1.16.0,cassandra-3.1.0,,,,,cassandra-3.1.0,,,,,,,,Connectors / Cassandra,,,,0,pull-request-available,,,CassandraSplitEnumerator#prepareSplits() raises  java.lang.NoClassDefFoundError: com/codahale/metrics/Gauge when calling cluster.getMetadata() leading to NPE in CassandraSplitEnumerator#start() async callback. ,,echauchot,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu May 04 08:42:52 UTC 2023,,,,,,,,,,"0|z1hi2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/May/23 12:59;echauchot;This bug has slipped through the ITests because, in the cassandra source tests, the cluster builder is configured using _withoutMetrics()_  . In a real production use case, a user could want to get the Cassandra driver metrics.;;;","02/May/23 13:28;echauchot;Putting the summary of a private discussion with [~chesnay] here: we should only enable these metrics if they are forwarded to the Flink metrics system. So, for now we disable them. We could enable them and implement the metrics forwarding in a follow-up ticket if users ask for this feature.;;;","04/May/23 08:42;echauchot;commits on main:
[275804c445e9c47ab408a108c6583118ea13ec69|https://github.com/apache/flink-connector-cassandra/commit/275804c445e9c47ab408a108c6583118ea13ec69]
[bff833170ae2aa856c409d7a7ab825d088057478|https://github.com/apache/flink-connector-cassandra/commit/bff833170ae2aa856c409d7a7ab825d088057478];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connector weekly runs are only testing main branches instead of all supported branches,FLINK-31923,13533836,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,24/Apr/23 11:47,27/Jun/23 18:06,13/Jul/23 08:29,20/Jun/23 07:47,,,,,,,cassandra-4.0.0,elasticsearch-4.0.0,gcp-pubsub-3.0.2,jdbc-3.2.0,mongodb-1.1.0,opensearch-1.1.0,pulsar-4.0.1,rabbitmq-3.0.2,Build System,Connectors / Common,,,0,pull-request-available,,,"We have a weekly scheduled build for connectors. That's only triggered for the {{main}} branches, because that's how the Github Actions {{schedule}} works, per https://docs.github.com/en/actions/using-workflows/events-that-trigger-workflows#schedule

We can resolve that by having the Github Action flow checkout multiple branches as a matrix to run these weekly tests.",,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32448,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 15 11:11:00 UTC 2023,,,,,,,,,,"0|z1hhrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Jun/23 08:00;martijnvisser;Fixed in:

apache/flink-connector-shared-utils@ci_utils: 7c237004da01ac1d325d37cec5dd2eea84fbd101;;;","15/Jun/23 11:11;martijnvisser;Fixed in:

apache/flink-connector-gcp-pubsub@main: 01e1b6fa0d830aea734201f63c1ee58874d5fa23
apache/flink-connector-rabbitmq@main: 6b70965d331c5f0f94bf08a7defcb7ecf62dbc5c
apache/flink-connector-jdbc@main: bd371d64be644a36b8000ed06c9afa9928cb8fc4
apache/flink-connector-pulsar@main: f463d3f707c8824bc61cce29c0efcdd4de94257e
apache/flink-connector-mongodb@main: fe65806824ae181831b3440f04cbd13bee9af95d
apache/flink-connector-opensearch@main: aa2d57ee7af212757815378ae43eec8536fcde1a
apache/flink-connector-cassandra@main: ac6cf71fb3ed73f2ca1f6414d6e22abc9c756529
apache/flink-connector-elasticsearch@main: be0f30428fec7871644509cd431e088f1d39f390

AWS will be done later, since they are currently blocked on the Maven upgrade;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loss of Idempotence in JsonSerDe Round Trip for AggregateCall and RexNode,FLINK-31917,13533800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,24/Apr/23 08:37,16/May/23 16:31,13/Jul/23 08:29,16/May/23 16:31,1.18.0,,,,,,1.18.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"JsonSerDeTestUtil#testJsonRoundTrip only checks the equality between spec and deserialized object. Some corner cases are detected when serializing the deserialized object again.
{code:java}
static <T> T testJsonRoundTrip(SerdeContext serdeContext, T spec, Class<T> clazz)
            throws IOException {
        String actualJson = toJson(serdeContext, spec);
        T actual = toObject(serdeContext, actualJson, clazz);

        assertThat(actual).isEqualTo(spec);
        assertThat(actualJson).isEqualTo(toJson(serdeContext, actual)); // this will eval some corner cases
        return actual;
    }
{code}
The discovered corner cases are listed as follows.
h5. 1. SerDe for AggregateCall

When deserializing the aggregate call, we should check the JsonNodeType to avoid converting null to ""null"" string.
[https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/serde/AggregateCallJsonDeserializer.java#L64]
h5. Suggested Fix
{code:java}
JsonNode nameNode = jsonNode.required(FIELD_NAME_NAME);
final String name = JsonNodeType.NULL ? null : nameNode.asText();
{code}
h5. 2. SerDe for RexNode

RexNodeJsonSerdeTest#testSystemFunction should create the temporary system function instead of the temporary catalog function.
[https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-table/flink-table-planner/src/test/java/org/apache/flink/table/planner/plan/nodes/exec/serde/RexNodeJsonSerdeTest.java#L209]
h5. Suggested Fix

Use functionCatalog#registerTemporarySystemFunction to test.
h5. 3. About RexLiteral type

RexNodeJsonSerdeTest#testRexNodeSerde has a test spec as follows
{code:java}
//This will create the literal with DOUBLE as the literal type, and DECIMAL as the broad type of this literal. You can refer to Calcite for more details
rexBuilder.makeExactLiteral(BigDecimal.valueOf(Double.MAX_VALUE), FACTORY.createSqlType(SqlTypeName.DOUBLE))
{code}
The RexNodeJsonSerializer uses `typeName`(which is DECIMAL) as the literal's type, as a result, the rel data type is serialized as double, but the value is serialized as a string (in case lost the precision)
[https://github.com/apache/flink/blob/f9b3e0b7bc0432001b4a197539a0712b16e0b33b/flink-table/flink-table-planner/src/main/java/org/apache/flink/table/planner/plan/nodes/exec/serde/RexNodeJsonSerializer.java#L197]

And then, during the deserialization, according to the JSON, the deserialized literal will assign DOUBLE as the literal type and the broad type of the literal.
This will cause the comparison failure
{code:java}
expected: {""kind"": ""LITERAL"", ""value"": ""1.7976931348623157E+308""}
actual: {""kind"": ""LITERAL"", ""value"": 1.7976931348623157E+308}
{code}
h5. Suggested Fix

SARG is a special case and can be coped first, and for the rest type, we can use literal.getType().getSqlTypeName() instead of literal.getTypeName().
{code:java}
// first cope with SARG type
if (literal.getTypeName() == SARG) {
    serializeSargValue(
        (Sarg<?>) value, literal.getType().getSqlTypeName(), gen, serializerProvider);
} else {
    serializeLiteralValue(
        value,
        literal.getType().getSqlTypeName(),
        gen,
        serializerProvider);
}
{code}",,fsk119,godfrey,hackergin,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25217,,,,,FLINK-31791,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 16:31:12 UTC 2023,,,,,,,,,,"0|z1hhjc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Apr/23 10:46;godfrey;good catch [~qingyue] ;;;","04/May/23 02:28;fsk119;Merged into master: 333e023196d90d265c286632f8c01c41b8911ef8;;;","16/May/23 16:31;qingyue;Merged into master: 333e023196d90d265c286632f8c01c41b8911ef8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Failing to close FlinkKafkaInternalProducer created in KafkaWriter with exactly-once semantic results in memory leak,FLINK-31914,13533791,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,datariver,datariver,24/Apr/23 08:24,25/Apr/23 06:19,13/Jul/23 08:29,25/Apr/23 06:19,1.15.0,,,,,,1.15.2,,,,,,,,Connectors / Kafka,,,,0,,,,"Hi [~arvid] , If Exactly-Once writing is enabled, Kafka's transactional writing will be used. KafkaWriter will create FlinkKafkaInternalProducer in the initialization and snapshotState methods, but there is no place to close it. As Checkpoints increase, Producers will continue to accumulate. Each Producer maintains a Buffer, which will cause memory leaks and Job OOM.
By dumping an in-memory instance of Task Manager, you can see that there are a lot of Producers:

!image-2023-04-25-13-47-25-703.png!",,chouc,datariver,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/23 05:47;datariver;image-2023-04-25-13-47-25-703.png;https://issues.apache.org/jira/secure/attachment/13057535/image-2023-04-25-13-47-25-703.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 25 06:19:01 UTC 2023,,,,,,,,,,"0|z1hhhc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 02:09;chouc;{color:#000000}The FlinkKafkaInternalProducer is registered in Closer and closer is closed when Flink Writer invoke close.

{color};;;","25/Apr/23 06:17;datariver;[~chouc] Thanks for the reply, I found that the 1.15.2 version has been fixed.;;;","25/Apr/23 06:19;datariver;相关记录：https://issues.apache.org/jira/browse/FLINK-28250;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Fix some typo in java doc, comments and assertion message",FLINK-31900,13533744,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,24/Apr/23 03:11,28/Apr/23 11:49,13/Jul/23 08:29,25/Apr/23 02:50,,,,,,,1.18.0,,,,,,,,Documentation,,,,0,pull-request-available,,,As the title.,,Feifan Wang,martijnvisser,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 28 11:49:00 UTC 2023,,,,,,,,,,"0|z1hh6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Apr/23 02:50;Weijie Guo;master(1.18) via e21de2dbaeb6b624b3c1f9e5c204743d81841a86.;;;","25/Apr/23 02:50;Feifan Wang;Thanks [~Weijie Guo] for review and merge the PR !;;;","28/Apr/23 11:49;martijnvisser;[~Feifan Wang] Keep in mind for future PRs that documentation fixes like these don't require a Jira. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade ExecNode to new version causes the old serialized plan failed to pass Json SerDe round trip,FLINK-31884,13533673,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,23/Apr/23 06:39,16/May/23 16:29,13/Jul/23 08:29,16/May/23 16:29,1.18.0,,,,,,1.18.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"h4. How to Reproduce

Firstly, add a test to dump the compiled plan JSON.
{code:java}
@Test
public void debug() {
    tableEnv.executeSql(""create table foo (f0 int, f1 string) with ('connector' = 'datagen')"");
    tableEnv.executeSql(""create table bar (f0 int, f1 string) with ('connector' = 'print')"");
    tableEnv.compilePlanSql(""insert into bar select * from foo"")
            .writeToFile(new File(""/path/to/debug.json""));
}
{code}
The JSON context is as follows
{code:json}
{
  ""flinkVersion"" : ""1.18"",
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""stream-exec-table-source-scan_1"",
    ""scanTableSource"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`foo`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""f0"",
              ""dataType"" : ""INT""
            }, {
              ""name"" : ""f1"",
              ""dataType"" : ""VARCHAR(2147483647)""
            } ],
            ""watermarkSpecs"" : [ ]
          },
          ""partitionKeys"" : [ ],
          ""options"" : {
            ""connector"" : ""datagen""
          }
        }
      }
    },
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""TableSourceScan(table=[[default_catalog, default_database, foo]], fields=[f0, f1])"",
    ""inputProperties"" : [ ]
  }, {
    ""id"" : 2,
    ""type"" : ""stream-exec-sink_1"",
    ""configuration"" : {
      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    },
    ""dynamicTableSink"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`bar`"",
        ""resolvedTable"" : {
          ""schema"" : {
            ""columns"" : [ {
              ""name"" : ""f0"",
              ""dataType"" : ""INT""
            }, {
              ""name"" : ""f1"",
              ""dataType"" : ""VARCHAR(2147483647)""
            } ],
            ""watermarkSpecs"" : [ ]
          },
          ""partitionKeys"" : [ ],
          ""options"" : {
            ""connector"" : ""print""
          }
        }
      }
    },
    ""inputChangelogMode"" : [ ""INSERT"" ],
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""Sink(table=[default_catalog.default_database.bar], fields=[f0, f1])""
  } ],
  ""edges"" : [ {
    ""source"" : 1,
    ""target"" : 2,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  } ]
}
{code}
Then upgrade the StreamExecSink to a new version
{code:java}
@ExecNodeMetadata(
        name = ""stream-exec-sink"",
        version = 1,
        consumedOptions = {
            ""table.exec.sink.not-null-enforcer"",
            ""table.exec.sink.type-length-enforcer"",
            ""table.exec.sink.upsert-materialize"",
            ""table.exec.sink.keyed-shuffle""
        },
        producedTransformations = {
            CommonExecSink.CONSTRAINT_VALIDATOR_TRANSFORMATION,
            CommonExecSink.PARTITIONER_TRANSFORMATION,
            CommonExecSink.UPSERT_MATERIALIZE_TRANSFORMATION,
            CommonExecSink.TIMESTAMP_INSERTER_TRANSFORMATION,
            CommonExecSink.SINK_TRANSFORMATION
        },
        minPlanVersion = FlinkVersion.v1_15,
        minStateVersion = FlinkVersion.v1_15)
@ExecNodeMetadata(
        name = ""stream-exec-sink"",
        version = 2,
        consumedOptions = {
            ""table.exec.sink.not-null-enforcer"",
            ""table.exec.sink.type-length-enforcer"",
            ""table.exec.sink.upsert-materialize"",
            ""table.exec.sink.keyed-shuffle""
        },
        producedTransformations = {
            CommonExecSink.CONSTRAINT_VALIDATOR_TRANSFORMATION,
            CommonExecSink.PARTITIONER_TRANSFORMATION,
            CommonExecSink.UPSERT_MATERIALIZE_TRANSFORMATION,
            CommonExecSink.TIMESTAMP_INSERTER_TRANSFORMATION,
            CommonExecSink.SINK_TRANSFORMATION
        },
        minPlanVersion = FlinkVersion.v1_18,
        minStateVersion = FlinkVersion.v1_15)
public class StreamExecSink extends CommonExecSink implements StreamExecNode<Object> {
}
{code}
And then load the previous plan and print it as JSON text
{code:java}
tableEnv.loadPlan(PlanReference.fromFile(""/path/to/debug.json"")).printJsonString();
{code}
The SerDe lost idempotence since the version for StreamExecSink became version 2.
{code:json}
{
  ""flinkVersion"" : ""1.18"",
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""stream-exec-table-source-scan_1"",
    ""scanTableSource"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`foo`""
      }
    },
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""TableSourceScan(table=[[default_catalog, default_database, foo]], fields=[f0, f1])"",
    ""inputProperties"" : [ ]
  }, {
    ""id"" : 2,
    ""type"" : ""stream-exec-sink_2"",
    ""configuration"" : {
      ""table.exec.sink.keyed-shuffle"" : ""AUTO"",
      ""table.exec.sink.not-null-enforcer"" : ""ERROR"",
      ""table.exec.sink.type-length-enforcer"" : ""IGNORE"",
      ""table.exec.sink.upsert-materialize"" : ""AUTO""
    },
    ""dynamicTableSink"" : {
      ""table"" : {
        ""identifier"" : ""`default_catalog`.`default_database`.`bar`""
      }
    },
    ""inputChangelogMode"" : [ ""INSERT"" ],
    ""inputProperties"" : [ {
      ""requiredDistribution"" : {
        ""type"" : ""UNKNOWN""
      },
      ""damBehavior"" : ""PIPELINED"",
      ""priority"" : 0
    } ],
    ""outputType"" : ""ROW<`f0` INT, `f1` VARCHAR(2147483647)>"",
    ""description"" : ""Sink(table=[default_catalog.default_database.bar], fields=[f0, f1])""
  } ],
  ""edges"" : [ {
    ""source"" : 1,
    ""target"" : 2,
    ""shuffle"" : {
      ""type"" : ""FORWARD""
    },
    ""shuffleMode"" : ""PIPELINED""
  } ]
}
{code}
h4. Root Cause

ExecNodeBase#getContextFromAnnotation always uses the newest ExecNode version for SerDe. As a result, although the deserialized CompilePlan object is correct, #printAsJson will create a new context with the newest version.
 
h4. Suggested Fix

If the member variable `isCompiled` is true, then #getContextFromAnnotation should return the context which reads from the JSON plan instead of instantiating a new one.
 ",,godfrey,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25217,,,,,FLINK-31791,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 16:29:40 UTC 2023,,,,,,,,,,"0|z1hgs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Apr/23 03:29;godfrey;Fixed in master: 3664609c7622ccae80e36e85099a1b79b5935fe9;;;","16/May/23 16:29;qingyue;Fixed in master: 3664609c7622ccae80e36e85099a1b79b5935fe9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGateway will throw exception when executing DeleteFromFilterOperation,FLINK-31882,13533666,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,yzl,yzl,23/Apr/23 02:30,17/May/23 01:42,13/Jul/23 08:29,17/May/23 01:42,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Table SQL / API,Table SQL / Gateway,,,0,pull-request-available,,,"Reproduce step:

Our sink implements `SupportsDeletePushDown`, so when we test a DELETE statement, the `TableEnvironmentImpl` will call the 
`TableResultInternal executeInternal(DeleteFromFilterOperation deleteFromFilterOperation)` at line 895. This method won't return the JobClient, but the SqlGateway requires one, thus a exception occurs.
Stack:
Caused by: org.apache.flink.table.gateway.service.utils.SqlExecutionException: Can't get job client for the operation d4ba1029-664c-44c0-922b-021eb9e1c527.
at org.apache.flink.table.gateway.service.operation.OperationExecutor.lambda$callModifyOperations$6(OperationExecutor.java:521)
at java.base/java.util.Optional.orElseThrow(Optional.java:408)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.callModifyOperations(OperationExecutor.java:518)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeOperation(OperationExecutor.java:431)
at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:200)
at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:119)
at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:258)",,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 17 01:42:07 UTC 2023,,,,,,,,,,"0|z1hgqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/23 02:38;luoyuxia;[~yzl] Thanks for reporting. I'll try to fix it.;;;","17/May/23 01:42;luoyuxia;master:

886dda2938c7fccfeb68ecbadc9c926124c42352

1.17:

ad7b6b45b494a32667453e69e9bf6c0c1e52ec0c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the wrong name of PauseOrResumeSplitsTask#toString in connector fetcher ,FLINK-31878,13533513,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,tanyuxin,tanyuxin,21/Apr/23 07:51,14/Jun/23 11:23,13/Jul/23 08:29,21/Apr/23 17:23,1.18.0,,,,,,1.18.0,,,,,,,,Connectors / Common,,,,0,pull-request-available,,,The class name PauseOrResumeSplitsTask#toString is not right. Users will be very confused when calling the toString method of the class. So we should fix it.,,mapohl,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31838,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jun 14 11:23:16 UTC 2023,,,,,,,,,,"0|z1hfsg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/23 17:23;Weijie Guo;master(1.18) via 0104427dc9e38e898ba3865b499cc515004041c9.;;;","14/Jun/23 11:23;mapohl;I accidentally added FLINK-31878 as the Jira issue to the message of a commit that actually fixes FLINK-31838. I'm linking the two tickets here to make this visible in Jira.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add setMaxParallelism to the DataStreamSink Class,FLINK-31873,13533458,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,eric.xiao,eric.xiao,eric.xiao,20/Apr/23 20:24,28/Apr/23 14:35,13/Jul/23 08:29,28/Apr/23 14:35,1.18.0,,,,,,1.18.0,,,,,,,,API / DataStream,,,,0,pull-request-available,,,"When turning on Flink reactive mode, it is suggested to convert all {{setParallelism}} calls to {{setMaxParallelism}} from [elastic scaling docs|https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/deployment/elastic_scaling/#configuration].

With the current implementation of the {{DataStreamSink}} class, only the {{[setParallelism|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/datastream/DataStreamSink.java#L172-L181]}} function of the {{[Transformation|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/dag/Transformation.java#L248-L285]}} class is exposed - {{Transformation}} also has the {{[setMaxParallelism|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/dag/Transformation.java#L277-L285]}} function which is not exposed.

 

This means for any sink in the Flink pipeline, we cannot set a max parallelism.",,eric.xiao,huwh,luoyuxia,martijnvisser,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,FLINK-21711,,,,,,,,,,,,,,"20/Apr/23 20:33;eric.xiao;Screenshot 2023-04-20 at 4.33.14 PM.png;https://issues.apache.org/jira/secure/attachment/13057461/Screenshot+2023-04-20+at+4.33.14+PM.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 28 14:35:34 UTC 2023,,,,,,,,,,"0|z1hfgg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 20:35;eric.xiao;I have an open PR to address this issue: https://github.com/apache/flink/pull/22438;;;","21/Apr/23 01:27;luoyuxia;[~eric.xiao] Thanks for raising it. Since it add a new public api, it must need a FLIP before we review it.;;;","21/Apr/23 07:38;martijnvisser;I've downgraded this to a Major, since this can't be considered a release blocker. I would recommend opening a discussion on the Dev mailing list first;;;","21/Apr/23 14:12;eric.xiao;Thanks [~martijnvisser] and [~luoyuxia], I will start with opening up a thread in the Dev mailing list before making a FLIP :).;;;","28/Apr/23 14:35;Weijie Guo;master(1.18) via 7a1c2b5750d9576f09023cf2810ed965072b699e.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
test_multi_sessionjob.sh gets stuck very frequently,FLINK-31869,13533409,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,gyfora,gyfora,gyfora,20/Apr/23 14:25,21/Apr/23 12:54,13/Jul/23 08:29,21/Apr/23 12:54,,,,,,,kubernetes-operator-1.5.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,The test_multi_sessionjob.sh gets stuck almost all the time on recent builds.,,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 12:54:34 UTC 2023,,,,,,,,,,"0|z1hf5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/23 12:54;gyfora;Merged to main 1f54ffa484c359c4b81a409f27092dbfba82157b

There are still frequent test failures but at least the CI doesnt get stuck for hours;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix DefaultInputSplitAssigner javadoc for class,FLINK-31868,13533388,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,pvary,pvary,pvary,20/Apr/23 11:40,21/Apr/23 09:31,13/Jul/23 08:29,21/Apr/23 06:05,1.18.0,,,,,,1.18.0,,,,,,,,API / Core,,,,0,pull-request-available,,,"Based on the discussion[1] on the mailing list {{there
is no requirement of the order of splits by Flink itself}}, we should fix the discrepancy between the code and the comment by updating the comment.

 

[[1] https://lists.apache.org/thread/74m7z2kzgpzylhrp1oq4lz37pnqjmbkh|https://lists.apache.org/thread/74m7z2kzgpzylhrp1oq4lz37pnqjmbkh]

 ",,pvary,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 06:05:59 UTC 2023,,,,,,,,,,"0|z1hf0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Apr/23 06:05;zhuzh;Fixed via f240a5110a0d28473b534cf377d287d5d072a93e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Autoscaler metric trimming reduces the number of metric observations on recovery,FLINK-31866,13533384,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,20/Apr/23 11:25,27/Apr/23 15:15,13/Jul/23 08:29,27/Apr/23 15:15,,,,,,,kubernetes-operator-1.5.0,,,,,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"The autoscaler uses a ConfigMap to store past metric observations which is used to re-initialize the autoscaler state in case of failures or upgrades.

Whenever trimming of the ConfigMap occurs, we need to make sure we also update the timestamp for the start of the metric collection, so any removed observations can be compensated with by collecting new ones. If we don't do this, the metric window will effectively shrink due to removing observations.

This can lead to triggering scaling decisions when the operator gets redeployed due to the removed items.",,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-04-20 11:25:30.0,,,,,,,,,,"0|z1hf00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException in operators.window.slicing.SliceAssigners$AbstractSliceAssigner.assignSliceEnd,FLINK-31840,13533036,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,rmetzger,rmetzger,18/Apr/23 11:55,18/Apr/23 14:24,13/Jul/23 08:29,18/Apr/23 11:57,,,,,,,1.16.0,,,,,,,,Table SQL / Runtime,,,,0,,,,"While running a Flink SQL Query (with a hop window), I got this error.
{code}
Caused by: org.apache.flink.streaming.runtime.tasks.ExceptionInChainedOperatorException: Could not forward element to next operator
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:99)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at StreamExecCalc$11.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	... 23 more
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.runtime.operators.window.slicing.SliceAssigners$AbstractSliceAssigner.assignSliceEnd(SliceAssigners.java:558)
	at org.apache.flink.table.runtime.operators.aggregate.window.LocalSlicingWindowAggOperator.processElement(LocalSlicingWindowAggOperator.java:114)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	... 29 more
{code}

It was caused by a timestamp field containing NULL values.",,lincoln.86xy,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 18 14:24:06 UTC 2023,,,,,,,,,,"0|z1hcuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 11:55;rmetzger;This issue was fixed in https://github.com/apache/flink/pull/20302/files;;;","18/Apr/23 11:56;rmetzger;Note: I've filed this ticket just for tracking purposes, because I couldn't find any information about this error on the internet. Now the problem and solution is at least publicly available.;;;","18/Apr/23 12:49;lincoln.86xy;[~rmetzger]thanks for creating this ticket! Now the issue can be searched more easily (it reminds me that it's really more important for users to create a jira rather than just a hotfix of #20302);;;","18/Apr/23 14:24;rmetzger;Yeah, I wanted to mention the stack trace and fixed version somewhere, Jira is ideal for that.

Ideally, your PR should have included a test case to make sure nobody is breaking this in the future.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Token delegation fails when both flink-s3-fs-hadoop and flink-s3-fs-presto plugins are used,FLINK-31839,13533035,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,18/Apr/23 11:52,20/Apr/23 13:38,13/Jul/23 08:29,19/Apr/23 16:50,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,FileSystems,,,,0,pull-request-available,,,"{code:java}
2023-04-07 09:18:32,814 [main] ERROR
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager [] -
Failed to initialize delegation token provider s3
java.lang.IllegalStateException: Delegation token provider with service
name {} has multiple implementations [s3]
at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$loadProviders$0(DefaultDelegationTokenManager.java:133)
~[flink-dist-1.17.0.jar:1.17.0]
at java.util.Iterator.forEachRemaining(Unknown Source) ~[?:?]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.loadProviders(DefaultDelegationTokenManager.java:156)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.<init>(DefaultDelegationTokenManager.java:111)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerFactory.create(DefaultDelegationTokenManagerFactory.java:50)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:392)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282)
~[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
~[flink-dist-1.17.0.jar:1.17.0]
at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
at javax.security.auth.Subject.doAs(Unknown Source) [?:?]
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
[hadoop-common-2.8.5.jar:?]
at
org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729)
[flink-dist-1.17.0.jar:1.17.0]
at
org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint.main(StandaloneApplicationClusterEntryPoint.java:82)
[flink-dist-1.17.0.jar:1.17.0]
2023-04-07 09:18:32,824 [main] INFO
org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting
StandaloneApplicationClusterEntryPoint down with application status FAILED.
Diagnostics org.apache.flink.util.FlinkRuntimeException:
java.lang.IllegalStateException: Delegation token provider with service
name {} has multiple implementations [s3]
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$loadProviders$0(DefaultDelegationTokenManager.java:151)
at java.base/java.util.Iterator.forEachRemaining(Unknown Source)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.loadProviders(DefaultDelegationTokenManager.java:156)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.<init>(DefaultDelegationTokenManager.java:111)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManagerFactory.create(DefaultDelegationTokenManagerFactory.java:50)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.initializeServices(ClusterEntrypoint.java:392)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:282)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
at java.base/java.security.AccessController.doPrivileged(Native Method)
at java.base/javax.security.auth.Subject.doAs(Unknown Source)
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1844)
at
org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
at
org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runClusterEntrypoint(ClusterEntrypoint.java:729)
at
org.apache.flink.container.entrypoint.StandaloneApplicationClusterEntryPoint.main(StandaloneApplicationClusterEntryPoint.java:82)
Caused by: java.lang.IllegalStateException: Delegation token provider with
service name {} has multiple implementations [s3]
at org.apache.flink.util.Preconditions.checkState(Preconditions.java:215)
at
org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$loadProviders$0(DefaultDelegationTokenManager.java:133)
... 14 more
{code}
",,gaborgsomogyi,KristoffSC,martijnvisser,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 16:50:36 UTC 2023,,,,,,,,,,"0|z1hcug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 12:17;mbalassi;839b3b9 in master;;;","19/Apr/23 16:50;mbalassi;af4c68a in release-1.17.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DataTypeHint don't support Row<i Array<int>>,FLINK-31835,13533004,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,aitozi,jeff-zou,jeff-zou,18/Apr/23 09:10,12/Jun/23 09:53,13/Jul/23 08:29,12/Jun/23 09:53,1.15.4,,,,,,1.18.0,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,"Using DataTypeHint(""Row<t ARRAY<INT>>"") in a UDF gives the following error:

 
{code:java}
Caused by: java.lang.ClassCastException: class [I cannot be cast to class [Ljava.lang.Object; ([I and [Ljava.lang.Object; are in module java.base of loader 'bootstrap')
org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.toInternal(ArrayObjectArrayConverter.java:40)
org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:75)
org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:37)
org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
StreamExecCalc$251.processElement_split9(Unknown Source)
StreamExecCalc$251.processElement(Unknown Source)
org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82) {code}
 

The function is as follows:
{code:java}
@DataTypeHint(""Row<t ARRAY<INT>>"")
public Row eval() {
int[] i = new int[3];
return Row.of(i);
} {code}
 

This error is not reported when testing other simple types, so it is not an environmental problem.",,aitozi,jark,jeff-zou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 12 09:53:12 UTC 2023,,,,,,,,,,"0|z1hcnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/23 04:02;jark;Hi [~jeff-zou], thanks for reporting this. I think this is expected. You should declare the type hint as ""ARRAY<INT NOT NULL>"" to map to {{int[]}}. ""ARRAY<INT>"" is recognized as a result of {{Integer[]}}. That's why the conversion class is {{ArrayObjectArrayConverter}}. 

But I think the exception message can be improved. ;;;","23/Apr/23 14:14;aitozi;In this case, ARRAY<INT NOT NULL> will still maps to {{ArrayObjectArrayConverter}}. I think it's not expected;;;","23/Apr/23 14:31;aitozi;When creating Array CollectionDataType it will use the array's element type to construct an array conversion class. But the conversionClass is {{Integer}} for both  {{INT NOT NULL}} and {{INT}}. So the conversion class for Array<INT NOT NULL> will become {{Integer[]}}


{code:java}
if (logicalType.getTypeRoot() == LogicalTypeRoot.ARRAY && clazz == null) {
    return Array.newInstance(elementDataType.getConversionClass(), 0).getClass();
}
{code}


;;;","23/Apr/23 15:13;aitozi;The first thought in my mind is that the conversion class for atomic type eg: {{IntType}} should respect to the nullability. So, when a {{IntType}} copy from nullable to not null. Its default conversionClass will change from {{Integer}} to {{int}}.

And in the DataType: {{AtomicType}} and {{CollectionDataType}} should also respect to the nullable and notNull call. The conversionClass of the dataType should be changed after these call.

I have verified this locally, it can solve this problem, what do you think this solution [~jark] ?;;;","24/Apr/23 09:05;jark;This sounds good to me. My only concern is the compatibility problem. What do you think [~twalthr]? Is there any potential problems?;;;","25/Apr/23 08:00;jeff-zou;hi [~jark] [~aitozi] 

Under Flink 1.15, how do I solve this problem?  Even I have change the hint to Array<int not null>, it's still reporting this error:
{code:java}
Caused by: java.lang.ClassCastException: [I cannot be cast to [Ljava.lang.Object;
    at org.apache.flink.table.data.conversion.ArrayObjectArrayConverter.toInternal(ArrayObjectArrayConverter.java:40)
    at org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:75)
    at org.apache.flink.table.data.conversion.RowRowConverter.toInternal(RowRowConverter.java:37)
    at org.apache.flink.table.data.conversion.DataStructureConverter.toInternalOrNull(DataStructureConverter.java:61)
    at StreamExecCalc$278.processElement_trueFilter1_split13(Unknown Source)
    at StreamExecCalc$278.processElement_trueFilter1(Unknown Source)
    at StreamExecCalc$278.processElement_split3(Unknown Source) {code};;;","25/Apr/23 10:25;aitozi;Yes, the reason is shown above. I have pushed a PR to try to solve this. But it needs some discussion to avoid break the compatibility. ;;;","26/Apr/23 05:39;aitozi;[~jark] The PR have passed the CI and I think the current solution will not cause compatibility problem by only fix the conversion class according to the nullability when creating CollectionDataType, could you help review that ?;;;","12/Jun/23 09:53;jark;Fixed in master: a6adbdda0cdf90635f0cd7a3427486bced301fbd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Azure Warning: no space left on device,FLINK-31834,13533001,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rmetzger,rmetzger,rmetzger,18/Apr/23 08:17,05/May/23 06:47,13/Jul/23 08:29,19/Apr/23 10:08,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,Build System / Azure Pipelines,,,,0,build-stability,pull-request-available,,"In this CI run: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=841082b6-1a93-5908-4d37-a071f4387a5f&l=21

There was this warning:
{code}
Loaded image: confluentinc/cp-kafka:6.2.2
Loaded image: testcontainers/ryuk:0.3.3
ApplyLayer exit status 1 stdout:  stderr: write /opt/jdk-15.0.1+9/lib/modules: no space left on device
##[error]Bash exited with code '1'.
Finishing: Restore docker images
{code}",,mapohl,rmetzger,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 05 06:47:32 UTC 2023,,,,,,,,,,"0|z1hcmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 14:39;rmetzger;Looking closer at this issue, I notice:

a) this message didn't fail the build. Probably the caching didn't properly work
b) the cleanup script runs after this caching step. It reports 3.9 GB of free disk space: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=affb2083-df4e-5398-e502-35356824fd45&l=290 After the cleanup script. 32GB of disk space is available: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48213&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=affb2083-df4e-5398-e502-35356824fd45&l=1804

It looks like I can move the cleanup script before the caching. I'll try that.;;;","19/Apr/23 10:08;rmetzger;Merged to master in https://github.com/apache/flink/commit/15c4d88eb78cb8f702e7e56563cb10e49d424b14;;;","20/Apr/23 18:17;Sergey Nuyanzin;1.16: https://github.com/apache/flink/commit/1e2c2e69590d7a55e68bcb87ab0d1af1894a8ae5;;;","24/Apr/23 14:14;mapohl;[~rmetzger] [~snuyanzin] is there a reason why we backported the fix to 1.16 but not 1.17?;;;","05/May/23 06:47;rmetzger;No, I think that's just an oversight.
I've just pushed to 1.17: https://github.com/apache/flink/commit/91dfb22e0bc7ac10a9a9f59cd9da6d62a723dadd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure is unstable,FLINK-31831,13532973,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wanglijie,Sergey Nuyanzin,Sergey Nuyanzin,18/Apr/23 05:40,26/Apr/23 06:32,13/Jul/23 08:29,26/Apr/23 06:32,1.18.0,,,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48212&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=8399

{noformat}
Apr 18 04:17:09 [ERROR] org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure  Time elapsed: 2.844 s  <<< FAILURE!
Apr 18 04:17:09 java.lang.AssertionError: Failed to initialize the cluster entrypoint .
Apr 18 04:17:09 	at org.junit.Assert.fail(Assert.java:89)
Apr 18 04:17:09 	at org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure(TaskManagerDisconnectOnShutdownITCase.java:136)
Apr 18 04:17:09 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Apr 18 04:17:09 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Apr 18 04:17:09 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)


{noformat}",,Sergey Nuyanzin,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 26 06:32:06 UTC 2023,,,,,,,,,,"0|z1hch4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Apr/23 06:42;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48358&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10562;;;","24/Apr/23 05:20;wanglijie;{code:java}
04:17:08,719 [ForkJoinPool-1-worker-1] INFO  org.apache.flink.runtime.entrypoint.ClusterEntrypoint        [] - Shutting  down with application status FAILED. Diagnostics org.apache.flink.util.FlinkException: Could not create the DispatcherResourceManagerComponent.
  at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:288)
  at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.runCluster(ClusterEntrypoint.java:293)
  at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.lambda$startCluster$1(ClusterEntrypoint.java:232)
  at java.security.AccessController.doPrivileged(Native Method)
  at javax.security.auth.Subject.doAs(Subject.java:422)
  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1938)
  at org.apache.flink.runtime.security.contexts.HadoopSecurityContext.runSecured(HadoopSecurityContext.java:41)
  at org.apache.flink.runtime.entrypoint.ClusterEntrypoint.startCluster(ClusterEntrypoint.java:229)
  at org.apache.flink.test.recovery.TaskManagerDisconnectOnShutdownITCase.testTaskManagerProcessFailure(TaskManagerDisconnectOnShutdownITCase.java:117)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
  at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
  at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
  at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
  at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
  at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
  at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
  at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
  at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
  at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService.invokeAll(ForkJoinPoolHierarchicalTestExecutorService.java:129)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:155)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
  at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
  at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
  at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
  at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
  at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
  at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
  at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
  at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
  at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.net.BindException: Could not start rest endpoint on any port in port range 8081
  at org.apache.flink.runtime.rest.RestServerEndpoint.start(RestServerEndpoint.java:281)
  at org.apache.flink.runtime.entrypoint.component.DefaultDispatcherResourceManagerComponentFactory.create(DefaultDispatcherResourceManagerComponentFactory.java:177)
  ... 65 more
{code}

This failure is caused by port bind exception. I will prepare a fix PR soon to use random port for this case.;;;","26/Apr/23 06:32;wanglijie;Fixed via:
master: aa2110fdb23db1f7b927c31c51b18bf1e01a1f5f
release-1.17: 1651908f02b7990e25d7fdde428aac44473cde65
release-1.16: 5eb7188f732c9091e0f8322acae77e4bbd806343;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RestHandlerConfigurationTest.testWebRescaleFeatureFlagWithReactiveMode is unstable,FLINK-31823,13532865,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,Sergey Nuyanzin,Sergey Nuyanzin,17/Apr/23 11:22,18/Apr/23 13:30,13/Jul/23 08:29,18/Apr/23 13:30,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Web Frontend,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48177&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8509]

{noformat}
Apr 16 01:15:08 [ERROR] Failures: 
Apr 16 01:15:08 [ERROR]   RestHandlerConfigurationTest.testWebRescaleFeatureFlagWithReactiveMode:84 
Apr 16 01:15:08 expected: false
Apr 16 01:15:08  but was: true

{noformat}",,dmvk,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31471,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 18 13:30:34 UTC 2023,,,,,,,,,,"0|z1hbt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 11:24;Sergey Nuyanzin;// cc [~dmvk] since test were added within  FLINK-31471 ;;;","17/Apr/23 11:26;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48164&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8434;;;","18/Apr/23 05:30;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48212&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8509;;;","18/Apr/23 08:34;dmvk;This did not appear on PR because we only smoke test AdaptiveScheduler against the regular test suite on scheduled builds against the master branch.;;;","18/Apr/23 13:30;dmvk;master: 1d9f12a9b40c74fac83a84ee4e7e7d4e9fd52e00;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
parsing error of 'security.kerberos.access.hadoopFileSystems' in flink-conf.yaml,FLINK-31818,13532827,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,seung-min,seung-min,seung-min,17/Apr/23 05:57,17/Apr/23 15:17,13/Jul/23 08:29,17/Apr/23 15:17,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Runtime / Configuration,,,,0,bug,pull-request-available,,"There is a parsing error when I gave two or more hdfs namenodes URI separated by commas as the value of key attribute 'security.kerberos.access.hadoopFileSystems'.

 

For example, I set this key attribute and value like below in flink-conf.yaml,
{code:java}
security.kerberos.access.hadoopFileSystems: hdfs://hadoop-nn1.testurl.com:8020,hdfs://hadoop-nn2.testurl.com:8020 {code}
 

then, the slash ""/"" is missing in second URI in parsed value
{code:java}
hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020{code}
 

 

Received error message is here.
{code:java}
Caused by: org.apache.flink.util.FlinkRuntimeException: java.io.IOException: Incomplete HDFS URI, no host: hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$getFileSystemsToAccess$2(HadoopFSDelegationTokenProvider.java:168) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_362]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.getFileSystemsToAccess(HadoopFSDelegationTokenProvider.java:157) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$obtainDelegationTokens$1(HadoopFSDelegationTokenProvider.java:113) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_362]
   at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_362]
   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1966) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.java:108) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$obtainDelegationTokensAndGetNextRenewal$1(DefaultDelegationTokenManager.java:228) ~[flink-dist-1.17.0.jar:1.17.0]
   ... 13 more
Caused by: java.io.IOException: Incomplete HDFS URI, no host: hdfs://hadoop-nn1.testurl.com:8020,hdfs:/hadoop-nn2.testurl.com:8020
   at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:156) ~[hadoop-hdfs-client-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3241) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:122) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3290) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3258) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:471) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$getFileSystemsToAccess$2(HadoopFSDelegationTokenProvider.java:163) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.util.ArrayList.forEach(ArrayList.java:1259) ~[?:1.8.0_362]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.getFileSystemsToAccess(HadoopFSDelegationTokenProvider.java:157) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.lambda$obtainDelegationTokens$1(HadoopFSDelegationTokenProvider.java:113) ~[flink-dist-1.17.0.jar:1.17.0]
   at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_362]
   at javax.security.auth.Subject.doAs(Subject.java:422) ~[?:1.8.0_362]
   at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1966) ~[hadoop-common-2.10.0-khp-20210414.jar:?]
   at org.apache.flink.runtime.security.token.hadoop.HadoopFSDelegationTokenProvider.obtainDelegationTokens(HadoopFSDelegationTokenProvider.java:108) ~[flink-dist-1.17.0.jar:1.17.0]
   at org.apache.flink.runtime.security.token.DefaultDelegationTokenManager.lambda$obtainDelegationTokensAndGetNextRenewal$1(DefaultDelegationTokenManager.java:228) ~[flink-dist-1.17.0.jar:1.17.0]
   ... 13 more {code}
 ",,gaborgsomogyi,JunRuiLi,seung-min,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 17 15:09:01 UTC 2023,,,,,,,,,,"0|z1hbkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 06:16;JunRuiLi;[~seung-min] I think this issue is due to the type of security.kerberos.access.hadoopFileSystems being ConfigOption<List<String>>, which requires "";"" as the separator instead of "","" for the value of the configuration option. You can retry with ""hdfs://hadoop-nn1.testurl.com:8020;hdfs://hadoop-nn2.testurl.com:8020"" as the value. If this modification is correct, would you be willing to create a pr to correct the documentation? If not, I am happy to do it.;;;","17/Apr/23 06:54;seung-min;[~JunRuiLi] 

Oh, I will test, and if this modification is correct than i will create pr to correct the documentation  :D;;;","17/Apr/23 07:17;seung-min;[~JunRuiLi]

It works

I will create pr!;;;","17/Apr/23 08:11;gaborgsomogyi;Yeah, the doc is wrong. Thanks for fixing it(y);;;","17/Apr/23 15:09;Weijie Guo;master(1.18) via fe8ff46123ee2f9278066c801228416e38e2cbd4.
release-1.17 via fa2263d1de1789a5200f909333cfe2dc30104b2f.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SavePoint from /jars/:jarid:/run api on body is not anymore set to null if empty,FLINK-31812,13532664,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,nfraison.datadog,nfraison.datadog,nfraison.datadog,14/Apr/23 13:26,07/Jun/23 03:02,13/Jul/23 08:29,07/Jun/23 03:02,1.17.0,,,,,,1.17.2,1.18.0,,,,,,,,,,,0,pull-request-available,,,"Since https://issues.apache.org/jira/browse/FLINK-29543 the 
savepointPath from the body is not anymore transform to null if empty: [https://github.com/apache/flink/pull/21012/files#diff-c6d9a43d970eb07642a87e4bf9ec6a9dc7d363b1b5b557ed76f73d8de1cc5a54R145]
 
This leads to issue running a flink job in release 1.17 with lyft operator which set savePoint in body to empty string: [https://github.com/lyft/flinkk8soperator/blob/master/pkg/controller/flinkapplication/flink_state_machine.go#L721]
 
Issue faced by the job as the savepointPath is setto empty string:
{code:java}
org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster.
3	at org.apache.flink.runtime.jobmaster.DefaultJobMasterServiceProcess.lambda$new$0(DefaultJobMasterServiceProcess.java:97)
4	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
5	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
6	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
7	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1705)
8	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
9	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
10	at java.base/java.lang.Thread.run(Thread.java:829)
11Caused by: java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: empty checkpoint pointer
12	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:314)
13	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:319)
14	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1702)
15	... 3 more
16Caused by: java.lang.IllegalArgumentException: empty checkpoint pointer
17	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
18	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpointPointer(AbstractFsCheckpointStorageAccess.java:240)
19	at org.apache.flink.runtime.state.filesystem.AbstractFsCheckpointStorageAccess.resolveCheckpoint(AbstractFsCheckpointStorageAccess.java:136)
20	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.restoreSavepoint(CheckpointCoordinator.java:1824)
21	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.tryRestoreExecutionGraphFromSavepoint(DefaultExecutionGraphFactory.java:223)
22	at org.apache.flink.runtime.scheduler.DefaultExecutionGraphFactory.createAndRestoreExecutionGraph(DefaultExecutionGraphFactory.java:198)
23	at org.apache.flink.runtime.scheduler.SchedulerBase.createAndRestoreExecutionGraph(SchedulerBase.java:365)
24	at org.apache.flink.runtime.scheduler.SchedulerBase.<init>(SchedulerBase.java:210)
25	at org.apache.flink.runtime.scheduler.DefaultScheduler.<init>(DefaultScheduler.java:136)
26	at org.apache.flink.runtime.scheduler.DefaultSchedulerFactory.createInstance(DefaultSchedulerFactory.java:152)
27	at org.apache.flink.runtime.jobmaster.DefaultSlotPoolServiceSchedulerFactory.createScheduler(DefaultSlotPoolServiceSchedulerFactory.java:119)
28	at org.apache.flink.runtime.jobmaster.JobMaster.createScheduler(JobMaster.java:371)
29	at org.apache.flink.runtime.jobmaster.JobMaster.<init>(JobMaster.java:348)
30	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.internalCreateJobMasterService(DefaultJobMasterServiceFactory.java:123)
31	at org.apache.flink.runtime.jobmaster.factories.DefaultJobMasterServiceFactory.lambda$createJobMasterService$0(DefaultJobMasterServiceFactory.java:95)
32	at org.apache.flink.util.function.FunctionUtils.lambda$uncheckedSupplier$4(FunctionUtils.java:112)
33	at java.base/java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1700)
34	... 3 more
35 {code}",,ConradJam,nfraison.datadog,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 06 13:58:50 UTC 2023,,,,,,,,,,"0|z1hal4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 09:57;Weijie Guo;IIUC, It seems that this [change|https://github.com/apache/flink/pull/21012/files#diff-c6d9a43d970eb07642a87e4bf9ec6a9dc7d363b1b5b557ed76f73d8de1cc5a54R145] in FLINK-29543 is not unnecessary. 
cc [~ConradJam] for more context.;;;","21/Apr/23 08:24;ConradJam; 
 emptyToNull is missing here, and honestly I think it would be nice to have a check here, if the Savepoint path is empty until the end, should we throw an exception? Now there doesn't seem to be a mechanism for that, so it just keeps going down

cc [~nfraison.datadog] ;;;","21/Apr/23 08:36;Weijie Guo;Thanks [~ConradJam] for the confirm! [~nfraison.datadog] you are assigned.;;;","06/Jun/23 13:58;Weijie Guo;master(1.18) via bc2fee44a87591d08ab0df6cba06bbf9ac065095.
release-1.17 via c003709916540aa272e15a4c8df4374f594ebad7.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
wrong examples of how to set operator name  in documents,FLINK-31808,13532639,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,huwh,huwh,14/Apr/23 09:26,15/Apr/23 05:14,13/Jul/23 08:29,15/Apr/23 05:14,,,,,,,1.18.0,,,,,,,,Documentation,,,,0,pull-request-available,,,"[https://nightlies.apache.org/flink/flink-docs-master/docs/dev/datastream/operators/overview/#name-and-description]

 
{code:java}
.setName(""filter""){code}
 should be
{code:java}
.name(""filter""){code}",,huwh,libenchao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 15 05:14:12 UTC 2023,,,,,,,,,,"0|z1hafk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Apr/23 05:14;libenchao;Fixed via https://github.com/apache/flink/commit/97dee4bd2ade278805241a245385df3ceeb90150 (1.18.0)

[~huwh] Thanks for your contribution!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra Source shouldn't use IOUtils,FLINK-31805,13532620,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,14/Apr/23 08:11,14/Apr/23 10:28,13/Jul/23 08:29,14/Apr/23 08:42,cassandra-4.0.0,,,,,,cassandra-3.1.0,,,,,,,,Connectors / Cassandra,,,,0,pull-request-available,,,IOUtils is not part of the public API and shouldn't be used.,,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 14 10:28:00 UTC 2023,,,,,,,,,,"0|z1habc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 08:42;chesnay;main: 1b8794380b268aad3b74d2b67ded8f59c1f07ea9;;;","14/Apr/23 10:28;dannycranmer;Given this is backwards compatible I am moving to v3.1.0
 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UpdateJobResourceRequirementsRecoveryITCase.testRescaledJobGraphsWillBeRecoveredCorrectly(Path) is unstable on azure,FLINK-31803,13532617,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,Sergey Nuyanzin,Sergey Nuyanzin,14/Apr/23 07:58,21/Apr/23 10:39,13/Jul/23 08:29,21/Apr/23 10:39,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Coordination,Tests,,,0,pull-request-available,test-stability,,"{noformat}
Apr 07 01:28:23 java.util.concurrent.CompletionException: 
Apr 07 01:28:23 org.apache.flink.runtime.rest.util.RestClientException: [org.apache.flink.runtime.rest.NotFoundException: Job d3538259fba86dfc0bd9bd5680076836 not found
Apr 07 01:28:23 	at org.apache.flink.runtime.rest.handler.job.AbstractExecutionGraphHandler.lambda$handleRequest$1(AbstractExecutionGraphHandler.java:99)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.uniExceptionally(CompletableFuture.java:884)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture$UniExceptionally.tryFire(CompletableFuture.java:866)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 07 01:28:23 	at org.apache.flink.runtime.rest.handler.legacy.DefaultExecutionGraphCache.lambda$getExecutionGraphInternal$0(DefaultExecutionGraphCache.java:109)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 07 01:28:23 	at org.apache.flink.runtime.rpc.akka.AkkaInvocationHandler.lambda$invokeRpc$1(AkkaInvocationHandler.java:260)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Apr 07 01:28:23 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Apr 07 01:28:23 	at org.apache.flink.util.concurrent.FutureUtils.doForward(FutureUtils.java:1275)
Apr 07 01:28:23 	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.lambda$null$1(ClassLoadingUtils.java:93)

{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47996&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=7713",,dmvk,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31470,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 10:39:44 UTC 2023,,,,,,,,,,"0|z1haao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 08:01;Sergey Nuyanzin;[~dmvk] I noticed that you've added this test under https://issues.apache.org/jira/browse/FLINK-31470, may I ask you to have a look?;;;","21/Apr/23 10:39;dmvk;master: c2ab806a3624471bb36f87ba98d51f672b7894fe;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing elasticsearch connector on maven central repository ,FLINK-31801,13532588,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,zjureel,zjureel,14/Apr/23 00:59,19/Jun/23 12:26,13/Jul/23 08:29,19/Jun/23 12:26,1.17.0,,,,,,,,,,,,,,Connectors / ElasticSearch,,,,0,,,,There are no versions 3.0.0-1.17 of flink-connector-elasticsearch6 and flink-connector-elasticsearch7 on maven central repository in document https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/datastream/elasticsearch/,,ruibin,Weijie Guo,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 14 06:29:32 UTC 2023,,,,,,,,,,"0|z1ha48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 05:59;Weijie Guo;Thanks [~zjureel] for reporting this. This is because es connector has not yet released a version bump to flink-1.17. We use a unified form connector_version_flink_version in the doc. Maybe we can make some improvement to the shortcode of {{connector_artifact}} to make it prompt more information for unsupported flink versions instead of providing a non-existent artifact. 
Looking directly at it, we may should be able to extend the {{data/xxx.yml}} in the externalized connector repository to declare the supported {connector version & flink verion} or retrieve it from {{https://repo.maven.apache.org/}}.;;;","14/Apr/23 06:05;ruibin;Hi [~Weijie Guo], to clarify, does that mean if our project depends on elastic search connector, we can't upgrade to 1.17.0 for now? May I ask when will the 1.17.0 version for es connector be released? Thanks!;;;","14/Apr/23 06:15;Weijie Guo;[~ruibin] Actually, we have already start releasing connectors for Flink 1.17, see this [mail|https://lists.apache.org/thread/7h1n80oktpvzykndm6wnqhpfxghdx9s5].

As for blocking migrate to flink-1.17, I don't think this is a big problem. For es connector, 3.0.0-1.16 has a high probability of being compatible with flink-1.17. You can try it, or wait for the release process.;;;","14/Apr/23 06:21;Weijie Guo;You can vote for https://lists.apache.org/thread/zyc6r380td78733t4cg20z34gww8hl7z to verify and  speed up the progress of the release of es connector.;;;","14/Apr/23 06:29;ruibin;[~Weijie Guo] Thanks, I will look into it!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Errors are not reported in the Web UI,FLINK-31792,13532442,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,dmvk,dmvk,13/Apr/23 05:59,28/Apr/23 16:58,13/Jul/23 08:29,13/Apr/23 09:14,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Runtime / Web Frontend,,,,0,pull-request-available,,,"After FLINK-29747, NzNotificationService can no longer be resolved by injector, and because we're using the injector directly, this is silently ignored.",,dmvk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29747,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 13 09:13:45 UTC 2023,,,,,,,,,,"0|z1h97s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Apr/23 09:13;dmvk;master: 3f70900fadb3e88d98530aa61b39956ed223fe46

release-1.17: c10393826c4d9c5335e5e46f2d7afba89187de41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KinesisStreamsSink Performance regression due to AIMD rate limiting strategy,FLINK-31772,13532179,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chalixar,chalixar,chalixar,11/Apr/23 12:07,16/May/23 13:43,13/Jul/23 08:29,16/May/23 13:43,1.16.1,aws-connector-4.1.0,,,,,aws-connector-4.2.0,,,,,,,,Connectors / Kinesis,,,,1,pull-request-available,,,"h1. Issue

While benchmarking the {{KinesisStreamSink}} for 1.15 against the legacy {{FlinkKinesisProduced}} , it is observed that the new sink has a performance regression against the deprecated sink for same environment setting.

Further investigation identified that the AIMD Ratelimiting strategy is the bottleneck for the regression. 

Attached results for {{KinesisStreamSink}}  against {FlinkKinesisProducer} and {KinesisStreamSink} after disabling {{AIMDRatelimitingStrategy}}



h2. Environment Settings
- Benchmarking was performed on AWS KDA.
- Application logic is just sending records downstream
- Application parallelism was tested to be 1.
- Kinesis stream number of shards was tested with 8 and 12.
- payload size was 1Kb and 100Kb.

 ",,chalixar,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/23 12:02;chalixar;Screenshot 2023-04-11 at 12.56.10.png;https://issues.apache.org/jira/secure/attachment/13057191/Screenshot+2023-04-11+at+12.56.10.png","11/Apr/23 12:02;chalixar;Screenshot 2023-04-11 at 12.58.09.png;https://issues.apache.org/jira/secure/attachment/13057190/Screenshot+2023-04-11+at+12.58.09.png","11/Apr/23 12:02;chalixar;Screenshot 2023-04-11 at 13.01.47.png;https://issues.apache.org/jira/secure/attachment/13057189/Screenshot+2023-04-11+at+13.01.47.png","17/Apr/23 12:04;chalixar;Screenshot 2023-04-17 at 13.02.31.png;https://issues.apache.org/jira/secure/attachment/13057325/Screenshot+2023-04-17+at+13.02.31.png","17/Apr/23 12:04;chalixar;Screenshot 2023-04-17 at 13.03.24.png;https://issues.apache.org/jira/secure/attachment/13057326/Screenshot+2023-04-17+at+13.03.24.png","17/Apr/23 12:32;chalixar;Screenshot 2023-04-17 at 13.03.34-1.png;https://issues.apache.org/jira/secure/attachment/13057329/Screenshot+2023-04-17+at+13.03.34-1.png","17/Apr/23 12:04;chalixar;Screenshot 2023-04-17 at 13.03.34.png;https://issues.apache.org/jira/secure/attachment/13057327/Screenshot+2023-04-17+at+13.03.34.png",,7.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 13:42:13 UTC 2023,,,,,,,,,,"0|z1h7ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 12:27;dannycranmer;Thanks [~chalixar] , I have assigned the Jira to you;;;","17/Apr/23 12:34;chalixar;Thanks [~dannycranmer]

I have published a fix for the issue [https://github.com/apache/flink-connector-aws/pull/70]

Attaching Results of performance benchmark and KDS performance after applying fix in regression cases.

 

 
h2. Performace Benchmark Results

 
|Parallelism/Shards/Payload|paralellism|shards|payload|records/sec|Async Sink|Async Sink With Configured Ratelimiting Strategy Thourouput (MB/s)|KPL Thourouput (MB/s)|Percentage of Maximum Thourouput|Improvement Percentage against KPL|Improvement Percentage against Async Sink|
|Low/Low/Low|1|1|1024|10000|0.991|1|0.958|1|4.2|0.9|
|Low/Low/High|1|1|102400|100|0.9943|1|0.975|1|2.5|0.57|
|Low/Med/Low|1|8|1024|80000|4.12|4.57|6.45|0.57125|-23.5|5.625|
|Low/Med/High|1|8|102400|800|4.35|7.65|7.45|0.95625|2.5|41.25|
|Med/Low/Low|8|1|1024|20000|0.852|0.846|0.545|0.846|30.1|-0.6|
|Med/Low/High|8|1|102400|200|0.921|0.867|0.975|0.867|-10.8|-5.4|
|Med/Med/Low|8|8|1024|80000|5.37|4.76|5.87|0.595|-13.875|-7.625|
|Med/Med/High|8|8|102400|800|7.53|7.69|5.95|0.96125|21.75|2|
|Med/High/Low|8|64|1024|80000|32.5|37.4|40.7|0.58438|-5.15625|7.65625|
|Med/High/High|8|64|102400|800|47.27|60.4|56.27|0.94375|6.45312|20.51562|
|High/High/Low|256|256|1024|300000|127|127|131|0.49609|-1.5625|0|
|High/High/High|256|256|102400|3000|225|246|215|0.96094|12.10938|8.20313|

 

 
h2. KDS Sink performance after applying fix


!Screenshot 2023-04-17 at 13.03.34.png!




!Screenshot 2023-04-17 at 13.02.31.png!;;;","16/May/23 13:42;dannycranmer;Merged commit [{{0be8192}}|https://github.com/apache/flink-connector-aws/commit/0be819249cfb2930b9356a8228bdea025c04d74e] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OracleExactlyOnceSinkE2eTest.testInsert fails for JDBC connector,FLINK-31770,13532142,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,11/Apr/23 08:10,13/Jun/23 09:38,13/Jul/23 08:29,13/Jun/23 09:38,jdbc-3.1.0,,,,,,jdbc-3.1.1,,,,,,,,Connectors / JDBC,,,,0,test-stability,,,"{code:java}
Caused by: org.apache.flink.util.FlinkRuntimeException: unable to start XA transaction, xid: 201:cea0dbd44c6403283f4050f627bed37c020000000000000000000000:e0070697, error -3: resource manager error has occurred. [XAErr (-3): A resource manager error has occured in the transaction branch. ORA-2045 SQLErr (0)]
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.wrapException(XaFacadeImpl.java:369)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.access$800(XaFacadeImpl.java:67)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl$Command.lambda$fromRunnable$0(XaFacadeImpl.java:301)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl$Command.lambda$fromRunnable$4(XaFacadeImpl.java:340)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.execute(XaFacadeImpl.java:280)
	at org.apache.flink.connector.jdbc.xa.XaFacadeImpl.start(XaFacadeImpl.java:170)
	at org.apache.flink.connector.jdbc.xa.XaFacadePoolingImpl.start(XaFacadePoolingImpl.java:84)
	at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.beginTx(JdbcXaSinkFunction.java:316)
	at org.apache.flink.connector.jdbc.xa.JdbcXaSinkFunction.open(JdbcXaSinkFunction.java:241)
	at org.apache.flink.api.common.functions.util.FunctionUtils.openFunction(FunctionUtils.java:34)
	at org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.open(AbstractUdfStreamOperator.java:100)
	at org.apache.flink.streaming.api.operators.StreamSink.open(StreamSink.java:46)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:731)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.call(StreamTaskActionExecutor.java:100)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:706)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:672)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.lang.Thread.run(Thread.java:750)
{code}

https://github.com/apache/flink-connector-jdbc/actions/runs/4647776511/jobs/8224977183#step:13:325",,eskabetxe,KristoffSC,martijnvisser,,,,,,,,,,,,,,,,,,,FLINK-30790,,,,,FLINK-31847,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 13 09:38:33 UTC 2023,,,,,,,,,,"0|z1h7dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 08:11;martijnvisser;Other failed runs: 

https://github.com/apache/flink-connector-jdbc/actions/runs/4611631661/jobs/8151551281#step:13:328

https://github.com/apache/flink-connector-jdbc/actions/runs/4616119228/jobs/8160730869#step:13:328;;;","17/Apr/23 06:42;martijnvisser;https://github.com/apache/flink-connector-jdbc/actions/runs/4710470564/jobs/8354139817#step:13:354

[~eskabetxe] Any idea why these issues now appear? Is it because of the refactoring? ;;;","17/Apr/23 08:31;eskabetxe;Hi [~martijnvisser],
On Jdbc the [PR is not merged|https://github.com/apache/flink-connector-jdbc/pull/22] (only on 1.16 that I made a small PR)

The current PR has more changes as this problem was discovered during that changes..
But all Jdbc PRs are stuck and not being reviewed.;;;","13/Jun/23 09:38;martijnvisser;This problem was caused by one commit that wasn't backported from {{main}} to {{v3.1}}. This is now resolved via:

apache/flink-connector-jdbc:v3.1 - b6991d8e888ed3504fefcbedf8a47277c5e088f9 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable changelog backend for the StatefulJobSavepointMigrationITCase and StatefulJobWBroadcastStateMigrationITCase with RocksDB and checkpoints,FLINK-31765,13532133,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,11/Apr/23 06:38,13/Apr/23 06:03,13/Jul/23 08:29,13/Apr/23 06:03,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Coordination,Tests,,,0,pull-request-available,test-stability,,"In FLINK-31593 we discovered an instability when generating the migration test data for 1.17 for {{StatefulJobSavepointMigrationITCase}} and {{StatefulJobWBroadcastStateMigrationITCase}}. According to the discussion in FLINK-31593, we concluded that it's caused by a non-determinism that's happening in the changelog backend code. As a workaround, we're going to disable the changelog backend in these tests for now.

We're not touching 1.16 because it didn't appear in that branch. The non-determinism seems to kick in only when generating the checkpoint files. 

For 1.17, we're gonna create a backport for consistency reasons because we have to enable it for the data generation.",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31766,,,,,,,,,FLINK-31593,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 13 06:03:51 UTC 2023,,,,,,,,,,"0|z1h7bk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 16:57;mapohl;After a [discussion|https://github.com/apache/flink/pull/22377/files#r1163828497] we had in the PR, we concluded that no backport is necessary.;;;","13/Apr/23 06:03;mapohl;master: a11a8b48b2090677f811f1d735f2a2f8ccaba54d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Convert requested buffers to overdraft  buffers when pool size is decreased,FLINK-31763,13532124,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,11/Apr/23 05:38,17/Apr/23 09:29,13/Jul/23 08:29,17/Apr/23 09:29,1.16.1,,,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Network,,,,0,pull-request-available,,,"As we discussed in FLINK-31610, new buffers can be requested only when ""{_}numOfRequestedMemorySegments + numberOfRequestedOverdraftMemorySegments < poolSize + maxOverdraftBuffersPerGate""{_}.

Consider such a scenario, the {{{}CurrentPoolSize = 5{}}}, {{{}numOfRequestedMemorySegments = 7{}}}, {{{}maxOverdraftBuffersPerGate = 2{}}}. If {{{}numberOfRequestedOverdraftMemorySegments = 0{}}}, then 2 buffers can be requested now. 

We should convert {{numberOfRequestedMemorySegments}} to {{numberOfRequestedOverdraftMemorySegments}} when poolSize is decreased. Further more, we can changes the definition of overdraft buffer from static to dynamic: 
 * When _numberOfRequestedMemorySegments <= poolSize,_ all buffers are ordinary buffer
 * When _numberOfRequestedMemorySegments > poolSize,_ the `{_}ordinary buffer size = poolSize`{_}, and `{_}the overdraft buffer size = numberOfRequestedMemorySegments - poolSize`{_}

This allows us to remove {{{}numberOfRequestedOverdraftMemorySegments{}}}, which helps us simplify logic and maintain consistency.

 ",,fanrui,pnowojski,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31610,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 17 09:29:07 UTC 2023,,,,,,,,,,"0|z1h79k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Apr/23 09:29;Weijie Guo;master(1.18) via ffc6f3bfabd22b49b08f027400c194a8e7c9c51a.
release-1.17 via be0f9293c2ce00465154ef03f7cef29dd3116b8e.
release-1.16 via ab2ba9612a6a7f27c2921c7c329b3077ea4c33b2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some external connectors sql client jar has a wrong download url in document,FLINK-31758,13532057,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,10/Apr/23 14:59,12/Apr/23 10:11,13/Jul/23 08:29,12/Apr/23 10:11,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Connectors / Common,Documentation,,,0,pull-request-available,,,"After FLINK-30378, we can load sql connector data from external connector's own data file. However, we did not replace \{{$full_version}}, resulting in an incorrect URL in the download link. for example: {{https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-mongodb/$full_version/flink-sql-connector-mongodb-$full_version.jar.}}",,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 10:11:12 UTC 2023,,,,,,,,,,"0|z1h6uo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 10:11;Weijie Guo;master(1.18) via 91e405dd285ecb62312254b4b906dac4fcdfa4de.
release-1.17 via d00bc40a33426bf0e336fc675447237e472da594.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SourceOperatorStreamTask increments numRecordsOut twice,FLINK-31752,13531831,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunfengzhou,huwh,huwh,07/Apr/23 08:00,24/Apr/23 05:07,13/Jul/23 08:29,23/Apr/23 06:26,1.17.0,,,,,,,,,,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"The counter of numRecordsOut was introduce to ChainingOutput to reduce the function call stack depth in 
https://issues.apache.org/jira/browse/FLINK-30536

But SourceOperatorStreamTask.AsyncDataOutputToOutput increments the counter of numRecordsOut too. This results in the source operator's numRecordsOut are doubled.

We should delete the numRecordsOut.inc in SourceOperatorStreamTask.AsyncDataOutputToOutput.

[~xtsong][~lindong] Could you please take a look at this.
",,huwh,leonard,lindong,martijnvisser,mason6345,tanjialiang,yunfengzhou,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/23 07:51;huwh;image-2023-04-07-15-51-44-304.png;https://issues.apache.org/jira/secure/attachment/13057133/image-2023-04-07-15-51-44-304.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 24 05:07:34 UTC 2023,,,,,,,,,,"0|z1h5gg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Apr/23 09:04;lindong;[~huwh] Thanks for reporting this bug. I will look into this.;;;","14/Apr/23 11:58;lindong;[~yunfengzhou] Could you help fix this bug?;;;","23/Apr/23 06:26;lindong;Merged to apache/flink master branch 26bd5fe390e638e97925245da4ccb706b9e658e2.;;;","24/Apr/23 05:07;lindong;Merged to apache/flink release-1.17 branch be13d05b7f66dc5f0c926565ff139aa8313fcdc5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid relocating the RocksDB's log failure when filename exceeds 255 characters,FLINK-31743,13531718,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,assassinj,assassinj,06/Apr/23 08:54,19/May/23 15:41,13/Jul/23 08:29,19/May/23 05:44,1.15.4,1.16.1,,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"Since FLINK-24785 , the file name of the rocksdb LOG is generated by parsing the db path, when the db path is long and the filename exceeds 255 characters, the creation of the file will fail, so the relevant rocksdb LOG cannot be seen in the flink log dir.",,assassinj,fanrui,Feifan Wang,RocMarshal,roman,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 19 15:40:06 UTC 2023,,,,,,,,,,"0|z1h4rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 12:34;yunta;[~assassinj] Thanks for creating this ticket, please go ahead.;;;","11/Apr/23 19:45;roman;[~assassinj] are you still planning to work on this issue?;;;","13/Apr/23 03:13;assassinj;Hi [~roman] 

Previously, Tang Yun and I discussed a two-step approach to addressing this issue: 
firstly, avoiding relocation of logs in RocksDB when the path name exceeds 255 characters on the Flink side. 
Secondly, renaming the file from the FRocksDB perspective. 
I have completed development of the second step, but I am still working on the first. 
It's important to consider the potential impact on distinguishing between multiple operator logs in the previous database directory. 
While I am quite busy this week, I plan to allocate time to address this over the next couple of days.

What do you think?;;;","22/Apr/23 12:45;assassinj;H,[~yunta] 
Sorry that due to the adjustment of my work direction, this issue will be followed up by my colleague. Please assign the task to [~Feifan Wang] . 
Thanks a lot.;;;","23/Apr/23 05:37;yunta;[~assassinj] Thanks for the information, already assigned to him.;;;","08/May/23 11:04;yunta;merged in master: bdb0233eb83629c3bb1b1487057b22891c41c437

For older Flink versions, please take a look at my comment https://github.com/apache/flink/pull/22458/files#r1187314041 to create another PR (for 1.17) [~Feifan Wang];;;","09/May/23 01:40;Feifan Wang;Thanks [~yunta] , I submit a new [PR|https://github.com/apache/flink/pull/22545] to fix this in 1.17, PTAL.;;;","11/May/23 12:52;yunta;merged in flink
release-1.17: c29af139a8f3055c96c641016a31cd3a92ca022a
release-1.16: 7b41bd2b6df4f4794b0b66b52a6af2693a299ca9;;;","19/May/23 11:45;yunta;[~Feifan Wang] You can create another ticket to focus on the changes on FRocksDB repo.;;;","19/May/23 15:40;Feifan Wang;Thanks for reminding [~yunta] , I created a new ticket, can you assign it to me ?
And I have submitted a [PR on FRocksDB|https://github.com/ververica/frocksdb/pull/66], can you help me review it ?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticSearch and Cassandra connector v3.0 branch's CI is not working properly,FLINK-31739,13531631,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,05/Apr/23 15:54,06/Apr/23 14:01,13/Jul/23 08:29,06/Apr/23 13:59,cassandra-3.0.1,elasticsearch-3.0.1,,,,,cassandra-3.0.1,elasticsearch-3.0.1,,,,,,,Connectors / Cassandra,Connectors / ElasticSearch,,,0,pull-request-available,,,"After FLINK-30963, we no longer manually set {{flink_url}}, but it is required in some connector's own {{ci.yml}}, which causes CI to fail to run like [this|https://github.com/apache/flink-connector-elasticsearch/actions/runs/4620241065]). The root of this problem is that these branch does not use the {{ci.yml}} in {{flink-connector-shared-utils}}.",,martijnvisser,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 06 13:59:18 UTC 2023,,,,,,,,,,"0|z1h488:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 13:59;martijnvisser;Elasticsearch v3.0: 57e7883e270b1e3d5814bf52e808ce6099e45481
Cassandra v3.0: 9d930f58ce3ae4379ff8aca42dadb4cbff978b09;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlameGraphTypeQueryParameter#Type clashes with java.reflect.Type in generated clients,FLINK-31738,13531610,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Apr/23 14:38,11/Apr/23 10:30,13/Jul/23 08:29,11/Apr/23 10:30,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Documentation,Runtime / REST,,,0,pull-request-available,,,"Generating a client with the openapi generators causes compile errors because the generated file imports java.reflect.Type, but also the generated ""Type"" model.

For convenience it would be neat to give this enum a slightly different name, because working around this issue is surprisingly annoying.",,dmvk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 11 06:55:37 UTC 2023,,,,,,,,,,"0|z1h43k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Apr/23 06:55;dmvk;master: 4f15da77dcb7d83e211eb65f24e1b49cb46618a2

release-1.17: e026cb58e1771f21cba5b5dfb4950ef004f6a261;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JobDetailsInfo plan incorrectly documented as string,FLINK-31735,13531583,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Apr/23 13:19,06/Apr/23 15:22,13/Jul/23 08:29,06/Apr/23 15:22,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Documentation,Runtime / REST,,,0,pull-request-available,,,"The {{plan}} field in the JobDefaultsInfo contains an object, not a string. Internally we handle it as a string, but write it out as an object.
The docs generators aren't aware of this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 06 15:22:37 UTC 2023,,,,,,,,,,"0|z1h3xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 15:22;chesnay;master: 8ee66535879ce90882f0320b0c0d8ab7ef44ed37
1.17: a70554b51a4f73527c92e2f739538727d81e391d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Model name clashes in OpenAPI spec,FLINK-31733,13531564,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,05/Apr/23 10:32,06/Apr/23 14:59,13/Jul/23 08:29,06/Apr/23 14:59,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Documentation,Runtime / REST,,,0,pull-request-available,,,"The OpenAPi spec uses simple class names for naming models. There are however several models, usually inner classes, that share simple names, like ""Summary"".

This goes undetected and breaks the model for some API calls.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 06 14:59:21 UTC 2023,,,,,,,,,,"0|z1h3tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Apr/23 14:59;chesnay;master:
76d1a4678c383a927caf596ee594ba0ae5b5e1df
dcca520cb47a22354301174d0e1d2bde7f65061e
97a4cc88eee32ad766e5e0c1b647ebce5dc2bfee
1.17:
aadbc0e7d380c588dd63a3d00e2eebd19202b8ba
75c4a7c67fcebb03d073cc4eedbf495e1355b01e
3a69b075ed16a5827bd8539623543b4b277b1b7d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DispatcherTest#testCancellationDuringInitialization is unstable,FLINK-31723,13531409,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,Sergey Nuyanzin,Sergey Nuyanzin,04/Apr/23 08:46,21/Apr/23 13:12,13/Jul/23 08:29,21/Apr/23 13:12,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47889&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=6761

{noformat}
Apr 04 02:26:26 [ERROR] org.apache.flink.runtime.dispatcher.DispatcherTest.testCancellationDuringInitialization  Time elapsed: 0.033 s  <<< FAILURE!
Apr 04 02:26:26 java.lang.AssertionError: 
Apr 04 02:26:26 
Apr 04 02:26:26 Expected: is <CANCELLING>
Apr 04 02:26:26      but: was <CANCELED>
Apr 04 02:26:26 	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
Apr 04 02:26:26 	at org.junit.Assert.assertThat(Assert.java:964)
Apr 04 02:26:26 	at org.junit.Assert.assertThat(Assert.java:930)
Apr 04 02:26:26 	at org.apache.flink.runtime.dispatcher.DispatcherTest.testCancellationDuringInitialization(DispatcherTest.java:389)
[...]
{noformat}",,dmvk,Sergey Nuyanzin,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 13:11:59 UTC 2023,,,,,,,,,,"0|z1h2uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Apr/23 07:47;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47960&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8489;;;","14/Apr/23 10:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48095&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8390;;;","17/Apr/23 11:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48188&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8574;;;","21/Apr/23 06:15;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48319&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8386;;;","21/Apr/23 13:11;dmvk;master: 378d3ca0d4b487b1ebc9354e9ebe8952cc3a9d11 52bf14b0ba949e048c78862be2ed8ebfb58c780e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector v3.0 branch's configuration html files are empty,FLINK-31720,13531399,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,04/Apr/23 07:25,04/Apr/23 09:43,13/Jul/23 08:29,04/Apr/23 09:43,pulsar-3.0.1,,,,,,pulsar-3.0.1,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"Currently, the generated configuration html files of Pulsar connector v3.0 branch are empty. We should add it.",,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 09:43:10 UTC 2023,,,,,,,,,,"0|z1h2so:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 09:43;Weijie Guo;v3.0 via 11765a00b1db95c73ae209f2f7af50d93829f948.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
pulsar connector v3.0 branch's CI is not working properly,FLINK-31718,13531379,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,04/Apr/23 04:43,04/Apr/23 06:38,13/Jul/23 08:29,04/Apr/23 06:38,pulsar-3.0.1,,,,,,pulsar-3.0.1,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"After FLINK-30963, we no longer manually set {{flink_url}}, but it is required in pulsar connector's own {{ci.yml}}, which causes CI to fail to run normally. The root of the problem is that the v3.0 branch does not use the {{ci.yml}} in {{flink-connector-shared-utils}}.",,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 06:38:03 UTC 2023,,,,,,,,,,"0|z1h2og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 06:38;Weijie Guo;v3.0 via 4719bab7dde1e5082ddbf9cdfd9dd71217e81ccd.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests running with local kube config,FLINK-31717,13531340,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mateczagany,morhidi,morhidi,03/Apr/23 20:28,12/May/23 11:52,13/Jul/23 08:29,12/May/23 11:52,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.6.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Some unit tests are using local kube environment. This can be dangerous when pointing to sensitive clusters e.g. in prod.

{quote}2023-04-03 12:32:53,956 i.f.k.c.Config                 [DEBUG] Found for Kubernetes config at: [/Users/<redacted>/.kube/config].
{quote}

A misconfigured kube config environment revealed the issue:

{quote}[ERROR] Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.012 s <<< FAILURE! - in org.apache.flink.kubernetes.operator.FlinkOperatorTest
[ERROR] org.apache.flink.kubernetes.operator.FlinkOperatorTest.testConfigurationPassedToJOSDK  Time elapsed: 0.008 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.kubernetes.operator.FlinkOperatorTest.testConfigurationPassedToJOSDK(FlinkOperatorTest.java:63)

[ERROR] org.apache.flink.kubernetes.operator.FlinkOperatorTest.testLeaderElectionConfig  Time elapsed: 0.004 s  <<< ERROR!
java.lang.NullPointerException
	at org.apache.flink.kubernetes.operator.FlinkOperatorTest.testLeaderElectionConfig(FlinkOperatorTest.java:108){quote}

",,gyfora,mateczagany,mbalassi,morhidi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 12 11:52:55 UTC 2023,,,,,,,,,,"0|z1h2fs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 12:15;mbalassi;To clarify the errors only appear when your local k8s context points to an unreacheable API server (e.g. one behind a VPN that you are not connected to at the moment).

I suggest to use this JOSDK utility to solve the issue:
https://csviri.medium.com/introducing-jenvtest-kubernetes-api-server-tests-made-easy-for-java-4d02a9bb26d4;;;","04/Apr/23 12:38;morhidi;SGTM;;;","19/Apr/23 18:11;mateczagany;Sorry, I didn't have much time to work on this ticket so far.

 

I've investigated and found 3 test cases that load ~/.kube/config in some ways:
 * FlinkOperatorTest
 * FlinkOperatorITCase
 * HealthProbeTest


I could add jenvtest to FlinkOperatorTest easily. I had to re-write some of the code in HealthProbeTest.java so it doesn't try to call FlinkOperator.run() to start OperatorHealthService.

But I could not get around FlinkOperatorITCase since that test class is about testing a Kubernetes cluster that already has the operator installed.
What do you suggest we do with that test case?;;;","20/Apr/23 06:33;mbalassi;Thanks, [~mateczagany]. I think the `FlinkOperatorITCase` can be retired in favor of the e2e tests:

https://github.com/apache/flink-kubernetes-operator/blob/main/.github/workflows/ci.yml#L94
https://github.com/apache/flink-kubernetes-operator/actions/runs/4607822219/jobs/8142835592

What do you think [~morhidi]?;;;","08/May/23 12:22;mateczagany;We have agreed offline to instead get rid of FlinkOperatorITCase and add set a system property during execution of FlinkOperatorTest and HealthProbeTest to make sure they don't try to access ~/.kube/config. 

I will also tweak the CI workflow so that minikube doesn't get started for the unit tests, so any new tests added in the future that might use the config file will fail.;;;","12/May/23 11:52;gyfora;merged to main e399c9bbd37d2dca352d973de01be0cb403fec55;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Event UID field is missing the first time that an event is consumed,FLINK-31716,13531330,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,rodmeneses,rodmeneses,rodmeneses,03/Apr/23 18:08,19/Apr/23 08:05,13/Jul/23 08:29,19/Apr/23 08:05,,,,,,,kubernetes-operator-1.5.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"on `EventUtils.createOrUpdateEvent` we use a `Consumer<Event>` instance to `accept` the underlying event that is being created or updated.

The first time an event is created, we are calling `client.resource(event).createOrReplace()` but we are discarding the return value of such method, and we are returning the `event` that we just created, which has an empty UID field.",,mbalassi,rodmeneses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 08:05:03 UTC 2023,,,,,,,,,,"0|z1h2dk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 18:32;mbalassi;Good catch [~rodmeneses];;;","04/Apr/23 21:29;mbalassi;658ad63 in main;;;","19/Apr/23 08:05;mbalassi;Additionally a5aca7c in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenAPI spec omits complete-statement request body,FLINK-31711,13531280,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,03/Apr/23 13:03,04/Apr/23 08:36,13/Jul/23 08:29,04/Apr/23 08:36,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Documentation,Runtime / REST,,,0,pull-request-available,,,"The OpenAPI generator omits request bodies for get requests because it is usually a bad idea.

Still, the generator shouldn't omit this on it's own.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28796,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 08:36:20 UTC 2023,,,,,,,,,,"0|z1h22g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 08:36;chesnay;master: 46aeebd150fe56995d85365cf5d44dd54854224e
1.17: 8d7b42c9480dd61bc0bbe935ca5b746697bc0ad8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RuntimeException/KryoException thrown when deserializing an empty protobuf record,FLINK-31708,13531261,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,shenjiaqi,shenjiaqi,shenjiaqi,03/Apr/23 11:39,20/Apr/23 07:05,13/Jul/23 08:29,20/Apr/23 07:05,1.10.0,1.17.0,,,,,,,,,,,,,API / Type Serialization System,,,,0,pull-request-available,,,"h1. Problem description

I am using protobuf defined Class in Flink job. When the application runs on production, the job throws following Exception:
{code:java}
java.lang.RuntimeException: Could not create class com.MYClass <==== generated by protobuf
        at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:76)
        at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:40)
        at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:813)
        at org.apache.flink.api.java.typeutils.runtime.kryo.KryoSerializer.deserialize(KryoSerializer.java:346)
        at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:205)
        at org.apache.flink.streaming.runtime.streamrecord.StreamElementSerializer.deserialize(StreamElementSerializer.java:46)
        at org.apache.flink.runtime.plugable.NonReusingDeserializationDelegate.read(NonReusingDeserializationDelegate.java:55)
        at org.apache.flink.runtime.io.network.api.serialization.SpillingAdaptiveSpanningRecordDeserializer.getNextRecord(SpillingAdaptiveSpanningRecordDeserializer.java:141)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.emitNext(StreamTaskNetworkInput.java:121)
        at org.apache.flink.streaming.runtime.io.StreamTwoInputProcessor.processInput(StreamTwoInputProcessor.java:185)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:319)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:187)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:494)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:478)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:708)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:533)
        at java.lang.Thread.run(Thread.java:748)
Caused by: com.esotericsoftware.kryo.KryoException: java.io.EOFException: No more bytes left.
        at org.apache.flink.api.java.typeutils.runtime.NoFetchingInput.readBytes(NoFetchingInput.java:127)
        at com.esotericsoftware.kryo.io.Input.readBytes(Input.java:332)
        at com.twitter.chill.protobuf.ProtobufSerializer.read(ProtobufSerializer.java:73)
        ... 16 common frames omitted
 {code}
h1. How to reproduce

I think this is similar to another issue: FLINK-29347.

Follwing is an example to reproduce the problem:
{code:java}
package com.test;

import com.test.ProtobufGeneratedClass;

import com.google.protobuf.Message;
import com.twitter.chill.protobuf.ProtobufSerializer;
import lombok.extern.slf4j.Slf4j;
import org.apache.flink.api.common.functions.MapFunction;
import org.apache.flink.api.common.restartstrategy.RestartStrategies;
import org.apache.flink.api.common.state.MapStateDescriptor;
import org.apache.flink.api.common.time.Time;
import org.apache.flink.api.java.utils.MultipleParameterTool;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.contrib.streaming.state.RocksDBStateBackend;
import org.apache.flink.streaming.api.CheckpointingMode;
import org.apache.flink.streaming.api.datastream.BroadcastStream;
import org.apache.flink.streaming.api.datastream.DataStreamSource;
import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.co.KeyedBroadcastProcessFunction;
import org.apache.flink.streaming.api.functions.sink.SinkFunction;
import org.apache.flink.streaming.api.functions.source.RichParallelSourceFunction;
import org.apache.flink.util.Collector;
import org.apache.flink.util.OutputTag;

import java.util.Random;
@Slf4j
public class app {
  public static final OutputTag<ProtobufGeneratedClass> OUTPUT_TAG_1 =
      new OutputTag<ProtobufGeneratedClass>(""output-tag-1"") {
  };

  public static final OutputTag<ProtobufGeneratedClass> OUTPUT_TAG_2 =
      new OutputTag<ProtobufGeneratedClass>(""output-tag-2"") {
  };

  public static final OutputTag<ProtobufGeneratedClass> OUTPUT_TAG_3 =
      new OutputTag<ProtobufGeneratedClass>(""output-tag-3"") {
  };

  public static class MySourceFunction extends RichParallelSourceFunction<ProtobufGeneratedClass> {
    Random rnd = new Random();
    private final String name;

    private boolean running = true;

    private MySourceFunction(String name) {
      this.name = name;
    }

    @Override
    public void run(SourceContext<ProtobufGeneratedClass> sourceContext) throws Exception {
      final int index = getRuntimeContext().getIndexOfThisSubtask();
      int counter = 0;

      while (running) {
        synchronized (sourceContext.getCheckpointLock()) {
          ++counter;
          ProtobufGeneratedClass.Builder builder = ProtobufGeneratedClass.newBuilder();
          if (rnd.nextBoolean()) {

            builder.addGraphIds(rnd.nextInt(10));
            byte[] bytes;
            if (rnd.nextInt(10) == 1) {
              // make sure record is large enough to reproduce the problem
              // in which case, SpillingAdaptiveSpanningRecordDeserializer#spanningWrapper may be activated
              bytes = new byte[rnd.nextInt(5000000)];
            } else if (rnd.nextInt(10) == 2) {
              bytes = new byte[rnd.nextInt(50000)];
            } else {
              bytes = new byte[rnd.nextInt(50)];
            }
            builder.addUserTagNames(new String(bytes));
          } else {
				// create an empty record by do nothing.
          }
          sourceContext.collect(builder.build());
          Thread.sleep(5);
        }
      }
    }

    @Override
    public void cancel() {
      running = false;
    }
  }

  public static void main(String[] args) throws Exception {

    final int SHARD_NUM = 64;
    final MultipleParameterTool params = MultipleParameterTool.fromArgs(args);

    // set up the execution environment
    Configuration config = new Configuration();
    config.setInteger(""state.checkpoints.num-retained"", 5);
    config.setInteger(""taskmanager.numberOfTaskSlots"", 1);
    config.setInteger(""local.number-taskmanager"", 4);
    StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(3, config);

    RocksDBStateBackend rocksDBStateBackend =
        new RocksDBStateBackend(""file:///Users/shenjiaqi/Workspace/state/checkpoints/"", true);

    env.setParallelism(3);
    env.setStateBackend(rocksDBStateBackend);
    env.getCheckpointConfig().setCheckpointTimeout(100000);
    env.enableCheckpointing(1000, CheckpointingMode.EXACTLY_ONCE);
    env.setRestartStrategy(RestartStrategies.fixedDelayRestart(1000, Time.seconds(10)));
    env.addDefaultKryoSerializer(Message.class, ProtobufSerializer.class); // make sure ProtobufSerializer is serialized/deserialized by protobuf.

    // make parameters available in the web interface
    env.getConfig().setGlobalJobParameters(params);

    String[] words = new String[100000];
    Random rnd = new Random();
    for (int i = 0; i < words.length; ++i) {
      words[i] = String.valueOf(rnd.nextInt(10));
    }
    DataStreamSource<ProtobufGeneratedClass> stream1 = env.addSource(new MySourceFunction(""randomProtobufGeneratedClass1"")).setParallelism(4);
    BroadcastStream<ProtobufGeneratedClass> stream2 = env.addSource(new MySourceFunction(""randomProtobufGeneratedClass2"")).setParallelism(3)
        .broadcast(new MapStateDescriptor[0]);
    SingleOutputStreamOperator<ProtobufGeneratedClass> output = stream1.shuffle()
        .map(new MapFunction<ProtobufGeneratedClass, ProtobufGeneratedClass>() {

          @Override
          public ProtobufGeneratedClass map(ProtobufGeneratedClass value) throws Exception {
            return value;
          }
        }).setParallelism(2).disableChaining()
        .keyBy(x -> x.hashCode() % 10)
        .connect(stream2)
        .process(new MyProcessFunction()).disableChaining();

    output.getSideOutput(OUTPUT_TAG_1).rescale().
        addSink(new SinkFunction<ProtobufGeneratedClass>() {
          @Override
          public void invoke(ProtobufGeneratedClass value) throws Exception {
            log.info(""blah 1"");
          }
        }).setParallelism(1);

    output.getSideOutput(OUTPUT_TAG_2).rescale().
        addSink(new SinkFunction<ProtobufGeneratedClass>() {
          @Override
          public void invoke(ProtobufGeneratedClass value) throws Exception {
            log.info(""blah 2"");
          }
        }).setParallelism(2);

    output.getSideOutput(OUTPUT_TAG_3).rescale().
        addSink(new SinkFunction<ProtobufGeneratedClass>() {
          @Override
          public void invoke(ProtobufGeneratedClass value) throws Exception {
            log.info(""blah 3"");
          }
        }).setParallelism(3);

    output.map(new MapFunction<ProtobufGeneratedClass, String>() {
      @Override
      public String map(ProtobufGeneratedClass value) throws Exception {
        return """" + value.toString().length();
      }
    }).print();
    env.execute(""reproduce-the-problem"");
  }

  public static class MyProcessFunction extends
      KeyedBroadcastProcessFunction<ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass> {

    @Override
    public void processElement(ProtobufGeneratedClass ProtobufGeneratedClass,
        KeyedBroadcastProcessFunction<ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass>.ReadOnlyContext readOnlyContext,
        Collector<ProtobufGeneratedClass> collector) throws Exception {
      collector.collect(ProtobufGeneratedClass);
    }

    @Override
    public void processBroadcastElement(ProtobufGeneratedClass s,
        KeyedBroadcastProcessFunction<ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass, ProtobufGeneratedClass>.Context context,
        Collector<ProtobufGeneratedClass> collector) throws Exception {
      context.output(OUTPUT_TAG_1, s);
      context.output(OUTPUT_TAG_2, s);
      context.output(OUTPUT_TAG_3, s);
    }
  }
}

{code}",,gaoyunhaii,shenjiaqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 20 07:02:12 UTC 2023,,,,,,,,,,"0|z1h1y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 12:09;shenjiaqi;pull request:https://github.com/apache/flink/pull/22335;;;","20/Apr/23 07:02;gaoyunhaii;Merged on master via d9e9d1ca741c84a18dbfac0ed44bd38cb5e11f3f

Merged on release-1.17 via 729043df687a96711d3591fcdf5e8e712cd21b87

Merged on release-1.16 via 389389fabd0eeb7894065e42f395db67f3e722e1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Constant string cannot be used as input arguments of Pandas UDAF,FLINK-31707,13531260,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,03/Apr/23 11:39,04/Apr/23 05:26,13/Jul/23 08:29,04/Apr/23 02:18,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,API / Python,,,,0,pull-request-available,,,"It will throw exceptions as following when using constant strings in Pandas UDAF:
{code}
E                       raise ValueError(""field_type %s is not supported."" % field_type)
E                   ValueError: field_type type_name: CHAR
E                   char_info {
E                     length: 3
E                   }
E                    is not supported.
{code}",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 02:18:01 UTC 2023,,,,,,,,,,"0|z1h1y0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 02:18;dianfu;Fixed in:
- master via 7c6d8b0134cbcdc60d56b87d39ff2f28c310b1eb
- release-1.17 via 9c5ca0590806932e4e8f9d3f942f0a2a5442fe2d
- release-1.16 via 3291e4d6f9afff40e1e9718e23388610577de741;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The current key is not set for KeyedCoProcessOperator,FLINK-31690,13531142,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,03/Apr/23 02:32,03/Apr/23 05:44,13/Jul/23 08:29,03/Apr/23 05:44,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,API / Python,,,,0,pull-request-available,,,See https://apache-flink.slack.com/archives/C03G7LJTS2G/p1680294701254239 for more details.,,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 05:44:33 UTC 2023,,,,,,,,,,"0|z1h17s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 05:44;dianfu;Fixed in:
- master via 6c1ffe544e31bb67df94175a559f2f40362795a4
- release-1.17 via e3d612e7e98bedde42c365df3f2ed2a2ca76aefa
- release-1.16 via 01cdaee25cdc41773a5c42638f4c5209373b5aa4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken links in docs for Azure Table Storage,FLINK-31688,13531121,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,tanyuxin,tanyuxin,02/Apr/23 13:40,03/Apr/23 05:39,13/Jul/23 08:29,03/Apr/23 05:39,1.18.0,,,,,,1.18.0,,,,,,,,Documentation,,,,0,pull-request-available,,,"The doc page of https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/connectors/dataset/formats/azure_table_storage/ has a broken links, we should fix it.",,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 05:39:29 UTC 2023,,,,,,,,,,"0|z1h134:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 05:39;Weijie Guo;master(1.18) via 6101ad313be78a658cc25487b9d3976252625feb.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Align the outdated Chinese filesystem connector docs,FLINK-31683,13530941,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,31/Mar/23 09:27,31/Mar/23 13:55,13/Jul/23 08:29,31/Mar/23 13:55,1.15.4,1.16.1,1.17.0,,,,1.15.5,1.16.2,1.17.1,1.18.0,,,,,Documentation,,,,0,pull-request-available,,,"The current Chinese doc of the file system SQL connector is outdated from Flink-1.15, we should fix it to avoid misunderstanding.",,liyu,martijnvisser,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 31 13:55:45 UTC 2023,,,,,,,,,,"0|z1gzzc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 13:55;yunta;merged.
master: 370929c838fbac21cc51a225f3c690c1ee8931a8
release-1.17: 49db0c628e50182f160a99ec7bbd5148ed725792
release-1.16: 701fccf4556765c5c75eb7b7c66f0a4d9edfc957
release-1.15: 27f93da8f4a6813af5030691e72d132bb35ec1c0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Network connection timeout between operators should trigger either network re-connection or job failover,FLINK-31681,13530923,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lindong,lindong,31/Mar/23 08:18,12/Apr/23 08:45,13/Jul/23 08:29,12/Apr/23 08:45,1.15.1,,,,,,,,,,,,,,Runtime / Network,,,,0,,,,"If a network connection error occurs between two operators, the upstream operator may log the following error message in the method PartitionRequestQueue#handleException and subsequently close the connection. When this happens, the Flink job may become stuck without completing or failing. 

To avoid this issue, we can either allow the upstream operator to reconnect with the downstream operator, or enable job failover so that users can take corrective action promptly.

org.apache.flink.runtime.io.network.netty.PartitionRequestQueue - Encountered error while consuming partitions org.apache.flink.shaded.netty4.io.netty.channel.unix.Errors#NativeIOException: writeAccess(...) failed: Connection timed out.",,lindong,martijnvisser,pnowojski,wanglijie,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 08:45:29 UTC 2023,,,,,,,,,,"0|z1gzvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 08:36;martijnvisser;[~pnowojski] Any thought on this one?;;;","31/Mar/23 08:50;pnowojski;Which version does this issue affect? Why doesn't {{PartitionRequestQueue#handleException}} failover the subtask? Shouldn't this be triggered by {{PartitionRequestQueue#releaseAllResources}}?;;;","31/Mar/23 14:25;lindong;This happens with Flink version 1.15.1 when we were testing Flink ML with parallelism = 200.

Upgrading the internal Flink library and related connectors needed by Flink ML would take some time. Thus we have not tried to reproduce this issue with Flink 1.17.

Thus I choose to write down the phenomena and the error message in this JIRA to make sure this issue will be tracked. I will close this JIRA if we can not reproduce the issue with the latest Flink version.;;;","12/Apr/23 08:45;lindong;I didn't not get to reproduce this issue myself and the users no longer report this issue. Will close this issue for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deadlock in AWS Connectors following content-length AWS SDK exception,FLINK-31675,13530819,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,antoniovespoli,antoniovespoli,antoniovespoli,30/Mar/23 14:34,26/Apr/23 14:33,13/Jul/23 08:29,26/Apr/23 14:33,1.15.4,1.16.1,1.17.0,,,,aws-connector-3.1.0,aws-connector-4.2.0,,,,,,,Connectors / AWS,,,,0,pull-request-available,,,"Connector calls to AWS services can hang on a canceled future following a content-length mismatch that isn't handled gracefully by the SDK:

 
{code:java}
org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.FutureCancelledException: java.io.IOException: Response had content-length of 31 bytes, but only received 0 bytes before the connection was closed.
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.NettyRequestExecutor.lambda$null$3(NettyRequestExecutor.java:136)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.PromiseTask.run(PromiseTask.java:106)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:469)
at org.apache.flink.kinesis.shaded.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
at org.apache.flink.kinesis.shaded.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: java.io.IOException: Response had content-length of 31 bytes, but only received 0 bytes before the connection was closed.
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler.validateResponseContentLength(ResponseHandler.java:163)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler.access$700(ResponseHandler.java:75)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler$PublisherAdapter$1.onComplete(ResponseHandler.java:369)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.nrs.HandlerPublisher.complete(HandlerPublisher.java:447)
at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.nrs.HandlerPublisher.channelInactive(HandlerPublisher.java:430)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
at org.apache.flink.kinesis.shaded.io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)
at org.apache.flink.kinesis.shaded.io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)
at org.apache.flink.kinesis.shaded.io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)
at org.apache.flink.kinesis.shaded.io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)
at org.apache.flink.kinesis.shaded.io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)
at org.apache.flink.kinesis.shaded.io.netty.handler.codec.http2.AbstractHttp2StreamChannel$Http2ChannelUnsafe$2.run(AbstractHttp2StreamChannel.java:737)
... 6 more {code}
Related AWS SDK issue: [https://github.com/aws/aws-sdk-java-v2/issues/3335]

AWS SDK fix: [https://github.com/aws/aws-sdk-java-v2/pull/3855/files]

 

This mishandled exception creates a deadlock situation that prevents the connectors from making any progress.

We should update the AWS SDK v2 to 2.20.32: [https://github.com/aws/aws-sdk-java-v2/commit/eb5619e24e4eaca6f80effa1c43c0cd409cdd53e]",,antoniovespoli,dannycranmer,samrat007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 26 14:33:31 UTC 2023,,,,,,,,,,"0|z1gz88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 14:37;dannycranmer;Thanks for raising [~antoniovespoli] , I have assigned the issue to you;;;","26/Apr/23 14:32;dannycranmer;Merged commit [{{9a51b52}}|https://github.com/apache/flink-connector-aws/commit/9a51b5242563f2870148a62e07ef6e9763050519] into apache:main
Merged commit [{{721772e}}|https://github.com/apache/flink-connector-aws/commit/721772eef0f409bb2d9b522d42d810110d9387d6] into apache:v3.0;;;","26/Apr/23 14:33;dannycranmer;Dropping from 1.15.5 since 1.15.x is no longer supported;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ElasticSearch connector's document was not incorrect linked to external repo,FLINK-31670,13530769,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,30/Mar/23 09:06,11/Apr/23 13:15,13/Jul/23 08:29,06/Apr/23 07:17,1.16.1,,,,,,1.16.2,1.17.1,1.18.0,,,,,,Documentation,,,,0,pull-request-available,,,"In the [doc|https://nightlies.apache.org/flink/flink-docs-master/docs/connectors/datastream/elasticsearch/], It still use ""flink-version"" for flink-connector-elastiacsearch instead of the version in the external repository.

<dependency>
    <groupId>org.apache.flink</groupId>
    <artifactId>flink-connector-elasticsearch6</artifactId>
    <version>1.18-SNAPSHOT</version>
</dependency>",,martijnvisser,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 11 13:15:54 UTC 2023,,,,,,,,,,"0|z1gyx4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 09:47;Weijie Guo;It seems that we sync the doc of {{elasticsearch}} connector with the tag {{v3.0.0 (integrate_connector_docs elasticsearch v3.0.0)}}, but we allows connectors to specify it's own version in FLINK-29958 after this tag. I'm not sure whether we should directly sync with the latest v3.0(integrate_connector_docs elasticsearch v3.0) branch or create another release(i.e. rc3) for v3.0 branch, while also including the v3.0.0 tag. But I'm not very familiar with the management process of external connectors. cc [~martijnvisser] for further information.;;;","06/Apr/23 07:16;Weijie Guo;master(1.18) via d60431b05ed8adbd320e6413b55b748666206319.
release-1.17 via bc1aaa66b9539885bfcfec94a06012396f7aa9a1.
release-1.16 via 8e961e4cb6ee89c286568e3e37d3366f88e1bd96.;;;","11/Apr/23 13:15;martijnvisser;[~Weijie Guo] Sorry that I missed this. 

In my opinion, docs should point to the branch where that version of the docs are published (because else documentation fixes are not visible until a new release is made). 
The connector_artifact shortcode should point to a released version, since you should be able to download it from the repository;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connectors-kafka ITCases are not runnable in the IDE,FLINK-31660,13530662,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,nateab,nateab,nateab,29/Mar/23 17:27,08/May/23 14:08,13/Jul/23 08:29,08/May/23 14:08,1.18.0,,,,,,1.17.1,,,,,,,,Table SQL / Ecosystem,,,,0,pull-request-available,,,"The following exception is thrown when trying to run {{KafkaChangelogTableITCase}} or {{KafkaTableITCase}}
{code:java}
java.lang.NoClassDefFoundError: org/apache/flink/table/shaded/com/jayway/jsonpath/spi/json/JsonProvider    at java.base/java.lang.Class.getDeclaredMethods0(Native Method)
    at java.base/java.lang.Class.privateGetDeclaredMethods(Class.java:3166)
    at java.base/java.lang.Class.getMethodsRecursive(Class.java:3307)
    at java.base/java.lang.Class.getMethod0(Class.java:3293)
    at java.base/java.lang.Class.getMethod(Class.java:2106)
    at org.apache.calcite.linq4j.tree.Types.lookupMethod(Types.java:309)
    at org.apache.calcite.util.BuiltInMethod.<init>(BuiltInMethod.java:670)
    at org.apache.calcite.util.BuiltInMethod.<clinit>(BuiltInMethod.java:357)
    at org.apache.calcite.rel.metadata.BuiltInMetadata$PercentageOriginalRows.<clinit>(BuiltInMetadata.java:344)
    at org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows$RelMdPercentageOriginalRowsHandler.getDef(RelMdPercentageOriginalRows.java:231)
    at org.apache.calcite.rel.metadata.ReflectiveRelMetadataProvider.reflectiveSource(ReflectiveRelMetadataProvider.java:134)
    at org.apache.calcite.rel.metadata.RelMdPercentageOriginalRows.<clinit>(RelMdPercentageOriginalRows.java:42)
    at org.apache.calcite.rel.metadata.DefaultRelMetadataProvider.<init>(DefaultRelMetadataProvider.java:42)
    at org.apache.calcite.rel.metadata.DefaultRelMetadataProvider.<clinit>(DefaultRelMetadataProvider.java:28)
    at org.apache.calcite.plan.RelOptCluster.<init>(RelOptCluster.java:97)
    at org.apache.calcite.plan.RelOptCluster.create(RelOptCluster.java:106)
    at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory$.create(FlinkRelOptClusterFactory.scala:36)
    at org.apache.flink.table.planner.calcite.FlinkRelOptClusterFactory.create(FlinkRelOptClusterFactory.scala)
    at org.apache.flink.table.planner.delegation.PlannerContext.<init>(PlannerContext.java:132)
    at org.apache.flink.table.planner.delegation.PlannerBase.<init>(PlannerBase.scala:121)
    at org.apache.flink.table.planner.delegation.StreamPlanner.<init>(StreamPlanner.scala:65)
    at org.apache.flink.table.planner.delegation.DefaultPlannerFactory.create(DefaultPlannerFactory.java:65)
    at org.apache.flink.table.planner.loader.DelegatePlannerFactory.create(DelegatePlannerFactory.java:36)
    at org.apache.flink.table.factories.PlannerFactoryUtil.createPlanner(PlannerFactoryUtil.java:58)
    at org.apache.flink.table.api.bridge.java.internal.StreamTableEnvironmentImpl.create(StreamTableEnvironmentImpl.java:127)
    at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:122)
    at org.apache.flink.table.api.bridge.java.StreamTableEnvironment.create(StreamTableEnvironment.java:94)
    at org.apache.flink.streaming.connectors.kafka.table.KafkaTableTestBase.setup(KafkaTableTestBase.java:93)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.base/java.lang.reflect.Method.invoke(Method.java:566)
    at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
    at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
    at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
    at org.junit.internal.runners.statements.RunBefores.invokeMethod(RunBefores.java:33)
    at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:24)
    at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
    at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
    at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
    at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
    at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
    at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
    at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
    at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
    at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
    at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
    at org.testcontainers.containers.FailureDetectingExternalResource$1.evaluate(FailureDetectingExternalResource.java:30)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
    at org.junit.rules.RunRules.evaluate(RunRules.java:20)
    at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
    at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
    at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
    at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
    at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
    at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:235)
    at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:54)
Caused by: java.lang.ClassNotFoundException: Class 'org.apache.flink.table.shaded.com.jayway.jsonpath.spi.json.JsonProvider' not found. Perhaps you forgot to add the module 'flink-table-runtime' to the classpath?
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:123)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    ... 61 more
Caused by: java.lang.ClassNotFoundException: org.apache.flink.table.shaded.com.jayway.jsonpath.spi.json.JsonProvider
    at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
    at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
    at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromOwnerOnly(ComponentClassLoader.java:164)
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClassFromComponentFirst(ComponentClassLoader.java:158)
    at org.apache.flink.core.classloading.ComponentClassLoader.loadClass(ComponentClassLoader.java:104)
    ... 62 more {code}
This is a similar problem to https://issues.apache.org/jira/browse/FLINK-25525 ",,dwysakowicz,nateab,pgaref,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-25525,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 08 14:08:59 UTC 2023,,,,,,,,,,"0|z1gy9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 21:23;twalthr;This is a very annoying issue but iirc should not be a problem once the Kafka connector is externalized into a dedicated repository.;;;","29/Mar/23 21:26;twalthr;[~dwysakowicz] You also ran into this problem recently, how did you fix it for development?;;;","30/Mar/23 19:32;nateab;[~pnowojski] was able to get a rough working pom by looking at {{flink-connector-hive}} and added a few dependencies and a new maven plugin, and then [~pgaref] helped me by telling me to have intellij use the maven wrapper, and we fiddled with the pom a bit more. I will try to polish things up and put out a bug fix;;;","31/Mar/23 08:51;dwysakowicz;I think the way I did it is that I added one of the table modules (either planner or the uber jar) to the classpath of the test configuration in IntelliJ. (""Modify options"" > ""Modify classpath"") That way I didn't need to change a valid pom configuration which is not handled 100% correctly by IntelliJ.;;;","31/Mar/23 09:16;chesnay;Bundling json-path in the table-planner also does the trick btw; at least for the kafka tests.;;;","08/May/23 14:08;chesnay;1.17: 7d813376e905dee04041ada03cb4f3b8b33280fc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConfigurationInfo generates incorrect openapi schema,FLINK-31657,13530599,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,29/Mar/23 11:08,30/Mar/23 08:31,13/Jul/23 08:29,30/Mar/23 08:31,1.16.0,,,,,,1.16.2,1.17.1,1.18.0,,,,,,Documentation,Runtime / REST,,,0,pull-request-available,,,"ConfigurationInfo extends ArrayList, and the schema generator picks up List#isEmpty as a property.
This results in an invalid schema, as arrays cant have properties.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 08:31:36 UTC 2023,,,,,,,,,,"0|z1gxvs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Mar/23 08:31;chesnay;master: bb24ec9cd84d6754ec1d99c6183e8c4ec0db7422
1.17: b5a6c5c9a0be89ccd70544546793db21c9a2c3b1
1.16: 8e470b52e6055ddf6871bf9e3f7dc8bbe9cc1447;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink should handle the delete event if the pod was deleted while pending,FLINK-31652,13530560,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiasun,xiasun,xiasun,29/Mar/23 06:57,03/Apr/23 08:05,13/Jul/23 08:29,03/Apr/23 08:05,1.16.1,1.17.0,,,,,1.16.2,1.17.1,1.18.0,,,,,,Deployment / Kubernetes,,,,0,pull-request-available,,,"I found that in kubernetes deployment, if the taskmanager pod is deleted in 'Pending' phase, the flink job will get stuck and keep waiting for the pod scheduled. We can reproduce this issue with the 'kubectl delete pod' command to delete the pod when it is in the pending phase.
 
The cause reason is that the pod status will not be updated in time in this case, so the KubernetesResourceManagerDriver won't detect the pod is terminated, and I also verified this by logging the pod status in KubernetesPod#isTerminated(), and it shows as follows.
{code:java}
public boolean isTerminated() {
    log.info(""pod status: "" + getInternalResource().getStatus());
    if (getInternalResource().getStatus() != null) {
        final boolean podFailed =
                PodPhase.Failed.name().equals(getInternalResource().getStatus().getPhase());
        final boolean containersFailed =
                getInternalResource().getStatus().getContainerStatuses().stream()
                        .anyMatch(
                                e ->
                                        e.getState() != null
                                                && e.getState().getTerminated() != null);
        return containersFailed || podFailed;
    }
    return false;
} {code}
In the case, this function will return false because `containersFailed` and `podFailed` are both false.
{code:java}
PodStatus(conditions=[PodCondition(lastProbeTime=null, lastTransitionTime=2023-03-28T12:35:10Z, reason=Unschedulable, status=False, type=PodScheduled, additionalProperties={})], containerStatuses=[], ephemeralContainerStatuses=[], hostIP=null, initContainerStatuses=[], message=null, nominatedNodeName=null, phase=Pending, podIP=null, podIPs=[], qosClass=Guaranteed, reason=null, startTime=null, additionalProperties={}){code}
 
 ",,gyfora,huwh,tanyuxin,wanglijie,xiasun,xtsong,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 08:05:32 UTC 2023,,,,,,,,,,"0|z1gxn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 07:10;xiasun;Hi [~xtsong] , could you please help to take a look at this ticket in your free time, thanks!;;;","29/Mar/23 11:24;xtsong;[~xiasun], thanks for reporting this. I think this is a valid issue.

Would you like to open a pull request to fix this?;;;","29/Mar/23 12:23;xiasun;[~xtsong] Thanks for the reply! I am glad to take this ticket.;;;","30/Mar/23 01:10;xtsong;[~xiasun] You are assigned. Please go ahead.;;;","03/Apr/23 08:05;xtsong;- master (1.18): 9e83858c7dc309f272a03c62b1e295d192acaf89
- release-1.17: 98d8e2712feb5077c4b35698cea5ecbdd72e4c06
- release-1.16: 99c025b438a99eb8ddcf8214aba5f285972106ca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
watermark aligned idle source can't resume,FLINK-31632,13530316,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,haishui,haishui,haishui,28/Mar/23 02:07,26/Apr/23 02:57,13/Jul/23 08:29,20/Apr/23 16:56,1.15.4,1.16.1,1.17.0,,,,1.16.2,1.17.1,1.18.0,,,,,,API / DataStream,,,,0,pull-request-available,,," 
{code:java}
WatermarkStrategy<String> watermarkStrategy = WatermarkStrategy
        .<String>forBoundedOutOfOrderness(Duration.ofMillis(0))
        .withTimestampAssigner((element, recordTimestamp) -> Long.parseLong(element))
        .withWatermarkAlignment(""group"", Duration.ofMillis(10), Duration.ofSeconds(2))
        .withIdleness(Duration.ofSeconds(10)); 
DataStreamSource<String> s1 = env.fromSource(kafkaSource(""topic1""), watermarkStrategy, ""S1"");
DataStreamSource<String> s2 = env.fromSource(kafkaSource(""topic2""), watermarkStrategy, ""S2"");{code}
send ""0"" to kafka topic1 and topic2

 

After 10s, source1 and source2 is idle，and logs are

 
{code:java}
09:44:30,403 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:30,404 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:32,019 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:32,019 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:32,417 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:32,418 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ -1 (1970-01-01 07:59:59.999) from subTaskId=0
09:44:34,028 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:34,028 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=9 to subTaskIds=[0]
09:44:34,423 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0
09:44:34,424 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0
09:44:36,023 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=-9223372036854775799 to subTaskIds=[0]
09:44:36,023 INFO  org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - Distributing maxAllowedWatermark=-9223372036854775799 to subTaskIds=[0]
09:44:36,433 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0
09:44:36,433 DEBUG org.apache.flink.runtime.source.coordinator.SourceCoordinator [] - New reported watermark=Watermark @ 9223372036854775807 (292278994-08-17 15:12:55.807) from subTaskId=0 {code}
send message to topic1 or topic2 now, the message can't be consumed。

 

the reason is: 

when a source is marked idle, the lastEmittedWatermark = Long.MAX_VALUE and 
currentMaxDesiredWatermark = Long.MAX_VALUE + maxAllowedWatermarkDrift in org.apache.flink.streaming.api.operators.SourceOperator.
currentMaxDesiredWatermark is negative and always less than lastEmittedWatermark
operatingMode always is WAITING_FOR_ALIGNMENT",,danderson,gyfora,haishui,martijnvisser,mason6345,qinjunjerry,stevenz3wu,tzulitai,wanglijie,xtsong,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 20 16:56:42 UTC 2023,,,,,,,,,,"0|z1gw54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Mar/23 14:42;martijnvisser;[~tzulitai] WDYT?;;;","29/Mar/23 03:23;haishui;This problem seems to be caused by the overflow of maxAllowedWatermark in SourceCoordinator.;;;","14/Apr/23 08:28;haishui;Hi there. The PR may be ready to merge. When can it get into 1.15 1.16 and 1.17 branch? It's the first time for me to report and fix a bug, and I don't know what next to do.;;;","14/Apr/23 16:44;tzulitai;Hey [~haishui], I'm currently taking a look at the PR as well.

Looks like a real issue, thanks for reporting it. I'll backport it to 1.17 / 1.15 / 1.16 once it is merged to \{{master}} .;;;","20/Apr/23 16:56;tzulitai;* 1.18.x: a1daa989e96c7cd1d574a8e998c39b54a4cc503e
 * 1.17.x298fd32b1286e1e5f47fcedbb56d4fee7cfc4eb1
 * 1.16.x: 8d98e6981f9ec137557148a520fac5b81fee1131;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trying to access closed classloader when submit query to restSqlGateway via SqlClient,FLINK-31629,13530229,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,27/Mar/23 10:53,21/Apr/23 03:36,13/Jul/23 08:29,21/Apr/23 03:36,,,,,,,,,,,,,,,Connectors / Hive,Table SQL / Client,,,0,,,,"When I attempted to resubmit the same SQL job(Using HiveCatalog) to SqlGateway through SqlClient, I encountered the error shown in the figure.

!screenshot-1.png|width=649,height=263!

 ",,jark,Weijie Guo,,,,,,,,,,,,,,,,,,,,FLINK-31398,,,,,,,,,,,,,,,,,,,"27/Mar/23 10:54;Weijie Guo;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13056835/screenshot-1.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 03:36:26 UTC 2023,,,,,,,,,,"0|z1gvls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Apr/23 12:15;jark;cc [~fsk119] [~luoyuxia];;;","08/Apr/23 15:01;Weijie Guo;By offline discuss with [~luoyuxia], this issue can be fixed by FLINK-31398. Let's track this issue on that ticket.

 ;;;","21/Apr/23 03:36;Weijie Guo;Fixed by FLINK-31398.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException in watermark processing,FLINK-31628,13530214,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,michael.helmling,michael.helmling,27/Mar/23 09:25,13/Apr/23 08:06,13/Jul/23 08:29,13/Apr/23 08:06,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Runtime / Task,,,,0,pull-request-available,,,"After upgrading a job from Flink 1.16.1 to 1.17.0, my task managers throw the following exception:

 

 
{code:java}
java.lang.ArrayIndexOutOfBoundsException: Index -2147483648 out of bounds for length 5
	at org.apache.flink.streaming.runtime.watermarkstatus.HeapPriorityQueue.removeInternal(HeapPriorityQueue.java:155)
	at org.apache.flink.streaming.runtime.watermarkstatus.HeapPriorityQueue.remove(HeapPriorityQueue.java:100)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve$InputChannelStatus.removeFrom(StatusWatermarkValve.java:300)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve$InputChannelStatus.access$200(StatusWatermarkValve.java:266)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.markWatermarkUnaligned(StatusWatermarkValve.java:222)
	at org.apache.flink.streaming.runtime.watermarkstatus.StatusWatermarkValve.inputWatermarkStatus(StatusWatermarkValve.java:140)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:153)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:550)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:839)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:788)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:931)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562)
	at java.base/java.lang.Thread.run(Unknown Source){code}
I never saw this before. The job has multiple Kafka inputs, but doesn't use watermark alignment.

 

 

Initially reported [on Slack|https://apache-flink.slack.com/archives/C03G7LJTS2G/p1679908171461309], where a relation to FLINK-28853 was suspected.",Kubernetes with Flink operator 1.4.0.,gyfora,jackwangcs,martijnvisser,michael.helmling,wanglijie,xzw0223,yunta,zhuzh,,,,,,,,,,,,,,,,,,,,,,FLINK-30544,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 13 08:06:30 UTC 2023,,,,,,,,,,"0|z1gvig:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 09:34;martijnvisser;[~mxm] My initial thinking was that this might be caused by FLINK-28853, WDYT?;;;","07/Apr/23 04:27;wanglijie;This is caused by FLINK-30544, in which we introduced a heap to help find the minimum watermark. This error occurs when the channel status changes like this:

phase 1: channel becomes IDLE (receives WatermarkStatus.IDLE)
phase 2: channel becomes ACTIVE (receives WatermarkStatus.ACTIVE), but current watermark of the channel is less than the last output watermark (last watermark sent to downstream tasks)
phase 3: channel becomes IDLE again (receives WatermarkStatus.IDLE again)

In phase 1, we remove the channel from the heap because it is idle, should no longer participate in the calculation of the minimum watermark. In phase 2, although the channel becomes active, but its watermark is expired(less than the last output watermark), so we don't add it back to the heap. And then in phase 3, we try to remove the channel again, but the channel is not in the heap, which causes the above problem.;;;","07/Apr/23 04:30;wanglijie;We should check if the channel is in the heap before removing it(or let the heap does not throw exception when trying to remove an absent element). I'll prepare a fix soon.;;;","07/Apr/23 04:36;wanglijie;This problem may occur when the source configures {{table.exec.source.idle-timeout}} or uses the {{WatermarkStrategyWithIdleness}} strategy;;;","13/Apr/23 08:06;wanglijie;Fixed via 

master: a9bc791abdff5bd8e4cb3d67bb1417b2f02c990a

release-1.17: 508de2f41fcbd2a4d64a5e8dd291926e273b4f50;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docker-build.sh build fails on Linux machines,FLINK-31627,13530212,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,27/Mar/23 09:16,29/Mar/23 14:33,13/Jul/23 08:29,29/Mar/23 14:33,,,,,,,,,,,,,,,Project Website,,,,0,pull-request-available,,,"Building the Flink website on Linux fails due to how Docker is used as a Daemon running under root in Linux (see [this blog|https://jtreminio.com/blog/running-docker-containers-as-current-host-user/#ok-so-what-actually-works] for more details).

Building the website will fail when copying the artifacts because they are owned by the root user. 

{code}
./docker-build.sh build                                                                                     
latest: Pulling from jakejarvis/hugo-extended                                                                                                
Digest: sha256:7d7eb41d7949b5ed338c27926098b84e152e7e1d8ad8f1955c29b383a2336548                                                              
Status: Image is up to date for jakejarvis/hugo-extended:latest                                                                              
docker.io/jakejarvis/hugo-extended:latest                                                                                                    
Start building sites …                                                                                                                       
hugo v0.111.3-5d4eb5154e1fed125ca8e9b5a0315c4180dab192+extended linux/amd64 BuildDate=2023-03-12T11:40:50Z VendorInfo=docker                 
Error: Error building site: open /src/target/news/2014/08/26/release-0.6.html: permission denied                                             
Total in 153 ms                                                                                                                              
mv: cannot move 'docs/target/2014' to 'content/2014': Permission denied                                                                      
mv: cannot move 'docs/target/2015' to 'content/2015': Permission denied                                                                      
mv: cannot move 'docs/target/2016' to 'content/2016': Permission denied                                                                      
mv: cannot move 'docs/target/2017' to 'content/2017': Permission denied                                                                      
mv: cannot move 'docs/target/2018' to 'content/2018': Permission denied                                                                      
mv: cannot move 'docs/target/2019' to 'content/2019': Permission denied                                                                      
mv: cannot move 'docs/target/2020' to 'content/2020': Permission denied                                                                      
mv: cannot move 'docs/target/2021' to 'content/2021': Permission denied                                                                      
mv: cannot move 'docs/target/2022' to 'content/2022': Permission denied                                                                                                                                                                                                                   
mv: cannot move 'docs/target/2023' to 'content/2023': Permission denied                                                                      
mv: cannot move 'docs/target/categories' to 'content/categories': Permission denied
[...]
{code}",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-22922,,,,,,,,,FLINK-31081,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 14:33:24 UTC 2023,,,,,,,,,,"0|z1gvi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 14:33;mapohl;asf-site: a935b1201c1197ab05406e0db5a5206cdba0608d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsSubpartitionFileReaderImpl should recycle skipped read buffers.,FLINK-31626,13530203,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,27/Mar/23 08:45,31/Mar/23 09:42,13/Jul/23 08:29,31/Mar/23 09:42,1.17.1,,,,,,1.17.1,1.18.0,,,,,,,Runtime / Network,,,,0,pull-request-available,,,"In FLINK-30189, {{HsSubpartitionFileReaderImpl}} will skip the buffer has been consumed from memory to avoid double-consumption. But these buffers were not returned to the {{BatchShuffleReadBufferPool}}, resulting in read buffer leaks. In addition, all loaded buffers should also be recycled after data view released.",,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 31 09:42:16 UTC 2023,,,,,,,,,,"0|z1gvg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Mar/23 09:42;Weijie Guo;master(1.18) via 59462197ba725b9fc0118ec54ec9f1325b8a874a.
release-1.17 via 0f59c8f7b161e96f026529f542c00b1094107371.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix DataStreamUtils#sample with approximate uniform sampling,FLINK-31623,13530191,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hongfanxo,hongfanxo,27/Mar/23 07:25,03/Apr/23 07:16,13/Jul/23 08:29,03/Apr/23 07:16,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"Current implementation employs two-level sampling method.

However, when data instances are imbalanced distributed among partitions (subtasks), the probabilities of instances to be sampled are different in different partitions (subtasks), i.e., not a uniform sampling.

 

In addition, one side-effect of current implementation is: one subtask has a memory footprint of `2 * numSamples * sizeof(element)`, which could cause unexpected OOM in some situations.",,hongfanxo,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 03 07:16:56 UTC 2023,,,,,,,,,,"0|z1gvdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Apr/23 07:16;zhangzp;Resolved on master via fe338b194b73fd51218f4d842fa7b0065fb76c56;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ReducingUpsertWriter does not flush the wrapped writer,FLINK-31620,13530148,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Gerrrr,Gerrrr,Gerrrr,26/Mar/23 22:33,12/Apr/23 17:11,13/Jul/23 08:29,12/Apr/23 17:05,1.16.1,1.17.0,,,,,kafka-3.0.0,,,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"According to {{SinkWriter#flush}} [javadoc|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/api/connector/sink2/SinkWriter.java#L43-L47], the writer must flush its records to guarantee AT_LEAST_ONCE.

{{upsert-kafka}}'s {{ReducingUpsertWriter}} inserts buffered records into the wrapped writer, but does not flush it:
1. SinkWriter#flush implementation - https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java#L88-L91.
2. The actual flush code - https://github.com/apache/flink/blob/f3c653ed2e4264315ed83a5b4b2494a7dcc41474/flink-connectors/flink-connector-kafka/src/main/java/org/apache/flink/streaming/connectors/kafka/table/ReducingUpsertWriter.java#L143-L150.
",,Gerrrr,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 17:05:47 UTC 2023,,,,,,,,,,"0|z1gv3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 17:05;tzulitai;Merged to flink-connector-kafka via 15b51f17a1daa2edb4cd17b4100eb407d83c8eb2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClassNotFoundException when using GCS path as HA directory,FLINK-31612,13530050,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,mohit.aggarwal,mohit.aggarwal,25/Mar/23 08:13,28/Mar/23 14:32,13/Jul/23 08:29,28/Mar/23 14:32,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,FileSystems,,,,0,pull-request-available,,,"Hi,

When I am trying to run Flink job in HA mode with GCS path as a HA directory (eg: [gs://flame-poc/ha]) or while starting a job from checkpoints in GCS I am getting following exception:
{code:java}
Caused by: java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback not found
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2688) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2712) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.<init>(Groups.java:107) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.<init>(Groups.java:102) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:451) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:338) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300) ~[?:?]
	at org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575) ~[?:?]
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.getUgiUserName(GoogleHadoopFileSystemBase.java:1226) ~[?:?]
	at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.listStatus(GoogleHadoopFileSystemBase.java:858) ~[?:?]
	at org.apache.flink.fs.gs.org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.listStatus(HadoopFileSystem.java:170) ~[?:?] {code}
{*}Observations{*}:

While using File system as a HA path and GCS as checkpointing directory the job is able to write checkpoints to GCS checkpoint path. 

After debugging what I found was all the *org.apache.hadoop* paths are shaded to {*}org.apache.flink.fs.shaded.hadoop3.org{*}{*}.apache.hadoop{*}. Ideally the code should look for  {*}org.apache.flink.fs.shaded.hadoop3.org{*}{*}.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback{*} instead of  *org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback.*
I think it is not getting shaded over here due to reflection being used here:
[https://github.com/apache/hadoop/blob/branch-3.3.4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/Groups.java#L108]

As a workaround I rebuilt *flink-gs-fs-hadoop* plugin removing this relocation and it worked for me.
{code:java}
<relocation>
<pattern>org.apache.hadoop</pattern>
<shadedPattern>org.apache.flink.fs.shaded.hadoop3.org.apache.hadoop</shadedPattern>
</relocation> {code}","Flink Kuberenetes operator: 1.4

Flink version: 1.17

GKE Kubernetes cluster.

 ",czchen,martijnvisser,mohit.aggarwal,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 09:14:24 UTC 2023,,,,,,,,,,"0|z1gui0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Mar/23 01:30;czchen;The following information are from slack:
 * From Ta-Chun in slack, this is possible caused by https://github.com/apache/flink/pull/21318.
 * From Jessy, this issue alsp happens in Azure blob storage and ADLS.;;;","27/Mar/23 08:18;martijnvisser;I'll check with [~chesnay] on this;;;","27/Mar/23 08:25;chesnay;The problem is that the azure/gs filesystems still relocated hadoop which isn't required anymore.;;;","27/Mar/23 11:12;martijnvisser;Could those who are running into this problem validate that this is resolved when using https://github.com/apache/flink/pull/22279 ?;;;","28/Mar/23 01:05;czchen;We have confirmed this PR can fix GCS backend problem.;;;","28/Mar/23 09:14;martijnvisser;Fixed in:

master: 3924ba85e62f04a0a181007d573cb84e3c09a636
release-1.17: 52a2b98bb5af842633df0c051b5da95d437a6b2f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fatal error in ResourceManager caused YARNSessionFIFOSecuredITCase.testDetachedMode to fail,FLINK-31609,13529972,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,ferenc-csaky,mapohl,mapohl,24/Mar/23 12:33,15/May/23 15:09,13/Jul/23 08:29,15/May/23 15:09,1.18.0,,,,,,1.18.0,,,,,,,,Deployment / YARN,,,,0,pull-request-available,test-stability,,"This looks like FLINK-30908. I created a follow-up ticket because we reached a new minor version.

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47547&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461

{code}
Mar 24 09:32:29 2023-03-24 09:31:50,001 ERROR org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Exception on heartbeat
Mar 24 09:32:29 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Mar 24 09:32:29 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client.call(Client.java:1461) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client.call(Client.java:1403) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at com.sun.proxy.$Proxy33.allocate(Unknown Source) ~[?:?]
Mar 24 09:32:29 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-2.10.2.jar:?]
Mar 24 09:32:29 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
Mar 24 09:32:29 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
Mar 24 09:32:29 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
Mar 24 09:32:29 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:433) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:166) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:158) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:96) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:362) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at com.sun.proxy.$Proxy34.allocate(Unknown Source) ~[?:?]
Mar 24 09:32:29 	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:297) ~[hadoop-yarn-client-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:274) [hadoop-yarn-client-2.10.2.jar:?]
Mar 24 09:32:29 Caused by: java.lang.InterruptedException
Mar 24 09:32:29 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]
Mar 24 09:32:29 	at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1177) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	at org.apache.hadoop.ipc.Client.call(Client.java:1456) ~[hadoop-common-2.10.2.jar:?]
Mar 24 09:32:29 	... 17 more
{code}",,ferenc-csaky,gaborgsomogyi,huwh,knaufk,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30908,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 15 15:09:49 UTC 2023,,,,,,,,,,"0|z1gu0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 08:24;knaufk;[~ferenc-csaky] Hi Ferenc, do you or someone in your team have time to look into this?;;;","18/Apr/23 12:06;ferenc-csaky;Yeah, we can give it a check in the next 1-2 weeks, feel free to assign it to me.;;;","08/May/23 12:06;ferenc-csaky;According to the logs, the fatal exception in the {{ResourceManager}} does not happen anymore (cause of the changes made in FLINK-30908). According to [~xtsong]'s analysis on FLINK-30908, the AMRM heartbeat interruption can happen anyways, so the ex about the interrupt is written to the logs, hence the failure, because the string
{code}
java.io.InterruptedIOException: Interrupted waiting to send RPC request to server {code}
is not whitelisted. If we are okay whitelisting that exactly, it should fix the tests.;;;","15/May/23 15:09;gaborgsomogyi;4415c5150eda071b219db5532c359ca29730a378 on master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The unaligned checkpoint type is wrong at subtask level,FLINK-31588,13529747,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,fanrui,fanrui,23/Mar/23 10:45,25/Apr/23 02:23,13/Jul/23 08:29,25/Apr/23 02:23,1.16.0,1.17.0,,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"FLINK-20488 supported show checkpoint type for each subtask, and it based on received `CheckpointOptions` and it's right.

However, FLINK-27251 supported timeout aligned to unaligned checkpoint barrier in the output buffers. It means the received `CheckpointOptions` can be converted from aligned checkpoint to unaligned checkpoint.

So, the unaligned checkpoint type may be wrong at subtask level. For example, as shown in the figure below, Unaligned checkpoint type is false, but it is actually Unaligned checkpoint (persisted data > 0).

 

!image-2023-03-23-18-45-01-535.png|width=1879,height=797!

 ",,fanrui,pnowojski,RocMarshal,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27251,,,,FLINK-27251,,FLINK-31864,,,,,"23/Mar/23 10:45;fanrui;image-2023-03-23-18-45-01-535.png;https://issues.apache.org/jira/secure/attachment/13056617/image-2023-03-23-18-45-01-535.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 23 02:15:51 UTC 2023,,,,,,,,,,"0|z1gsmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Mar/23 11:02;fanrui;Hi [~pnowojski] [~yuanmei] , please help take a look this ticket in your free time, thanks!

I prefer generate unaligned checkpoint type based on persisted data. If it is switched to an unaligned checkpoint and no data is persisted, it is still considered an aligned checkpoint for flink users. WDYT?;;;","07/Apr/23 20:21;pnowojski;Sorry for late response, I've just found an old tab with WIP comment that I wanted to write, but somehow didn't send as something must have interrupted me :(

Thanks for reporting the issue. I see the problem. I think ideally we should try to keep the semantic of that flag in sync with what {{StreamTask}} was actually doing. If checkpoint was unaligned, as it arrived unaligned, it should be reported as such, even if that particular subtask didn't persist any data. Can we still achieve that? ;;;","08/Apr/23 02:11;fanrui;Thanks for your feedback.
{quote} If checkpoint was unaligned, as it arrived unaligned, it should be reported as such, even if that particular subtask didn't persist any data.
{quote}
Sounds make sense. I will prepare this PR next week.;;;","23/Apr/23 02:15;fanrui;Thanks [~pnowojski] for the review and discussion.

Merged master commit: d46d8d0f6b590f185608b23fbe8b2fcbded111de

Merged 1.17-release commit : a3c00eed371f8a2c9bfe557fa07dc7bc8a04f14e

Merged 1.16-release commit : 60cadec7c1f3beb3c9eb7e45cbd7ab9d99062e48;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"flink-master-regression-check is failing since March 15, 2023",FLINK-31561,13529585,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Yanfei Lei,mapohl,mapohl,22/Mar/23 10:18,27/Mar/23 11:29,13/Jul/23 08:29,23/Mar/23 13:17,1.18.0,,,,,,1.18.0,,,,,,,,Benchmarks,,,,0,pull-request-available,,,"[flink-master-benchmarks-regression-check|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/] is failing since March 15, 2023:
{code}
Started by timer
hudson.plugins.git.GitException: Command ""git fetch --tags --progress --prune origin +refs/heads/cutoff:refs/remotes/origin/cutoff"" returned status code 128:
stdout: 
stderr: fatal: Couldn't find remote ref refs/heads/cutoff

	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandIn(CliGitAPIImpl.java:2372)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.launchCommandWithCredentials(CliGitAPIImpl.java:1985)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl.access$500(CliGitAPIImpl.java:80)
	at org.jenkinsci.plugins.gitclient.CliGitAPIImpl$1.execute(CliGitAPIImpl.java:563)
	at jenkins.plugins.git.GitSCMFileSystem$BuilderImpl.build(GitSCMFileSystem.java:348)
	at jenkins.scm.api.SCMFileSystem.of(SCMFileSystem.java:197)
	at jenkins.scm.api.SCMFileSystem.of(SCMFileSystem.java:173)
	at org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition.create(CpsScmFlowDefinition.java:115)
	at org.jenkinsci.plugins.workflow.cps.CpsScmFlowDefinition.create(CpsScmFlowDefinition.java:69)
	at org.jenkinsci.plugins.workflow.job.WorkflowRun.run(WorkflowRun.java:299)
	at hudson.model.ResourceController.execute(ResourceController.java:97)
	at hudson.model.Executor.run(Executor.java:429)
Finished: FAILURE
{code}

As a consequence, no regressions are reported in Slack",,KristoffSC,mapohl,pnowojski,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29825,,,,,,,,,,,"23/Mar/23 08:40;Yanfei Lei;image-2023-03-23-16-40-05-880.png;https://issues.apache.org/jira/secure/attachment/13056607/image-2023-03-23-16-40-05-880.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 27 11:29:00 UTC 2023,,,,,,,,,,"0|z1grmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 10:23;mapohl;[~Yanfei Lei] can you help out with this issue. I'm still a bit confused on why we're expecting a {{cutoff}} branch here? I couldn't find anything related to it in the {{flink-benchmarks}} code repository.;;;","22/Mar/23 10:31;Yanfei Lei;I updated the [regression check script|https://issues.apache.org/jira/browse/FLINK-29825] but forgot to update the Jenkins configuration. Fixed it now, let's wait for the result of next run.;;;","22/Mar/23 11:31;mapohl;Cool, thanks for the clarification (y) I'm a bit puzzled why we see the test failure starting on March 15 whereas the commit ended up on master on March 10 already.;;;","22/Mar/23 13:05;mapohl;The initial issue seems to be fixed but the builds are still failing:
{code:java}
/home/jenkins/workspace/flink-master-benchmarks-regression-check/flink-benchmarks@tmp/durable-9c5531ba/script.sh: 1: ./regression_report_v2.py: Permission denied {code};;;","23/Mar/23 07:32;Yanfei Lei;[~mapohl] I add execute permission for regression check script, could you please help take a look?

> The test failure starting on March 15 whereas the commit ended up on master on March 10 already.

 

Because the Jenkins was configured to FLINK-30870's branch, I updated the Jenkins configuration to the master branch on March 15, the new detection algorithm has not detected any regression recently, so I didn't notice that the pipeline failed🤦‍♂️

 ;;;","23/Mar/23 08:29;mapohl;There's still a permission. It seem to have worked temporarily?
{quote}Because the Jenkins was configured to FLINK-30870's branch, I updated the Jenkins configuration to the master branch on March 15, the new detection algorithm has not detected any regression recently, so I didn't notice that the pipeline failed🤦‍♂️
{quote}
Ah, FLINK-30870 is where the term ""cutoff"" comes from. :D Just a hint: It's easier to for investigations to use Jira issues for branch names (or at least add the Jira issue along the descriptive name) to make it easier investigate things. :);;;","23/Mar/23 08:43;Yanfei Lei;> It seem to have worked temporarily?

I temporarily configured Jenkins pipeline and [jenkinsfile|https://github.com/apache/flink-benchmarks/blob/master/jenkinsfiles/regression-check.jenkinsfile#L23] to my repo for testing, [#1559|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1559/] and [#1560|http://codespeed.dak8s.net:8080/job/flink-master-benchmarks-regression-check/1559/] temporarily can work. Now Jenkins pipeline(like below screenshot) is reverted to the master branch, it should work after we merging this PR into master.

!image-2023-03-23-16-40-05-880.png|width=530,height=211!;;;","23/Mar/23 13:17;pnowojski;Hopefully fixed by commit d797583 into flink-benchmarks:master;;;","27/Mar/23 11:29;mapohl;FYI: I verified that the script is not failing anymore. (y);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Metric viewUpdater and reporter task in a SingleThreadScheduledExecutor lead to inaccurate PerSecond related metrics,FLINK-31557,13529531,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,huwh,LiuZeshan,LiuZeshan,22/Mar/23 03:06,30/Mar/23 13:57,13/Jul/23 08:29,30/Mar/23 13:56,,,,,,,1.17.1,1.18.0,,,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"Currently, metric viewUpdater and reporterTask share the same SingleThreadScheduledExecutor, and customized reporters may have unpredictable logic, such as unreasonable network timeout settings, which can affect viewUpdater's calculation of PerSecond related metrics. For example, a real online problem we encountered, the network timeout of the reporter is set to 10 seconds, and the reporting interval is 15 seconds. When the server is unavailable, the thread is blocked for 10s, resulting in 66.7% (5/3x) higher PerSecond related metrics.

Is it possible to optimize here, such as whether it can be changed to a ScheduledThreadPool executor?",,huwh,LiuZeshan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 13:56:55 UTC 2023,,,,,,,,,,"0|z1grao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 04:06;huwh;[~LiuZeshan] The MeterView does not record timestamp for each event. This reduces memory usage, but on the other hand, it relies on a just-in-time invoke to update the rate.

May be we could split a new SingleThreadScheduledExecutor for ViewUpdater to get ride of reporters. cc [~chesnay];;;","22/Mar/23 08:15;chesnay;??a new SingleThreadScheduledExecutor for ViewUpdater??

This makes sense to me.
Alternatively we could define a contract where the report() method must not block for a significant amount of time, and any reporter needing more time should instead run the operation asynchronously.;;;","22/Mar/23 08:47;huwh;[~chesnay] could you assign this to me? I will add a new SingleThreadScheduledExecutor for ViewUpdater, and add the contract to our documents;;;","30/Mar/23 13:56;chesnay;master: e5b553a12068511949b0415fc7e4c7b6930819ad
1.17: ae487b1ac16f710194a6b90bd61475a84919f0d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get metrics in Flink WEB UI error,FLINK-31541,13529430,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,huwh,tanjialiang,tanjialiang,21/Mar/23 12:32,29/Mar/23 08:54,13/Jul/23 08:29,29/Mar/23 08:54,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Runtime / Metrics,Runtime / Web Frontend,,,0,pull-request-available,,,"When i get a metrics from a operator which name contains '[' or ']', it will be return 400 from rest response.

The reason is we can not submit an GET request which params contains '[' or ']', it is invaild in REST.

 

!image-2023-03-21-20-28-56-348.png!",,huwh,tanjialiang,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/23 12:28;tanjialiang;image-2023-03-21-20-28-56-348.png;https://issues.apache.org/jira/secure/attachment/13056542/image-2023-03-21-20-28-56-348.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 09:51:08 UTC 2023,,,,,,,,,,"0|z1gqo8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Mar/23 14:30;huwh;[~tanjialiang] Thanks report this. It's caused by we put parameters in url directly, so http component not encode the special characters. 

I would like to take this ticket.;;;","28/Mar/23 09:51;Weijie Guo;master(1.18) via de27786eba56ce37c14f413eb8d7ba404eab50e1.

release-1.17 via 1e0b58aa8d962469fa9dd7b470037aeaece43500.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test HiveCatalogHiveMetadataTest.testCreateTableWithConstraint failed on azure,FLINK-31531,13529351,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,leonard,leonard,21/Mar/23 04:50,22/Mar/23 08:02,13/Jul/23 08:29,22/Mar/23 06:10,1.18.0,,,,,,1.18.0,,,,,,,,Connectors / Hive,,,,0,pull-request-available,,,"{noformat}
Mar 21 01:11:13 [ERROR] Failures: 
Mar 21 01:11:13 [ERROR]   HiveCatalogHiveMetadataTest.testCreateTableWithConstraints:295 
Mar 21 01:11:13 Expecting value to be true but was false
Mar 21 01:11:13 [INFO] 
Mar 21 01:11:13 [ERROR] Tests run: 370, Failures: 1, Errors: 0, Skipped: 0
{noformat}
 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47400&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f",,leonard,luoyuxia,mapohl,xzw0223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 22 08:02:58 UTC 2023,,,,,,,,,,"0|z1gq6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Mar/23 06:10;luoyuxia;master: e342f127e7e177e3d7be88c729ff440689614dd9;;;","22/Mar/23 08:02;mapohl;This build failure didn't contain the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47435&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=22877;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogRescalingITCase.test failed due to FileNotFoundException,FLINK-31527,13529202,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,20/Mar/23 07:56,22/Mar/23 17:00,13/Jul/23 08:29,22/Mar/23 17:00,1.17.0,,,,,,1.16.2,1.17.1,,,,,,,Runtime / State Backends,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47369&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=10238

{code}
Mar 20 01:31:54 [ERROR] Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 15.209 s <<< FAILURE! - in org.apache.flink.test.state.ChangelogRescalingITCase
Mar 20 01:31:54 [ERROR] ChangelogRescalingITCase.test  Time elapsed: 8.492 s  <<< FAILURE!
Mar 20 01:31:54 org.opentest4j.AssertionFailedError: Graph is in globally terminal state (FAILED)
Mar 20 01:31:54 	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:42)
Mar 20 01:31:54 	at org.junit.jupiter.api.Assertions.fail(Assertions.java:147)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.lambda$waitForAllTaskRunning$3(CommonTestUtils.java:213)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:150)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:208)
Mar 20 01:31:54 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForAllTaskRunning(CommonTestUtils.java:181)
Mar 20 01:31:54 	at org.apache.flink.test.state.ChangelogRescalingITCase.test(ChangelogRescalingITCase.java:163)
Mar 20 01:31:54 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 20 01:31:54 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 20 01:31:54 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 20 01:31:54 	at java.lang.reflect.Method.invoke(Method.java:498)
Mar 20 01:31:54 	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
Mar 20 01:31:54 	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
Mar 20 01:31:54 	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
Mar 20 01:31:54 	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
{code}",,Feifan Wang,leonard,mapohl,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 22 17:00:24 UTC 2023,,,,,,,,,,"0|z1gp9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 07:57;mapohl;[~roman] May you verify whether that's a 1.17-specific issue justifying the blocker priority or whether we can downgrade it to Critical?;;;","20/Mar/23 13:03;roman;I'd suppose it's not 1.17-specific, I'll try to verify that.;;;","21/Mar/23 14:43;mapohl;Any updates?;;;","21/Mar/23 21:17;roman;The issue is indeed not 1.17-specific:
{code:java}
ChangelogRescalingITCase failed on 1.17 with FileNotFoundException after restore attempt from chk-73.
At the same time, the log contained a message that chk-74 was not deleted. Meaning chk-74, not chk-73 was the latest checkpoint.
This is caused by not properly waiting for job termination in ChangelogRescalingITCase.checkpointAndCancel.{code}
I've opened a PR to fix it and going to lower the priority since it's a pure-test-stability issue.;;;","22/Mar/23 17:00;roman;Fix merged into master as 9ba3b1a5863c1aeeca0be25b4bb375abfe02b940

1.16 as 80ee512f00a9a8873926626d66cdcc97164c4595;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The watermark alignment docs is outdated after FLIP-217 finished,FLINK-31519,13529159,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mason6345,leonard,leonard,20/Mar/23 03:16,30/May/23 13:49,13/Jul/23 08:29,30/May/23 13:49,1.17.0,,,,,,1.17.2,1.18.0,,,,,,,Documentation,,,,0,,,,"With FLIP-217 finished, the watermark alignment limitation has been resolved, the *beta* tag and *warning note* can be safely removed.

[1] https://nightlies.apache.org/flink/flink-docs-release-1.17/docs/dev/datastream/event-time/generating_watermarks/#watermark-alignment-_beta_",,jingge,leonard,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28853,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 30 13:49:35 UTC 2023,,,,,,,,,,"0|z1gp00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Mar/23 10:06;jingge;IMHO, the watermark alignment should still be marked as beta, because:
 * only Kafka and Pulsar connectors support watermark alignment of source splits
 * there are some hints left in the source code for future work[1] 

 

[1]https://github.com/apache/flink/blob/0180284381bf7999781d542219d2a097f3cbc098/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/source/reader/splitreader/SplitReader.java#L94;;;","30/May/23 13:49;pnowojski;Fixed via FLINK-28853

merged commit 678370b into apache:master
merged commit 6ccc3da into apache:release-1.17;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""org.apache.beam.sdk.options.PipelineOptionsRegistrar: Provider org.apache.beam.sdk.options.DefaultPipelineOptionsRegistrar not a subtype"" is thrown when executing Python UDFs in SQL Client ",FLINK-31503,13528980,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,17/Mar/23 14:10,19/Mar/23 06:49,13/Jul/23 08:29,19/Mar/23 06:49,1.15.0,,,,,,1.16.2,1.17.1,1.18.0,,,,,,API / Python,,,,0,pull-request-available,,,"The following exception will be thrown when executing SQL statements containing Python UDFs in SQL Client:
{code}
Caused by: java.util.ServiceConfigurationError: org.apache.beam.sdk.options.PipelineOptionsRegistrar: Provider org.apache.beam.sdk.options.DefaultPipelineOptionsRegistrar not a subtype
        at java.util.ServiceLoader.fail(ServiceLoader.java:239)
        at java.util.ServiceLoader.access$300(ServiceLoader.java:185)
        at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:376)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableCollection$Builder.addAll(ImmutableCollection.java:415)
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSet$Builder.addAll(ImmutableSet.java:507)
        at org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableSortedSet$Builder.addAll(ImmutableSortedSet.java:528)
        at org.apache.beam.sdk.util.common.ReflectHelpers.loadServicesOrdered(ReflectHelpers.java:199)
        at org.apache.beam.sdk.options.PipelineOptionsFactory$Cache.initializeRegistry(PipelineOptionsFactory.java:2089)
        at org.apache.beam.sdk.options.PipelineOptionsFactory$Cache.<init>(PipelineOptionsFactory.java:2083)
        at org.apache.beam.sdk.options.PipelineOptionsFactory$Cache.<init>(PipelineOptionsFactory.java:2047)
        at org.apache.beam.sdk.options.PipelineOptionsFactory.resetCache(PipelineOptionsFactory.java:581)
        at org.apache.beam.sdk.options.PipelineOptionsFactory.<clinit>(PipelineOptionsFactory.java:547)
        at org.apache.flink.streaming.api.runners.python.beam.BeamPythonFunctionRunner.open(BeamPythonFunctionRunner.java:241)
{code}",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 06:49:00 UTC 2023,,,,,,,,,,"0|z1gnwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 15:37;dianfu;If your Flink version doesn't contain this fix, you could simply work around this issue by adding the following configuration:

SET 'classloader.parent-first-patterns.additional' = 'org.apache.beam.';;;;","19/Mar/23 06:49;dianfu;Fixed in:
- master via de258f3ce01e720d23bec67c20892133f89293d3
- release-1.17 via 3163b8f9caa53e9487ce062eba2c3d399dfe08a2
- release-1.16 via d438b3bdc48a0456088594700d438725d0fb1480;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AWS Firehose Connector misclassifies IAM permission exceptions as retryable,FLINK-31492,13528830,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,samuelsiebenmann,samuelsiebenmann,samuelsiebenmann,16/Mar/23 16:40,28/Mar/23 17:11,13/Jul/23 08:29,28/Mar/23 17:11,aws-connector-4.1.0,,,,,,aws-connector-4.2.0,,,,,,,,Connectors / AWS,Connectors / Firehose,,,0,pull-request-available,,,"The AWS Firehose connector uses an exception classification mechanism to decide if errors writing requests to AWS Firehose are fatal (i.e. non-retryable) or not (i.e. retryable).
{code:java}
private boolean isRetryable(Throwable err) {
    if (!FIREHOSE_FATAL_EXCEPTION_CLASSIFIER.isFatal(err, getFatalExceptionCons())) {
        return false;
    }
    if (failOnError) {
        getFatalExceptionCons()
                .accept(new KinesisFirehoseException.KinesisFirehoseFailFastException(err));
        return false;
    }

    return true;
} {code}
([github|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws-kinesis-firehose/src/main/java/org/apache/flink/connector/firehose/sink/KinesisFirehoseSinkWriter.java#L252])

This exception classification mechanism compares an exception's actual type with known, fatal exception types (by using Flink's [FatalExceptionClassifier.withExceptionClassifier|https://github.com/apache/flink/blob/master/flink-connectors/flink-connector-base/src/main/java/org/apache/flink/connector/base/sink/throwable/FatalExceptionClassifier.java#L60]).  An exception is considered fatal if it is assignable to a given known fatal exception ([code|https://github.com/apache/flink/blob/master/flink-core/src/main/java/org/apache/flink/util/ExceptionUtils.java#L479]).

The AWS Firehose SDK throws fatal IAM permission exceptions as [FirehoseException|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/firehose/model/FirehoseException.html]s, e.g.
{code:java}
software.amazon.awssdk.services.firehose.model.FirehoseException: User: arn:aws:sts::000000000000:assumed-role/example-role/kiam-kiam is not authorized to perform: firehose:PutRecordBatch on resource: arn:aws:firehose:us-east-1:000000000000:deliverystream/example-stream because no identity-based policy allows the firehose:PutRecordBatch action{code}
At the same time, certain subtypes of FirehoseException are retryable and non-fatal (e.g.[https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/firehose/model/LimitExceededException.html]).

The AWS Firehose connector currently wrongly classifies the fatal IAM permission exception as non-fatal. However, the current exception classification mechanism does not easily handle a case where a super-type should be considered fatal, but its child type shouldn't.

To address this issue, AWS services and the AWS SDK use error codes (see e.g. [Firehose's error codes|https://docs.aws.amazon.com/firehose/latest/APIReference/CommonErrors.html] or [S3's error codes|https://docs.aws.amazon.com/AmazonS3/latest/API/ErrorResponses.html#ErrorCodeList], see API docs [here|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/awscore/exception/AwsErrorDetails.html#errorCode()] and [here|https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/awscore/exception/AwsServiceException.html#awsErrorDetails()]) to uniquely identify error conditions and to be used to handle errors by type.

The AWS Firehose connector (and other AWS connectors) currently log to debug when retrying fully failed records ([code|https://github.com/apache/flink-connector-aws/blob/main/flink-connector-aws-kinesis-firehose/src/main/java/org/apache/flink/connector/firehose/sink/KinesisFirehoseSinkWriter.java#L213]). This makes it difficult for users to root cause the above issue without enabling debug logs.

 

 

 ",,dannycranmer,samuelsiebenmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 17:11:32 UTC 2023,,,,,,,,,,"0|z1gmz4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 16:48;dannycranmer;Thanks for reporting this [~samuelsiebenmann] . I have assigned the Jira to you.;;;","28/Mar/23 17:11;dannycranmer;Merged commit [{{d166ee2}}|https://github.com/apache/flink-connector-aws/commit/d166ee24bdd2b238f1d909912ec1d038732ec1c4] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperationManagerTest.testCloseOperation failed because it couldn't find a submitted operation,FLINK-31489,13528822,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,mapohl,mapohl,16/Mar/23 15:56,17/Mar/23 11:36,13/Jul/23 08:29,17/Mar/23 11:36,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47218&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=13386

{code}
Mar 16 02:49:52 [ERROR] Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 30.433 s <<< FAILURE! - in org.apache.flink.table.gateway.service.operation.OperationManagerTest
Mar 16 02:49:52 [ERROR] org.apache.flink.table.gateway.service.operation.OperationManagerTest.testCloseOperation  Time elapsed: 0.042 s  <<< ERROR!
Mar 16 02:49:52 org.apache.flink.table.gateway.api.utils.SqlGatewayException: Can not find the submitted operation in the OperationManager with the 1734d6cf-cf52-40c5-804f-809e48a9818a.
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$getOperation$3(OperationManager.java:487)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.readLock(OperationManager.java:518)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.getOperation(OperationManager.java:482)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManager.awaitOperationTermination(OperationManager.java:149)
Mar 16 02:49:52 	at org.apache.flink.table.gateway.service.operation.OperationManagerTest.testCloseOperation(OperationManagerTest.java:199)
Mar 16 02:49:52 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,leonard,lincoln.86xy,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 10:49:52 UTC 2023,,,,,,,,,,"0|z1gmxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 15:58;mapohl;[~shengkai] this issue failed on {{master}}. Please deprioritize this issue to Critical if it's not a 1.17 issue or a test code instability.;;;","17/Mar/23 04:34;lincoln.86xy;[~mapohl] seems it's an instable test(confirmed with [~shengkai] offline, he's on vocation), the test code can be improved, I've submited a fix and deprioritize this issue to Critical.;;;","17/Mar/23 10:49;leonard;Fixed in :

master: ff08c7b1d7fb9334d624aa74bc5788d4d7ac1edc

release-1.17: f0a7dcc4a1c4b3c4b2e4e7029e8f60fb7e7720d2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Using KeySelector in IterationBody causes ClassCastException,FLINK-31486,13528790,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,Jiang Xin,Jiang Xin,16/Mar/23 12:10,21/Mar/23 03:06,13/Jul/23 08:29,21/Mar/23 03:06,,,,,,,ml-2.2.0,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"We have the following code which uses CoGroup along with KeySelector in an IterationBody. When we submit to Flink Session cluster, the exception raises.
{code:java}
public static void main(String[] args) throws Exception {
    Configuration config = new Configuration();
    config.set(HeartbeatManagerOptions.HEARTBEAT_TIMEOUT, 5000000L);
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(config);
    env.setStateBackend(new EmbeddedRocksDBStateBackend());
    env.getConfig().enableObjectReuse();
    env.setRestartStrategy(RestartStrategies.noRestart());
    env.setParallelism(1);
    env.getCheckpointConfig().disableCheckpointing();

    StreamTableEnvironment tEnv = StreamTableEnvironment.create(env);

    int num = 400;
    int types = num / 10;

    Random rand = new Random(0);
    long[] randoms = new long[types];
    for (int i = 0; i < types; i++) {
        randoms[i] = rand.nextInt(types);
    }

    SourceFunction<Row> rowGenerator =
            new SourceFunction<Row>() {
                @Override
                public final void run(SourceContext<Row> ctx) throws Exception {
                    int cnt = 0;
                    while (cnt < num) {
                        ctx.collect(
                                Row.of(
                                        randoms[cnt % (types)],
                                        randoms[cnt % (types)],
                                        new DenseVector(10)));
                        cnt++;
                    }
                }

                @Override
                public void cancel() {}
            };

    Table trainDataTable =
            tEnv.fromDataStream(
                    env.addSource(rowGenerator, ""sourceOp-"" + 1)
                            .returns(
                                    Types.ROW(
                                            Types.LONG,
                                            Types.LONG,
                                            DenseVectorTypeInfo.INSTANCE)));

    testCoGroupWithIteration(tEnv, trainDataTable);
}

public static void testCoGroupWithIteration(StreamTableEnvironment tEnv, Table trainDataTable)
        throws Exception {
    DataStream<Row> data1 = tEnv.toDataStream(trainDataTable);
    DataStream<Row> data2 = tEnv.toDataStream(trainDataTable);
    DataStreamList coResult =
            Iterations.iterateBoundedStreamsUntilTermination(
                    DataStreamList.of(data1),
                    ReplayableDataStreamList.notReplay(data2),
                    IterationConfig.newBuilder().build(),
                    new TrainIterationBody());

    List<Integer> counts = IteratorUtils.toList(coResult.get(0).executeAndCollect());
    System.out.println(counts.size());
}

private static class TrainIterationBody implements IterationBody {

    @Override
    public IterationBodyResult process(
            DataStreamList variableStreams, DataStreamList dataStreams) {

        DataStreamList feedbackVariableStream =
                IterationBody.forEachRound(
                        dataStreams,
                        input -> {
                            DataStream<Row> dataStream1 = variableStreams.get(0);
                            DataStream<Row> dataStream2 = dataStreams.get(0);

                            DataStream<Row> coResult =
                                    dataStream1
                                            .coGroup(dataStream2)
                                            .where(
                                                    (KeySelector<Row, Long>)
                                                            t2 -> t2.getFieldAs(0))
                                            .equalTo(
                                                    (KeySelector<Row, Long>)
                                                            t2 -> t2.getFieldAs(1))
                                            .window(EndOfStreamWindows.get())
                                            .apply(
                                                    new RichCoGroupFunction<Row, Row, Row>() {
                                                        @Override
                                                        public void coGroup(
                                                                Iterable<Row> iterable,
                                                                Iterable<Row> iterable1,
                                                                Collector<Row> collector) {
                                                            for (Row row : iterable1) {
                                                                collector.collect(row);
                                                            }
                                                        }
                                                    });
                            return DataStreamList.of(coResult);
                        });

        DataStream<Integer> terminationCriteria =
                feedbackVariableStream
                        .get(0)
                        .flatMap(new TerminateOnMaxIter(2))
                        .returns(Types.INT);

        return new IterationBodyResult(
                feedbackVariableStream, feedbackVariableStream, terminationCriteria);
    }
} {code}
The exception is as below. Note that the exception can not be reproduced in the unittest with MiniCluster since all classes are in the Java classpath.
{code:java}
Caused by: org.apache.flink.streaming.runtime.tasks.StreamTaskException: Could not instantiate state partitioner. at org.apache.flink.streaming.api.graph.StreamConfig.getStatePartitioner(StreamConfig.java:662) at org.apache.flink.iteration.operator.OperatorUtils.createWrappedOperatorConfig(OperatorUtils.java:96) at org.apache.flink.iteration.operator.perround.AbstractPerRoundWrapperOperator.getWrappedOperator(AbstractPerRoundWrapperOperator.java:168) at org.apache.flink.iteration.operator.perround.AbstractPerRoundWrapperOperator.getWrappedOperator(AbstractPerRoundWrapperOperator.java:146) at org.apache.flink.iteration.operator.perround.OneInputPerRoundWrapperOperator.processElement(OneInputPerRoundWrapperOperator.java:68) at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134) at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105) at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) at java.lang.Thread.run(Thread.java:748) 

Caused by: java.lang.ClassCastException: cannot assign instance of java.lang.invoke.SerializedLambda to field org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionKeySelector.keySelector1 of type org.apache.flink.api.java.functions.KeySelector in instance of org.apache.flink.streaming.api.datastream.CoGroupedStreams$UnionKeySelector at java.io.ObjectStreamClass$FieldReflector.setObjFieldValues(ObjectStreamClass.java:2302) at java.io.ObjectStreamClass.setObjFieldValues(ObjectStreamClass.java:1432) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2409) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2327) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2185) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1665) at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2403) at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2327) at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2185) at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1665) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:501) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:459) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:617) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:602) at org.apache.flink.util.InstantiationUtil.deserializeObject(InstantiationUtil.java:589) at org.apache.flink.util.InstantiationUtil.readObjectFromConfig(InstantiationUtil.java:543) at org.apache.flink.streaming.api.graph.StreamConfig.getStatePartitioner(StreamConfig.java:659) ... 17 more  {code}
 ",,Jiang Xin,leonard,lindong,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 21 03:06:12 UTC 2023,,,,,,,,,,"0|z1gmq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 01:40;zhangzp;Is this similar to this one [1]?

 

 [1] https://issues.apache.org/jira/browse/FLINK-31255;;;","17/Mar/23 04:57;Jiang Xin;[~zhangzp] I'm afraid not. This issue is most likely causing by incorrect class loader, but FLINK-31255 seems not.;;;","21/Mar/23 03:06;lindong;Merged to apache/flink-ml master branch fa5f47ea2a09360143aab5f39b85b373675636ad;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Connecting to Kafka and Avro Schema Registry fails with ClassNotFoundException,FLINK-31485,13528760,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,martijnvisser,martijnvisser,martijnvisser,16/Mar/23 09:04,17/Mar/23 07:56,13/Jul/23 08:29,17/Mar/23 07:56,1.17.0,,,,,,1.17.0,,,,,,,,"Formats (JSON, Avro, Parquet, ORC, SequenceFile)",,,,0,pull-request-available,,,"When running the SQL Client and using flink-sql-connector-kafka, flink-sql-avro and flink-sql-avro-confluent-registry and trying to query Schema Registry, the job will fail with

{code:bash}
[ERROR] Could not execute SQL statement. Reason:
java.lang.ClassNotFoundException: com.google.common.base.Ticker
{code}",,leonard,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 07:56:24 UTC 2023,,,,,,,,,,"0|z1gmjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 07:56;martijnvisser;Fixed in

master: a7605b0f5b0a7b47c0d698912e105ffafd76e9aa

release-1.17: 64f17ee01ae2d526176f4adba8a5041adec50322;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Close blocking iterators in tests,FLINK-31479,13528720,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,dianer17,dianer17,16/Mar/23 03:10,23/Mar/23 05:32,13/Jul/23 08:29,19/Mar/23 05:31,,,,,,,,,,,,,,,Table Store,,,,0,pull-request-available,,,Several blocking iterators are not closed in `ContinuousFileStoreITCase`,,dianer17,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:31:12 UTC 2023,,,,,,,,,,"0|z1gmao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 05:31;lzljs3620320;Use github issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"TypeError: a bytes-like object is required, not 'JavaList' is thrown when ds.execute_and_collect() is called on a KeyedStream",FLINK-31478,13528716,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,16/Mar/23 02:44,17/Mar/23 15:16,13/Jul/23 08:29,17/Mar/23 15:16,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,API / Python,,,,0,pull-request-available,,,"{code}
################################################################################
#  Licensed to the Apache Software Foundation (ASF) under one
#  or more contributor license agreements.  See the NOTICE file
#  distributed with this work for additional information
#  regarding copyright ownership.  The ASF licenses this file
#  to you under the Apache License, Version 2.0 (the
#  ""License""); you may not use this file except in compliance
#  with the License.  You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
#  See the License for the specific language governing permissions and
# limitations under the License.
################################################################################
import argparse
import logging
import sys

from pyflink.common import WatermarkStrategy, Encoder, Types
from pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionMode
from pyflink.datastream.connectors.file_system import (FileSource, StreamFormat, FileSink,
                                                       OutputFileConfig, RollingPolicy)


word_count_data = [""To be, or not to be,--that is the question:--"",
                   ""Whether 'tis nobler in the mind to suffer"",
                   ""The slings and arrows of outrageous fortune"",
                   ""Or to take arms against a sea of troubles,"",
                   ""And by opposing end them?--To die,--to sleep,--"",
                   ""No more; and by a sleep to say we end"",
                   ""The heartache, and the thousand natural shocks"",
                   ""That flesh is heir to,--'tis a consummation"",
                   ""Devoutly to be wish'd. To die,--to sleep;--"",
                   ""To sleep! perchance to dream:--ay, there's the rub;"",
                   ""For in that sleep of death what dreams may come,"",
                   ""When we have shuffled off this mortal coil,"",
                   ""Must give us pause: there's the respect"",
                   ""That makes calamity of so long life;"",
                   ""For who would bear the whips and scorns of time,"",
                   ""The oppressor's wrong, the proud man's contumely,"",
                   ""The pangs of despis'd love, the law's delay,"",
                   ""The insolence of office, and the spurns"",
                   ""That patient merit of the unworthy takes,"",
                   ""When he himself might his quietus make"",
                   ""With a bare bodkin? who would these fardels bear,"",
                   ""To grunt and sweat under a weary life,"",
                   ""But that the dread of something after death,--"",
                   ""The undiscover'd country, from whose bourn"",
                   ""No traveller returns,--puzzles the will,"",
                   ""And makes us rather bear those ills we have"",
                   ""Than fly to others that we know not of?"",
                   ""Thus conscience does make cowards of us all;"",
                   ""And thus the native hue of resolution"",
                   ""Is sicklied o'er with the pale cast of thought;"",
                   ""And enterprises of great pith and moment,"",
                   ""With this regard, their currents turn awry,"",
                   ""And lose the name of action.--Soft you now!"",
                   ""The fair Ophelia!--Nymph, in thy orisons"",
                   ""Be all my sins remember'd.""]


def word_count(input_path, output_path):
    env = StreamExecutionEnvironment.get_execution_environment()
    env.set_runtime_mode(RuntimeExecutionMode.BATCH)
    # write all the data to one file
    env.set_parallelism(1)

    # define the source
    if input_path is not None:
        ds = env.from_source(
            source=FileSource.for_record_stream_format(StreamFormat.text_line_format(),
                                                       input_path)
                             .process_static_file_set().build(),
            watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),
            source_name=""file_source""
        )
    else:
        print(""Executing word_count example with default input data set."")
        print(""Use --input to specify file input."")
        ds = env.from_collection(word_count_data)

    def split(line):
        yield from line.split()

    # compute word count
    ds = ds.flat_map(split) \
           .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \
           .key_by(lambda i: i[0])
           # .reduce(lambda i, j: (i[0], i[1] + j[1]))

    # define the sink
    if output_path is not None:
        ds.sink_to(
            sink=FileSink.for_row_format(
                base_path=output_path,
                encoder=Encoder.simple_string_encoder())
            .with_output_file_config(
                OutputFileConfig.builder()
                .with_part_prefix(""prefix"")
                .with_part_suffix("".ext"")
                .build())
            .with_rolling_policy(RollingPolicy.default_rolling_policy())
            .build()
        )
    else:
        print(""Printing result to stdout. Use --output to specify output path."")
        a = list(ds.execute_and_collect())


if __name__ == '__main__':
    logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=""%(message)s"")

    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--input',
        dest='input',
        required=False,
        help='Input file to process.')
    parser.add_argument(
        '--output',
        dest='output',
        required=False,
        help='Output file to write results to.')

    argv = sys.argv[1:]
    known_args, _ = parser.parse_known_args(argv)

    word_count(known_args.input, known_args.output)
{code}

For the above job, the following exception will be thrown:
{code}
Traceback (most recent call last):
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/udf/test_udf_perf.py"", line 131, in <module>
    word_count(known_args.input, known_args.output)
  File ""/Users/dianfu/code/src/workspace/pyflink-examples/udf/test_udf_perf.py"", line 110, in word_count
    a = list(ds.execute_and_collect())
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/data_stream.py"", line 2920, in __next__
    return self.next()
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/data_stream.py"", line 2931, in next
    return convert_to_python_obj(self._j_closeable_iterator.next(), self._type_info)
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/utils.py"", line 72, in convert_to_python_obj
    fields.append(pickled_bytes_to_python_converter(data, field_type))
  File ""/Users/dianfu/venv/examples-37/lib/python3.7/site-packages/pyflink/datastream/utils.py"", line 91, in pickled_bytes_to_python_converter
    data = pickle.loads(data)
TypeError: a bytes-like object is required, not 'JavaList'
{code}

See more details on https://apache-flink.slack.com/archives/C03G7LJTS2G/p1678894062180649",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 17 15:16:48 UTC 2023,,,,,,,,,,"0|z1gm9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 15:16;dianfu;Fixed in:
- master via 5e059efee864e17939a33f29272a848d00598531
- release-1.17 via ec5a09b3ce56426d1bdc8eeac4bf52cac9be015b
- release-1.16 via cadf4b35fb6f20c8cba310fa54626d0b9bae1361;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NestedLoopJoinTest.testLeftOuterJoinWithFilter failed on azure ,FLINK-31477,13528710,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,leonard,leonard,16/Mar/23 01:40,20/Mar/23 07:23,13/Jul/23 08:29,17/Mar/23 02:14,1.16.2,,,,,,1.16.2,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"{noformat}
 Failures: 
Mar 15 15:52:32 [ERROR]   NestedLoopJoinTest.testLeftOuterJoinWithFilter1:37 optimized exec plan expected:<...[InnerJoin], where=[[true], select=[a, e, f], build=[left])
Mar 15 15:52:32    :- Exchange(distribution=[broadcast])
Mar 15 15:52:32    :  +- Calc(select=[a], where=[(a = 10)])
Mar 15 15:52:32    :     +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
Mar 15 15:52:32    +- Calc(select=[e, f], where=[(d = 10])])
Mar 15 15:52:32       +- LegacyT...> but was:<...[InnerJoin], where=[[(a = d)], select=[a, d, e, f], build=[left])
Mar 15 15:52:32    :- Exchange(distribution=[broadcast])
Mar 15 15:52:32    :  +- Calc(select=[a], where=[SEARCH(a, Sarg[10])])
Mar 15 15:52:32    :     +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable1, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
Mar 15 15:52:32    +- Calc(select=[d, e, f], where=[SEARCH(d, Sarg[10]])])
Mar 15 15:52:32       +- LegacyT...>{noformat}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47202&view=logs&j=086353db-23b2-5446-2315-18e660618ef2&t=6cd785f3-2a2e-58a8-8e69-b4a03be28843",,jark,leonard,mapohl,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31491,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 20 07:23:45 UTC 2023,,,,,,,,,,"0|z1gm8g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Mar/23 15:38;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47191&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","16/Mar/23 15:39;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47194&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13073;;;","16/Mar/23 15:52;mapohl;Same build, multiple failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=13432
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=13071
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=13166
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=13901
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47220&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=13432;;;","17/Mar/23 02:14;leonard;Fixed in release-1.16 : 1bd25a48fa444390971149515810d057324b642b;;;","20/Mar/23 07:23;mapohl;The following build didn't contain the aforementioned fix, yet:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47263&view=results;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix hive catalog and connector jar name in the create release script for table store,FLINK-31460,13528542,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuangchong,zhuangchong,zhuangchong,15/Mar/23 01:58,15/Mar/23 11:59,13/Jul/23 08:29,15/Mar/23 11:59,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,,,lzljs3620320,zhuangchong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 11:59:59 UTC 2023,,,,,,,,,,"0|z1gl74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 11:59;lzljs3620320;master: d51ca84c8d711f74bbdd38e4acab627de78c2681;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scala suffix checker fails for release-1.15,FLINK-31442,13528356,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,martijnvisser,martijnvisser,14/Mar/23 07:53,14/Mar/23 10:20,13/Jul/23 08:29,14/Mar/23 10:20,1.15.5,,,,,,,,,,,,,,,,,,0,,,,"{code:bash}
08:53:07,511 ERROR org.apache.flink.tools.ci.suffixcheck.ScalaSuffixChecker     [] - Violations found:
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-formats/flink-sequence-file/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-hadoop-compatibility/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hbase-1.4/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hbase-2.2/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-hcatalog/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-connectors/flink-connector-hive/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-table/flink-sql-client/pom.xml'.
	Scala-free module 'flink-hadoop-compatibility' is referenced with scala suffix in 'flink-tests/pom.xml'.
	Scala-free module 'flink-hcatalog' is referenced with scala suffix in 'flink-connectors/flink-hcatalog/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-1.2.2' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-1.2.2/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-2.2.0' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-2.2.0/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-2.3.6' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-2.3.6/pom.xml'.
	Scala-free module 'flink-sql-connector-hive-3.1.2' is referenced with scala suffix in 'flink-connectors/flink-sql-connector-hive-3.1.2/pom.xml'.
==============================================================================

{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47102&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=25873",,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31227,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 14 07:56:07 UTC 2023,,,,,,,,,,"0|z1gk1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 07:56;martijnvisser;Reverted ""[FLINK-31227][docs] Remove Scala suffix for ORC and Parquet format download links on the the FileSystem documentation. This closes #22039""

78bae43288ad64511c298a067d66fa37667771d3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong key 'lookup.cache.caching-missing-key' in connector documentation,FLINK-31437,13528341,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,gaara,gaara,14/Mar/23 05:46,15/Mar/23 09:44,13/Jul/23 08:29,15/Mar/23 09:41,,,,,,,1.16.2,1.17.1,,,,,,,Connectors / HBase,Connectors / JDBC,Documentation,,0,pull-request-available,,,"'lookup.cache.caching-missing-key' change should be configured as 'lookup.partial-cache.caching-missing-key'.
An error occurred when I configured a dimension table.
The configuration given by the official website is not available.
!image-2023-03-14-05-45-06-230.png!
!image-2023-03-14-05-45-44-616.png!",,gaara,JunRuiLi,leonard,martijnvisser,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/23 05:45;gaara;image-2023-03-14-05-45-06-230.png;https://issues.apache.org/jira/secure/attachment/13056295/image-2023-03-14-05-45-06-230.png","14/Mar/23 05:45;gaara;image-2023-03-14-05-45-44-616.png;https://issues.apache.org/jira/secure/attachment/13056294/image-2023-03-14-05-45-44-616.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 09:41:35 UTC 2023,,,,,,,,,,"0|z1gjyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Mar/23 05:49;JunRuiLi;[~gaara] Thanks for creating this issue, and *[xuzhiwen1255|https://github.com/xuzhiwen1255]* has proposed a hot-fix pr to fix this bug: https://github.com/apache/flink/pull/22167;;;","14/Mar/23 05:53;gaara;ok;;;","14/Mar/23 05:59;gaara;flink-connector-jdbc has the same issue.;;;","14/Mar/23 06:03;JunRuiLi;[~gaara] -flink-connector-jdbc connector has been removed in release 1.17:- -https://issues.apache.org/jira/browse/FLINK-30465-

Sorry, I made a mistake, flink-connector-jdbc was moved to a separate repository, so you can fix it in flink version 1.16.2, and also fix in https://github.com/apache/flink-connector-jdbc. cc [~renqs] ;;;","14/Mar/23 07:50;martijnvisser;I've lowered the priority to Major, per https://cwiki.apache.org/confluence/display/FLINK/Flink+Jira+Process;;;","14/Mar/23 08:08;gaara;I found that this issue exists in versions 1.16 and later.;;;","14/Mar/23 08:31;JunRuiLi;[~gaara] Yes, you're right. I made a mistake and I've updated the comments. You can fix flink-connector-jdbc docs in flink version 1.16.2, and also fix in [https://github.com/apache/flink-connector-jdbc]. :D;;;","14/Mar/23 08:40;gaara;I just want to ask a question, I don't know how to solve it.:(

I see that the PR changes above are modifying the documentation. I don't know whether this situation requires changing the documentation or the code, but I believe it should involve modifying the code. I hope you can take a look and determine the best course of action.;;;","14/Mar/23 08:50;JunRuiLi;[~zhuzh] Could I take this tickets? Thx!;;;","15/Mar/23 09:41;leonard;Fixed by: 
 * Flink main:  0848815d41ac5bbafa230e378af049a20306ae76
 * Flink-connector-jdbc main: d15c59bb54a61e2dc12856d72eabbacc9c639488
 * Flink release-1.17: 43fafd43dff30a33f15d478c8f20ef4d1a6bcff1
 * Flink release-1.16: 5c723dd7a3d9f2be450bbc2f284acdeb990b9b8c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThreadInfoRequestCoordinatorTest.testShutDown failed on CI,FLINK-31420,13528219,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,renqs,renqs,13/Mar/23 10:29,15/Mar/23 14:46,13/Jul/23 08:29,15/Mar/23 14:46,1.16.2,,,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / REST,,,,0,pull-request-available,,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47076&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8394]
{code:java}
Mar 13 03:58:48 [ERROR] Failures: 
Mar 13 03:58:48 [ERROR]   ThreadInfoRequestCoordinatorTest.testShutDown:207 
Mar 13 03:58:48 Expecting
Mar 13 03:58:48   <CompletableFuture[Failed with the following stack trace:
Mar 13 03:58:48 java.lang.RuntimeException: Discarded
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator$PendingStatsRequest.discard(TaskStatsRequestCoordinator.java:266)
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.stats.TaskStatsRequestCoordinator.handleFailedResponse(TaskStatsRequestCoordinator.java:114)
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinator.lambda$requestThreadInfo$1(ThreadInfoRequestCoordinator.java:152)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
Mar 13 03:58:48 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
Mar 13 03:58:48 	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Mar 13 03:58:48 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
Mar 13 03:58:48 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
Mar 13 03:58:48 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Mar 13 03:58:48 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Mar 13 03:58:48 	at java.lang.Thread.run(Thread.java:748)
Mar 13 03:58:48 Caused by: java.util.concurrent.CompletionException: java.util.concurrent.TimeoutException: Request timeout.
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.encodeRelay(CompletableFuture.java:326)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.completeRelay(CompletableFuture.java:338)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.uniRelay(CompletableFuture.java:925)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture$UniRelay.tryFire(CompletableFuture.java:913)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
Mar 13 03:58:48 	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
Mar 13 03:58:48 	at org.apache.flink.runtime.webmonitor.threadinfo.ThreadInfoRequestCoordinatorTest.lambda$createMockTaskManagerGateway$2(ThreadInfoRequestCoordinatorTest.java:259)
Mar 13 03:58:48 	... 6 more
Mar 13 03:58:48 Caused by: java.util.concurrent.TimeoutException: Request timeout.
Mar 13 03:58:48 	... 7 more
Mar 13 03:58:48 ]>
Mar 13 03:58:48 not to be done.
Mar 13 03:58:48 Be aware that the state of the future in this message might not reflect the one at the time when the assertion was performed as it is evaluated later on {code}",,renqs,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/23 14:45;Weijie Guo;image-2023-03-15-22-45-54-585.png;https://issues.apache.org/jira/secure/attachment/13056376/image-2023-03-15-22-45-54-585.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 09:56:04 UTC 2023,,,,,,,,,,"0|z1gj7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 10:29;renqs;[~fanrui] Could you take a look on this one? Thanks;;;","13/Mar/23 14:17;Weijie Guo;After checking the code, I found that this is a test code issue. IIUC, This should be a problem that has existed for a long time, I don't know why it has not been revealed until now, but the probability of triggering this exception is very low. I will fix it asap.

The key point is {{ThreadInfoRequestCoordinatorTest#testShutDown}} using two gateways contains a timeout gateway but expected all request futures not done before {{shutdown}}. If the timeout reached, this future will be failed, result in request futures to be done. This makes the test strongly dependent on the execution order of assertion and timeout, thus causing this test unstable.;;;","15/Mar/23 09:56;Weijie Guo;master(1.18) via 7fccd5992f6222df62ed850542ef50b0714cd647.
release-1.17 via 6eb213d9093d90a7bd91801abfb822edf51f90bb.
release-1.16 via 3ffd113365e69185b871b2b683067b1541a17e5c.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortMergeResultPartitionReadSchedulerTest.testRequestBufferTimeout failed on CI,FLINK-31418,13528215,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,tanyuxin,renqs,renqs,13/Mar/23 10:17,16/May/23 11:46,13/Jul/23 08:29,16/May/23 11:46,1.16.1,1.17.0,,,,,1.16.2,1.17.0,1.18.0,,,,,,Runtime / Task,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47077&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8756]

Error message:
{code:java}
Mar 13 05:22:10 [ERROR] Failures: 
Mar 13 05:22:10 [ERROR]   SortMergeResultPartitionReadSchedulerTest.testRequestBufferTimeout:278 
Mar 13 05:22:10 Expecting value to be true but was false{code}",,kevin.cyj,mapohl,renqs,Sergey Nuyanzin,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue May 16 11:46:03 UTC 2023,,,,,,,,,,"0|z1gj6g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 10:17;renqs;[~tanyuxin] Could you take a look on this one? Thanks;;;","13/Mar/23 10:40;tanyuxin;I think https://issues.apache.org/jira/browse/FLINK-31396 may resolve it. But it's an occasional issue, and can not be produced in the local env to check whether it can be resolved.

But when we discuss it offline, [~kevin.cyj] have some different inputs about it. Could you please take a look?;;;","13/Mar/23 10:50;kevin.cyj;I will also try to reproduce it. If it can not reproduce, let's downgrade this issue. I think it not need to be a blocker, because it may exits since 1.16 and there is no relevant change recently.;;;","14/Mar/23 02:27;renqs;Thanks for the feedback [~kevin.cyj]. I'll downgrade this to critical as replied.;;;","14/Mar/23 02:38;kevin.cyj;It is a test issue, we will fix it soon.;;;","20/Mar/23 07:21;mapohl;Looks like this issue also exists in 1.16?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47263&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=7119

I'm reopening the issue and updating the affected versions accordingly.;;;","20/Mar/23 07:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47328&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=7090;;;","16/May/23 07:08;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49021&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=8387;;;","16/May/23 07:13;Weijie Guo;I will try to backport this to 1.16 and see if the problem can be solved.;;;","16/May/23 07:20;Weijie Guo;It seems that the commits information has been missed

master(1.18) via f949fe4eb1b83a324b9139d41a6db5976a8db539.
release-1.17 via 4e528036e70a9909b4cf1525be2e1e56da2d22f8.
release-1.16 via ffa58e1de3de0d084d820594304b4dbdbf705c9b.;;;","16/May/23 07:31;tanyuxin;[~Sergey Nuyanzin]   [~Weijie Guo], The patch is not in release-1.16. I created a PR to backport it.https://github.com/apache/flink/pull/22589;;;","16/May/23 11:46;Weijie Guo;Feel free to reopen this If it can still reproduce.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop version unknown when TrinoPageSourceBase.getNextPage,FLINK-31417,13528211,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,nonggia,nonggia,13/Mar/23 10:07,19/Mar/23 05:35,13/Jul/23 08:29,19/Mar/23 05:35,table-store-0.4.0,,,,,,,,,,,,,,Table Store,,,,0,,,,"Exception thrown when quering flink-table-store by trino
{code:java}
2023-03-13T11:46:36.694+0800    ERROR   SplitRunner-11-113      io.trino.execution.executor.TaskExecutor        Error processing Split 20230313_034504_00000_jdcet.1.0.0-11 {} (start = 3.599627617710298E10, wall = 89264 ms, cpu = 0 ms, wait = 1 ms, calls = 1)java.lang.NoClassDefFoundError: Could not initialize class org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.createReader(KeyValueFileStoreRead.java:204)        at org.apache.flink.table.store.table.source.KeyValueTableRead.createReader(KeyValueTableRead.java:44)        at org.apache.flink.table.store.trino.TrinoPageSourceProvider.createPageSource(TrinoPageSourceProvider.java:76)        at org.apache.flink.table.store.trino.TrinoPageSourceProvider.lambda$createPageSource$0(TrinoPageSourceProvider.java:52)        at org.apache.flink.table.store.trino.ClassLoaderUtils.runWithContextClassLoader(ClassLoaderUtils.java:30)        at org.apache.flink.table.store.trino.TrinoPageSourceProvider.createPageSource(TrinoPageSourceProvider.java:51)        at io.trino.split.PageSourceManager.createPageSource(PageSourceManager.java:68)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:308)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)  2023-03-13T11:46:36.775+0800    ERROR   remote-task-callback-2  io.trino.execution.scheduler.PipelinedStageExecution    Pipelined stage execution for stage 20230313_034504_00000_jdcet.1 failedjava.lang.ExceptionInInitializerError        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.readBatch(ConcatRecordReader.java:65)        at org.apache.flink.table.store.file.mergetree.DropDeleteReader.readBatch(DropDeleteReader.java:44)        at org.apache.flink.table.store.table.source.KeyValueTableRead$RowDataRecordReader.readBatch(KeyValueTableRead.java:61)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.nextPage(TrinoPageSourceBase.java:120)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.getNextPage(TrinoPageSourceBase.java:113)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:311)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NumberFormatException: For input string: ""Unknown""        at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)        at java.base/java.lang.Integer.parseInt(Integer.java:652)        at java.base/java.lang.Integer.parseInt(Integer.java:770)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.HadoopShimsFactory.get(HadoopShimsFactory.java:53)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils.<clinit>(RecordReaderUtils.java:47)        ... 30 more  2023-03-13T11:46:36.777+0800    ERROR   stage-scheduler io.trino.execution.scheduler.SqlQueryScheduler  Failure in distributed stage for query 20230313_034504_00000_jdcetjava.lang.ExceptionInInitializerError        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.readBatch(ConcatRecordReader.java:65)        at org.apache.flink.table.store.file.mergetree.DropDeleteReader.readBatch(DropDeleteReader.java:44)        at org.apache.flink.table.store.table.source.KeyValueTableRead$RowDataRecordReader.readBatch(KeyValueTableRead.java:61)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.nextPage(TrinoPageSourceBase.java:120)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.getNextPage(TrinoPageSourceBase.java:113)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:311)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NumberFormatException: For input string: ""Unknown""        at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)        at java.base/java.lang.Integer.parseInt(Integer.java:652)        at java.base/java.lang.Integer.parseInt(Integer.java:770)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.HadoopShimsFactory.get(HadoopShimsFactory.java:53)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils.<clinit>(RecordReaderUtils.java:47)        ... 30 more  2023-03-13T11:46:36.784+0800    ERROR   stage-scheduler io.trino.execution.StageStateMachine    Stage 20230313_034504_00000_jdcet.1 failedjava.lang.ExceptionInInitializerError        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderImpl.<init>(RecordReaderImpl.java:257)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.ReaderImpl.rows(ReaderImpl.java:649)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createRecordReader(OrcReaderFactory.java:284)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:98)        at org.apache.flink.table.store.format.orc.OrcReaderFactory.createReader(OrcReaderFactory.java:56)        at org.apache.flink.table.store.file.utils.FileUtils.createFormatReader(FileUtils.java:108)        at org.apache.flink.table.store.file.io.KeyValueDataFileRecordReader.<init>(KeyValueDataFileRecordReader.java:55)        at org.apache.flink.table.store.file.io.KeyValueFileReaderFactory.createRecordReader(KeyValueFileReaderFactory.java:95)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.lambda$readerForRun$1(MergeTreeReaders.java:89)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.create(ConcatRecordReader.java:50)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForRun(MergeTreeReaders.java:92)        at org.apache.flink.table.store.file.mergetree.MergeTreeReaders.readerForSection(MergeTreeReaders.java:74)        at org.apache.flink.table.store.file.operation.KeyValueFileStoreRead.lambda$createReader$2(KeyValueFileStoreRead.java:195)        at org.apache.flink.table.store.file.mergetree.compact.ConcatRecordReader.readBatch(ConcatRecordReader.java:65)        at org.apache.flink.table.store.file.mergetree.DropDeleteReader.readBatch(DropDeleteReader.java:44)        at org.apache.flink.table.store.table.source.KeyValueTableRead$RowDataRecordReader.readBatch(KeyValueTableRead.java:61)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.nextPage(TrinoPageSourceBase.java:120)        at org.apache.flink.table.store.trino.TrinoPageSourceBase.getNextPage(TrinoPageSourceBase.java:113)        at io.trino.operator.TableScanOperator.getOutput(TableScanOperator.java:311)        at io.trino.operator.Driver.processInternal(Driver.java:388)        at io.trino.operator.Driver.lambda$processFor$9(Driver.java:292)        at io.trino.operator.Driver.tryWithLock(Driver.java:685)        at io.trino.operator.Driver.processFor(Driver.java:285)        at io.trino.execution.SqlTaskExecution$DriverSplitRunner.processFor(SqlTaskExecution.java:1076)        at io.trino.execution.executor.PrioritizedSplitRunner.process(PrioritizedSplitRunner.java:163)        at io.trino.execution.executor.TaskExecutor$TaskRunner.run(TaskExecutor.java:488)        at io.trino.$gen.Trino_366_0____20230313_034413_2.run(Unknown Source)        at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)        at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)        at java.base/java.lang.Thread.run(Thread.java:834)Caused by: java.lang.NumberFormatException: For input string: ""Unknown""        at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)        at java.base/java.lang.Integer.parseInt(Integer.java:652)        at java.base/java.lang.Integer.parseInt(Integer.java:770)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.HadoopShimsFactory.get(HadoopShimsFactory.java:53)        at org.apache.flink.table.store.shaded.org.apache.orc.impl.RecordReaderUtils.<clinit>(RecordReaderUtils.java:47)        ... 30 more {code}
Seems the common-version-info.properties file in flink-shaded-hadoop-2-uber-2.8.3-10.0.jar is not found by the classloader. The stacks tell that the call is from TrinoPageSourceBase.getNextPage, where the classloader of the current thread is AppClassloader, rather than PluginClassloader.

Can we fix it by using runWithContextClassLoader to run TrinoPageSourceBase.getNextPage with TrinoPageSourceBase.class.getClassloader?

 ",,lzljs3620320,nonggia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 19 05:34:55 UTC 2023,,,,,,,,,,"0|z1gj5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Mar/23 05:34;lzljs3620320;Thanks [~nonggia] , this has been fixed in 0.3, will cp to 0.4.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
exceptions in the alignment timer are ignored,FLINK-31414,13528186,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,13/Mar/23 07:23,15/Mar/23 10:54,13/Jul/23 08:29,15/Mar/23 10:18,1.13.6,1.14.6,1.15.3,1.16.1,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"Alignment timer task in alternating aligned checkpoint run as a future task in mailbox thread, causing the exceptions ([SingleCheckpointBarrierHandler#registerAlignmentTimer()|https://github.com/apache/flink/blob/65ab8e820a3714d2134dfb4c9772a10c998bd45a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/checkpointing/SingleCheckpointBarrierHandler.java#L327]) to be ignored. These exceptions should have failed the task, but now this will cause the same checkpoint to fire twice initInputsCheckpoints in my test.

 
{code:java}
 switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: unable to send request to worker
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:247)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.addInputData(ChannelStateWriterImpl.java:161)
        at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.prepareSnapshot(StreamTaskNetworkInput.java:103)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.prepareSnapshot(StreamOneInputProcessor.java:83)
        at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.prepareSnapshot(StreamMultipleInputProcessor.java:122)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInputSnapshot(StreamTask.java:518)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.prepareInflightDataSnapshot(SubtaskCheckpointCoordinatorImpl.java:655)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.initInputsCheckpoint(SubtaskCheckpointCoordinatorImpl.java:515)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.initInputsCheckpoint(SingleCheckpointBarrierHandler.java:516)
        at org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.alignmentTimeout(AlternatingCollectingBarriers.java:46)
        at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlternatingAlignedBarrierHandlerState.barrierReceived(AbstractAlternatingAlignedBarrierHandlerState.java:54)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
        at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
        at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
        at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
        at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
        at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
        at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
        at java.lang.Thread.run(Thread.java:748)
        Suppressed: java.io.IOException: java.lang.IllegalStateException: writer not found for request start 17
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:235)
                at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:564)
                at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:551)
                at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:255)
                at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
                at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
                at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:943)
                at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:917)
                at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
                at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
                ... 3 more
        Caused by: java.lang.IllegalStateException: writer not found for request start 17
                at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:75)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96)
                at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75)
                ... 1 more
Caused by: java.lang.IllegalStateException: not running
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.ensureRunning(ChannelStateWriteRequestExecutorImpl.java:152)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submitInternal(ChannelStateWriteRequestExecutorImpl.java:144)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submit(ChannelStateWriteRequestExecutorImpl.java:128)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:244)
        ... 27 more
        [CIRCULAR REFERENCE:java.lang.IllegalStateException: writer not found for request start 17] {code}
 

 

see : [BarrierAlignmentUtil#createRegisterTimerCallback()|https://github.com/apache/flink/blob/65ab8e820a3714d2134dfb4c9772a10c998bd45a/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/io/checkpointing/BarrierAlignmentUtil.java#L50]

 ",,fanrui,Feifan Wang,martijnvisser,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 10:18:17 UTC 2023,,,,,,,,,,"0|z1gj00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 07:47;martijnvisser;[~pnowojski] WDYT?;;;","13/Mar/23 18:05;pnowojski;[~Feifan Wang], can you elaborate a bit more what's the problem? I see a couple of discrepancies in your description and the stack trace that you posted:
*  the stack trace doesn't match to the master code, so I'm not sure what Flink version you are using?
* doesn't the error message ""switched from RUNNING to FAILED"" refer to actually subtask/task switching to FAILED state, contradicting your statement that the exception is being ignored?
* in the PR, I don't see a test coverage - a working unit test/ITCase that used to be failing without your fix would be nice to have. Both for explaining what is the issue and for actually providing regression test coverage;;;","14/Mar/23 03:28;Feifan Wang;Thanks for reply [~pnowojski] , sorry for the lack of clarity in the previous description, let me answer your question first :
{quote}the stack trace doesn't match to the master code, so I'm not sure what Flink version you are using?
{quote}
based on *release-1.16.1* , cherry-picked some bug fix.
{quote}doesn't the error message ""switched from RUNNING to FAILED"" refer to actually subtask/task switching to FAILED state, contradicting your statement that the exception is being ignored?
{quote}
Yes, it is a subtask switching to FAILED state. I mean the exception thrown in the alignment timer task is being ignored, causing the subtask thread to continue executing to trigger the exception I posted above.

Here is the more complete log ( I change some log level from debug to info ) :

 
{code:java}
2023-03-10 12:09:42,416 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1): Received barrier from channel InputChannelInfo{gateIdx=1, inputChannelIdx=586} @ 17.
2023-03-10 12:09:42,673 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 starting checkpoint 17 (CheckpointOptions{checkpointType=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}, targetLocation=(default), alignmentType=UNALIGNED, alignedCheckpointTimeout=9223372036854775807})
2023-03-10 12:09:42,673 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 put ChannelStateWriteResult : 17
2023-03-10 12:09:42,675 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1): Triggering checkpoint 17 on the barrier announcement at 1678421367671.
2023-03-10 12:09:42,675 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.tasks.StreamTask           - triggerCheckpointOnBarrier Starting checkpoint 17 CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD} on task MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1
2023-03-10 12:09:42,675 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.tasks.StreamTask           - Starting checkpoint 17 CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD} on task MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1
2023-03-10 12:09:42,675 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 requested write result, checkpoint 17
2023-03-10 12:09:42,676 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.state.changelog.ChangelogKeyedStateBackend   - snapshot of MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 for checkpoint 17, change range: 39..46, materialization ID 4
2023-03-10 12:09:42,677 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.tasks.RegularOperatorChain  - Could not complete snapshot 17 for operator MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1. Failure reason: Checkpoint was declined.
org.apache.flink.runtime.checkpoint.CheckpointException: Could not complete snapshot 17 for operator MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1. Failure reason: Checkpoint was declined.
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:269)
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:173)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.snapshotState(AbstractStreamOperator.java:345)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.checkpointStreamOperator(RegularOperatorChain.java:228)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.buildOperatorSnapshotFutures(RegularOperatorChain.java:213)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.snapshotState(RegularOperatorChain.java:192)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.takeSnapshotSync(SubtaskCheckpointCoordinatorImpl.java:730)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.checkpointState(SubtaskCheckpointCoordinatorImpl.java:363)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$performCheckpoint$13(StreamTask.java:1291)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.performCheckpoint(StreamTask.java:1279)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.triggerCheckpointOnBarrier(StreamTask.java:1236)
    at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointBarrierHandler.notifyCheckpoint(CheckpointBarrierHandler.java:147)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.triggerCheckpoint(SingleCheckpointBarrierHandler.java:287)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.access$100(SingleCheckpointBarrierHandler.java:64)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.triggerGlobalCheckpoint(SingleCheckpointBarrierHandler.java:489)
    at org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.alignmentTimeout(AlternatingCollectingBarriers.java:50)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$registerAlignmentTimer$3(SingleCheckpointBarrierHandler.java:321)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50)
    at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:367)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:352)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: The upload for 44 has already failed previously
    at org.apache.flink.changelog.fs.FsStateChangelogWriter.ensureCanPersist(FsStateChangelogWriter.java:429)
    at org.apache.flink.changelog.fs.FsStateChangelogWriter.persistInternal(FsStateChangelogWriter.java:213)
    at org.apache.flink.changelog.fs.FsStateChangelogWriter.persist(FsStateChangelogWriter.java:208)
    at org.apache.flink.state.changelog.ChangelogKeyedStateBackend.snapshot(ChangelogKeyedStateBackend.java:402)
    at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:246)
    ... 31 more
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.FSLimitException$MaxDirectoryItemsExceededException): The directory item limit of /flink-yg-test01/feifan-changelog-test/dstl/460359d4d9311744142797ba23e69d16/dstl is exceeded: limit=1048576 items=1048576
    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyMaxDirItems(FSDirectory.java:1056)
    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addLastINode(FSDirectory.java:1101)
    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addINode(FSDirectory.java:967)
    at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addFile(FSDirectory.java:531)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2997)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2856)
    at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2691)
    at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:815)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:450)
    at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:713)
    at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:975)
    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1002)
    at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:923)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:422)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1726)
    at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2786)    at org.apache.hadoop.ipc.Client.call(Client.java:1578)
    at org.apache.hadoop.ipc.Client.call(Client.java:1505)
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
    at com.sun.proxy.$Proxy22.create(Unknown Source)
    at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.create(ClientNamenodeProtocolTranslatorPB.java:332)
    at sun.reflect.GeneratedMethodAccessor181.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
    at com.sun.proxy.$Proxy23.create(Unknown Source)
    at sun.reflect.GeneratedMethodAccessor181.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.hadoop.hdfs.RpcResponseHandler.invoke(RpcResponseHandler.java:55)
    at com.sun.proxy.$Proxy23.create(Unknown Source)
    at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForCreate(DFSOutputStream.java:1982)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1937)
    at org.apache.hadoop.hdfs.DFSClient.create(DFSClient.java:1871)
    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:452)
    at org.apache.hadoop.hdfs.DistributedFileSystem$7.doCall(DistributedFileSystem.java:448)
    at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:448)
    at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:391)
    at org.apache.hadoop.fs.FilterFileSystem.create(FilterFileSystem.java:179)
    at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.create(ChRootedFileSystem.java:189)
    at org.apache.hadoop.fs.viewfs.ViewFileSystem.create(ViewFileSystem.java:323)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1027)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1008)
    at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:905)
    at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:154)
    at org.apache.flink.runtime.fs.hdfs.HadoopFileSystem.create(HadoopFileSystem.java:37)
    at org.apache.flink.changelog.fs.DuplicatingStateChangeFsUploader.prepareStream(DuplicatingStateChangeFsUploader.java:96)
    at org.apache.flink.changelog.fs.AbstractStateChangeFsUploader.uploadInternal(AbstractStateChangeFsUploader.java:76)
    at org.apache.flink.changelog.fs.AbstractStateChangeFsUploader.upload(AbstractStateChangeFsUploader.java:69)
    at org.apache.flink.changelog.fs.BatchingStateChangeUploadScheduler$1.tryExecute(BatchingStateChangeUploadScheduler.java:310)
    at org.apache.flink.changelog.fs.BatchingStateChangeUploadScheduler$1.tryExecute(BatchingStateChangeUploadScheduler.java:307)
    at org.apache.flink.changelog.fs.RetryingExecutor$RetriableActionAttempt.run(RetryingExecutor.java:225)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
    at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    ... 1 more
2023-03-10 12:09:42,677 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 aborting, checkpoint 17, cleanup:true
2023-03-10 12:09:42,723 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1): Received barrier from channel InputChannelInfo{gateIdx=0, inputChannelIdx=324} @ 17.
2023-03-10 12:09:42,723 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 starting checkpoint 17 (CheckpointOptions{checkpointType=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}, targetLocation=(default), alignmentType=UNALIGNED, alignedCheckpointTimeout=9223372036854775807})
2023-03-10 12:09:42,724 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 put ChannelStateWriteResult : 17
2023-03-10 12:09:42,724 INFO  [Channel state writer MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 discarding 645 drained requests
2023-03-10 12:09:42,724 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 discarding 1023 drained requests
2023-03-10 12:09:42,725 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.state.common.PeriodicMaterializationManager  - Shutting down PeriodicMaterializationManager.
2023-03-10 12:09:42,725 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 aborting, checkpoint 17, cleanup:false
2023-03-10 12:09:42,726 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl  - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 discarding 1 drained requests
2023-03-10 12:09:43,099 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.contrib.streaming.state.RocksDBKeyedStateBackend  - Closed RocksDB State Backend. Cleaning up RocksDB working directory /data1/hadoop/yarn/nm-local-dir/usercache/hadoop-rt/appcache/application_1671247042382_4677267/tm_container_e74_1671247042382_4677267_01_000039/tmp/job_460359d4d9311744142797ba23e69d16_op_KeyedCoProcessOperator_4f7e0f4c19a43f929bda6907ee1f3150__4517_4800__uuid_785a6359-d191-4691-9272-d0590d2d696b.
2023-03-10 12:09:43,125 WARN  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.taskmanager.Task                     - MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1) switched from RUNNING to FAILED with failure cause: java.lang.RuntimeException: unable to send request to worker
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:247)
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.addInputData(ChannelStateWriterImpl.java:161)
    at org.apache.flink.streaming.runtime.io.StreamTaskNetworkInput.prepareSnapshot(StreamTaskNetworkInput.java:103)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.prepareSnapshot(StreamOneInputProcessor.java:83)
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.prepareSnapshot(StreamMultipleInputProcessor.java:122)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.prepareInputSnapshot(StreamTask.java:518)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.prepareInflightDataSnapshot(SubtaskCheckpointCoordinatorImpl.java:655)
    at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.initInputsCheckpoint(SubtaskCheckpointCoordinatorImpl.java:515)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler$ControllerImpl.initInputsCheckpoint(SingleCheckpointBarrierHandler.java:516)
    at org.apache.flink.streaming.runtime.io.checkpointing.AlternatingCollectingBarriers.alignmentTimeout(AlternatingCollectingBarriers.java:46)
    at org.apache.flink.streaming.runtime.io.checkpointing.AbstractAlternatingAlignedBarrierHandlerState.barrierReceived(AbstractAlternatingAlignedBarrierHandlerState.java:54)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.lambda$processBarrier$2(SingleCheckpointBarrierHandler.java:234)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.markCheckpointAlignedAndTransformState(SingleCheckpointBarrierHandler.java:262)
    at org.apache.flink.streaming.runtime.io.checkpointing.SingleCheckpointBarrierHandler.processBarrier(SingleCheckpointBarrierHandler.java:231)
    at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.handleEvent(CheckpointedInputGate.java:181)
    at org.apache.flink.streaming.runtime.io.checkpointing.CheckpointedInputGate.pollNext(CheckpointedInputGate.java:159)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:110)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.io.StreamMultipleInputProcessor.processInput(StreamMultipleInputProcessor.java:85)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:748)
    Suppressed: java.io.IOException: java.lang.IllegalStateException: writer not found for request start 17
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.close(ChannelStateWriteRequestExecutorImpl.java:175)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.close(ChannelStateWriterImpl.java:235)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:564)
        at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.close(SubtaskCheckpointCoordinatorImpl.java:551)
        at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:255)
        at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
        at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
        at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:943)
        at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:917)
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:917)
        ... 3 more
    Caused by: java.lang.IllegalStateException: writer not found for request start 17
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:75)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96)
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75)
        ... 1 more
Caused by: java.lang.IllegalStateException: not running
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.ensureRunning(ChannelStateWriteRequestExecutorImpl.java:152)
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submitInternal(ChannelStateWriteRequestExecutorImpl.java:144)
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.submit(ChannelStateWriteRequestExecutorImpl.java:128)
    at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriterImpl.enqueue(ChannelStateWriterImpl.java:244)
    ... 27 more
    [CIRCULAR REFERENCE:java.lang.IllegalStateException: writer not found for request start 17]
2023-03-10 12:09:43,125 INFO  [MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1] org.apache.flink.runtime.taskmanager.Task                     - Freeing task resources for MV_J_PV -&gt; mv-join-after-operator -&gt; extract-event-identifier (4517/4800)#1 (cb2e56879557c676c9897cda44fe3c9e_4f7e0f4c19a43f929bda6907ee1f3150_4516_1). {code}
 

 

Later I will try to write a test to explain the problem.;;;","14/Mar/23 08:49;pnowojski;Ah thanks for the explanation, I understand the problem now. I see that bug has been in the code base since at least https://issues.apache.org/jira/browse/FLINK-19682. ;;;","15/Mar/23 10:18;pnowojski;merged commit cbfeef6 to master
0dfb5abf038 to release-1.17
f2f9104dc46 to release-1.16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CliClientITCase print unexpected border when printing explain results,FLINK-31403,13528058,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,11/Mar/23 05:05,13/Mar/23 07:55,13/Jul/23 08:29,13/Mar/23 07:55,1.17.0,1.18.0,,,,,1.17.0,,,,,,,,Table SQL / Client,,,,0,,,,"The CliClientITCase fails in the https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47010&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=16287.

After comparing the actual output and expected output, the differences are the length of the border when printing explain results.
",,fsk119,jark,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30025,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 07:55:50 UTC 2023,,,,,,,,,,"0|z1gi7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Mar/23 01:49;fsk119;I think it has been fixed by the [commit|https://github.com/apache/flink/pull/21322/commits/8c6a9ab1952c7d397fd39d952448206123081ac7] in the master. I will cherry pick it to the release-1.17.;;;","13/Mar/23 07:55;leonard;Fixed in release-1.17:  1838619bdc3585768ded050e17b8abcfdbf5811c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
testTransformationSetParallelism fails on 10 core machines,FLINK-31401,13527999,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,mbalassi,mbalassi,mbalassi,10/Mar/23 15:30,12/Mar/23 18:52,13/Jul/23 08:29,12/Mar/23 18:52,1.17.0,,,,,,1.18.0,,,,,,,,API / DataStream,Tests,,,0,pull-request-available,,,"StreamingJobGraphGenerator#testTransformationSetParallelism fails if it is run in an environment where the default parallelism is 10:

{noformat}
org.opentest4j.AssertionFailedError: 
expected: 3
 but was: 2
Expected :3
Actual   :2
{noformat}

The fix is trivial, we need to make an implicit assumption in the test about paralellisms explicit.",,JunRuiLi,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Mar 12 18:52:49 UTC 2023,,,,,,,,,,"0|z1ghug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 18:23;JunRuiLi;[~mbalassi] Thanks so much for your volunteering and contribution!:D As you said, this is a bug: testTransformationSetParallelism implies that the map and source should have different parallelism so that they will not be chained together, but when the default parallelism is 10, it will break the rule. ;;;","12/Mar/23 18:52;mbalassi;Sure, thanks for the quick confirmation.;;;","12/Mar/23 18:52;mbalassi;64d5f54 in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Occasional inaccurate timeout time calculation with System.nanotime in batch read buffer pool,FLINK-31396,13527942,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,tanyuxin,tanyuxin,10/Mar/23 08:29,10/Mar/23 14:40,13/Jul/23 08:29,10/Mar/23 14:40,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Network,,,,0,pull-request-available,,,"When running TPC-DS tests, I encountered the read buffer request timeout because of configuring too less read buffers. But I found the timeout time may be less than 5m occasionally, 5m is the expected time. 
I read the docs of System.nanotime, the docs say that  t1 < t0 should not be used, because of the possibility of numerical overflow. I tested the System.currentTimeMillis and it can work as expected.",,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 14:40:52 UTC 2023,,,,,,,,,,"0|z1ghhs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 14:40;Weijie Guo;master(1.18) via 49929f844975418850ec0334adf4d06c7c895d08.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix spark jar name in the create release script for table store,FLINK-31394,13527931,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuangchong,zhuangchong,zhuangchong,10/Mar/23 06:54,10/Mar/23 08:14,13/Jul/23 08:29,10/Mar/23 08:14,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,,,lzljs3620320,zhuangchong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 08:14:38 UTC 2023,,,,,,,,,,"0|z1ghfc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 08:14;lzljs3620320;master: 54bfd291563e907e26ab8aa2c1584b88688703ed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsFileDataManager use an incorrect default timeout,FLINK-31393,13527922,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,10/Mar/23 06:08,12/Mar/23 15:02,13/Jul/23 08:29,12/Mar/23 15:02,1.16.1,1.17.0,,,,,1.17.0,,,,,,,,Runtime / Network,,,,0,pull-request-available,,,"For batch shuffle(i.e. hybrid shuffle & sort-merge shuffle), If there is a fierce contention of the batch shuffle read memory, it will throw a {{TimeoutException}} to fail downstream task to release memory. But for hybrid shuffle, It uses an incorrect default timeout(5ms), this will make the job very easy to fail.",,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 11 13:47:52 UTC 2023,,,,,,,,,,"0|z1ghdc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Mar/23 13:47;Weijie Guo;master(1.18) via b63c65a10bc76b4dadd6ae17305ee1941b773601.
release-1.17 via 02814d686592950b1b47b152cceca308d805af80.
release-1.16 via a97a50703ed62634baf4e017e9f32bdec163c0fa.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix spark jar name in docs for table store,FLINK-31389,13527901,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhuangchong,zhuangchong,zhuangchong,10/Mar/23 01:28,10/Mar/23 04:38,13/Jul/23 08:29,10/Mar/23 04:38,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,,,lzljs3620320,zhuangchong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 04:38:08 UTC 2023,,,,,,,,,,"0|z1gh8o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 01:29;zhuangchong;https://github.com/apache/flink-table-store/pull/588;;;","10/Mar/23 04:38;lzljs3620320;master: 0ffa6654b2d64fc65c430e453e656fa68ce74632;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalProcessingTimeTimersFromBeingFired failed with an assertion,FLINK-31387,13527850,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,mapohl,mapohl,09/Mar/23 16:04,10/Mar/23 10:32,13/Jul/23 08:29,10/Mar/23 09:24,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46994&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9253

{code}
Mar 09 14:04:42 [ERROR] org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalProcessingTimeTimersFromBeingFired  Time elapsed: 0.018 s  <<< FAILURE!
Mar 09 14:04:42 java.lang.AssertionError: 
Mar 09 14:04:42 
Mar 09 14:04:42 Expecting AtomicInteger(0) to have value:
Mar 09 14:04:42   10
Mar 09 14:04:42 but did not.
Mar 09 14:04:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalTimersFromBeingFired(StreamTaskCancellationTest.java:305)
Mar 09 14:04:42 	at org.apache.flink.streaming.runtime.tasks.StreamTaskCancellationTest.testCancelTaskShouldPreventAdditionalProcessingTimeTimersFromBeingFired(StreamTaskCancellationTest.java:281)
Mar 09 14:04:42 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,dmvk,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31370,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 10 10:32:54 UTC 2023,,,,,,,,,,"0|z1ggxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 09:24;dmvk;master: 0b497c56dbdff0d69a39dcf7048b7aefb5c8b9e4;;;","10/Mar/23 10:32;mapohl;The following test failure didn't contain the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47018&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9442;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the potential deadlock issue of blocking shuffle,FLINK-31386,13527845,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,kevin.cyj,kevin.cyj,kevin.cyj,09/Mar/23 15:35,23/Apr/23 10:01,13/Jul/23 08:29,10/Mar/23 14:38,1.16.0,1.16.1,,,,,1.16.2,1.17.0,,,,,,,Runtime / Network,,,,0,pull-request-available,,,"Currently, the SortMergeResultPartition may allocate more network buffers than the guaranteed size of the LocalBufferPool. As a result, some result partitions may need to wait other result partitions to release the over-allocated network buffers to continue. However, the result partitions which have allocated more than guaranteed buffers relies on the processing of input data to trigger data spilling and buffer recycling. The input data further relies on batch reading buffers used by the SortMergeResultPartitionReadScheduler which may already taken by those blocked result partitions which are waiting for buffers. Then deadlock occurs. We can easily fix this deadlock by reserving the guaranteed buffers on initializing.",,fanrui,kevin.cyj,leonard,lsy,martijnvisser,tanyuxin,Weijie Guo,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 23 10:01:35 UTC 2023,,,,,,,,,,"0|z1ggw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Mar/23 14:38;kevin.cyj;Merged into 1.17 to unblock release. Will pick to master latter.;;;","23/Apr/23 10:01;kevin.cyj;Cherry picked to 1.16 via 4e9516aa855cd5262a8574ecce60768553f0e7cf.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation fails to build due to lack of package,FLINK-31378,13527794,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Not a Priority,Fixed,Wencong Liu,loserwang1024,loserwang1024,09/Mar/23 11:26,13/Mar/23 07:19,13/Jul/23 08:29,13/Mar/23 07:18,1.18.0,,,,,,1.18.0,,,,,,,,Documentation,,,,1,pull-request-available,,,"In [Project Configuration Section|[https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/configuration/overview/#running-and-packaging],] it shows that ""If you want to run your job by simply executing the main class, you will need {{flink-runtime}} in your classpath"". 

However, when I just add flink-runtime in my classPath, an error is thrown like this:""
No ExecutorFactory found to execute the application"".

It seems that flink-clients is also needed to supply an excutor through Java Service Load.

Could you please add this in official article for beginners like me?

 ",,loserwang1024,Weijie Guo,Wencong Liu,xzw0223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/23 03:19;loserwang1024;image-2023-03-10-11-19-35-773.png;https://issues.apache.org/jira/secure/attachment/13056231/image-2023-03-10-11-19-35-773.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 13 07:18:55 UTC 2023,,,,,,,,,,"0|z1ggkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 12:07;Wencong Liu;Hello [~loserwang1024] , the links in your proposal may be not correct? Could you please give the correct link?;;;","10/Mar/23 01:43;loserwang1024;[~Wencong Liu], the link is here: https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/configuration/overview/#running-and-packaging;;;","10/Mar/23 02:56;Wencong Liu;The flink-streaming-java module does not have a dependency on flink-clients module.

[FLINK-15090] Reverse the dependency from flink-streaming-java to flink-client - ASF JIRA (apache.org)

I think the docs should be modified, WDYT? cc [~Weijie Guo] ;;;","10/Mar/23 02:57;xzw0223;I think you should use the table api.

please see this sentence
 *_In case of Table API programs, you will also need {{flink-table-runtime}} and {{{}flink-table-planner-loader{}}}._*

 ;;;","10/Mar/23 03:19;loserwang1024;[~xzw0223] , I have already tried table api without flink-clients. It shows that same error is still thrown.

!image-2023-03-10-11-19-35-773.png!;;;","10/Mar/23 03:28;xzw0223;I think you can upload your pom and describe more details, it will be easier for me to reproduce the problem.;;;","10/Mar/23 03:43;loserwang1024;Ok, [~xzw0223] , you just write any table api programs or stream programs (flink version is 1.16.0) without 
flink-clients dependency, then run by simply executing the main class rather than flink cluster. The problem can reproduce. 
In fact, it's not a problem or bug , just information lacks in official tutorials.;;;","10/Mar/23 03:49;Weijie Guo;If you want to run flink job in the IDE, the dependency of {{flink-clients}} need to be included in your pom.xml. In case of Table API programs, you will also need {{flink-table-runtime}} and {{{}flink-table-planner-loader{}}}. 

In general, we should mark these dependencies to `provided` scope. As a result, to make the applications run within IntelliJ IDEA, it is necessary to tick the {{Include dependencies with ""Provided"" scope}} box in the run configuration. If this option is not available (possibly due to using an older IntelliJ IDEA version), then a workaround is to create a test that calls the application’s {{main()}} method.;;;","10/Mar/23 03:56;xzw0223;I think there is no problem with the documentation, and I can execute it according to the documentation;;;","10/Mar/23 04:00;loserwang1024;[~Weijie Guo] ,if add this in corresponding documentation, it's will be better for beginners like me.;;;","10/Mar/23 04:10;Weijie Guo;[~loserwang1024] Yes, add document for beginners sounds good to me. But before starting this work, we should fix the error in `https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/dev/configuration/overview/#running-and-packaging` first.;;;","10/Mar/23 04:14;Wencong Liu;[~loserwang1024] | have a fix for this document. If you like, you can participate in the review together.;;;","10/Mar/23 04:17;loserwang1024;[~Wencong Liu] , Of course, I’d like to participate in it.;;;","10/Mar/23 05:52;Weijie Guo;[~xzw0223] What version of Flink do you use? I think {{flink-clients}} is necessary for local execution after FLINK-15090.
In addition, why do you think that the {{flink-runtime}} dependency needs to be included in the user's pom, doesn't it need to be modified?;;;","10/Mar/23 05:59;xzw0223;[~Weijie Guo] Sorry, I made a mistake,the test introduces the clients dependency.;;;","10/Mar/23 06:03;Weijie Guo;[~xzw0223] Never mind. If you like, you can participate in the review together.;;;","10/Mar/23 06:04;xzw0223;[~Weijie Guo] No problem.;;;","13/Mar/23 07:18;Weijie Guo;master(1.18) via 231df6276f172434e81a18ad4234557b2d1d711a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ProxyStreamPartitioner should implement ConfigurableStreamPartitioner,FLINK-31374,13527711,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,zhangzp,zhangzp,08/Mar/23 23:13,13/Apr/23 03:29,13/Jul/23 08:29,12/Apr/23 06:57,ml-2.2.0,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"In flink-ml-iterations module, we use ProxyStreamPartitioner to wrap StreamPartitioner to deal with records in iterations.

 

However, it did not implement ConfigurableStreamPartitioner interface. Thus that maxParallelism information is lost.",,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 06:57:08 UTC 2023,,,,,,,,,,"0|z1gg2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Apr/23 06:57;zhangzp;Resolved on master via 990337bf5a0a23b08be85c475043a047703772c8;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaSink failed to commit transactions under EXACTLY_ONCE semantics,FLINK-31363,13527557,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,tzulitai,lightzhao,lightzhao,08/Mar/23 02:55,12/Apr/23 17:13,13/Jul/23 08:29,12/Apr/23 17:13,1.16.1,1.17.0,1.18.0,,,,kafka-3.0.0,,,,,,,,Connectors / Kafka,,,,1,pull-request-available,,,"When KafkaSink starts Exactly once and no data is written to the topic during a checkpoint, the transaction commit exception is triggered, with the following exception.

[Transiting to fatal error state due to org.apache.kafka.common.errors.InvalidTxnStateException: The producer attempted a transactional operation in an invalid state.]",,leonard,lightzhao,martijnvisser,mason6345,rmetzger,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/23 02:54;lightzhao;image-2023-03-08-10-54-51-410.png;https://issues.apache.org/jira/secure/attachment/13056134/image-2023-03-08-10-54-51-410.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 12 17:13:17 UTC 2023,,,,,,,,,,"0|z1gf48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 09:31;martijnvisser;[~lightzhao] Do you mean when no data is written to a topic that's a source for Flink, or a topic that's a sink for Flink? ;;;","08/Mar/23 09:42;lightzhao;[~martijnvisser] when no data is written to a topic that's a sink for Flink.;;;","10/Mar/23 16:01;tzulitai;Thanks for reporting this [~lightzhao], I think this is a valid issue.

Kafka internally doesn't actually consider a transaction started until the first record is sent to a partition, and then that partition is added to a transaction.
So, when we start new transactions after every checkpoint in the KafkaSink via {{{}producer.beginTransaction(){}}}, there's actually no explicit txn request sent to Kafka until the first {{{}producer.send(){}}}.

In other words, I think the following would return an InvalidTxnStateException from Kafka:

 
{code:java}
producer.beginTransaction();
producer.commitTransaction();

// or

producer.beginTransaction();
producer.abortTransaction();{code}
And this can happen if, for example, within a checkpoint no data has been sent to Kafka at all. Which may be the case if e.g. some upstream filtering operator filtered out all records, all the job simply had no data to process because there was no records written to the Kafka source topic.

It is possible to address this by postponing the {{producer.beginTransaction()}} call until the first record after a checkpoint instead of pre-emptively starting a new transaction after the last checkpoint (like we do now), but that's going to add a redundant if-check on the main record processing loop.

Before deciding on anything, I need to check with our Kafka experts to see if this makes sense semantically.
Perhaps a different kind of exception could be returned from Kafka to indicate that the txn wasn't actually started on the server side, and we can then just safely ignore the exception.

 ;;;","10/Mar/23 22:35;tzulitai;Looking at the KafkaProducer code, the {{TransactionManager}} keeps a {{transactionStarted}} flag that is only set when a record has actually been sent to the transaction. On {{commitTransaction()}} / {{abortTransaction()}} API calls on the Java client, if the flag is false, then the client won't actually send a {{EndTxnRequest}} to the brokers. See here: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L789

So:
{code:java}
producer.beginTransaction();
producer.commitTransaction();

// or

producer.beginTransaction();
producer.abortTransaction(); {code}
the above doesn't throw in normal continuous execution.
It only throws if there was job downtime between the {{beginTransction()}} call and the commit/abort call (because the flag would have been cleared)

So - I think the correct way to fix this (until Kafka changes the transaction protocol to properly support Flink's distributed transaction use case) is that we need to additionally persist the {{transactionStarted}} flag in Flink checkpoints as transaction metadata, and then set that appropriately when creating the recovery producer at restore time. ;;;","11/Mar/23 13:19;lightzhao;[~tzulitai] You are right, it is true that data cannot be consumed in READ_COMMITTED mode, and it is indeed impossible to directly change it to 'false'.;;;","16/Mar/23 21:14;tzulitai;[~lightzhao] I'm preparing a fix for this.

I think the correct fix is to *not* add a txn's metadata into the checkpoint if there were no data written to the transaction.
i.e. the checkpoints will only reflect metadata of txns that actually have data that need to be committed.

This way, creation of recovery producers can always safely set the {{transactionStarted}} flag to {{true}};;;","12/Apr/23 17:13;tzulitai;Merged via flink-connector-kafka a5df3c7c7f4925ff70114f862bca7588b819ac21;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HsResultPartitionTest fails with fatal error,FLINK-31359,13527486,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,mapohl,mapohl,07/Mar/23 16:51,08/Mar/23 09:11,13/Jul/23 08:29,08/Mar/23 02:59,1.18.0,,,,,,1.18.0,,,,,,,,Runtime / Coordination,,,,0,test-stability,,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46910&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8512

{code}
Mar 07 13:20:39 [ERROR] Process Exit Code: 239
Mar 07 13:20:39 [ERROR] Crashed tests:
Mar 07 13:20:39 [ERROR] org.apache.flink.runtime.io.network.partition.hybrid.HsResultPartitionTest
[...]
{code}",,leonard,mapohl,martijnvisser,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31346,,,,,,,,,,FLINK-31360,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 09:11:14 UTC 2023,,,,,,,,,,"0|z1geog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 16:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46911&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8512;;;","07/Mar/23 17:03;mapohl;[~Weijie Guo] it looks like this is related to FLINK-31346? Can you double-check?;;;","07/Mar/23 17:28;Weijie Guo;Thanks [~mapohl] for reporting this. This is only test issue, caused by FLINK-31346. When I prepared to pick it to release-1.17, I found this problem and fixed it. I will open a pull request to fix this also for master branch.;;;","08/Mar/23 02:59;Weijie Guo;master(1.18) via eff9b16799f162f31058be5acac567943a446dea.
This issue only affect master(1.18).;;;","08/Mar/23 09:11;martijnvisser;Also failed in https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46927&view=results - That was started before this fix was merged. Adding it here for completeness reasons;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NettyClientServerSslTest.testValidSslConnectionAdvanced timed out,FLINK-31354,13527392,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,07/Mar/23 07:54,22/May/23 16:21,13/Jul/23 08:29,22/May/23 16:20,1.16.1,1.17.0,1.18.0,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46883&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8242

{code}
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) is running.
--------------------------------------------------------------------------------
05:15:10,904 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: localhost/127.0.0.1, server port: 42935, ssl enabled: true, memory segment size (bytes): 1024, transport type: AUTO, number of server threads: 1 (manual), number of client threads>
05:15:10,916 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
05:15:12,149 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 1245 ms). Listening on SocketAddress /127.0.0.1:42935.
05:15:12,150 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
05:15:13,249 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 1099 ms).
05:15:14,588 [                main] ERROR org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest [] - 
--------------------------------------------------------------------------------
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) failed with:
org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandshakeTimeoutException: handshake timed out after 1000ms
        at org.apache.flink.shaded.netty4.io.netty.handler.ssl.SslHandler$7.run(SslHandler.java:2115)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.PromiseTask.runTask(PromiseTask.java:98)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.ScheduledFutureTask.run(ScheduledFutureTask.java:153)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:174)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:167)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:470)
        at org.apache.flink.shaded.netty4.io.netty.channel.epoll.EpollEventLoop.run(EpollEventLoop.java:403)
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
        at java.lang.Thread.run(Thread.java:748)
{code}",,mapohl,renqs,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 16:20:52 UTC 2023,,,,,,,,,,"0|z1ge3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 10:24;mapohl;I'm lowering the priority for this one to Critical. I compared the runtimes with another recent 1.17 build (see [build 20230307.4|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46885&view=logs&j=a57e0635-3fad-5b08-57c7-a4142d7d6fa9&t=2ef0effc-1da1-50e5-c2bd-aab434b1c5b7&l=7103]):
{code}
================================================================================
Test testValidSslConnectionAdvanced[SSL provider = JDK](org.apache.flink.runtime.io.network.netty.NettyClientServerSslTest) is running.
--------------------------------------------------------------------------------
02:53:03,827 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyConfig        [] - NettyConfig [server address: localhost/127.0.0.1, server port: 40169, ssl enabled: true, memory segment size (bytes): 1024, transport type: AUTO, number of server threads: 1 (manual), number of client threads: 1 (manual), server connect backlog: 0 (use Netty's default), client connect timeout (sec): 120, send/receive buffer size (bytes): 0 (use Netty's default)]
02:53:03,828 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Transport type 'auto': using EPOLL.
02:53:04,016 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful initialization (took 188 ms). Listening on SocketAddress /127.0.0.1:40169.
02:53:04,016 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Transport type 'auto': using EPOLL.
02:53:04,203 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful initialization (took 187 ms).
02:53:04,309 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyServer        [] - Successful shutdown (took 2 ms).
02:53:04,309 [                main] INFO  org.apache.flink.runtime.io.network.netty.NettyClient        [] - Successful shutdown (took 0 ms).
{code}

It appears that the timeout was hit by a slow CI machine. The 1s might be too long for a timeout here.;;;","22/Apr/23 13:46;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48354&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=8673;;;","16/May/23 08:14;renqs;[~mapohl] Any updates on this one? Thanks ;;;","16/May/23 08:27;mapohl;I forgot about that one. Thanks for pinging me. I will provide a PR where we increase the timeout.;;;","22/May/23 16:20;mapohl;master: 2805f027e6b207f21fa6224ac8e9f8e258bd49f4
1.17: 553c72a894b25c10e71e54e6567c187876234fd5
1.16: ab836498111d0939e97523585d1549f4fdff86ea;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2 times out on CI,FLINK-31351,13527328,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fsk119,dmvk,dmvk,06/Mar/23 18:53,09/Mar/23 08:19,13/Jul/23 08:29,09/Mar/23 03:16,1.16.1,1.17.0,1.18.0,,,,,,,,,,,,Connectors / Hive,,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46872&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24908]

 
{code:java}
Mar 06 18:28:56 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007ff4b1832000 nid=0x21b2 waiting on condition [0x00007ff3a8c3e000]
Mar 06 18:28:56    java.lang.Thread.State: TIMED_WAITING (sleeping)
Mar 06 18:28:56 	at java.lang.Thread.sleep(Native Method)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.waitUntilJobIsRunning(HiveServer2EndpointITCase.java:1004)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.lambda$testExecuteStatementInSyncModeWithRuntimeException2$37(HiveServer2EndpointITCase.java:711)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase$$Lambda$2018/2127600974.accept(Unknown Source)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.runExecuteStatementInSyncModeWithRuntimeException(HiveServer2EndpointITCase.java:999)
Mar 06 18:28:56 	at org.apache.flink.table.endpoint.hive.HiveServer2EndpointITCase.testExecuteStatementInSyncModeWithRuntimeException2(HiveServer2EndpointITCase.java:709)
Mar 06 18:28:56 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Mar 06 18:28:56 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Mar 06 18:28:56 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Mar 06 18:28:56 	at java.lang.reflect.Method.invoke(Method.java:498)
 {code}",,dmvk,fsk119,leonard,luoyuxia,mapohl,tanyuxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 08:19:25 UTC 2023,,,,,,,,,,"0|z1gdpk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 07:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46881&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=25249;;;","07/Mar/23 07:10;mapohl;[~luoyuxia] Can you have a look at it? So far, it only happened on {{master}}. But did we do Hive-related {{master}}-only changes or is it also affecting 1.17.0?;;;","07/Mar/23 07:26;luoyuxia;AFAK, we didn't do Hive-related master-only changes. I think it will also affect 1.17.0. Not quite sure, but I suspect it's related to the fix in FLINK-31092. [~fsk119] Could you please take a look?;;;","07/Mar/23 07:50;mapohl;1.16 is also affected: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46882&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=28925;;;","07/Mar/23 16:54;mapohl;1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46912&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=24080
;;;","08/Mar/23 06:04;fsk119;I think the main cause is that the Thread#stop is not safe. The problem here is the worker thread is stopped by the canceler thread when the worker thread is still loading classes. However, the class can only be loaded once per classloader. When failed to load the class, the next time Classloader#loadClass will throw exceptions outside. So here I think we just notify the users we can not interrupt thread in time rather than just stop the thread by force.;;;","08/Mar/23 11:55;fsk119;Merged into release-1.16: 6fd3b9b338433d1e8240a1598bda883ef01cc9c4
Merged into release-1.17: 32b370181853f4129fd237c6a57491863a7e8b8c
Merged into master: 384d6b10a2d69b9384052c3d4c3ad82babd201d1;;;","09/Mar/23 08:19;mapohl;The following build failure didn't include the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46971&view=logs&j=a5ef94ef-68c2-57fd-3794-dc108ed1c495&t=2c68b137-b01d-55c9-e603-3ff3f320364b&l=27561;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation fails to build due to unclosed shortcodes,FLINK-31348,13527310,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,dmvk,dmvk,06/Mar/23 16:27,07/Mar/23 12:52,13/Jul/23 08:29,07/Mar/23 12:51,,,,,,,1.15.4,1.16.2,1.17.0,,,,,,Documentation,,,,0,pull-request-available,,,"After migration to HUGO and using Hugo version 0.111.0 or higher, there are a bunch of unclosed shortcodes which prevent the documentation from being served locally.

 

Example:
{code:java}
docker run -v $(pwd):/src -p 1313:1313 jakejarvis/hugo-extended:latest server --buildDrafts --buildFuture --bind 0.0.0.0
 
...

Error: Error building site: ""/src/content.zh/docs/connectors/datastream/formats/parquet.md:111:1"": failed to extract shortcode: unclosed shortcode ""tabs"" {code}
 
This is caused by the new Hugo 0.111.0 version https://github.com/gohugoio/hugo/releases/tag/v0.111.0 which includes ""Throw an error when shortcode is expected to be closed 7d78a49 @bep #10675""",,dmvk,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 12:51:20 UTC 2023,,,,,,,,,,"0|z1gdlk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 12:51;martijnvisser;Fixed in:

master: 3ea83baad0c8413f8e1f4a027866335d13789538
release-1.17: bef6f5137645dd135d5db0043b0de4e920cb1253
release-1.16: 49d9ea6e5d3fdf1beca2513ade47a352521b79f3
release-1.15: f11499ec1448d1056ad6af01d91adaefa10a2b98;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AdaptiveSchedulerClusterITCase.testAutomaticScaleUp times out,FLINK-31347,13527303,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,dmvk,mapohl,mapohl,06/Mar/23 16:05,07/Mar/23 13:41,13/Jul/23 08:29,07/Mar/23 13:21,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46850&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10451

{code}
Mar 06 14:11:24 ""main"" #1 prio=5 os_prio=0 tid=0x00007f482800b800 nid=0x6eee waiting on condition [0x00007f48325cd000]
Mar 06 14:11:24    java.lang.Thread.State: TIMED_WAITING (sleeping)
Mar 06 14:11:24 	at java.lang.Thread.sleep(Native Method)
Mar 06 14:11:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)
Mar 06 14:11:24 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:144)
Mar 06 14:11:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.waitUntilParallelismForVertexReached(AdaptiveSchedulerClusterITCase.java:265)
Mar 06 14:11:24 	at org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase.testAutomaticScaleUp(AdaptiveSchedulerClusterITCase.java:153)
[...]
{code}",,dmvk,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 08:51:53 UTC 2023,,,,,,,,,,"0|z1gdk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 16:21;mapohl;It looks like all tasks finished successfully before starting the additional TaskManager:
{code}
[...]
13:55:38,791 [Blocking operator (2/4)#1] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Blocking operator (2/4)#1 (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_1_1) switched from RUNNING to FINISHED.
[...]
13:55:38,793 [Blocking operator (3/4)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Blocking operator (3/4)#0 (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_2_0) switched from RUNNING to FINISHED.
13:55:38,793 [Blocking operator (4/4)#0] INFO  org.apache.flink.runtime.taskmanager.Task                    [] - Blocking operator (4/4)#0 (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_3_0) switched from RUNNING to FINISHED.
[...]
13:55:38,802 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Blocking operator (3/4) (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_2_0) switched from RUNNING to FINISHED.
13:55:38,803 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Blocking operator (2/4) (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_1_1) switched from RUNNING to FINISHED.
13:55:38,803 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Blocking operator (4/4) (593ece5f322345887225822f6671ab8a_e7ebf13a548d34fc7ae1345a82aa6ed5_3_0) switched from RUNNING to FINISHED.
13:55:38,806 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.executiongraph.ExecutionGraph       [] - Job Unnamed job (cb5c7a7cddf612e4d2af549183152ccb) switched from state RUNNING to FINISHED.
13:55:38,810 [flink-akka.actor.default-dispatcher-6] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Job cb5c7a7cddf612e4d2af549183152ccb reached terminal state FINISHED.
13:55:38,816 [flink-akka.actor.default-dispatcher-7] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job cb5c7a7cddf612e4d2af549183152ccb reached terminal state FINISHED.
13:55:38,819 [mini-cluster-io-thread-4] INFO  org.apache.flink.runtime.dispatcher.StandaloneDispatcher     [] - Job cb5c7a7cddf612e4d2af549183152ccb has been registered for cleanup in the JobResultStore after reaching a terminal state.
[...]
13:55:38,846 [                main] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveSchedulerClusterITCase [] - Start additional TaskManager to scale up to the full parallelism.
[...]
{code};;;","06/Mar/23 16:36;mapohl;[~dmvk] [~chesnay] do you have capacity to look into it? It looks like the lock was released before reaching the taskManager creation, which is odd.

We didn't touch this part of the code in 1.17 as far as I remember, did we?;;;","06/Mar/23 16:37;mapohl;I lowered the priority to Critical because it appears to be a test code issue.;;;","06/Mar/23 16:39;dmvk;I'll give it a ""timeboxed"" shot now.;;;","07/Mar/23 08:51;dmvk;master: babb82553c804c5abbf14353bcca84ad13401676

release-1.17: c02099bcbfd9ec32d1e915bd3efed7dfd23597b3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch shuffle IO scheduler does not throw TimeoutException if numRequestedBuffers is greater than 0,FLINK-31346,13527294,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,06/Mar/23 14:49,09/Mar/23 04:21,13/Jul/23 08:29,09/Mar/23 04:21,1.16.1,1.17.0,,,,,1.16.2,1.17.0,,,,,,,Runtime / Network,,,,0,pull-request-available,,,"We currently rely on throw exception to trigger downstream task failover to avoid read buffer request deadlock. But if {{numRequestedBuffers}} is greater than 0, IO scheduler does not throw {{TimeoutException}}. This will cause a deadlock.
",,kurt.ding,leonard,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31359,,,,FLINK-31330,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 12:35:34 UTC 2023,,,,,,,,,,"0|z1gdi0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 12:35;Weijie Guo;master(1.18) via 5ad2ae2c24ade2655981f609298978d26329466f.
release-1.17 via 54c67e5e08c11ef9a538abbf14618f9e27be18f7.
release-1.16 via 860ce4f57b2599516cd199a20204c047ca34c1e3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trim autoscaler configMap to not exceed 1mb size limit,FLINK-31345,13527286,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,mxm,mxm,06/Mar/23 14:08,17/Mar/23 12:09,13/Jul/23 08:29,08/Mar/23 16:40,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.5.0,,,,,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"When the {{autoscaler-<deployment_name>}} ConfigMap which is used to persist scaling decisions and metrics becomes too large, the following error is thrown consistently:

{noformat}
io.fabric8.kubernetes.client.KubernetesClientException: Operation: [replace]  for kind: [ConfigMap]  with name: [deployment]  in namespace: [namespace]  failed.
    at io.fabric8.kubernetes.client.KubernetesClientException.launderThrowable(KubernetesClientException.java:159)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.lambda$replace$0(HasMetadataOperation.java:169)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:172)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:113)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.replace(HasMetadataOperation.java:41)
    at io.fabric8.kubernetes.client.extension.ResourceAdapter.replace(ResourceAdapter.java:252)
    at org.apache.flink.kubernetes.operator.autoscaler.AutoScalerInfo.replaceInKubernetes(AutoScalerInfo.java:167)
    at org.apache.flink.kubernetes.operator.autoscaler.JobAutoScalerImpl.scale(JobAutoScalerImpl.java:113)
    at org.apache.flink.kubernetes.operator.reconciler.deployment.AbstractFlinkResourceReconciler.reconcile(AbstractFlinkResourceReconciler.java:178)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:130)
    at org.apache.flink.kubernetes.operator.controller.FlinkDeploymentController.reconcile(FlinkDeploymentController.java:56)
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:145)
    at io.javaoperatorsdk.operator.processing.Controller$1.execute(Controller.java:103)
    at org.apache.flink.kubernetes.operator.metrics.OperatorJosdkMetrics.timeControllerExecution(OperatorJosdkMetrics.java:80)
    at io.javaoperatorsdk.operator.processing.Controller.reconcile(Controller.java:102)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.reconcileExecution(ReconciliationDispatcher.java:139)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleReconcile(ReconciliationDispatcher.java:119)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleDispatch(ReconciliationDispatcher.java:89)
    at io.javaoperatorsdk.operator.processing.event.ReconciliationDispatcher.handleExecution(ReconciliationDispatcher.java:62)
    at io.javaoperatorsdk.operator.processing.event.EventProcessor$ReconcilerExecutor.run(EventProcessor.java:406)
    at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
    at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
    at java.base/java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: stream was reset: NO_ERROR
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.waitForResult(OperationSupport.java:514)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleResponse(OperationSupport.java:551)
    at io.fabric8.kubernetes.client.dsl.internal.OperationSupport.handleUpdate(OperationSupport.java:347)
    at io.fabric8.kubernetes.client.dsl.internal.BaseOperation.handleUpdate(BaseOperation.java:680)
    at io.fabric8.kubernetes.client.dsl.internal.HasMetadataOperation.lambda$replace$0(HasMetadataOperation.java:167)
    ... 21 more
Caused by: okhttp3.internal.http2.StreamResetException: stream was reset: NO_ERROR
    at okhttp3.internal.http2.Http2Stream.checkOutNotClosed$okhttp(Http2Stream.kt:646)
    at okhttp3.internal.http2.Http2Stream$FramingSink.emitFrame(Http2Stream.kt:557)
    at okhttp3.internal.http2.Http2Stream$FramingSink.write(Http2Stream.kt:532)
    at okio.ForwardingSink.write(ForwardingSink.kt:29)
    at okhttp3.internal.connection.Exchange$RequestBodySink.write(Exchange.kt:218)
    at okio.RealBufferedSink.emitCompleteSegments(RealBufferedSink.kt:255)
    at okio.RealBufferedSink.write(RealBufferedSink.kt:185)
    at okhttp3.RequestBody$Companion$toRequestBody$2.writeTo(RequestBody.kt:152)
    at okhttp3.internal.http.CallServerInterceptor.intercept(CallServerInterceptor.kt:59)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.connection.ConnectInterceptor.intercept(ConnectInterceptor.kt:34)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.cache.CacheInterceptor.intercept(CacheInterceptor.kt:95)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.http.BridgeInterceptor.intercept(BridgeInterceptor.kt:83)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.http.RetryAndFollowUpInterceptor.intercept(RetryAndFollowUpInterceptor.kt:76)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at org.apache.flink.kubernetes.operator.metrics.KubernetesClientMetrics.intercept(KubernetesClientMetrics.java:130)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at io.fabric8.kubernetes.client.okhttp.OkHttpClientBuilderImpl$InteceptorAdapter.intercept(OkHttpClientBuilderImpl.java:70)
    at okhttp3.internal.http.RealInterceptorChain.proceed(RealInterceptorChain.kt:109)
    at okhttp3.internal.connection.RealCall.getResponseWithInterceptorChain$okhttp(RealCall.kt:201)
    at okhttp3.internal.connection.RealCall$AsyncCall.run(RealCall.kt:517)
    ... 3 more
    Suppressed: okhttp3.internal.http2.StreamResetException: stream was reset: NO_ERROR
        ... 31 more
 {noformat}

We should trim the ConfigMap to not exceed the size limit.",,gyfora,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 16:40:50 UTC 2023,,,,,,,,,,"0|z1gdg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Mar/23 16:40;gyfora;Merged to main:
f88cbf3fd1b99a574a1ed8b8a2869b96d932e521
70bf6a9d920e9affadb253e7760db12d4e0dd554;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EmbeddedDataStreamBatchTests.test_keyed_co_broadcast_side_output,FLINK-31337,13527223,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Juntao Hu,mapohl,mapohl,06/Mar/23 09:39,06/Mar/23 15:41,13/Jul/23 08:29,06/Mar/23 15:41,1.16.1,1.17.0,,,,,1.16.2,1.17.0,,,,,,,API / Python,,,,0,pull-request-available,test-stability,,"Same build, multiple times:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=24566
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=821b528f-1eed-5598-a3b4-7f748b13f261&t=6bb545dd-772d-5d8c-f258-f5085fba3295&l=24235
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=e92ecf6d-e207-5a42-7ff7-528ff0c5b259&t=40fc352e-9b4c-5fd8-363f-628f24b01ec2&l=24545
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=3e4dd1a2-fe2f-5e5d-a581-48087e718d53&t=b4612f28-e3b5-5853-8a8b-610ae894217a&l=24481
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46799&view=logs&j=bf5e383b-9fd3-5f02-ca1c-8f788e2e76d3&t=85189c57-d8a0-5c9c-b61d-fc05cfac62cf&l=24757

{code}
Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:743: 
Mar 04 01:21:35 _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
Mar 04 01:21:35 pyflink/datastream/tests/test_data_stream.py:63: in assert_equals_sorted
Mar 04 01:21:35     self.assertEqual(expected, actual)
Mar 04 01:21:35 E   AssertionError: Lists differ: ['0', '1', '2', '4', '5', '5', '6', '6'] != ['0', '1', '2', '3', '5', '5', '6', '6']
Mar 04 01:21:35 E   
Mar 04 01:21:35 E   First differing element 3:
Mar 04 01:21:35 E   '4'
Mar 04 01:21:35 E   '3'
Mar 04 01:21:35 E   
Mar 04 01:21:35 E   - ['0', '1', '2', '4', '5', '5', '6', '6']
Mar 04 01:21:35 E   ?                  ^
Mar 04 01:21:35 E   
Mar 04 01:21:35 E   + ['0', '1', '2', '3', '5', '5', '6', '6']
Mar 04 01:21:35 E   ?   
{code}",,dianfu,Juntao Hu,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31185,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 15:41:42 UTC 2023,,,,,,,,,,"0|z1gd28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 09:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46801&view=logs&j=9cada3cb-c1d3-5621-16da-0f718fb86602&t=c67e71ed-6451-5d26-8920-5a8cf9651901&l=27607;;;","06/Mar/23 09:51;mapohl;All test failures in 1.17 as well: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46802&view=results;;;","06/Mar/23 10:11;mapohl;* master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46809&view=results (all test failures related to this issue)
* 1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46811&view=results
* 1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46812&view=results
* master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46817&view=results
* 1.16: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46819&view=results
* 1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46820&view=results;;;","06/Mar/23 10:11;mapohl;This test is failing consistently in any stage for master, release-1.17 and release-1.16;;;","06/Mar/23 10:53;mapohl;It appears to be caused by FLINK-31185 which had a green CI build. We can see this on {{master}} as well: It fails consistently for the nightlies (for both the cron and the ci job) but doesn't fail for the per-change CI (even the ci job) (see [master CI|https://dev.azure.com/apache-flink/apache-flink/_build?definitionId=1&_a=summary&repositoryFilter=1&branchFilter=2%2C2%2C2%2C2%2C2%2C2%2C2%2C2]). [~dianfu] [~Juntao Hu] why is the test only triggered by CI in the nightlies even for the CI stage?;;;","06/Mar/23 13:26;Juntao Hu;[~mapohl], FLINK-31185 introduces a test case that exposes a hidden bug in PyFlink broadcast process feature since 1.16, which accidently escaped from all test cases with our default test config parallelism=2. The failed test case only fails in Python 3.7, while CI triggered by commit only runs with Python 3.10. Meanwhile, there're some changes in pickle protocol between 3.7 and 3.8+, which produce the exact serialized keys that makes the ""escaping"" possible.

I already created a PR fixing the hidden bug, thx for the report.;;;","06/Mar/23 15:41;dianfu;Fixed in:
- master via 4fd3cf133a22210607038305e97d1a6b1cc4d6c1
- release-1.17 via c43b3d23bde1047e675793bf3e64cfe5c514f088
- release-1.16 via 07f07782ea4fe9be08ef3fb905652c2693a3da4c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink 1.16 should implement new LookupFunction,FLINK-31331,13527201,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,06/Mar/23 08:33,06/Mar/23 09:45,13/Jul/23 08:29,06/Mar/23 09:45,,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"Only implements new LookupFunction, retry lookup join can work.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 09:45:13 UTC 2023,,,,,,,,,,"0|z1gcxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 09:45;lzljs3620320;master: dfc0d558d2c0d5d299e2da2ffa0819d0c4720919;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix Parquet stats extractor,FLINK-31329,13527173,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,06/Mar/23 05:43,06/Mar/23 09:01,13/Jul/23 08:29,06/Mar/23 09:01,,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"Some bugs in Parquet stats extractor:
 # Decimal Supports
 # Timestamp Supports
 # Null nullCounts supports",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 09:01:40 UTC 2023,,,,,,,,,,"0|z1gcr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Mar/23 09:01;lzljs3620320;master: 97ca28c4f97ed705b0ae27cd0b30954db1dd6a18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disabled source scaling breaks downstream scaling if source busyTimeMsPerSecond is 0,FLINK-31326,13527137,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mateczagany,mateczagany,05/Mar/23 15:20,09/Mar/23 15:05,13/Jul/23 08:29,09/Mar/23 15:05,kubernetes-operator-1.5.0,,,,,,kubernetes-operator-1.5.0,,,,,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"In case of 'scaling.sources.enabled'='false' the 'TARGET_DATA_RATE' of the source vertex will be calculated as '(1000 / busyTimeMsPerSecond) * numRecordsOutPerSecond' which currently on the main branch results in an infinite value if 'busyTimeMsPerSecond' is 0. This will also affect downstream operators.

I'm not that familiar with the autoscaler code, but it's in my opinion it's quite unexpected to have such behavioral changes by setting 'scaling.sources.enabled' to false.

 

With PR #543 for FLINK-30575 (https://github.com/apache/flink-kubernetes-operator/pull/543) scaling will happen even with 'busyTimeMsPerSecond' being 0, but it will result in unreasonably high parallelism numbers for downstream operators because 'TARGET_DATA_RATE' will be very high where 0 'busyTimeMsPerSecond' will be replaced with 1e-10.


Metrics from the operator logs (source=e5a72f353fc1e6bbf3bd96a41384998c, sink=51312116a3e504bccb3874fc80d5055e)

'scaling.sources.enabled'='true':
{code:java}
 jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.MAX_PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_UP_RATE_THRESHOLD.Current: 5.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_DOWN_RATE_THRESHOLD.Current: 10.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Current: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Average: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Current: Infinity
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Current: 3.8666666666666667
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Average: 3.8833333333333333

jobVertexID.51312116a3e504bccb3874fc80d5055e.PARALLELISM.Current: 4.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.MAX_PARALLELISM.Current: 12.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Current: 4.827299209321681
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Average: 4.848351269098938
jobVertexID.51312116a3e504bccb3874fc80d5055e.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_UP_RATE_THRESHOLD.Current: 10.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_DOWN_RATE_THRESHOLD.Current: 21.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Current: 7.733333333333333
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Average: 7.766666666666667{code}

'scaling.sources.enabled'='false':
{code:java}
 jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.MAX_PARALLELISM.Current: 1.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_PROCESSING_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_UP_RATE_THRESHOLD.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.SCALE_DOWN_RATE_THRESHOLD.Current: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Current: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.OUTPUT_RATIO.Average: 2.0
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Current: Infinity
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TRUE_OUTPUT_RATE.Average: NaN
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Current: Infinity
jobVertexID.e5a72f353fc1e6bbf3bd96a41384998c.TARGET_DATA_RATE.Average: NaN

jobVertexID.51312116a3e504bccb3874fc80d5055e.PARALLELISM.Current: 4.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.MAX_PARALLELISM.Current: 12.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Current: 5.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.TRUE_PROCESSING_RATE.Average: 4.980555555555556
jobVertexID.51312116a3e504bccb3874fc80d5055e.CATCH_UP_DATA_RATE.Current: 0.0
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_UP_RATE_THRESHOLD.Current: NaN
jobVertexID.51312116a3e504bccb3874fc80d5055e.SCALE_DOWN_RATE_THRESHOLD.Current: NaN
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Current: Infinity
jobVertexID.51312116a3e504bccb3874fc80d5055e.TARGET_DATA_RATE.Average: NaN{code}
 

My guess is 'scaling.sources.enabled' exists to support connectors where `pendingRecords` is not available, but setting this to false also negatively impacts existing Kafka sources for example, and users cannot anticipate this from the documentation.

 

I think it would be worth it to include this in the docs, or if anyone has any suggested solutions I would gladly look into implementing it.",,gyfora,mateczagany,mxm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 11:17:19 UTC 2023,,,,,,,,,,"0|z1gcj4:",9223372036854775807,Handle new-style and old-style sources equally well and remove option to disable scaling sources,,,,,,,,,,,,,,,,,,,"05/Mar/23 18:26;gyfora;Good catch [~mateczagany] , there are a few oddities around source metrics at the moment when load / incoming data rate is low. After we merge [~mxm] 's work on the pending record / zero scaling improvements, we should revisit this config and simplify the code for the source metrics if possible. ;;;","06/Mar/23 11:17;mxm;Thanks [~mateczagany] for reporting! You are right that the disabled source scaling isn't working properly and has unexpected side effects. It was a testing workaround in the very beginning to work with legacy sources which do not report busy time. I would propose to remove this option altogether and automatically skip scaling of sources which do not report busy time (NaN metric values).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken SingleThreadFetcherManager constructor API,FLINK-31324,13527091,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,yunta,yunta,yunta,04/Mar/23 15:40,06/Mar/23 10:13,13/Jul/23 08:29,06/Mar/23 10:13,,,,,,,1.17.0,1.18.0,,,,,,,Connectors / Parent,,,,0,pull-request-available,,,"FLINK-28853 changed the default constructor of {{SingleThreadFetcherManager}}. Though the {{SingleThreadFetcherManager}} is annotated as {{Internal}}, it actually acts as some-degree public API, which is widely used in many connector projects:
[flink-cdc-connector|https://github.com/ververica/flink-cdc-connectors/blob/release-2.3.0/flink-connector-mysql-cdc/src/main/java/com/ververica/cdc/connectors/mysql/source/reader/MySqlSourceReader.java#L93], [flink-connector-mongodb|https://github.com/apache/flink-connector-mongodb/blob/main/flink-connector-mongodb/src/main/java/org/apache/flink/connector/mongodb/source/reader/MongoSourceReader.java#L58] and so on.

Once flink-1.17 is released, all these existing connectors are broken and cannot be used in new release version, and will throw exceptions like:

{code:java}
java.lang.NoSuchMethodError: org.apache.flink.connector.base.source.reader.fetcher.SingleThreadFetcherManager.<init>(Lorg/apache/flink/connector/base/source/reader/synchronization/FutureCompletingBlockingQueue;Ljava/util/function/Supplier;)V
	at com.ververica.cdc.connectors.mysql.source.reader.MySqlSourceReader.<init>(MySqlSourceReader.java:91) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
	at com.ververica.cdc.connectors.mysql.source.MySqlSource.createReader(MySqlSource.java:159) ~[flink-sql-connector-mysql-cdc-2.3.0.jar:2.3.0]
	at org.apache.flink.streaming.api.operators.SourceOperator.initReader(SourceOperator.java:312) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.SourceOperatorStreamTask.init(SourceOperatorStreamTask.java:94) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:699) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:675) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:952) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:921) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:745) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:750) ~[?:1.8.0_362]
{code}


Thus, I suggest to make the original SingleThreadFetcherManager constructor as depreacted instead of removing it.",,becket_qin,jark,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28853,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 10:13:53 UTC 2023,,,,,,,,,,"0|z1gc8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Mar/23 15:42;yunta;cc [~jark] [~mxm];;;","06/Mar/23 02:29;jark;+1 for keeping compatibility. 

Besides, I think {{SingleThreadFetcherManager}} should be annotated {{@PublicEvolving}} because it is referenced by the public constructor of the  {{@PublicEvolving}} class {{SingleThreadMultiplexSourceReaderBase}}. 

According to FLIP-196,
> Per default, all public members of an API object inherit the stability guarantee of the owning object.

But I think this annotating {{@PublicEvolving}} may need another issue and discussion on the dev ML. 

What do you think [~yunta]?;;;","06/Mar/23 03:31;yunta;I think we can discuss changing the annotation of {{SingleThreadFetcherManager}}. However, the {{Internal}} annotation is introduced in FLINK-22358, which changed a lot of classes. Since I am not so familiar with these changes, I think we need to involve more committers on connector topics for discussion. cc [~becket_qin], [~renqs];;;","06/Mar/23 08:22;becket_qin;I think `SingleThreadFetcherManager` is indeed somewhat public at the moment. Connector implementations extend this class from time to time. So we probably need to make it backwards compatible even though it is marked as internal.

It also looks OK if we make it PublicEvolving. If we do so, the only additional class that we also need to make public is `FutureCompletingBlockingQueue` as PublicEvolving. So it does not pull in much unnecessary class exposures to the users.;;;","06/Mar/23 08:41;yunta;[~becket_qin], thanks for the reply. As the expert on the connector modules, I think you're more suitable to launch the discussion in the dev ML, and we can have a deep discussion in another ticket.;;;","06/Mar/23 10:13;yunta;merged,
master: 4d285753b75157ca078ba0917412c273d147aba9
release-1.17: 8acec2c14a2a56b9293354693008cc9c3e77692e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix unstable E2E test for flink actions,FLINK-31323,13527067,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,04/Mar/23 07:44,07/Mar/23 02:33,13/Jul/23 08:29,07/Mar/23 02:33,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"Currently, the flink actions use Data Stream API to do insert job making batch configuration invalid. We should use Table API instead.",,lzljs3620320,yzl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 02:33:43 UTC 2023,,,,,,,,,,"0|z1gc3k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 02:33;lzljs3620320;master: 1840de127c453b710d4aa0e1709c0001880f62f1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kafka new source partitionDiscoveryIntervalMs=0 cause bounded source can not quit,FLINK-31319,13527030,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,taoran,taoran,taoran,03/Mar/23 17:44,12/Apr/23 17:06,13/Jul/23 08:29,18/Mar/23 09:59,1.16.1,1.17.0,,,,,1.16.2,1.17.0,kafka-3.0.0,,,,,,Connectors / Kafka,,,,0,pull-request-available,,,"As kafka option description, partitionDiscoveryIntervalMs <=0 means disabled.

!image-2023-03-04-01-37-29-360.png|width=781,height=147!

just like start kafka enumerator:

!image-2023-03-04-01-39-20-352.png|width=465,height=311!

but inner 
handlePartitionSplitChanges use error if condition( < 0):

!image-2023-03-04-01-40-44-124.png|width=576,height=237!
 
it will cause noMoreNewPartitionSplits can not be set to true. 
!image-2023-03-04-01-41-55-664.png|width=522,height=610!

Finally cause bounded source can not signalNoMoreSplits, so it will not quit.

Besides，Both ends of the if condition should be mutually exclusive.",,leonard,martijnvisser,ramanverma,renqs,taoran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/23 17:37;taoran;image-2023-03-04-01-37-29-360.png;https://issues.apache.org/jira/secure/attachment/13056028/image-2023-03-04-01-37-29-360.png","03/Mar/23 17:39;taoran;image-2023-03-04-01-39-20-352.png;https://issues.apache.org/jira/secure/attachment/13056027/image-2023-03-04-01-39-20-352.png","03/Mar/23 17:40;taoran;image-2023-03-04-01-40-44-124.png;https://issues.apache.org/jira/secure/attachment/13056026/image-2023-03-04-01-40-44-124.png","03/Mar/23 17:41;taoran;image-2023-03-04-01-41-55-664.png;https://issues.apache.org/jira/secure/attachment/13056025/image-2023-03-04-01-41-55-664.png",,,,,4.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 18 09:59:10 UTC 2023,,,,,,,,,,"0|z1gbvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 20:29;taoran;Reproduce:  Using bounded kafka, set partitionDiscoveryIntervalMs=0, then job never quit. [~martijnvisser] [~renqs]  WDYT? it's maybe a big bug.;;;","09/Mar/23 08:57;martijnvisser;[~tzulitai] WDYT?;;;","16/Mar/23 14:11;leonard;Both 'partitionDiscoveryIntervalMs=0' and 'partitionDiscoveryIntervalMs<0' are meaningless,  it makes sense to me to do not discovery partitions in these cases. ;;;","16/Mar/23 19:20;ramanverma;[~lemonjing] Can you please link all the back port PRs (1.16 and 1.17) here as well for easier tracking;;;","17/Mar/23 02:19;taoran;[~ramanverma] thanks.
BP-1.16: [https://github.com/apache/flink/pull/22192]
BP-1.17: [https://github.com/apache/flink/pull/22193]
original: [https://github.com/apache/flink-connector-kafka/pull/8];;;","17/Mar/23 07:39;leonard;[~lemonjing] minor tips: the PR links would be automatically created If you used PR title  *FLINK-31319[BP-1.16]xxx*  instead of *[BP-1.16]FLINK-31319xxx. :)*;;;","17/Mar/23 09:27;renqs;1.17: 09fb503f0f3bba2fa0bbab9452baebe07af288cc;;;","18/Mar/23 09:59;leonard;Fixed by: 
 * flink-connector-kafka main : 58f35374b6aec63491623321f4de69a0affa629a
 * flink 1.17: 09fb503f0f3bba2fa0bbab9452baebe07af288cc
 * flink 1.16: a5f085e042be70f45485165c5755171ac5ed8cb4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FlinkActionsE2eTest.testMergeInto is unstable,FLINK-31315,13527011,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,03/Mar/23 15:45,19/Mar/23 05:36,13/Jul/23 08:29,19/Mar/23 05:36,,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,,,,"{code:java}
Error:  Tests run: 4, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 320.272 s <<< FAILURE! - in org.apache.flink.table.store.tests.FlinkActionsE2eTest
82Error:  testMergeInto  Time elapsed: 111.826 s  <<< FAILURE!
83org.opentest4j.AssertionFailedError: 
84Result is still unexpected after 60 retries.
85Expected: {3, v_3, creation, 02-27=1, 2, v_2, creation, 02-27=1, 6, v_6, creation, 02-28=1, 1, v_1, creation, 02-27=1, 8, v_8, insert, 02-29=1, 11, v_11, insert, 02-29=1, 7, Seven, matched_upsert, 02-28=1, 5, v_5, creation, 02-28=1, 10, v_10, creation, 02-28=1, 9, v_9, creation, 02-28=1}
86Actual: {4, v_4, creation, 02-27=1, 8, v_8, creation, 02-28=1, 3, v_3, creation, 02-27=1, 7, v_7, creation, 02-28=1, 2, v_2, creation, 02-27=1, 6, v_6, creation, 02-28=1, 1, v_1, creation, 02-27=1, 5, v_5, creation, 02-28=1, 10, v_10, creation, 02-28=1, 9, v_9, creation, 02-28=1}
87	at org.junit.jupiter.api.AssertionUtils.fail(AssertionUtils.java:39)
88	at org.junit.jupiter.api.Assertions.fail(Assertions.java:134)
89	at org.apache.flink.table.store.tests.E2eTestBase.checkResult(E2eTestBase.java:261)
90	at org.apache.flink.table.store.tests.FlinkActionsE2eTest.testMergeInto(FlinkActionsE2eTest.java:355) {code}",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-03 15:45:42.0,,,,,,,,,,"0|z1gbr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KafkaWriter doesn't wait for errors for in-flight records before completing flush,FLINK-31305,13526920,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mason6345,mason6345,mason6345,02/Mar/23 23:47,17/Apr/23 07:16,13/Jul/23 08:29,12/Apr/23 17:05,1.16.1,1.17.0,,,,,1.16.2,1.17.1,kafka-3.0.0,,,,,,Connectors / Kafka,,,,1,pull-request-available,,,"The KafkaWriter flushing needs to wait for all in-flight records to send successfully. This can be achieved by tracking requests and returning a response from the registered callback from the producer#send() logic.

There is potential for data loss since the checkpoint does not accurately reflect that all records have been sent successfully, to preserve at least once semantics.",,asardaes,martijnvisser,mason6345,sap1ens,stevenz3wu,tzulitai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 17 07:13:39 UTC 2023,,,,,,,,,,"0|z1gb6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 23:49;mason6345;Hi all, this is a critical regression for deployments who have migrated from the FlinkKafkaProducer to KafkaSink. Can someone with permissions assign this issue to me? Thanks in advance!;;;","09/Mar/23 09:01;martijnvisser;[~tzulitai] FYI;;;","10/Mar/23 07:51;mason6345;Opened a PR! cc: [~stevenz3wu] [~martijnvisser] [~tzulitai] ;;;","10/Mar/23 08:06;martijnvisser;[~mason6345] IIRC this would also affect Flink 1.17, right? If so, we should open the PR in the Flink branch, so it can be fixed for Flink 1.17 (since the connector is still bundled in 1.17) and then later cherry-picked to the external connector repo, and potentially also Flink 1.16? ;;;","10/Mar/23 08:22;mason6345;[~martijnvisser] Yes it does affect 1.17. Makes sense, and my question from the other JIRA ticket is answered here!;;;","10/Mar/23 18:32;mason6345;BTW, I had a mishap with git and so this is the proper Github PR to review: https://github.com/apache/flink/pull/22150;;;","23/Mar/23 03:34;mason6345;[~martijnvisser] [~tzulitai] Have time to review this small change? I need help from a committer;;;","23/Mar/23 04:17;tzulitai;thanks [~mason6345] for the ping! I've actually been discussing your changes with Raman Verma, who has did a review already on the PR.
I'll also do a pass tomorrow and then will try to merge this by end of this week.;;;","27/Mar/23 19:06;tzulitai;Merged via
 * {{apache/flink:main}}  7f47c11da1bbbd6e7650d43742694f2fa0ee8a3f
 * {{{}apache/flink:release-1.16{}}}: df6c4345d2e1b321cd77e74aa2b028b482b69f3c
 * {{{}apache/flink:release-1.17{}}}: b8a9fe5a9942fb6cf11ed56a807d68d375339259

Pending merges for {{apache/flink-connector-kafka}} as well as any necessary backports.;;;","12/Apr/23 17:05;tzulitai;Merged to flink-connector-kafka via:
 * v3.0 - dd09fa9a3d05c7096c85cb14f2a792a66a915547;;;","15/Apr/23 12:22;asardaes;I understand the externalized release of the connector (v3.0) will only be compatible with Flink 1.17.x, but _if_ a Flink 1.16.2 patch is released, will it also include a non-externalized release of the connector? Given the criticality of this, I had hoped the externalized connector would also support 1.16.x so I could immediately use it with 1.16.1.;;;","17/Apr/23 07:13;martijnvisser;[~asardaes] Yes, given that this is backported to the Flink 1.16 branch and (now) has a fixversion set to Flink 1.16.2 (since it exists in the {{apache/flink:release-1.16}} branch this will also be made available with Flink 1.16.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TRY_CAST fails for constructed types,FLINK-31300,13526876,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,02/Mar/23 15:20,09/Mar/23 22:14,13/Jul/23 08:29,09/Mar/23 22:14,1.16.1,1.17.0,,,,,1.18.0,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,"In case of problems with cast it is expected to return {{null}}

however for arrays, maps it fails

example of failing queries
{code:sql}
select try_cast(array['a'] as array<int>);
select try_cast(map['a', '1'] as map<int, int>);
{code}

 {noformat}
[ERROR] Could not execute SQL statement. Reason:
java.lang.NumberFormatException: For input string: 'a'. Invalid character found.
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.numberFormatExceptionFor(BinaryStringDataUtil.java:585)
	at org.apache.flink.table.data.binary.BinaryStringDataUtil.toInt(BinaryStringDataUtil.java:518)
	at StreamExecCalc$15.processElement(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:82)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:57)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:56)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.streaming.api.functions.source.InputFormatSourceFunction.run(InputFormatSourceFunction.java:92)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{noformat}",,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 22:14:10 UTC 2023,,,,,,,,,,"0|z1gaxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Mar/23 22:14;Sergey Nuyanzin;Merged at [5fef51f60085306f2acc8c8d630fe08e64004fc3|https://github.com/apache/flink/commit/5fef51f60085306f2acc8c8d630fe08e64004fc3];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PendingRecords metric might not be available,FLINK-31299,13526857,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mxm,mxm,mxm,02/Mar/23 12:57,06/Mar/23 16:34,13/Jul/23 08:29,06/Mar/23 16:34,,,,,,,kubernetes-operator-1.5.0,,,,,,,,Autoscaler,Kubernetes Operator,,,0,pull-request-available,,,"The Kafka pendingRecords metric is only initialized on receiving the first record. For empty topics or checkpointed topics without any incoming data, the metric won't appear.

We need to handle this case in the autoscaler and allow downscaling.",,mxm,pbharaj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-03-02 12:57:04.0,,,,,,,,,,"0|z1gat4:",9223372036854775807,Work around idle Kafka readers not emitting the pendingRecords metric,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConnectionUtilsTest.testFindConnectingAddressWhenGetLocalHostThrows swallows IllegalArgumentException,FLINK-31298,13526830,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,mapohl,mapohl,02/Mar/23 10:01,09/Mar/23 06:01,13/Jul/23 08:29,09/Mar/23 06:01,1.15.3,1.16.1,1.17.0,,,,1.16.2,1.17.0,,,,,,,Runtime / Network,,,,1,pull-request-available,starter,test-stability,"FLINK-24156 introduced {{NetUtils.acceptWithoutTimeout}} which caused the test to print a the stacktrace of an {{IllegalArgumentException}}:
{code}
Exception in thread ""Thread-0"" java.lang.IllegalArgumentException: serverSocket SO_TIMEOUT option must be 0
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
	at org.apache.flink.util.NetUtils.acceptWithoutTimeout(NetUtils.java:139)
	at org.apache.flink.runtime.net.ConnectionUtilsTest$1.run(ConnectionUtilsTest.java:83)
	at java.lang.Thread.run(Thread.java:750)
{code}

This is also shown in the Maven output of CI runs and might cause confusion. The test should be fixed.",,mapohl,Weijie Guo,Wencong Liu,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-24156,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 06:38:00 UTC 2023,,,,,,,,,,"0|z1gan4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 03:03;Wencong Liu;Hello [~mapohl] , I'd like to take this ticket. SocketOptions.SO_TIMEOUT should be set to 0.;;;","06/Mar/23 12:22;mapohl;thanks [~Wencong Liu]. I assigned the ticket to you;;;","08/Mar/23 02:50;Wencong Liu;cc [~mapohl] ;;;","08/Mar/23 06:38;Weijie Guo;master(1.18) via 7c5b7be5bc165a9799f10b5761a6ff15edee43b6.
release-1.17 via 704076a36024d521957e4e2f31820bbad7a102b3.
release-1.16 via b7b1cced495e29075adda10496238f251fe74d53.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager unstable when running it a single time,FLINK-31297,13526828,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,huwh,mapohl,mapohl,02/Mar/23 09:51,06/Mar/23 15:03,13/Jul/23 08:29,06/Mar/23 13:30,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"We noticed a weird test-instability in {{FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager}} when switching to sequential test execution (see FLINK-31278). I couldn't reproduce it in 1.16, therefore, marking it as a blocker for now. But it feels to be more of a test code issue.
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46671&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9695

{code}
Mar 01 15:20:17 [ERROR] org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager  Time elapsed: 0.746 s  <<< FAILURE!
Mar 01 15:20:17 java.lang.AssertionError: 
Mar 01 15:20:17 
Mar 01 15:20:17 Expected size: 1 but was: 0 in:
Mar 01 15:20:17 []
Mar 01 15:20:17 	at org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager(FineGrainedSlotManagerTest.java:209)
Mar 01 15:20:17 
{code}",,huwh,leonard,mapohl,Weijie Guo,xzw0223,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18229,,,,,,,,,FLINK-31278,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 13:30:04 UTC 2023,,,,,,,,,,"0|z1gamo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 09:53;mapohl;I can reproduce it locally [~huwh] may you have a look at it?;;;","02/Mar/23 10:22;xzw0223;After adding the PendingTaskManager, the pending is cleaned up in the registerTaskManager, I think the expected value should be 0 instead of 1.;;;","02/Mar/23 10:33;Weijie Guo;Thanks [~mapohl] for reporting this, I will take a look at this and contact [~huwh] at the same time.;;;","02/Mar/23 10:37;Weijie Guo;[~xzw0223] Thanks for looking into this, but I do not think directly change the expected value is the right approach. As the comments before the assertion, we can see that `task manager with allocated slot cannot deduct pending task manager`, the expected behavior is the pending taskmanager will not be reduced to 0, right?;;;","02/Mar/23 10:55;Weijie Guo;[~mapohl] By offline discuss with [~huwh], this is just a test code issue, I think we can down the priority.;;;","02/Mar/23 11:05;mapohl;Thanks for your verification :) I downgraded it to Critical. It's kind of blocking some efforts around FLINK-31278, though.;;;","02/Mar/23 16:02;huwh;Thanks [~mapohl] for reporting this bug. Thanks [~Weijie Guo] [~xzw0223] for your attention.

This bug was introduced by FLINK-18229. We release the pending task manager when these is no more resource requirements.

In FineGrainedSlotManagerTest.testTaskManagerRegistrationDeductPendingTaskManager, we skip the FineGrainedSlotManager and invoke TaskManagerTracker.addPendingTaskManager directly to allocate pending task manager. This make the resource requirements different between SlotManager and TaskManagerTracker. After requirementCheckDelay(50ms by default), the requirement check will release the pending task manager.;;;","06/Mar/23 03:54;huwh;Hi, [~xtsong] [~Weijie Guo] Could you help review this bugfix.;;;","06/Mar/23 13:30;Weijie Guo;master(1.18) via bec6a4589703bb7619cfc04bf69822995b49893f.
release-1.17 via c71d880dd01bddf926b4cebc33beb6c92b9aa25d.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitterOperator forgot to close Committer when closing.,FLINK-31294,13526800,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Ming Li,Ming Li,Ming Li,02/Mar/23 05:05,03/Mar/23 11:18,13/Jul/23 08:29,03/Mar/23 11:18,,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"{{CommitterOperator}} does not close the {{Committer}} when it closes, which may lead to resource leaks.",,lzljs3620320,Ming Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 11:18:39 UTC 2023,,,,,,,,,,"0|z1gagg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 05:20;Ming Li;[~lzljs3620320], hi, please help take a look at this issue if you have time.;;;","03/Mar/23 03:06;lzljs3620320;[~Ming Li] Thanks!;;;","03/Mar/23 11:18;lzljs3620320;master: 2e053e445be99dc0e7fc445728c381bbb8e7af37;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Request memory segment from LocalBufferPool may hanging forever.,FLINK-31293,13526796,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,02/Mar/23 04:21,04/Apr/23 02:37,13/Jul/23 08:29,04/Apr/23 02:36,1.16.1,1.17.0,1.18.0,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Network,,,,0,pull-request-available,,,"In our TPC-DS test, we found that in the case of fierce competition in network memory, some tasks may hanging forever.

From the thread dump information, we can see that the task is waiting for the {{LocalBufferPool}} to become available. It is strange that other tasks have finished and released network memory already. Undoubtedly, this is an unexpected behavior, which implies that there must be a bug in the {{LocalBufferPool}} or {{{}NetworkBufferPool{}}}.

!image-2023-03-02-12-23-50-572.png|width=650,height=153!

By dumping the heap memory, we can find a strange phenomenon that there are available buffers in the {{{}LocalBufferPool{}}}, but it was considered to be un-available. Another thing to note is that it now holds an overdraft buffer.

!image-2023-03-02-12-28-48-437.png|width=520,height=200!

!image-2023-03-02-12-29-03-003.png|width=438,height=84!

TL;DR: This problem occurred in multi-thread race related to the introduction of overdraft buffer.

Suppose we have two threads, called A and B. For simplicity, {{LocalBufferPool}} is called {{LocalPool}} and {{NetworkBufferPool}} is called {{{}GlobalPool{}}}.

Thread A continuously request buffers blocking from the {{{}LocalPool{}}}.
Thread B continuously return buffers to {{{}GlobalPool{}}}.
 # If thread A takes the last available buffer of {{{}LocalPool{}}}, but {{GlobalPool}} does not have a buffer at this time, it will register a callback function with {{{}GlobalPool{}}}.
 # Thread B returns one buffer to {{{}GlobalPool{}}}, but has not started to trigger the callback.
 # Thread A continues to request buffer. Because the {{availableMemorySegments}} of {{LocalPool}} is empty, it requests the overdraftBuffer instead. But there is already a buffer in the {{{}GlobalPool{}}}, it successfully gets the buffer.
 # Thread B triggers the callback. Since there is no buffer in {{GlobalPool}} now, the callback is re-registered.
 # Thread A continues to request buffer. Because there is no buffer in {{{}GlobalPool{}}}, it will block on {{{}CompletableFuture#get{}}}.
 # Thread B continues to return a buffer and triggers the recently registered callback. As a result, {{LocalPool}} puts the buffer into {{{}availableMemorySegments{}}}. Unfortunately, the current logic of {{shouldBeAvailable}} method is: if there is an overdraft buffer, {{LocalPool}} is considered as un-available.
 # Thread A will keep blocking forever.",,clouding,fanrui,hackergin,leonard,lsy,maguowei,martijnvisser,Ming Li,pnowojski,qingyue,tanyuxin,Weijie Guo,,,,,,,,,,,FLINK-31104,,,,,,,FLINK-26762,,,,,,,,,,,"02/Mar/23 04:23;Weijie Guo;image-2023-03-02-12-23-50-572.png;https://issues.apache.org/jira/secure/attachment/13055956/image-2023-03-02-12-23-50-572.png","02/Mar/23 04:28;Weijie Guo;image-2023-03-02-12-28-48-437.png;https://issues.apache.org/jira/secure/attachment/13055957/image-2023-03-02-12-28-48-437.png","02/Mar/23 04:29;Weijie Guo;image-2023-03-02-12-29-03-003.png;https://issues.apache.org/jira/secure/attachment/13055958/image-2023-03-02-12-29-03-003.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 04 02:36:37 UTC 2023,,,,,,,,,,"0|z1gafk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Apr/23 02:36;Weijie Guo;master(1.18) via fb6caee13710348a9b53284c2cabbdb2e7aa9739.
release-1.17 via 6a476bee5e452d1f172173ec018939c8a154886c.
release-1.16 via 9582727387d368d1b9e358aedb55c3f2eaae4371.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disable overdraft buffer for batch shuffle,FLINK-31288,13526788,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,02/Mar/23 03:49,03/Mar/23 14:50,13/Jul/23 08:29,03/Mar/23 14:50,1.16.1,1.17.0,,,,,1.17.0,,,,,,,,,,,,0,pull-request-available,,,"Only pipelined / pipelined-bounded partition needs overdraft buffer. More specifically, there is no reason to request more buffers for non-pipelined (i.e. batch) shuffle. The reasons are as follows:
 # For BoundedBlockingShuffle, each full buffer will be directly released.
 # For SortMergeShuffle, the maximum capacity of buffer pool is 4 * numSubpartitions. It is efficient enough to spill this part of memory to disk.
 # For Hybrid Shuffle, the buffer pool is unbounded. If it can't get a normal buffer, it also can't get an overdraft buffer.",,fanrui,leonard,renqs,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,FLINK-31104,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 15:35:00 UTC 2023,,,,,,,,,,"0|z1gads:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 10:08;renqs;Marked as blocker of 1.17 since we need to wait for this one before RC. ;;;","02/Mar/23 15:35;Weijie Guo;master(1.18) via 382148e1229901ab54503c8d9af6a18ea4c078dc.
release-1.17 via 7dd61c31714c1b07790982d21a486f5f803708df.
release-1.16 via 01c8eb59c1be92f1f8c1b81c66073eeb6009eb86.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python processes are still alive when shutting down a session cluster directly without stopping the jobs,FLINK-31286,13526778,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,02/Mar/23 02:57,08/Mar/23 17:54,13/Jul/23 08:29,02/Mar/23 06:38,,,,,,,1.15.4,1.16.2,1.17.0,,,,,,API / Python,,,,0,pull-request-available,,,"Reproduce steps:
1) start a standalone cluster: ./bin/start_cluster.sh
2) submit a PyFlink job which contains Python UDFs
3) stop the cluster: ./bin/stop_cluster.sh
4) Check if Python process still exists: ps aux | grep -i beam_boot

!image-2023-03-02-10-55-34-863.png!",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Mar/23 02:55;dianfu;image-2023-03-02-10-55-34-863.png;https://issues.apache.org/jira/secure/attachment/13055946/image-2023-03-02-10-55-34-863.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 06:38:36 UTC 2023,,,,,,,,,,"0|z1gabk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 06:38;dianfu;Fixed in:
- master: ab4e85e8bda51088cf64d5ddfb9bc0dab1c6e1fd
- 1.17: ecec13a1cdf6622b8f0257f35c24d597f9956f41
- 1.16: ed47440231a75e5de50038919a21a1e914458baa
- 1.15: ada67c0c1bd5b51a2c7cf10624a0e4f2870a9cc5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Correct the description of building from source with scala version,FLINK-31283,13526697,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yunta,yunta,yunta,01/Mar/23 14:14,07/Mar/23 06:19,13/Jul/23 08:29,07/Mar/23 06:19,,,,,,,1.15.4,1.16.2,1.17.0,1.18.0,,,,,Documentation,,,,0,pull-request-available,,,"After FLINK-20845, Flink dropped the support of Scala 2.11. However, the doc of ""building from source"" still has the description on scala-2.11.",,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 07 06:19:25 UTC 2023,,,,,,,,,,"0|z1g9tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Mar/23 06:19;yunta;merged
master: 6ac40847d3bd60aa9a373dfb6a390f4c67c6c48d
release-1.17: 07c193b20a89243fd59f068cc64e073b9a0f6f34
release-1.16: 085a70c8c69fef29a2ffe4a08f0a29a6874a3fbd
release-1.15: 4f27e70d317357a9eb2a3f8c6f1dbb6f86779149;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix MULTIPLY(TIMES) function doesn't support interval types,FLINK-31279,13526652,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,01/Mar/23 09:01,08/Mar/23 09:33,13/Jul/23 08:29,08/Mar/23 09:33,1.18.0,,,,,,1.18.0,,,,,,,,,,,,0,pull-request-available,,,"{code:java}
// code placeholder
Flink SQL> select 2 * interval '3'  day;
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.table.planner.codegen.CodeGenException: Interval expression type expected. {code}",,jackylau,jark,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 08 09:33:02 UTC 2023,,,,,,,,,,"0|z1g9jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 09:55;martijnvisser;[~jackylau] Why is this is a bug, the documentation shows that you should use INTERVAL like

{code:java}
E.g., INTERVAL ‘10 00:00:00.004’ DAY TO SECOND, INTERVAL ‘10’ DAY, or INTERVAL ‘2-10’ YEAR TO MONTH return intervals.
{code}

https://nightlies.apache.org/flink/flink-docs-master/docs/dev/table/functions/systemfunctions/;;;","01/Mar/23 10:06;jackylau;[~martijnvisser] i say the times function with args number and interval type;;;","08/Mar/23 09:33;jark;Fixed in master: 75d94108a8e7dc51825ff9063c2c3f4649bf0eb4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JM Deployment recovery logic inconsistent with health related restart ,FLINK-31277,13526639,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,gyfora,gyfora,01/Mar/23 08:13,02/Mar/23 08:42,13/Jul/23 08:29,02/Mar/23 08:42,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.5.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"The current JM Deployment logic that restarts missing deployments strictly requires HA metadata event for stateless deployments.

This is inconsistent with how the cluster health check related restarts work which can cause the operator to delete an unhealthy deployment and potentially leave it missing if the first deploy attempt fails.",,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 08:42:38 UTC 2023,,,,,,,,,,"0|z1g9go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 08:42;gyfora;Merged to main 8a30e9bd770b32e28d2a7c0fe1830f5f6d9ab090;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Left join with IS_NULL filter be wrongly pushed down and get wrong join results,FLINK-31273,13526624,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,01/Mar/23 07:27,28/Jun/23 14:35,13/Jul/23 08:29,15/Mar/23 10:14,1.16.1,1.17.0,,,,,1.16.2,1.17.1,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Left join with IS_NULL filter be wrongly pushed down and get wrong join results. The sql is:
{code:java}
SELECT * FROM MyTable1 LEFT JOIN MyTable2 ON a1 = a2 WHERE a2 IS NULL AND a1 < 10

The wrongly plan is:

LogicalProject(a1=[$0], b1=[$1], c1=[$2], b2=[$3], c2=[$4], a2=[$5])
+- LogicalFilter(condition=[IS NULL($5)])
   +- LogicalJoin(condition=[=($0, $5)], joinType=[left])
      :- LogicalValues(tuples=[[]])
      +- LogicalTableScan(table=[[default_catalog, default_database, MyTable2]]) {code}",,337361684@qq.com,aitozi,godfrey,jark,leonard,libenchao,lincoln.86xy,qingyue,yunta,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32471,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 15 10:14:02 UTC 2023,,,,,,,,,,"0|z1g9dc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Mar/23 10:14;godfrey;Fixed in

master: 8990822bd77d70f3249e1220a853e16dadd8ef54

1.17.1: 33278628dc599bed8944733efb9495ce77993d4b

1.16.2: f0361c720cb18c4ae7dc669c6a5da5b09bc8f563;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate operators appear in the StreamGraph for Python DataStream API jobs,FLINK-31272,13526622,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,dianfu,dianfu,01/Mar/23 07:19,08/Mar/23 17:54,13/Jul/23 08:29,01/Mar/23 13:25,1.15.0,,,,,,1.15.4,1.16.2,1.17.0,,,,,,API / Python,,,,0,pull-request-available,,,"For the following job:
{code}
import argparse
import json
import sys
import time
from typing import Iterable, cast

from pyflink.common import Types, Time, Encoder
from pyflink.datastream import StreamExecutionEnvironment, ProcessWindowFunction, EmbeddedRocksDBStateBackend, \
    PredefinedOptions, FileSystemCheckpointStorage, CheckpointingMode, ExternalizedCheckpointCleanup
from pyflink.datastream.connectors.file_system import FileSink, RollingPolicy, OutputFileConfig
from pyflink.datastream.state import ReducingState, ReducingStateDescriptor
from pyflink.datastream.window import TimeWindow, Trigger, TriggerResult, T, TumblingProcessingTimeWindows, \
    ProcessingTimeTrigger


class CountWithProcessTimeoutTrigger(ProcessingTimeTrigger):

    def __init__(self, window_size: int):
        self._window_size = window_size
        self._count_state_descriptor = ReducingStateDescriptor(
            ""count"", lambda a, b: a + b, Types.LONG())

    @staticmethod
    def of(window_size: int) -> 'CountWithProcessTimeoutTrigger':
        return CountWithProcessTimeoutTrigger(window_size)

    def on_element(self,
                   element: T,
                   timestamp: int,
                   window: TimeWindow,
                   ctx: 'Trigger.TriggerContext') -> TriggerResult:
        count_state = cast(ReducingState, ctx.get_partitioned_state(self._count_state_descriptor))
        count_state.add(1)
        # print(""element arrive:"", element, ""count_state:"", count_state.get(), window.max_timestamp(),
        #       ctx.get_current_watermark())

        if count_state.get() >= self._window_size:  # 必须fire&purge！！！！
            print(""fire element count"", element, count_state.get(), window.max_timestamp(),
                  ctx.get_current_watermark())
            count_state.clear()
            return TriggerResult.FIRE_AND_PURGE
        if timestamp >= window.end:
            count_state.clear()
            return TriggerResult.FIRE_AND_PURGE
        else:
            return TriggerResult.CONTINUE

    def on_processing_time(self,
                           timestamp: int,
                           window: TimeWindow,
                           ctx: Trigger.TriggerContext) -> TriggerResult:
        if timestamp >= window.end:
            return TriggerResult.CONTINUE
        else:
            print(""fire with process_time:"", timestamp)
            count_state = cast(ReducingState, ctx.get_partitioned_state(self._count_state_descriptor))
            count_state.clear()
            return TriggerResult.FIRE_AND_PURGE

    def on_event_time(self,
                      timestamp: int,
                      window: TimeWindow,
                      ctx: 'Trigger.TriggerContext') -> TriggerResult:
        return TriggerResult.CONTINUE

    def clear(self,
              window: TimeWindow,
              ctx: 'Trigger.TriggerContext') -> None:
        count_state = ctx.get_partitioned_state(self._count_state_descriptor)
        count_state.clear()


def to_dict_map(v):
    time.sleep(1)
    dict_value = json.loads(v)
    return dict_value


def get_group_key(value, keys):
    group_key_values = []
    for key in keys:
        one_key_value = 'null'
        if key in value:
            list_value = value[key]
            if list_value:
                one_key_value = str(list_value[0])
        group_key_values.append(one_key_value)
    group_key = '_'.join(group_key_values)
    # print(""group_key="", group_key)
    return group_key


class CountWindowProcessFunction(ProcessWindowFunction[dict, dict, str, TimeWindow]):

    def __init__(self, uf):
        self._user_function = uf

    def process(self,
                key: str,
                context: ProcessWindowFunction.Context[TimeWindow],
                elements: Iterable[dict]) -> Iterable[dict]:
        result_list = self._user_function.process_after_group_by_function(elements)
        return result_list


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--output',
        dest='output',
        required=False,
        help='Output file to write results to.')

    argv = sys.argv[1:]
    known_args, _ = parser.parse_known_args(argv)
    output_path = known_args.output

    env = StreamExecutionEnvironment.get_execution_environment()
    # write all the data to one file
    env.set_parallelism(1)

    # process time
    env.get_config().set_auto_watermark_interval(0)
    state_backend = EmbeddedRocksDBStateBackend(True)
    state_backend.set_predefined_options(PredefinedOptions.FLASH_SSD_OPTIMIZED)
    env.set_state_backend(state_backend)
    config = env.get_checkpoint_config()
    # config.set_checkpoint_storage(FileSystemCheckpointStorage(""hdfs://ha-nn-uri/tmp/checkpoint/""))
    config.set_checkpoint_storage(FileSystemCheckpointStorage(""file:///Users/10030122/Downloads/pyflink_checkpoint/""))
    config.set_checkpointing_mode(CheckpointingMode.AT_LEAST_ONCE)
    config.set_checkpoint_interval(5000)
    config.set_externalized_checkpoint_cleanup(ExternalizedCheckpointCleanup.DELETE_ON_CANCELLATION)

    # define the source
    data_stream1 = env.from_collection(['{""user_id"": [""0""], ""goods_id"": [0,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [1,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [2,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [3,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [4,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [5,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [6,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [7,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [8,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [9,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [10,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [11,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [12,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [13,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [14,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [15,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [16,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [17,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [18,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [19,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [20,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [21,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [22,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [23,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [24,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [25,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [26,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [27,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [28,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [29,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [30,0]}'])

    data_stream2 = env.from_collection(['{""user_id"": [""0""], ""goods_id"": [0,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [1,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [2,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [3,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [4,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [5,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [6,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [7,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [8,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [9,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [10,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [11,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [12,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [13,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [14,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [15,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [16,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [17,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [18,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [19,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [20,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [21,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [22,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [23,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [24,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [25,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [26,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [27,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [28,0]}',
                                        '{""user_id"": [""1""], ""goods_id"": [29,0]}',
                                        '{""user_id"": [""2""], ""goods_id"": [30,0]}'])

    # group_keys = ['user_id', 'goods_id']
    group_keys = ['user_id']

    sink_to_file_flag = True

    data_stream = data_stream1.union(data_stream2)

    # user_function = __import__(""UserFunction"")

    ds = data_stream.map(lambda v: to_dict_map(v)) \
        .filter(lambda v: v) \
        .map(lambda v: v) \
        .key_by(lambda v: get_group_key(v, group_keys)) \
        .window(TumblingProcessingTimeWindows.of(Time.seconds(12))) \
        .process(CountWindowProcessFunction(lambda v: v), Types.STRING())

    ds = ds.map(lambda v: v, Types.PRIMITIVE_ARRAY(Types.BYTE()))

    base_path = ""/tmp/1.txt""
    encoder = Encoder.simple_string_encoder()
    file_sink_builder = FileSink.for_row_format(base_path, encoder)
    file_sink = file_sink_builder \
        .with_bucket_check_interval(1000) \
        .with_rolling_policy(RollingPolicy.on_checkpoint_rolling_policy()) \
        .with_output_file_config(
        OutputFileConfig.builder().with_part_prefix(""pre"").with_part_suffix(""suf"").build()) \
        .build()
    ds.sink_to(file_sink)

    # submit for execution
    env.execute()
{code}

The stream graph is as following:
{code}
{
  ""nodes"" : [ {
    ""id"" : 1,
    ""type"" : ""Source: Collection Source"",
    ""pact"" : ""Data Source"",
    ""contents"" : ""Source: Collection Source"",
    ""parallelism"" : 1
  }, {
    ""id"" : 2,
    ""type"" : ""Source: Collection Source"",
    ""pact"" : ""Data Source"",
    ""contents"" : ""Source: Collection Source"",
    ""parallelism"" : 1
  }, {
    ""id"" : 9,
    ""type"" : ""TumblingProcessingTimeWindows"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Window(TumblingProcessingTimeWindows(12000, 0), ProcessingTimeTrigger, CountWindowProcessFunction)"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 15,
      ""ship_strategy"" : ""HASH"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 10,
    ""type"" : ""Map"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Map"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 9,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 15,
    ""type"" : ""Map, Filter, Map, _stream_key_by_map_operator"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Map, Filter, Map, _stream_key_by_map_operator"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 1,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    }, {
      ""id"" : 2,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 16,
    ""type"" : ""TumblingProcessingTimeWindows, Map"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Window(TumblingProcessingTimeWindows(12000, 0), ProcessingTimeTrigger, CountWindowProcessFunction)"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 15,
      ""ship_strategy"" : ""HASH"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 18,
    ""type"" : ""Sink: Writer"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Sink: Writer"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 10,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  }, {
    ""id"" : 20,
    ""type"" : ""Sink: Committer"",
    ""pact"" : ""Operator"",
    ""contents"" : ""Sink: Committer"",
    ""parallelism"" : 1,
    ""predecessors"" : [ {
      ""id"" : 18,
      ""ship_strategy"" : ""FORWARD"",
      ""side"" : ""second""
    } ]
  } ]
}
{code}

The plan is incorrect as we can see that TumblingProcessingTimeWindows appears twice.",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 13:25:29 UTC 2023,,,,,,,,,,"0|z1g9cw:",9223372036854775807,"It may produce duplicate operators for Python DataStream API jobs of versions 1.15.0 ~ 1.15.3 and 1.16.0 ~ 1.16.1. It has addressed this issue since 1.15.4, 1.16.2 and 1.17.0. For jobs which are not affected by this issue, there are no backward compatibility issues. However, for jobs which are affected, it may not be possible to restore from savepoints generated from versions 1.15.0 ~ 1.15.3 and 1.16.0 ~ 1.16.1.",,,,,,,,,,,,,,,,,,,"01/Mar/23 13:25;dianfu;Fixed in:
- master: 753734f0a1a6f44b9e33e84377f63ea6f0a85769
- 1.17: c19c6e5288429bf8df2550e85cefd91fffffb760
- 1.16: 823aa25381ee67673407d581da7fd7259cfbac06
- 1.15: 52cef9e5c6ae747e4f9b1f1d5725bd90db9da7b7;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix flink jar name in docs for table store,FLINK-31270,13526606,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,01/Mar/23 04:30,01/Mar/23 05:46,13/Jul/23 08:29,01/Mar/23 05:46,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,,,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 05:46:01 UTC 2023,,,,,,,,,,"0|z1g99c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 05:46;lzljs3620320;master: 32e0c37e291b239132d8b8959845439c63eeef94;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
can not get kerberos keytab in flink operator,FLINK-31258,13526515,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mbalassi,zhangjun,zhangjun,28/Feb/23 12:02,02/Mar/23 16:07,13/Jul/23 08:29,02/Mar/23 16:07,,,,,,,1.17.0,,,,,,,,Kubernetes Operator,,,,0,,,,"env:

flink k8s operator 1.4

flink 1.14.6 :

the conf
{code:java}
  flinkConfiguration:
    security.kerberos.login.keytab=/path/your/user.keytab 
   security.kerberos.login.principal=your@HADOOP.COM  {code}
and I get an exception:

 
{code:java}
Status:
  Cluster Info:
  Error:                          {""type"":""org.apache.flink.kubernetes.operator.exception.ReconciliationException"",""message"":""org.apache.flink.client.deployment.ClusterDeploymentException: Could not create Kubernetes cluster \""basic-example\""."",""throwableList"":[{""type"":""org.apache.flink.client.deployment.ClusterDeploymentException"",""message"":""Could not create Kubernetes cluster \""basic-example\"".""},{""type"":""org.apache.flink.configuration.IllegalConfigurationException"",""message"":""Kerberos login configuration is invalid: keytab [/path/your/user.keytab] doesn't exist!""}]} {code}
 

 ",,mbalassi,zhangjun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 02 16:07:12 UTC 2023,,,,,,,,,,"0|z1g8p4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Mar/23 16:07;mbalassi;This is actually due to the KerberosConfigMount in the flink-kubernetes module.

We introduced a config to disable this in 1.17:
https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#kubernetes-decorator-kerberos-mount-enabled

It is straight-forward to backport it if you need it in previous versions on your end, I would not cut a new Apache release for this of older versions.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OperatorUtils#createWrappedOperatorConfig should update input and sideOutput serializers,FLINK-31255,13526478,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Jiang Xin,zhangzp,zhangzp,28/Feb/23 08:15,18/Apr/23 05:50,13/Jul/23 08:29,18/Apr/23 05:50,ml-2.0.0,ml-2.1.0,ml-2.2.0,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"Currently we use operator wrapper to enable using normal operators in iterations. However, the operatorConfig is not correctly unwrapped. For example, the following code fails because of wrong type serializer.

 
{code:java}
@Test
public void testIterationWithMapPartition() throws Exception {
    StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
    DataStream<Long> input =
        env.fromParallelCollection(new NumberSequenceIterator(0L, 5L), Types.LONG);
    DataStreamList result =
        Iterations.iterateBoundedStreamsUntilTermination(
            DataStreamList.of(input),
            ReplayableDataStreamList.notReplay(input),
            IterationConfig.newBuilder()
                .setOperatorLifeCycle(OperatorLifeCycle.PER_ROUND)
                .build(),
            new IterationBodyWithMapPartition());

    List<Integer> counts = IteratorUtils.toList(result.get(0).executeAndCollect());
    System.out.println(counts.size());
}

private static class IterationBodyWithMapPartition implements IterationBody {

    @Override
    public IterationBodyResult process(
        DataStreamList variableStreams, DataStreamList dataStreams) {
        DataStream<Long> input = variableStreams.get(0);

        DataStream<Long> mapPartitionResult =
            DataStreamUtils.mapPartition(
                input,
                new MapPartitionFunction <Long, Long>() {
                    @Override
                    public void mapPartition(Iterable <Long> iterable, Collector <Long> collector)
                        throws Exception {
                        for (Long iter: iterable) {
                            collector.collect(iter);
                        }
                    }
                });

        DataStream<Integer> terminationCriteria =
            mapPartitionResult.<Long>flatMap(new TerminateOnMaxIter(2)).returns(Types.INT);

        return new IterationBodyResult(
            DataStreamList.of(mapPartitionResult), variableStreams, terminationCriteria);
    }
} {code}
The error stack is:

Caused by: java.lang.ClassCastException: java.lang.Long cannot be cast to org.apache.flink.iteration.IterationRecord
    at org.apache.flink.iteration.typeinfo.IterationRecordSerializer.serialize(IterationRecordSerializer.java:34)
    at org.apache.flink.iteration.datacache.nonkeyed.FileSegmentWriter.addRecord(FileSegmentWriter.java:79)
    at org.apache.flink.iteration.datacache.nonkeyed.DataCacheWriter.addRecord(DataCacheWriter.java:107)
    at org.apache.flink.iteration.datacache.nonkeyed.ListStateWithCache.add(ListStateWithCache.java:148)
    at org.apache.flink.ml.common.datastream.DataStreamUtils$MapPartitionOperator.processElement(DataStreamUtils.java:445)
    at org.apache.flink.iteration.operator.perround.OneInputPerRoundWrapperOperator.processElement(OneInputPerRoundWrapperOperator.java:69)
    at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
    at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
    at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
    at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
    at java.lang.Thread.run(Thread.java:748)",,lindong,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 18 05:48:41 UTC 2023,,,,,,,,,,"0|z1g8h4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Feb/23 08:17;zhangzp;It seems that the bug comes from not unwrapping all the streamconfig of the wrapped operator in OperatorUtils#createWrappedOperatorConfig.;;;","18/Apr/23 05:48;lindong;Merged to apache/flink-ml master branch 894685455d1c26fd45198857b7a96ee850725a59.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix possible bug of array_distinct,FLINK-31237,13526289,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,jackylau,jackylau,27/Feb/23 08:40,27/Feb/23 14:10,13/Jul/23 08:29,27/Feb/23 14:10,1.18.0,,,,,,1.18.0,,,,,,,,,,,,0,pull-request-available,,,"as talked here [https://github.com/apache/flink/pull/19623,] we should use builtin expressions/functions. because the sql semantic is different from  java equals",,jackylau,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 14:10:26 UTC 2023,,,,,,,,,,"0|z1g7b4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 14:10;Sergey Nuyanzin;Merged to master: [6797d6f2592373b2606ddd8c8aad316d677c1cc6|https://github.com/apache/flink/commit/6797d6f2592373b2606ddd8c8aad316d677c1cc6];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Limit pushdown should not open useless RecordReader,FLINK-31236,13526288,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,27/Feb/23 08:26,27/Feb/23 10:21,13/Jul/23 08:29,27/Feb/23 10:21,,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 10:21:12 UTC 2023,,,,,,,,,,"0|z1g7aw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Feb/23 10:21;lzljs3620320;master: ed454b8f909a85cdd43fac70c4280b24d0c0e34f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken links on flink.apache.org,FLINK-31206,13525993,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,martijnvisser,tison,tison,24/Feb/23 04:02,24/Feb/23 11:09,13/Jul/23 08:29,24/Feb/23 09:29,,,,,,,,,,,,,,,Project Website,,,,0,,,,"Previously page link https://flink.apache.org/contribute/code-style-and-quality/preamble/ is broken, new link is https://flink.apache.org/how-to-contribute/code-style-and-quality-preamble/.

Shall we set up a redirection or just let those broken links wait for maintainers fixing?

cc [~martijnvisser]",,martijnvisser,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 11:09:03 UTC 2023,,,,,,,,,,"0|z1g5hs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 09:29;martijnvisser;[~tison] Thanks for flagging it! https://flink.apache.org/contribute/code-style-and-quality/preamble/ was an incorrect redirect, now https://flink.apache.org/contributing/code-style-and-quality-preamble.html correctly redirects again to https://flink.apache.org/how-to-contribute/code-style-and-quality-preamble/

Fixed in asf-site: 9dc7f0cc1954ff4f845a34f9aeaa2723b345ba74;;;","24/Feb/23 11:09;tison;[~martijnvisser] Thanks for your follow-ups! See https://github.com/apache/flink-web/commit/9dc7f0cc1954ff4f845a34f9aeaa2723b345ba74#r101925551.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HiveCatalogITCase fails due to avro conflict in table store,FLINK-31204,13525970,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,24/Feb/23 00:25,24/Feb/23 04:00,13/Jul/23 08:29,24/Feb/23 04:00,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"Test fails in IDEA

	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	... 4 more
Caused by: java.lang.NoSuchMethodError: org.apache.avro.Schema.isNullable()Z
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.nullableSchema(AvroSchemaConverter.java:203)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:172)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:147)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:147)
	at org.apache.flink.table.store.format.avro.AvroSchemaConverter.convertToSchema(AvroSchemaConverter.java:55)
	at org.apache.flink.table.store.format.avro.AvroFileFormat$AvroGenericRecordBulkFormat.<init>(AvroFileFormat.java:95)
	at org.apache.flink.table.store.format.avro.AvroFileFormat.createReaderFactory(AvroFileFormat.java:80)
	at org.apache.flink.table.store.format.FileFormat.createReaderFactory(FileFormat.java:71)
	at org.apache.flink.table.store.format.FileFormat.createReaderFactory(FileFormat.java:67)
	at org.apache.flink.table.store.file.manifest.ManifestList$Factory.create(ManifestList.java:130)
	at org.apache.flink.table.store.file.operation.AbstractFileStoreScan.<init>(AbstractFileStoreScan.java:95)
	at org.apache.flink.table.store.file.operation.KeyValueFileStoreScan.<init>(KeyValueFileStoreScan.java:57)
	at org.apache.flink.table.store.file.KeyValueFileStore.newScan(KeyValueFileStore.java:118)
	at org.apache.flink.table.store.file.KeyValueFileStore.newScan(KeyValueFileStore.java:71)
	at org.apache.flink.table.store.file.KeyValueFileStore.newScan(KeyValueFileStore.java:38)
	at org.apache.flink.table.store.file.AbstractFileStore.newCommit(AbstractFileStore.java:116)
	at org.apache.flink.table.store.file.AbstractFileStore.newCommit(AbstractFileStore.java:43)
	at org.apache.flink.table.store.table.AbstractFileStoreTable.newCommit(AbstractFileStoreTable.java:121)
	at org.apache.flink.table.store.connector.sink.FileStoreSink.lambda$createCommitterFactory$63124b4e$1(FileStoreSink.java:69)
	at org.apache.flink.table.store.connector.sink.CommitterOperator.initializeState(CommitterOperator.java:104)
	at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.initializeOperatorState(StreamOperatorStateHandler.java:122)
	at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:283)
	at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:726)
	at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:702)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
	at java.lang.Thread.run(Thread.java:750)
",,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 04:00:42 UTC 2023,,,,,,,,,,"0|z1g5co:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 04:00;lzljs3620320;master: 306a9ededbebc1d825cbb02c18338f5accf7faca;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FullChangelogStoreSinkWrite bucket writer conflicts with rescale,FLINK-31195,13525841,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,lzljs3620320,lzljs3620320,23/Feb/23 08:22,29/Mar/23 03:07,13/Jul/23 08:29,29/Mar/23 03:07,,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,,,,"At present, this operator relies on ListState, Flink distributes data according to round-robin when rescaling, which may be different from the distribution rules of our bucket after rescaling.

We need to change the mode of UnionListState, broadcast to each node, and finally decide whether it belongs to the task.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-23 08:22:03.0,,,,,,,,,,"0|z1g4k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VectorIndexer should check whether doublesByColumn is null before snapshot,FLINK-31191,13525823,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,23/Feb/23 06:04,01/Mar/23 10:10,13/Jul/23 08:29,01/Mar/23 10:10,ml-2.2.0,,,,,,ml-2.2.0,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"Currently VectorIndexer would lead to NPE when doing checkpoint. It should check whether `doublesByColumn` is null before calling snapshot.

 

logview: [https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039]

details:
 
 
[735|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:736]Caused by: java.lang.NullPointerException 
[736|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:737] at org.apache.flink.ml.feature.vectorindexer.VectorIndexer$ComputeDistinctDoublesOperator.convertToListArray(VectorIndexer.java:232) 
[737|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:738] at org.apache.flink.ml.feature.vectorindexer.VectorIndexer$ComputeDistinctDoublesOperator.snapshotState(VectorIndexer.java:228) 
[738|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:739] at org.apache.flink.streaming.api.operators.StreamOperatorStateHandler.snapshotState(StreamOperatorStateHandler.java:222) 
[739|https://github.com/apache/flink-ml/actions/runs/4249415318/jobs/7389547039#step:4:740] ... 33 more",,lindong,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 10:10:45 UTC 2023,,,,,,,,,,"0|z1g4g0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 10:10;lindong;Merged to flink-ml master branch 10c08277fd7c03faef8ffb9fadd5008f2b101a19;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Standalone HA mode does not work if dynamic properties are supplied,FLINK-31187,13525768,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mateczagany,mateczagany,mateczagany,22/Feb/23 18:09,18/Jun/23 15:29,13/Jul/23 08:29,01/Mar/23 15:26,kubernetes-operator-1.4.0,,,,,,kubernetes-operator-1.5.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"With FLINK-30518 '--host $(POD_IP)' has been added to the arguments of the JMs which fixes the issue with HA on standalone mode, but it always gets appended to the end of the final JM arguments: https://github.com/usamj/flink-kubernetes-operator/blob/72ec9d384def3091ce50c2a3e2a06cded3b572e6/flink-kubernetes-standalone/src/main/java/org/apache/flink/kubernetes/operator/kubeclient/decorators/CmdStandaloneJobManagerDecorator.java#L107

But this will not be parsed properly in case any dynamic properties were set in the arguments, e.g.:
{code:java}
 Program Arguments:
   --configDir
   /opt/flink/conf
   -D
   jobmanager.memory.off-heap.size=134217728b
   -D
   jobmanager.memory.jvm-overhead.min=201326592b
   -D
   jobmanager.memory.jvm-metaspace.size=268435456b
   -D
   jobmanager.memory.heap.size=469762048b
   -D
   jobmanager.memory.jvm-overhead.max=201326592b
   --job-classname
   org.apache.flink.streaming.examples.statemachine.StateMachineExample
   --test
   test
   --host
   172.17.0.11{code}
You can verify this bug by using the YAML I've attached and in the JM logs you can see this line: 
{code:java}
Remoting started; listening on addresses :[akka.tcp://flink@flink-example-statemachine.flink:6123]{code}
Without any program arguments supplied this would correctly be:
{code:java}
Remoting started; listening on addresses :[akka.tcp://flink@172.17.0.8:6123]{code}

I believe this could be easily fixed by appending the --host parameter before JobSpec.args and if a committer can assign this ticket to me I can create a PR for this.",,gyfora,mateczagany,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30518,,,,,,,"22/Feb/23 18:06;mateczagany;standalone-ha.yaml;https://issues.apache.org/jira/secure/attachment/13055738/standalone-ha.yaml",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 15:26:13 UTC 2023,,,,,,,,,,"0|z1g43s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Mar/23 15:26;gyfora;merged to main 2af2c99d251bb84c77200f2896260bbe72a3ee6f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python BroadcastProcessFunction not support side output,FLINK-31185,13525738,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,22/Feb/23 12:01,06/Mar/23 10:37,13/Jul/23 08:29,03/Mar/23 12:10,1.16.1,,,,,,1.16.2,1.17.0,,,,,,,API / Python,,,,0,pull-request-available,,,,,dianfu,Juntao Hu,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31337,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 10:37:09 UTC 2023,,,,,,,,,,"0|z1g3x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Mar/23 12:10;dianfu;Fixed in:
- master: 8d52415a05bdc67eb67a59bbc2e53f48762da374
- 1.17: 7040af5b7933905798ff6af0b35ac364b5fbe432
- 1.16: 8713b176abc5c9f5267d7559ace0b6bd8afc6d3f;;;","06/Mar/23 10:14;martijnvisser;[~dianfu] Why was this added to Flink 1.17 or Flink 1.16? This doesn't seem like a bugfix, but it adds a new feature. Given that 1.17 is in featue freeze, this should only have been added to 1.17? This is now causing FLINK-31337;;;","06/Mar/23 10:28;dianfu;[~martijnvisser] The PR title seems like a feature, however it is trying to fix a bug reported in the slack channel: [https://apache-flink.slack.com/archives/C03G7LJTS2G/p1676950459225789?thread_ts=1676924426.566459&cid=C03G7LJTS2G];;;","06/Mar/23 10:30;dianfu;[~martijnvisser] Thanks for the reminder.  I have pinged [~Juntao Hu] to fix this issue ASAP.;;;","06/Mar/23 10:31;martijnvisser;[~dianfu] TBH that Slack thread still kind of reads like something that just wasn't supported before (""Okay it must just be not supported in the python layer. I can make a simple process function to get around it, but sad that the broadcast function can't emit side outputs."") - Since this is now blocking Flink 1.17, will this be a simple fix or do we need to revert this change?;;;","06/Mar/23 10:32;martijnvisser;[~dianfu] Thank you.;;;","06/Mar/23 10:37;dianfu;[~martijnvisser] It seems like an instable test(still not reproduced locally). I will disable this test for now if we cannot fix it in one or two hours later to not block the release. What do you think?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Kinesis EFO Consumer can fail to stop gracefully,FLINK-31183,13525726,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dannycranmer,dannycranmer,dannycranmer,22/Feb/23 10:36,23/Feb/23 17:06,13/Jul/23 08:29,23/Feb/23 17:06,1.15.3,1.16.1,aws-connector-4.0.0,,,,1.15.4,1.16.2,aws-connector-4.1.0,,,,,,Connectors / Kinesis,,,,0,pull-request-available,,,"*Background*

When stopping a Flink job using the stop-with-savepoint API the EFO Kinesis source can fail to close gracefully.

 

Sample stack trace
{code:java}
2023-02-16 20:45:40
org.apache.flink.runtime.checkpoint.CheckpointException: Task has failed.
	at org.apache.flink.runtime.messages.checkpoint.SerializedCheckpointException.unwrap(SerializedCheckpointException.java:51)
	at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.receiveDeclineMessage(CheckpointCoordinator.java:1013)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$declineCheckpoint$2(ExecutionGraphHandler.java:103)
	at org.apache.flink.runtime.scheduler.ExecutionGraphHandler.lambda$processCheckpointCoordinatorMessage$3(ExecutionGraphHandler.java:119)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
Caused by: org.apache.flink.runtime.checkpoint.CheckpointException: Task name with subtask : Source: vas_source_stream (38/48)#0 Failure reason: Task has failed.
	at org.apache.flink.runtime.taskmanager.Task.declineCheckpoint(Task.java:1395)
	at org.apache.flink.runtime.taskmanager.Task.lambda$triggerCheckpointBarrier$3(Task.java:1338)
	at java.base/java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:930)
	at java.base/java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:907)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:343)
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.RejectedExecutionException: event executor terminated
	at java.base/java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:331)
	at java.base/java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:346)
	at java.base/java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:1063)
	... 3 more
Caused by: java.util.concurrent.RejectedExecutionException: event executor terminated
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:923)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:350)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:343)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:825)
	at org.apache.flink.kinesis.shaded.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:815)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.nrs.HandlerPublisher$ChannelSubscription.cancel(HandlerPublisher.java:502)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.cancel(DelegatingSubscription.java:37)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.http2.Http2ResetSendingSubscription.cancel(Http2ResetSendingSubscription.java:41)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.cancel(DelegatingSubscription.java:37)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.http.nio.netty.internal.ResponseHandler$OnCancelSubscription.cancel(ResponseHandler.java:409)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber$1.cancel(FlatteningSubscriber.java:98)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber.handleStateUpdate(FlatteningSubscriber.java:170)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber.access$100(FlatteningSubscriber.java:29)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.FlatteningSubscriber$1.request(FlatteningSubscriber.java:93)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.request(DelegatingSubscription.java:32)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.request(DelegatingSubscription.java:32)
	at org.apache.flink.kinesis.shaded.software.amazon.awssdk.utils.async.DelegatingSubscription.request(DelegatingSubscription.java:32)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber$FanOutShardSubscription.requestRecord(FanOutShardSubscriber.java:401)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.consumeAllRecordsFromKinesisShard(FanOutShardSubscriber.java:355)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutShardSubscriber.subscribeToShardAndConsumeRecords(FanOutShardSubscriber.java:189)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.runWithBackoff(FanOutRecordPublisher.java:169)
	at org.apache.flink.streaming.connectors.kinesis.internals.publisher.fanout.FanOutRecordPublisher.run(FanOutRecordPublisher.java:124)
	at org.apache.flink.streaming.connectors.kinesis.internals.ShardConsumer.run(ShardConsumer.java:114)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
 {code}",,dannycranmer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-23528,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 12:41:36 UTC 2023,,,,,,,,,,"0|z1g3ug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 12:41;dannycranmer;Merged commit [{{a8c34db}}|https://github.com/apache/flink/commit/a8c34db3d601c534f92e68a2709a6467eb94276e] into apache:release-1.15 

Merged commit [{{cd7b049}}|https://github.com/apache/flink/commit/cd7b0495bcdadc3a9808a475be819c9808d5f17e] into apache:release-1.16 

Merged commit [{{fdfe982}}|https://github.com/apache/flink-connector-aws/commit/fdfe9821b36027e9afd8db4d32ac8eff080dad2d] into apache:main ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompiledPlan cannot deserialize BridgingSqlFunction with MissingTypeStrategy,FLINK-31182,13525719,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,twalthr,qingyue,qingyue,22/Feb/23 10:17,06/Mar/23 14:33,13/Jul/23 08:29,06/Mar/23 14:33,1.17.0,1.17.1,1.18.0,,,,1.16.2,1.17.0,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"This issue is reported from the [user mail list|https://lists.apache.org/thread/y6fgzyx330omhkr40376knw8k4oczz3s].

The stacktrace is 
{code:java}
Unable to find source-code formatter for language: text. Available languages are: actionscript, ada, applescript, bash, c, c#, c++, cpp, css, erlang, go, groovy, haskell, html, java, javascript, js, json, lua, none, nyan, objc, perl, php, python, r, rainbow, ruby, scala, sh, sql, swift, visualbasic, xml, yamlCaused by: org.apache.flink.table.api.TableException: Could not resolve internal system function '$UNNEST_ROWS$1'. This is a bug, please file an issue.
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeInternalFunction(RexNodeJsonDeserializer.java:392)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeSqlOperator(RexNodeJsonDeserializer.java:337)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserializeCall(RexNodeJsonDeserializer.java:307)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:146)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:128)
    at org.apache.flink.table.planner.plan.nodes.exec.serde.RexNodeJsonDeserializer.deserialize(RexNodeJsonDeserializer.java:115) {code}
The root cause is that although ModuleManager can resolve '$UNNEST_ROWS$1', the output type strategy is ""Missing""; as a result, FunctionCatalogOperatorTable#convertToBridgingSqlFunction returns empty.
!screenshot-1.png|width=675,height=295!",,qingyue,Sergey Nuyanzin,twalthr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/23 10:38;qingyue;screenshot-1.png;https://issues.apache.org/jira/secure/attachment/13055730/screenshot-1.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 14:33:13 UTC 2023,,,,,,,,,,"0|z1g3sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 12:03;qingyue;Correct me if I'm wrong, but for all BultinFunctionDefinition with output type strategy as TypeStrategies.MISSING, the deserialization will fail. By removing the check, the execution of the deserialized plan is successful. However, I'm not very sure about the purpose of the check. Can anyone shed some light?;;;","22/Feb/23 19:51;Sergey Nuyanzin;it looks this code is present for a long time... Doesn't it work with older versions as well?
Or if it works with e.g. 1.16.x or 1.15.x then probably the reason in some other changes;;;","23/Feb/23 02:24;qingyue;[~Sergey Nuyanzin] Yes, the check on the output type inference strategy was introduced in FunctionCatalogTableOperator in FLINK-15487 for FLIP-65 a long time ago.

I guess it worked well before FLIP-190 was introduced because`LogicalUnnestRule` will infer the output type and convert UNNEST to EXPLODE. (see [LogicalUnnestRule L#99|https://github.com/apache/flink/blob/master/flink-table/flink-table-planner/src/main/scala/org/apache/flink/table/planner/plan/rules/logical/LogicalUnnestRule.scala#L99]). As a result, although the BuiltinFunctionDefinition for UNNEST always has a ""MISSING"" type inference, it does not affect the execution.

However, after CompiledPlan is introduced, RexNodeJsonDeserializer relies on the OperatorTable to lookup functions, which always get a static placeholder (i.e. MISSING) for UNNEST, and due to this check, the deserialization failed. 

Could you help to take a look, cc [~godfreyhe] and [~twalthr] ;;;","27/Feb/23 13:44;twalthr;[~qingyue] I opened a PR. Would be great if you can help me with a review.;;;","05/Mar/23 07:40;qingyue;Hi [~twalthr], thanks for the fix!  LGTM, and feel free to merge.;;;","06/Mar/23 14:33;twalthr;Fixed in master: 86e0a0b384291f9d8bacc3bbef3c58fcfb79bb04
Fixed in 1.17: 5c1ec1980b0adf24a1557119e6b28efa7fffe93e
Fixed in 1.16: d71a87dc722e2b311d929623d5715321a003353b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix several bugs in flink-ml-iteration module,FLINK-31173,13525668,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,22/Feb/23 02:31,20/Apr/23 09:48,13/Jul/23 08:29,20/Apr/23 08:40,ml-2.0.0,ml-2.1.0,ml-2.2.0,,,,ml-2.3.0,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"In flink-ml-iteration, there are several bugs as follows:
 # TailOperator should have one input operator. We have added a Tail operator to increment the epoch watermark at each iteration. We have made an assumption that each Tail operator have only one input and did not align the epoch watermarks from different inputs. This assumption might not be true if the input is an `union`.
 # ProxyOperatorStateBackend does not correctly initialize the state descriptor.",,lindong,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 20 08:40:20 UTC 2023,,,,,,,,,,"0|z1g3hk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Apr/23 08:40;lindong;Merged to apache/flink-ml master branch:
- 92ecb0e591f30ff7dc4bd4db027350ad4edf4000
- 0c852da7a1b87bc632e21a19148dbb8d19a8bd30;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesResourceManagerDriverTest.testOnPodDeleted fails fatally due to 239 exit code,FLINK-31169,13525560,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,xtsong,mapohl,mapohl,21/Feb/23 12:57,22/Feb/23 10:06,13/Jul/23 08:29,22/Feb/23 09:56,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"master: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46341&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=27329

{code}
[...]
Feb 21 04:44:11 [ERROR] Process Exit Code: 239
Feb 21 04:44:11 [ERROR] Crashed tests:
Feb 21 04:44:11 [ERROR] org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest
Feb 21 04:44:11 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
[...]
{code}

{code}
[...]
Test org.apache.flink.kubernetes.KubernetesResourceManagerDriverTest.testOnPodDeleted[testOnPodDeleted()] is running.
--------------------------------------------------------------------------------
04:43:57,681 [ForkJoinPool-4-worker-1] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Recovered 0 pods from previous attempts, current attempt id is 1.
04:43:57,701 [testing-rpc-main-thread] INFO  org.apache.flink.runtime.externalresource.ExternalResourceUtils [] - Enabled external resources: []
04:43:57,705 [testing-rpc-main-thread] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Creating new TaskManager pod with name testing-flink-cluster-taskmanager-1-1 and resource <704,0.0>.
04:43:57,708 [testing-rpc-main-thread] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Received new TaskManager pod: testing-flink-cluster-taskmanager-1-1
04:43:57,708 [testing-rpc-main-thread] INFO  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Pod testing-flink-cluster-taskmanager-1-1 is created.
04:43:57,708 [testing-rpc-main-thread] WARN  org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Pod testing-flink-cluster-taskmanager-1-1 is terminated before being scheduled.
04:43:57,709 [testing-rpc-main-thread] ERROR org.apache.flink.kubernetes.KubernetesResourceManagerDriver  [] - Error completing resource request.
org.apache.flink.util.FlinkException: Pod is terminated.
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.onPodTerminated(KubernetesResourceManagerDriver.java:379) ~[classes/:?]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.lambda$handlePodEventsInMainThread$2(KubernetesResourceManagerDriver.java:347) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
04:43:57,724 [testing-rpc-main-thread] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'testing-rpc-main-thread' produced an uncaught exception. Stopping the process...
java.util.concurrent.CompletionException: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Pod is terminated.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:838) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniHandle.tryFire(CompletableFuture.java:811) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_292]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.onPodTerminated(KubernetesResourceManagerDriver.java:379) ~[classes/:?]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.lambda$handlePodEventsInMainThread$2(KubernetesResourceManagerDriver.java:347) ~[classes/:?]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.lang.RuntimeException: org.apache.flink.util.FlinkException: Pod is terminated.
        at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321) ~[flink-core-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.kubernetes.KubernetesResourceManagerDriver.lambda$requestResource$1(KubernetesResourceManagerDriver.java:233) ~[classes/:?]
        at java.util.concurrent.CompletableFuture.uniHandle(CompletableFuture.java:836) ~[?:1.8.0_292]
        ... 12 more
Caused by: org.apache.flink.util.FlinkException: Pod is terminated.
        ... 9 more
[...]
{code}",,mapohl,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30908,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 10:06:37 UTC 2023,,,,,,,,,,"0|z1g2tk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 13:02;mapohl;release-1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46343&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=27329;;;","21/Feb/23 13:11;mapohl;[~xtsong] may you have a look at this? Looks like it's related to FLINK-30908;;;","22/Feb/23 02:59;xtsong;This is indeed caused by FLINK-30908. The ""pod termination before being scheduled"" is mis-treated as fatal error. I have provided a fix. Without the fix, I can reproduce the problem locally in about 200~300 runs. After the fix, I cannot reproduce the problem in 100k run.;;;","22/Feb/23 07:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46382&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=26468;;;","22/Feb/23 09:56;xtsong;- master (1.18): d7cae5365d730272a4089988c235c2038eafab53
- release-1.17: bfadd9c69223765d485ed2371fa108b25756c1fc;;;","22/Feb/23 10:06;mapohl;The following build failure didn't contain the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46385&view=logs&j=5cae8624-c7eb-5c51-92d3-4d2dacedd221&t=5acec1b4-945b-59ca-34f8-168928ce5199&l=26468;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
array_contains does NOT work when haystack elements are not nullable and needle is nullable,FLINK-31166,13525538,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jackylau,jackylau,jackylau,21/Feb/23 10:41,09/Mar/23 23:43,13/Jul/23 08:29,09/Mar/23 23:43,1.18.0,,,,,,1.18.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"{{ARRAY_CONTAINS}} works ok for the case when both haystack elements and needle are not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], 0);{code}
it works ok when both haystack elements and needle are nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], CAST(NULL AS INT));{code}
it works ok when haystack elements are nullable and needle is not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], 1);{code}
and it does NOT work when haystack elements are not nullable and needle is nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], CAST(NULL AS INT));{code}
 

!image-2023-02-22-09-56-59-257.png!

 

!image-2023-02-21-18-41-19-385.png!",,jackylau,martijnvisser,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/23 10:37;jackylau;image-2023-02-21-18-37-45-202.png;https://issues.apache.org/jira/secure/attachment/13055681/image-2023-02-21-18-37-45-202.png","21/Feb/23 10:41;jackylau;image-2023-02-21-18-41-19-385.png;https://issues.apache.org/jira/secure/attachment/13055679/image-2023-02-21-18-41-19-385.png","22/Feb/23 01:56;jackylau;image-2023-02-22-09-56-59-257.png;https://issues.apache.org/jira/secure/attachment/13055702/image-2023-02-22-09-56-59-257.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 09 23:43:14 UTC 2023,,,,,,,,,,"0|z1g2oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 12:20;martijnvisser;[~jackylau] Can you please add a reproducible example for this?;;;","22/Feb/23 01:56;jackylau;[~martijnvisser] 

select array_contains(array[1, 2, 3], cast(null as int));;;;","22/Feb/23 07:58;martijnvisser;[~jackylau] So this is specifically about the behaviour when the needle (given ARRAY_CONTAINS(haystack, needle) is the function) is null? ;;;","09/Mar/23 10:22;Sergey Nuyanzin;[~jackylau] can you please update jira description with some example e.g. from the PR.
Having screenshots in description does not help much since it is impossible to copy&paste and run on Flink.

if i understand correctly from the PR:
{{ARRAY_CONTAINS}} works ok for the case when both haystack elements and needle are not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], 0);{code}
it works ok when both haystack elements and needle are nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], CAST(NULL AS INT));{code}
it works ok when haystack elements are nullable and needle is not nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1, NULL], 1);{code}
and it does NOT work when haystack elements are not nullable and needle is nullable e.g.
{code:sql}
SELECT array_contains(ARRAY[0, 1], CAST(NULL AS INT));{code}
I'm asking since for me 
in fact it was impossible to get to this from jira description reading and I still want to double check if I understand it correctly;;;","09/Mar/23 11:04;jackylau;hi [~Sergey Nuyanzin] thanks for your description. yeap, you are right. and i will describe the Jira will later ;;;","09/Mar/23 11:06;jackylau;have fixed this pr info [~Sergey Nuyanzin] , thanks for your detail review very much and i have learned a lot ;;;","09/Mar/23 23:43;Sergey Nuyanzin;Merged as [10dce7cf0a04b80d7416a5760e1a6dbc430d9f88|https://github.com/apache/flink/commit/10dce7cf0a04b80d7416a5760e1a6dbc430d9f88];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Over Agg: The window rank function without order by error in top N query,FLINK-31165,13525522,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,rohankrao,rohankrao,21/Feb/23 08:54,10/Apr/23 13:06,13/Jul/23 08:29,30/Mar/23 02:00,1.16.0,,,,,,1.17.1,1.18.0,,,,,,,Table SQL / API,,,,0,pull-request-available,,," 
{code:java}
val env: StreamExecutionEnvironment = StreamExecutionEnvironment.getExecutionEnvironment

val tableEnv = StreamTableEnvironment.create(env)


val td = TableDescriptor.forConnector(""datagen"").option(""rows-per-second"", ""10"")
  .option(""number-of-rows"", ""10"")
  .schema(Schema
    .newBuilder()
    .column(""NAME"", DataTypes.VARCHAR(2147483647))
    .column(""ROLLNO"", DataTypes.DECIMAL(5, 0))
    .column(""DOB"", DataTypes.DATE())
    .column(""CLASS"", DataTypes.DECIMAL(2, 0))
    .column(""SUBJECT"", DataTypes.VARCHAR(2147483647))
    .build())
  .build()

val table = tableEnv.from(td)


tableEnv.createTemporaryView(""temp_table"", table)

val newTable = tableEnv.sqlQuery(""select temp_table.*,cast('2022-01-01' as date) SRC_NO from temp_table"")

tableEnv.createTemporaryView(""temp_table2"", newTable)


val newTable2 = tableEnv.sqlQuery(""select * from (select NAME,ROLLNO,row_number() over (partition by NAME ORDER BY SRC_NO) AS rownum  from temp_table2 a) where rownum <= 1"")

tableEnv.toChangelogStream(newTable2).print()

env.execute()
 {code}
 

 

I am getting the below error if I run the above code.

I have already provided an order by column.

If I change the order by column to some other column, such as ""SUBJECT"", then the job runs fine.

 

 
{code:java}
Exception in thread ""main"" java.lang.RuntimeException: Error while applying rule FlinkLogicalOverAggregateConverter(in:NONE,out:LOGICAL), args [rel#245:LogicalWindow.NONE.any.None: 0.[NONE].[NONE](input=RelSubset#244,window#0=window(partition {0} rows between UNBOUNDED PRECEDING and CURRENT ROW aggs [ROW_NUMBER()]))]
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:256)
    at org.apache.calcite.plan.volcano.IterativeRuleDriver.drive(IterativeRuleDriver.java:58)
    at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:510)
    at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:62)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:187)
    at scala.collection.TraversableOnce$folder$1$.apply(TraversableOnce.scala:185)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:189)
    at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:184)
    at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
    at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:176)
    at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:83)
    at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
    at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:315)
    at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:195)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:224)
    at org.apache.flink.table.api.bridge.internal.AbstractStreamTableEnvironmentImpl.toStreamInternal(AbstractStreamTableEnvironmentImpl.java:219)
    at org.apache.flink.table.api.bridge.scala.internal.StreamTableEnvironmentImpl.toChangelogStream(StreamTableEnvironmentImpl.scala:160)
    at org.example.OverAggregateBug$.main(OverAggregateBug.scala:39)
    at org.example.OverAggregateBug.main(OverAggregateBug.scala)
Caused by: org.apache.flink.table.api.ValidationException: Over Agg: The window rank function without order by. please re-check the over window statement.
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$2(FlinkLogicalOverAggregate.scala:95)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$2$adapted(FlinkLogicalOverAggregate.scala:92)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$1(FlinkLogicalOverAggregate.scala:92)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.$anonfun$convert$1$adapted(FlinkLogicalOverAggregate.scala:89)
    at scala.collection.Iterator.foreach(Iterator.scala:943)
    at scala.collection.Iterator.foreach$(Iterator.scala:943)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
    at scala.collection.IterableLike.foreach(IterableLike.scala:74)
    at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
    at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
    at org.apache.flink.table.planner.plan.nodes.logical.FlinkLogicalOverAggregateConverter.convert(FlinkLogicalOverAggregate.scala:89)
    at org.apache.calcite.rel.convert.ConverterRule.onMatch(ConverterRule.java:167)
    at org.apache.calcite.plan.volcano.VolcanoRuleCall.onMatch(VolcanoRuleCall.java:229)
    ... 27 more {code}
 

 ",,csq,godfrey,lincoln.86xy,martijnvisser,qingyue,rohankrao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Mar 30 02:00:47 UTC 2023,,,,,,,,,,"0|z1g2l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 10:07;martijnvisser;[~godfrey] [~lincoln.86xy] Any thoughts on this one?;;;","22/Mar/23 03:42;qingyue;Hi [~rohankrao] , thanks for reporting this issue.

Actually, the order by field SRC_NO is a constant and is folded during query rewrite. See [LogicalWindow.java#L371|https://github.com/apache/flink/blob/268fc1a46f8af171c7102229a010af71c56623d0/flink-table/flink-table-planner/src/main/java/org/apache/calcite/rel/logical/LogicalWindow.java#L371], when applying Calcite's ProjectToWindowRule.
{code:java}
LogicalProject(inputs=[0..2])
+- LogicalFilter(condition=[<=($2, 1)])
   +- LogicalProject(inputs=[0..1], exprs=[[ROW_NUMBER() OVER (PARTITION BY $0 ORDER BY 2022-01-01 NULLS FIRST)]])
      +- LogicalTableScan(table=[[default_catalog, default_database, temp_table]]) {code}
From the perspective of SQL semantics, using a constant as the order by key for row_number has no meaning, because the constant will not change the sorting result of row_number, and each row will get the same rank. As a current workaround, please try to specify another field to ensure that row_number is sorted in the correct order.

While I tested the query against MySQL, PostgreSQL, SQLServer, Spark, and Hive. SQLServer will throw an exception that ""Windowed functions and NEXT VALUE FOR functions do not support constants as ORDER BY clause expressions.""; the rest do allow this to happen, and just output the first inserted row. Do you think we need to align this behavior? Or at least throwing a more meaningful error. cc [~godfreyhe]  [~lincoln.86xy] 

 

 ;;;","22/Mar/23 06:01;qingyue;I rethink it, from the streaming semantics the result might be non-deterministic if supporting order by constants. So I suggest throwing a meaningful error to indicate users not to use constants as the order by key. WDYT? [~godfrey] [~lincoln.86xy] ;;;","22/Mar/23 12:23;lincoln.86xy;[~qingyue] can you verify the behavior of this case under flink batch? Personally I prefer to keep a unified behavior on streaming and batch, non-determinism should not be the only reason to reject the query, because similar proctime based computations on streaming are also mostly non-deterministic, WDYT?;;;","22/Mar/23 13:05;qingyue;The current behavior under the Flink batch is the same as under streaming mode since this rewrite rule is applied during the LogicalWindow creation. 

My concern mainly comes from the implementation aspect. The reason that caused this problem is the use of constant folding optimization when creating LogicalWindow, which leads to the orderByKey being empty when passed to FlinkLogicalOverAggregateConverter.

There are two possible solutions. The first one is to remove the constant folding optimization or add some judgment here, such as giving up optimization when orderByKey becomes empty after optimization. The second one is to remove the check of orderByKey in FlinkLogicalOverAggregateConverter, but then the problem becomes how to distinguish between order by constants and no order by clause. ;;;","23/Mar/23 03:01;lincoln.86xy;[~qingyue] If there is a large cost on implementation, then I would prefer to optimize the current error message to prompt user for a clearer indication. [~godfreyhe] WDYT?;;;","29/Mar/23 10:12;godfrey;[~lincoln.86xy] [~qingyue] I prefer to just improve the error message in FlinkLogicalOverAggregateConverter, and it can be: The window rank function requires the order by with variable column.;;;","29/Mar/23 10:23;qingyue;I agree with the error msg improvement, and I'd like to do this task. Cc [~lincoln.86xy] [~godfrey] ;;;","29/Mar/23 10:25;lincoln.86xy;[~qingyue] thanks for driving this! assigned to you.;;;","30/Mar/23 02:00;lincoln.86xy;fixed in master: eeb446c0ed07c38175efdadf7e2e21702ff02b70
release-1.17: 7cf6150f1b7660198fb03aea31d13030a01744d5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid setting private tokens to AM container context when kerberos delegation token fetch is disabled,FLINK-31162,13525502,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,vsowrirajan,vsowrirajan,vsowrirajan,21/Feb/23 06:20,23/Feb/23 12:12,13/Jul/23 08:29,23/Feb/23 12:12,1.16.1,,,,,,1.16.2,,,,,,,,Deployment / YARN,,,,0,pull-request-available,,,"In our internal env, we have enabled [Consistent Reads from HDFS Observer NameNode|https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/ObserverNameNode.html]. With this, some of the _ObserverReadProxyProvider_ implementation clone the delegation token for HA service and mark those tokens private so that they won't be accessible through _ugi.getCredentials()._

But Flink internally uses _currUsr.getTokens()_ [here|https://github.com/apache/flink/blob/release-1.16.1/flink-yarn/src/main/java/org/apache/flink/yarn/Utils.java#L222] to get the current user credentials tokens to be set in AM context for submitting the YARN app to RM.

This fails with the following error:
{code:java}
Unable to add the application to the delegation token renewer.
java.io.IOException: Failed to renew token: Kind: HDFS_DELEGATION_TOKEN, Service: test01-ha4.abc:9000, Ident: (HDFS_DELEGATION_TOKEN token 151335106 for john)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:495)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$900(DelegationTokenRenewer.java:79)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:939)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:916)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category WRITE is not supported in state standby. Visit https://s.apache.org/sbnn-error
at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:108)
at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:2044)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1451)
at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewDelegationToken(FSNamesystem.java:5348)
at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renewDelegationToken(NameNodeRpcServer.java:733)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.renewDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:1056)
at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:525)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:495)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1038)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1003)
at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:931)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1905)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2856)

at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1499)
at org.apache.hadoop.ipc.Client.call(Client.java:1445)
at org.apache.hadoop.ipc.Client.call(Client.java:1342)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118)
at com.sun.proxy.$Proxy87.renewDelegationToken(Unknown Source)
at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.renewDelegationToken(ClientNamenodeProtocolTranslatorPB.java:986)
at sun.reflect.GeneratedMethodAccessor37.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
at com.sun.proxy.$Proxy88.renewDelegationToken(Unknown Source)
at org.apache.hadoop.hdfs.DFSClient$Renewer.renew(DFSClient.java:761)
at org.apache.hadoop.security.token.Token.renew(Token.java:466)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:629)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:626)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:422)
at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1905)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.renewToken(DelegationTokenRenewer.java:625)
at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:481)
... 6 more
{code}
Based on the [code comment here in HAUtilClient.java|https://github.com/apache/hadoop/blob/trunk/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/HAUtilClient.java#L128], it seems like the user credentials should be obtained using _ugi.getCredentials()_ instead of {_}ugi.getTokens(){_}. Also Spark seems to use _ugi.getCredentials()_ [here|https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L348] to set the credentials obtained to AM.",,gaborgsomogyi,hexiaoqiao,martijnvisser,vsowrirajan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 12:12:28 UTC 2023,,,,,,,,,,"0|z1g2go:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 12:12;martijnvisser;Fixed in release-1.16: d5c0944b8eacdb5d842c2aa443621442f6684244;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Slow scheduling on large-scale batch jobs ,FLINK-31144,13525451,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,jto,jto,20/Feb/23 16:59,03/Apr/23 09:02,13/Jul/23 08:29,10/Mar/23 09:11,1.15.3,1.16.1,1.17.0,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"When executing a complex job graph at high parallelism `DefaultPreferredLocationsRetriever.getPreferredLocationsBasedOnInputs` can get slow and cause long pauses where the JobManager becomes unresponsive and all the taskmanagers just wait. I've attached a VisualVM snapshot to illustrate the problem.[^flink-1.17-snapshot-1676473798013.nps]

At Spotify we have complex jobs where this issue can cause batch ""pause"" of 40+ minutes and make the overall execution 30% slower or more.
More importantly this prevent us from running said jobs on larger cluster as adding resources to the cluster worsen the issue.

We have successfully tested a modified Flink version where `DefaultPreferredLocationsRetriever.getPreferredLocationsBasedOnInputs` was completely commented and simply returns an empty collection and confirmed it solves the issue.

In the same spirit as a recent change ([https://github.com/apache/flink/blob/43f419d0eccba86ecc8040fa6f521148f1e358ff/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultPreferredLocationsRetriever.java#L98-L102)] there could be a mechanism in place to detect when Flink run into this specific issue and just skip the call to `getInputLocationFutures`  [https://github.com/apache/flink/blob/43f419d0eccba86ecc8040fa6f521148f1e358ff/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultPreferredLocationsRetriever.java#L105-L108.]

I'm not familiar enough with the internals of Flink to propose a more advanced fix, however it seems like a configurable threshold on the number of consumer vertices above which the preferred location is not computed would do. If this  solution is good enough, I'd be happy to submit a PR.",,freeke,huwh,jto,JunRuiLi,luoyuxia,martijnvisser,Weijie Guo,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/23 13:22;martijnvisser;Screenshot 2023-03-13 at 14.22.27.png;https://issues.apache.org/jira/secure/attachment/13056281/Screenshot+2023-03-13+at+14.22.27.png","20/Feb/23 16:54;jto;flink-1.17-snapshot-1676473798013.nps;https://issues.apache.org/jira/secure/attachment/13055642/flink-1.17-snapshot-1676473798013.nps","21/Feb/23 09:29;jto;image-2023-02-21-10-29-49-388.png;https://issues.apache.org/jira/secure/attachment/13055672/image-2023-02-21-10-29-49-388.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 24 09:28:55 UTC 2023,,,,,,,,,,"0|z1g25c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 07:08;huwh;Thanks for reporting this issue. [~jto] 

DefaultPreferredLocationsRetriever.getPreferredLocationsBasedOnInputs will go over all the input locations which may cost much time when the topology is complex. Maybe make PreferredLocations configurable is an option to solve this problem.

I have some more questions about this issue:

1. Is the slow scheduling or the sheduled result of lolcation preferred make your job slow?
2. ""we have complex jobs where this issue can cause batch ""pause"" of 40+ minutes""      What does ""pause"" meaning? Is the getPreferredLocationsBasedOnInputs take more than 40+ minutes? Could you provide the topology of the complex job.;;;","21/Feb/23 09:30;jto;Hi [~huwh],


Thank you for the quick reply :)


{quote}when the topology is complex.
{quote}

Indeed. For the issue to be noticeable, the jobgraph has to be fairly complex, feature all-to-all distributions and execute with a high parallelism.

 
{quote}1. Is the slow scheduling or the scheduled result of location preferred make your job slow?
{quote}

Yes it very much does. We have a job that takes ~2h30 (after many many tweaks to get the best possible perf.). It's impossible to get it to run in less time because adding more taskmanagers make the scheduling slow and overall the execution gets longer. Removing preferred location makes it possible to run it in less that 2h (We're aiming at ~1h45min).

 
{quote}2. ""we have complex jobs where this issue can cause batch ""pause"" of 40+ minutes""  What does ""pause"" meaning? Is the getPreferredLocationsBasedOnInputs take more than 40+ minutes?
{quote}

By ""pause"" I mean that at the beginning of the execution, the taskmanagers will wait for the JobManager for ~40min and then will start processing. With Flink 1.17 and no preferred location, the ""pause"" is down to ~5min.

I should also mention the JM is very unresponsive and the web console struggles the show anything. 


{quote}Could you provide the topology of the complex job.
{quote}

I can but not sure what format to use. The graph is quite big and a simple screenshot is unreadable: !image-2023-02-21-10-29-49-388.png!

I can maybe share the archived execution json file (~500Mb) if that's helpful ?

 ;;;","22/Feb/23 03:33;huwh;{quote}however it seems like a configurable threshold on the number of consumer vertices above which the preferred location is not computed would do
{quote}
IMO, It's too hard to decide the threshold for user. Introduce a config to disable input location preferences is more understandable.

[~zhuzh] WDYT;;;","22/Feb/23 04:24;zhuzh;Thanks for reporting this issue! [~jto]
What's the the parallelism of the job vertices? Unfortunately I cannot tell it from the attached image.

IIUC, the problem is caused by vertices with large parallelism and ALL-to-ALL edges. If so, changing the `if` condition of [this logic|https://github.com/apache/flink/blob/43f419d0eccba86ecc8040fa6f521148f1e358ff/flink-runtime/src/main/java/org/apache/flink/runtime/scheduler/DefaultPreferredLocationsRetriever.java#L98-L102)] to {{consumedPartitionGroup.size() > MAX_DISTINCT_CONSUMERS_TO_CONSIDER}} may help. Would you give a try?;;;","22/Feb/23 17:13;jto;{quote}IMO, It's too hard to decide the threshold for user. Introduce a config to disable input location preferences is more understandable
{quote}
Yes. My suggestion was to have a reasonable default value that would work for almost everyone but still make it configurable for advanced edge cases under [advanced-scheduling-options.|https://nightlies.apache.org/flink/flink-docs-master/docs/deployment/config/#advanced-scheduling-options]. A simple enable / disabled config also work for me.


Hey [~zhuzh]!

Thank you for comment. The parallelism on all the vertices is 4000.
I did not have the time to test the change in the condition but I think I can try it tomorrow and I'll keep you posted :);;;","23/Feb/23 07:06;zhuzh;I suspect the slowness is caused by the N^2 complexity to compute the input locations when there are N upstream task and N downstream tasks.
If so, as long as N is not too large, e.g. not larger than MAX_DISTINCT_CONSUMERS_TO_CONSIDER=8, the cost of input location computation should be acceptable. Also, if there are too many distinct consumers, input locality would make none sense.

As [~huwh] mentioned, it's hard for users to decides a proper threshold for each job. It's also inconvenient if users had to decide whether to enable input locality or not. Therefore, I prefer to let Flink decide it automatically for users, like the proposed change above.
;;;","23/Feb/23 07:19;huwh;[~zhuzh] That sounds good to me.;;;","27/Feb/23 08:33;zhuzh;Sorry I made a mistake! [~jto]
What I meant is to change the check to {{consumedPartitionGroup.getConsumerVertexGroup().size() > MAX_DISTINCT_CONSUMERS_TO_CONSIDER}}, like [this|https://github.com/zhuzhurk/flink/commit/860ec8855de5f36ee2758336a5eba7abac3f215a].
;;;","28/Feb/23 13:50;jto;Hey there!

I finally found some time to test the fix proposed by [~zhuzh] and it does solve the problem :);;;","03/Mar/23 06:16;JunRuiLi;Hi, [~zhuzh]. I'd like to fix this issue, could you help to assign this ticket to me? ;;;","03/Mar/23 06:23;zhuzh;Thanks for volunteering! [~JunRuiLi]
I have assigned you the ticket.;;;","03/Mar/23 09:21;jto;Awesome! Thanks for volunteering [~JunRuiLi] 

I'm happy to help if you need something. Just let me know :);;;","10/Mar/23 09:11;zhuzh;master:
6d79f6fc52bcd338d0528d19cd1f7289d8e339da

release-1.17:
5b90c075bf9b987de2fa4a02e0cf63602152eba1;;;","13/Mar/23 13:23;martijnvisser;[~zhuzh] Not 100% sure but I think this PR has resulted in a drastic performance improvement in startScheduling.STREAMING, see http://codespeed.dak8s.net:8000/timeline/#/?exe=8&ben=startScheduling.STREAMING&extr=on&quarts=on&equid=off&env=2&revs=200;;;","13/Mar/23 13:48;huwh;[~martijnvisser], yes, this change will improve the performance both of Streaming and Batch jobs with high parallelism. The benchmark result is same with my local.;;;","14/Mar/23 05:41;zhuzh;IIRC, the parallelism of that benchmark job (startScheduling.STREAMING) is 4000, which is relatively large. So the scheduling time can be significantly reduced with this improvement.;;;","24/Mar/23 09:28;jto;Hey [~martijnvisser]  this is great! Thanks for sharing :) 
Spotify is in a good position to identify inefficiencies and possible optimizations on high parallelism jobs.
I'm hopeful we can contribute to improve he performances further in the future!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some queries lead to abrupt sql client close,FLINK-31142,13525444,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,20/Feb/23 14:46,26/Feb/23 14:00,13/Jul/23 08:29,26/Feb/23 14:00,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / Client,,,,0,pull-request-available,,,"Although the behavior has been changed in 1.17.0, I'm not sure whether it is a blocker or not, since in both cases it is invalid query.
I put it to blocker just because of regression.

The difference in the behavior is that before 1.17.0
a query like 
{code:sql}
select /* multiline comment;
{code}
fails to execute and sql client prompts to submit another query.

In 1.17.0 it  shuts down the session failing with 
{noformat}
Exception in thread ""main"" org.apache.flink.table.client.SqlClientException: Could not read from command line.
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:205)
	at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:168)
	at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:113)
	at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:169)
	at org.apache.flink.table.client.SqlClient.start(SqlClient.java:118)
	at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:228)
	at org.apache.flink.table.client.SqlClient.main(SqlClient.java:179)
Caused by: org.apache.flink.sql.parser.impl.TokenMgrError: Lexical error at line 1, column 29.  Encountered: <EOF> after : """"
	at org.apache.flink.sql.parser.impl.FlinkSqlParserImplTokenManager.getNextToken(FlinkSqlParserImplTokenManager.java:26752)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl$TokenIterator.scan(SqlCommandParserImpl.java:89)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl$TokenIterator.next(SqlCommandParserImpl.java:81)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl.checkIncompleteStatement(SqlCommandParserImpl.java:141)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl.getCommand(SqlCommandParserImpl.java:111)
	at org.apache.flink.table.client.cli.parser.SqlCommandParserImpl.parseStatement(SqlCommandParserImpl.java:52)
	at org.apache.flink.table.client.cli.parser.SqlMultiLineParser.parse(SqlMultiLineParser.java:82)
	at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2964)
	at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3778)
	at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:679)
	at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:183)
	... 6 more

Shutting down the session...
done.

{noformat}

",,mapohl,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 22:51:05 UTC 2023,,,,,,,,,,"0|z1g23s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"23/Feb/23 10:50;mapohl;[~fsk119] can you guide us on that one? It sounds like a regression.;;;","23/Feb/23 12:54;Sergey Nuyanzin;It seems that before this change (https://issues.apache.org/jira/browse/FLINK-29945) all the parser exceptions are handled via {{org.apache.calcite.sql.parser.SqlAbstractParserImpl#normalizeException}} which under the hood catches {{{}TokenMgrError{}}}. Now it works directly with {{TokenIterator}} that's why extra catch is required.;;;","24/Feb/23 22:51;Sergey Nuyanzin;Merged to master: 464ded1c2a0497255b70f711167c3b7ae52ea0f7
Merged to 1.17     : 05f207cab637f46709feb9986b137456ecbe5b7a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client doesn't print results for SHOW CREATE TABLE/DESC in hive dialect,FLINK-31137,13525415,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,20/Feb/23 10:23,21/Feb/23 12:03,13/Jul/23 08:29,21/Feb/23 12:03,1.17.0,,,,,,1.17.0,1.18.0,,,,,,,Connectors / Hive,,,,0,pull-request-available,,,,,fsk119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30936,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 07:06:40 UTC 2023,,,,,,,,,,"0|z1g1xc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"21/Feb/23 07:06;fsk119;Merged into release-1.17: 3ae7b1f3d85db71f0950f1166d60be76720b49f5
Merged into master: 0508d82ae9377bf5674c409c74ffe36f1887cdab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL Client Gateway mode should not read read execution config,FLINK-31136,13525414,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,20/Feb/23 10:21,23/Feb/23 03:24,13/Jul/23 08:29,23/Feb/23 03:24,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / Client,,,,0,pull-request-available,,,,,fsk119,Paul Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30936,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 02:47:40 UTC 2023,,,,,,,,,,"0|z1g1x4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"22/Feb/23 02:47;fsk119;Merged into release-1.17: db15e1e2014f02cbeb58d8a4fee1befdbd5a3ac8
Merged into master: dec3ba078decbdc212a6ea16ad8728aa7409d9c3;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PartiallyFinishedSourcesITCase hangs if a checkpoint fails,FLINK-31133,13525410,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,roman,mapohl,mapohl,20/Feb/23 10:08,08/Mar/23 17:55,13/Jul/23 08:29,03/Mar/23 20:13,1.15.3,1.16.1,1.17.1,1.18.0,,,1.15.4,1.16.2,1.17.1,1.18.0,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b

This build ran into a timeout. Based on the stacktraces reported, it was either caused by [SnapshotMigrationTestBase.restoreAndExecute|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=13475]:
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007f23d800b800 nid=0x60cdd waiting on condition [0x00007f23e1c0d000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.test.checkpointing.utils.SnapshotMigrationTestBase.restoreAndExecute(SnapshotMigrationTestBase.java:382)
	at org.apache.flink.test.migration.TypeSerializerSnapshotMigrationITCase.testSnapshot(TypeSerializerSnapshotMigrationITCase.java:172)
	at sun.reflect.GeneratedMethodAccessor47.invoke(Unknown Source)
[...]
{code}

or [PartiallyFinishedSourcesITCase.test|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10401]:
{code}
2023-02-20T07:13:05.6084711Z ""main"" #1 prio=5 os_prio=0 tid=0x00007fd35c00b800 nid=0x8c8a waiting on condition [0x00007fd363d0f000]
2023-02-20T07:13:05.6085149Z    java.lang.Thread.State: TIMED_WAITING (sleeping)
2023-02-20T07:13:05.6085487Z 	at java.lang.Thread.sleep(Native Method)
2023-02-20T07:13:05.6085925Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:145)
2023-02-20T07:13:05.6086512Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:138)
2023-02-20T07:13:05.6087103Z 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitForSubtasksToFinish(CommonTestUtils.java:291)
2023-02-20T07:13:05.6087730Z 	at org.apache.flink.runtime.operators.lifecycle.TestJobExecutor.waitForSubtasksToFinish(TestJobExecutor.java:226)
2023-02-20T07:13:05.6088410Z 	at org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase.test(PartiallyFinishedSourcesITCase.java:138)
2023-02-20T07:13:05.6088957Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}

Still, it sounds odd: Based on a code analysis it's quite unlikely that those two caused the issue. The former one has a 5 min timeout (see related code in [SnapshotMigrationTestBase:382|https://github.com/apache/flink/blob/release-1.15/flink-tests/src/test/java/org/apache/flink/test/checkpointing/utils/SnapshotMigrationTestBase.java#L382]). For the other one, we found it being not responsible in the past when some other concurrent test caused the issue (see FLINK-30261).

An investigation on where we lose the time for the timeout revealed that {{AdaptiveSchedulerITCase}} took 2980s to finish (see [build logs|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46299&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=5265]).
{code}
2023-02-20T03:43:55.4546050Z Feb 20 03:43:55 [ERROR] Picked up JAVA_TOOL_OPTIONS: -XX:+HeapDumpOnOutOfMemoryError
2023-02-20T03:43:58.0448506Z Feb 20 03:43:58 [INFO] Running org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
2023-02-20T04:33:38.6824634Z Feb 20 04:33:38 [INFO] Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2,980.445 s - in org.apache.flink.test.scheduling.AdaptiveSchedulerITCase
{code}",,csq,dmvk,liyu,mapohl,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 20:13:48 UTC 2023,,,,,,,,,,"0|z1g1w8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 10:14;mapohl;[~chesnay] could you have a look at that one if you find time? I'm worried that there's an issue that's also present in newer versions. But I guess, it's hard to investigate if we don't have logs.;;;","20/Feb/23 12:54;mapohl;[~chesnay] discovered multiple checkpoint failure due to exceeding the tolerable failure threshold for creating a checkpoint (which seems to be 10min):
{code:java}
04:24:18,587 [    Checkpoint Timer] WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 4 for job af8411cc8af5e2485beb4466ffd452a3. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint expired before completing.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:2143) [flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
04:24:18,588 [flink-akka.actor.default-dispatcher-8] INFO  org.apache.flink.runtime.scheduler.adaptive.AdaptiveScheduler [] - Restarting job.
org.apache.flink.util.FlinkRuntimeException: Exceeded checkpoint tolerable failure threshold.
        at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.checkFailureAgainstCounter(CheckpointFailureManager.java:206) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleJobLevelCheckpointException(CheckpointFailureManager.java:169) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointFailureManager.handleCheckpointException(CheckpointFailureManager.java:122) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2082) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.abortPendingCheckpoint(CheckpointCoordinator.java:2061) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.access$600(CheckpointCoordinator.java:98) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator$CheckpointCanceller.run(CheckpointCoordinator.java:2143) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) ~[?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_292]
[...]
{code}
[~roman]  can you help with that one?;;;","21/Feb/23 16:25;roman;I think the issue is actually PartiallyFinishedSourcesITCase.

It starts at 3:40
{code:java}
03:40:55,702 [                main] INFO  org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase [] -
================================================================================
Test test[complex graph ALL_SUBTASKS, failover: true, strategy: full](org.apache.flink.runtime.operators.lifecycle.PartiallyFinishedSourcesITCase) is running.
{code}
then a checkpoint fails because of a timeout:
{code:java}
03:41:10,775 [ChangelogRetryScheduler-1] INFO  org.apache.flink.changelog.fs.RetryingExecutor               [] - failed with 3 attempts: Attempt 3 timed out after 1000ms
03:41:10,777 [AsyncOperations-thread-1] INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - transform-2-keyed (4/4)#0 - asynchronous part of checkpoint 2 could not be completed.
java.util.concurrent.CompletionException: java.io.IOException: java.util.concurrent.TimeoutException: Attempt 3 timed out after 1000ms
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_292]
        at org.apache.flink.changelog.fs.FsStateChangelogWriter$UploadCompletionListener.onFailure(FsStateChangelogWriter.java:383) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.changelog.fs.FsStateChangelogWriter.lambda$null$0(FsStateChangelogWriter.java:223) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.ArrayList.removeIf(ArrayList.java:1415) ~[?:1.8.0_292]
        at org.apache.flink.changelog.fs.FsStateChangelogWriter.lambda$handleUploadFailure$4(FsStateChangelogWriter.java:222) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsWhenDefaultActionUnavailable(MailboxProcessor.java:338) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:324) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:201) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:807) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:756) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.io.IOException: java.util.concurrent.TimeoutException: Attempt 3 timed out after 1000ms
        at org.apache.flink.changelog.fs.FsStateChangelogWriter$UploadCompletionListener.onFailure(FsStateChangelogWriter.java:377) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        ... 15 more
Caused by: java.util.concurrent.TimeoutException: Attempt 3 timed out after 1000ms
        at org.apache.flink.changelog.fs.RetryingExecutor$RetriableActionAttempt.fmtError(RetryingExecutor.java:285) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.changelog.fs.RetryingExecutor$RetriableActionAttempt.lambda$scheduleTimeout$1(RetryingExecutor.java:280) ~[flink-dstl-dfs-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[?:1.8.0_292]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) ~[?:1.8.0_292]
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) ~[?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
{code}
 

After which it runs normally, ~1.4K checkpoints succeed.

At 6:56, it finally reches no space left on device:
{code:java}
06:56:53,713 [jobmanager-io-thread-1] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Completed checkpoint 1396 for job 1e2e4e86643f8249324da01fe5f8a04a (34948548049 bytes, checkpointDuration=6869 ms, finalizationTime=12 ms).
06:56:53,715 [    Checkpoint Timer] INFO  org.apache.flink.runtime.checkpoint.CheckpointCoordinator    [] - Triggering checkpoint 1397 (type=CheckpointType{name='Checkpoint', sharingFilesStrategy=FORWARD_BACKWARD}) @ 1676876213715 for job 1e2e4e86643f8249324da01fe5f8a04a.
...
06:57:02,402 [AsyncOperations-thread-1] INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - MultipleInputOperator (1/4)#1 - asynchronous part of checkpoint 1397 could not be completed.
java.util.concurrent.ExecutionException: java.io.IOException: No space left on device
        at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357) ~[?:1.8.0_292]
        at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908) ~[?:1.8.0_292]
        at org.apache.flink.streaming.api.operators.OperatorSnapshotFinalizer.<init>(OperatorSnapshotFinalizer.java:66) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.finalizeNonFinishedSnapshots(AsyncCheckpointRunnable.java:191) ~[flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.run(AsyncCheckpointRunnable.java:124) [flink-streaming-java-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_292]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_292]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.io.IOException: No space left on device
        at java.io.FileOutputStream.writeBytes(Native Method) ~[?:1.8.0_292]
        at java.io.FileOutputStream.write(FileOutputStream.java:326) ~[?:1.8.0_292]
        at org.apache.flink.core.fs.local.LocalDataOutputStream.write(LocalDataOutputStream.java:68) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.core.fs.FSDataOutputStreamWrapper.write(FSDataOutputStreamWrapper.java:65) ~[flink-core-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.state.filesystem.FsCheckpointStreamFactory$FsCheckpointStateOutputStream.write(FsCheckpointStreamFactory.java:296) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAP  SHOT]
        at java.io.DataOutputStream.write(DataOutputStream.java:107) ~[?:1.8.0_292]
        at java.io.FilterOutputStream.write(FilterOutputStream.java:97) ~[?:1.8.0_292]
        at org.apache.flink.runtime.io.network.buffer.NetworkBuffer.getBytes(NetworkBuffer.java:397) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.buffer.AbstractUnpooledSlicedByteBuf.getBytes(AbstractUnpooledSlicedByteBuf.java:392) ~[flink-shaded-netty-4.1.70.Final-15.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.buffer.SlicedByteBuf.getBytes(SlicedByteBuf.java:26) ~[flink-shaded-netty-4.1.70.Final-15.0.jar:?]
        at org.apache.flink.shaded.netty4.io.netty.buffer.ReadOnlyByteBuf.getBytes(ReadOnlyByteBuf.java:264) ~[flink-shaded-netty-4.1.70.Final-15.0.jar:?]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateSerializerImpl.writeData(ChannelStateSerializer.java:164) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.lambda$write$2(ChannelStateCheckpointWriter.java:171) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.runWithChecks(ChannelStateCheckpointWriter.java:295) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.write(ChannelStateCheckpointWriter.java:165) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateCheckpointWriter.writeInput(ChannelStateCheckpointWriter.java:138) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$write$0(ChannelStateWriteRequest.java:63) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequest.lambda$buildWriteRequest$2(ChannelStateWriteRequest.java:92) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.CheckpointInProgressRequest.execute(ChannelStateWriteRequest.java:211) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatchInternal(ChannelStateWriteRequestDispatcherImpl.java:85) ~[flink-runtime-1.15-SNAPSHOT.jar:1  .15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestDispatcherImpl.dispatch(ChannelStateWriteRequestDispatcherImpl.java:62) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAP  SHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.loop(ChannelStateWriteRequestExecutorImpl.java:96) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.channel.ChannelStateWriteRequestExecutorImpl.run(ChannelStateWriteRequestExecutorImpl.java:75) ~[flink-runtime-1.15-SNAPSHOT.jar:1.15-SNAPSHOT]
        ... 1 more
{code}
 

Some other tests failed as a result with serialization failures or ""no such file""

 

I'm going to change the priority to Major because it's a pure test issue.;;;","25/Feb/23 22:34;roman;There are two issues in case of a checkpoint failure:
 # FAIL command might be dispatched to the source task that's already finished execution
 # waiting for the failover times out, but it then waits indefinitely to obtain job status result

The issue affects only 1.15 because in later versions, state upload timeout and nr. of attempts were increased in FLINK-27169.

I've created a [PR|https://github.com/apache/flink/pull/22022] to address (1) and (2) and reopened FLINK-27169 to backport increased timeouts/attempts to 1.15.;;;","02/Mar/23 06:58;liyu;The 1.15.4 version is about to release with [RC under vote|https://lists.apache.org/thread/4463cypc257l7j9rj2pycofbsdbbjx59]. Please check and confirm whether this issue could still make into 1.15.4 and move it out if not. Thanks.;;;","02/Mar/23 07:44;roman;Thanks [~liyu] , you are right, it will likely not make it into 1.15.4. I'll updated the version.;;;","02/Mar/23 08:23;liyu;Thanks for the quick action [~roman];;;","03/Mar/23 20:13;roman;Fix merged as 

1.15 51660f840cfc505b9b9cb72530fde7f9f8a4dee2
1.16 cf04b2c08fa04091845bd310990497129c3bcbe8
1.17 6e7703738cdefed17277ea86d2c9dc25393eceac
master 4aacff572a9e3996c5dee9273638831e4040c767;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compact without setting parallelism does not follow the configured sink parallelism for HiveTableSink,FLINK-31132,13525408,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,fsk119,fsk119,20/Feb/23 10:03,22/Feb/23 14:45,13/Jul/23 08:29,22/Feb/23 14:45,1.17.0,,,,,,1.17.0,,,,,,,,Connectors / Hive,Table SQL / Planner,,,0,pull-request-available,,,"If the parallelism of compact operator was not set, it should use the sink parallelism and disable parallelism inference when using adaptive batch scheduler to avoid take much time to finish compaction.",,fsk119,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30951,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 10:18:26 UTC 2023,,,,,,,,,,"0|z1g1vs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 11:37;Weijie Guo;This may be related to the change of batch default scheduler.;;;","22/Feb/23 10:18;Weijie Guo;master(1.18) via 2e91543836d667a0b367688bb5ce290c3164479c.
release-1.17 via 2f86dcb98e065503e12ba121813d643ba976d041.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The INITIALIZING of ExecutionState is missed in the state_machine doc,FLINK-31131,13525404,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Wencong Liu,fanrui,fanrui,20/Feb/23 09:40,24/Apr/23 02:07,13/Jul/23 08:29,24/Apr/23 02:07,1.16.0,1.17.0,,,,,1.16.2,1.17.1,1.18.0,,,,,,Documentation,,,,0,pull-request-available,,,"[https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/internals/job_scheduling/#jobmanager-data-structures]

 

The INITIALIZING of ExecutionState is missed in the state_machine doc, it should be between DEPLOYING and RUNNING.

 

!image-2023-02-20-17-39-22-557.png!

 ",,fanrui,Weijie Guo,Wencong Liu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/23 09:39;fanrui;image-2023-02-20-17-39-22-557.png;https://issues.apache.org/jira/secure/attachment/13055627/image-2023-02-20-17-39-22-557.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Apr 23 07:45:03 UTC 2023,,,,,,,,,,"0|z1g1uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Apr/23 08:36;Wencong Liu;[~fanrui] Are you working on this? If you are busy on other issues, I could take it. WDYT?;;;","20/Apr/23 10:12;fanrui;Hi [~Wencong Liu] , thanks for your reply.

I see the svg is drawn by Inkscape[1], I'm not sure whether the flink community has a specification for making svg, so I prefer use Inkscape. I tried to use Inkscape last weekend, however, I didn't finish it due to I'm very busy this week.

If you are interested in this issue and Inkscape, I can assign it to you. WDYT?

 

[1] https://github.com/apache/flink/blob/d2e4b74e4a291f36e0771b2c6a7ded76d393f235/docs/static/fig/state_machine.svg?short_path=8d0f570#L21;;;","20/Apr/23 10:15;Wencong Liu;[~fanrui]  Sure, I'll be happy to finish it.;;;","23/Apr/23 07:25;fanrui;Hi [~Wencong Liu] , thanks for your contribution. Would you mind backport it to 1.16 and 1.17?

 

Merged master commit: 5c4db55d306a4c65175f7cb4250d02729542901c

Merged 1.17-release commit: ca27fb9f05ebc250148c405003d4c70ca1d7a5e4

Merged 1.16-release commit: 226ec59837db39d1142934d79df9c59c8c3c8163;;;","23/Apr/23 07:45;Wencong Liu;OK. [~fanrui]. Another two pull requests have been opened.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException occurred in StringFunctionsITCase.test,FLINK-31120,13525178,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,csq,mapohl,mapohl,17/Feb/23 10:09,23/Feb/23 07:24,13/Jul/23 08:29,23/Feb/23 02:43,1.17.0,,,,,,1.16.2,1.17.0,1.18.0,,,,,,Table SQL / Runtime,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46255&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12334

{code}
Feb 17 04:51:25 [ERROR] Tests run: 4, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.725 s <<< FAILURE! - in org.apache.flink.table.planner.functions.StringFunctionsITCase
Feb 17 04:51:25 [ERROR] org.apache.flink.table.planner.functions.StringFunctionsITCase.test(TestCase)[4] Time elapsed: 4.367 s <<< ERROR!
Feb 17 04:51:25 org.apache.flink.table.api.TableException: Failed to execute sql
Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:974)
Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1422)
Feb 17 04:51:25 at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:476)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$ResultTestItem.test(BuiltInFunctionTestBase.java:354)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestSetSpec.lambda$getTestCase$4(BuiltInFunctionTestBase.java:320)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase$TestCase.execute(BuiltInFunctionTestBase.java:113)
Feb 17 04:51:25 at org.apache.flink.table.planner.functions.BuiltInFunctionTestBase.test(BuiltInFunctionTestBase.java:93)
Feb 17 04:51:25 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[...]
{code}",,aitozi,csq,leonard,mapohl,samrat007,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 23 07:24:30 UTC 2023,,,,,,,,,,"0|z1g0gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 12:28;Weijie Guo;Sorry, I accidentally touched it.;;;","17/Feb/23 18:14;csq;Maybe it needs concurrent control for access to the static field `collectIterators` in `StreamExecutionEnvironment`. There are four test cases executing when running StringFunctionsITCase in a concurrent execution mode, that has chance  for a thread to add a collectorIterator through `registerCollectIterator` while there is a foreach loop in executeAsync() in another thread.;;;","18/Feb/23 15:19;leonard;[~TsReaper] Would you like to review this PR?;;;","20/Feb/23 09:10;aitozi;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46301&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=0c940707-2659-5648-cbe6-a1ad63045f0a;;;","21/Feb/23 07:22;samrat007;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46347&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a;;;","22/Feb/23 03:24;TsReaper;I've left my review in the github PR. My main concern is that why do we need a static variable. I see [~chesnay] is the author of the related code. Would you please take a look?;;;","22/Feb/23 09:41;mapohl;{{ConstructedAccessFunctionsITCase.testTableApiFlattenStructuredType}} instability due to a {{ConcurrentModificationException}} with a similar stacktrace:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46387&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=11938
{code}
Feb 22 04:26:07 [ERROR] Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 10.108 s <<< FAILURE! - in org.apache.flink.table.planner.functions.ConstructedAccessFunctionsITCase
Feb 22 04:26:07 [ERROR] org.apache.flink.table.planner.functions.ConstructedAccessFunctionsITCase.testTableApiFlattenStructuredType  Time elapsed: 5.107 s  <<< ERROR!
Feb 22 04:26:07 org.apache.flink.table.api.TableException: Failed to execute sql
Feb 22 04:26:07 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:974)
Feb 22 04:26:07 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1422)
Feb 22 04:26:07 	at org.apache.flink.table.api.internal.TableImpl.execute(TableImpl.java:476)
Feb 22 04:26:07 	at org.apache.flink.table.planner.functions.ConstructedAccessFunctionsITCase.testTableApiFlattenStructuredType(ConstructedAccessFunctionsITCase.java:192)
Feb 22 04:26:07 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 22 04:26:07 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 22 04:26:07 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
Feb 22 04:26:07 	at java.lang.reflect.Method.invoke(Method.java:498)
Feb 22 04:26:07 	at org.junit.platform.commons.util.ReflectionUtils.invokeMethod(ReflectionUtils.java:727)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.MethodInvocation.proceed(MethodInvocation.java:60)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$ValidatingInvocation.proceed(InvocationInterceptorChain.java:131)
Feb 22 04:26:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.intercept(TimeoutExtension.java:156)
Feb 22 04:26:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestableMethod(TimeoutExtension.java:147)
Feb 22 04:26:07 	at org.junit.jupiter.engine.extension.TimeoutExtension.interceptTestMethod(TimeoutExtension.java:86)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker$ReflectiveInterceptorCall.lambda$ofVoidMethod$0(InterceptingExecutableInvoker.java:103)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.lambda$invoke$0(InterceptingExecutableInvoker.java:93)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain$InterceptedInvocation.proceed(InvocationInterceptorChain.java:106)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.proceed(InvocationInterceptorChain.java:64)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.chainAndInvoke(InvocationInterceptorChain.java:45)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InvocationInterceptorChain.invoke(InvocationInterceptorChain.java:37)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:92)
Feb 22 04:26:07 	at org.junit.jupiter.engine.execution.InterceptingExecutableInvoker.invoke(InterceptingExecutableInvoker.java:86)
Feb 22 04:26:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.lambda$invokeTestMethod$7(TestMethodTestDescriptor.java:217)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 22 04:26:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.invokeTestMethod(TestMethodTestDescriptor.java:213)
Feb 22 04:26:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:138)
Feb 22 04:26:07 	at org.junit.jupiter.engine.descriptor.TestMethodTestDescriptor.execute(TestMethodTestDescriptor.java:68)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:151)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:141)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:139)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:138)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:95)
Feb 22 04:26:07 	at org.junit.platform.engine.support.hierarchical.ForkJoinPoolHierarchicalTestExecutorService$ExclusiveTask.compute(ForkJoinPoolHierarchicalTestExecutorService.java:185)
Feb 22 04:26:07 	at java.util.concurrent.RecursiveAction.exec(RecursiveAction.java:189)
Feb 22 04:26:07 	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
Feb 22 04:26:07 	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
Feb 22 04:26:07 	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
Feb 22 04:26:07 	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Feb 22 04:26:07 Caused by: java.util.ConcurrentModificationException
Feb 22 04:26:07 	at java.util.ArrayList.forEach(ArrayList.java:1262)
Feb 22 04:26:07 	at org.apache.flink.streaming.api.environment.StreamExecutionEnvironment.executeAsync(StreamExecutionEnvironment.java:2202)
Feb 22 04:26:07 	at org.apache.flink.table.planner.delegation.DefaultExecutor.executeAsync(DefaultExecutor.java:95)
Feb 22 04:26:07 	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeQueryOperation(TableEnvironmentImpl.java:955)
Feb 22 04:26:07 	... 40 more
{code};;;","22/Feb/23 09:46;csq;I agree with [~TsReaper] that the variable does not need to be static since it is accessed only by current environment. Or is there any other consideration?;;;","23/Feb/23 02:44;TsReaper;master: fa6a2aed6136ae59ed14cd01819e8f94867840b7
release-1.17: 7201d0aa1c69ae0a52f07cea38a0545203f67c17
release-1.16: 4ba657ea60452f16d3f6175031f8471b3b7f042f;;;","23/Feb/23 07:24;mapohl;This build failure didn't contain the aforementioned fix, yet:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46433&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12384;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch job fails with IllegalStateException when using adaptive batch scheduler,FLINK-31114,13525139,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,wanglijie,wanglijie,wanglijie,17/Feb/23 05:44,10/May/23 02:25,13/Jul/23 08:29,24/Feb/23 11:39,,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"This is caused by FLINK-30942. Currently, if two job vertices have the same input and the same parallelism(even the parallelism is -1), they will share partitions. However after FLINK-30942, the scheduler may change the job vertices' parallelism before scheduling, resulting in two job vertices having the same parallelism in  compilation phase (in which case will share partitions), but different parallelism in the scheduling phase, and then cause the following exception:

{code:java}
Caused by: java.util.concurrent.CompletionException: java.lang.IllegalStateException: Consumers must have the same max parallelism.
        at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
        at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:975)
        at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
        ... 37 more
Caused by: java.lang.IllegalStateException: Consumers must have the same max parallelism.
        at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
        at org.apache.flink.runtime.executiongraph.IntermediateResult.getConsumersMaxParallelism(IntermediateResult.java:219)
        at org.apache.flink.runtime.executiongraph.Execution.getPartitionMaxParallelism(Execution.java:501)
        at org.apache.flink.runtime.executiongraph.Execution.registerProducedPartitions(Execution.java:472)
        at org.apache.flink.runtime.executiongraph.Execution.registerProducedPartitions(Execution.java:431)
        at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$registerProducedPartitions$5(DefaultExecutionDeployer.java:277)
        at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
        ... 38 more
{code}

Putting the following test into {{AdaptiveBatchSchedulerITCase}} can reproduce the problem：

{code:java}
    @Test
    void testDifferentConsumerParallelism() throws Exception {
        final Configuration configuration = createConfiguration();
        final StreamExecutionEnvironment env =
                StreamExecutionEnvironment.createLocalEnvironment(configuration);
        env.setRuntimeMode(RuntimeExecutionMode.BATCH);
        env.setParallelism(8);

        final DataStream<Long> source1 =
                env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
                        .setParallelism(8)
                        .name(""source1"")
                        .slotSharingGroup(""group1"");

        final DataStream<Long> source2 =
                env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
                        .setParallelism(8)
                        .name(""source2"")
                        .slotSharingGroup(""group2"");

        source1.forward()
                .union(source2)
                .map(new NumberCounter())
                .name(""map1"")
                .slotSharingGroup(""group3"");

        source2.map(new NumberCounter()).name(""map2"").slotSharingGroup(""group4"");

        env.execute();
    }
{code}

",,freeke,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30942,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 24 11:36:56 UTC 2023,,,,,,,,,,"0|z1g088:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"24/Feb/23 11:36;wanglijie;Fixed via 

master: b987ae18d0bc353c631bc54871b0c16be39dbad2

release-1.17: c66ef2540c3cb53f4cf3218ff07f5b440511ad84;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fails with proxy user not supported even when security.kerberos.fetch.delegation-token is set to false,FLINK-31109,13525087,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,vsowrirajan,vsowrirajan,vsowrirajan,16/Feb/23 21:23,27/Feb/23 19:06,13/Jul/23 08:29,27/Feb/23 13:54,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"With
{code:java}
security.kerberos.fetch.delegation-token: false
{code}
and delegation tokens obtained through our internal service which sets both HADOOP_TOKEN_FILE_LOCATION to pick up the DTs and also sets the HADOOP_PROXY_USER which fails with the below error
{code:java}
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/export/home/vsowrira/flink-1.18-SNAPSHOT/lib/log4j-slf4j-impl-2.17.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/export/apps/hadoop/hadoop-bin_2100503/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]
org.apache.flink.runtime.security.modules.SecurityModule$SecurityInstallException: Unable to set the Hadoop login user
	at org.apache.flink.runtime.security.modules.HadoopModule.install(HadoopModule.java:106)
	at org.apache.flink.runtime.security.SecurityUtils.installModules(SecurityUtils.java:76)
	at org.apache.flink.runtime.security.SecurityUtils.install(SecurityUtils.java:57)
	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1188)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.UnsupportedOperationException: Proxy user is not supported
	at org.apache.flink.runtime.security.token.hadoop.KerberosLoginProvider.throwProxyUserNotSupported(KerberosLoginProvider.java:137)
	at org.apache.flink.runtime.security.token.hadoop.KerberosLoginProvider.isLoginPossible(KerberosLoginProvider.java:81)
	at org.apache.flink.runtime.security.modules.HadoopModule.install(HadoopModule.java:73)
	... 4 more
{code}

This seems to have gotten changed after [480e6edf|https://github.com/apache/flink/commit/480e6edf9732f8334ef7576080fdbfc98051cb28] ([FLINK-28330][runtime][security] Remove old delegation token framework code)",,gaborgsomogyi,martijnvisser,vsowrirajan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Feb 26 20:09:21 UTC 2023,,,,,,,,,,"0|z1fzwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 00:35;vsowrirajan;[~gaborgsomogyi] I will take a stab on fixing the issue.;;;","17/Feb/23 07:59;gaborgsomogyi;[~vsowrirajan] thanks for reporting the issue. I agree that proxy support is not allowed only in case of DT but in normal usage it should work. The mentioned check must be turned off/avoided all cases where no DT is involved (HadoopModule for sure, YARN area maybe affected). Ping me on the PR and I can review it...;;;","17/Feb/23 08:02;gaborgsomogyi;cc [~martijnvisser];;;","17/Feb/23 08:26;martijnvisser;Which version has been used for testing? Please update the affected version for this. Especially if it's marked as a blocker;;;","17/Feb/23 08:34;gaborgsomogyi;This exists on the latest master.;;;","17/Feb/23 08:49;martijnvisser;OK, then this is a release blocker for 1.17. We should get this fixed asap;;;","17/Feb/23 09:40;gaborgsomogyi;Waiting on [~vsowrirajan] since they have the test env where it can be validated. What I can help is to review when PR is available.;;;","17/Feb/23 09:46;martijnvisser;Perfect, thanks [~gaborgsomogyi] and [~vsowrirajan];;;","20/Feb/23 19:47;vsowrirajan;[~gaborgsomogyi] Thanks for the confirmation. Yeah I am still working on the fix. Will post it soon after testing it internally.;;;","21/Feb/23 08:08;martijnvisser;[~vsowrirajan] Is there any eta on this? Since its a blocker for 1.17, we would like to understand how long this is expecting to take before being resolved.;;;","21/Feb/23 18:25;vsowrirajan;[~martijnvisser] I'm still looking into it. I would probably need till the end of this week for the fix and all the internal testing that needs to be done before raising the PR. Do you have any timelines in mind?;;;","21/Feb/23 22:08;martijnvisser;[~vsowrirajan] Its planned to have all release testing and blockers resolved at the end of this week, so we can create a release candidate on Monday next week. ;;;","24/Feb/23 02:09;vsowrirajan;[~martijnvisser] Sure, I have posted a PR with the fix after discussing with [~gaborgsomogyi] . Please take a look whenever you get a chance. Thanks.;;;","26/Feb/23 20:09;martijnvisser;Fixed in

Master: 78136133fbec4ca145dec66d4bc0c324c8e16d82
Release-1.17: 29f4181e1076712dcaeaadeaad0c1bcb2ef25b70;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chained WindowOperator throws NPE in PyFlink ThreadMode,FLINK-31099,13524963,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxbks2ks,hxbks2ks,hxbks2ks,16/Feb/23 07:22,16/Feb/23 09:31,13/Jul/23 08:29,16/Feb/23 09:31,1.16.1,1.17.0,,,,,1.16.2,1.17.0,,,,,,,API / Python,,,,0,pull-request-available,,,"Test case
{code:python}
config = Configuration()
config.set_string(""python.execution-mode"", ""process"")
env = StreamExecutionEnvironment.get_execution_environment(config)

class MyTimestampAssigner(TimestampAssigner, ABC):
    def extract_timestamp(self, value: tuple, record_timestamp: int) -> int:
        return value[0]

ds = env.from_collection(
    [(1676461680000, ""a1"", ""b1"", 1), (1676461680000, ""a1"", ""b1"", 1),
     (1676461680000, ""a2"", ""b2"", 1), (1676461680000, ""a1"", ""b2"", 1),
     (1676461740000, ""a1"", ""b1"", 1), (1676461740000, ""a2"", ""b2"", 1)]
).assign_timestamps_and_watermarks(
    WatermarkStrategy.for_monotonous_timestamps().with_timestamp_assigner(MyTimestampAssigner())
)
ds.key_by(
    lambda x: (x[0], x[1], x[2])
).window(
    TumblingEventTimeWindows.of(Time.minutes(1))
).reduce(
    lambda x, y: (x[0], x[1], x[2], x[3] + y[3]),
    output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.STRING(), Types.INT()])
# ).filter(
#     lambda x: x[1] == ""a1""
).map(
    lambda x: (x[0], x[1], x[3]),
    output_type=Types.TUPLE([Types.LONG(), Types.STRING(), Types.INT()])
).print()
env.execute()
{code}",,hxbks2ks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 09:31:57 UTC 2023,,,,,,,,,,"0|z1fz54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 09:31;hxbks2ks;Merged into master via ca770b3d905936d8a93071210bd6542b6733221d
Merged into release-1.17 via c7c035a2413c04cd75948d8364e0770b97499901
Merged into release-1.16 via e3c0060e7fca53e0e01cb91e00607c8146b85604;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
mysql catalog datatype mapping error tinyint mapping boolean ,FLINK-31097,13524938,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,hiscat,hiscat,16/Feb/23 03:16,16/Feb/23 03:23,13/Jul/23 08:29,16/Feb/23 03:23,1.16.0,1.16.1,,,,,,,,,,,,,Connectors / JDBC,,,,0,,,,"mysql ddl 
`auto_borrow` tinyint(1) DEFAULT '0',
 
flink sql client
mysql catalog query 
  `auto_borrow` BOOLEAN,
 
so weird.",,hiscat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 03:23:58 UTC 2023,,,,,,,,,,"0|z1fyzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 03:23;hiscat;mysql driver bug;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive ITCases fail with OutOfMemoryError,FLINK-31092,13524864,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fsk119,mapohl,mapohl,15/Feb/23 14:55,06/Mar/23 11:04,13/Jul/23 08:29,06/Mar/23 11:04,1.16.1,1.17.0,1.18.0,,,,1.16.2,1.17.0,1.18.0,,,,,,Connectors / Hive,,,,0,pull-request-available,test-stability,,"We're experiencing an OutOfMemoryError where the heap space reaches the upper limit:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46161&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=23142

{code}
Feb 15 05:05:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCase
Feb 15 05:05:17 [INFO] java.lang.OutOfMemoryError: Java heap space
Feb 15 05:05:17 [INFO] Dumping heap to java_pid9669.hprof ...
Feb 15 05:05:28 [INFO] Heap dump file created [1957090051 bytes in 11.718 secs]
java.lang.OutOfMemoryError: Java heap space
	at org.apache.maven.surefire.booter.ForkedBooter.cancelPingScheduler(ForkedBooter.java:209)
	at org.apache.maven.surefire.booter.ForkedBooter.acknowledgedExit(ForkedBooter.java:419)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:186)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
{code}",,fsk119,leonard,luoyuxia,mapohl,martijnvisser,Wencong Liu,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30556,FLINK-20945,,,,,,"01/Mar/23 07:40;fsk119;-__w-2-s-flink-connectors-flink-connector-hive-target-surefire-reports-2023-02-15T05-01-18_982-jvmRun4.dump;https://issues.apache.org/jira/secure/attachment/13055926/-__w-2-s-flink-connectors-flink-connector-hive-target-surefire-reports-2023-02-15T05-01-18_982-jvmRun4.dump","15/Feb/23 15:03;mapohl;VisualVM-FLINK-31092.png;https://issues.apache.org/jira/secure/attachment/13055473/VisualVM-FLINK-31092.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 07:41:08 UTC 2023,,,,,,,,,,"0|z1fyj4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 14:58;mapohl;I'm linking FLINK-30556 and FLINK-20945 here because they also deal with OOM in the Hive connector. That's the reason why I'm keeping this one at {{Critical}} instead of {{Blocker}}.;;;","15/Feb/23 15:03;mapohl;There's a heap dump created that reveals that the majority of the data (44.2%=~850MB) comes from {{char[]}} instances (e.g. SQL queries). There are also various {{ServiceLoaderUtil.LoadResult}} instances stored in the heap (24%=~460MB)  that reference an {{IllegalStateException}}.
 !VisualVM-FLINK-31092.png! ;;;","15/Feb/23 15:04;mapohl;[~Wencong Liu] may you have a look at it since you worked on FLINK-30556?;;;","15/Feb/23 15:18;Wencong Liu;Hi [~mapohl], is the heap dump generated before OOM error occurs?;;;","15/Feb/23 16:35;mapohl;I'm not sure - based on the logs, it sounds like it was created as soon as the OOM error happened:
{code}
Feb 15 05:05:14 [INFO] Running org.apache.flink.table.catalog.hive.HiveCatalogITCase
Feb 15 05:05:17 [INFO] java.lang.OutOfMemoryError: Java heap space
Feb 15 05:05:17 [INFO] Dumping heap to java_pid9669.hprof ...
Feb 15 05:05:28 [INFO] Heap dump file created [1957090051 bytes in 11.718 secs]
java.lang.OutOfMemoryError: Java heap space
[...]
{code}
We're also setting {{+HeapDumpOnOutOfMemoryError}} in [tools/ci/test_controller.sh:67|https://github.com/apache/flink/blob/7e37d59f834bca805f5fbee99db87eb909d1814f/tools/ci/test_controller.sh#L67]. AFAIU, this makes the JVM generate the heap dump as soon as the OOM occurs.;;;","21/Feb/23 08:15;martijnvisser;[~luoyuxia] Can you take a look?;;;","22/Feb/23 02:20;luoyuxia;Sure, I'll have a look. ;;;","27/Feb/23 04:28;luoyuxia;Try to analyze the heap dump, I found most of object will be `ServiceLoaderUtil#LoadResult(IllegalStateException)`, and the exception message is `

Trying to access closed classloader.xxxx`. I think that's the cause of OOM. From the code:

 
{code:java}
static <T> List<LoadResult<T>> load(Class<T> clazz, ClassLoader classLoader) {
    List<LoadResult<T>> loadResults = new ArrayList<>();

    Iterator<T> serviceLoaderIterator = ServiceLoader.load(clazz, classLoader).iterator();

    while (true) {
        try {
            T next = serviceLoaderIterator.next();
            loadResults.add(new LoadResult<>(next));
        } catch (NoSuchElementException e) {
            break;
        } catch (Throwable t) {
            loadResults.add(new LoadResult<>(t));
        }
    }

    return loadResults;
} {code}
Seems it'll then loop indefinitely when `serviceLoaderIterator.next()` throw exception other than NoSuchElementException. And it'll add more and more `LoadResult`  utill OOM.

 

And the stack where OOM happens is as follows:

 
{code:java}
 at java.lang.OutOfMemoryError.<init>(OutOfMemoryError.java:48)
    at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.ensureInner(FlinkUserCodeClassLoaders.java:179)
    at org.apache.flink.util.FlinkUserCodeClassLoaders$SafetyNetWrapperClassLoader.getResources(FlinkUserCodeClassLoaders.java:213)
    at java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:348)
    at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:364)
    at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
    at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
    at org.apache.flink.table.factories.ServiceLoaderUtil.load(ServiceLoaderUtil.java:42)
    at org.apache.flink.table.factories.FactoryUtil.discoverFactories(FactoryUtil.java:805)
    at org.apache.flink.table.factories.FactoryUtil.discoverFactory(FactoryUtil.java:524)
    at org.apache.flink.table.factories.PlannerFactoryUtil.createPlanner(PlannerFactoryUtil.java:45)
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.createStreamTableEnvironment(OperationExecutor.java:375)
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.getTableEnvironment(OperationExecutor.java:332)
    at org.apache.flink.table.gateway.service.operation.OperationExecutor.executeStatement(OperationExecutor.java:190)
    at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl.lambda$executeStatement$1(SqlGatewayServiceImpl.java:212)
    at org.apache.flink.table.gateway.service.SqlGatewayServiceImpl$$Lambda$1007.apply(<unknown string>)
    at org.apache.flink.table.gateway.service.operation.OperationManager.lambda$submitOperation$1(OperationManager.java:110)
    at org.apache.flink.table.gateway.service.operation.OperationManager$$Lambda$1008.call(<unknown string>)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation.lambda$run$0(OperationManager.java:242)
    at org.apache.flink.table.gateway.service.operation.OperationManager$Operation$$Lambda$1010.run(<unknown string>)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
    at java.util.concurrent.FutureTask.run(FutureTask.java:266)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
    at java.lang.Thread.run(Thread.java:748){code}
 

 

[~fsk119] Could you please have a look? Is it possible it'll access a closed classloader? 

 ;;;","27/Feb/23 10:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46558&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22559;;;","27/Feb/23 11:11;mapohl;I'm increasing the priority of this issue to blocker since there was actual work done on the Hive code in 1.17 and so far we're only seeing the issue in 1.17 (i.e. {{master}} or {{{}release-1.17{}}});;;","28/Feb/23 07:55;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46609&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=22520;;;","01/Mar/23 12:12;fsk119;Hi, all. I think [~luoyuxia] is right. `ServiceLoaderUtil#LoadResult(IllegalStateException)` keeps loading the exception when the classloader is closed by the SessionManager. The case is possible to happen when the session is closed but the operation is running. But it's difficult for Gateway to cancel the task that is submitted to the `ExecutorService` by force if the task doesn't respect the interrupted flag.

[~slinkydeveloper], [~twalthr] could you share some thoughts about this? Can we limit the type of exception here to prevent endless loading?
{code:java}
static <T> List<LoadResult<T>> load(Class<T> clazz, ClassLoader classLoader) {
    List<LoadResult<T>> loadResults = new ArrayList<>();

    Iterator<T> serviceLoaderIterator = ServiceLoader.load(clazz, classLoader).iterator();

    while (true) {
        try {
            T next = serviceLoaderIterator.next();
            loadResults.add(new LoadResult<>(next));
        } catch (NoSuchElementException e) {
            break;
        } catch (Throwable t) {
            loadResults.add(new LoadResult<>(t));
        }
    }

    return loadResults;
} 

{code};;;","01/Mar/23 15:34;mapohl;From an outsider's perspective: What is preventing us from providing cancellation functionality in the Operator interface? It feels like the session should stop any running operation when it is shut down, shouldn't it? 🤔 As a quickfix it sounds reasonable to replace the {{IllegalStateException}} in [FlinkUserCodeClassLoaders:179|https://github.com/apache/flink/blob/9201f1e3684b130c3d665114f28208f248848b46/flink-core/src/main/java/org/apache/flink/util/FlinkUserCodeClassLoaders.java#L179] by a {{ClosedClassloaderException}} which extends {{FlinkRuntimeException}} and make this being caught in the code you already mentioned ([ServiceLoaderUtil:44|https://github.com/apache/flink/blob/dd446c9d56be5f33c683611102ec7026cf95e395/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/ServiceLoaderUtil.java#L44]). But again, it sounds we're missing this cancel feature if I understand you correctly.;;;","02/Mar/23 08:05;fsk119;Yes, Gateway will try to cancel the execution of the running operation by explicitly interrupting the thread in [Operation:377|https://github.com/apache/flink/blob/9201f1e3684b130c3d665114f28208f248848b46/flink-table/flink-sql-gateway/src/main/java/org/apache/flink/table/gateway/service/operation/OperationManager.java#L377] and wait all task finish. However, the Gateway seems close the resource before all task finishes. This confuses me a lot. BTW, I think we can propose a quick fix like this to prevent endless loading here. WDYT [~mapohl] ,[~Leonard] ?
{code:java}
static <T> List<LoadResult<T>> load(Class<T> clazz, ClassLoader classLoader) {
        List<LoadResult<T>> loadResults = new ArrayList<>();

        Iterator<T> serviceLoaderIterator = ServiceLoader.load(clazz, classLoader).iterator();

        while (true) {
            try {
                T next = serviceLoaderIterator.next();
                loadResults.add(new LoadResult<>(next));
            } catch (NoSuchElementException e) {
                break;
            } catch (Throwable t) {
                // check whether the throwable is as expected
                if (!clazz.isInstance(t)) {
                    throw t;
                }
                loadResults.add(new LoadResult<>(t));
            }
        }

        return loadResults;
    }
{code};;;","02/Mar/23 08:43;mapohl;I guess, your proposal wouldn't work: It looks like {{LoadResult(Throwable)}} is used to mark failed load operations. {{clazz}} in contrast is used for actually loading a specifc class. The proposed if condition seems to mix up these two separate code paths.;;;","02/Mar/23 08:53;mapohl;{quote}
Yes, Gateway will try to cancel the execution of the running operation by explicitly interrupting the thread in Operation:377 and wait all task finish. However, the Gateway seems close the resource before all task finishes. 
{quote}
Do we understand why exactly the classloaders are closed before cancelling the operation. IIUC, this should happen the other way around.;;;","02/Mar/23 09:29;fsk119;After discussing with [~lsy] , I think we get why the behavior is not expected on the Gateway side. It's because of the wrong usage of the `FutureTask` in line [Operation:377|https://github.com/apache/flink/blob/9201f1e3684b130c3d665114f28208f248848b46/flink-table/flink-sql-gateway/src/main/java/org/apache/flink/table/gateway/service/operation/OperationManager.java#L377]. {{{}FutureTask#cancel{}}} doesn't promise the cancel the execution of the running thread by force[1]. {{FutureTask}} only interrupts the worker thread and then invokes the {{FutureTask#done}}. The {{FutureTask#done}} notifies the Gateway that all tasks are finished and ready to release used resources in the failed test. I will open a fix about this to truly kill the thread. 

[1] https://stackoverflow.com/questions/11158454/future-task-of-executorservice-not-truly-cancelling;;;","02/Mar/23 09:44;fsk119;> I guess, your proposal wouldn't work: It looks like LoadResult(Throwable) is used to mark failed load operations. clazz in contrast is used for actually loading a specifc class. The proposed if condition seems to mix up these two separate code paths.

I think the {{loadResults}} only should put the Object is an instance of the specified type rather than any objects. It's a little weird we put something unrelated into the list. What's more, we check the type explicitly in this [line|https://github.com/apache/flink/blob/9201f1e3684b130c3d665114f28208f248848b46/flink-table/flink-table-common/src/main/java/org/apache/flink/table/factories/FactoryUtil.java#L528].  ;;;","02/Mar/23 10:30;mapohl;{quote}
[...] What's more, we check the type explicitly in this line.
{quote}
Good pointer: I guess, the error handling could be refactored. Right now it's separated into two places ({{FactoryUtil.discoverFactories}} handles the {{NoClassDefFoundError}} and translates any other exception that was handled in {{ServiceLoaderUtil.load}} into a {{TableException}}) which shouldn't be the case. We shouldn't make the service loading loop forever because of an unexpected error but immediately throw a {{TableException}} in that case.;;;","02/Mar/23 11:36;fsk119;+1 to refactor this.;;;","03/Mar/23 07:41;fsk119;Merged into release-1.17: 
594010624f8084efd99d6d405b5310ab24013aeb
86e12eb3fcec54d234154e59f8cb0557fc616494

Merged into master:
d2296422933c0e3b8895e3b7e0a0a95dacdd3257
f47b3704867b1d5aa754b1f7325f54e1830014cd

Merged into release-1.16:
2dcd6cc6af0109fd1a411320cab400b393d07fe9
93cd23c55436f3d9c38051e31cfd34f2fd9c4aff
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL interval related queries stop working via SQL client,FLINK-31091,13524862,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,Sergey Nuyanzin,Sergey Nuyanzin,Sergey Nuyanzin,15/Feb/23 14:37,22/Feb/23 15:23,13/Jul/23 08:29,22/Feb/23 14:48,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / Client,,,,0,pull-request-available,,,"I put blocker since it works in 1.16.x and stopped working in 1.17 after a certain commit

Any interval related query run via SQL Client is failing with 

{noformat}
[ERROR] Could not execute SQL statement. Reason:
org.apache.flink.runtime.rest.util.RestClientException: [Internal server error. Could not map response to JSON.]
	at org.apache.flink.runtime.rest.RestClient.parseResponse(RestClient.java:536)
	at org.apache.flink.runtime.rest.RestClient.lambda$submitRequest$3(RestClient.java:516)
	at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)
	at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)
	at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:750)

{noformat}

example of query
{code:sql}
SELECT INTERVAL '2' DAY;
SELECT 1, INTERVAL '2' YEAR;
{code}

based on tests it stopped working after this commit 
https://issues.apache.org/jira/browse/FLINK-29945

More traces from logs
{noformat}
org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException: Unable to serialize logical type 'INTERVAL MONTH NOT NULL'. Please check the documentation for supported types. (through reference chain: java.util.Coll
ections$UnmodifiableRandomAccessList[1]->org.apache.flink.table.gateway.rest.serde.ColumnInfo[""logicalType""])
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:392) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.JsonMappingException.wrapWithPath(JsonMappingException.java:351) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.StdSerializer.wrapAndThrow(StdSerializer.java:316) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:782) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanSerializer.serialize(BeanSerializer.java:178) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serializeContents(IndexedListSerializer.java:119) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:79) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.impl.IndexedListSerializer.serialize(IndexedListSerializer.java:18) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.SerializerProvider.defaultSerializeField(SerializerProvider.java:1166) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.ResultInfoSerializer.serialize(ResultInfoSerializer.java:82) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.ResultInfoSerializer.serialize(ResultInfoSerializer.java:47) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.SerializerProvider.defaultSerializeField(SerializerProvider.java:1166) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.FetchResultsResponseBodySerializer.serialize(FetchResultsResponseBodySerializer.java:60) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.FetchResultsResponseBodySerializer.serialize(FetchResultsResponseBodySerializer.java:31) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.DefaultSerializerProvider._serialize(DefaultSerializerProvider.java:480) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.DefaultSerializerProvider.serializeValue(DefaultSerializerProvider.java:319) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper._writeValueAndClose(ObjectMapper.java:4568) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ObjectMapper.writeValue(ObjectMapper.java:3804) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.util.HandlerUtils.sendResponse(HandlerUtils.java:92) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.lambda$respondToRequest$1(AbstractSqlGatewayRestHandler.java:93) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670) ~[?:1.8.0_362]
        at java.util.concurrent.CompletableFuture.uniAcceptStage(CompletableFuture.java:683) ~[?:1.8.0_362]
        at java.util.concurrent.CompletableFuture.thenAccept(CompletableFuture.java:2010) ~[?:1.8.0_362]
        at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:91) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.handler.AbstractSqlGatewayRestHandler.respondToRequest(AbstractSqlGatewayRestHandler.java:52) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.AbstractHandler.respondAsLeader(AbstractHandler.java:196) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.lambda$channelRead0$0(LeaderRetrievalHandler.java:83) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]

        at java.util.Optional.ifPresent(Optional.java:159) [?:1.8.0_362]
        at org.apache.flink.util.OptionalConsumer.ifPresent(OptionalConsumer.java:45) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:80) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.LeaderRetrievalHandler.channelRead0(LeaderRetrievalHandler.java:49) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.routed(RouterHandler.java:115) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:94) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.handler.router.RouterHandler.channelRead0(RouterHandler.java:55) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:208) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.runtime.rest.FileUploadHandler.channelRead0(FileUploadHandler.java:69) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:336) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:308) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.netty4.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:750) [?:1.8.0_362]
Caused by: java.lang.UnsupportedOperationException: Unable to serialize logical type 'INTERVAL MONTH NOT NULL'. Please check the documentation for supported types.
        at org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer.serializeInternal(LogicalTypeJsonSerializer.java:174) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer.serialize(LogicalTypeJsonSerializer.java:100) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer.serialize(LogicalTypeJsonSerializer.java:51) ~[flink-sql-gateway-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.BeanPropertyWriter.serializeAsField(BeanPropertyWriter.java:728) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        at org.apache.flink.shaded.jackson2.com.fasterxml.jackson.databind.ser.std.BeanSerializerBase.serializeFields(BeanSerializerBase.java:774) ~[flink-dist-1.18-SNAPSHOT.jar:1.18-SNAPSHOT]
        ... 67 more

{noformat}
//cc [~fsk119]",,fsk119,mapohl,qingyue,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29945,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 15:23:07 UTC 2023,,,,,,,,,,"0|z1fyio:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 13:59;mapohl;[~fsk119] [~lic] [~qingyue] (I selected you based on a proposal from [~Leonard]) do you have any insights here?;;;","21/Feb/23 03:29;qingyue;Thanks, [~Sergey Nuyanzin] and [~mapohl] for reporting this.

The reason is that org.apache.flink.table.gateway.rest.serde.LogicalTypeJsonSerializer does not cope with `INTERVAL_YEAR_MONTH` and `INTERVAL_DAY_TIME`. cc [~fsk119] ;;;","21/Feb/23 05:26;Sergey Nuyanzin;Hi [~qingyue] thanks for volunteering, however there is already existing fix for that in ""links to"" section.
At the same time you are welcome to participate in review process
 ;;;","22/Feb/23 02:31;fsk119;Mregd into master: 14adc1679fb3d025a2808af91f23f14e7c6f6e24;;;","22/Feb/23 14:46;Sergey Nuyanzin;Merged into release-1.17 branch: 8535c26198d0e3372465aa283055ce39f4577f86;;;","22/Feb/23 15:09;mapohl;Thanks for resolving this issue. FYI in terms of fixVersion: Right now, we're at a stage where a fix (if added to {{master}} and {{release-1.17}}) is actually targeting 1.17.0. If the fix ends up on {{master}} only, the fixVersion would be 1.18.0.

The fixVersion is relevant for the creating the release notes, i.e. (assuming that this Jira issue would have release notes) you have to ask yourself in which release notes the change is meant to be included: 1.17.0 vs 1.18.0.;;;","22/Feb/23 15:20;Sergey Nuyanzin;Thanks for explanation, I think I got your point.

To double check, am i right that in case there was an issue in a released version then there could be several fix versions: one in current and others in minors in case they are still maintaining?;;;","22/Feb/23 15:23;mapohl;Yes, if you have to create a backport for release-1.16, you would have to add 1.16.2 as an additional fixVersion.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Python ProcessFunction with OutputTag cannot be reused,FLINK-31083,13524805,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,15/Feb/23 09:30,15/Feb/23 11:28,13/Jul/23 08:29,15/Feb/23 11:28,1.16.1,,,,,,1.16.2,1.17.0,,,,,,,API / Python,,,,0,pull-request-available,,,"{code:java}
output_tag = OutputTag(""side"", Types.STRING())

def udf(i):
    yield output_tag, i

ds1.map(udf).get_side_output(output_tag)
ds2.map(udf){code}
raises TypeError: cannot pickle '_thread.RLock' object",,dianfu,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 11:28:24 UTC 2023,,,,,,,,,,"0|z1fy60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 11:28;dianfu;Fixed in:
- master: 29b1be61fefd11bf1ef4c9bed5543c4c6f056f4e
- release-1.17: 712c9eeb7335c91fcecde70042881916b2045088
- release-1.16: 99582b5a5b4cd5355935450812ae3f75a26c865c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting maven property 'flink.resueForks' to false in table planner module ,FLINK-31082,13524793,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,15/Feb/23 08:36,01/Mar/23 12:26,13/Jul/23 08:29,01/Mar/23 01:29,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"This issue is created to alleviate the OOM problem mentioned in issue: https://issues.apache.org/jira/browse/FLINK-18356

Setting maven property 'flink.resueForks' to false in table planner module can only reduce the frequency of oom, but can't solve this problem. To completely solve this problem, we need to identify the specific reasons, but this is a time-consuming work.",,337361684@qq.com,godfrey,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-18356,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 01 12:26:06 UTC 2023,,,,,,,,,,"0|z1fy3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 13:36;martijnvisser;Fixed in:

master: 3b8951e91a961c150758043c3fc87de90c63fa2a
release-1.17: 12908421b7852d7716338a2edc89ac1ccd2e9e4c;;;","28/Feb/23 08:12;martijnvisser;Re-opened to create backport to release-1.16;;;","01/Mar/23 01:29;godfrey;Fixed in 1.16.2: 247a099cc358e0006aa0e387a55cf6d547814f98;;;","01/Mar/23 12:26;martijnvisser;[~godfrey] Thank you!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Jira links in How To Contribute guide,FLINK-31081,13524792,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,ericbrzezenski,mapohl,mapohl,15/Feb/23 08:13,27/Mar/23 10:51,13/Jul/23 08:29,27/Mar/23 10:51,1.17.0,,,,,,,,,,,,,,Project Website,,,,0,pull-request-available,starter,,"FLINK-30007 added a description in [the community docs|https://flink.apache.org/community.html#issue-tracker] on how to get access to the Flink Jira. But several other locations mentioned and link Jira as well (especially in the how to contribute sections). Newcomers might be miss the paragraph in community and wonder how they could get a working Jira account.

This issue is about replacing all the Jira links (where it's useful) by a reference to the issue track section in the community docs (https://flink.apache.org/community.html#issue-tracker). That way, they are redirected to the information on how they can gain access to the Flink jira board.

Locations that needs to be updated are (not exclusively!):
* https://flink.apache.org/contributing/contribute-code.html
* https://flink.apache.org/gettinghelp.html
*  https://flink.apache.org/contributing/how-to-contribute.html",,ericbrzezenski,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30007,,,,FLINK-31627,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 27 10:51:12 UTC 2023,,,,,,,,,,"0|z1fy34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Mar/23 04:23;ericbrzezenski;Do you mind assigning this to me? ill update these locations and any others that I may find. ;;;","20/Mar/23 09:12;mapohl;Thanks for offering your help, [~ericbrzezenski];;;","27/Mar/23 10:51;mapohl;asf-site: 8478a252a1238da60465bcf22847b40c554bb5ff (rebuild: 3fd8d6b48d7b47eab772743edea026b8e64879ae);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trigger checkpoint failed but it were shown as COMPLETED by rest API,FLINK-31077,13524769,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,15/Feb/23 06:09,27/Feb/23 07:09,13/Jul/23 08:29,27/Feb/23 07:09,1.15.3,1.16.1,1.17.0,,,,1.16.2,1.17.0,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,,,"Currently, we can trigger a checkpoint and poll the status of the checkpoint until it is finished by rest according to FLINK-27101. However, even if the checkpoint status returned by rest is completed, it does not mean that the checkpoint is really completed. If an exception occurs after marking the pendingCheckpoint completed([here|https://github.com/apache/flink/blob/bf0ad52cbcb052961c54c94c7013f5ac0110ef8a/flink-runtime/src/main/java/org/apache/flink/runtime/checkpoint/CheckpointCoordinator.java#L1309]), the checkpoint is not written to the HA service and we can not failover from this checkpoint.",,gaoyunhaii,JunRuiLi,mason6345,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27101,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 27 07:09:45 UTC 2023,,,,,,,,,,"0|z1fxy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 06:09;JunRuiLi;cc [~gaoyunhaii] ;;;","15/Feb/23 06:40;zhuzh;Thanks for reporting this issue! [~JunRuiLi]
-I think it is indeed a problem. Considering the case of stop-with-savepoint, it's possible that the final savepoint is lost if the savepoint is considered to be done and the job gets terminated, before it is recorded to HA.-
Do you want to fix it?

Correction: The problem does not affect savepoints which do not rely on CompletedCheckpointStore. So the actual problem will be that the query result of a [manually triggered checkpoint|https://nightlies.apache.org/flink/flink-docs-master/docs/ops/rest_api/#jobs-jobid-checkpoints] is returned as ""COMPLETED"", while on the web UI it is ""FAILED"", which may confuse users. 
Therefore the problem is not that critical. I will lower its priority.;;;","15/Feb/23 06:43;JunRuiLi;[~zhuzh] Sure, I'll fix it.;;;","27/Feb/23 07:09;zhuzh;master:
eb17ec3f05d4bd512bc70ee79296d0b884894eaf

release-1.17:
dca819556fb9b675852df99ada45e0f22262cb28

release-1.16:
4c8159140028cd0654a93dcb7c25fe074ad1f059;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add pull request template for flink-connector-pulsar,FLINK-31067,13524680,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,14/Feb/23 14:52,15/Feb/23 06:20,13/Jul/23 08:29,15/Feb/23 06:20,pulsar-4.0.0,,,,,,pulsar-4.0.0,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,,,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 06:20:04 UTC 2023,,,,,,,,,,"0|z1fxeg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 06:20;Weijie Guo;main(4.0) via a39770d36153d7cee31dee4b69e8a33aa35e1639.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent duplicate reading when restoring from a checkpoint.,FLINK-31063,13524649,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,jiabao.sun,jiabao.sun,jiabao.sun,14/Feb/23 10:48,20/Feb/23 10:22,13/Jul/23 08:29,20/Feb/23 10:22,mongodb-1.0.0,,,,,,mongodb-1.0.0,,,,,,,,Connectors / MongoDB,,,,0,pull-request-available,,,"Exact-once semantics may not be guaranteed at present on partial reads.
We use a number fetchSize to limit the records count for every fetch loop but we didn't record the offset into the split state. When resuming the split reader from a partially completed split, we may re-read some data.

We should record the current reading offset into split state.
Skip this offset when restoring to prevent duplicate reading.",,jiabao.sun,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 10:22:49 UTC 2023,,,,,,,,,,"0|z1fx7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 10:49;jiabao.sun;Hi [~chesnay], please assign this ticket to me.;;;","20/Feb/23 10:22;chesnay;main: f67176de5c46ae11e8c791cbd986dab5826646b9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The dynamic flag of stream graph does not take effect when translating the transformations,FLINK-31055,13524605,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,14/Feb/23 07:12,16/Feb/23 15:16,13/Jul/23 08:29,16/Feb/23 15:16,,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Currently, the dynamic flag of stream graph is not set when [translate transformations|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraphGenerator.java#L324]. However, the dynamic flag will be used ([here|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/graph/StreamGraph.java#L696]) when translating, we should set the dynamic flag before the translating.",,JunRuiLi,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30683,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 15:16:08 UTC 2023,,,,,,,,,,"0|z1fwxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 07:15;wanglijie;cc [~JunRuiLi] [~zhuzh] ;;;","14/Feb/23 08:14;JunRuiLi;[~wanglijie] Thanks for creating this issue. As you said, I think it is a bug. Could you fix it？Thanks!;;;","14/Feb/23 09:31;wanglijie;[~JunRuiLi] I 'll fix it.;;;","16/Feb/23 15:16;wanglijie;Fixed via
master: 1c0870ae08730688706540b999c04b2f4c4498ee
release-1.17: 629bc9a3d35f5141324ef4cfa9b255dfcc069d20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Java doc of PulsarSourceBuilder was not updated in time,FLINK-31048,13524506,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,Weijie Guo,Weijie Guo,13/Feb/23 17:00,14/Feb/23 03:04,13/Jul/23 08:29,14/Feb/23 03:03,pulsar-4.0.0,,,,,,pulsar-4.0.0,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"The java doc of `PulsarSourceBuilder` was not updated in time.

IIUC, 
{code:java}
setDeserializationSchema(PulsarDeserializationSchema.flinkSchema(new SimpleStringSchema())) {code}
should be replaced by
{code:java}
setDeserializationSchema(new SimpleStringSchema()) {code}
BTW, I also checked `pulsar.md` and luckily it was correct.",,syhily,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 03:03:36 UTC 2023,,,,,,,,,,"0|z1fwbs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 03:03;Weijie Guo;main(4.0) via : fb98096d9a67d26b1965b8ef181125317a59a5c4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KeyError exception is thrown in CachedMapState,FLINK-31043,13524463,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hxb,dianfu,dianfu,13/Feb/23 11:50,13/Feb/23 12:13,13/Jul/23 08:29,13/Feb/23 12:12,1.15.0,,,,,,1.15.4,1.16.2,1.17.0,,,,,,API / Python,,,,0,,,,"Have seen the following exception in a PyFlink job which runs in Flink 1.15. It happens occasionally and may indicate a bug of the state cache of MapState:
{code:java}
Caused by: java.lang.RuntimeException: Error received from SDK harness for instruction 131: Traceback (most recent call last):
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 289, in _execute
    response = task()
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 362, in <lambda>
    lambda: self.create_worker().do_instruction(request), request)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 607, in do_instruction
    getattr(request, request_type), request.instruction_id)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/sdk_worker.py"", line 644, in process_bundle
    bundle_processor.process_bundle(instruction_id))
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 1000, in process_bundle
    element.data)
  File ""/usr/local/python3/lib/python3.7/site-packages/apache_beam/runners/worker/bundle_processor.py"", line 228, in process_encoded
    self.output(decoded_value)
  File ""apache_beam/runners/worker/operations.py"", line 357, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 359, in apache_beam.runners.worker.operations.Operation.output
  File ""apache_beam/runners/worker/operations.py"", line 221, in apache_beam.runners.worker.operations.SingletonConsumerSet.receive
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 158, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""pyflink/fn_execution/beam/beam_operations_fast.pyx"", line 170, in pyflink.fn_execution.beam.beam_operations_fast.FunctionOperation.process
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/table/operations.py"", line 417, in finish_bundle
    return self.group_agg_function.finish_bundle()
  File ""pyflink/fn_execution/table/aggregate_fast.pyx"", line 597, in pyflink.fn_execution.table.aggregate_fast.GroupTableAggFunction.finish_bundle
  File ""pyflink/fn_execution/table/aggregate_fast.pyx"", line 652, in pyflink.fn_execution.table.aggregate_fast.GroupTableAggFunction.finish_bundle
  File ""pyflink/fn_execution/table/aggregate_fast.pyx"", line 389, in pyflink.fn_execution.table.aggregate_fast.SimpleTableAggsHandleFunction.emit_value
  File ""/tmp/pyflink/17360444-8c0b-46a5-90a4-689c376ea4ed/0e2967b5-181c-4663-bd7a-267d47509cf5/whms_dws_stock_python_sps_1_output.py"", line 29, in emit_value
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/table/state_data_view.py"", line 147, in get
    return self._map_state.get(key)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 915, in get
    return self.get_internal_state().get(key)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 773, in get
    self._state_key, map_key, self._map_key_encoder, self._map_value_decoder)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 418, in blocking_get
    cached_map_state.put(map_key, (exists, value))
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 319, in put
    super(CachedMapState, self).put(key, exists_and_value)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 68, in put
    self._on_evict(name, value)
  File ""/usr/local/python3/lib/python3.7/site-packages/pyflink/fn_execution/state_impl.py"", line 305, in on_evict
    self._cached_keys.remove(key)
KeyError: 'SPAREPARTS_M11F010L4L1_01'
{code}",,dianfu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 12:12:52 UTC 2023,,,,,,,,,,"0|z1fw28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 12:12;dianfu;This issue has already been fixed in:
- master via 838b79f5b9cc1a4cf253b2c17009f337bf569ecc
- release-1.16 via f91ca7d2cd4e466c0aa6d4dc56ee982abc5ca5e1
- release-1.15 via fa77fd4b1333b23a2769c77994644ba8a8d7d4ff;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AfterMatchSkipStrategy not working on notFollowedBy ended pattern,FLINK-31042,13524460,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,13/Feb/23 11:17,17/Feb/23 08:28,13/Jul/23 08:29,17/Feb/23 08:28,1.16.0,,,,,,1.16.2,1.17.0,,,,,,,Library / CEP,,,,0,,,,"Pattern: begin(""A"", SkipToNext()).oneOrMore().allowCombinations().followedBy(""C"").notFollowedBy(""B"").within(Time.milliseconds(10L))

Sequence: <a1, 1L> <a2, 2L> <a3, 3L> <c1, 4L> will produce

[a1, a2, a3, c1]

[a1, a2, c1]

[a1, c1]

[a2, a3, c1]

[a2, c1]

[a3, c1]

Using SkipPastLastEvent() also produce the same result.",,dianfu,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 08:28:48 UTC 2023,,,,,,,,,,"0|z1fw1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 08:28;dianfu;Merged to:
- master: 919d46ef91488627a282d1474208490b6dc7a820
- 1.17: 257be14bb47f7481927ea17bc82b4b6ff63e8b30
- 1.16: d61e4a1884e265e0a33f0f4dbad50898df14677f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build up of pending global failures causes JM instability,FLINK-31041,13524459,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,huwh,dannycranmer,dannycranmer,13/Feb/23 11:09,23/Feb/23 08:48,13/Jul/23 08:29,23/Feb/23 08:43,1.15.3,1.16.1,,,,,1.15.4,1.16.2,1.17.0,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"h4. Context

When a job creates multiple sources that use the {{SourceCoordinator}} (FLIP-27), there is a failure race condition that result in a ""leak"" of ExecutionVertextVersion due to a ""queue"" of pending global failures. 

This results in the Job Manager becoming unresponsive.
h4. !flink-31041-heap-dump.png!
h4. Reproduction Steps

This can be reproduced by a job that creates multiple sources that fail in the {{{}SplitEnumerator{}}}. We observed this with multiple {{KafkaSource's}} trying to load a non-existent cert from the file system and throwing FNFE. Thus, here is a simple job to reproduce (BE WARNED: running this locally will lock up your IDE):
{code:java}
StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
env.setParallelism(1);
env.setRestartStrategy(new RestartStrategies.FailureRateRestartStrategyConfiguration(10000, Time.of(10, TimeUnit.SECONDS), Time.of(10, TimeUnit.SECONDS)));

KafkaSource<String> source = KafkaSource.<String>builder()
        .setProperty(""security.protocol"", ""SASL_SSL"")
        // SSL configurations
        // Configure the path of truststore (CA) provided by the server
        .setProperty(""ssl.truststore.location"", ""/path/to/kafka.client.truststore.jks"")
        .setProperty(""ssl.truststore.password"", ""test1234"")
        // Configure the path of keystore (private key) if client authentication is required
        .setProperty(""ssl.keystore.location"", ""/path/to/kafka.client.keystore.jks"")
        .setProperty(""ssl.keystore.password"", ""test1234"")
        // SASL configurations
        // Set SASL mechanism as SCRAM-SHA-256
        .setProperty(""sasl.mechanism"", ""SCRAM-SHA-256"")
        // Set JAAS configurations
        .setProperty(""sasl.jaas.config"", ""org.apache.kafka.common.security.scram.ScramLoginModule required username=\""username\"" password=\""password\"";"")
        .setBootstrapServers(""http://localhost:3456"")
        .setTopics(""input-topic"")
        .setGroupId(""my-group"")
        .setStartingOffsets(OffsetsInitializer.earliest())
        .setValueOnlyDeserializer(new SimpleStringSchema())
        .build();

List<SingleOutputStreamOperator<String>> sources = IntStream.range(0, 32)
        .mapToObj(i -> env
                .fromSource(source, WatermarkStrategy.noWatermarks(), ""Kafka Source "" + i).uid(""source-"" + i)
                .keyBy(s -> s.charAt(0))
                .map(s -> s))
        .collect(Collectors.toList());

env.fromSource(source, WatermarkStrategy.noWatermarks(), ""Kafka Source"").uid(""source"")
        .keyBy(s -> s.charAt(0))
        .union(sources.toArray(new SingleOutputStreamOperator[] {}))
        .print();

env.execute(""test job""); {code}
h4. Root Cause

We can see that the {{OperatorCoordinatorHolder}} already has a [debounce mechanism|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/operators/coordination/OperatorCoordinatorHolder.java#L609], however the {{DefaultScheduler}} does not. We need a debounce mechanism in the {{DefaultScheduler}} since it handles many {{{}OperatorCoordinatorHolder{}}}.
h4. Fix

I have managed to fix this, I will open a PR, but would need feedback from people who understand this code better than me!

 

 ",,dannycranmer,huwh,wanglijie,Weijie Guo,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Feb/23 10:43;zhuzh;failovers.log;https://issues.apache.org/jira/secure/attachment/13055458/failovers.log","13/Feb/23 11:11;dannycranmer;flink-31041-heap-dump.png;https://issues.apache.org/jira/secure/attachment/13055400/flink-31041-heap-dump.png","15/Feb/23 10:42;zhuzh;test-restart-strategy.log;https://issues.apache.org/jira/secure/attachment/13055457/test-restart-strategy.log",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 22 07:55:32 UTC 2023,,,,,,,,,,"0|z1fw1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 10:41;zhuzh;Thanks for reporting this issue! [~dannycranmer]

I have tried to re-produce the problem locally, but not able yet to fully re-produce it.
Here is what I see by running the example in the JIRA description:
1. The restart strategy works. by changing the strategy to {{RestartStrategies.fixedDelayRestart(100, Time.of(10, TimeUnit.SECONDS)}}, the job failed after encountering 100 failures.(see attached test-restart-strategy.log) I guess the reason it is not working as expected in the above example is that it tolerates 10000 failures in 10 seconds.
2. The debounce mechanism of DefaultScheduler works. The execution vertex versioning is introduced for this purpose.
I tested it by reducing the source number to 2(see attached failovers.log) for easier diagnostics. I can see 2 global failures happened every 10 seconds, and only one job restarts in triggered by them.

Regarding the ""leak of ExecutionVertexVersion"", I'm not sure but guess it is caused by that there are many pending global failures in JM's main thread. It's not leaking, although not ideal. Yet this can be avoided if failures are not triggered too frequently.

Regarding the specific case(or similar ones) in the JIRA description, looks to me it can be resolved by setting the tolerable failure rate to a proper value. 
WDYT?

;;;","15/Feb/23 11:02;dannycranmer;[~zhuzh] thanks for looking in to this.

 

If I understand correctly, you are saying the ""leak"" is due to a ""queue"" of pending global failures. They would eventually get debounced, but there is some intermediate holding area.

 

We are running with jobmanager.execution.failover-strategy = full. In this mode I would not expect multiple errors to trigger a global job failure. Possibly we can debounce the failures earlier (as per my naive PR) and prevent this build up? For additional context, the actual job that was causing the problem for us originally, simply had 2x Kafka sources, and the same FileNotFoundException was triggering the failover. 

 

> Regarding the specific case(or similar ones) in the JIRA description, looks to me it can be resolved by setting the tolerable failure rate to a proper value.

Unfortunately this is not possible for us. We do not want the job to transition to \{{FAILED}}. We require the job to retry indefinitely. The job could be failing due to transient errors, like permission issues that can be fixed without resubmitting the job. We use a fixed delay of 10 seconds.;;;","16/Feb/23 09:45;huwh;[~zhuzh] [~dannycranmer]
I successfully reproduced this problem locally. I found that:
 # OperatorCoordinators are re-created and started unexpectedly. DefaultScheduler will manage multiple OperatorCoordinators. Each coordinator could call DefaultScheduler#handleGlobalFailure in their thread, and DefaultScheduler would handle them one by one. Executions will not be restarted multiple times Since the vertex version provides the debounce mechanism. But some other functions will be called multiple times, which may cause unexpected impact. Such as SchedulerBase#restoreState, it will recreate-start new operator coordinators. If there are two SourceCoordinators (A, B), and both of them will throw exception in start(). The fail of A will recreate A1,B1 and restart all executions, The fail of B will recreate A2, B2 but not restart executions. Then A1/B1/A2/B2 will fail in start(), triggering more DefaultScheduler#handleGlobalFailure.
 # The retrigger of DefaultScheduler#handleGlobalFailure will make RestartStrategy  not working as expected. For example, if we set RestartStrategies.fixedDelayRestart(1, Time.of(10, TimeUnit.SECONDS). The job should restart twices, but when this job has 2 source coordinators, the job will fail immediately.

IMO, DefaultScheduler should also have debounce mechanism in handleGlobalFailure. 
Please correct me if I am wrong.;;;","16/Feb/23 09:49;zhuzh;Thanks for the inputs! [~dannycranmer]

I took another look through the failure handling process. I think that why the problem is disturbing is that outdated global failure recovery is still heavy, compared to outdated regional failure recovery. An outdated regional failure recovery almost does nothing because it operates on an empty task set (tasks to restart are filtered out due to outdated). For a global failure recovery which is superseded by another, however, it still conducts {{checkpointCoordinator#restoreLatestCheckpointedStateToAll(...)}}, which I think is a heavy invocation.

So I think maybe we can just skip that {{restoreLatestCheckpointedStateToAll(...)}} invocation if {{jobVerticesToRestore}} is empty, like [this change|https://github.com/zhuzhurk/flink/commit/a522edeb4e23ab1ce3f5a34eb3269116723e77a5]. I tested it locally, the test case runs much more smoothly. Would you also verify if it works for you?

I think we cannot totally ignore a global failure if another global failover is in progress. Because these global failures can be different ones (with different reasons), which should be recorded and exposed in the exception history, even though we do not want the later one to trigger one more job restart.
;;;","16/Feb/23 10:20;zhuzh;[~huwh] I just see you comments. I think it's mostly answered in my above comment, except for the restart strategy.

Currently, Flink notifies the {{RestartBackoffStrategy}} each time a failure happens. So it is more about how many failures are tolerable, instead of how many restarts are tolerable. This means superseded failures are still counted. This is not aligned with the [description of the restart strategies|https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/state/task_failure_recovery/#task-failure-recovery]. Maybe we can improve it in later versions. However, compatibility issues should be considered for existing jobs.;;;","16/Feb/23 11:54;huwh;[~zhuzh]  Thanks for the explanation, I have one more small question, could we just skip the restoreState when verticesToRestart is empty?;;;","17/Feb/23 04:06;zhuzh;I think yes, or even a wider scope in {{DefaultScheduler#restartTasks()}}. ;;;","17/Feb/23 10:16;huwh;[~dannycranmer] Does it solve your problem?  I can take this ticket (add some ut and do more tests) if you have no PR yet.  [~zhuzh] ;;;","17/Feb/23 12:58;dannycranmer;Hey [~zhuzh] / [~huwh] . Thank-you for the deep dive and explanation. Yes [this change|https://github.com/zhuzhurk/flink/commit/a522edeb4e23ab1ce3f5a34eb3269116723e77a5] solved the problem for me. Thanks [~huwh] for offering to pick up the fix, that is most appreciated. ;;;","18/Feb/23 03:29;zhuzh;I have assigned you the ticket. [~huwh]
Feel free to open a pr for it.;;;","22/Feb/23 07:55;dannycranmer;Merged commit [{{b3e1492}}|https://github.com/apache/flink/commit/b3e14928e815dd6dbdffbe3c5616733d4c7c8825] into apache:master

Merged commit [{{50021a2}}|https://github.com/apache/flink/commit/50021a220261123caa6b31fe1a6938aa03479f17] into apache:release-1.17

Merged commit [{{df3ac1e}}|https://github.com/apache/flink/commit/df3ac1ea706080a0637d6c54574e5f1862d3b80b] into apache:release-1.16 

Merged commit [{{613650f}}|https://github.com/apache/flink/commit/613650fea229816d2a6390c09aa8ea46c74c6555] into apache:release-1.15;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogWithKeyFileStoreTableITCase in table store is not stable,FLINK-31039,13524456,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,zjureel,zjureel,13/Feb/23 10:52,29/Mar/23 02:03,13/Jul/23 08:29,29/Mar/23 02:03,table-store-0.4.0,,,,,,,,,,,,,,Table Store,,,,0,,,,"FAILURE! - in org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase
Error:  testFullCompactionChangelogProducerStreamingRandom  Time elapsed: 600.077 s  <<< ERROR!
org.junit.runners.model.TestTimedOutException: test timed out after 600000 milliseconds
	at java.lang.Thread.sleep(Native Method)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.sleepBeforeRetry(CollectResultFetcher.java:244)
	at org.apache.flink.streaming.api.operators.collect.CollectResultFetcher.next(CollectResultFetcher.java:114)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.nextResultFromFetcher(CollectResultIterator.java:106)
	at org.apache.flink.streaming.api.operators.collect.CollectResultIterator.hasNext(CollectResultIterator.java:80)
	at org.apache.flink.table.planner.connectors.CollectDynamicSink$CloseableRowIteratorWrapper.hasNext(CollectDynamicSink.java:222)
	at org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase.checkFullCompactionTestResult(ChangelogWithKeyFileStoreTableITCase.java:395)
	at org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase.testFullCompactionChangelogProducerRandom(ChangelogWithKeyFileStoreTableITCase.java:343)
	at org.apache.flink.table.store.connector.ChangelogWithKeyFileStoreTableITCase.testFullCompactionChangelogProducerStreamingRandom(ChangelogWithKeyFileStoreTableITCase.java:300)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:299)
	at org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:293)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.lang.Thread.run(Thread.java:750)

[INFO] 
[INFO] Results:
[INFO] 
Error:  Errors: 
Error:    ChangelogWithKeyFileStoreTableITCase.testFullCompactionChangelogProducerStreamingRandom:300->testFullCompactionChangelogProducerRandom:343->checkFullCompactionTestResult:395 » TestTimedOut

https://github.com/apache/flink-table-store/actions/runs/4161755735/jobs/7200106408",,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-13 10:52:45.0,,,,,,,,,,"0|z1fw0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid accessing non-TableStore tables in HiveCatalog.listTables,FLINK-31038,13524447,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,13/Feb/23 10:23,13/Feb/23 12:17,13/Jul/23 08:29,13/Feb/23 12:17,,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"In HiveCatalog.listTables, in the current implementation, getTable will be called for each TableName. However, the environment here may not be able to access non-TableStore tables.
We can avoid access non-TableStore tables by judging whether it is a TableStore table in advance.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 12:17:00 UTC 2023,,,,,,,,,,"0|z1fvyo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 12:17;lzljs3620320;master: bd6036cc1f9cc853e04f45dac7b65b66ffdb669f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StateCheckpointedITCase timed out,FLINK-31036,13524431,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fanrui,mapohl,mapohl,13/Feb/23 09:22,27/Feb/23 10:33,13/Jul/23 08:29,21/Feb/23 10:03,1.17.0,,,,,,1.17.0,1.18.0,,,,,,,Runtime / Checkpointing,Tests,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10608

{code}
""Legacy Source Thread - Source: Custom Source -> Filter (6/12)#69980"" #13718026 prio=5 os_prio=0 tid=0x00007f05f44f0800 nid=0x128157 waiting on condition [0x00007f059feef000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000f0a974e8> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestMemorySegmentBlocking(LocalBufferPool.java:384)
	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.requestBufferBuilderBlocking(LocalBufferPool.java:356)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewBufferBuilderFromPool(BufferWritingResultPartition.java:414)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.requestNewUnicastBufferBuilder(BufferWritingResultPartition.java:390)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.appendUnicastDataForRecordContinuation(BufferWritingResultPartition.java:328)
	at org.apache.flink.runtime.io.network.partition.BufferWritingResultPartition.emitRecord(BufferWritingResultPartition.java:161)
	at org.apache.flink.runtime.io.network.api.writer.RecordWriter.emit(RecordWriter.java:107)
	at org.apache.flink.runtime.io.network.api.writer.ChannelSelectorRecordWriter.emit(ChannelSelectorRecordWriter.java:55)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.pushToRecordWriter(RecordWriterOutput.java:105)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:91)
	at org.apache.flink.streaming.runtime.io.RecordWriterOutput.collect(RecordWriterOutput.java:45)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:59)
	at org.apache.flink.streaming.api.operators.CountingOutput.collect(CountingOutput.java:31)
	at org.apache.flink.streaming.api.operators.StreamFilter.processElement(StreamFilter.java:39)
	at org.apache.flink.streaming.runtime.io.RecordProcessorUtils$$Lambda$1311/1256184070.accept(Unknown Source)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.pushToOperator(CopyingChainingOutput.java:75)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:50)
	at org.apache.flink.streaming.runtime.tasks.CopyingChainingOutput.collect(CopyingChainingOutput.java:29)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$ManualWatermarkContext.processAndCollect(StreamSourceContexts.java:418)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$WatermarkContext.collect(StreamSourceContexts.java:513)
	- locked <0x00000000d55035c0> (a java.lang.Object)
	at org.apache.flink.streaming.api.operators.StreamSourceContexts$SwitchingOnClose.collect(StreamSourceContexts.java:103)
	at org.apache.flink.test.checkpointing.StateCheckpointedITCase$StringGeneratingSourceFunction.run(StateCheckpointedITCase.java:178)
	- locked <0x00000000d55035c0> (a java.lang.Object)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:110)
	at org.apache.flink.streaming.api.operators.StreamSource.run(StreamSource.java:67)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:333)
{code}",,fanrui,Feifan Wang,mapohl,pnowojski,roman,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-26803,,,,,,FLINK-31138,,,,,"16/Feb/23 12:29;fanrui;image-2023-02-16-20-29-52-050.png;https://issues.apache.org/jira/secure/attachment/13055511/image-2023-02-16-20-29-52-050.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 20 15:16:08 UTC 2023,,,,,,,,,,"0|z1fvv4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 09:25;mapohl;[~roman] [~ym] can you help identifying the cause? I struggle to pin-point the change that might have caused the issue.;;;","13/Feb/23 09:56;roman;Thanks [~mapohl] ,

I briefly looked at the issue and it'ss related to State Backends, but rather to Unaligned Checkpoints (channel state writer).

 

Also not sure if this is a deadlock:
 * Legacy Source Thread locked 0x00000000a29b0ae8 (""checkpoint lock"") 
 * multiple instances of ChannelStateWriteRequestExecutorFactory are trying to acquire it (as part of task initialization)

The latter was introduced in FLINK-26803.

[~fanrui] I beleive you have more context, could you please take a look?

cc: [~pnowojski] ;;;","13/Feb/23 10:19;fanrui;Thanks [~mapohl]  reports this issue, and thanks [~roman] help analyze it, I will take a look asap~;;;","14/Feb/23 10:44;fanrui;Hi [~mapohl] [~roman] , I'm sorry, after analysis, I didn't find the root cause, I just suspect it's not caused by FLINK-26803.

From jstack we can know that StateCheckpointedITCase has timed out. The logic of this test is: OnceFailingAggregator fails after running for a while, and then tests whether the checkpoint meets expectations. 

I suspect that the test has encountered some problems and is restarting frequently, so the tasks are frequently initialized. FLINK-26803 added a lock in TM level during task is initializing, so we see that many tasks are waiting for locks.

I want to check the log of the test to analyze what these tasks are doing, however I didn't found it here[1]. Whether the log is too big due to it runs for 4 hours? And I checked the upload log[2], it can't be shown.

 

Please correct me if I'm wrong. BTW, I run this test locally and it always succeeds. Does this test fail only once?

 

[1] https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=artifacts&pathAsName=false&type=publishedArtifacts

[2]https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=8856ec62-ca1b-53d4-5e1a-5db3770f6c8a;;;","14/Feb/23 10:55;mapohl;{quote}
BTW, I run this test locally and it always succeeds. Does this test fail only once?
{quote}

so far, I'm only aware of this one test failure.;;;","15/Feb/23 02:14;fanrui;{quote} I'm only aware of this one test failure.
{quote}
Thanks for your feedback, I can continue the analysis  after it happens again and hope the log can be viewed next time.

Or if other masters have some ideas, welcome to give feedback here.;;;","15/Feb/23 15:07;mapohl;Why would you assume to get more from the logs than what is already offered in the most-recent test failure? Did you add more log messages? I'm just wondering because there's no PR opened for that issue, yet.;;;","16/Feb/23 03:08;fanrui;{quote}Why would you assume to get more from the logs than what is already offered in the most-recent test failure?
{quote}
I suspect that the test has encountered some problems and is restarting frequently, so the tasks are frequently initialized. FLINK-26803 added a lock at TM level during task is initializing, so we see that many tasks are waiting for locks. 

I want to look at the logs of the failed tests to analyze what those tasks are doing, and what's wrong with this test?

 
{quote}Did you add more log messages?
{quote}
 

I didn't add any log, the current log is enough. We can use the log to confirm whether the test job has been restarting and the reason for restarting.

 

I have run it dozens of times locally, but I can't reproduce it. If it can be reproduced and has a log, it should be helpful to find the root cause.

 ;;;","16/Feb/23 07:33;mapohl;{quote}
FLINK-26803 added a lock at TM level during task is initializing, so we see that many tasks are waiting for locks. I want to look at the logs of the failed tests to analyze what those tasks are doing, and what's wrong with this test?
{quote}
I see. But I still miss to understand why the current stacktrace isn't enough to do so when we don't add more logs and are fine with what the current logs are revealing?
The reason I am asking is because we might want to come up with a strategy if the issue cannot be resolved before the rc creation is started (because it might not appear that frequent). Reverting FLINK-26803 by then sounds reasonable considering that it's ""only"" an improvement. WDYT?;;;","16/Feb/23 07:48;fanrui;Hi, [~pnowojski] , do you have time help take a look this JIRA?

I have analyzed the stack, but I didn't find why `StateCheckpointedITCase` failed, thanks a lot.:);;;","16/Feb/23 08:25;fanrui;{quote}Reverting FLINK-26803 by then sounds reasonable considering that it's ""only"" an improvement. WDYT?
{quote}
Hi [~mapohl] , after my analysis, any bugs that cause jobs to be restarted or checkpoints not to be recovered can cause this stacktrace. This stack runs on FLINK-26803 code, but may not be caused by FLINK-26803.

Has flink-1.17 or master branch fixed some bugs that caused jobs to be restarted or checkpoints not to be restored?

 

BTW, StateCheckpointedITCase also enabled the ChangeLogStateBackend [1],  I see FLINK-28440 and FLINK-30561 is merged recently. It is possible that the root cause has been resolved, and StateCheckpointedITCase will always succeeds in the future.

So after reverting FLINK-26803, if StateCheckpointedITCase always succeeds, it does not prove that FLINK-26803 caused the problem. WDYT?

cc [~pnowojski] [~roman] 

Please correct me if I'm wrong.

[1] https://github.com/apache/flink/blob/b64739a5ef976e003bf87250b41ae1142e541497/flink-tests/src/test/java/org/apache/flink/test/checkpointing/StreamFaultToleranceTestBase.java#L91

 ;;;","16/Feb/23 08:34;fanrui;I see that StateCheckpointedITCase fails when this commit[1] is merged.

I have an idea: Based on this commit, use a script to run StateCheckpointedITCase(with LOG enabled) repeatedly to see if it can be reproduced. I'll update here when I come to any conclusions.

[1]https://github.com/flink-ci/flink-mirror/commit/331abad6dd2d6ec79ecde7116f3b48f2a249b9ef;;;","16/Feb/23 08:55;mapohl;Is this a copy&paste error? The commit [331abad6d|https://github.com/flink-ci/flink-mirror/commit/331abad6dd2d6ec79ecde7116f3b48f2a249b9ef] points to a docs change in {{flink-ci/flink-mirror}};;;","16/Feb/23 09:05;fanrui;{quote}Is this a copy&paste error? The commit [331abad6d|https://github.com/flink-ci/flink-mirror/commit/331abad6dd2d6ec79ecde7116f3b48f2a249b9ef] points to a docs change in {{flink-ci/flink-mirror}}
{quote}
No, I copied the commit from the failed CI [1], I'm not saying this commit is the root cause, I mean: root cause should be before this commit, so I run the StateCheckpointedITCase based on this commit to troubleshoot.

 

[1] [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10608];;;","16/Feb/23 09:43;mapohl;Looks like it's not really a one-time thing:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10608;;;","16/Feb/23 10:09;fanrui;{quote}Looks like it's not really a one-time thing:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=39d5b1d5-3b41-54dc-6458-1e2ddd1cdcf3&t=0c010d0c-3dec-5bf1-d408-7b18988b1b2b&l=10608]
{quote}
It’s your first link, right?;;;","16/Feb/23 10:26;mapohl;Yes, sorry for the confusion. I must have ran into a copy&paste error when comparing the builds. Here's the actual one:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46199&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=11651
{code}
""main"" #1 prio=5 os_prio=0 tid=0x00007fce4000b800 nid=0x28152 waiting on condition [0x00007fce478dc000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x00000000a468c860> (a java.util.concurrent.CompletableFuture$Signaller)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3323)
	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.test.util.TestUtils.submitJobAndWaitForResult(TestUtils.java:99)
	at org.apache.flink.test.checkpointing.StreamFaultToleranceTestBase.runCheckpointedProgram(StreamFaultToleranceTestBase.java:136)
[...]
{code}

It doesn't state the actual ITCase in the stacktrace. {{StateCheckpointedITCase}} [is started|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46199&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=10020] but never finished based on the Maven output.;;;","16/Feb/23 11:12;pnowojski;I will try to take a quick look.

When trying to reproduce it locally, keep in mind that we have some parameters/configuraiton randomisation implemented AFAIR based on the git commit. Most likely one of the parameters that gets randomised is unaligned checkpoints turned on/off, so if the failures are happening only in one of those modes, make sure that locally you are using the same setting. If it's randomised based on the git commit hash, then it's probably best to just loop the test on the same commit that has failed in the CI. 

Sometimes it also helps to stress the local machine much more, like loop the same test 4 or 8 times running in parallel. ;;;","16/Feb/23 12:34;fanrui;{quote}Here's the actual one:
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46199&view=logs&j=4d4a0d10-fca2-5507-8eed-c07f0bdf4887&t=7b25afdf-cc6c-566f-5459-359dc2585798&l=11651]
{quote}
Thanks a lot for the link, it's very very useful, It has log. The exception is `java.lang.RuntimeException: Test failed due to unexpected recovered state size 0`. I will continue to analyze why the restored state is wrong. 

!image-2023-02-16-20-29-52-050.png!;;;","16/Feb/23 16:26;pnowojski;In this recent log I see two issues

# {{checkpoint 20}} is failing due to ""Size of the state is larger than the maximum permitted memory-backed state. Size=5621456, maxSize=5242880. Consider using a different checkpoint storage, like the FileSystemCheckpointStorage""
# recovery from {{checkpoint 19}} is failing because ""java.lang.RuntimeException: Test failed due to unexpected recovered state size 0""



# Is probably caused by FLINK-26803, probably a benign configuration issue
# is just a minor bug/unsupported case in this test, since shortly before {{checkpoint 19}}, some tasks have finished. {{StateCheckpointedITCase.StringRichFilterFunction#restoreState}} simply doesn't support that. This test was created before FLIP-147 and doesn't expect the second failover caused by the 1.;;;","17/Feb/23 03:51;fanrui;Hi [~pnowojski] , thanks a lot for the analysis, sounds makes sense to me.
{quote}{{checkpoint 20}} is failing due to ""Size of the state is larger than the maximum permitted memory-backed state. Size=5621456, maxSize=5242880. Consider using a different checkpoint storage, like the FileSystemCheckpointStorage""

Is probably caused by FLINK-26803, probably a benign configuration issue
{quote}
The maxSize of memory checkpoint stream cannot be changed, and I see some comments in the `JobManagerCheckpointStorage`, it means we cannot increase the maxSize due to the limitation in the PRC side.
{code:java}
<p><b>WARNING:</b> Increasing the size of this value beyond the default value ({@value
* #DEFAULT_MAX_STATE_SIZE}) should be done with care. The checkpointed state needs to be send
* to the JobManager via limited size RPC messages, and there and the JobManager needs to be
* able to hold all aggregated state in its memory.{code}
Also, I think all tests may have this problem, especially some tests with high parallelism. It is not introduced by FLINK-26803. If the test of the high parallelism job does not merge the channel state file, this problem may also exist.

So should we use the `FileSystemCheckpointStorage` instead of `JobManagerCheckpointStorage`?

 
{quote}Recovery from {{checkpoint 19}} is failing because ""java.lang.RuntimeException: Test failed due to unexpected recovered state size 0""

Is just a minor bug/unsupported case in this test, since shortly before {{{}checkpoint 19{}}}, some tasks have finished. {{StateCheckpointedITCase.StringRichFilterFunction#restoreState}} simply doesn't support that. This test was created before FLIP-147 and doesn't expect the second failover caused by the 1.
{quote}
As I understand, when problem 1 is solved, this problem will also be solved incidentally.

However, I'm not sure if these tests are robust after FLIP-147. When OnceFailingAggregator throw exception after some source tasks are finished, StateCheckpointedITCase will also fail(This is more likely to happen when the dataset is small.). Therefore, I guess there may be many old checkpoint-related tests that have similar risks after FLIP-147.;;;","17/Feb/23 07:56;pnowojski;{quote}
So should we use the `FileSystemCheckpointStorage` instead of `JobManagerCheckpointStorage`?
{quote}
Indeed that seems like the only feasible solution. Do you mean setting it for this test, or for all of the unit tests? ITCases? What would be the scope of such change?

Re the 2nd problem, I think indeed the problem should go away. Nevertheless it would be nice if there is an easy way to fix this in  {{StateCheckpointedITCase}}, but that's I think optional.;;;","20/Feb/23 14:37;mapohl;Is FLINK-31138 a duplicate of this Jira issue?;;;","20/Feb/23 14:47;fanrui;merged commit ae53a8de47b31e248fef216428a83c8bf3f07b13 into apache:release-1.17

merged commit 6fae8b58a49216d50e422d5e4e7be3d7ea3b4462 into apache:master;;;","20/Feb/23 15:16;fanrui;{quote}Is FLINK-31138 a duplicate of this Jira issue?
{quote}
Hi [~mapohl] , I did a quick checked, it isn't similar to this Jira, I have commented there. But I don't find the root cause now, I can continue analyze it tomorrow.;;;",,,,,,,,,,,,,,,,,,,,,,,,
KBinsDiscretizer gives wrong bin edges in 'quantile' strategy when input data contains only 2 distinct values,FLINK-31029,13524405,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,hongfanxo,hongfanxo,13/Feb/23 06:23,01/Apr/23 02:52,13/Jul/23 08:29,01/Apr/23 02:52,,,,,,,,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"When one input column contains only 2 distinct values and their counts are same, KBinsDiscretizer transforms this column to all 0s using `quantile` strategy. An example of such column is `[0, 0, 0, 1, 1, 1]`.

When the 2 distinct values have different counts, the transformed values are also all 0s, which cannot distinguish them.",,hongfanxo,lindong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Apr 01 02:52:37 UTC 2023,,,,,,,,,,"0|z1fvp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Apr/23 02:52;lindong;Merged to apache/flink-ml master branch 5dacbd97429a525b0f7e81931f55f3d87f79de57;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KBinsDiscretizer gives wrong bin edges when all values are same.,FLINK-31026,13524391,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hongfanxo,hongfanxo,hongfanxo,13/Feb/23 04:02,16/Feb/23 11:38,13/Jul/23 08:29,16/Feb/23 11:38,,,,,,,ml-2.2.0,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"Current implements gives bin edges of \{Double.MIN_VALUE, Double.MAX_VALUE} when all values are same.
However, this bin cannot cover negative values and 0.",,hongfanxo,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 11:38:04 UTC 2023,,,,,,,,,,"0|z1fvm0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 11:38;zhangzp;Fixed on master via 0af95f256d438a12946f9152c0a65ec916a61323;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Early-started partial match timeout not yield completed matches,FLINK-31017,13524233,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,10/Feb/23 14:35,17/Feb/23 08:29,13/Jul/23 08:29,17/Feb/23 08:28,1.16.0,,,,,,1.16.2,1.17.0,,,,,,,Library / CEP,,,,0,pull-request-available,,,"Pattern example:
{code:java}
Pattern.begin(""A"").where(startsWith(""a"")).oneOrMore().consecutive().greedy()
    .followedBy(""B"")
    .where(count(""A"") > 2 ? startsWith(""b"") : startsWith(""c""))
    .within(Time.seconds(3));{code}
Sequence example, currently without any output:

a1 a2 a3 a4 c1

When match[a3, a4, c1] completes, partial match[a1, a2, a3, a4] is earlier, so NFA#processMatchesAccordingToSkipStrategy() won't give any result, which is the expected behavior. However, when partial match[a1, a2, a3, a4] is timed-out, completed match[a3, a4, c1] should be ""freed"" from NFAState to output.",,dianfu,Juntao Hu,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 08:28:07 UTC 2023,,,,,,,,,,"0|z1fun4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 15:11;martijnvisser;[~Juntao Hu] Are you committing to fixing this in 1.18? Else I'll remove the fixVersion for now, because that should only be set if indeed it's expected that it will make that version;;;","17/Feb/23 07:33;Juntao Hu;[~martijnvisser] I've changed the tags, thx for reminding;;;","17/Feb/23 08:28;dianfu;Merged to:
- master: 919d46ef91488627a282d1474208490b6dc7a820
- 1.17: 257be14bb47f7481927ea17bc82b4b6ff63e8b30
- 1.16: d61e4a1884e265e0a33f0f4dbad50898df14677f;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Session window aggregation cannot trigger window using event time,FLINK-31013,13524188,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,qingyue,qingyue,10/Feb/23 09:52,29/Mar/23 02:00,13/Jul/23 08:29,29/Mar/23 02:00,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,,,,"{code:sql}
-- test against Flink 1.16.0

create catalog fscat with (
    'type' = 'table-store',
    'warehouse' = 'file:///tmp/fscat'
);


use catalog fscat;
create table events (
  `id` int, 
  `type` string, 
  `date` TIMESTAMP(3), 
  watermark for `date` AS `date`);
  
insert into events 
values (1, 'T1', to_timestamp('2018-01-24', 'yyyy-MM-dd')), 
(2, 'T1', to_timestamp('2018-01-26', 'yyyy-MM-dd')), 
(1, 'T2', to_timestamp('2018-01-28', 'yyyy-MM-dd')), 
(1, 'T2', to_timestamp('2018-01-28', 'yyyy-MM-dd'));  

-- no output
select `id`,
    `type`, 
    COUNT(1) as event_cnt, 
    session_start(`date`, interval '1' DAY) as ss, 
    session_end(`date`, interval '1' DAY) as se 
from events group by `id`, `type`, session(`date`, interval '1' DAY); 

-- explain plan
== Abstract Syntax Tree ==
LogicalProject(id=[$0], type=[$1], event_cnt=[$3], ss=[SESSION_START($2)], se=[SESSION_END($2)])
+- LogicalAggregate(group=[{0, 1, 2}], event_cnt=[COUNT()])
   +- LogicalProject(id=[$0], type=[$1], $f2=[$SESSION($2, 86400000:INTERVAL DAY)])
      +- LogicalWatermarkAssigner(rowtime=[date], watermark=[$2])
         +- LogicalTableScan(table=[[fscat, default, events]])


== Optimized Physical Plan ==
Calc(select=[id, type, event_cnt, w$start AS ss, w$end AS se])
+- GroupWindowAggregate(groupBy=[id, type], window=[SessionGroupWindow('w$, date, 86400000)], properties=[w$start, w$end, w$rowtime, w$proctime], select=[id, type, COUNT(*) AS event_cnt, start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime])
   +- Exchange(distribution=[hash[id, type]])
      +- TableSourceScan(table=[[fscat, default, events, watermark=[date]]], fields=[id, type, date])


== Optimized Execution Plan ==
Calc(select=[id, type, event_cnt, w$start AS ss, w$end AS se])
+- GroupWindowAggregate(groupBy=[id, type], window=[SessionGroupWindow('w$, date, 86400000)], properties=[w$start, w$end, w$rowtime, w$proctime], select=[id, type, COUNT(*) AS event_cnt, start('w$) AS w$start, end('w$) AS w$end, rowtime('w$) AS w$rowtime, proctime('w$) AS w$proctime])
   +- Exchange(distribution=[hash[id, type]])
      +- TableSourceScan(table=[[fscat, default, events, watermark=[date]]], fields=[id, type, date])

-- however, if switch to filesystem source, the window can be triggered normally

CREATE TEMPORARY TABLE `fscat`.`default`.`event_file_source` (
  `id` INT,
  `type` VARCHAR(2147483647),
  `date` TIMESTAMP(3),
  WATERMARK FOR `date` AS `date`
) WITH (
  'format' = 'csv',
  'path' = '/tmp/events.csv',
  'source.monitor-interval' = '1 min',
  'connector' = 'filesystem'
);

// cat events.csv                                
1,T1,2018-01-24 00:00:00.000
2,T1,2018-01-26 00:00:00.000
1,T2,2018-01-28 00:00:00.000
1,T2,2018-01-28 00:00:00.000


-- same query using filesystem source
select `id`, `type`, COUNT(1) as event_cnt, session_start(`date`, interval '1' DAY) as ss, session_end(`date`, interval '1' DAY) as se from event_file_source group by `id`, `type`, session(`date`, interval '1' DAY);

-- output

          id                           type            event_cnt                      ss                      se
           1                             T1                    1 2018-01-24 00:00:00.000 2018-01-25 00:00:00.000
           2                             T1                    1 2018-01-26 00:00:00.000 2018-01-27 00:00:00.000{code}",,qingyue,xzw0223,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 06:49:49 UTC 2023,,,,,,,,,,"0|z1fud4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 01:43;xzw0223;I refer you to the documentation [https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/dev/table/sql/queries/window-agg/#group-window-aggregation]  on Windows

Instead of dividing the window by the specified interval, the sessionwindow will fire only after the specified interval window has no data and is inactive.

 

This is your data
1,T1,2018-01-24 00:00:00.000
2,T1,2018-01-26 00:00:00.000
1,T2,2018-01-28 00:00:00.000
1,T2,2018-01-28 00:00:00.000
 

When the first data is accepted and the second data is received, the second data event time is longer than the activity interval of the window, then the window will be triggered.

When three pieces of data are received, his event event is greater than the window interval, triggering the window calculation.

When the fourth piece of data is received, it has the same time as the third piece of data, and they will be divided into the same sessionwindow. Since no new data is continuously received, the sessionwindow will not trigger calculation until the received data It will only be triggered when the event time of is greater than the window interval.

 

 ;;;","17/Feb/23 06:49;qingyue;[~xzw0223] What's your point?


According to the table schema
||id||type||date||
|1|T1|2018-01-24 00:00:00.000|
|2|T1|2018-01-26 00:00:00.000|
|1|T2|2018-01-28 00:00:00.000|
|1|T2|2018-01-28 00:00:00.000|

 
And the query with a session window gap of 1 DAY, the first two windows should be triggered.
{code:sql}
select `id`,
    `type`, 
    COUNT(1) as event_cnt, 
    session_start(`date`, interval '1' DAY) as ss, 
    session_end(`date`, interval '1' DAY) as se 
from events group by `id`, `type`, session(`date`, interval '1' DAY); 
{code}
 

Expected Output
||id||type||event_cnt||ss||se||
|1|T1|1|2018-01-24 00:00:00.000|2018-01-25 00:00:00.000|
|2|T1|1|2018-01-26 00:00:00.000|2018-01-27 00:00:00.000|

 

Table Store Actual Output

No output;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update docs for files table in table store,FLINK-31012,13524187,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,10/Feb/23 09:44,10/Feb/23 10:42,13/Jul/23 08:29,10/Feb/23 10:42,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,Update docs to add partition,,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 10:42:41 UTC 2023,,,,,,,,,,"0|z1fucw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 10:42;lzljs3620320;master: d325dcef41c6d3dc815189983abded5cf1b19830;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Flink][Table Store] The Split allocation of the same bucket in ContinuousFileSplitEnumerator may be out of order,FLINK-31008,13524166,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Ming Li,Ming Li,Ming Li,10/Feb/23 07:27,13/Feb/23 11:31,13/Jul/23 08:29,13/Feb/23 11:31,,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"There are two places in {{ContinuousFileSplitEnumerator}} that add {{FileStoreSourceSplit}} to {{{}bucketSplits{}}}: {{addSplitsBack}} and {{{}processDiscoveredSplits{}}}. {{processDiscoveredSplits}} will continuously check for new splits and add them to the queue.  At this time, the order of the splits is in order.
{code:java}
private void addSplits(Collection<FileStoreSourceSplit> splits) {
    splits.forEach(this::addSplit);
}

private void addSplit(FileStoreSourceSplit split) {
    bucketSplits
            .computeIfAbsent(((DataSplit) split.split()).bucket(), i -> new LinkedList<>())
            .add(split);
}{code}
However, when the task failover, the splits that have been allocated before will be returned. At this time, these returned splits are also added to the end of the queue, which leads to disorder in the allocation of splits.

 

I think these returned splits should be added to the head of the queue to ensure the order of allocation.",,lzljs3620320,Ming Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 11:31:13 UTC 2023,,,,,,,,,,"0|z1fu88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 07:35;Ming Li;hi, [~lzljs3620320], if you have time, please help to take a look at this issue, thank you.;;;","10/Feb/23 07:38;lzljs3620320;[~Ming Li] Thanks for reporting!
Wow, you're right. This should be a blocker issue.
Do you want to contribute this jira? ;;;","10/Feb/23 07:43;Ming Li;[~lzljs3620320] Yes, thanks a lot, I've been doing some related work recently that I think will help fix this issue.

In addition, if you have time, please help to take a look at this issue([FLINK-30985|https://issues.apache.org/jira/browse/FLINK-30985]), I think it is also helpful for split allocation. ;;;","10/Feb/23 08:59;lzljs3620320;[~Ming Li] Assigned to u~;;;","10/Feb/23 12:06;Ming Li;[~lzljs3620320] I have created a pull request, please review it if you have time. Thanks.;;;","13/Feb/23 11:31;lzljs3620320;master: e9875b4d0bb9d3ccd05b51349ffe7a1ea78510e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configuration table.exec.spill-compression.block-size not take effect in batch job,FLINK-30989,13523959,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,shenjiaqi,shenjiaqi,09/Feb/23 11:59,07/Apr/23 08:07,13/Jul/23 08:29,07/Apr/23 03:05,1.16.1,,,,,,1.16.2,1.17.1,1.18.0,,,,,,Runtime / Configuration,Table SQL / Runtime,,,0,pull-request-available,,,"h1. Description

I tried to config table.exec.spill-compression.block-size in TableEnv in my job and failed. I  attached to TaskManager and found conf passed to constructor of [BinaryExternalSorter|https://github.com/apache/flink/blob/release-1.16.1/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sort/BinaryExternalSorter.java#L204] is empty:

!image-2023-02-09-19-37-44-927.png|width=306,height=185!
h1. How to reproduce

A simple code to reproduce this problem:
{code:java}
// App.java

package test.flink403;

import static org.apache.flink.configuration.ExecutionOptions.RUNTIME_MODE;
import static org.apache.flink.table.api.config.ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE;

import org.apache.flink.api.common.RuntimeExecutionMode;
import org.apache.flink.configuration.AlgorithmOptions;
import org.apache.flink.configuration.Configuration;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.table.api.Table;
import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;
import org.apache.flink.table.api.config.ExecutionConfigOptions;

import java.util.Arrays; public class App {

  public static void main(String argc[]) throws Exception {

    Configuration config = new Configuration();
    config.set(RUNTIME_MODE, RuntimeExecutionMode.BATCH);
    config.set(ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_ENABLED, true);
    config.set(AlgorithmOptions.HASH_JOIN_BLOOM_FILTERS, true);
    config.setString(TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE.key(), ""32 m""); // <---- cannot take effect
    config.set(AlgorithmOptions.SORT_SPILLING_THRESHOLD, Float.valueOf(0.5f));
    final StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment(1, config);

    final StreamTableEnvironment tableEnv = StreamTableEnvironment.create(env);
    tableEnv.getConfig().set(""table.exec.spill-compression.block-size"", ""32 m""); // <---- cannot take effect
    final DataStream<Order> orderA =
        env.fromCollection(
            Arrays.asList(
                new Order(1L, ""beer"", 3),
                new Order(1L, ""diaper"", 4),
                new Order(3L, ""rubber"", 2)));

    final Table tableA = tableEnv.fromDataStream(orderA);

    final Table result =
        tableEnv.sqlQuery(
            ""SELECT * FROM ""
                + tableA
                + "" ""
                + "" order by user"");

    tableEnv.toDataStream(result, Order.class).print();
    env.execute();
  }
}

// ---------------------------------------------------------------
// Order.java
package test.flink403;

public class Order {
  public Long user;
  public String product;
  public int amount;

  // for POJO detection in DataStream API
  public Order() {}

  // for structured type detection in Table API
  public Order(Long user, String product, int amount) {
    this.user = user;
    this.product = product;
    this.amount = amount;
  }

  @Override
  public String toString() {
    return ""Order{""
        + ""user=""
        + user
        + "", product='""
        + product
        + '\''
        + "", amount=""
        + amount
        + '}';
  }
}{code}
 

I think it is because [SortOperator|https://github.com/apache/flink/blob/release-1.16.1/flink-table/flink-table-runtime/src/main/java/org/apache/flink/table/runtime/operators/sort/SortOperator.java#L88] try to get conf from JobConfiguration, which should be set in JobGraph. 
Following are the Classes use the same method to get conf from JobConfiguration:
 * BinaryExternalSorter
 ** ExecutionConfigOptions.TABLE_EXEC_SORT_ASYNC_MERGE_ENABLED
 ** ExecutionConfigOptions.TABLE_EXEC_SORT_MAX_NUM_FILE_HANDLES
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_ENABLED
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE
 * BinaryHashTable，BaseHybridHashTable
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_ENABLED
 ** ExecutionConfigOptions.TABLE_EXEC_SPILL_COMPRESSION_BLOCK_SIZE
 * SortDataInput
 ** AlgorithmOptions.SORT_SPILLING_THRESHOLD
 ** AlgorithmOptions.SPILLING_MAX_FAN
 ** AlgorithmOptions.USE_LARGE_RECORDS_HANDLER",,godfrey,lsy,shenjiaqi,wanglijie,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/23 11:37;shenjiaqi;image-2023-02-09-19-37-44-927.png;https://issues.apache.org/jira/secure/attachment/13055305/image-2023-02-09-19-37-44-927.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 07 08:07:18 UTC 2023,,,,,,,,,,"0|z1fsyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 04:29;Weijie Guo;cc [~lsy] ;;;","17/Feb/23 05:50;lsy;Thanks for repport it. For the table module, I will take a look.;;;","17/Feb/23 07:45;Weijie Guo;After offline discuss, the configuration does not take effect in both the streaming module and the table module. [~lsy] want to fix the table module, the others leave to me.;;;","02/Mar/23 13:31;godfrey;Fixed in

1.18.0: b4d43b47c993b7b4d5e4f7a78610c54124fcbcb4

1.17.0: 333088113993f4607038dae391863b5c30d0bc95;;;","04/Apr/23 09:59;Weijie Guo;[~lsy] Can you confirm whether the changes related to table part need backport to release-1.16 branch?;;;","05/Apr/23 14:23;Weijie Guo;Summary:

Fixed in streaming module:
master(1.18) via ccd0fe2d75a26a158ad64ab25bb5063a7031d428.
release-1.17 via 40e9501a5fcd7a71af4a7e79cd1556e190488137
release-1.16 via 75fab2759a1a2a2664d6b9f4a006a56f1a65d2fe.

Fixed in table module:
master(1.18) via b4d43b47c993b7b4d5e4f7a78610c54124fcbcb4.
release-1.17 via 333088113993f4607038dae391863b5c30d0bc95.;;;","07/Apr/23 08:07;lsy;[~Weijie Guo] Yes, we need to pick it back to release-1.16.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
the security.ssl.algorithms configuration does not take effect in rest ssl,FLINK-30983,13523913,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,tanyuxin,lyssg,lyssg,09/Feb/23 07:59,08/Mar/23 03:36,13/Jul/23 08:29,06/Mar/23 05:46,1.16.0,,,,,,1.17.0,1.18.0,,,,,,,Runtime / Network,,,,0,pull-request-available,,,"The security.ssl.algorithms configuration does not take effect in rest ssl.

 

SSLUtils#createRestNettySSLContext does not call SslContextBuilder#ciphers as  SSLUtils#createInternalNettySSLContext.

!image-2023-02-09-15-58-36-254.png!

 

!image-2023-02-09-15-58-43-963.png!",,lyssg,tanyuxin,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/23 07:58;lyssg;image-2023-02-09-15-58-36-254.png;https://issues.apache.org/jira/secure/attachment/13055289/image-2023-02-09-15-58-36-254.png","09/Feb/23 07:58;lyssg;image-2023-02-09-15-58-43-963.png;https://issues.apache.org/jira/secure/attachment/13055288/image-2023-02-09-15-58-43-963.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 05:46:24 UTC 2023,,,,,,,,,,"0|z1fso8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"15/Feb/23 06:10;tanyuxin;[~lyssg] Hi, I will take a look at the issue.;;;","06/Mar/23 05:46;Weijie Guo;master(1.18) via 84f532e65498164bc03529dc387a852f0e18d31d.
release-1.17 via 8ff09cbb9a1bef6a5b6644e9d06035663f10535b.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
explain_sql throws java method not exist,FLINK-30981,13523900,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,09/Feb/23 07:19,13/Feb/23 03:38,13/Jul/23 08:29,13/Feb/23 03:38,1.17.0,,,,,,1.17.0,,,,,,,,API / Python,,,,0,pull-request-available,,,"Execute `t_env.explainSql(""ANY VALID SQL"")` will throw error:
{code:java}
Traceback (most recent call last):
  File ""ISSUE/FLINK-25622.py"", line 42, in <module>
    main()
  File ""ISSUE/FLINK-25622.py"", line 34, in main
    print(t_env.explain_sql(
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 799, in explain_sql
    return self._j_tenv.explainSql(stmt, j_extra_details)
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/py4j/java_gateway.py"", line 1322, in __call__
    return_value = get_return_value(
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/util/exceptions.py"", line 146, in deco
    return f(*a, **kw)
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/py4j/protocol.py"", line 330, in get_return_value
    raise Py4JError(
py4j.protocol.Py4JError: An error occurred while calling o11.explainSql. Trace:
org.apache.flink.api.python.shaded.py4j.Py4JException: Method explainSql([class java.lang.String, class [Lorg.apache.flink.table.api.ExplainDetail;]) does not exist
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:321)
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:329)
    at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:274)
    at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
    at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.base/java.lang.Thread.run(Thread.java:829) {code}
[30668|https://issues.apache.org/jira/browse/FLINK-30668] changed TableEnvironment#explainSql to an interface default method, while both TableEnvironmentInternal and TableEnvironmentImpl not overwriting it, it triggers a bug in py4j, see [https://github.com/py4j/py4j/issues/506] .",,dianfu,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 03:38:14 UTC 2023,,,,,,,,,,"0|z1fslc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 03:38;dianfu;Fixed in:
- master via a92892fea747f81f0e8a6cd4ec4ee207c95fa625
- release-1.17 via c33ee8decd7733436bec3ed102c6422e9083c558;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutorImplITCase.testInterruptExecution hangs waiting for SQL gateway service closing,FLINK-30978,13523894,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,renqs,renqs,09/Feb/23 06:52,06/Mar/23 10:17,13/Jul/23 08:29,06/Mar/23 10:17,1.17.0,,,,,,1.17.0,1.18.0,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,test-stability,,https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45921&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=44674,,fsk119,mapohl,renqs,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Mar 03 12:28:04 UTC 2023,,,,,,,,,,"0|z1fsk0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"20/Feb/23 09:20;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46290&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15744

{code}
Feb 19 02:06:14 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007f38910e8000 nid=0x2250 waiting on condition [0x00007f3798588000]
Feb 19 02:06:14    java.lang.Thread.State: TIMED_WAITING (sleeping)
Feb 19 02:06:14 	at java.lang.Thread.sleep(Native Method)
Feb 19 02:06:14 	at org.apache.flink.runtime.testutils.CommonTestUtils.waitUntilCondition(CommonTestUtils.java:151)
Feb 19 02:06:14 	at org.apache.flink.table.client.gateway.ExecutorImplITCase.testInterrupting(ExecutorImplITCase.java:626)
Feb 19 02:06:14 	at org.apache.flink.table.client.gateway.ExecutorImplITCase.testInterruptExecution(ExecutorImplITCase.java:513)
Feb 19 02:06:14 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
Feb 19 02:06:14 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
Feb 19 02:06:14 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
{code};;;","20/Feb/23 09:22;mapohl;[~fsk119] I'm increasing the priority for that one to blocker. May you provide details on whether that's a test code issue or actually relevant for production?;;;","27/Feb/23 10:20;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46558&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=18102;;;","28/Feb/23 08:11;fsk119;Hello [~mapohl]. Thanks for sharing this with me. It's very wired to me the case fails. I need to add some logs about this part and test it in my private pipeline. But I don't think it's a blocking issue here because the failed tests only tell us the `Executor` fails to close the Operation. But the Gateway will automatically detect the session whether are idle and close used resources by the idle session. ;;;","01/Mar/23 07:21;fsk119;After discussing with colleagues, I find the problem is because the `RestClient#sendRequest` and `CompletableFuture#get` are not atomic. The Client may send the request and the Server returns the response immediately before  ExecutorImpl tries to invoke `CompletableFuture#get`. In this case, the Executor will not check the interrupted flag on the thread and continue execution here. 

I will fix this after I figure out other blocking issues.;;;","01/Mar/23 08:36;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46643&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=16997;;;","02/Mar/23 10:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46687&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=18050;;;","03/Mar/23 12:28;fsk119;Merged into master: d96bb2f66d71fecdc5dba183ad04c9ba75e40845
Merged into release-1.17: 5dddc0dba2be20806e67769314eecadf56b87a53;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink tumbling window stream converting to pandas dataframe not work,FLINK-30977,13523893,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Joekwal,Joekwal,09/Feb/23 06:50,02/Mar/23 09:04,13/Jul/23 08:29,02/Mar/23 09:04,,,,,,,,,,,,,,,API / Python,,,,0,,,,"I want to know if tumbling window supported to convert to pandas?
{code:java}
code... #create env

kafka_src = """"""
CREATE TABLE if not exists `kafka_src` (
...
`event_time` as CAST(`end_time` as TIMESTAMP(3)),
WATERMARK FOR event_time as event_time - INTERVAL '5' SECOND
)
with (
'connector' = 'kafka',
'topic' = 'topic',
'properties.bootstrap.servers' = '***',
'properties.group.id' = '***',
'scan.startup.mode' = 'earliest-offset',
'value.format' = 'debezium-json'
);
""""""  
  
t_env.execute_sql(kafka_src)
table = st_env.sql_query(""SELECT columns,`event_time`  \
    FROM TABLE(TUMBLE(TABLE table_name, DESCRIPTOR(event_time), INTERVAL '1' MINUTES))"")

table.execute().print()  #could print the result

df = table.to_pandas()

#schema is correct!
schema = DataTypes.ROW([DataTypes.FIELD(""column1"", DataTypes.STRING()),
                        .......
                            ])
table = st_env.from_pandas(df,schema=schema)
st_env.create_temporary_view(""view_table"",table)

st_env.sql_query(""select * from view_table"").execute().print() # Not work!Can't print the result {code}
Tumbling window stream from kafka source convert to pandas dataframe and it can't print the result.The schema is right.I have tested in another job with using batch stream from jdbc source.It can print the result.The only different thing is the input stream.Is tumbling windows supported to convert to Pandas?",pyflink1.15.2,Joekwal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-02-09 06:50:27.0,,,,,,,,,,"0|z1fsjs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docs_404_check fails occasionally,FLINK-30976,13523892,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,09/Feb/23 06:48,09/Feb/23 15:03,13/Jul/23 08:29,09/Feb/23 14:18,1.17.0,1.18.0,,,,,pulsar-4.0.0,,,,,,,,Documentation,,,,0,pull-request-available,test-stability,,"We've seen the docs_404_check failing in nightly builds (only the cron stage but not the ci stage):
{code}
Re-run Hugo with the flag --panicOnWarning to get a better error message.
ERROR 2023/02/09 01:27:27 ""docs/connectors/datastream/pulsar.md"": Invalid use of artifact shortcode. Unknown flag `4.0.0-SNAPSHOT`
ERROR 2023/02/09 01:27:34 ""docs/connectors/datastream/pulsar.md"": Invalid use of artifact shortcode. Unknown flag `4.0.0-SNAPSHOT`
Error: Error building site: logged 2 error(s)
Total in 12945 ms
Error building the docs
{code}
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45909&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98&l=133
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45906&view=logs&j=6dc02e5c-5865-5c6a-c6c5-92d598e3fc43&t=ddd6d61a-af16-5d03-2b9a-76a279badf98&l=132",,mapohl,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 14:18:47 UTC 2023,,,,,,,,,,"0|z1fsjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 06:49;mapohl;It seems to be fixed again. But I'm still wondering what caused and what fixed it and why it didn't cause a test failure in the CI stage.;;;","09/Feb/23 07:17;renqs;{{docs_404_check}} is only run in PR triggered and cron jobs, so most CI runs skip this stage. 

This could be reproduced in my own Azure pipeline:

[https://dev.azure.com/renqs/Apache%20Flink/_build/results?buildId=438&view=logs&j=c5d67f7d-375d-5407-4743-f9d0c4436a81&t=38411795-40c9-51fa-10b0-bd083cf9f5a5&l=133] ;;;","09/Feb/23 08:10;mapohl;I see, we only run the docs check for the PR if the PR touches the docs: https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/tools/azure-pipelines/build-apache-repo.yml#L87
I'm still puzzled, though, why your change triggered the docs build, [~renqs]. You didn't touch the docs.;;;","09/Feb/23 08:20;renqs;[~mapohl] My guess is that personal CIs use {{azure-pipelines.yml}} instead of the {{{}tools/azure-pipelines/build-apache-repo.yml{}}}. The former one doesn't have the doc touching check.;;;","09/Feb/23 08:47;mapohl;[~martijnvisser] do you have time to look into this?;;;","09/Feb/23 09:07;chesnay;You just have to replace references to the {{artifact}} shortcode with {{connector_artifact}} in flink-connector-pulsar.;;;","09/Feb/23 14:18;mapohl;pulsar/main: 6d5629f23f6306993a2782919fd392032f798088;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"E2e tests always fail in phase ""Prepare E2E run""",FLINK-30972,13523877,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,renqs,wanglijie,wanglijie,09/Feb/23 05:27,06/Mar/23 09:57,13/Jul/23 08:29,09/Feb/23 08:10,1.15.4,1.16.2,1.17.0,1.18.0,,,1.15.4,1.16.2,1.17.0,1.18.0,,,,,Build System / CI,Tests,,,0,test-stability,,,"{code:java}
Installing required software
Reading package lists...
Building dependency tree...
Reading state information...
bc is already the newest version (1.07.1-2build1).
bc set to manually installed.
libapr1 is already the newest version (1.6.5-1ubuntu1).
libapr1 set to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 13 not upgraded.
--2023-02-09 04:38:47--  http://security.ubuntu.com/ubuntu/pool/main/o/openssl1.0/libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb
Resolving security.ubuntu.com (security.ubuntu.com)... 91.189.91.39, 185.125.190.36, 185.125.190.39, ...
Connecting to security.ubuntu.com (security.ubuntu.com)|91.189.91.39|:80... connected.
HTTP request sent, awaiting response... 404 Not Found
2023-02-09 04:38:47 ERROR 404: Not Found.


WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

Reading package lists...
E: Unsupported file ./libssl1.0.0_1.0.2n-1ubuntu5.10_amd64.deb given on commandline
##[error]Bash exited with code '100'.
Finishing: Prepare E2E run
{code}",,leonard,mapohl,martijnvisser,renqs,wanglijie,,,,,,,,,,,,,,,,,,,,FLINK-30973,,FLINK-31334,,,,,,,,,FLINK-30965,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 06:55:03 UTC 2023,,,,,,,,,,"0|z1fsg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 05:28;wanglijie;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45926&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=11b0df07-3e5e-58da-eb81-03003e470195;;;","09/Feb/23 05:30;wanglijie;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45913&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=11b0df07-3e5e-58da-eb81-03003e470195;;;","09/Feb/23 06:20;renqs;Thanks for reporting this [~wanglijie] ! The required package was removed from the Ubuntu repository. I'll make a hotfix first and try to find a permanent solution later. ;;;","09/Feb/23 07:44;martijnvisser;[~renqs] This always happens when a new version of OpenSSL has been released; fix should be easy;;;","09/Feb/23 08:09;renqs;Fixed on master: 4029f730e454fb4c13e92d1ada5253c80ea645ba

1.17: f4410f0f97ec6e857f246c9649833b6eb15f6d2a

1.16: d5bd173f35844bbb56d32b9315073c59d12fe067

1.15: 998b7a1d1e4335fe8771150ed6b7a08fa08f6da4;;;","10/Feb/23 06:55;mapohl;The following builds didn't contain the fixes mentioned above, yet:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45934&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45919&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45922&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45979&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46021&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46030&view=results
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46034&view=results;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify the default value of parameter 'table.exec.local-hash-agg.adaptive.sampling-threshold',FLINK-30971,13523871,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,09/Feb/23 03:55,02/Mar/23 09:52,13/Jul/23 08:29,17/Feb/23 07:18,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"In our test environment, we set the default parallelism to  1 and got the most appropriate default value of parameter 'table.exec.local-hash-agg.adaptive.sampling-threshold'  is 5000000. However, for these batch jobs with high parallelism in produce environment,  the amount of data in single parallelism is almost less than 5000000. Therefore, after testing, we found that set to 500000 can get better results.",,337361684@qq.com,godfrey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 17 07:18:22 UTC 2023,,,,,,,,,,"0|z1fsew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Feb/23 07:18;godfrey;Fixed in master: 55b927b0e6eb2d5d71487b9bb2c4dab80017a7e5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Pyflink table example throws ""module 'pandas' has no attribute 'Int8Dtype'""",FLINK-30969,13523860,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,09/Feb/23 02:52,13/Feb/23 03:43,13/Jul/23 08:29,13/Feb/23 03:43,1.17.0,,,,,,1.17.0,,,,,,,,API / Python,,,,0,pull-request-available,,,"After apache-beam is upgraded to 2.43.0 in 1.17, running `python pyflink/examples/table/basic_operations.py` will throw error:
{code:java}
Traceback (most recent call last):
  File ""pyflink/examples/table/basic_operations.py"", line 484, in <module>
    basic_operations()
  File ""pyflink/examples/table/basic_operations.py"", line 29, in basic_operations
    t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 121, in create
    return TableEnvironment(j_tenv)
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 100, in __init__
    self._open()
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 1640, in _open
    startup_loopback_server()
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/table/table_environment.py"", line 1631, in startup_loopback_server
    from pyflink.fn_execution.beam.beam_worker_pool_service import \
  File ""/Users/vancior/Documents/Github/flink-back/flink-python/pyflink/fn_execution/beam/beam_worker_pool_service.py"", line 31, in <module>
    from apache_beam.options.pipeline_options import DebugOptions
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/__init__.py"", line 92, in <module>
    from apache_beam import coders
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/coders/__init__.py"", line 17, in <module>
    from apache_beam.coders.coders import *
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/coders/coders.py"", line 59, in <module>
    from apache_beam.coders import coder_impl
  File ""apache_beam/coders/coder_impl.py"", line 63, in init apache_beam.coders.coder_impl
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/typehints/__init__.py"", line 31, in <module>
    from apache_beam.typehints.pandas_type_compatibility import *
  File ""/Users/vancior/miniconda3/envs/flink-python/lib/python3.8/site-packages/apache_beam/typehints/pandas_type_compatibility.py"", line 81, in <module>
    (pd.Int8Dtype(), Optional[np.int8]),
AttributeError: module 'pandas' has no attribute 'Int8Dtype' {code}",,dianfu,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 03:43:48 UTC 2023,,,,,,,,,,"0|z1fscg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 03:43;dianfu;Fixed in:
- master via c096c03df70648b60b665a09816635b956b201cc
- release-1.17 via 19c05ef0c864644512e5643589fe550f7e281254;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink SQL IF FUNCTION logic error,FLINK-30966,13523766,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,hiscat,hiscat,08/Feb/23 13:20,06/Jun/23 02:56,13/Jul/23 08:29,06/Jun/23 02:54,1.16.0,1.16.1,,,,,1.16.3,1.17.2,1.18.0,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"my data is 
{code:java}
//
{ ""before"": { ""status"": ""sent"" }, ""after"": { ""status"": ""succeed"" }, ""op"": ""u"", ""ts_ms"": 1671926400225, ""transaction"": null } {code}
my sql is 

 
{code:java}
CREATE TABLE t
(
    before ROW (

        status varchar (32)

        ),
    after ROW (

        status varchar (32)

        ),
    ts_ms                bigint,
    op                   string,
    kafka_timestamp      timestamp METADATA FROM 'timestamp',
--     @formatter:off
    proctime AS PROCTIME()
--     @formatter:on
) WITH (
    'connector' = 'kafka',
--     'topic' = '',
    'topic' = 'test',
    'properties.bootstrap.servers' = ' ',
    'properties.group.id' = '',
    'format' = 'json',
    'scan.topic-partition-discovery.interval' = '60s',
    'scan.startup.mode' = 'earliest-offset',
    'json.ignore-parse-errors' = 'true'
 );
create table p
(
    status                  STRING ,
    before_status                  STRING ,
    after_status                  STRING ,
    metadata_operation      STRING COMMENT '源记录操作类型',
    dt                      STRING
)WITH (
    'connector' = 'print'
 );
INSERT INTO p
SELECT
       IF(op <> 'd', after.status, before.status),
        before.status,
        after.status,
       op                                         AS metadata_operation,
       DATE_FORMAT(kafka_timestamp, 'yyyy-MM-dd') AS dt
FROM t;

 {code}
 my local env output is 

 

 
{code:java}
+I[null, sent, succeed, u, 2023-02-08] {code}
 

 my produtionc env output is 
{code:java}
+I[sent, sent, succeed, u, 2023-02-08]  {code}
why?  
This look like a bug.

 ",,aitozi,csq,hiscat,lincoln.86xy,luoyuxia,martijnvisser,,,,,,,,,,,,,,,,,FLINK-30559,FLINK-30018,FLINK-31653,FLINK-30559,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 09:48:11 UTC 2023,,,,,,,,,,"0|z1frrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 02:38;csq;Hi [~hiscat], I have reproduced the same error and it seems a bug in IFCallGen.
After investigated the generated code, there are two problems:
1. It perform the result term casting before the calculation logic, and finally the actual result always refer to a non-initialized field.
2. when normalizing arguments, it always align to the type of ARG1, like IF(1 >  2, 'true', 'false')
the result will be string 'fals' which length is the same as 'true'.

I would like to help fix it. ;;;","10/Feb/23 02:41;csq;BTW, the expected result of your query might be:
+I[succeed, sent, u, 2023-02-08];;;","13/Feb/23 02:37;luoyuxia;Seems same to FLINK-31003? ;;;","13/Feb/23 02:48;csq;[~luoyuxia]The matter of [FLINK-31003|https://issues.apache.org/jira/browse/FLINK-31003] is return type inferencing, while there is another issue that the code block position of result term casting might be wrong.;;;","13/Feb/23 07:15;lincoln.86xy;[~csq] assigned to you;;;","31/May/23 03:45;luoyuxia;Hi, [~csq] I have reviewed your pr. Hope you can find sometime to have a look again.;;;","31/May/23 03:51;csq;[~luoyuxia]Sorry for the delay, I would continue to work on the issue and update the PR  as soon as possible.;;;","05/Jun/23 09:48;luoyuxia;master: 6663c8b29f672b961d34baed314940621f3754ca

1.17: 8e82359be4982c8534b271259c7e649e4c12df1d

1.16: 6bdf2ba59678289b517b208ac5173c8e58b50690;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
git-repo-sync doesn't pick up the 2nd-most-recently published Flink version (even though it's still supported) after a new release branch is cut,FLINK-30965,13523765,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,mapohl,mapohl,mapohl,08/Feb/23 13:18,16/Feb/23 09:39,13/Jul/23 08:29,16/Feb/23 09:39,,,,,,,,,,,,,,,Test Infrastructure,,,,0,,,,"I noticed that we're always synchronizing the most-recent release branches (see [flink-ci/git-repo-sync:git-repo-synch:27|https://github.com/flink-ci/git-repo-sync/blob/7c0c2ed4b8f1cdf343e75021ca89e9dcc9421b93/sync_repo.sh#L27]). That means that we stop running CI on the Flink version that is soon to be deprecated after a new release branch is cut, e.g.: 1.16 & 1.15 are supported and we cut {{release-1.17}} to start the release process for 1.17. From this point onwards, we are synchronizing 1.17 and 1.16 but skip 1.15 eventhough 1.15 is still supported.",,leonard,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30972,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 09:39:50 UTC 2023,,,,,,,,,,"0|z1frrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Feb/23 09:32;mapohl;1.15 doesn't get updated anymore: We see the issue of FLINK-30972 being fixed but it's not picked up anymore:
 * [20230210.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45979&view=results]
 * [20230211.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46021&view=results]
 * [20230212.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46030&view=results]
 * [20230213.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46034&view=results]
 * [20230214.2|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46085&view=results];;;","13/Feb/23 14:48;mapohl;[~chesnay] I still don't understand why the build is retriggered if there's no new commit sync'd to the {{flink-mirror}} repo. Is there some build pipeline configuration that triggers the nightly? The YAML config in the code looks like we're only running {{master}} even if there's no change pushed in the meantime (see [tools/azure-pipelines/build-apache-repo.yml|https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/tools/azure-pipelines/build-apache-repo.yml#L32]).

May you have a look at the repo-sync script change? I'm wondering whether ignoring the oldest release branch was intentionally done in the past or just missed.;;;","13/Feb/23 15:28;chesnay;nightlies are configured in the azure UI.

I don't believe we have concluded to not create a final 1.15 release. Hence the nightlies should keep running, as should the sync.;;;","13/Feb/23 15:30;chesnay;Interestingly enough in the UI we did set that nightlies should run if something changed, so it's weird that it runs them anyway. #JustAzureThings;;;","16/Feb/23 09:39;mapohl;The PR was merged and redeployed. I verified that the {{release-1.15}} is sync'd now again in [flink-ci/flink-mirror:release-1.15|https://github.com/flink-ci/flink-mirror/commits/release-1.15].

CI also succeed with the fix of FLINK-30972 being considered as well: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46200&view=results;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-mirror repo sync release branches failed,FLINK-30964,13523761,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,leonard,leonard,leonard,08/Feb/23 12:38,08/Feb/23 13:40,13/Jul/23 08:29,08/Feb/23 13:39,1.17.0,,,,,,1.17.0,,,,,,,,Build System / CI,,,,0,,,,"we use https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh to sync the master the latest 2 release-X branches  from apache/flink to flink-ci/flink-mirror.  
 but the scripts https://github.com/flink-ci/git-repo-sync/blob/master/sync_repo.sh exists a wrong condition judgement which lead the release-1.17 sync fail.",,leonard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 13:39:23 UTC 2023,,,,,,,,,,"0|z1frqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 13:39;leonard;Fixed in https://github.com/flink-ci/git-repo-sync master: 9563d8a8564b5709607122ad146753c925e05827;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Rest API doc generation failure caused by JobClientHeartbeatHeaders,FLINK-30958,13523715,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,qingyue,qingyue,08/Feb/23 06:34,08/Feb/23 10:56,13/Jul/23 08:29,08/Feb/23 10:56,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / REST,,,,0,pull-request-available,,,`JobClientHeartbeatHeaders` should override `operationId` since `getHttpMethod` returns `POST`. Otherwise `UnsupportedOperationException` is thrown at `OpenApiSpecGenerator` when generating the REST API doc.,,qingyue,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29640,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 10:56:00 UTC 2023,,,,,,,,,,"0|z1frg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 10:56;xtsong;* master (1.18): e33034f8aa359c9233e5c1fa570e5270c7a03737
* release-1.17: 353c8016f06c350b05121dc905a37daf33fd1727;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FTS does not support multiple writers into the same table and topic,FLINK-30945,13523528,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,vicky_papavas,vicky_papavas,07/Feb/23 15:43,29/Mar/23 03:06,13/Jul/23 08:29,29/Mar/23 03:06,,,,,,,,,,,,,,,Table Store,,,,0,,,,"When creating two different streaming jobs that INSERT INTO the same table and kafka topic, the second job is never able to make progress as the transaction gets constantly aborted due to the producer getting fenced.

FTS should set the transactionalIdPrefix to avoid transactions of different jobs clashing.
{code:java}
2023-02-06 17:13:36,088 WARN org.apache.flink.runtime.taskmanager.Task [] - Writer -> Global Committer -> Sink: end (1/1)#0 (8cf4197af9716623c3c19e7fa3d7c071_b5c8d46f3e7b141acf271f12622e752b_0_0) switched from RUNNING to FAILED with failure cause: org.apache.flink.util.FlinkRuntimeException: Committing one of transactions failed, logging first encountered failure at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.notifyCheckpointComplete(TwoPhaseCommitSinkFunction.java:323) at org.apache.flink.table.store.connector.sink.StoreWriteOperator.notifyCheckpointComplete(StoreWriteOperator.java:175) at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.notifyCheckpointComplete(StreamOperatorWrapper.java:104) at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.notifyCheckpointComplete(RegularOperatorChain.java:145) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpoint(SubtaskCheckpointCoordinatorImpl.java:479) at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.notifyCheckpointComplete(SubtaskCheckpointCoordinatorImpl.java:413) at org.apache.flink.streaming.runtime.tasks.StreamTask.notifyCheckpointComplete(StreamTask.java:1412) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointCompleteAsync$15(StreamTask.java:1353) at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$notifyCheckpointOperation$18(StreamTask.java:1392) at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.runThrowing(StreamTaskActionExecutor.java:50) at org.apache.flink.streaming.runtime.tasks.mailbox.Mail.run(Mail.java:90) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMail(MailboxProcessor.java:398) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMailsNonBlocking(MailboxProcessor.java:383) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.processMail(MailboxProcessor.java:345) at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:229) at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831) at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780) at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914) at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) at java.lang.Thread.run(Thread.java:750) Caused by: org.apache.flink.table.store.shaded.org.apache.kafka.common.errors.ProducerFencedException: There is a newer producer with the same transactionalId which fences the current one. {code}
Sample queries:
 
 
{code:java}
CREATE CATALOG table_store_catalog WITH (
    'type'='table-store',
    'warehouse'='s3://my-bucket/table-store'
 );
USE CATALOG table_store_catalog;
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count_kafka (
     word STRING PRIMARY KEY NOT ENFORCED,
     cnt BIGINT
 ) WITH (
     'log.system' = 'kafka',
     'kafka.bootstrap.servers' = 'broker:9092',
     'kafka.topic' = 'word_count_log'
 );
CREATE TEMPORARY TABLE word_table (
     word STRING
 ) WITH (
     'connector' = 'datagen',
     'fields.word.length' = '1'
 );
{code}
 

And the two INSERT jobs:
{code:java}
INSERT INTO word_count_kafka SELECT word, COUNT(*) FROM word_table GROUP BY word;{code}",,binh,libenchao,lzljs3620320,vicky_papavas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 03:06:53 UTC 2023,,,,,,,,,,"0|z1fqao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Feb/23 07:39;lzljs3620320;[~vicky_papavas] Thanks for reporting! I think this needs to be improved!;;;","17/Feb/23 20:08;binh;[~lzljs3620320] would you assign this to me?;;;","29/Mar/23 03:06;lzljs3620320;https://github.com/apache/incubator-paimon/issues/745;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ExecutionGraphPartitionReleaseTest leaks threads,FLINK-30944,13523524,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Weijie Guo,mapohl,mapohl,07/Feb/23 15:08,13/Feb/23 08:30,13/Jul/23 08:29,13/Feb/23 08:30,1.15.3,1.16.1,1.17.0,,,,1.16.2,1.17.0,,,,,,,Runtime / Coordination,Tests,,,0,pull-request-available,test-stability,,{{ExecutionGraphPartitionReleaseTest}} leaks threads through {{ExecutionGraphPartitionReleaseTest.scheduledExecutorService}}. The {{ScheduledExecutorService}} is instantiated but never shut down.,,mapohl,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-27518,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 13 08:30:58 UTC 2023,,,,,,,,,,"0|z1fq9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 15:09;mapohl;We identified the thread leaking while reviewing FLINK-27518;;;","08/Feb/23 03:44;Weijie Guo;Thanks [~mapohl] for reporting this, I'd like to do this work.;;;","13/Feb/23 08:30;mapohl;* master: df23acfc0f124476365370b5e9945e7c957e4fce (with prereq hotfix commits 82ba1f7ab2168b0b0e84bdfe66bb9971b55a7338 and 7bdc7549fdfb4002d09ae390bdef8e4e565eb433)
* 1.17: a5536f2f2f5b0574d8e8ecb768afd049667a1fba (with prereq hotfix commits 8e8aa2aac165620529ef674e7dc5d8e39ddfeae0 and fa85ca20ac5919c52a548a069551d04b00885db1)
* 1.16: 26a8fe566b792d6e49c974978b6cc2347ea8606a (with prereq hotfix commits d4242f11bed61eeba71ac562ecd048489691f93c and 080ddecbb1884eea9deed51f3ed2bc4a48b2de6c);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the bug that the decided parallelism by adaptive batch scheduler may be larger than the max parallelism,FLINK-30942,13523499,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,07/Feb/23 12:23,17/Feb/23 05:54,13/Jul/23 08:29,10/Feb/23 10:45,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Currently, when using the adaptive batch scheduler, the vertex parallelism decided by  forward group may be larger than the global max parallelism(which is configured by option {{parallelism.default}} or {{execution.batch.adaptive.auto-parallelism.max-parallelism}}, see FLINK-30686 for details), which will cause the following exception:

{code:java}
Caused by: java.lang.IllegalArgumentException: Vertex's parallelism should be smaller than or equal to vertex's max parallelism.
	at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:138)
	at org.apache.flink.runtime.scheduler.DefaultVertexParallelismInfo.setParallelism(DefaultVertexParallelismInfo.java:95)
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.setParallelism(ExecutionJobVertex.java:317)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.changeJobVertexParallelism(AdaptiveBatchScheduler.java:385)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.initializeVerticesIfPossible(AdaptiveBatchScheduler.java:284)
	at org.apache.flink.runtime.scheduler.adaptivebatch.AdaptiveBatchScheduler.onTaskFinished(AdaptiveBatchScheduler.java:183)
	at org.apache.flink.runtime.scheduler.SchedulerBase.onTaskExecutionStateUpdate(SchedulerBase.java:745)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:725)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:80)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:479)
	... 30 more
{code}

Following code can reproduce the above exception:

{code:java}
final Configuration configuration = new Configuration();
configuration.setString(RestOptions.BIND_PORT, ""0"");
configuration.setLong(JobManagerOptions.SLOT_REQUEST_TIMEOUT, 5000L);
configuration.setInteger(
        BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_MAX_PARALLELISM, 2);
configuration.set(
        BatchExecutionOptions.ADAPTIVE_AUTO_PARALLELISM_AVG_DATA_VOLUME_PER_TASK,
        MemorySize.parse(""150kb""));
configuration.set(TaskManagerOptions.MEMORY_SEGMENT_SIZE, MemorySize.parse(""4kb""));
configuration.set(TaskManagerOptions.NUM_TASK_SLOTS, 1);

final StreamExecutionEnvironment env =
        StreamExecutionEnvironment.createLocalEnvironment(configuration);
env.setRuntimeMode(RuntimeExecutionMode.BATCH);
env.setParallelism(4);

final DataStream<Long> source =
        env.fromSequence(0, NUMBERS_TO_PRODUCE - 1)
                .setParallelism(4)
                .name(""source"")
                .slotSharingGroup(""group1"");

source.forward().map(new NumberCounter()).name(""map"").slotSharingGroup(""group2"");
env.execute();
{code}

",,wanglijie,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31114,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 13:33:12 UTC 2023,,,,,,,,,,"0|z1fq48:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 13:33;zhuzh;master:
5a4e0ea31aef51dfc50b9c401c91c7b869a154c4

release-1.17:
7c7156bb486d7b480ead19c0c486c82afcb92c3a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InterruptedException in ExecutorImplITCase which doesn't fail the test,FLINK-30940,13523491,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,fsk119,mapohl,mapohl,07/Feb/23 11:50,09/Feb/23 12:01,13/Jul/23 08:29,09/Feb/23 12:01,1.17.0,,,,,,1.17.0,1.18.0,,,,,,,Table SQL / API,,,,0,pull-request-available,test-stability,,"We're experiencing a test failure {{CliClientITCase.testSqlStatements}} which might be caused by an {{InterruptedException}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45828&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=46490

{code}
Exception in thread ""worker"" org.apache.flink.table.client.gateway.SqlExecutionException: Interrupted to fetch results.
	at org.apache.flink.table.client.gateway.ExecutorImpl.lambda$fetchUtilResultsReady$4(ExecutorImpl.java:375)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:398)
	at org.apache.flink.table.client.gateway.ExecutorImpl.fetchUtilResultsReady(ExecutorImpl.java:368)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:234)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterruptExecution$7(ExecutorImplITCase.java:507)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterrupting$12(ExecutorImplITCase.java:607)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:396)
	... 5 more
Exception in thread ""worker"" org.apache.flink.table.client.gateway.SqlExecutionException: Interrupted to fetch results.
	at org.apache.flink.table.client.gateway.ExecutorImpl.lambda$fetchUtilResultsReady$4(ExecutorImpl.java:375)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:398)
	at org.apache.flink.table.client.gateway.ExecutorImpl.fetchUtilResultsReady(ExecutorImpl.java:368)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:234)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterruptFetching$8(ExecutorImplITCase.java:515)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterrupting$12(ExecutorImplITCase.java:607)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getFetchResultResponse(ExecutorImpl.java:396)
	... 5 more
Exception in thread ""worker"" org.apache.flink.table.client.gateway.SqlExecutionException: Interrupted to get response.
	at org.apache.flink.table.client.gateway.ExecutorImpl.lambda$executeStatement$2(ExecutorImpl.java:228)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getResponse(ExecutorImpl.java:429)
	at org.apache.flink.table.client.gateway.ExecutorImpl.executeStatement(ExecutorImpl.java:210)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterruptSubmitting$6(ExecutorImplITCase.java:502)
	at org.apache.flink.table.client.gateway.ExecutorImplITCase.lambda$testInterrupting$12(ExecutorImplITCase.java:607)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:347)
	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
	at org.apache.flink.table.client.gateway.ExecutorImpl.getResponse(ExecutorImpl.java:424)
	... 4 more
{code}",,fsk119,mapohl,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 12:01:01 UTC 2023,,,,,,,,,,"0|z1fq2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 08:51;fsk119;Hi. Thanks for pointing it out. When a thread is interrupted, it will print the interrupted exception to the {{System.out}} by default. I will fix this;;;","09/Feb/23 12:01;fsk119;Merged into mater: 8ff06420312b87377e36aa43be20390f5a7acf7d

Merged into release-1.17: 72cf4eaa42379ad38fd6bc7cb52330af070cfe63;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Result of join inside iterationBody loses max watermark,FLINK-30933,13523406,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zhangzp,zhangzp,zhangzp,07/Feb/23 06:06,19/Apr/23 01:38,13/Jul/23 08:29,19/Apr/23 01:38,ml-2.0.0,ml-2.1.0,,,,,ml-2.2.0,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"Currently if we execute a join inside an iteration body, the following program produces empty output. (In which the right result should be a list with \{2}.
{code:java}
public class Test {

    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        env.setParallelism(1);

        DataStream<Tuple2<Long, Integer>> input1 =
                env.fromElements(Tuple2.of(1L, 1), Tuple2.of(2L, 2));

        DataStream<Tuple2<Long, Long>> input2 =
                env.fromElements(Tuple2.of(1L, 2L), Tuple2.of(2L, 3L));

        DataStream<Tuple2<Long, Long>> iterationJoin =
                Iterations.iterateBoundedStreamsUntilTermination(
                                DataStreamList.of(input1),
                                ReplayableDataStreamList.replay(input2),
                                IterationConfig.newBuilder()
                                        .setOperatorLifeCycle(
                                                IterationConfig.OperatorLifeCycle.PER_ROUND)
                                        .build(),
                                new MyIterationBody())
                        .get(0);

        DataStream<Long> left = iterationJoin.map(x -> x.f0);
        DataStream<Long> right = iterationJoin.map(x -> x.f1);
        DataStream<Long> result =
                left.join(right)
                        .where(x -> x)
                        .equalTo(x -> x)
                        .window(EndOfStreamWindows.get())
                        .apply((JoinFunction<Long, Long, Long>) (l1, l2) -> l1);

        List<Long> collectedResult = IteratorUtils.toList(result.executeAndCollect());
        List<Long> expectedResult = Arrays.asList(2L);
        compareResultCollections(expectedResult, collectedResult, Long::compareTo);
    }

    private static class MyIterationBody implements IterationBody {
        @Override
        public IterationBodyResult process(
                DataStreamList variableStreams, DataStreamList dataStreams) {
            DataStream<Tuple2<Long, Integer>> input1 = variableStreams.get(0);
            DataStream<Tuple2<Long, Long>> input2 = dataStreams.get(0);

            DataStream<Long> terminationCriteria = input1.flatMap(new TerminateOnMaxIter(1));

            DataStream<Tuple2<Long, Long>> res =
                    input1.join(input2)
                            .where(x -> x.f0)
                            .equalTo(x -> x.f0)
                            .window(EndOfStreamWindows.get())
                            .apply(
                                    new JoinFunction<
                                            Tuple2<Long, Integer>,
                                            Tuple2<Long, Long>,
                                            Tuple2<Long, Long>>() {
                                        @Override
                                        public Tuple2<Long, Long> join(
                                                Tuple2<Long, Integer> longIntegerTuple2,
                                                Tuple2<Long, Long> longLongTuple2)
                                                throws Exception {
                                            return longLongTuple2;
                                        }
                                    });

            return new IterationBodyResult(
                    DataStreamList.of(input1), DataStreamList.of(res), terminationCriteria);
        }
    }
}
{code}
 

There are two possible reasons:
 * The timer in `HeadOperator` is not a daemon process and it does not exit even flink job finishes.
 * The max watermark from the iteration body is missed.

 

 ",,lindong,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Apr 19 01:38:45 UTC 2023,,,,,,,,,,"0|z1fpjk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 01:38;lindong;Merged to apache/flink-ml master branch 80fd4dfb843aee1d9cfd93130cfff016a9966b7b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Several tests started generate output with two non-abstract methods  have the same parameter types, declaring type and return type",FLINK-30927,13523230,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,KristoffSC,Sergey Nuyanzin,Sergey Nuyanzin,06/Feb/23 12:45,09/Feb/23 09:33,13/Jul/23 08:29,09/Feb/23 09:33,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"e.g. org.apache.flink.table.planner.runtime.stream.sql.MatchRecognizeITCase#testUserDefinedFunctions

 

it seems during code splitter it starts generating some methods with same signature

 

{noformat}

org.codehaus.janino.InternalCompilerException: Compiling ""MatchRecognizePatternProcessFunction$77"": Two non-abstract methods ""default void MatchRecognizePatternProcessFunction$77.processMatch_0(java.util.Map, org.apache.flink.cep.functions.PatternProcessFunction$Context, org.apache.flink.util.Collector) throws java.lang.Exception"" have the same parameter types, declaring type and return type

{noformat}

 

Probably could be a side effect of https://issues.apache.org/jira/browse/FLINK-27246",,KristoffSC,leonard,mapohl,Sergey Nuyanzin,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 09 09:33:08 UTC 2023,,,,,,,,,,"0|z1fogg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 13:54;KristoffSC;I already have fix for this, will provide PR shortly.
It's caused by https://github.com/apache/flink/pull/21393.

Could someone assign this ticket to me?;;;","06/Feb/23 14:02;mapohl;Thanks, [~KristoffSC]. I assigned the issue to you.;;;","06/Feb/23 14:25;KristoffSC;Pull request available
https://github.com/apache/flink/pull/21871;;;","06/Feb/23 14:44;KristoffSC;Provided PR above is fixing the reported issue. 

However CI build was not failing due to this problem. The reason why it was not failing is that code splitter has a safety net, that whenever rewritten code fails the compilation, Flink tries to use original code + print failing class into the logs. That is how the problem was spotted. 

Maybe it would worth to add an enhancement such this issue would in fact failed the build? A separate issue?;;;","06/Feb/23 20:56;KristoffSC;OK,
CI build is green for provided PR, also I dont see any `InternalCompilerException ... Two non-abstract methods` exception in table_ci_table nor other tests from flink-table-planer.

I would appreciate for review for this small Bug FIX PR and sorry for any inconvenience caused by this.;;;","07/Feb/23 09:37;KristoffSC;PR needs to be merged to 1.17 branch aswell. ;;;","07/Feb/23 10:56;KristoffSC;master: 96a296db723575d64857482a1278744e4c41201f

PR for 1.17 - https://github.com/apache/flink/pull/21879;;;","09/Feb/23 09:33;TsReaper;master: 96a296db723575d64857482a1278744e4c41201f
release-1.17: fd2ccf2d9586e2ffb92d8a6ccb5a5a303d32ef2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SQL validate fail in parsing writable metadata,FLINK-30922,13523211,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,csq,tanjialiang,tanjialiang,06/Feb/23 10:35,07/Mar/23 01:27,13/Jul/23 08:29,06/Mar/23 14:32,1.16.1,,,,,,1.17.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"When i tried an simple demo sql with writing metadata to the kafka in flink sql client
{code:java}
CREATE TABLE KafkaTable (
  `user_id` BIGINT,
  `item_id` BIGINT,
  `behavior` STRING,
  `ts` TIMESTAMP(3) METADATA FROM 'timestamp'
) WITH (
  'connector' = 'kafka',
  'topic' = 'user_behavior',
  'properties.bootstrap.servers' = 'localhost:9092',
  'properties.group.id' = 'testGroup',
  'scan.startup.mode' = 'earliest-offset',
  'format' = 'csv'
)

INSERT INTO KafkaTable(user_id, ts) SELECT '1', CURRENT_TIMESTAMP; {code}
 

it will be throw an error
{code:java}
org.apache.flink.table.client.gateway.SqlExecutionException: Failed to parse statement: INSERT INTO KafkaTable(user_id, ts) SELECT '1', CURRENT_TIMESTAMP;
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:174) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.SqlCommandParserImpl.parseCommand(SqlCommandParserImpl.java:45) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.SqlMultiLineParser.parse(SqlMultiLineParser.java:71) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.jline.reader.impl.LineReaderImpl.acceptLine(LineReaderImpl.java:2964) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.jline.reader.impl.LineReaderImpl$$Lambda$364/1900307803.apply(Unknown Source) ~[?:?]
        at org.jline.reader.impl.LineReaderImpl$1.apply(LineReaderImpl.java:3778) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.jline.reader.impl.LineReaderImpl.readLine(LineReaderImpl.java:679) ~[flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.CliClient.getAndExecuteStatements(CliClient.java:295) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.CliClient.executeInteractive(CliClient.java:280) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.cli.CliClient.executeInInteractiveMode(CliClient.java:228) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.openCli(SqlClient.java:151) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.start(SqlClient.java:95) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.startClient(SqlClient.java:187) [flink-sql-client-1.16.1.jar:1.16.1]
        at org.apache.flink.table.client.SqlClient.main(SqlClient.java:161) [flink-sql-client-1.16.1.jar:1.16.1]
Caused by: org.apache.flink.table.api.ValidationException: SQL validation failed. From line 1, column 33 to line 1, column 34: Unknown target column 'ts'
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:186) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
        at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261) ~[?:?]
        at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
        ... 13 more
Caused by: org.apache.calcite.runtime.CalciteContextException: From line 1, column 33 to line 1, column 34: Unknown target column 'ts'
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_41]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_41]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_41]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422) ~[?:1.8.0_41]
        at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.newValidationError(PreValidateReWriter.scala:401) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.validateField(PreValidateReWriter.scala:389) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$appendPartitionAndNullsProjects$3(PreValidateReWriter.scala:172) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$$$Lambda$610/614335089.apply(Unknown Source) ~[?:?]
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike$$Lambda$329/456314134.apply(Unknown Source) ~[?:?]
        at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.appendPartitionAndNullsProjects(PreValidateReWriter.scala:164) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.rewriteInsert(PreValidateReWriter.scala:71) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:61) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:50) ~[?:?]
        at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:118) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
        at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261) ~[?:?]
        at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
        ... 13 more
Caused by: org.apache.calcite.sql.validate.SqlValidatorException: Unknown target column 'ts'
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_41]
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_41]
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_41]
        at java.lang.reflect.Constructor.newInstance(Constructor.java:422) ~[?:1.8.0_41]
        at org.apache.calcite.runtime.Resources$ExInstWithCause.ex(Resources.java:467) ~[?:?]
        at org.apache.calcite.runtime.Resources$ExInst.ex(Resources.java:560) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:883) ~[?:?]
        at org.apache.calcite.sql.SqlUtil.newContextException(SqlUtil.java:868) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.newValidationError(PreValidateReWriter.scala:401) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.validateField(PreValidateReWriter.scala:389) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.$anonfun$appendPartitionAndNullsProjects$3(PreValidateReWriter.scala:172) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$$$Lambda$610/614335089.apply(Unknown Source) ~[?:?]
        at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike$$Lambda$329/456314134.apply(Unknown Source) ~[?:?]
        at scala.collection.Iterator.foreach(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.Iterator.foreach$(Iterator.scala:937) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1425) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach(IterableLike.scala:70) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.IterableLike.foreach$(IterableLike.scala:69) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map(TraversableLike.scala:233) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.TraversableLike.map$(TraversableLike.scala:226) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at scala.collection.AbstractTraversable.map(Traversable.scala:104) ~[flink-scala_2.12-1.16.1.jar:1.16.1]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter$.appendPartitionAndNullsProjects(PreValidateReWriter.scala:164) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.rewriteInsert(PreValidateReWriter.scala:71) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:61) ~[?:?]
        at org.apache.flink.table.planner.calcite.PreValidateReWriter.visit(PreValidateReWriter.scala:50) ~[?:?]
        at org.apache.calcite.sql.SqlCall.accept(SqlCall.java:139) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.org$apache$flink$table$planner$calcite$FlinkPlannerImpl$$validate(FlinkPlannerImpl.scala:118) ~[?:?]
        at org.apache.flink.table.planner.calcite.FlinkPlannerImpl.validate(FlinkPlannerImpl.scala:113) ~[?:?]
        at org.apache.flink.table.planner.operations.SqlToOperationConverter.convert(SqlToOperationConverter.java:261) ~[?:?]
        at org.apache.flink.table.planner.delegation.ParserImpl.parse(ParserImpl.java:106) ~[?:?]
        at org.apache.flink.table.client.gateway.local.LocalExecutor.parseStatement(LocalExecutor.java:172) ~[flink-sql-client-1.16.1.jar:1.16.1]
        ... 13 more{code}
 ",,csq,lincoln.86xy,tanjialiang,,,,,,,,,,,,,,,,,,,,,,FLINK-31313,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 06 14:32:16 UTC 2023,,,,,,,,,,"0|z1foc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"08/Feb/23 08:48;csq;Hi [~tanjialiang], thank you for reporting the issue. I have reproduced the same error with the code you provided. 
According to [FLIP-107|https://cwiki.apache.org/confluence/display/FLINK/FLIP-107%3A+Handling+of+metadata+in+SQL+connectors], it is possible to write metadata columns in SQL.

The cause of this error is a bug that it excludes all computed columns and metadata columns when doing appendPartitionAndNullsProjects in PreValidateReWriter. Actually, it is expected to include all persisted columns. I would like to fix it. ;;;","09/Feb/23 03:00;csq;I have created a pull request to fixed the issue, Anyone who help review the PR will be highly appreciated.;;;","10/Feb/23 09:03;tanjialiang;Great! Thanks for [~csq] 's contribute. Hope to merge it soon.;;;","06/Mar/23 14:32;lincoln.86xy;fixed in master: a2d78e60a96616ad9f575c7a3391f7322cb220af
release-1.17: 865a05678a0cee4a3d013ddc8685d9420cc59b12;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Too many CI failed due to ""Could not connect to azure.archive.ubuntu.com""",FLINK-30921,13523205,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,fanrui,fanrui,06/Feb/23 09:59,03/May/23 08:49,13/Jul/23 08:29,03/May/23 08:49,,,,,,,1.16.2,1.17.1,1.18.0,,,,,,Test Infrastructure,,,,0,pull-request-available,test-stability,,"!image-2023-02-06-17-59-20-019.png!

 

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45762&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14]

 

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45766&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a",,fanrui,leonard,mapohl,martijnvisser,renqs,,,,,,,,,,,,,,,,,,FLINK-25292,,,,FLINK-30152,,,,,,,,,FLINK-30941,,,,,"06/Feb/23 09:59;fanrui;image-2023-02-06-17-59-20-019.png;https://issues.apache.org/jira/secure/attachment/13055175/image-2023-02-06-17-59-20-019.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 03 08:49:54 UTC 2023,,,,,,,,,,"0|z1foaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 10:06;fanrui;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45764&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","06/Feb/23 12:24;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45759&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=bea52777-eaf8-5663-8482-18fbc3630e81;;;","06/Feb/23 12:41;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45778&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=34;;;","06/Feb/23 12:42;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45761&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=34;;;","06/Feb/23 12:46;renqs;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45745&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=11410;;;","06/Feb/23 12:55;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45748&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","06/Feb/23 13:01;leonard;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45763&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14;;;","06/Feb/23 13:10;leonard;This is an azure mirror crash issue, please see more info here https://github.com/actions/runner-images/issues/675,
I think we can avoid use azure mirror to fix this ticket;;;","06/Feb/23 16:19;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45778&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=35;;;","06/Feb/23 16:20;mapohl;Same build, different jobs:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45788&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5035
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45788&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=35;;;","06/Feb/23 16:21;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45779&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=39;;;","06/Feb/23 16:22;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45772&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5364;;;","06/Feb/23 16:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45761&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=33;;;","06/Feb/23 16:39;mapohl;There's [actions/runner-images:#7048|https://github.com/actions/runner-images/issues/7048] where users are reporting issues with the Ubuntu mirrors ;;;","06/Feb/23 19:49;martijnvisser;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45800&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=25;;;","07/Feb/23 02:20;fanrui;I see it's recovered, the new CI is green.;;;","07/Feb/23 02:53;leonard;I downgrade the issue priority to Critical as it's an external(azure infra) service crash and has recovered, we have use this service for a long time and thus it should not block our release.

 Sure that we need to fix this in our community.;;;","06/Mar/23 08:48;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46730&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=35;;;","06/Mar/23 09:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46766&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=35;;;","28/Mar/23 09:53;mapohl;Slightly different error but same outcome: 
{code}
After this operation, 771 kB of additional disk space will be used.
Err:1 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libio-pty-perl amd64 1:1.12-1
  503  Service Unavailable [IP: 51.11.236.225 80]
Err:2 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libipc-run-perl all 20180523.0-2
  503  Service Unavailable [IP: 51.11.236.225 80]
Err:3 http://azure.archive.ubuntu.com/ubuntu focal/universe amd64 libtime-duration-perl all 1.21-1
  503  Service Unavailable [IP: 51.11.236.225 80]
Err:4 http://azure.archive.ubuntu.com/ubuntu focal/main amd64 libtimedate-perl all 2.3200-1
  503  Service Unavailable [IP: 51.11.236.225 80]
Err:5 http://azure.archive.ubuntu.com/ubuntu focal/universe amd64 moreutils amd64 0.63-1
  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/libi/libio-pty-perl/libio-pty-perl_1.12-1_amd64.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/libi/libipc-run-perl/libipc-run-perl_20180523.0-2_all.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/universe/libt/libtime-duration-perl/libtime-duration-perl_1.21-1_all.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/main/libt/libtimedate-perl/libtimedate-perl_2.3200-1_all.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/universe/m/moreutils/moreutils_0.63-1_amd64.deb  503  Service Unavailable [IP: 51.11.236.225 80]
E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
{code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47642&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=25;;;","03/May/23 08:49;mapohl;master: 742685b76c7f001a08799a539cad2bb683d5d29d
1.17: 13fc226d4c049a63efcce57460f7dedc331f088f
1.16: d3f93d0fa1c25a4525dbd411523c891bed2d95d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,
The user configured max parallelism does not take effect when using adaptive batch scheduler,FLINK-30917,13523189,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,wanglijie,wanglijie,wanglijie,06/Feb/23 08:43,15/Feb/23 06:19,13/Jul/23 08:29,15/Feb/23 06:19,1.16.1,1.17.0,,,,,1.16.2,1.17.0,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"Currently, the adaptive batch scheduler only respects the global maximum parallelism(which is configured by option {{parallelism.default}} or {{execution.batch.adaptive.auto-parallelism.max-parallelism}}, see FLINK-30686 for details) when deciding parallelism for job vertices, the maximum parallelism of vertices configured by the user through {{setMaxParallelism}} will not be respected.

In this ticket, we will change the behavior so that the user-configured max parallelism also be respected.",,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 06:18:01 UTC 2023,,,,,,,,,,"0|z1fo7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 06:05;wanglijie;master: b3998324b685afc779954f7e54cc0d8f281267ec
release-1.17 : e1c6352d18a19403e3eb80736c58b842de21bc88;;;","15/Feb/23 06:18;wanglijie;release-1.16: d83044908a019d2c7f73a6c823276968b9d2ce23;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap fails with assertion,FLINK-30910,13523169,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,mapohl,mapohl,mapohl,06/Feb/23 07:56,08/Feb/23 07:51,13/Jul/23 08:29,08/Feb/23 07:51,1.15.3,1.16.1,1.17.0,,,,1.15.4,1.16.2,1.17.0,1.18.0,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"A build failure in {{ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45722&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9831

{code}
Feb 05 01:13:44 [ERROR] Tests run: 30, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.174 s <<< FAILURE! - in org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest
Feb 05 01:13:44 [ERROR] org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap  Time elapsed: 2.026 s  <<< FAILURE!
Feb 05 01:13:44 org.opentest4j.AssertionFailedError: 
Feb 05 01:13:44 
Feb 05 01:13:44 Expecting value to be true but was false
Feb 05 01:13:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
Feb 05 01:13:44 	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
Feb 05 01:13:44 	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
Feb 05 01:13:44 	at org.apache.flink.client.deployment.application.ApplicationDispatcherBootstrapTest.testApplicationIsStoppedWhenStoppingBootstrap(ApplicationDispatcherBootstrapTest.java:361)
[...]
{code}",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 08 07:51:57 UTC 2023,,,,,,,,,,"0|z1fo2w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 11:20;mapohl;[~chesnay] do you have capacity to look into this issue?;;;","06/Feb/23 12:42;mapohl;{quote}
Chesnay Schepler do you have capacity to look into this issue?
{quote}

Never mind, the other investigation went faster than expected. I'm gonna pick this one as well. Let me know if you already started on it.;;;","06/Feb/23 13:04;mapohl;It looks like the bootstrap code finished faster than the test executed the stop call. This results in the {{applicationExecutionFuture}} future completing and not being cancelled. This can be reproduced by adding a {{Thread.sleep(1000)}} before the [stop call|https://github.com/apache/flink/blob/6da5d36243329497b3b005b1900ba01f1a9d8935/flink-clients/src/test/java/org/apache/flink/client/deployment/application/ApplicationDispatcherBootstrapTest.java#L350];;;","08/Feb/23 07:51;mapohl;master: a6de5f23a30fcd580f4d005ade0fb2eba0e901dc
1.17: 5ded7b320eff4fe7587d9eeb1fe5fa87e217718b
1.16: cb83c268df09426060d1d45cc7fde9a999637691
1.15: eaf35c80342ab0df2fd70ef730e9299521b5a9d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fatal error in ResourceManager caused YARNSessionFIFOSecuredITCase.testDetachedMode to fail,FLINK-30908,13523165,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,xtsong,mapohl,mapohl,06/Feb/23 07:28,03/May/23 15:03,13/Jul/23 08:29,20/Feb/23 02:12,1.17.0,,,,,,1.17.0,,,,,,,,Deployment / YARN,Runtime / Coordination,,,0,pull-request-available,test-stability,,"There's a build failure in {{YARNSessionFIFOSecuredITCase.testDetachedMode}} which is caused by a fatal error in the ResourceManager:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45720&view=logs&j=245e1f2e-ba5b-5570-d689-25ae21e5302f&t=d04c9862-880c-52f5-574b-a7a79fef8e0f&l=29869

{code}
Feb 05 02:41:58 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Feb 05 02:41:58 java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client.call(Client.java:1480) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client.call(Client.java:1422) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at com.sun.proxy.$Proxy31.allocate(Unknown Source) ~[?:?]
Feb 05 02:41:58 	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-3.2.3.jar:?]
Feb 05 02:41:58 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
Feb 05 02:41:58 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
Feb 05 02:41:58 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
Feb 05 02:41:58 	at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at com.sun.proxy.$Proxy32.allocate(Unknown Source) ~[?:?]
Feb 05 02:41:58 	at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:325) ~[hadoop-yarn-client-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:311) [hadoop-yarn-client-3.2.3.jar:?]
Feb 05 02:41:58 Caused by: java.lang.InterruptedException
Feb 05 02:41:58 	at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]
Feb 05 02:41:58 	at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1180) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	at org.apache.hadoop.ipc.Client.call(Client.java:1475) ~[hadoop-common-3.2.3.jar:?]
Feb 05 02:41:58 	... 17 more
{code}",,leonard,mapohl,wangyang0918,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-31169,FLINK-31974,FLINK-22312,,FLINK-31609,,,,,"06/Feb/23 10:41;mapohl;mvn-1.FLINK-30908.log;https://issues.apache.org/jira/secure/attachment/13055177/mvn-1.FLINK-30908.log",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 21 02:01:53 UTC 2023,,,,,,,,,,"0|z1fo20:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 09:52;mapohl;Not sure, yet, whether that's related but there's a {{ApplicationAttemptNotFoundException}} which causes application {{application_1675564836997_0002}} to be killed:
{code}
02:41:17,442 [IPC Server handler 9 on default port 46716] INFO  org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl [] - Stopping container with container Id: container_1675564836997_0002_01_000002
02:41:17,458 [IPC Server handler 2 on default port 45213] ERROR org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService [] - Application attempt appattempt_1675564836997_0002_000001 doesn't exist in ApplicationMasterService cache.
02:41:17,459 [IPC Server handler 2 on default port 45213] INFO  org.apache.hadoop.ipc.Server                                 [] - IPC Server handler 2 on default port 45213, call Call#8 Retry#0 org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.allocate from 192.168.144.2:35386
org.apache.hadoop.yarn.exceptions.ApplicationAttemptNotFoundException: Application attempt appattempt_1675564836997_0002_000001 doesn't exist in ApplicationMasterService cache.
        at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:407) ~[hadoop-yarn-server-resourcemanager-3.2.3.jar:?]
        at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60) ~[hadoop-yarn-common-3.2.3.jar:?]
        at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99) ~[hadoop-yarn-api-3.2.3.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:549) ~[hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:518) ~[hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1086) ~[hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:1029) [hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:957) [hadoop-common-3.2.3.jar:?]
        at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_292]
        at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_292]
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762) [hadoop-common-3.2.3.jar:?]
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2957) [hadoop-common-3.2.3.jar:?]
02:41:17,560 [Listener at 2c636c5cfb18/37932] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl        [] - Killed application application_1675564836997_0002
{code}

Application container {{container_1675564836997_0002_01_000002}} is used for the TaskManager ({{container_1675564836997_0002_01_000001}} for the JobManager).;;;","06/Feb/23 09:58;mapohl;The above mentioned error happens closely (time-wise) to the fatal error in Flink's ResourceManager:
{code}
2023-02-05 02:41:17,459 ERROR org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl [] - Exception on heartbeat
java.io.InterruptedIOException: Interrupted waiting to send RPC request to server
    at org.apache.hadoop.ipc.Client.call(Client.java:1480) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.ipc.Client.call(Client.java:1422) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:231) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:118) ~[hadoop-common-3.2.3.jar:?]
    at com.sun.proxy.$Proxy31.allocate(Unknown Source) ~[?:?]
    at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationMasterProtocolPBClientImpl.allocate(ApplicationMasterProtocolPBClientImpl.java:77) ~[hadoop-yarn-common-3.2.3.jar:?]
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_292]
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_292]
    at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_292]
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359) ~[hadoop-common-3.2.3.jar:?]
    at com.sun.proxy.$Proxy32.allocate(Unknown Source) ~[?:?]
    at org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.allocate(AMRMClientImpl.java:325) ~[hadoop-yarn-client-3.2.3.jar:?]
    at org.apache.hadoop.yarn.client.api.async.impl.AMRMClientAsyncImpl$HeartbeatThread.run(AMRMClientAsyncImpl.java:311) [hadoop-yarn-client-3.2.3.jar:?]
Caused by: java.lang.InterruptedException
    at java.util.concurrent.FutureTask.awaitDone(FutureTask.java:404) ~[?:1.8.0_292]
    at java.util.concurrent.FutureTask.get(FutureTask.java:191) ~[?:1.8.0_292]
    at org.apache.hadoop.ipc.Client$Connection.sendRpcRequest(Client.java:1180) ~[hadoop-common-3.2.3.jar:?]
    at org.apache.hadoop.ipc.Client.call(Client.java:1475) ~[hadoop-common-3.2.3.jar:?]
    ... 17 more
{code} ;;;","06/Feb/23 10:31;mapohl;The WordCount job finishes successfully. This triggers the shutdown from the Flink side.
* Application finishes at {{2023-02-05 02:41:17,116}} where the JobMaster is removed from the ResourceManager
* YarnTestBase sends an application status at {{02:41:17,357}} and triggers the shutdown afterwards
* Kill request received on the Yarn side at {{02:41:17,359}}
* ClusterEntrypoint receives SIGTERM at {{02:41:17,381}};;;","06/Feb/23 10:32;mapohl;-There is an KRB error reported on INFO log level by the KdcHandler at {{02:41:13:755}} Just to be sure: [~bamrabi]: This issue is actual not a problem, is it? ...considering that we worked on Kerberos support in 1.17.-
{code:java}
02:41:13,755 [     pool-3-thread-1] INFO  org.apache.kerby.kerberos.kerb.server.request.KdcRequest     [] - The preauth data is empty.
02:41:13,755 [     pool-3-thread-1] INFO  org.apache.kerby.kerberos.kerb.server.KdcHandler             [] - KRB error occurred while processing request:Additional pre-authentication required
02:41:13,808 [     pool-3-thread-1] INFO  org.apache.kerby.kerberos.kerb.server.request.AsRequest      [] - AS_REQ ISSUE: authtime 1675564873807,hadoop/localhost@EXAMPLE.COM for krbtgt/EXAMPLE.COM@EXAMPLE.COM
{code}
{*}Update{*}: The concern shared above shouldn't be the reason for the cause. The error appears during the graceful shutdown of the JobManager process/ResourceManager. It looks like a race condition between YARN and Flink. There's s still communication happening between the ResourceManager and the TaskManager while the TaskManager is shut down by YARN.;;;","06/Feb/23 10:50;mapohl;[~xtsong], [~huwh] may you have a look at it? Could that be an issue related to your work on FLINK-20988 subtasks?;;;","06/Feb/23 11:13;mapohl;I've found a comment from Xintong in FLINK-22312 on this error:
{quote}
The InterruptedIOException is expected. It can happen if AMRMClient happens to be waiting for a heartbeat response when the cluster is shutdown.
I'll add it to the whitelist for the log prohibited string checking.
{quote}

We also whitelist this error in [YarnTestBase:172-174|https://github.com/apache/flink/blob/573ed922346c791760d27653543c2b8df56f51f7/flink-yarn-tests/src/test/java/org/apache/flink/yarn/YarnTestBase.java#L172-L174]. Do we need to find a way to avoid triggering the fatal error handler for this issue? ;;;","06/Feb/23 11:15;mapohl;Can we assume based on FLINK-22312 that it's a ongoing issue that is not necessarily only an issue in 1.17 meaning that this issue isn't a blocker for the 1.17 release?;;;","06/Feb/23 11:44;xtsong;This is indeed related to FLINK_20988. The {{InterruptedIOException}} is expected and is included in the whitelist for prohibited string checking. However, this should not be handled as a fatal error.

I think this is a blocker. Depending on which process finishes first (the gracefully shutdown and the fatal error handling), it may cause the process to terminate with an error exit code, which lead to restarting of the application being shutdown.

I'll provide a fix for it asap.;;;","07/Feb/23 03:48;xtsong;After looking more into the logs and Hadoop codes, we believe FLINK-20988 is not the cause of this failure.

The test failure is caused by:
1. {{AMRMClientAsync}} sends an {{InterruptedIOException}} to the callback handler ({{YarnContainerEventHandler}}) after being stopped.
2. All errors sent to {{YarnContainerEventHandler}} are treated as fatal error in Flink.

This is not a newly introduced issue. 1) exists in Hadoop 2.9+ versions (https://issues.apache.org/jira/browse/YARN-5999), and 2) is the behavior since yarn deployment is supported. FLINK-20988 did introduce another chance for exceptions during shutdown to be handled as fatal error, but that is not the cause of this test failure. Given that this issue already exist in previous releases, I'm downgrading this ticket to Critical priority.

The proper fix might be to ignore the exceptions in {{YarnContainerEventHandler}} after being terminated. I'll update the PR and fix this.;;;","07/Feb/23 03:59;wangyang0918;+1 for Xintong's analysis and proposal.

YARN-5999 introduced a side effect that {{CallbackHandler#onError}} will have a chance to be executed when stopping the AMRMAyncClient.;;;","20/Feb/23 02:12;xtsong;- master (1.18): 6b47f45ffd22e0ff332a528dccfbd8b664e28702
- release-1.17: 6b47f45ffd22e0ff332a528dccfbd8b664e28702;;;","20/Feb/23 11:22;mapohl;Thanks for providing a fix for that issue, [~xtsong]. Just to fix things (the matching commit hash caught my attention) - here's an updated of the commits per branch:
* master (1.18): 
** 296ee78efdd5c19308a2967f5b7a2f994324b0f4
** 9b92a89c1853e140be3889df47c12bc36f42a96b
* release-1.17: 
** aecefe756df39b44b18574483a02b6753b8092fc
** 6b47f45ffd22e0ff332a528dccfbd8b664e28702 ;;;","21/Feb/23 02:01;xtsong;Nice catch, thanks [~mapohl].;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The max parallelism used in adaptive batch scheduler doesn't fallbacks to default parallelism,FLINK-30903,13523148,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,06/Feb/23 06:06,07/Feb/23 08:09,13/Jul/23 08:29,07/Feb/23 08:09,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"In FLINK-30684 we mark the vertices which use the default parallelism, and in AdaptiveBatchScheduler we allow users to use parallelism.default as the max parallelism if they don't configure the configuration item ""execution.batch.adaptive.auto-parallelism.max-parallelism"". This issue will fix the fallback logic.

 ",,JunRuiLi,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 08:09:17 UTC 2023,,,,,,,,,,"0|z1fny8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 08:09;zhuzh;Fixed via 4bc6d6dac29b1d7d89850a512b8283858824ae21;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The jobVertex's parallelismConfigured is incorrect when chaining with source operators,FLINK-30901,13523123,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,06/Feb/23 03:21,07/Feb/23 11:17,13/Jul/23 08:29,07/Feb/23 11:17,1.17.0,,,,,,1.17.0,1.18.0,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"When creating OperatorChainInfo in StreamingJobGenerator, the chained source are not included in OperatorChainInfo#chainedNodes, because they are not added to OperatorChainInfo via #addNodeToChain().

This will affect jobVertex which has a MultiInput operator chained with sources. The vertex's parallelismConfigured will be false even if the chained sources have a parallelism configured. ",,JunRuiLi,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30685,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 11:17:32 UTC 2023,,,,,,,,,,"0|z1fnso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 11:17;zhuzh;master:
10ced269a8ec3970d2f567109e8983eb7202d45c

release-1.17:
5e198e57df35a9bc0b979f9e79e22872e753211c;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SlotSharingSlotAllocator may waste slots,FLINK-30895,13522930,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,chesnay,chesnay,chesnay,03/Feb/23 10:30,20/Feb/23 11:20,13/Jul/23 08:29,16/Feb/23 13:51,1.16.0,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"The allocated evenly distributes slots across slot sharing groups independent of how many slots the vertices in that group actually need.

This can cause slots to be unused.",,ram_krish,roman,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,FLINK-30931,,,,,,,,,,,FLINK-31119,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 16 13:51:42 UTC 2023,,,,,,,,,,"0|z1fmls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Feb/23 13:51;chesnay;master: 97e1dbc90cc3ea6e6e48e901813ab3bfbb693c6a
1.17: 9e1cf08e24677108758a227a42384a482885b371;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix the key of configuration SPECULATIVE_ENABLED.,FLINK-30889,13522861,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,03/Feb/23 03:41,03/Feb/23 09:33,13/Jul/23 08:29,03/Feb/23 08:54,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Configuration,,,,0,pull-request-available,,,"In FLINK-30686 we refined the adaptive batch configuration. However, the key of a configuration item `SPECULATIVE_ENABLED` has been modified incorrectly. According to flip-283, it should be modified to `execution.batch.speculative.enabled`.",,JunRuiLi,leonard,mapohl,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30682,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 09:33:44 UTC 2023,,,,,,,,,,"0|z1fm6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 08:39;mapohl;master: 8efd5e8c809d607529bb251f6f034893c918905e;;;","03/Feb/23 08:41;mapohl;[~JunRuiLi] can we close this issue? The build on {{master}} succeeded (ignoring the e2e1 module which is caused by FLINK-30881): https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45664&view=results

The [next build|https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45669&view=results] on {{master}} will include your change and the FLINK-30881: That one should, hopefully turn green. Or maybe, just close this issue after the next build is done. ...to be on the safe side.;;;","03/Feb/23 08:54;JunRuiLi;Thanks [~mapohl] , I'll close this issue.;;;","03/Feb/23 09:00;mapohl;Thanks [~JunRuiLi]. Just on a side note: I removed the release note entry again. ""Release note"" is meant to be filled by a description that would end up in the release notes of the corresponding release (in this case 1.17.0). But it looks like you accidentally added the version there. If you think that this change deserves release notes, you might come up with something more meaningful to the user describing this fix. :-);;;","03/Feb/23 09:33;JunRuiLi;[~mapohl] Thank you for reminding me. I accidentally made a mistake.:D;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Flink Doc(zh version) has misspelled words ,FLINK-30888,13522845,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,marlin,marlin,03/Feb/23 00:42,17/Apr/23 03:59,13/Jul/23 08:29,13/Apr/23 03:27,,,,,,,1.18.0,,,,,,,,,,,,0,pull-request-available,,,"[https://github.com/apache/flink/blob/master/docs/content.zh/docs/dev/datastream/overview.md]

Iterative streaming 程序实现了 +{color:#de350b}setp function{color}+ 并将其嵌入到 {{IterativeStream}} 。由于 DataStream 程序可能永远不会完成，因此没有最大迭代次数。相反，你需要指定流的哪一部分反馈给迭代，哪一部分使用[旁路输出](\{{< ref ""docs/dev/datastream/side_output"" >}})或{{{}过滤器{}}}转发到下游。

==> step function

 ",,marlin,paul8263,tanyuxin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Apr 13 03:28:00 UTC 2023,,,,,,,,,,"0|z1fm34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 09:02;paul8263;Hi all,

I would like to fix this.;;;","13/Apr/23 03:28;tanyuxin;I found the typo has been fixed. 
https://github.com/apache/flink/blob/master/docs/content.zh/docs/dev/datastream/overview.md;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CI workflow for externalized connectors doesn't cache Flink's binary download,FLINK-30887,13522790,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,martijnvisser,martijnvisser,martijnvisser,02/Feb/23 13:25,09/Feb/23 16:05,13/Jul/23 08:29,02/Feb/23 13:36,,,,,,,,,,,,,,,Build System / CI,,,,0,pull-request-available,,,The current CI workflow for PRs that runs on externalized connectors doesn't cache Flink binary downloads properly. We should fix this. ,,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 13:36:16 UTC 2023,,,,,,,,,,"0|z1flqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 13:36;martijnvisser;Fixed in ci_utils:
7227de526e09f0a891a4a8ec05f7054c1f4bac52
9141a06c6a6d44d6a483bf602fe8f9dfd03e82ac
2f47df76f83d5e07e5e736b0344585d77cf51c85;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optional group pattern starts with non-optional looping pattern get wrong result on followed-by,FLINK-30885,13522782,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,02/Feb/23 12:31,17/Feb/23 07:43,13/Jul/23 08:29,14/Feb/23 09:14,1.15.3,1.16.1,,,,,1.15.4,1.16.2,1.17.0,,,,,,Library / CEP,,,,0,,,,"{code:java}
Pattern.begin(""A"")
  .followedBy(
    Pattern.begin(""B"").oneOrMore().greedy().consecutive()
      .next(""C""))
  .optional()
  .next(""D""){code}
This can match ""a1 e1 d1"", which is not the expected behavior.",,dianfu,Juntao Hu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 14 09:14:14 UTC 2023,,,,,,,,,,"0|z1flp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 09:14;dianfu;Fixed in:
- master via 845d04d07d623e93a5bc3276eab45eed9edec264
- release-1.17 via cf448f05c5605f37dd170f8709f28d836feba5a6
- release-1.16 via b13a11c04c0fe96ee5a05878fbab99df7ec3478f
- release-1.15 via 1ac1e0d7dd078a85cb0f127aa75e161e03b2fd41;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crictl/Minikube version mismatch causes errors in k8s setup,FLINK-30881,13522764,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,02/Feb/23 11:12,03/Feb/23 08:55,13/Jul/23 08:29,03/Feb/23 07:45,1.15.3,1.16.1,1.17.0,,,,1.15.4,1.16.2,1.17.0,,,,,,Deployment / Kubernetes,Test Infrastructure,,,0,pull-request-available,test-stability,,"We observed constant failures in the e2e k8s tests with permission issues. This was initially accidentally reported through FLINK-29671. But FLINK-29671 actually covers a different instability.

Here are the build failures initially reported in FLINK-29671:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45548&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4972]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4900]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45588&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=5597]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4818]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45587&view=logs&j=e9d3d34f-3d15-59f4-0e3e-35067d100dfe&t=f8a6d3eb-38cf-5cca-9a99-d0badeb5fe62&l=5852]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45591&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4915]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45598&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4921]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45603&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4991]

{code:java}
Feb 01 11:00:45 Starting minikube ...
Feb 01 11:00:45 * minikube v1.29.0 on Ubuntu 20.04
Feb 01 11:00:45 * Using the none driver based on existing profile
Feb 01 11:00:45 * Starting control plane node minikube in cluster minikube
Feb 01 11:00:45 * Restarting existing none bare metal machine for ""minikube"" ...
Feb 01 11:00:45 * OS release is Ubuntu 20.04.5 LTS
Feb 01 11:01:22 
X Exiting due to RUNTIME_ENABLE: Temporary Error: sudo /usr/local/bin/crictl version: exit status 1
stdout:
[...]  
Feb 01 11:01:22 
E0201 11:01:22.809164  241870 root.go:80] failed to log command start to audit: failed to open the audit log: open /home/vsts/.minikube/logs/audit.json: permission denied
Feb 01 11:01:22 
X Exiting due to HOST_HOME_PERMISSION: open /home/vsts/.minikube/profiles/minikube/config.json: permission denied
* Suggestion: Your user lacks permissions to the minikube profile directory. Run: 'sudo chown -R $USER $HOME/.minikube; chmod -R u+wrx $HOME/.minikube' to fix
* Related issue: https://github.com/kubernetes/minikube/issues/9165
[...]{code}",,leonard,mapohl,renqs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-29671,,,,,FLINK-30893,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 08:55:55 UTC 2023,,,,,,,,,,"0|z1fll4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 11:14;mapohl;[~wangyang0918] reported in FLINK-29671:

{quote}
It seems that the minikube in the azure pipeline upgraded from v1.28.0 to v1.29.0. So it might be with incompatible crictl@v1.24.2.
{quote};;;","02/Feb/23 11:26;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45603&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4951;;;","02/Feb/23 13:19;mapohl;Are you going to provide a fix, [~wangyang0918]?;;;","02/Feb/23 14:17;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45616&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5878
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45617&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4857
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45618&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5126;;;","02/Feb/23 15:09;mapohl;There's a Minikube Github issue talking about problem: https://github.com/kubernetes/minikube/issues/15758 ;;;","02/Feb/23 15:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45625&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5050;;;","02/Feb/23 23:06;mapohl;[~wangyang0918] I fixed the Minikube version to 1.28.0 now. I think having a fixed Minikube version in general instead of relying on whatever Azure's container is offering is a good thing for more stable builds, anyway. Feel free to approve and merge the PRs if you agree.;;;","03/Feb/23 06:09;renqs;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45651&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5674]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45651&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5658]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45651&view=logs&j=af885ea8-6b05-5dc2-4a37-eab9c0d1ab09&t=f779a55a-0ffe-5bbc-8824-3a79333d4559&l=5088]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45652&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5639] 

 ;;;","03/Feb/23 07:45;mapohl;master: 0e4fb6bbfbcf9bb623a2b26826e53d720b74c898
1.16: e13a73f36e8f3582c60253bfda81823c85b6f3ca
1.15: 2cef2cb3f9cfd331bbf493f0d1648b688f9e9c65;;;","03/Feb/23 08:55;mapohl;Some pre-fix build failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45660&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5007
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45659&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4943
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45664&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=4970;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KubernetesHighAvailabilityRecoverFromSavepointITCase fails due to a deadlock,FLINK-30878,13522716,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,mapohl,mapohl,mapohl,02/Feb/23 08:30,02/Feb/23 11:03,13/Jul/23 08:29,02/Feb/23 11:03,1.15.4,1.16.2,1.17.0,,,,1.15.4,1.16.2,1.17.0,,,,,,Deployment / Kubernetes,Runtime / Coordination,,,0,pull-request-available,test-stability,,"We're seeing a test failure in {{KubernetesHighAvailabilityRecoverFromSavepointITCase}} due to a deadlock:
{code:java}
2023-02-01T18:53:35.5540322Z ""ForkJoinPool-1-worker-1"" #14 daemon prio=5 os_prio=0 tid=0x00007f68ecb18000 nid=0x43dd1 waiting on condition [0x00007f68c1711000]
2023-02-01T18:53:35.5540900Z    java.lang.Thread.State: TIMED_WAITING (parking)
2023-02-01T18:53:35.5541272Z 	at sun.misc.Unsafe.park(Native Method)
2023-02-01T18:53:35.5541932Z 	- parking to wait for  <0x00000000d14d7b60> (a java.util.concurrent.CompletableFuture$Signaller)
2023-02-01T18:53:35.5542496Z 	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
2023-02-01T18:53:35.5543088Z 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1709)
2023-02-01T18:53:35.5543672Z 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
2023-02-01T18:53:35.5544240Z 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1788)
2023-02-01T18:53:35.5544801Z 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
2023-02-01T18:53:35.5545632Z 	at org.apache.flink.kubernetes.highavailability.KubernetesHighAvailabilityRecoverFromSavepointITCase.testRecoverFromSavepoint(KubernetesHighAvailabilityRecoverFromSavepointITCase.java:113)
2023-02-01T18:53:35.5546409Z 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) {code}
[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45565&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=61916]

The build failure happens on 1.16. I'm adding 1.17 and 1.15 as fixVersions as well because it might be due to some recent changes which were introduced with FLINK-30462 and/or FLINK-30474",,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30474,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 11:03:06 UTC 2023,,,,,,,,,,"0|z1flag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 10:30;mapohl;{code:java}
Java stack information for the threads listed above:
===================================================
""config-map-watch-handler-thread-2851"":
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$WatchCallback.lambda$run$0(KubernetesSharedInformer.java:243)
        - waiting to lock <0x00000000d14d1928> (a java.lang.Object)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$WatchCallback$$Lambda$879/261416860.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
""config-map-watch-handler-thread-4"":
        at org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.notifyAllKnownLeaderInformation(DefaultMultipleComponentLeaderElectionService.java:264)
        - waiting to lock <0x00000000d14d6bd0> (a java.lang.Object)
        at org.apache.flink.kubernetes.highavailability.KubernetesMultipleComponentLeaderElectionDriver$ConfigMapCallbackHandlerImpl.onModified(KubernetesMultipleComponentLeaderElectionDriver.java:238)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$EventHandler.lambda$null$3(KubernetesSharedInformer.java:208)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$EventHandler$$Lambda$945/45359867.accept(Unknown Source)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$WatchCallback.lambda$run$0(KubernetesSharedInformer.java:246)
        - locked <0x00000000d14d1928> (a java.lang.Object)
        at org.apache.flink.kubernetes.kubeclient.resources.KubernetesSharedInformer$WatchCallback$$Lambda$879/261416860.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
""leadershipOperationExecutor-thread-1"":
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.onLeaderInformationChange(DefaultLeaderElectionService.java:254)
        - waiting to lock <0x00000000d14d6bf8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.lambda$sendLeaderInformationChange$3(DefaultMultipleComponentLeaderElectionService.java:254)
        - locked <0x00000000d14d6bd0> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService$$Lambda$948/665822281.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:750)
""mini-cluster-io-thread-2"":
        at org.apache.flink.runtime.leaderelection.DefaultMultipleComponentLeaderElectionService.hasLeadership(DefaultMultipleComponentLeaderElectionService.java:180)
        - waiting to lock <0x00000000d14d6bd0> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.MultipleComponentLeaderElectionDriverAdapter.hasLeadership(MultipleComponentLeaderElectionDriverAdapter.java:51)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.hasLeadership(DefaultLeaderElectionService.java:155)
        - locked <0x00000000d14d6bf8> (a java.lang.Object)
        at org.apache.flink.runtime.leaderelection.DefaultLeaderElectionService.confirmLeadership(DefaultLeaderElectionService.java:120)
        - locked <0x00000000d14d6bf8> (a java.lang.Object)
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner.lambda$forwardConfirmLeaderSessionFuture$2(DefaultDispatcherRunner.java:175)
        at org.apache.flink.runtime.dispatcher.runner.DefaultDispatcherRunner$$Lambda$886/1947395283.accept(Unknown Source)
        at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
        at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
{code}
The deadlock occurs due to the lock being acquired in FLINK-30474 before triggering the {{onLeaderInformationChange}} in [DefaultMultipleComponentLeaderElectionService.java|https://github.com/apache/flink/pull/21537/commits/dbce30603bc9436032a98ae1b9d33f99e0790099#diff-8e1b9ed8178f117c8e5f3be67ce6f5b341862d06ce27f14d52901b503f6a3a0bR254]
[:254|https://github.com/apache/flink/pull/21537/commits/dbce30603bc9436032a98ae1b9d33f99e0790099#diff-8e1b9ed8178f117c8e5f3be67ce6f5b341862d06ce27f14d52901b503f6a3a0bR254];;;","02/Feb/23 10:53;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45592&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=34767;;;","02/Feb/23 11:03;mapohl;master: e3cd3b311c1c8a6a0e0cdc849d7c951ef8beea5c
1.16: 3366c1f941c45e7fc47d57dc46ca7728bc1df33b
1.15: 8ee1ceb2f532f06d3cceefefc204772fb9b83594;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ResetTransformationProcessor don't reset the transformation of ExecNode in BatchExecMultiInput.rootNode,FLINK-30876,13522711,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,02/Feb/23 08:21,06/Feb/23 03:23,13/Jul/23 08:29,06/Feb/23 03:23,1.16.1,1.17.0,,,,,1.16.2,1.17.0,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"Now, ResetTransformationProcessor don't reset the transformation of ExecNode in BatchExecMultiInput.rootNode. This may cause error while creating StreamGraph for BatchExecMultiInput due to different id of rootNode and inputNode.",,337361684@qq.com,godfrey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 02:58:30 UTC 2023,,,,,,,,,,"0|z1fl9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 02:58;godfrey;Fixed in master: 111342f37bdc0d582d3f7af458d9869f0548299f

1.16.2: 00a4ab9011cf13facde364c52a170a7a897cdcce;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix usages of legacy AdaptiveBatchScheduler configuration,FLINK-30875,13522705,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,02/Feb/23 07:43,03/Feb/23 10:28,13/Jul/23 08:29,03/Feb/23 10:28,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"In FLINK-30686, we deprecated the JobManagerOptions's AdaptiveBatchScheduler configuration. However, these configuration items still have some calls. And we should change these calls to new configuration.",,JunRuiLi,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 10:28:06 UTC 2023,,,,,,,,,,"0|z1fl80:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 10:28;zhuzh;Fixed via 851771035ed964063a9bfec8a59b4669a58d8aa1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regressions notifications in Slack are cut off,FLINK-30870,13522480,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Yanfei Lei,martijnvisser,martijnvisser,01/Feb/23 13:15,10/Feb/23 07:41,13/Jul/23 08:29,10/Feb/23 07:41,,,,,,,,,,,,,,,Benchmarks,,,,0,pull-request-available,,,"Example from today at https://apache-flink.slack.com/archives/C0471S0DFJ9/p1675253720571659

{code}
Performance regression
mapRebalanceMapSink.F27_UNBOUNDED median=17231.7398765 recent_median=16165.0549395
multiInputOneIdleMapSink median=11254.5329375 recent_median=10727.7280915
calculateRegionToRestart.BATCH median=12.881527 recent_median=12.096391
partitionRelease.BATCH median=23.2130145 recent_median=21.4858475
checkpointMultiInput median=2.6094395 recent_median=2.477736
checkpointSingleInput.UNALIGNED median=339.229515 recent_median=67.2695295
checkpointSingleInput.UNALIGNED_1 median=215.2789775 recent_median=40.1294965
fireProcessingTimers median=50.9977185 recent_median=44.0925955
globalWindow median=5459.689767 recent_median=5045.436655
<http…
{code}

As you can see, the last part is cut off with {{<htt...}} listed. I'm not sure if this is because there are more regressions, but they aren't posted to Slack (could be) or if this is just a mistake in the output and the {{globalWindow}} was the last regression to be reported. It would be great if this could be validated. ",,fanrui,leonard,martijnvisser,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/23 04:26;Yanfei Lei;image-2023-02-06-12-26-57-268.png;https://issues.apache.org/jira/secure/attachment/13055171/image-2023-02-06-12-26-57-268.png",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 07:41:49 UTC 2023,,,,,,,,,,"0|z1fjuo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 13:05;martijnvisser;[~Yanfei Lei] Could you help clarify if this is indeed an issue or if this is as expected? ;;;","03/Feb/23 02:42;Yanfei Lei;[~martijnvisser] Thanks for the ping, this is a known issue with the slack plugin of jenkinsci [https://github.com/jenkinsci/slack-plugin/issues/735#issuecomment-774131103] , because the notification messages are relatively long recently, exceeding the limit of 2048. I'm going to try it with an attachment instead of a message.;;;","03/Feb/23 03:43;leonard;I downgrade the issue priority to Major as it's a known slack plugin issue according to Yanfei's comment;;;","06/Feb/23 04:27;Yanfei Lei;The message after updating the Jenkins script became normal:

!image-2023-02-06-12-26-57-268.png|width=552,height=255!;;;","10/Feb/23 07:41;martijnvisser;Fixed in flink-benchmarks master: 1a6da18e0933b6e43297c1fc44a556c0c162b3e0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Revert to use LongSerializer for seralization in the TimeIndicatorTypeInfo,FLINK-30868,13522467,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,fsk119,fsk119,fsk119,01/Feb/23 12:22,06/Feb/23 07:13,13/Jul/23 08:29,06/Feb/23 07:13,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,,,fsk119,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 07:13:59 UTC 2023,,,,,,,,,,"0|z1fjrs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 07:13;fsk119;Merged into master: 48a2e5a440067bcab429fd9cf90e3888e0498473;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optional pattern at the start of a group pattern not working,FLINK-30864,13522456,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Juntao Hu,Juntao Hu,Juntao Hu,01/Feb/23 11:25,17/Feb/23 07:44,13/Jul/23 08:29,15/Feb/23 01:40,1.15.3,1.16.1,,,,,1.15.4,1.16.2,1.17.0,,,,,,Library / CEP,,,,0,pull-request-available,,,"The optional pattern at the start of a group pattern turns out be ""not optional"", e.g.
{code:java}
Pattern.<String>begin(""A"").next(Pattern.<String>begin(""B"").optional().next(""C"")).next(""D"")
{code}
cannot match sequence ""a1 c1 d1"".",,dianfu,Juntao Hu,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 15 01:40:42 UTC 2023,,,,,,,,,,"0|z1fjpc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"14/Feb/23 09:14;dianfu;Fixed in:
- master via 845d04d07d623e93a5bc3276eab45eed9edec264
- release-1.17 via cf448f05c5605f37dd170f8709f28d836feba5a6
- release-1.16 via b13a11c04c0fe96ee5a05878fbab99df7ec3478f
- release-1.15 via 1ac1e0d7dd078a85cb0f127aa75e161e03b2fd41;;;","14/Feb/23 12:55;mapohl;This change causes compiler errors in {{release-1.16}} and {{release-1.15}}.

Reverts happened for the following branches:

1.16: 20da7a9f09b48b61c1bc6c73527e8e58aac0331f
1.15: 735609c44261d631527e492dfdfa68f0dcbd51b0;;;","15/Feb/23 01:40;dianfu;Thanks [~mapohl].

Have fixed in:
- release-1.16: 14531dac756b87eb12a6864a0850415496a6bab4
- release-1.15: 66dc917542419ff351593412d1ddde69bb624f27;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Hive Catalog throws java.lang.ClassNotFoundException: org.apache.hadoop.hive.common.ValidWriteIdList under certain environment,FLINK-30861,13522413,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,01/Feb/23 07:16,01/Feb/23 08:05,13/Jul/23 08:29,01/Feb/23 08:05,table-store-0.3.1,table-store-0.4.0,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,Table Store,,,,0,pull-request-available,,,Table Store Hive Catalog throws {{java.lang.ClassNotFoundException: org.apache.hadoop.hive.common.ValidWriteIdList}} under certain environment. We need to package {{hive-storage-api}} dependency.,,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 08:05:36 UTC 2023,,,,,,,,,,"0|z1fjg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 08:05;TsReaper;master: 5e09dfec910c2aa2dfcf3eca4adcb8191524bf04
release-0.3: 78fdd09077431a7284cd8519cca5ae06c7acb60e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Kubernetes operator does not update reconciled generation,FLINK-30858,13522370,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gyfora,thw,thw,01/Feb/23 00:50,04/Feb/23 02:10,13/Jul/23 08:29,04/Feb/23 02:10,kubernetes-operator-1.3.1,,,,,,kubernetes-operator-1.4.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Kubernetes manages the generation field as part of the spec metadata. It will be increased when changes are made to the resource. The counterpart in status is ""observed generation"", provided by a controller. By comparing the two, the client can determine that the controller has processed the spec and in conjunction with other status information conclude that a change has been reconciled.

The Flink operator currently tracks the generation as part of reconciled and stable specs but these cannot be used as ""observed generation"" to perform the check. The value isn't updated in cases where operator determines that there are no changes to the spec that require deployment. This can be reproduced through PUT/replace with the same spec or a change in upgrade mode.

The operator should provide the observed spec, which in conjunction with deployment state can then be used by clients to determine that the spec has been reconciled.",,gyfora,mbalassi,thw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Feb 04 02:10:48 UTC 2023,,,,,,,,,,"0|z1fj74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 01:08;thw;https://lists.apache.org/thread/8y1zp4ogssy8ltsl42ppzvbo64dlzc3v;;;","01/Feb/23 08:09;gyfora;I discussed this with Max yesterday, and it should also be possible to use the current generation metadata in the status to track the last observed generation with a slight change to the reconciler logic.

I just want to point out that the original intention with the generation field was to be able to match deployed (jobmanager generation label) with the last reconciled spec for resiliency to failures during deploy operations. This is not currently intended as a user facing information.

With this change however it would be easier to is for users, I agree :);;;","04/Feb/23 02:10;mbalassi;b45122c in main;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create table does not create topic with multiple partitions,FLINK-30857,13522296,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,vicky_papavas,vicky_papavas,31/Jan/23 20:01,29/Mar/23 02:04,13/Jul/23 08:29,29/Mar/23 02:04,,,,,,,,,,,,,,,Table Store,,,,0,,,," 
{code:java}
CREATE CATALOG table_store_catalog WITH (
    'type'='table-store',
    'warehouse'='s3://my-bucket/table-store'
 );
USE CATALOG table_store_catalog;
SET 'execution.checkpointing.interval' = '10 s';
CREATE TABLE word_count_kafka (
     word STRING PRIMARY KEY NOT ENFORCED,
     cnt BIGINT
 ) WITH (
     'log.system' = 'kafka',
     'kafka.bootstrap.servers' = 'broker:9092',
     'kafka.topic' = 'word_count_log',
     'bucket'='4'
 );
{code}
 

The created topic has only one partition
{code:java}
Topic: word_count_log    TopicId: udeJwBIkRsSybkf1EerphA    PartitionCount: 1    ReplicationFactor: 1    Configs:
    Topic: word_count_log    Partition: 0    Leader: 1    Replicas: 1    Isr: 1{code}",,lzljs3620320,martijnvisser,paul8263,vicky_papavas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 02:04:33 UTC 2023,,,,,,,,,,"0|z1fiqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Feb/23 08:48;paul8263;Hi [~vicky_papavas] ,

Could you please tell me what version of Kafka are you currently using? Thanks.;;;","07/Feb/23 08:30;paul8263;Hi all,

I am currently investigating this issue.

It seems that the onCreateTable method in flink-table-store-kafka/src/main/java/org/apache/flink/table/store/kafka/KafkaLogStoreFactory.java invokes AdminClient of Kafka to create topic with number of partitions assigned. The relevant codes are listed below:
{code:java}
@Override
public void onCreateTable(Context context, int numBucket, boolean ignoreIfExists) {
    Configuration options = Configuration.fromMap(context.getCatalogTable().getOptions());
    try (AdminClient adminClient = AdminClient.create(toKafkaProperties(options))) {
        Map<String, String> configs = new HashMap<>();
        options.getOptional(LOG_RETENTION)
                .ifPresent(
                        retention ->
                                configs.put(
                                        TopicConfig.RETENTION_MS_CONFIG,
                                        String.valueOf(retention.toMillis())));
        NewTopic topicObj =
                new NewTopic(topic(context), Optional.of(numBucket), Optional.empty())
                        .configs(configs);
        adminClient.createTopics(Collections.singleton(topicObj)).all().get();

// ...

} {code}
However, onCreateTable method never has the chance to be executed, as Flink table store catalog(implemented by flink-table-store-connector/src/main/java/org/apache/flink/table/store/connector/FlinkCatalog.java) does not support managed table (it does not override thesupportsManagedTable method in org/apache/flink/table/catalog/Catalog.java).

I tried to make it support managed table and updated the sql as below:
{code:java}
                String.format(
                        ""CREATE TABLE T (a STRING, b STRING, c STRING) WITH (""
                                + ""'log.system'='kafka', ""
                                + ""'root-path'='/path/to/tablestore-data',""
                                + ""'kafka.bootstrap.servers'='%s',""
                                + ""'kafka.transaction.timeout.ms'='300000',""
                                + ""'table.type'='MANAGED_TABLE',""
                                + ""'bucket'='9'""
                                + "")"",
                        ""kafka1:9092:kafka2:9092,kafka3:9092"", ""flink-demo"")); {code}
(As managed table does not allow customizing kafka topic so kafka.topic option was removed. )

Now the onCreateTable method is able to be invoked but I got another exception:
{code:java}
Creating topics with default partitions/replication factor are only supported in CreateTopicRequest version 4+ {code}
It says default values of partitions or replication factor are not supported while creating new topic. I made another change:

From: 
{code:java}
new NewTopic(topic(context), Optional.of(numBucket), Optional.empty()) .configs(configs); {code}
To:
{code:java}
// Optional.of((short) 3) is just a value for test purpose only
 NewTopic topicObj = new NewTopic(topic(context), Optional.of(numBucket), Optional.of((short) 3))
                            .configs(configs);{code}
Finally the topic with correct partitions and replication factor was successfully created.

 

I think there are 2 questions need some further discussion:
 # Should FlinkCatalog support managed table? Even if it is working as supporting external tables, the Kafka topic also has to be created via AdminClient.
 # Creating new topic with default partitions/replication factor by AdminClient seems to have limited support. We have to assign default partitions/replication factor in Flink configuration or SQL with clause.

 ;;;","07/Feb/23 18:57;vicky_papavas;Hi [~paul8263] ! 

Thank you for looking into this.

I am using AK 3.2.0

 ;;;","09/Feb/23 08:42;paul8263;Hi community,

After further investigation, I think FlinkCatalog should support managed table as it does not have a connector option. However org/apache/flink/table/store/kafka/KafkaLogStoreFactory.java does not support customized topic. The relevant codes are:
{code:java}
@Override
public Map<String, String> enrichOptions(Context context) {
    Map<String, String> options = new HashMap<>(context.getCatalogTable().getOptions());
    Preconditions.checkArgument(
            !options.containsKey(TOPIC.key()),
            ""Managed table can not contain custom topic. ""
                    + ""You need to remove topic in table options or session config."");

    String topic = context.getObjectIdentifier().asSummaryString();
    options.put(TOPIC.key(), topic);
    return options;
} {code}
I suggest it should support customized topic name. If the topic was not specified, it could be context.getObjectIdentifier().asSummaryString() instead.

As the issue of Kafka AdminClient, we might add default replication factor for better compatibility.

Correct me if I am wrong.

 

 

 ;;;","13/Feb/23 02:59;paul8263;Hi [~lzljs3620320] ,

As the latest hotfix removed managed table related method in LogStoreFactoryTable, it seems that it is not planned to support managed table in the future. Should we support creating Kafka topic explicitly with specified replication factor/partitions?;;;","29/Mar/23 02:04;lzljs3620320;Kafka topic should be created by users..;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskTest.testCleanupWhenSwitchToInitializationFails reports AssertionError but doesn't fail,FLINK-30852,13522217,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,akalash,mapohl,mapohl,31/Jan/23 12:00,21/Apr/23 11:50,13/Jul/23 08:29,21/Apr/23 11:50,1.17.0,,,,,,1.17.1,1.18.0,,,,,,,Runtime / Task,,,,0,pull-request-available,test-stability,,"While investigating FLINK-30844, I noticed that {{TaskTest.testCleanup}} reports an AssertionError in the logs but doesn't fail:
{code}
00:59:01,886 [                main] ERROR org.apache.flink.runtime.taskmanager.Task                    [] - Error while canceling task Test Task (1/1)#0.
java.lang.AssertionError: This should not be called
        at org.junit.Assert.fail(Assert.java:89) ~[junit-4.13.2.jar:4.13.2]
        at org.apache.flink.runtime.taskmanager.TaskTest$TestInvokableCorrect.cancel(TaskTest.java:1304) ~[test-classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.cancelInvokable(Task.java:1529) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:796) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:562) ~[classes/:?]
        at org.apache.flink.runtime.taskmanager.TaskTest.testCleanupWhenSwitchToInitializationFails(TaskTest.java:184) ~[test-classes/:?]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_292]
[...]
{code}

[~akalashnikov] is this expected?

The affected build is https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8",,akalash,akalashnikov,leonard,mapohl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30844,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Apr 21 11:50:25 UTC 2023,,,,,,,,,,"0|z1fi9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 14:49;akalashnikov;Oh, It is not the correct test. It actually expects that this method would be called. I will change that.

Thanks for the report.;;;","14/Apr/23 13:57;akalash;merged to master: a6f0165c;;;","20/Apr/23 13:35;mapohl;The issue's version is misleading. It appears that you've merged the change into {{master}} (which means that it was fixed in 1.18.0). I'm gonna update the fixVersion accordingly. May you also provide a 1.17 backport? ;;;","20/Apr/23 14:06;akalash;Yeh, sorry, I missed up with versions. I will backport it to 1.17 shortly.;;;","21/Apr/23 11:50;akalash;merged to release-1.17: aa47c3f862414511e92637d9816f6908c86b4cf6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource fails,FLINK-30846,13522183,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,JunRuiLi,mapohl,mapohl,31/Jan/23 07:57,06/Feb/23 07:17,13/Jul/23 08:29,04/Feb/23 02:13,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Coordination,,,,0,pull-request-available,test-stability,,"{{SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource}} is timing out
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8599
{code}
Jan 31 02:02:28 ""ForkJoinPool-1-worker-25"" #27 daemon prio=5 os_prio=0 tid=0x00007fcf74f2b800 nid=0x5476 waiting on condition [0x00007fce2b078000]
Jan 31 02:02:28    java.lang.Thread.State: WAITING (parking)
Jan 31 02:02:28 	at sun.misc.Unsafe.park(Native Method)
Jan 31 02:02:28 	- parking to wait for  <0x00000000a22933e0> (a java.util.concurrent.CompletableFuture$Signaller)
Jan 31 02:02:28 	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
Jan 31 02:02:28 	at java.util.concurrent.CompletableFuture$Signaller.block(CompletableFuture.java:1707)
Jan 31 02:02:28 	at java.util.concurrent.ForkJoinPool.managedBlock(ForkJoinPool.java:3313)
Jan 31 02:02:28 	at java.util.concurrent.CompletableFuture.waitingGet(CompletableFuture.java:1742)
Jan 31 02:02:28 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1908)
Jan 31 02:02:28 	at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.executeJob(SpeculativeSchedulerITCase.java:216)
Jan 31 02:02:28 	at org.apache.flink.test.scheduling.SpeculativeSchedulerITCase.testSpeculativeExecutionOfInputFormatSource(SpeculativeSchedulerITCase.java:162)
{code}",,JunRuiLi,leonard,mapohl,renqs,wanglijie,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30683,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 07:17:22 UTC 2023,,,,,,,,,,"0|z1fi1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"01/Feb/23 06:07;wanglijie;If we don't set {{jobmangaer.scheduler}} manually and set the system properties {{flink.tests.enable-adaptive-scheduler}}, it will use the adaptive scheduler for streaming jobs and use the default scheduler for batch jobs, this ticket occured in this scenario.;;;","01/Feb/23 06:10;wanglijie; In FLINK-30683, we removed the logic of manually configuring the adaptive batch scheduler, which led to this test running with default scheduler (we expected this test must run with adaptive batch scheduler).;;;","01/Feb/23 06:15;wanglijie;[~JunRuiLi] Would you take a look?;;;","01/Feb/23 06:20;JunRuiLi;[~wanglijie] This issue will not be a block to production availability, it will only appear in the test. I will fix it immediately, thanks!;;;","01/Feb/23 09:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=10711;;;","01/Feb/23 11:15;mapohl;I'm lowering the priority of this issue to Critical based on [Junrui's comment|https://issues.apache.org/jira/browse/FLINK-30846?focusedCommentId=17682866&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17682866] that it's only a test-related issue and doesn't indicate a bug in production.;;;","02/Feb/23 08:57;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45586&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8899;;;","03/Feb/23 06:09;renqs;[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45651&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8893] ;;;","04/Feb/23 02:13;zhuzh;Fixed via b1e70aebd3e248d68cf41a43db385ec9c9b6235a;;;","06/Feb/23 07:17;mapohl;The following failed build didn't include the above mentioned fix, yet: 
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45707&view=logs&j=8fd9202e-fd17-5b26-353c-ac1ff76c8f28&t=ea7cf968-e585-52cb-e0fc-f48de023a7ca&l=8893;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Params in jarURI end up in file name,FLINK-30845,13522182,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,fabiowanner,fabiowanner,fabiowanner,31/Jan/23 07:55,07/Feb/23 11:29,13/Jul/23 08:29,07/Feb/23 11:29,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.4.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,starter,,"*Context*

Jar files for jobs are submitted to the operator by supplying a URI to the .jar file. This URI can be a file system path or a URI to some HTTP resource. If a HTTP URI is given, the file will be fetched using the {{{}HttpArtifactFetcher{}}}. 

There are cases where the supplied URI will contain additional params. For example if pre-signed S3 URLs are used.

Example:
{code:java}
https://some-domain.example.com/some.jar?some=params{code}
*Problem*

When the HttpArtifactFetcher determines the name of the .jar file it does also use the params as part of the file name. In the example from above the resulting file name would be:  {{some.jar?some=params}}

Submitting this job to Flink will result in an error as it will be checked for the file name to end with {{.jar}}

*Possible Solution*
In the {{HttpArtifactFetcher}} it would be enough to replace:
{code:java}
String fileName = FilenameUtils.getName(url.getFile());{code}
with
{code:java}
String fileName = FilenameUtils.getName(url.getPath());{code}",,fabiowanner,gyfora,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 11:29:58 UTC 2023,,,,,,,,,,"0|z1fi1k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 11:29;gyfora;merged to main f0578a57f6395634b9ad301c7ab7176a45f9c438;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TaskTest.testInterruptibleSharedLockInInvokeAndCancel causes a JVM shutdown with exit code 239,FLINK-30844,13522181,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,akalashnikov,mapohl,mapohl,31/Jan/23 07:50,26/May/23 08:17,13/Jul/23 08:29,06/Apr/23 15:02,1.16.2,1.17.0,,,,,1.16.3,1.17.1,,,,,,,Runtime / Coordination,Runtime / Task,,,0,pull-request-available,test-stability,,"We're experiencing a fatal crash in {{TaskTest}}:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=8334
{code}
[...]
Jan 31 01:03:12 [ERROR] Process Exit Code: 239
Jan 31 01:03:12 [ERROR] Crashed tests:
Jan 31 01:03:12 [ERROR] org.apache.flink.runtime.taskmanager.TaskTest
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.fork(ForkStarter.java:748)
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter.access$700(ForkStarter.java:121)
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:393)
Jan 31 01:03:12 [ERROR] at org.apache.maven.plugin.surefire.booterclient.ForkStarter$1.call(ForkStarter.java:370)
Jan 31 01:03:12 [ERROR] at java.util.concurrent.FutureTask.run(FutureTask.java:266)
Jan 31 01:03:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
Jan 31 01:03:12 [ERROR] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
Jan 31 01:03:12 [ERROR] at java.lang.Thread.run(Thread.java:748)
Jan 31 01:03:12 [ERROR] -> [Help 1]
Jan 31 01:03:12 [ERROR] 
Jan 31 01:03:12 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Jan 31 01:03:12 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Jan 31 01:03:12 [ERROR] 
Jan 31 01:03:12 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Jan 31 01:03:12 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
Jan 31 01:03:12 [ERROR] 
Jan 31 01:03:12 [ERROR] After correcting the problems, you can resume the build with the command
Jan 31 01:03:12 [ERROR]   mvn <goals> -rf :flink-runtime
{code}",,akalash,akalashnikov,fanrui,mapohl,Sergey Nuyanzin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30852,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri May 26 08:17:15 UTC 2023,,,,,,,,,,"0|z1fi1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"31/Jan/23 12:02;mapohl;An AssertionError is reported that should be independent from this build failure. I created FLINK-30852 to cover the issue.;;;","31/Jan/23 12:14;mapohl;{{TaskTest.testInterruptibleSharedLockInInvokeAndCancel}} caused the failure 
{code}
00:59:02,291 [Cancellation Watchdog for Test Task (1/1)#0 (003bbd51a0b61b0ff2925c31e749f53e_00000000000000000000000000000000_0_0).] ERROR org.apache.flink.util.FatalExitExceptionHandler              [] - FATAL: Thread 'Cancellation Watchdog for Test Task (1/1)#0 (003bbd51a0b61b0ff2925c31e749f53e_00000000000000000000000000000000_0_0).' produced an uncaught exception. Stopping the process...
org.apache.flink.util.FlinkRuntimeException: Error in Task Cancellation Watch Dog
        at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1801) ~[classes/:?]
        at java.lang.Thread.run(Thread.java:748) [?:1.8.0_292]
Caused by: java.lang.RuntimeException: Unexpected FatalError notification
        at org.apache.flink.runtime.taskmanager.TaskTest$ProhibitFatalErrorTaskManagerActions.notifyFatalError(TaskTest.java:1278) ~[test-classes/:?]
        at org.apache.flink.runtime.taskmanager.Task$TaskCancelerWatchDog.run(Task.java:1798) ~[classes/:?]
        ... 1 more
{code}

The {{TaskCancelerWatchDog}} causes the System.exit when the executor thread is still alive (see [Task:1781|https://github.com/apache/flink/blob/3b6d08e57f644cddcdac1fb5a110d44172652c3a/flink-runtime/src/main/java/org/apache/flink/runtime/taskmanager/Task.java#L1781]).

 [~piotr.nowicki] [~akalashnikov] May one of you have a look at this?;;;","31/Jan/23 15:02;piotr.nowicki;[~mapohl] I believe I'm not the person you wanted to notify here :);;;","31/Jan/23 15:12;mapohl;ah, typo! 🤦‍♂️  sorry for the spam, Piotr Nowicki. I meant [~pnowojski];;;","31/Jan/23 18:11;akalashnikov;I don't see a problem here.  I just see that the thread is finishing longer than 50ms but I see it isn't stuck and make progress which is good. Normally, it should take about 1ms but 50ms  is also not something extraordinal on the overloaded machine. The only thing is a synchronization inside the finishing loop which can delay the actual finish but I have no evidence that it is a reason. 
I will try to check more ideas but if nothing works I will just increase the waiting interval for this test.(I actually think that 50ms is low anyway);;;","01/Feb/23 11:13;mapohl;thanks for sharing your thoughts. It sounds reasonable. I'm gonna lower the priority for that one to Major.;;;","20/Mar/23 07:37;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47318&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8351;;;","31/Mar/23 06:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47748&view=logs&j=a549b384-c55a-52c0-c451-00e0477ab6db&t=eef5922c-08d9-5ba3-7299-8393476594e7&l=8807;;;","31/Mar/23 08:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47750&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=8385;;;","06/Apr/23 15:02;akalash;In conclusion, I haven't found any deadlocks or other suspicious things. I was able to reproduce it locally it seems it indeed just works pretty slowly on an overloaded machine. So I just increased the timeout. We will see how it will be.

merged to master: 6e95bfaf;;;","26/May/23 08:17;Sergey Nuyanzin;Merged to 1.16 as [141b47a80092134f95dfdf6b3d0e7051d4fec6bb|https://github.com/apache/flink/commit/141b47a80092134f95dfdf6b3d0e7051d4fec6bb];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect calc merge generate wrong plan,FLINK-30841,13522174,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,31/Jan/23 07:00,03/Feb/23 13:38,13/Jul/23 08:29,03/Feb/23 13:38,1.16.1,,,,,,1.17.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"currently we have a `FlinkCalcMergeRuleTest`, take one test as example:
{code:java}
  @Test
  def testCalcMergeWithNonDeterministicExpr1(): Unit = {
    val sqlQuery = ""SELECT a, a1 FROM (SELECT a, random_udf(a) AS a1 FROM MyTable) t WHERE a1 > 10""
    util.verifyRelPlan(sqlQuery)
  }
{code}
the current final optimized plan will be wrong:
{code:java}
Calc(select=[a, random_udf(b) AS a1], where=[(random_udf(b) > 10)])
+- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
{code}
the merged calc contains two `random_udf` call, users may encounter the result satisfied by where predicate (>10) but the selected column <= 10, that's counter-intuitive for users

the expected plan is:
{code:java}
Calc(select=[a, a1], where=[(a1 > 10)])
+- Calc(select=[a, random_udf(b) AS a1])
   +- LegacyTableSourceScan(table=[[default_catalog, default_database, MyTable, source: [TestTableSource(a, b, c)]]], fields=[a, b, c])
{code}",,libenchao,lincoln.86xy,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 03 13:38:39 UTC 2023,,,,,,,,,,"0|z1fhzs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Feb/23 13:38;lincoln.86xy;fixed in master: c3a376f5380ee85c9d34a8e72806e6ce6893be6a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SortAggITCase.testLeadLag failed,FLINK-30828,13522075,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,zhuzh,mapohl,mapohl,30/Jan/23 13:30,31/Jan/23 10:27,13/Jul/23 08:29,31/Jan/23 02:57,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45389&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12560

{code}
Jan 30 11:03:32 [ERROR] Tests run: 72, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 37.42 s <<< FAILURE! - in org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase
Jan 30 11:03:32 [ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.SortAggITCase.testLeadLag  Time elapsed: 0.547 s  <<< FAILURE!
Jan 30 11:03:32 java.lang.AssertionError: 
Jan 30 11:03:32 
Jan 30 11:03:32 Results do not match for query:
Jan 30 11:03:32   
Jan 30 11:03:32 SELECT
Jan 30 11:03:32   a,
Jan 30 11:03:32   b, LEAD(b, 1) over (order by a)  AS bLead, LAG(b, 1) over (order by a)  AS bLag,
Jan 30 11:03:32   c, LEAD(c, 1) over (order by a)  AS cLead, LAG(c, 1) over (order by a)  AS cLag,
Jan 30 11:03:32   d, LEAD(d, 1) over (order by a)  AS dLead, LAG(d, 1) over (order by a)  AS dLag,
Jan 30 11:03:32   e, LEAD(e, 1) over (order by a)  AS eLead, LAG(e, 1) over (order by a)  AS eLag,
Jan 30 11:03:32   f, LEAD(f, 1) over (order by a)  AS fLead, LAG(f, 1) over (order by a)  AS fLag,
Jan 30 11:03:32   g, LEAD(g, 1) over (order by a)  AS gLead, LAG(g, 1) over (order by a)  AS gLag,
Jan 30 11:03:32   h, LEAD(h, 1) over (order by a)  AS hLead, LAG(h, 1) over (order by a)  AS hLag,
Jan 30 11:03:32   i, LEAD(i, 1) over (order by a)  AS iLead, LAG(i, 1) over (order by a)  AS iLag,
Jan 30 11:03:32   j, LEAD(j, 1) over (order by a)  AS jLead, LAG(j, 1) over (order by a)  AS jLag,
Jan 30 11:03:32   k, LEAD(k, 1) over (order by a)  AS kLead, LAG(k, 1) over (order by a)  AS kLag,
Jan 30 11:03:32   l, LEAD(l, 1) over (order by a)  AS lLead, LAG(l, 1) over (order by a)  AS lLag,
Jan 30 11:03:32   m, LEAD(m, 1) over (order by a)  AS mLead, LAG(m, 1) over (order by a)  AS mLag,
Jan 30 11:03:32   n, LEAD(n, 1) over (order by a)  AS nLead, LAG(n, 1) over (order by a)  AS nLag
Jan 30 11:03:32 
Jan 30 11:03:32 FROM UnnamedTable$230
Jan 30 11:03:32 order by a
Jan 30 11:03:32 
Jan 30 11:03:32 
Jan 30 11:03:32 Results
Jan 30 11:03:32  == Correct Result - 3 ==                                                                                                                                                                                                                                                                                                  == Actual Result - 3 ==
Jan 30 11:03:32  +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]   +I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]
Jan 30 11:03:32  +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]   +I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]
Jan 30 11:03:32 !+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99]                                     +I[Alice, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null]
Jan 30 11:03:32         
Jan 30 11:03:32 Plan:
Jan 30 11:03:32   == Abstract Syntax Tree ==
Jan 30 11:03:32 LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
Jan 30 11:03:32 +- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])
Jan 30 11:03:32    +- LogicalUnion(all=[true])
[...]
{code}",,mapohl,zhuzh,,,,,,,,,,,,,,,,,,,,,,,FLINK-30826,,,,,,,,,,,FLINK-30834,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 10:27:23 UTC 2023,,,,,,,,,,"0|z1fhe8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 13:57;mapohl;[~shengkai] [~zhengyunhong97] may you have a look at it?;;;","30/Jan/23 16:29;zhuzh;I can re-produce this problem locally.
This problem started to appear after FLINK-30683 which changed the SQL test to test against {{BatchShuffleMode.ALL_EXCHANGES_BLOCKING}} 
 instead of {{BatchShuffleMode.ALL_EXCHANGES_PIPELINED}}.
By changing it back to test against {{BatchShuffleMode.ALL_EXCHANGES_PIPELINED}}, the problem is gone. So seems the case is unstable with blocking shuffle.
I will change the case to run with {{BatchShuffleMode.ALL_EXCHANGES_PIPELINED}} first to unblock CI.

cc [~godfreyhe] [~lzljs3620320]
;;;","31/Jan/23 02:57;zhuzh;Fixed via 9a5c2dbb85f1143c2ae24f92cd7033d8e35c69d9;;;","31/Jan/23 04:06;zhuzh;I opened a separate ticket FLINK-30834 to track the issue that SortAggITCase.testLeadLag becomes unstable in ALL_EXCHANGES_BLOCKING mode.;;;","31/Jan/23 10:27;mapohl;The following build is based on [dcbb2068|https://github.com/apache/flink/commit/dcbb2068] didn't contain the aforementioned fix [9a5c2dbb|https://github.com/apache/flink/commit/9a5c2dbb85f1143c2ae24f92cd7033d8e35c69d9], yet:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45445&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12565;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HashAggITCase.testLeadLag is failing,FLINK-30826,13522060,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Sergey Nuyanzin,Sergey Nuyanzin,30/Jan/23 12:25,31/Jan/23 13:53,13/Jul/23 08:29,31/Jan/23 11:50,1.17.0,,,,,,,,,,,,,,Table SQL / Planner,,,,0,test-stability,,,"With jdk8 it is green.
At the same time it is constantly failing for jdk11 like
{noformat}
[ERROR] org.apache.flink.table.planner.runtime.batch.sql.agg.HashAggITCase.testLeadLag  Time elapsed: 0.336 s  <<< FAILURE!
java.lang.AssertionError: 

Results do not match for query:
  
SELECT
  a,
  b, LEAD(b, 1) over (order by a)  AS bLead, LAG(b, 1) over (order by a)  AS bLag,
  c, LEAD(c, 1) over (order by a)  AS cLead, LAG(c, 1) over (order by a)  AS cLag,
  d, LEAD(d, 1) over (order by a)  AS dLead, LAG(d, 1) over (order by a)  AS dLag,
  e, LEAD(e, 1) over (order by a)  AS eLead, LAG(e, 1) over (order by a)  AS eLag,
  f, LEAD(f, 1) over (order by a)  AS fLead, LAG(f, 1) over (order by a)  AS fLag,
  g, LEAD(g, 1) over (order by a)  AS gLead, LAG(g, 1) over (order by a)  AS gLag,
  h, LEAD(h, 1) over (order by a)  AS hLead, LAG(h, 1) over (order by a)  AS hLag,
  i, LEAD(i, 1) over (order by a)  AS iLead, LAG(i, 1) over (order by a)  AS iLag,
  j, LEAD(j, 1) over (order by a)  AS jLead, LAG(j, 1) over (order by a)  AS jLag,
  k, LEAD(k, 1) over (order by a)  AS kLead, LAG(k, 1) over (order by a)  AS kLag,
  l, LEAD(l, 1) over (order by a)  AS lLead, LAG(l, 1) over (order by a)  AS lLag,
  m, LEAD(m, 1) over (order by a)  AS mLead, LAG(m, 1) over (order by a)  AS mLag,
  n, LEAD(n, 1) over (order by a)  AS nLead, LAG(n, 1) over (order by a)  AS nLag

FROM UnnamedTable$18
order by a


Results
 == Correct Result - 3 ==                                                                                                                                                                                                                                                                                                  == Actual Result - 3 ==
!+I[Alice, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99, null]   +I[Alice, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null, null]
!+I[Alice, 1, null, 1, 1, null, 1, 2, null, 2, 9223, null, 9223, -2.3, null, -2.3, 9.9, null, 9.9, true, null, true, varchar, null, varchar, char                , null, char                , 2021-08-03, null, 2021-08-03, 20:08:17, null, 20:08:17, 2021-08-03T20:08:29, null, 2021-08-03T20:08:29, 9.99, null, 9.99]   +I[Alice, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99, null, null]
!+I[Alice, null, null, 1, null, null, 1, null, null, 2, null, null, 9223, null, null, -2.3, null, null, 9.9, null, null, true, null, null, varchar, null, null, char                , null, null, 2021-08-03, null, null, 20:08:17, null, null, 2021-08-03T20:08:29, null, null, 9.99]                                     +I[Alice, null, 1, 1, null, 1, 1, null, 2, 2, null, 9223, 9223, null, -2.3, -2.3, null, 9.9, 9.9, null, true, true, null, varchar, varchar, null, char                , char                , null, 2021-08-03, 2021-08-03, null, 20:08:17, 20:08:17, null, 2021-08-03T20:08:29, 2021-08-03T20:08:29, null, 9.99, 9.99]
        
Plan:
  == Abstract Syntax Tree ==
LogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
+- LogicalProject(inputs=[0..1], exprs=[[LEAD($1, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($1, 1) OVER (ORDER BY $0 NULLS FIRST), $2, LEAD($2, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($2, 1) OVER (ORDER BY $0 NULLS FIRST), $3, LEAD($3, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($3, 1) OVER (ORDER BY $0 NULLS FIRST), $4, LEAD($4, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($4, 1) OVER (ORDER BY $0 NULLS FIRST), $5, LEAD($5, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($5, 1) OVER (ORDER BY $0 NULLS FIRST), $6, LEAD($6, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($6, 1) OVER (ORDER BY $0 NULLS FIRST), $7, LEAD($7, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($7, 1) OVER (ORDER BY $0 NULLS FIRST), $8, LEAD($8, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($8, 1) OVER (ORDER BY $0 NULLS FIRST), $9, LEAD($9, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($9, 1) OVER (ORDER BY $0 NULLS FIRST), $10, LEAD($10, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($10, 1) OVER (ORDER BY $0 NULLS FIRST), $11, LEAD($11, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($11, 1) OVER (ORDER BY $0 NULLS FIRST), $12, LEAD($12, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($12, 1) OVER (ORDER BY $0 NULLS FIRST), $13, LEAD($13, 1) OVER (ORDER BY $0 NULLS FIRST), LAG($13, 1) OVER (ORDER BY $0 NULLS FIRST)]])
   +- LogicalUnion(all=[true])
      :- LogicalProject(exprs=[[CAST(_UTF-16LE'Alice':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", CAST(1:TINYINT):TINYINT, CAST(1:SMALLINT):SMALLINT, CAST(2):INTEGER, CAST(9223:BIGINT):BIGINT, CAST(-2.3E0:FLOAT):FLOAT, CAST(9.9E0:DOUBLE):DOUBLE, CAST(true):BOOLEAN, CAST(_UTF-16LE'varchar':VARCHAR(20) CHARACTER SET ""UTF-16LE""):VARCHAR(20) CHARACTER SET ""UTF-16LE"", CAST(_UTF-16LE'char                '):CHAR(20) CHARACTER SET ""UTF-16LE"", CAST(2021-08-03):DATE, CAST(20:08:17):TIME(0), CAST(2021-08-03 20:08:29:TIMESTAMP(6)):TIMESTAMP(6), CAST(9.99E0:DECIMAL(3, 2)):DECIMAL(3, 2)]])
      :  +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])
      :- LogicalProject(exprs=[[CAST(_UTF-16LE'Alice':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", null:TINYINT, null:SMALLINT, null:INTEGER, null:BIGINT, null:FLOAT, null:DOUBLE, null:BOOLEAN, null:VARCHAR(20) CHARACTER SET ""UTF-16LE"", null:CHAR(20) CHARACTER SET ""UTF-16LE"", null:DATE, null:TIME(0), null:TIMESTAMP(6), null:DECIMAL(3, 2)]])
      :  +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])
      +- LogicalProject(exprs=[[CAST(_UTF-16LE'Alice':VARCHAR(2147483647) CHARACTER SET ""UTF-16LE""):VARCHAR(2147483647) CHARACTER SET ""UTF-16LE"", CAST(1:TINYINT):TINYINT, CAST(1:SMALLINT):SMALLINT, CAST(2):INTEGER, CAST(9223:BIGINT):BIGINT, CAST(-2.3E0:FLOAT):FLOAT, CAST(9.9E0:DOUBLE):DOUBLE, CAST(true):BOOLEAN, CAST(_UTF-16LE'varchar':VARCHAR(20) CHARACTER SET ""UTF-16LE""):VARCHAR(20) CHARACTER SET ""UTF-16LE"", CAST(_UTF-16LE'char                '):CHAR(20) CHARACTER SET ""UTF-16LE"", CAST(2021-08-03):DATE, CAST(20:08:17):TIME(0), CAST(2021-08-03 20:08:29:TIMESTAMP(6)):TIMESTAMP(6), CAST(9.99E0:DECIMAL(3, 2)):DECIMAL(3, 2)]])
         +- LogicalValues(type=[RecordType(INTEGER ZERO)], tuples=[[{ 0 }]])

== Optimized Logical Plan ==
Sort(orderBy=[a ASC])
+- Calc(select=[a, b, w0$o0 AS bLead, w0$o1 AS bLag, c, w0$o2 AS cLead, w0$o3 AS cLag, d, w0$o4 AS dLead, w0$o5 AS dLag, e, w0$o6 AS eLead, w0$o7 AS eLag, f, w0$o8 AS fLead, w0$o9 AS fLag, g, w0$o10 AS gLead, w0$o11 AS gLag, h, w0$o12 AS hLead, w0$o13 AS hLag, i, w0$o14 AS iLead, w0$o15 AS iLag, j, w0$o16 AS jLead, w0$o17 AS jLag, k, w0$o18 AS kLead, w0$o19 AS kLag, l, w0$o20 AS lLead, w0$o21 AS lLag, m, w0$o22 AS mLead, w0$o23 AS mLag, n, w0$o24 AS nLead, w0$o25 AS nLag])
   +- OverAggregate(orderBy=[a ASC], window#0=[LEAD(b, 1) AS w0$o0, LAG(b, 1) AS w0$o1, LEAD(c, 1) AS w0$o2, LAG(c, 1) AS w0$o3, LEAD(d, 1) AS w0$o4, LAG(d, 1) AS w0$o5, LEAD(e, 1) AS w0$o6, LAG(e, 1) AS w0$o7, LEAD(f, 1) AS w0$o8, LAG(f, 1) AS w0$o9, LEAD(g, 1) AS w0$o10, LAG(g, 1) AS w0$o11, LEAD(h, 1) AS w0$o12, LAG(h, 1) AS w0$o13, LEAD(i, 1) AS w0$o14, LAG(i, 1) AS w0$o15, LEAD(j, 1) AS w0$o16, LAG(j, 1) AS w0$o17, LEAD(k, 1) AS w0$o18, LAG(k, 1) AS w0$o19, LEAD(l, 1) AS w0$o20, LAG(l, 1) AS w0$o21, LEAD(m, 1) AS w0$o22, LAG(m, 1) AS w0$o23, LEAD(n, 1) AS w0$o24, LAG(n, 1) AS w0$o25 RANG BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW], select=[a, b, c, d, e, f, g, h, i, j, k, l, m, n, w0$o0, w0$o1, w0$o2, w0$o3, w0$o4, w0$o5, w0$o6, w0$o7, w0$o8, w0$o9, w0$o10, w0$o11, w0$o12, w0$o13, w0$o14, w0$o15, w0$o16, w0$o17, w0$o18, w0$o19, w0$o20, w0$o21, w0$o22, w0$o23, w0$o24, w0$o25])
      +- Sort(orderBy=[a ASC])
         +- Exchange(distribution=[single])
            +- Union(all=[true], union=[a, b, c, d, e, f, g, h, i, j, k, l, m, n])
               :- Calc(select=[CAST('Alice' AS VARCHAR(2147483647)) AS a, CAST(1 AS TINYINT) AS b, CAST(1 AS SMALLINT) AS c, CAST(2 AS INTEGER) AS d, CAST(9223 AS BIGINT) AS e, CAST(-2.3E0 AS FLOAT) AS f, CAST(9.9E0 AS DOUBLE) AS g, CAST(true AS BOOLEAN) AS h, CAST('varchar' AS VARCHAR(20)) AS i, CAST('char                ' AS CHAR(20)) AS j, CAST(2021-08-03 AS DATE) AS k, CAST(20:08:17 AS TIME(0)) AS l, CAST(2021-08-03 20:08:29 AS TIMESTAMP(6)) AS m, CAST(9.99E0 AS DECIMAL(3, 2)) AS n])
               :  +- Values(tuples=[[{ 0 }]], values=[ZERO])
               :- Calc(select=[CAST('Alice' AS VARCHAR(2147483647)) AS a, null:TINYINT AS b, null:SMALLINT AS c, null:INTEGER AS d, null:BIGINT AS e, null:FLOAT AS f, null:DOUBLE AS g, null:BOOLEAN AS h, null:VARCHAR(20) AS i, null:CHAR(20) AS j, null:DATE AS k, null:TIME(0) AS l, null:TIMESTAMP(6) AS m, null:DECIMAL(3, 2) AS n])
               :  +- Values(tuples=[[{ 0 }]], values=[ZERO])
               +- Calc(select=[CAST('Alice' AS VARCHAR(2147483647)) AS a, CAST(1 AS TINYINT) AS b, CAST(1 AS SMALLINT) AS c, CAST(2 AS INTEGER) AS d, CAST(9223 AS BIGINT) AS e, CAST(-2.3E0 AS FLOAT) AS f, CAST(9.9E0 AS DOUBLE) AS g, CAST(true AS BOOLEAN) AS h, CAST('varchar' AS VARCHAR(20)) AS i, CAST('char                ' AS CHAR(20)) AS j, CAST(2021-08-03 AS DATE) AS k, CAST(20:08:17 AS TIME(0)) AS l, CAST(2021-08-03 20:08:29 AS TIMESTAMP(6)) AS m, CAST(9.99E0 AS DECIMAL(3, 2)) AS n])
                  +- Values(tuples=[[{ 0 }]], values=[ZERO])

       
	at org.junit.Assert.fail(Assert.java:89)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1(BatchTestBase.scala:152)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.$anonfun$check$1$adapted(BatchTestBase.scala:145)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.check(BatchTestBase.scala:145)
	at org.apache.flink.table.planner.runtime.utils.BatchTestBase.checkResult(BatchTestBase.scala:106)
	at org.apache.flink.table.planner.runtime.batch.sql.agg.AggregateITCaseBase.testLeadLag(AggregateITCaseBase.scala:958)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.apache.flink.util.TestNameProvider$1.evaluate(TestNameProvider.java:45)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:54)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:115)
	at org.junit.vintage.engine.execution.RunnerExecutor.execute(RunnerExecutor.java:42)
	at org.junit.vintage.engine.VintageTestEngine.executeAllChildren(VintageTestEngine.java:80)
	at org.junit.vintage.engine.VintageTestEngine.execute(VintageTestEngine.java:72)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:147)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:127)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:90)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:55)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:102)
	at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:54)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)
	at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)
	at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)
	at org.junit.platform.launcher.core.SessionPerRequestLauncher.execute(SessionPerRequestLauncher.java:53)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.lambda$execute$1(JUnitPlatformProvider.java:199)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.execute(JUnitPlatformProvider.java:193)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invokeAllTests(JUnitPlatformProvider.java:154)
	at org.apache.maven.surefire.junitplatform.JUnitPlatformProvider.invoke(JUnitPlatformProvider.java:120)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)

{noformat}",,mapohl,Sergey Nuyanzin,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30828,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 13:53:25 UTC 2023,,,,,,,,,,"0|z1fhaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 16:33;zhuzh;I think it has the same cause of FLINK-30828. The test is a bit unstable though.;;;","31/Jan/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45440&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12842;;;","31/Jan/23 08:01;zhuzh;This problem should have been fixed in FLINK-30828.

The failed test above did not include the fix.;;;","31/Jan/23 08:12;zhuzh;[~Sergey Nuyanzin] is the problem still happening with JDK11 when using the latest Flink master?;;;","31/Jan/23 11:50;Sergey Nuyanzin;I tested it against latest master with jdk11 and it seems the issue is fixed

Thank you.;;;","31/Jan/23 13:53;zhuzh;Thanks for reporting this issue and verifying the fix! [~Sergey Nuyanzin];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ingress creation failed on K8s with version < 1.19,FLINK-30825,13522052,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,,idealities,idealities,30/Jan/23 11:52,09/Feb/23 08:56,13/Jul/23 08:29,09/Feb/23 08:56,kubernetes-operator-1.2.0,kubernetes-operator-1.3.1,,,,,kubernetes-operator-1.4.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"Currently flink-kubernetes-operator only use io.fabric8.kubernetes.api.model.networking.v1.Ingress, but this is only available in and after K8s 1.19 ([https://kubernetes.io/docs/reference/using-api/deprecation-guide/] use the *networking.k8s.io/v1* API version, available since v1.19.)

With K8s before 1.19, we can use Ingress in *extensions/v1beta1* or {*}networking.k8s.io/v1beta1{*}, to avoid Ingress creation failure.","K8s 1.18

 ",gyfora,idealities,,,,,,,,,,,1209600,1209600,,0%,1209600,1209600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Feb 09 08:56:32 UTC 2023,,,,,,,,,,"0|z1fh94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Feb/23 08:56;gyfora;merged to main 33771a5b6748e951448f80f1627d502c4e77a67b;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BatchTestBase/BatchAbstractTestBase are using JUnit4 while some child tests are using JUnit5,FLINK-30815,13521897,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tanyuxin,zhuzh,zhuzh,29/Jan/23 04:24,11/May/23 08:12,13/Jul/23 08:29,10/May/23 03:44,1.16.0,,,,,,1.18.0,,,,,,,,Tests,,,,0,pull-request-available,,,"BatchTestBase/BatchAbstractTestBase are using Junit4, while some child tests (e.g. DynamicFilteringITCase) are using JUnit5. This may break some assumption and hide some problems.
For example, the child test will create a MiniCluster by itself, instead of using the MiniCluster(TM=1, slots=3)  created in BatchAbstractTestBase. The created MiniCluster may  have more slots and hide resource deadlock issues.",,godfrey,tanyuxin,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-32048,,,,FLINK-32055,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed May 10 03:44:04 UTC 2023,,,,,,,,,,"0|z1fgaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Apr/23 06:37;tanyuxin;[~zhuzh] Thanks for starting the issue, I will take a look at it.;;;","10/May/23 03:44;godfrey;Fixed in master:

ed9ee279e50781b7bd2d85f1486721c02fc7e32b

7a423666d0f8452382ad5fe2635de5ad1475dd46;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The parallelism&maxParallelism of sort after a global partitioning is not forced to be 1,FLINK-30814,13521895,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,godfrey,zhuzh,zhuzh,29/Jan/23 03:16,30/Jan/23 01:55,13/Jul/23 08:29,30/Jan/23 01:55,1.16.0,,,,,,1.17.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"The parallelism&maxParallelism of sort after a global partitioning is not forced to be 1. The may lead to the parallelism to be changed by adaptive batch scheduler, which is unexpected.",,godfrey,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 01:55:35 UTC 2023,,,,,,,,,,"0|z1fgag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Jan/23 07:54;godfrey;[~zhuzh] thanks for reporting this issue.

Currently, the parallelism of global sort is set as 1, but the max parallelism is not set because ExecNodeBase#inputsContainSingleton does not work for global sort.  This works fine for the default scheduler, but may occur wrong result if  adaptive batch scheduler changes the global sort parallelism.

 

I will fix it.

 ;;;","30/Jan/23 01:55;godfrey;Fixed in 1.17.0: 916ff76b61a0bfa8283d52be53cd33e317d0d550;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix sql gateway can not stop job correctly,FLINK-30811,13521842,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,qingyue,fsk119,fsk119,28/Jan/23 08:01,07/Feb/23 02:32,13/Jul/23 08:29,07/Feb/23 02:32,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,,,,,fsk119,Paul Lin,qingyue,zoucao,,,,,,,,,,,,,,,,,,FLINK-30538,,,,,,,,,,,,,,,,,FLINK-27344,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 02:32:52 UTC 2023,,,,,,,,,,"0|z1ffyw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/23 13:05;qingyue;I'd like to fix this issue, cc [~fsk119] ;;;","31/Jan/23 08:21;qingyue;*Phenomenon*
STOP JOB <job_identifier> occasionally not working as expected, and the following exception stacktrace can be found in the sql-client.log
{code:java}
2023-01-30 23:40:16,456 WARN org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel [] - Force-closing a channel whose registration task was not accepted by an event loop: [id: 0xe1c04c6e]
java.util.concurrent.RejectedExecutionException: event executor terminated
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:934) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:351) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:344) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:836) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute0(SingleThreadEventExecutor.java:827) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:817) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AbstractUnsafe.register(AbstractChannel.java:483) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.SingleThreadEventLoop.register(SingleThreadEventLoop.java:89) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.SingleThreadEventLoop.register(SingleThreadEventLoop.java:83) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.MultithreadEventLoopGroup.register(MultithreadEventLoopGroup.java:86) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.bootstrap.AbstractBootstrap.initAndRegister(AbstractBootstrap.java:323) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.doResolveAndConnect(Bootstrap.java:155) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.connect(Bootstrap.java:139) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.bootstrap.Bootstrap.connect(Bootstrap.java:123) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.submitRequest(RestClient.java:471) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:394) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:308) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.client.program.rest.RestClusterClient.lambda$null$38(RestClusterClient.java:962) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:580) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) [?:1.8.0_202]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_202]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_202]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_202]
2023-01-30 23:40:16,464 ERROR org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.rejectedExecution [] - Failed to submit a listener notification task. Event loop shut down?
java.util.concurrent.RejectedExecutionException: event executor terminated
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.reject(SingleThreadEventExecutor.java:934) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.offerTask(SingleThreadEventExecutor.java:351) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.addTask(SingleThreadEventExecutor.java:344) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:836) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute0(SingleThreadEventExecutor.java:827) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.SingleThreadEventExecutor.execute(SingleThreadEventExecutor.java:817) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.safeExecute(DefaultPromise.java:841) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:499) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.util.concurrent.DefaultPromise.addListener(DefaultPromise.java:184) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:95) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.shaded.netty4.io.netty.channel.DefaultChannelPromise.addListener(DefaultChannelPromise.java:30) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.submitRequest(RestClient.java:475) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:394) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.runtime.rest.RestClient.sendRequest(RestClient.java:308) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at org.apache.flink.client.program.rest.RestClusterClient.lambda$null$38(RestClusterClient.java:962) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
at java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:952) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:926) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:474) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture.postFire(CompletableFuture.java:561) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:580) [?:1.8.0_202]
at java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:442) [?:1.8.0_202]
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_202]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_202]
at java.lang.Thread.run(Thread.java:748) [?:1.8.0_202] {code}
*Root cause*
OperationExecutor#runClusterAction manages RestClusterClient lifecycle via try-with-resource; thus, the client will be closed immediately after leaving the code block, which shuts down the netty NioEventLoopGroup.

 

*Proposed fix*

Block the job cancel call until timeout.;;;","01/Feb/23 06:52;Paul Lin;[~qingyue] [~fsk119] Sorry for introducing this bug, I should have fixed this via FLINK-30538.;;;","03/Feb/23 09:51;qingyue;Hi [~Paul Lin], yes, the PR has been merged. This issue is discovered when reviewing FLINK-29945. You can reach the discussion at https://github.com/apache/flink/pull/21717#discussion_r1089640498.
I've rebased your fix and added the test case to verify that all problems have been fixed.;;;","07/Feb/23 02:32;fsk119;Merged into master: bbd5a7876eb1542ff89f05f5f5d82bb8bd41b7bd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MultipleInputITCase failed with AdaptiveBatch Scheduler,FLINK-30808,13521831,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,JunRuiLi,JunRuiLi,JunRuiLi,28/Jan/23 03:43,29/Jan/23 08:09,13/Jul/23 08:29,29/Jan/23 08:09,1.16.0,1.17.0,,,,,1.16.2,1.17.0,,,,,,,Runtime / Coordination,,,,0,pull-request-available,,,"MultipleInputITCase#testRelatedInputs failed with AdaptiveBatch Scheduler.
{code:java}
java.lang.UnsupportedOperationException: Forward partitioning does not allow change of parallelism. Upstream operation: Calc[10]-14 parallelism: 1, downstream operation: HashJoin[15]-20 parallelism: 3 You must use another partitioning strategy, such as broadcast, rebalance, shuffle or global. {code}",,JunRuiLi,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30683,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jan 29 08:09:21 UTC 2023,,,,,,,,,,"0|z1ffwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"28/Jan/23 03:45;JunRuiLi;cc [~zhuzh] ;;;","28/Jan/23 04:04;JunRuiLi;The failure is because currently in StreamGraph#createActualEdge, if the partitioner is a ForwardPartitioner, then the operator parallelism of the upstream and downstream of the partitioner will be required to be consistent. However, for the ForwardForConsecutiveHashPartitioner(which is a subclass of ForwardPartitioner) used in this case, it is not required that the parallelism of upstream and downstream operators must be consistent. So we can fix this bug by fixing the StreamGraph#createActualEdge.;;;","29/Jan/23 08:09;zhuzh;master:
a55ae1c6b426acdf499fc4df766fc43daa2dcce4

release-1.16:
f8e2b618c73814236873e4d74173072acc9282d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PyFlink mishandles script dependencies,FLINK-30803,13521780,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,nuafonso,nuafonso,27/Jan/23 15:04,02/Feb/23 06:13,13/Jul/23 08:29,02/Feb/23 06:13,1.15.2,1.15.3,1.16.0,,,,1.15.4,1.16.2,1.17.0,,,,,,API / Python,,,,0,pull-request-available,,,"h2. Summary

Since Flink 1.15, PyFlink is unable to run scripts that import scripts under other directories. For instance, if _main.py_ imports {_}job/word_count.py{_}, PyFlink will fail due to not finding the _job_ directory.

The issue seems to have started after a [refactoring of _PythonDriver_|https://github.com/apache/flink/commit/330aae0c6e0811f50888d17830f10f7a29efe7d7] to address FLINK-26847. The path to the Python script is removed, which forces PyFlink to use the copy in its temporary directory. When files are copied to this directory, the original directory structure is not maintained and ends up breaking the imports.
h2. Testing

To confirm the regression, I ran the attached application in both Flink 1.14.6 and 1.15.3 clusters.
h3. Flink 1.14.6

Application was able to start after being submitted via CLI:

 
{code:java}
% ./bin/flink run --python ~/sandbox/word_count_split/main.py
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/.../flink-1.14.6/lib/flink-dist_2.12-1.14.6.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Job has been submitted with JobID 6f7be21072384ca3a314af10860c4ba8 {code}
 
h3. Flink 1.15.3

Application did not start due to not finding the _job_ directory:

 
{code:java}
% ./bin/flink run --python ~/sandbox/word_count_split/main.py
Traceback (most recent call last):
  File ""/usr/lib64/python3.7/runpy.py"", line 193, in _run_module_as_main
    ""__main__"", mod_spec)
  File ""/usr/lib64/python3.7/runpy.py"", line 85, in _run_code
    exec(code, run_globals)
  File ""/tmp/pyflink/40c649c3-24af-46ef-ae27-e0019cb55769/3673dd18-adff-40e0-bb11-06a3f00ba29c/main.py"", line 5, in <module>
    from job.word_count import word_count
ModuleNotFoundError: No module named 'job'
org.apache.flink.client.program.ProgramAbortException: java.lang.RuntimeException: Python process exits with code: 1
        at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:140)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.base/java.lang.reflect.Method.invoke(Method.java:566)
        at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
        at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
        at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:114)
        at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:841)
        at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:240)
        at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1085)
        at org.apache.flink.client.cli.CliFrontend.lambda$main$10(CliFrontend.java:1163)
        at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
        at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1163)
Caused by: java.lang.RuntimeException: Python process exits with code: 1
        at org.apache.flink.client.python.PythonDriver.main(PythonDriver.java:130)
        ... 13 more {code}
 

 ",,dannycranmer,dianfu,nuafonso,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jan/23 14:46;nuafonso;word_count_split.zip;https://issues.apache.org/jira/secure/attachment/13054841/word_count_split.zip",,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 06:13:02 UTC 2023,,,,,,,,,,"0|z1ffls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"27/Jan/23 15:56;nuafonso;Hello [~dianfu],

I am wondering if you can provide some context about your changes for FLINK-26847.
The quickest fix would be to keep the path to the script, but it looks like it might revert the fix to YARN.

Thank you,

Nuno;;;","29/Jan/23 03:14;dianfu;[~nuafonso]
Could you try if the following command works:
./bin/flink run --python ~/sandbox/word_count_split/main.py --pyFiles ~/sandbox/word_count_split;;;","30/Jan/23 17:37;nuafonso;Hello [~dianfu],

Thank you for the quick reply.

I can confirm that the application is able to run after adding the _--pyFiles_ argument.
{code:java}
[flink-1.15.3]  
% ./bin/flink run --python ~/sandbox/word_count_split/main.py --pyFiles ~/sandbox/word_count_split
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.flink.api.java.ClosureCleaner (file:/.../flink-1.15.3/lib/flink-dist-1.15.3.jar) to field java.lang.String.value
WARNING: Please consider reporting this to the maintainers of org.apache.flink.api.java.ClosureCleaner
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a future release
Job has been submitted with JobID b8e00aa7a58d1622cf8df25d414c8a0a{code}
Thank you,

Nuno;;;","31/Jan/23 07:19;dianfu;[~nuafonso] Thanks for the confirmation. You could work around it using the above approach. I will figure out if there is a better way to handle this.;;;","31/Jan/23 12:55;dianfu;[~nuafonso] Let me explain a bit more about the changes made in FLINK-26847. It's introduced to make sure `-py` works in YARN application mode. That's, users could submit PyFlink jobs using the following command:
{code:java}
./bin/flink run-application -t yarn-application \
-Djobmanager.memory.process.size=1024m \
-Dtaskmanager.memory.process.size=1024m \
-Dyarn.application.name=<ApplicationName> \
-Dyarn.ship-files=/path/to/shipfiles \
-pyarch shipfiles/venv.zip \
-pyclientexec venv.zip/venv/bin/python3 \
-pyexec venv.zip/venv/bin/python3 \
-py shipfiles/word_count.py {code}

The reason is that, for {*}-py shipfiles/word_count.py{*}, as it will create a temporary directory as the working directory of the Python process which is responsible to compile the job, so the path `shipfiles/word_count.py ` will not be found during compiling as it's not inside the working directory.

After rethinking about this issue, I think a better approach would be let users to use `-pym` explicitly in YARN application mode. So I'd like to revert the changes made in FLINK-26847. After that, users should submit PyFlink jobs using the following command in YARN application mode:
{code:java}
./bin/flink run-application -t yarn-application \
      -Djobmanager.memory.process.size=1024m \
      -Dtaskmanager.memory.process.size=1024m \
      -Dyarn.application.name=<ApplicationName> \
      -Dyarn.ship-files=/path/to/shipfiles \
      -pyarch shipfiles/venv.zip \
      -pyclientexec venv.zip/venv/bin/python3 \
      -pyexec venv.zip/venv/bin/python3 \
      -pyfs shipfiles \
      -pym word_count {code}
 ;;;","02/Feb/23 06:13;dianfu;Fixed in:
- master via 68c78dcd0a17f3d13d82b09b61246808e85961c8
- release-1.16 via 592815cec5ed5f33ae503194fccfe88c7df2eaa5
- release-1.15 via a2a7b664e827118c2d54a936e010c63b8acb2668;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Codespeed machine is not responding,FLINK-30791,13521536,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,jingge,pnowojski,pnowojski,25/Jan/23 16:43,25/Jan/23 20:06,13/Jul/23 08:29,25/Jan/23 18:29,1.16.0,1.17.0,,,,,,,,,,,,,Benchmarks,,,,0,,,,"Neither speedcenter: [http://codespeed.dak8s.net:8000/]

nor jenkins: [http://codespeed.dak8s.net:8080|http://codespeed.dak8s.net:8080/]

are responding. Both services were hosted on the same EC2 machine from Ververica's AWS account.",,jingge,pnowojski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 25 20:06:53 UTC 2023,,,,,,,,,,"0|z1fe40:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/23 16:56;jingge;Thanks for creating this ticket. We are working on it.;;;","25/Jan/23 17:46;jingge;the machine is running with no issue, there are some issues with the dns.;;;","25/Jan/23 18:29;jingge;fixed, the dns is back.;;;","25/Jan/23 20:06;pnowojski;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dmesg fails to save data to file due to permissions,FLINK-30787,13521356,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,mapohl,mapohl,25/Jan/23 11:39,30/Jan/23 11:51,13/Jul/23 08:29,30/Jan/23 11:50,1.15.3,1.16.0,1.17.0,,,,1.17.0,,,,,,,,Test Infrastructure,,,,0,pull-request-available,,,"We're not collecting the {{dmesg}} output due to a permission issue in any build:
{code}
2023-01-12T10:10:25.1598207Z dmesg: read kernel buffer failed: Operation not permitted
{code}",,mapohl,rmetzger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-13978,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 11:50:52 UTC 2023,,,,,,,,,,"0|z1fd00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"25/Jan/23 11:41;mapohl;I linked the FLINK-19699 PR that introduced this feature in the CI pipeline. [~rmetzger] can you recall whether we checked that it actual worked? Or can we assume that it was broken from the start and nobody noticed before?;;;","25/Jan/23 11:49;rmetzger;I'm pretty sure this worked before. Maybe it only worked on the alibaba machines, not on the Azure machines (or vice versa);;;","25/Jan/23 12:13;mapohl;good point, I'm gonna double-check;;;","30/Jan/23 11:50;mapohl;Backports were omitted because the issue actually doesn't create any harm besides creating an error message.

master: 5f5abb69187524de9a2d0e1f6380bed3d69e3fa1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RocksDB Memory Management end-to-end test failed due to unexpected exception,FLINK-30785,13521245,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,roman,mapohl,mapohl,25/Jan/23 09:09,10/Feb/23 09:40,13/Jul/23 08:29,10/Feb/23 09:40,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Checkpointing,,,,0,pull-request-available,test-stability,,"We see a test instability with {{RocksDB Memory Management end-to-end test}}. The test failed because an exception was detected in the logs:
{code}
2023-01-25T02:47:38.7172354Z Jan 25 02:47:38 Checking for errors...
2023-01-25T02:47:39.1661969Z Jan 25 02:47:39 No errors in log files.
2023-01-25T02:47:39.1662430Z Jan 25 02:47:39 Checking for exceptions...
2023-01-25T02:47:39.2893767Z Jan 25 02:47:39 Found exception in log files; printing first 500 lines; see full logs for details:
[...]
2023-01-25T02:47:39.5674568Z Jan 25 02:47:39 Checking for non-empty .out files...
2023-01-25T02:47:39.5675055Z Jan 25 02:47:39 No non-empty .out files.
2023-01-25T02:47:39.5675352Z Jan 25 02:47:39 
2023-01-25T02:47:39.5676104Z Jan 25 02:47:39 [FAIL] 'RocksDB Memory Management end-to-end test' failed after 1 minutes and 50 seconds! Test exited with exit code 0 but the logs contained errors, exceptions or non-empty .out files
{code}

The only exception being reported in the Flink logs is due to a warning:
{code}
2023-01-25 02:47:38,242 WARN  org.apache.flink.runtime.checkpoint.CheckpointFailureManager [] - Failed to trigger or complete checkpoint 1 for job 421e4c00ef175b3b133d63cbfe9bca8b. (0 consecutive failed attempts so far)
org.apache.flink.runtime.checkpoint.CheckpointException: Checkpoint Coordinator is suspending.
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinator.stopCheckpointScheduler(CheckpointCoordinator.java:1970) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.checkpoint.CheckpointCoordinatorDeActivator.jobStatusChanges(CheckpointCoordinatorDeActivator.java:46) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.notifyJobStatusChange(DefaultExecutionGraph.java:1578) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.transitionState(DefaultExecutionGraph.java:1145) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.executiongraph.DefaultExecutionGraph.cancel(DefaultExecutionGraph.java:973) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.scheduler.SchedulerBase.cancel(SchedulerBase.java:671) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.jobmaster.JobMaster.cancel(JobMaster.java:461) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[?:1.8.0_352]
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) ~[?:1.8.0_352]
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[?:1.8.0_352]
        at java.lang.reflect.Method.invoke(Method.java:498) ~[?:1.8.0_352]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:309) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:307) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:222) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:84) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:168) ~[flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse(PartialFunction.scala:127) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction.applyOrElse$(PartialFunction.scala:126) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:175) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:176) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive(Actor.scala:537) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.Actor.aroundReceive$(Actor.scala:535) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:579) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.actor.ActorCell.invoke(ActorCell.scala:547) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.run(Mailbox.scala:231) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at akka.dispatch.Mailbox.exec(Mailbox.scala:243) [flink-rpc-akka_98d6268d-6cd0-412b-bd3c-ff411c887a5b.jar:1.17-SNAPSHOT]
        at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692) [?:1.8.0_352]
        at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175) [?:1.8.0_352]
{code}

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45185&view=logs&j=bea52777-eaf8-5663-8482-18fbc3630e81&t=b2642e3a-5b86-574d-4c8a-f7e2842bfb14&l=5117",,mapohl,roman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 09:40:40 UTC 2023,,,,,,,,,,"0|z1fcbc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"30/Jan/23 14:05;mapohl;[~roman] [~yuanmei] can someone look into this issue?;;;","30/Jan/23 15:28;roman;Thanks [~mapohl] , 

The test failed because the logs contained exceptions. Those are CheckpointException and CancellationException.

According to [this pattern|https://github.com/apache/flink/blob/c9e87fe410c42f7e7c19c81456d4212a58564f5e/flink-end-to-end-tests/test-scripts/common.sh#L405], they both should be ignored.

However, one of the stacktraces contained ""completeExceptionally"", which I think is not ignored:
{code:java}
  2023-01-25 02:47:38,530 INFO  org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable [] - Source: EventSource -> Timestamps/Watermarks (1/2)#0 - asynchronous part of checkpoint 1 could
   not be completed.
  java.util.concurrent.CancellationException: null
          at java.util.concurrent.CompletableFuture.cancel(CompletableFuture.java:2276) ~[?:1.8.0_352]
          at org.apache.flink.runtime.state.StateUtil.discardStateFuture(StateUtil.java:78) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.lambda$cancel$0(OperatorSnapshotFutures.java:173) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.shaded.guava30.com.google.common.io.Closer.close(Closer.java:213) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.api.operators.OperatorSnapshotFutures.cancel(OperatorSnapshotFutures.java:185) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.cleanup(AsyncCheckpointRunnable.java:391) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.runtime.tasks.AsyncCheckpointRunnable.close(AsyncCheckpointRunnable.java:356) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.util.IOUtils.closeQuietly(IOUtils.java:295) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.util.IOUtils.closeAllQuietly(IOUtils.java:282) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.runtime.tasks.SubtaskCheckpointCoordinatorImpl.cancel(SubtaskCheckpointCoordinatorImpl.java:547) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$cancel$10(StreamTask.java:985) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]
          at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:774) ~[?:1.8.0_352]
          at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:750) ~[?:1.8.0_352]
          at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488) ~[?:1.8.0_352]
          at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990) ~[?:1.8.0_352]
          at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:344) ~[flink-dist-1.17-SNAPSHOT.jar:1.17-SNAPSHOT]{code}
So this looks like a false positive. I think we should add ""completeExceptionally"" to the ignore pattern.

WDYT?

 

cc: [~chesnay] ;;;","30/Jan/23 15:40;mapohl;Good point. That sounds reasonable.;;;","10/Feb/23 09:40;roman;Merged as d7bbb763ded6c0c8cd99560eeafbfd4ce800a8cf into master,

as 05e687cdea804b74534b991057b40b100aefefb3 into 1.17.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ExceptionThrowingDelegationTokenProvider/Receiver multi-threaded test issues,FLINK-30754,13520168,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,19/Jan/23 09:50,01/Feb/23 13:45,13/Jul/23 08:29,26/Jan/23 10:26,1.17.0,,,,,,1.17.0,,,,,,,,Tests,,,,0,pull-request-available,,,,,gaborgsomogyi,mbalassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 26 10:26:33 UTC 2023,,,,,,,,,,"0|z1f5pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"26/Jan/23 10:26;mbalassi;3a64648 in master;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove references to disableDataSync in RocksDB documentation,FLINK-30751,13520142,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Minor,Fixed,dchristle,dchristle,dchristle,19/Jan/23 05:53,05/Jun/23 07:08,13/Jul/23 08:29,05/Jun/23 03:45,1.16.0,,,,,,1.16.3,1.17.2,1.18.0,,,,,,Documentation,,,,0,pull-request-available,,,"The EmbeddedRocksDBStateBackend allows configuration using some predefined options via the .setPredefinedOptions method. The documentation for PredefinedOptions ([link|https://nightlies.apache.org/flink/flink-docs-master/api/java/org/apache/flink/contrib/streaming/state/PredefinedOptions.html]) mentions disableDataSync is called for {{FLASH_SSD_OPTIMIZED}} and {{{}SPINNING_DISK_OPTIMIZED{}}}.

 

But this option was removed several years ago in RocksDB 5.3.0 ([link|https://github.com/facebook/rocksdb/blob/main/HISTORY.md#530-2017-03-08]), and according to the code [PredefinedOptions.java|https://github.com/apache/flink/blob/0bbc7b1e9fed89b8c3e8ec67b7b0dad5999c2c01/flink-state-backends/flink-statebackend-rocksdb/src/main/java/org/apache/flink/contrib/streaming/state/PredefinedOptions.java#L72], it is no longer actually set in Flink.

We should remove references to disableDataSync in PredefinedOptions.java, and in state_backend.py, so that it does not appear in the documentation.",,dchristle,yunta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jun 05 03:45:48 UTC 2023,,,,,,,,,,"0|z1f5jk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jun/23 03:45;yunta;merged in master: 581936f340a5aa7cfbf9166f2b1d0ad5ed457ce2, b232a7ebbe3fbf0c444fd0f48eaeafa0aa13a27a

release-1.17: cc8b37af7c2c20e20ec71eb6b87a0103041dfd3b, 22696eec97e251d7b12dbbcc6530857643d322d2

release-1.16: b65b11bc9c87c489be0a514edd8c3290e090996c,
5c0108be3ae54bad5d7af8b8d4ed7ffac4bb8002;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactActionITCase.testBatchCompact in table store is unstable,FLINK-30750,13520135,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,zjureel,zjureel,19/Jan/23 03:29,02/Feb/23 03:22,13/Jul/23 08:29,02/Feb/23 03:22,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"https://github.com/apache/flink-table-store/actions/runs/3954989166/jobs/6772877033


2023-01-17T11:45:17.9511390Z [INFO] Results:
2023-01-17T11:45:17.9511641Z [INFO] 
2023-01-17T11:45:17.9511838Z [ERROR] Errors: 
2023-01-17T11:45:17.9512585Z [ERROR]   CompactActionITCase.testBatchCompact » JobExecution Job execution failed.
2023-01-17T11:45:17.9512964Z [INFO] 
2023-01-17T11:45:17.9513223Z [ERROR] Tests run: 224, Failures: 0, Errors: 1, Skipped: 4




Besides above error, there's another exception as followed

https://github.com/apache/flink-table-store/actions/runs/3964547232/jobs/6793496230
Caused by: java.util.concurrent.CompletionException: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
	at org.apache.flink.runtime.scheduler.DefaultExecutionDeployer.lambda$assignResource$4(DefaultExecutionDeployer.java:227)
	... 37 more
Caused by: java.util.concurrent.CompletionException: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniApply(CompletableFuture.java:607)
	at java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:591)
	... 35 more
Caused by: org.apache.flink.runtime.jobmanager.scheduler.NoResourceAvailableException: Slot request bulk is not fulfillable! Could not allocate the required slot within slot request timeout
	at org.apache.flink.runtime.jobmaster.slotpool.PhysicalSlotRequestBulkCheckerImpl.lambda$schedulePendingRequestBulkWithTimestampCheck$0(PhysicalSlotRequestBulkCheckerImpl.java:86)
	... 28 more
Caused by: java.util.concurrent.TimeoutException: Timeout has occurred: 300000 ms
	... 29 more

",,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 02 03:22:54 UTC 2023,,,,,,,,,,"0|z1f5i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 03:22;lzljs3620320;master: f2b3ba97cbcc46f2ffa519b174576eaa35e15c82;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Delegation token provider enabled flag documentation is wrong,FLINK-30749,13520088,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,18/Jan/23 15:25,19/Jan/23 15:26,13/Jul/23 08:29,19/Jan/23 15:26,1.17.0,,,,,,1.17.0,,,,,,,,Documentation,Runtime / Configuration,,,0,pull-request-available,,,,,gaborgsomogyi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 15:26:11 UTC 2023,,,,,,,,,,"0|z1f588:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"19/Jan/23 15:26;chesnay;master: 2698078ea47b828fb13b0ec1ce09b1534f7496de;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
INSERT to Kafka does not work when Kafka config auto.create.topics.enabled is set to false,FLINK-30740,13520061,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,cadonna,cadonna,18/Jan/23 13:21,29/Mar/23 01:55,13/Jul/23 08:29,29/Mar/23 01:55,table-store-0.3.0,,,,,,,,,,,,,,Table Store,,,,0,,,,"If I use Kafka as the log system and set {{auto.create.topics.enabled}} to false in Kafka INSERTs do not work.

Steps to reproduce:

# Start a Kafka broker and set {{auto.create.topics.enabled}} to false
# Issue the following statements
{code:sql}
CREATE CATALOG table_store_catalog WITH (
   'type'='table-store',
   'warehouse'=<path to object store>
);

USE CATALOG table_store_catalog;

CREATE TABLE word_count (
      word STRING PRIMARY KEY NOT ENFORCED,
      cnt BIGINT
 ) WITH (
   'log.system' = 'kafka',
   'kafka.bootstrap.servers' = <address to broker>,
   'kafka.topic' = 'test-topic,
   'log.consistency' = 'eventual'
 );
 
 INSERT INTO word_count VALUES ('foo', 1);
{code} 

The task manager logs show:
{code}
flink-sandbox-taskmanager-1  | 2023-01-18 12:46:17,085 WARN  org.apache.flink.table.store.shaded.org.apache.kafka.clients.NetworkClient [] - [Producer clientId=producer-1] Error while fetching metadata with correlation id 544 : {test-topic=UNKNOWN_TOPIC_OR_PARTITION}
{code}

The INSERT job on the task manager fails with
{code:java}
org.apache.flink.runtime.JobException: Recovery is suppressed by NoRestartBackoffTimeStrategy
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.handleFailure(ExecutionFailureHandler.java:138)
	at org.apache.flink.runtime.executiongraph.failover.flip1.ExecutionFailureHandler.getFailureHandlingResult(ExecutionFailureHandler.java:82)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.handleTaskFailure(DefaultScheduler.java:301)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.maybeHandleTaskFailure(DefaultScheduler.java:291)
	at org.apache.flink.runtime.scheduler.DefaultScheduler.updateTaskExecutionStateInternal(DefaultScheduler.java:282)
	at org.apache.flink.runtime.scheduler.SchedulerBase.updateTaskExecutionState(SchedulerBase.java:739)
	at org.apache.flink.runtime.scheduler.SchedulerNG.updateTaskExecutionState(SchedulerNG.java:78)
	at org.apache.flink.runtime.jobmaster.JobMaster.updateTaskExecutionState(JobMaster.java:443)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.lambda$handleRpcInvocation$1(AkkaRpcActor.java:304)
	at org.apache.flink.runtime.concurrent.akka.ClassLoadingUtils.runWithContextClassLoader(ClassLoadingUtils.java:83)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcInvocation(AkkaRpcActor.java:302)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleRpcMessage(AkkaRpcActor.java:217)
	at org.apache.flink.runtime.rpc.akka.FencedAkkaRpcActor.handleRpcMessage(FencedAkkaRpcActor.java:78)
	at org.apache.flink.runtime.rpc.akka.AkkaRpcActor.handleMessage(AkkaRpcActor.java:163)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:24)
	at akka.japi.pf.UnitCaseStatement.apply(CaseStatements.scala:20)
	at scala.PartialFunction.applyOrElse(PartialFunction.scala:123)
	at scala.PartialFunction.applyOrElse$(PartialFunction.scala:122)
	at akka.japi.pf.UnitCaseStatement.applyOrElse(CaseStatements.scala:20)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:171)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:172)
	at akka.actor.Actor.aroundReceive(Actor.scala:537)
	at akka.actor.Actor.aroundReceive$(Actor.scala:535)
	at akka.actor.AbstractActor.aroundReceive(AbstractActor.scala:220)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:580)
	at akka.actor.ActorCell.invoke(ActorCell.scala:548)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:270)
	at akka.dispatch.Mailbox.run(Mailbox.scala:231)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:243)
	at java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:289)
	at java.util.concurrent.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1056)
	at java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)
	at java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)
Caused by: java.lang.NullPointerException
	at org.apache.flink.table.store.kafka.KafkaSinkFunction.lambda$open$0(KafkaSinkFunction.java:75)
	at org.apache.flink.table.store.shaded.org.apache.kafka.clients.producer.KafkaProducer.doSend(KafkaProducer.java:982)
	at org.apache.flink.table.store.shaded.org.apache.kafka.clients.producer.KafkaProducer.send(KafkaProducer.java:885)
	at org.apache.flink.table.store.shaded.streaming.connectors.kafka.internals.FlinkKafkaInternalProducer.send(FlinkKafkaInternalProducer.java:142)
	at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:926)
	at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.invoke(FlinkKafkaProducer.java:101)
	at org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.invoke(TwoPhaseCommitSinkFunction.java:245)
	at org.apache.flink.table.store.connector.sink.StoreWriteOperator.processElement(StoreWriteOperator.java:134)
	at org.apache.flink.streaming.runtime.tasks.OneInputStreamTask$StreamTaskNetworkOutput.emitRecord(OneInputStreamTask.java:233)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.processElement(AbstractStreamTaskNetworkInput.java:134)
	at org.apache.flink.streaming.runtime.io.AbstractStreamTaskNetworkInput.emitNext(AbstractStreamTaskNetworkInput.java:105)
	at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:519)
	at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:203)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:804)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:753)
	at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
	at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:927)
	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:741)
	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:563)
	at java.lang.Thread.run(Thread.java:750)
	Suppressed: org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaException: Failed to send data to Kafka: Pending record count must be zero at this point: 1
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.checkErroneous(FlinkKafkaProducer.java:1428)
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:976)
		at org.apache.flink.api.common.functions.util.FunctionUtils.closeFunction(FunctionUtils.java:41)
		at org.apache.flink.table.store.connector.sink.StoreWriteOperator.close(StoreWriteOperator.java:166)
		at org.apache.flink.streaming.runtime.tasks.StreamOperatorWrapper.close(StreamOperatorWrapper.java:163)
		at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.closeAllOperators(RegularOperatorChain.java:125)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.closeAllOperators(StreamTask.java:997)
		at org.apache.flink.util.IOUtils.closeAll(IOUtils.java:254)
		at org.apache.flink.core.fs.AutoCloseableRegistry.doClose(AutoCloseableRegistry.java:72)
		at org.apache.flink.util.AbstractAutoCloseableRegistry.close(AbstractAutoCloseableRegistry.java:127)
		at org.apache.flink.streaming.runtime.tasks.StreamTask.cleanUp(StreamTask.java:916)
		at org.apache.flink.runtime.taskmanager.Task.lambda$restoreAndInvoke$0(Task.java:930)
		at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:948)
		at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:930)
		... 3 more
	Caused by: java.lang.IllegalStateException: Pending record count must be zero at this point: 1
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.flush(FlinkKafkaProducer.java:1111)
		at org.apache.flink.table.store.shaded.streaming.connectors.kafka.FlinkKafkaProducer.close(FlinkKafkaProducer.java:936)
		... 15 more
{code}

Apparently, the Kafka topic is created when the first record is written to the Kafka topic, although I found code to create a Kafka topic explicitly on table creation:
https://github.com/apache/flink-table-store/blob/f201b507fef88501c4beb4c62807bef818e31be5/flink-table-store-kafka/src/main/java/org/apache/flink/table/store/kafka/KafkaLogStoreFactory.java#L123

Topic creation should not rely on enabling auto topic creation in Kafka, because users might opt to disable auto topic creation to prevent unexpected costs when a fully-managed Kafka service is used. For example see the Confluent Cloud documentation:
https://docs.confluent.io/cloud/current/clusters/broker-config.html#enable-automatic-topic-creation  

IMO, when a table is created, the corresponding Kafka topic should be explicitly created.",,cadonna,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 01:55:34 UTC 2023,,,,,,,,,,"0|z1f528:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 01:55;lzljs3620320;https://github.com/apache/incubator-paimon/issues/740;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StringIndexer cannot handle null values correctly,FLINK-30730,13520000,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,hongfanxo,hongfanxo,hongfanxo,18/Jan/23 07:58,08/Feb/23 02:39,13/Jul/23 08:29,07/Feb/23 11:32,ml-2.1.0,,,,,,ml-2.2.0,,,,,,,,Library / Machine Learning,,,,0,pull-request-available,,,"When training data contains null values, StringIndexer throws a exception. The reason is this method [1]: null values are neither String type nor Number type.

In StringIndexerModel, null values are also not handled correctly when performing transformation.

 

[1] [https://github.com/apache/flink-ml/blob/966cedd7bbab4e12d8d8b37dbd582146714e68a6/flink-ml-lib/src/main/java/org/apache/flink/ml/feature/stringindexer/StringIndexer.java#L164]",,hongfanxo,zhangzp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 07 11:32:24 UTC 2023,,,,,,,,,,"0|z1f4oo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"07/Feb/23 11:32;zhangzp;Fixed on master via 2b83d247cb5e81fb04b31399d436dbb9e809a473;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JoinReorderITCase.testBushyTreeJoinReorder failed due to IOException,FLINK-30727,13519992,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,337361684@qq.com,mapohl,mapohl,18/Jan/23 07:25,06/Feb/23 09:03,13/Jul/23 08:29,06/Feb/23 06:49,1.17.0,,,,,,1.17.0,,,,,,,,Runtime / Network,Table SQL / Planner,,,0,pull-request-available,test-stability,,"IOException due to timeout occurring while requesting exclusive NetworkBuffer caused JoinReorderITCase.testBushyTreeJoinReorder to fail:
{code}
[...]
Jan 18 01:11:27 Caused by: java.io.IOException: Timeout triggered when requesting exclusive buffers: The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys 'taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max',  or you may increase the timeout which is 30000ms by setting the key 'taskmanager.network.memory.exclusive-buffers-request-timeout-ms'.
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalRequestMemorySegments(NetworkBufferPool.java:256)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestPooledMemorySegmentsBlocking(NetworkBufferPool.java:179)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.buffer.LocalBufferPool.reserveSegments(LocalBufferPool.java:262)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setupChannels(SingleInputGate.java:517)
Jan 18 01:11:27 	at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:277)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.InputGateWithMetrics.setup(InputGateWithMetrics.java:105)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:962)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:648)
Jan 18 01:11:27 	at org.apache.flink.runtime.taskmanager.Task.run(Task.java:556)
Jan 18 01:11:27 	at java.lang.Thread.run(Thread.java:748)
{code}
Same build, 2 failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14300
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=14362",,337361684@qq.com,fsk119,godfrey,leonard,lincoln.86xy,mapohl,pnowojski,SleePy,tanyuxin,wanglijie,,,,,,,,,,,,,,,,,,,,FLINK-30376,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 09:03:51 UTC 2023,,,,,,,,,,"0|z1f4mw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"18/Jan/23 07:25;mapohl;[~zhengyunhong97] may you have a look at it?;;;","18/Jan/23 07:48;mapohl;This is marked as a blocker because it looks like it was newly introduced.;;;","18/Jan/23 07:58;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44993&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14068;;;","19/Jan/23 02:28;337361684@qq.com;Ok, [~mapohl] , I will look at it right away.;;;","20/Jan/23 00:59;lincoln.86xy;[~337361684@qq.com] I also encountered this failure in a local machine

 

{code}

[ERROR] Errors:
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithLeftOuterJoin:230->assertEquals:47 » Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithLeftOuterJoin:230->assertEquals:47 » Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testBushyTreeJoinReorder:352->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testBushyTreeJoinReorder:352->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithFullOuterJoin:174->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithFullOuterJoin:174->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerAndFullOuterJoin:192->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerAndFullOuterJoin:192->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerAndLeftOuterJoin:249->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerAndLeftOuterJoin:249->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerJoin:210->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithInnerJoin:210->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithLeftOuterJoin:230->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithLeftOuterJoin:230->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithMixedJoinTypeAndCondition:300->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithMixedJoinTypeAndCondition:300->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithRightOuterJoin:267->assertEquals:65 Runtime
[ERROR]   JoinReorderITCase>JoinReorderITCaseBase.testJoinReorderWithRightOuterJoin:267->assertEquals:65 Runtime
[INFO]
[ERROR] Tests run: 5110, Failures: 0, Errors: 18, Skipped: 10

{code}

 

{code}

[ERROR] org.apache.flink.table.planner.runtime.batch.sql.join.JoinReorderITCase.testJoinReorderWithLeftOuterJoin(boolean)[1]  Time elapsed: 2.297 s  <<< ERROR!

Caused by: java.lang.IllegalArgumentException
    at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:122)
    at org.apache.flink.table.runtime.hashtable.BaseHybridHashTable.<init>(BaseHybridHashTable.java:164)
    at org.apache.flink.table.runtime.hashtable.BinaryHashTable.<init>(BinaryHashTable.java:163)
    at org.apache.flink.table.runtime.operators.join.HashJoinOperator.open(HashJoinOperator.java:135)
    at org.apache.flink.table.runtime.operators.multipleinput.MultipleInputStreamOperatorBase.open(MultipleInputStreamOperatorBase.java:128)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:107)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:730)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:706)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:673)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:945)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:738)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:556)
    at java.lang.Thread.run(Thread.java:877)

...

Caused by: java.io.IOException: Insufficient number of network buffers: required 97, but only 74 available. The total number of network buffers is currently set to 2048 of 32768 bytes each. You can increase this number by setting the configuration keys '
taskmanager.memory.network.fraction', 'taskmanager.memory.network.min', and 'taskmanager.memory.network.max'.
        at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.internalCreateBufferPool(NetworkBufferPool.java:493)
        at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.createBufferPool(NetworkBufferPool.java:466)
        at org.apache.flink.runtime.io.network.partition.ResultPartitionFactory.lambda$createBufferPoolFactory$0(ResultPartitionFactory.java:337)
        at org.apache.flink.runtime.io.network.partition.ResultPartition.setup(ResultPartition.java:156)
        at org.apache.flink.runtime.taskmanager.Task.setupPartitionsAndGates(Task.java:956)
        at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:648)
        at org.apache.flink.runtime.taskmanager.Task.run(Task.java:556)
        at java.lang.Thread.run(Thread.java:877)

{code};;;","20/Jan/23 07:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45082&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12221;;;","20/Jan/23 07:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45088&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12225;;;","22/Jan/23 07:55;337361684@qq.com;Hi, [~mapohl]  and [~lincoln.86xy]. Now, I cannot reproduce this problem in CI and local machine, I tried to fix it in this [pr|[https://github.com/apache/flink/pull/21724],] but it seems did not works.  It seems that this error is related to runtime, can [~wanglijie] also have a look at it.;;;","23/Jan/23 08:15;mapohl;Thanks [~337361684@qq.com]. Does it make sense to add log messages to get more information out of the CI builds?;;;","23/Jan/23 08:15;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45142&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12515;;;","23/Jan/23 08:17;mapohl;Here's another one where the same test fails but it has a different stacktrace:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45142&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12520

{code}
[...]
Jan 22 01:14:38 Caused by: org.apache.flink.runtime.io.network.partition.PartitionNotFoundException: Partition 05545fca496f58e5ce23b9216a2e6882#30@bc473b1e60621f0a2630dc1128de559d_2f556c400f5feeb5f996b57664ad8a98_30_0 not found.
Jan 22 01:14:38 	at org.apache.flink.runtime.io.network.partition.ResultPartitionManager.createSubpartitionView(ResultPartitionManager.java:70)
Jan 22 01:14:38 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel.requestSubpartition(LocalInputChannel.java:136)
Jan 22 01:14:38 	at org.apache.flink.runtime.io.network.partition.consumer.LocalInputChannel$1.run(LocalInputChannel.java:186)
Jan 22 01:14:38 	at java.util.TimerThread.mainLoop(Timer.java:555)
Jan 22 01:14:38 	at java.util.TimerThread.run(Timer.java:505)
{code};;;","24/Jan/23 07:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45153&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12515;;;","24/Jan/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45135&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=12521;;;","24/Jan/23 15:24;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45169&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12749;;;","24/Jan/23 15:25;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45170&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12231;;;","25/Jan/23 08:40;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45184&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12590;;;","26/Jan/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45202&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12590;;;","27/Jan/23 07:33;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45229&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12514;;;","27/Jan/23 09:12;pnowojski;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45225&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","29/Jan/23 11:12;SleePy;Another instance: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45311&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","30/Jan/23 07:52;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45240&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12537;;;","30/Jan/23 08:07;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45289&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12231;;;","30/Jan/23 08:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45352&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12306;;;","30/Jan/23 12:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45388&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12521;;;","30/Jan/23 13:28;zhuzh;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45404&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","31/Jan/23 02:05;godfrey;Fixed in 1.17.0: dcbb20688c0de238f65a6986f9888c5c5088e34a which adds more network memory

We will continue to follow this issue;;;","31/Jan/23 02:31;337361684@qq.com;Hi, all. The reason for the failure of this ITCase is that the network buffer are insufficient in streaming mode. Because I cannot reproduce it in local environment, I can't find the real cause of this error. So, the current solution is to allocate more network buffer by setting parameter 'taskmanager.memory.network.fraction' .

    After setting this parameter,  CI is stable.  We will continue to follow up on this error.;;;","31/Jan/23 07:39;mapohl;Both builds didn't include the aforementioned fix:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45408&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12241
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45418&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12235;;;","31/Jan/23 18:03;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45501&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12816;;;","01/Feb/23 01:18;lincoln.86xy;The build failure has included the fix

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45498&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4;;;","01/Feb/23 01:22;337361684@qq.com;Hi, [~mapohl] , The root cause of this error may be I didn't set a parallelism for TableEnvironment in this ITCase, so this ITCase used default parallelism which equals to CPU cores (In azure CI, CPU cores equals to 32) as the parallelism. For setting parallelism as 32 with the complex job graph in this case, network memory may be insufficient.

The solution to this error is to set the parallelism manually. I will verify it on a machine with a large number of cpu cores.;;;","01/Feb/23 06:13;fsk119;I meet mulitple times in the pipeline:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45530&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4]

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45530&view=logs&s=ae4f8708-9994-57d3-c2d7-b892156e7812&j=0c940707-2659-5648-cbe6-a1ad63045f0a]

Hope we can fix this soon;;;","01/Feb/23 09:19;mapohl;The fix [mentioned above|https://issues.apache.org/jira/browse/FLINK-30727?focusedCommentId=17682344&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17682344] is not included in the following failed builds:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45519&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12521
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12596
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=ce3801ad-3bd5-5f06-d165-34d37e757d90&t=5e4d9387-1dcc-5885-a901-90469b7e6d2f&l=12523
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45521&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=12520;;;","01/Feb/23 09:28;mapohl;[~337361684@qq.com] [~godfrey] The following build failed but included your fix:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45524&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12237;;;","01/Feb/23 10:20;337361684@qq.com;The root cause hotfix pr: https://github.com/apache/flink/pull/21821;;;","01/Feb/23 13:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45548&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12525;;;","02/Feb/23 08:41;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45586&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12521;;;","02/Feb/23 14:18;mapohl;The run doesn't include the fix 3ac2c330bad mentioned below, yet:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45616&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=12820];;;","02/Feb/23 14:20;mapohl;master: 3ac2c330bad3f7d41721232397581b682ee7d64a;;;","02/Feb/23 14:23;mapohl;[~337361684@qq.com] is [PR #21821|https://github.com/apache/flink/pull/21821] meant to be the final fix and this Jira issue can be closed?;;;","03/Feb/23 01:43;337361684@qq.com;Hi, [~mapohl] , I need to observe the running of CI for a day. If there is no unstable CI Tests caused by this error, it can be closed. Thanks.;;;","06/Feb/23 01:21;337361684@qq.com;Hi, [~mapohl], I think this Jira can be closed. Thanks.;;;","06/Feb/23 06:49;mapohl;Thanks for letting me know. You're free to resolve the issue yourself since you're the one who has worked on it. Anyway, I closed the issue at your request. :-);;;","06/Feb/23 09:03;337361684@qq.com;Sorry, [~mapohl] ,  I thought I didn't have the right to close it. I got it now. Thank you very much(y);;;",,,,,
Fix invalid field id for nested type in spark catalog,FLINK-30710,13519847,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,zjureel,zjureel,zjureel,17/Jan/23 05:46,17/Jan/23 07:52,13/Jul/23 08:29,17/Jan/23 07:52,table-store-0.4.0,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"Current user can create table by spark sql, but the field id will start from 0 for nested type which causes exception",,lzljs3620320,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 17 07:52:06 UTC 2023,,,,,,,,,,"0|z1f3r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"17/Jan/23 07:52;lzljs3620320;master: 6fb528f234e6d33bc3797ec23e8a7b7206a7a063;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not load the data of hive dim table when project-push-down is introduced,FLINK-30679,13518324,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,hehuiyuan,hehuiyuan,hehuiyuan,13/Jan/23 11:58,14/Feb/23 10:43,13/Jul/23 08:29,30/Jan/23 08:00,1.14.6,,,,,,1.14.7,1.15.4,1.16.2,1.17.0,,,,,Connectors / Hive,,,,0,pull-request-available,,,"维表project push down优化引入：
https://issues.apache.org/jira/browse/FLINK-29138

hive维表的两个问题：
https://issues.apache.org/jira/browse/FLINK-29992
https://issues.apache.org/jira/browse/FLINK-30679

 

 

 

Can not load the data of hive dim table when project-push-down is introduced.

 

hive-exec  version: 2.3.4

flink version: 1.14.6

flink-hive-connector: the latest code for release-1.14 branch

 

vectorize read:

 
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 3
    at org.apache.flink.connectors.hive.read.HiveTableInputFormat.useOrcVectorizedRead(HiveTableInputFormat.java:276) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:129) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader.open(HiveInputFormatPartitionReader.java:86) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.checkCacheReload(FileSystemLookupFunction.java:132) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.eval(FileSystemLookupFunction.java:105) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at LookupFunction$26.flatMap(Unknown Source) ~[?:?] {code}
 

 

mapreduce read:

 
{code:java}
java.lang.ArrayIndexOutOfBoundsException: 3
    at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.lambda$new$0(HiveMapredSplitReader.java:139) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at java.util.stream.IntPipeline$4$1.accept(IntPipeline.java:250) ~[?:1.8.0_301]
    at java.util.Spliterators$IntArraySpliterator.forEachRemaining(Spliterators.java:1032) ~[?:1.8.0_301]
    at java.util.Spliterator$OfInt.forEachRemaining(Spliterator.java:693) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:546) ~[?:1.8.0_301]
    at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260) ~[?:1.8.0_301]
    at java.util.stream.ReferencePipeline.toArray(ReferencePipeline.java:438) ~[?:1.8.0_301]
    at org.apache.flink.connectors.hive.read.HiveMapredSplitReader.<init>(HiveMapredSplitReader.java:141) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveTableInputFormat.open(HiveTableInputFormat.java:157) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.connectors.hive.read.HiveInputFormatPartitionReader.open(HiveInputFormatPartitionReader.java:86) ~[flink-connector-hive-1.14.1-SNAPSHOT.jar:1.14-SNAPSHOT]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.checkCacheReload(FileSystemLookupFunction.java:132) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at org.apache.flink.table.filesystem.FileSystemLookupFunction.eval(FileSystemLookupFunction.java:105) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at LookupFunction$26.flatMap(Unknown Source) ~[?:?]
    at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:81) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6]
    at org.apache.flink.table.runtime.operators.join.lookup.LookupJoinRunner.processElement(LookupJoinRunner.java:34) ~[flink-table-runtime_2.11-1.14.6.jar:1.14.6] {code}
 

 

The sql :

 
{code:java}
CREATE TABLE kafkaTableSource (
    name string,
    age int,
    sex string,
    address string,
    ptime AS PROCTIME()
) WITH (
    'connector' = 'kafka',
    'topic' = 'hehuiyuan1',
    'scan.startup.mode' = 'latest-offset',
    'properties.bootstrap.servers' = 'localhost:9092',
    'properties.client.id' = 'test-consumer-group',
    'properties.group.id' = 'test-consumer-group',
    'format' = 'csv'
);

CREATE TABLE printsink (
    name string,
    age int,
    sex string,
    address string,
    score bigint,
    dt string
) WITH (
    'connector' = 'print'
);

CREATE CATALOG myhive
WITH (
        'type' = 'hive',
        'default-database' = 'hhy',
        'hive-version' = '2.0.0',
        'hadoop-conf-dir'='/Users/hehuiyuan/soft/hadoop/hadoop-2.7.3/etc/hadoop'
);

USE CATALOG myhive;
USE hhy;

set table.sql-dialect=hive;
CREATE TABLE IF NOT EXISTS tmp_flink_test_text (
    name STRING,
    age INT,
    score BIGINT
) PARTITIONED BY (dt STRING) STORED AS TEXTFILE TBLPROPERTIES (
    'streaming-source.enable' = 'false',
    'streaming-source.partition.include' = 'all',
    'lookup.join.cache.ttl' = '5 min'
);
set table.sql-dialect=default;

USE CATALOG default_catalog;
INSERT INTO default_catalog.default_database.printsink
SELECT s.name, s.age, s.sex, s.address, r.score, r.dt
FROM default_catalog.default_database.kafkaTableSource  as s
JOIN myhive.hhy.tmp_flink_test_text FOR SYSTEM_TIME AS OF s.ptime  AS r
ON r.name = s.name;
 {code}
 

 ",,hehuiyuan,jark,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 08:00:47 UTC 2023,,,,,,,,,,"0|z1eucw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"16/Jan/23 01:31;hehuiyuan;[~luoyuxia] ,hi

Take a look when you have time;;;","16/Jan/23 06:25;luoyuxia;[~hehuiyuan] Thanks for contribution. I will have a look.;;;","30/Jan/23 08:00;jark;Fixed in 
 - master: 7df2a12472f4bf990f86b905cdfcbf83d19e76b3
 - release-1.16: b5f50c354c47f27ca66c2d26c78c744d1b75c65a
 - release-1.15: b412bc315fc851c453266cd7f6f98ef3ca0ea747
 - release-1.14: 08dbb6d5eff46e72af412643f80ac353636821de
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SqlGatewayServiceStatementITCase.testFlinkSqlStatements fails,FLINK-30677,13518319,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Paul Lin,mapohl,mapohl,13/Jan/23 10:50,19/Jan/23 13:42,13/Jul/23 08:29,19/Jan/23 07:08,1.17.0,,,,,,1.17.0,,,,,,,,Table SQL / Gateway,,,,0,pull-request-available,test-stability,,"We're observing a test instability with {{SqlGatewayServiceStatementITCase.testFlinkSqlStatements}} in the following builds:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44775&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14251]

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44775&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=14608
{code:java}
Jan 13 02:46:10 [ERROR] Tests run: 9, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 27.279 s <<< FAILURE! - in org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase
Jan 13 02:46:10 [ERROR] org.apache.flink.table.gateway.service.SqlGatewayServiceStatementITCase.testFlinkSqlStatements(String)[5]  Time elapsed: 1.573 s  <<< FAILURE!
Jan 13 02:46:10 org.opentest4j.AssertionFailedError: 
Jan 13 02:46:10 
Jan 13 02:46:10 expected: 
Jan 13 02:46:10   ""# table.q - CREATE/DROP/SHOW/ALTER/DESCRIBE TABLE
Jan 13 02:46:10   #
Jan 13 02:46:10   # Licensed to the Apache Software Foundation (ASF) under one or more
Jan 13 02:46:10   # contributor license agreements.  See the NOTICE file distributed with
Jan 13 02:46:10   # this work for additional information regarding copyright ownership.
Jan 13 02:46:10   # The ASF licenses this file to you under the Apache License, Version 2.0
Jan 13 02:46:10   # (the ""License""); you may not use this file except in compliance with
Jan 13 02:46:10   # the License.  You may obtain a copy of the License at
Jan 13 02:46:10   #
Jan 13 02:46:10   # http://www.apache.org/licenses/LICENSE-2.0
Jan 13 02:46:10   #
Jan 13 02:46:10   # Unless required by applicable law or agreed to in writing, software
Jan 13 02:46:10   # distributed under the License is distributed on an ""AS IS"" BASIS,
Jan 13 02:46:10   # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
Jan 13 02:46:10   # See the License for the specific language governing permissions and
Jan 13 02:46:10   # limitations under the License.
[...] {code}",,fsk119,mapohl,martijnvisser,Paul Lin,,,,,,,,,,,,,,,,,,,,,FLINK-30698,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 19 13:40:46 UTC 2023,,,,,,,,,,"0|z1euc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 10:59;mapohl;It might be related to [f1770892|https://github.com/apache/flink/commit/f1770892c5b8bce408093fc4c2fa52aafd42d7c6] (FLINK-28655) or [6a15b7e9|https://github.com/apache/flink/commit/6a15b7e9b2459f2bb398c8d7cd062c8fb92da0f2] (FLINK-29950). Both commits were added before the previous (successful) nightly run. [~yzl] and [~Paul Lin] May you have a look at it?;;;","14/Jan/23 04:16;Paul Lin;I think the problem lies here:
{code:java}
2023-01-13T01:32:32.9258077Z Jan 13 01:32:32   # test explain select with CHANGELOG_MODE
2023-01-13T01:32:32.9258568Z Jan 13 01:32:32   explain changelog_mode select `user`, product from orders;
2023-01-13T01:32:32.9259004Z Jan 13 01:32:32   !output
2023-01-13T01:32:32.9259402Z Jan 13 01:32:32   java.util.NoSuchElementException: null
2023-01-13T01:32:32.9259800Z Jan 13 01:32:32   !error {code}
It seems not directly related to f1770892 (FLINK-28655), but I will take a look.

 ;;;","14/Jan/23 09:07;Paul Lin;-I couldn't reproduce the error locally. Is that fixed already?-

Managed to reproduce the problem, like a 1 in 10 chance.;;;","16/Jan/23 09:46;mapohl;Mentioned in FLINK-30698 already but quoting it here once more:
{quote}[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44835&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=17487]

It's a different cause: A NullPointerException is thrown.
{code:java}
Jan 15 01:54:40 [ERROR] Tests run: 6, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 22.284 s <<< FAILURE! - in org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase
Jan 15 01:54:40 [ERROR] org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.testFlinkSqlStatements(String)[5]  Time elapsed: 1.212 s  <<< ERROR!
Jan 15 01:54:40 java.lang.NullPointerException
Jan 15 01:54:40 	at org.apache.flink.table.gateway.rest.SqlGatewayRestEndpointStatementITCase.stringifyException(SqlGatewayRestEndpointStatementITCase.java:194)
Jan 15 01:54:40 	at org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.runStatements(AbstractSqlGatewayStatementITCase.java:144)
Jan 15 01:54:40 	at org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.runTest(AbstractSqlGatewayStatementITCase.java:269)
Jan 15 01:54:40 	at org.apache.flink.table.gateway.AbstractSqlGatewayStatementITCase.testFlinkSqlStatements(AbstractSqlGatewayStatementITCase.java:118)
[...] {code}
{quote};;;","16/Jan/23 09:51;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44863&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14251;;;","17/Jan/23 08:36;martijnvisser;[~fsk119] Any thoughts on this one?;;;","17/Jan/23 10:00;fsk119;Sorry for introducing unstable test. I think the main root cause is because of the FLINK-29950. Before FLINK-29950, we always use a DUMMY_RESULT_STORE for the non-query results. With the DUMMY_RESULT_STORE, the ResultFetcher will actively put all data into the current buffer. But in the FLINK-29950, we uses a non-dummy result store that will start a thread periodically to fetch results. In this case, the rest client may get an empty results.  

So I think we should keep the origin behaviour to use the dummy result store. WDYT [~Paul Lin] 

 ;;;","17/Jan/23 11:25;Paul Lin;[~fsk119] I prefer the old approach too. It's intuitive and efficient.;;;","17/Jan/23 15:40;mapohl;* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44927&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=14499
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44927&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=14501;;;","18/Jan/23 07:47;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44987&view=logs&j=f2c100be-250b-5e85-7bbe-176f68fcddc5&t=05efd11e-5400-54a4-0d27-a4663be008a9&l=14503;;;","19/Jan/23 07:08;fsk119;Fixed in the 4fdb5c40094cfaa5fb3b6d7ce9ec891dab3ef32a;;;","19/Jan/23 13:40;mapohl;This build didn't contain the fix for the Shengkai's previous comment, yet. I document it anyway for completeness reasons (build git sha: b35cef59).

1 build, 2 failures:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45036&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=12619]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45036&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=12976] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update recent job status in FlinkDeployment resource object.,FLINK-30669,13517966,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,kmozaid,kmozaid,13/Jan/23 04:26,31/Jan/23 08:24,13/Jul/23 08:29,31/Jan/23 08:24,,,,,,,kubernetes-operator-1.4.0,,,,,,,,,,,,0,pull-request-available,,,"User jar has code as  -
{code:java}
main() {
 init env
 pipelines.foreach{
  env.fromSource(pipeline.getSource())
     .map(pipeline.transform())
     .sinkTo(pipeline.getSink())
  env.execute(pipeline.getName())
 }
}{code}
and below configuration -
{code:java}
execution.runtime-mode: ""BATCH""
execution.attached: ""true""
$internal.pipeline.job-id: """" {code}
When this single jar executed in Application Mode by using flink-kubernetes-operator, multiple jobs are submitted sequentially and as per design only one of the JobStatus is always associated with FlinkDeployment k8s resource, this job status is periodically updated by operator. To update job status in k8s resource, it fetches all of the job status from job-manager rest endpoint and pick the first one and update that one. Problem is, job status list returned by job-manager rest api is not sorted on time.

!image-2023-01-13-10-04-32-891.png|width=883,height=489!
!image-2023-01-13-09-54-54-280.png|width=353,height=284!

As you can see in above example, job autoscaling-3 is first one in the rest response and same updated in FlinkDeployment resource, but FlinkDeployment should have status of job autoscaling-19 because that is the last job finished.",,gyfora,kmozaid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/23 04:24;kmozaid;image-2023-01-13-09-54-13-457.png;https://issues.apache.org/jira/secure/attachment/13054568/image-2023-01-13-09-54-13-457.png","13/Jan/23 04:24;kmozaid;image-2023-01-13-09-54-54-280.png;https://issues.apache.org/jira/secure/attachment/13054567/image-2023-01-13-09-54-54-280.png","13/Jan/23 04:34;kmozaid;image-2023-01-13-10-04-32-891.png;https://issues.apache.org/jira/secure/attachment/13054569/image-2023-01-13-10-04-32-891.png",,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 08:24:10 UTC 2023,,,,,,,,,,"0|z1es5s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 12:16;gyfora;The operator doesn't really work for multiple jobs in general. The Job observer and upgrade logic already depends on a single job for detecting errors taking savepoints etc.

I don't really see the value of fixing this.;;;","31/Jan/23 08:24;gyfora;merged to main 9168c98b2e52538648670f79ee37a19e95d9580a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove Shared and Key_Shared related tests in Pulsar connector,FLINK-30657,13517938,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,syhily,syhily,syhily,13/Jan/23 01:07,31/Jan/23 10:51,13/Jul/23 08:29,31/Jan/23 10:51,1.15.3,1.16.0,pulsar-3.0.0,,,,pulsar-3.0.1,,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,test-stability,,"As the [FLINK-30413|https://issues.apache.org/jira/browse/FLINK-30413] issue talked, we have dropped the Shared and Key_Shared supported in upcoming flink-connector-pulsar 4.0 release. The flaky tests of Shared and Key_Shared still matters the old Flink build.

Cause these tests are useless now, we can just disable them without any fix.",,mapohl,martijnvisser,syhily,tison,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30351,,FLINK-30413,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 10:51:44 UTC 2023,,,,,,,,,,"0|z1erzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 08:53;mapohl;Quoting what I commented on in FLINK-30413:
{quote}[...] I realize that this very test that popped up as documented in my previous comment, should actually be disabled already. That's something we might have to look into as part of FLINK-30657. Thanks for taking care of it.
{quote};;;","13/Jan/23 21:55;syhily;[~mapohl] Thanks for notify. I have seem that these codes are already disabled, so I change the title to remove them.;;;","13/Jan/23 23:10;syhily;[~mapohl] Can you help me review these three PR?

# https://github.com/apache/flink/pull/21670
# https://github.com/apache/flink/pull/21671
# https://github.com/apache/flink-connector-pulsar/pull/17;;;","16/Jan/23 13:55;mapohl;While investigating why a disabled test as mentioned in [my previous comment|https://issues.apache.org/jira/browse/FLINK-30657?focusedCommentId=17676524&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-17676524] failed, I noticed that I overlooked the spelling of the test. Not the already disabled {{PulsarUnorderedPartitionSplitReaderTest}} is unstable but the corresponding {{PulsarOrderedPartitionSplitReaderTest}}. I created FLINK-30703 to document this.;;;","16/Jan/23 15:41;syhily;Thanks. I'll check that issue.;;;","19/Jan/23 09:19;martijnvisser;I've downgraded this ticket from a Blocker to Critical, given that I don't think this should block the Flink 1.16.1 release. ;;;","31/Jan/23 02:53;tison;flink-connector-pulsar 3.0 via https://github.com/apache/flink-connector-pulsar/pull/17

I'm not sure whether we should backport to 1.15 and 1.16.;;;","31/Jan/23 10:51;mapohl;Considering that the still open test instability is covered by FLINK-30703 and the other tests are already disabled in 1.15 and 1.16, backporting the removal might not be necessary.

I'm closing this issue

flink-connector-pulsar/v3.0: b708afd5f1b0b5fc41f2177b27a6aab0ac9136b4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store Hive catalog throws ClassNotFoundException when custom hive-site.xml is presented,FLINK-30646,13517803,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,12/Jan/23 10:35,13/Jan/23 02:15,13/Jul/23 08:29,13/Jan/23 02:15,table-store-0.3.0,table-store-0.4.0,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,Table Store,,,,0,pull-request-available,,,"For Hive 2.3.9, if a custom {{hive-site.xml}} is presented in {{$HIVE_HOME/conf}}, when creating Table Store Hive catalog in Flink, the following exception will be thrown.

{code}
Caused by: java.lang.ClassNotFoundException: Class org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl not found
	at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:2273) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2367) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.hadoop.conf.Configuration.getClass(Configuration.java:2393) ~[flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:2.8.3-10.0]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.loadFilterHooks(HiveMetaStoreClient.java:250) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:145) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:1.8.0_352]
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) ~[?:1.8.0_352]
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:1.8.0_352]
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423) ~[?:1.8.0_352]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1740) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:83) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:133) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.shaded.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:97) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalog.createClient(HiveCatalog.java:415) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalog.<init>(HiveCatalog.java:82) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.hive.HiveCatalogFactory.create(HiveCatalogFactory.java:51) ~[flink-table-store-hive-catalog-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.file.catalog.CatalogFactory.createCatalog(CatalogFactory.java:106) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:66) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:57) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.store.connector.FlinkCatalogFactory.createCatalog(FlinkCatalogFactory.java:31) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
	at org.apache.flink.table.factories.FactoryUtil.createCatalog(FactoryUtil.java:435) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.createCatalog(TableEnvironmentImpl.java:1426) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1172) ~[flink-table-api-java-uber-1.16.0.jar:1.16.0]
	at org.apache.flink.table.client.gateway.local.LocalExecutor.executeOperation(LocalExecutor.java:206) ~[flink-sql-client-1.16.0.jar:1.16.0]
	... 10 more
{code}

This is because {{hive-default.xml.template}} contains a property named {{hive.metastore.filter.hook}}. Its default value is {{org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl}}. However all Hive packages in Table Store are shaded, so the class loader cannot find the original class.

we need to remove relocation of Hive classes.",,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 13 02:15:18 UTC 2023,,,,,,,,,,"0|z1er5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"13/Jan/23 02:15;TsReaper;master: 072640a72fe18bf3c0439cd4f0ec7602e0b0ff80
release-0.3: fcea22e114888af239e3a04e130f942df655122e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
docs_404_check fail,FLINK-30641,13517757,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,dannycranmer,wanglijie,wanglijie,12/Jan/23 07:12,12/Jan/23 10:38,13/Jul/23 08:29,12/Jan/23 10:38,,,,,,,,,,,,,,,Build System,,,,0,,,,"{code:java}
Cloning into 'flink-connector-rabbitmq'...
Note: switching to '325b6ba8d866bb02204736229aa54ade304119a3'.

You are in 'detached HEAD' state. You can look around, make experimental
changes and commit them, and you can discard any commits you make in this
state without impacting any branches by switching back to a branch.

If you want to create a new branch to retain commits you create, you may
do so (now or later) by using -c with the switch command. Example:

  git switch -c <new-branch-name>

Or undo this operation with:

  git switch -

Turn off this advice by setting config variable advice.detachedHead to false

Start building sites … 
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:45:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:46:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:53:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:62:21"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:63:21"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:103:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/formats/overview.md:104:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/dynamodb"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:43:22"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:44:34"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/datastream/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/datastream/overview.md:45:35"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/dynamodb"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:70:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:76:20"": page not found
ERROR 2023/01/12 06:49:21 [en] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content/docs/connectors/table/overview.md:82:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:45:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:46:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:53:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:54:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:62:21"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:63:21"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:103:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/formats/overview.md:104:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/dynamodb"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:42:22"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:43:34"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/datastream/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/datastream/overview.md:44:35"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/dynamodb"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:70:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/kinesis"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:76:20"": page not found
ERROR 2023/01/12 06:49:30 [zh] REF_NOT_FOUND: Ref ""docs/connectors/table/firehose"": ""/home/vsts/work/1/s/docs/content.zh/docs/connectors/table/overview.md:82:20"": page not found
Total in 16618 ms
Error: Error building site: logged 28 error(s)
Error building the docs
##[error]Bash exited with code '1'. {code}",,dannycranmer,martijnvisser,wanglijie,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30643,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 12 08:03:19 UTC 2023,,,,,,,,,,"0|z1eqvc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 07:16;wanglijie;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44745&view=logs&j=c5d67f7d-375d-5407-4743-f9d0c4436a81&t=38411795-40c9-51fa-10b0-bd083cf9f5a5;;;","12/Jan/23 07:17;martijnvisser;[~danny.cranmer] It looks like this is caused by the AWS repo, based on https://github.com/apache/flink/actions/runs/3897787417/jobs/6655818209#step:5:165 - Can you have a look?

[~wanglijie] Thanks for raising the ticket, I've marked it as a blocker since it blocks all documentation builds for master;;;","12/Jan/23 07:53;dannycranmer;Yes will fix asap;;;","12/Jan/23 08:03;dannycranmer;The issue is (was) that I had deleted the {{v4}} branch in favour of {{main}} in {{flink-connector-aws}} to align with other connector repos. The Flink docs build points to this branch. Have restored the branch and kicked a build to resolve this. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unstable test in CliClientITCase,FLINK-30640,13517755,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lsy,yzl,yzl,12/Jan/23 06:44,11/Mar/23 05:08,13/Jul/23 08:29,11/Mar/23 05:08,1.17.0,,,,,,1.17.0,,,,,,,,Connectors / Hive,Table SQL / Client,,,0,pull-request-available,test-stability,,"[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44743&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4]

 

The failed test can work normally in my local environment.",,fsk119,leonard,luoyuxia,mapohl,qingyue,tartarus,,,,,,,,,,,,,,,,,FLINK-30508,,,,FLINK-30508,,,,,,,,,,,,,,"10/Mar/23 09:49;qingyue;image-2023-03-10-17-49-26-971.png;https://issues.apache.org/jira/secure/attachment/13056236/image-2023-03-10-17-49-26-971.png","10/Mar/23 09:51;qingyue;image-2023-03-10-17-51-42-192.png;https://issues.apache.org/jira/secure/attachment/13056237/image-2023-03-10-17-51-42-192.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Mar 11 05:08:26 UTC 2023,,,,,,,,,,"0|z1equw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 06:51;luoyuxia;It happens 
{code:java}
CREATE TABLE foo as select 1; 

SELECT * from foo; 

// error message
java.lang.RuntimeException: Error while running command to get file permissions : ExitCodeException exitCode=2: ls: cannot access '/tmp/junit2938020000869140718/hive_warehouse/foo/.part-ac6e74fd-9996-4d9e-bdb4-c7f4028d27ca-0-0.inprogress.6437ce4c-f979-4770-8a7a-fe6a48c6aa51': No such file or directory{code}
Seems related to CTAS, may [~tartarus] have a look?;;;","13/Jan/23 08:00;tartarus;It looks like ctas didn't execute successfully.

Will retrying with run azure work?;;;","15/Jan/23 12:58;yzl;Re-run is OK. Seems it is just not stable.;;;","16/Jan/23 07:26;mapohl;I'm closing this issue in favor of FLINK-30508.;;;","31/Jan/23 10:27;fsk119;Hi. [~mapohl] There are two different problems. ;;;","31/Jan/23 10:28;fsk119;I meet the problem again:

https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45471&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4

 ;;;","31/Jan/23 14:08;mapohl;Thanks for clarification. How can we differentiate the Jira issues? The test failure in the build reported in this Jira's description shows {{CliClientITCase.testSqlStatements}} similar to what's reported for FLINK-30508?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44743&view=logs&j=0c940707-2659-5648-cbe6-a1ad63045f0a&t=075c2716-8010-5565-fe08-3c4bb45824a4&l=15651;;;","31/Jan/23 16:43;mapohl;On another note: Are we considering this a blocker for 1.17?;;;","03/Feb/23 10:31;mapohl;[~tartarus] is this considered a testing issue or indicates a more fundamental problem in the production code?;;;","09/Feb/23 02:33;fsk119;Merged into master: 550697af3df222df906aae108e409c197c178876 

Merged into release-1.17: 7cc2b4f35ea54c260365cdbc3e1ba92a06c5b112;;;","09/Feb/23 06:24;mapohl;The following build doesn't include the aforementioned fix, yet:
1.17: https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45909&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=42892;;;","10/Mar/23 07:26;mapohl;We have this issue happening again on 1.17:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=47010&view=logs&j=a9db68b9-a7e0-54b6-0f98-010e0aff39e2&t=cdd32e0b-6047-565b-c58f-14054472f1be&l=16287;;;","10/Mar/23 07:32;mapohl;I'm increasing the priority of this issue to Blocker until it's clear whether it's a only test code issue or more severe
[~fsk119] [~tartarus] [~lsy] may you have a look at it?;;;","10/Mar/23 09:51;qingyue;I compared the `table.q` file between ""expected"" v.s. ""but was"", and found that the only difference is that the table column lengths are different from L#1358~L#1360 and L#1424.

cc [~fsk119] Do you have any clue about this?

!image-2023-03-10-17-51-42-192.png|width=2634,height=613!

 

!image-2023-03-10-17-49-26-971.png|width=1733,height=214!;;;","11/Mar/23 05:02;fsk119;SQL Client infers the column width from the input size and print the border for the content. If the content are almost same but the border size is different, I think the content may contains some magic char inside. You can take a look at the [line|https://github.com/apache/flink/blob/master/flink-table/flink-table-common/src/main/java/org/apache/flink/table/utils/print/TableauStyle.java#L338]. I think it just a test issue, we can decrease the priority for this now.  ;;;","11/Mar/23 05:08;fsk119;Because the current problem is much different with the origin problem, I open a new ticket FLINK-31403 for this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"In linux-aarch64 environment, using “is” judgment to match the window type of overwindow have returned incorrect matching results",FLINK-30637,13517615,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xinchen147,xinchen147,xinchen147,11/Jan/23 12:55,29/Jan/23 01:54,13/Jul/23 08:29,17/Jan/23 07:13,1.13.6,,,,,,1.15.4,1.16.1,1.17.0,,,,,,API / Python,,,,0,pull-request-available,,,"In  linux-arch64 environment, “window_type is OverWindow.ROW_UNBOUNDED_FOLLOWING” in  in the PandasBatchOverWindowAggregateFunctionOperation class of the pyflink source code has returned the wrong result.

For example, when window_type is 6, it represents the window type of ‘ROW_UNBOUNDED_FOLLOWING’, but “window_type is OverWindow.ROW_UNBOUNDED_FOLLOWING” return false because the memory address of window_type has changed. It will lead to the wrong type of window, such as row sliding window, so, the wrong input data of python udf have been assembled and wrong results of that have appeared.

 

Specifically, the pyflink unit testcase is ‘test_over_window_aggregate_function’ in ‘pyflink\table\tests\test_pandas_udaf.py’. It performance incorrectly when I execute it by pytest on linux-aarch64 system. I cut this unit use case to the following code and executed it in the flink standalone mode of aarch64 system, and got the same error result:

 
{code:java}
import unittest
from pyflink.table import DataTypes, TableEnvironment, EnvironmentSettings
from pyflink.table.udf import udaf, AggregateFunction


class MaxAdd(AggregateFunction, unittest.TestCase):

    def open(self, function_context):
        mg = function_context.get_metric_group()
        self.counter = mg.add_group(""key"", ""value"").counter(""my_counter"")
        self.counter_sum = 0

    def get_value(self, accumulator):
        # counter
        self.counter.inc(10)
        self.counter_sum += 10
        return accumulator[0]

    def create_accumulator(self):
        return []

    def accumulate(self, accumulator, *args):
        result = 0
        for arg in args:
            result += arg.max()
        accumulator.append(result)


@udaf(result_type=DataTypes.FLOAT(), func_type=""pandas"")
def mean_udaf(v):
    import logging
    logging.error(""debug"")
    logging.error(v)
    return v.mean()


t_env = TableEnvironment.create(
    EnvironmentSettings.new_instance().in_batch_mode().use_blink_planner().build())
t_env.get_config().get_configuration().set_string(""parallelism.default"", ""2"")
t_env.get_config().get_configuration().set_string(
    ""python.fn-execution.bundle.size"", ""1"")

import datetime

t = t_env.from_elements(
    [
        (1, 2, 3, datetime.datetime(2018, 3, 11, 3, 10, 0, 0)),
        (1, 3, 1, datetime.datetime(2018, 3, 11, 3, 10, 0, 0)),
        (1, 8, 5, datetime.datetime(2018, 3, 11, 4, 20, 0, 0))
    ],
    DataTypes.ROW(
        [DataTypes.FIELD(""a"", DataTypes.TINYINT()),
         DataTypes.FIELD(""b"", DataTypes.SMALLINT()),
         DataTypes.FIELD(""c"", DataTypes.INT()),
         DataTypes.FIELD(""rowtime"", DataTypes.TIMESTAMP(3))]))

# sink
t_env.execute_sql(""""""
        CREATE TABLE mySink (
          c INT,
          d FLOAT 
        ) WITH (
          'connector' = 'print'
        )
    """""")

t_env.create_temporary_system_function(""mean_udaf"", mean_udaf)
t_env.register_function(""max_add"", udaf(MaxAdd(),
                                        result_type=DataTypes.INT(),
                                        func_type=""pandas""))
t_env.register_table(""T"", t)
t_env.execute_sql(""""""
        insert into mySink
        select
         max_add(b, c)
         over (PARTITION BY a ORDER BY rowtime
         ROWS BETWEEN UNBOUNDED preceding AND 0 FOLLOWING),
         mean_udaf(b)
         over (PARTITION BY a ORDER BY rowtime
         ROWS BETWEEN 1 PRECEDING AND UNBOUNDED FOLLOWING)
        from T
    """""").wait()
'''
assert_equals(actual,
              [""5,4.3333335"",
               ""13,5.5"",
               ""6,4.3333335""])
'''{code}
The expected results are [""5,4.3333335"", ""13,5.5"", ""6,4.3333335""], but actual results are List(5,2.0, 13,5.5, 4,2.5). For ‘mean_udaf’ and ‘OverWindow.UNBOUNDED FOLLOWING’ in the code, by adding the error log, I found that when window_type is 6 and 'OverWindow.ROW_UNBOUNDED_FOLLOWING' also represents 6, the following code from pyflink source code returned false.
{code:java}
// pyflink\fn_execution\operations.py (line 273)
elif window_type is OverWindow.ROW_UNBOUNDED_FOLLOWING:
    # row unbounded following window
    window_start = window.lower_boundary
    for j in range(input_cnt):
        start = max(j + window_start, 0)
        series_slices = [s.iloc[start: input_cnt] for s in input_series]
        result.append(func(series_slices)){code}
And it It finally chose row sliding window to assemble input data of mean_udaf:
{code:java}
// pyflink\fn_execution\operations.py (line 280) 
else:
    # row sliding window
    window_start = window.lower_boundary
    window_end = window.upper_boundary
    for j in range(input_cnt):
        start = max(j + window_start, 0)
        end = min(j + window_end + 1, input_cnt)
        series_slices = [s.iloc[start: end] for s in input_series]
        result.append(func(series_slices)){code}
Obviously, that's not right. The right choice will be made in x86 environment.

The reason is window_ type‘s memory address  is different from ‘OverWindow.ROW_ UNBOUNDED_ FOLLOWING’ in linux-aarch64 environment. On the contrary, they are the same in the linux-x86 environment. The reason why the memory address is different is unknown yet. But I observed that window_type comes from 'serialized_fn.windows':
{code:java}
def __init__(self, spec): 
super(PandasBatchOverWindowAggregateFunctionOperation, self).__init__(spec) 
self.windows = [window for window in self.spec.serialized_fn.windows]
{code}
Perhaps grpc, protobuf dependencies or serialization operations in the arrch environment have affected the memory address of the int variables, I'll explore the underlying reasons later.

 

Solution and suggestion:

Since the window selections need to compare the values of two integer variables(window_type, OverWindow.ROW_ UNBOUNDED_ FOLLOWING), I recommend replacing ‘is’ with ‘==’ at the window type matching. That can also prevents erroneous results caused by python small integer object pool failure which may also affects the memory address. And this modification has been verified to perform correctly on both x86 and aarch64 environments, either this unit test case or the case I cut.

 

 

 

 

 ","Linux version 5.10.0-60.18.0.50.oe2203.aarch64

(abuild@obs-worker-002) (gcc_old (GCC) 10.3.1, GNU ld (GNU Binutils) 2.37) #1 SMP Wed Mar 30 02:43:08 UTC 2022

 

pyflink-version：1.13.6",hxbks2ks,martijnvisser,xinchen147,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,Tue Jan 17 07:13:29 UTC 2023,,,,,,,,,,"0|z1eq0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 13:16;martijnvisser;[~xinchen147] Can you please verify this in Flink 1.16, since Flink 1.13 is no longer supported in the community?;;;","11/Jan/23 13:43;xinchen147;[~martijnvisser] Thank you for your reply. Maybe I can have a try in Flink 1.16, but the same part of Pyflink‘s source code has not changed yet in Flink 1.16. And I'll provide a detailed description of the problem and suggested solutions to this section later. [~hxbks2ks] also knows the problem because I have consulted him earlier.;;;","11/Jan/23 14:07;martijnvisser;[~xinchen147] I can imagine, but there have been a number of dependency updates in later versions which is why I thought it could have a different outcome on a later version;;;","12/Jan/23 12:52;xinchen147;Why the window_type's memory address from serialized_fn.windows is diffrent from ‘OverWindow.ROW_ UNBOUNDED_ FOLLOWING’?

I learned that python uses the small integer object pool to control. When a represents 5 and b also represents 5, they are the same object, ‘a is b’ will return ‘True‘. But this is not the case here. And I do experiments in linux-aarch64 system as follows:
{code:java}
a = 5 
logging.error(a is window_type)  # Window_type represents 5, but the result is False.
logging.error(a is OverWindow.ROW_UNBOUNDED_FOLLOWING) # OverWindow.ROW_UNBOUNDED_FOLLOWING represents 5, the result is True

##### test for small integer object pool
b = 5
c = 5 
logging.error(b is c) # The result is True.{code}
It seems that the python small integer pool is not invalid, so I think it may be necessary to trace the source of window_type. I hope someone can give some suggestions and ideas. Thank you very much.

 

 ;;;","17/Jan/23 07:13;hxbks2ks;Merged into master via d053867fb5c0fc647ea9266aab35598d7f3fc5c4
Merged into release-1.16 via eca940c5bf9e17c90dbb6f35e4ba370027137368
Merged into release-1.15 via 4035d61a2756ec16046fb687f533be0501fbbd35;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClientHeartbeatTest.testJobRunningIfClientReportHeartbeat is unstable,FLINK-30629,13517561,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,Jiangang,xtsong,xtsong,11/Jan/23 08:33,27/Jun/23 07:43,13/Jul/23 08:29,27/Jun/23 06:21,1.17.0,1.18.0,,,,,1.17.2,1.18.0,,,,,,,Client / Job Submission,,,,0,pull-request-available,test-stability,,"https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=10819

{code:java}
Jan 11 04:32:39 [ERROR] Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 21.02 s <<< FAILURE! - in org.apache.flink.client.ClientHeartbeatTest
Jan 11 04:32:39 [ERROR] org.apache.flink.client.ClientHeartbeatTest.testJobRunningIfClientReportHeartbeat  Time elapsed: 9.157 s  <<< ERROR!
Jan 11 04:32:39 java.lang.IllegalStateException: MiniCluster is not yet running or has already been shut down.
Jan 11 04:32:39 	at org.apache.flink.util.Preconditions.checkState(Preconditions.java:193)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniCluster.getDispatcherGatewayFuture(MiniCluster.java:1044)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniCluster.runDispatcherCommand(MiniCluster.java:917)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniCluster.getJobStatus(MiniCluster.java:841)
Jan 11 04:32:39 	at org.apache.flink.runtime.minicluster.MiniClusterJobClient.getJobStatus(MiniClusterJobClient.java:91)
Jan 11 04:32:39 	at org.apache.flink.client.ClientHeartbeatTest.testJobRunningIfClientReportHeartbeat(ClientHeartbeatTest.java:79)
{code}
",,Jiangang,mapohl,renqs,Sergey Nuyanzin,Weijie Guo,xtsong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/23 10:02;Jiangang;ClientHeartbeatTestLog.txt;https://issues.apache.org/jira/secure/attachment/13054544/ClientHeartbeatTestLog.txt","31/May/23 06:02;Sergey Nuyanzin;logs-cron_azure-test_cron_azure_core-1685497478.zip;https://issues.apache.org/jira/secure/attachment/13058664/logs-cron_azure-test_cron_azure_core-1685497478.zip",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jun 27 06:21:33 UTC 2023,,,,,,,,,,"0|z1epow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 08:36;xtsong;[~Jiangang], could you please take a look at this？;;;","11/Jan/23 09:28;mapohl;Same build, multiple test failures:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=10895
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44690&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=10821;;;","11/Jan/23 09:47;xtsong;Offline reached out to [~Jiangang] and he said he would look into this.

I've disabled the test case in 9b5394ad8f1c935ecadcf5015b8ed90d78efd3c7.;;;","11/Jan/23 12:18;Jiangang;[~xtsong] Thanks. I would fix it as soon as possible.;;;","12/Jan/23 10:08;Jiangang;[~xtsong] I have reproduced the failure locally and upload the log. From the log, we can see that the dispatcher-5 is busy with scheduling the job and the process jobClientAlivenessCheck is delayed. The client heartbeat may be not received during this time for the same reason. 

To solve the problem, we can move the method initJobClientExpiredTime after the method runJob in Dispatcher's  method runRecoveredJob. What do you think? Thanks.;;;","13/Jan/23 02:25;xtsong;[~Jiangang], Thanks for looking into this. Sounds good to me.;;;","16/Jan/23 06:39;Jiangang;[~xtsong] Have a look at the code? Thanks.;;;","09/Feb/23 03:37;xtsong;- master (1.18): 2ad9585f47fe7fa3dcad286194bdcc4dcd131712
- release-1.17: 5c14d4a266a32c91ba6512d831f36a211907c16f;;;","09/Feb/23 06:45;renqs;The unstable case still exists on 1.17:

[https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45921&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9864]

The failed case is ClientHeartbeatTest.testJob{*}CancelledIfClientHeartbeatTimeout{*}, which looks like related to this one, so I report the case here. 

[~Jiangang] Could you take a look when you are available? Thanks;;;","10/Feb/23 07:02;mapohl;I'm posting it here as well because [~renqs] already opened this reopened this one:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45934&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9716
{code}
Feb 09 07:53:07 [ERROR] Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 9.939 s <<< FAILURE! - in org.apache.flink.client.ClientHeartbeatTest
Feb 09 07:53:07 [ERROR] org.apache.flink.client.ClientHeartbeatTest.testJobCancelledIfClientHeartbeatTimeout  Time elapsed: 2.708 s  <<< FAILURE!
Feb 09 07:53:07 java.lang.AssertionError: 
Feb 09 07:53:07 
Feb 09 07:53:07 Expecting actual throwable to be an instance of:
Feb 09 07:53:07   java.util.concurrent.ExecutionException
Feb 09 07:53:07 but was:
Feb 09 07:53:07   java.util.concurrent.TimeoutException
Feb 09 07:53:07 	at java.util.concurrent.CompletableFuture.timedGet(CompletableFuture.java:1784)
Feb 09 07:53:07 	at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1928)
Feb 09 07:53:07 	at org.assertj.core.internal.Futures.assertFailedWithin(Futures.java:118)
Feb 09 07:53:07 	...(61 remaining lines not displayed - this can be changed with Assertions.setMaxStackTraceElementsDisplayed)
Feb 09 07:53:07 	at org.apache.flink.client.ClientHeartbeatTest.testJobCancelledIfClientHeartbeatTimeout(ClientHeartbeatTest.java:64)
[...]
{code};;;","10/Feb/23 12:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46001&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9788;;;","10/Feb/23 12:33;mapohl;This seems to pop up more generally now, huh?
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46002&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9713;;;","13/Feb/23 09:08;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46020&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9713;;;","13/Feb/23 09:08;mapohl;[~Jiangang] can you have a look at this once more?;;;","13/Feb/23 09:17;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9713;;;","13/Feb/23 09:35;Weijie Guo;[~mapohl] It seems that the recent failed test case is different from the previous one. The new failure case is `testJobCanceledIfClientHeartbeatTimeout` instead of `testJobRunningIfClientReportHeartbeat`. 

I looked at the code a little. It seems that the reason for the failure is that the test gave an inappropriate timeout value, which will make it very unstable. Since we should not rely on local timeout is the consensus of flink community , we can make this test rely on global timeout to avoid this problem. Of course, I will open a pr to fix this.

 ;;;","13/Feb/23 09:40;mapohl;Thanks for looking into it, [~Weijie Guo];;;","13/Feb/23 09:40;mapohl;Multiple test failures in same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46023&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9713
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46029&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9932;;;","13/Feb/23 09:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46032&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9714;;;","13/Feb/23 09:52;mapohl;2 failures, same build:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46033&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9792
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46033&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9714;;;","13/Feb/23 10:01;Weijie Guo;I have opened a pr([https://github.com/apache/flink/pull/21916]) to handle this, [~mapohl] Would you mind taking a look.;;;","13/Feb/23 10:02;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46036&view=logs&j=f0ac5c25-1168-55a5-07ff-0e88223afed9&t=50bf7a25-bdc4-5e56-5478-c7b4511dde53&l=9929;;;","14/Feb/23 10:46;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46087&view=logs&j=0e7be18f-84f2-53f0-a32d-4a5e4a174679&t=7c1d86e3-35bd-5fd5-3b7c-30c126a78702&l=9712;;;","14/Feb/23 14:45;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46124&view=logs&j=0da23115-68bb-5dcd-192c-bd4c8adebde1&t=24c3384f-1bcb-57b3-224f-51bf973bbee8&l=9792;;;","15/Feb/23 03:34;Weijie Guo;master(1.18) via bf0ad52cbcb052961c54c94c7013f5ac0110ef8a

release-1.17 via 21158c06516be4d888cd1dcf907717ea38747dfb;;;","15/Feb/23 07:47;mapohl;Thanks for taking care of it, [~Weijie Guo]. Just a reminder: The fixVersion is only set to 1.18.0 for an issue after 1.17.0 is actually released. Right now, merging to {{master}} and {{release-1.17}} doesn't mean that the fixVersion (and to some degree also the affected version) has to be set to 1.18.0. If we merge to both branches, it essentially means that it is fixed in 1.17.0. If, theoretically, we have a change that only goes into {{master}}, that would mean that the corresponding Jira issue would get a fixVersion {{1.18.0}} but not {{1.17.0}}. 

This scheme only applies in the phase where we have a release branch created but the corresponding release not finalized. It took me a bit to get my head around that one as well. And I even do it wrong sometimes now. I just wanted to clarify it once more. :-) I updated the ticket accordingly.;;;","15/Feb/23 07:51;Weijie Guo;Thanks [~mapohl] for the clarification. Your reminder is very helpful~(y);;;","15/Feb/23 08:31;mapohl;This build doesn't contain the aforementioned fix, yet:
* https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=46153&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9784;;;","02/May/23 05:31;Sergey Nuyanzin;Reopen since it appeared again for master
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48608&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9871;;;","02/May/23 05:34;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48604&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9871;;;","02/May/23 11:30;Sergey Nuyanzin;[~Weijie Guo] could you please have a look here?;;;","09/May/23 07:04;mapohl;https://dev.azure.com/mapohl/flink/_build/results?buildId=1310&view=logs&j=9dc1b5dc-bcfa-5f83-eaa7-0cb181ddc267&t=511d2595-ec54-5ab7-86ce-92f328796f20&l=9775;;;","09/May/23 09:37;Weijie Guo;It seems that the original problem fixed by [~Jiangang] has reappeared. Would you mind taking  a look at this?;;;","12/May/23 05:39;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48940&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9775;;;","15/May/23 04:00;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=48972&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9703;;;","20/May/23 07:29;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49173&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9993;;;","20/May/23 07:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49175&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9704;;;","23/May/23 04:08;Jiangang;[~Weijie Guo] [~Sergey Nuyanzin]  I will have a look. ;;;","27/May/23 05:34;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49421&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9992;;;","29/May/23 10:11;Jiangang;[~Sergey Nuyanzin] I have tried multi times but can not reproduce it locally. I see that the time elapsed nearly 20+ second between the
ClientHeartbeatTest' start and the final error. In normal case, each case should be less than 2 second. I wonder whether the info log is printed in somewhere? It is hard to see the problem just from the code. We should more infos. Or can you give me some suggestions? Thanks.;;;","31/May/23 05:35;Weijie Guo;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49492&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9920;;;","31/May/23 06:03;Sergey Nuyanzin;[~Jiangang] I've attached logs(related ci job) for the latest failure
hope it will help
Otherwise we could think of adding more logs to have more info next time;;;","02/Jun/23 12:05;Jiangang;[~Sergey Nuyanzin] Thanks. From the log, we can see the logs in time order:
 # The dispatcher shuts down for that the client's heartbeat timeout.
 # The client begins to report its heartbeat.

The reason is that the client will report its heartbeat after calling the method waitUntilJobInitializationFinished. In this method, we try to get the job's status by waiting 

exponentially and it may take a while. There are two ways to fix the test:
 # Increase the client's timeout from 500 ms to 1 second or more.
 # In the method waitUntilJobInitializationFinished, try to get the job's status more frequently.

What do you think? cc [~xtsong] ;;;","06/Jun/23 14:43;Sergey Nuyanzin;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=49532&view=logs&j=77a9d8e1-d610-59b3-fc2a-4766541e0e33&t=125e07e7-8de0-5c6c-a541-a567415af3ef&l=9921;;;","06/Jun/23 14:47;Sergey Nuyanzin;[~Jiangang] thanks for your update 
can we increase it till 1 sec it see if it helps?;;;","09/Jun/23 02:09;Jiangang;[~Sergey Nuyanzin] Fixed. Please review the code. Thanks.;;;","09/Jun/23 09:03;Sergey Nuyanzin;Thanks [~Jiangang]
Could you please rebase branch to the latest master since it's failed because of FLINK-32231;;;","09/Jun/23 12:16;Jiangang;Rebased. Thanks.;;;","27/Jun/23 06:21;Sergey Nuyanzin;Thanks [~Jiangang] 
the pr merged to master as 02ab5374e88780bce3c9a6991e36a7b681f88a20, a couple of weeks ago and there is no new cases so far
closing the issue;;;"
Kerberos in HiveCatalog is not work,FLINK-30628,13517528,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,11/Jan/23 04:21,19/Mar/23 05:39,13/Jul/23 08:29,19/Mar/23 05:39,,,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,Table Store,,,,0,pull-request-available,,,We should read kerberos keytab from catalog options and doAs for hive metastore client.,,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,2023-01-11 04:21:05.0,,,,,,,,,,"0|z1ephk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in stateBackends.FS / FS_ASYNC / MEMORY on 05.01.2023,FLINK-30624,13517476,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,wanglijie,martijnvisser,martijnvisser,10/Jan/23 17:05,01/Feb/23 06:13,13/Jul/23 08:29,01/Feb/23 06:13,,,,,,,1.17.0,,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"stateBackends.FS median=4147.01197 recent_median=3957.8419495
stateBackends.FS_ASYNC median=4148.8160595 recent_median=3973.418166
stateBackends.MEMORY median=4114.406091 recent_median=3935.8805775

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=stateBackends.FS&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=stateBackends.FS_ASYNC&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=stateBackends.MEMORY&extr=on&quarts=on&equid=off&env=2&revs=200

",,martijnvisser,masteryhx,pnowojski,roman,surendralilhore,wanglijie,zhuzh,,,,,,,,,,,,,,,,,,,,FLINK-30625,,,,,,,,,FLINK-30625,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 01 06:12:57 UTC 2023,,,,,,,,,,"0|z1ep60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"12/Jan/23 03:07;masteryhx;I guess the regression is caused by FLINK-30544.
In theory, FLINK-30544 trade o( n ) space which is located at heap for o(logn) time.

It will reduce heap spaces StateBackend could use so that make heap-related StateBackend like FS/MEMORY regress.

I also submitted benchmark requests to verify this:
 * [179|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/179/] – before FLINK-30544:
 ** stateBackends.MEMORY:  4177.022 (+-53.816)
 ** stateBackends.FS: 4153.435 (+-66.577)
 ** stateBackends.FS_ASYNC: 4139.056 (+-40.393)
 * [180|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/180/] - after FLINK-30544:
 ** stateBackends.MEMORY:  3936.158 (+-65.696)
 ** stateBackends.FS: 3963.934 (+-64.942)
 ** stateBackends.FS_ASYNC: 3930.955 (+-68.809)

So maybe [~wanglijie] could help to take a look ?;;;","12/Jan/23 03:56;wanglijie;[~masteryhx] I 'll take a look.;;;","12/Jan/23 07:18;martijnvisser;Thank you for picking this up!;;;","29/Jan/23 15:42;wanglijie;After an offline discussion with [~zhuzh], we believe that the regressions are caused by some statebackend-related code blocks([HeapPriorityQueue|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapPriorityQueue.java], [HeapPriorityQueueElement|https://github.com/apache/flink/blob/master/flink-runtime/src/main/java/org/apache/flink/runtime/state/heap/HeapPriorityQueueElement.java]) that can no longer be inlined by jvm after FLINK-30544. Before FLINK-30544, there was only one implementation of {{HeapPriorityQueueElement}} (the [TimerHeapInternalTimer|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/api/operators/TimerHeapInternalTimer.java]) while running the benchmark, which allowed the jvm to inline its methods (getInternalIndex,setInternalIndex). However, we introduced [InputChannelStatus|https://github.com/apache/flink/blob/master/flink-streaming-java/src/main/java/org/apache/flink/streaming/runtime/watermarkstatus/StatusWatermarkValve.java#:~:text=protected%20static%20class%20InputChannelStatus%20implements%20HeapPriorityQueueElement%20%7B] (implements {{{}HeapPriorityQueueElement{}}}) in FLINK-30544, resulting in two implementations of {{{}HeapPriorityQueueElement{}}}, the related methods can no longer be inlined, introduced the extra virtual method call overhead, resulting in the performance regression.

I have ran a [test branch|https://github.com/wanglijie95/flink/tree/after-30544-fix] to verify our thought. I created new classes {{{}HeapPriorityQueue2/HeapPriorityQueueElement2{}}}, which have exactly the same logic as {{{}HeapPriorityQueue/HeapPriorityQueueElement{}}}, and used them in {{StatusWatermarkValve}} as replacements. Then I ran the benchmarks and found that the regressions disappeared:
 * [209 - test branch|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/209/]
 ** stateBackends.MEMORY:  4138.693 (+-66.226)
 ** stateBackends.FS: 4172.001 (+-50.766)
 ** stateBackends.FS_ASYNC: 4200.842 (+-51.285)

I think we can solve this ticket by the about approach(introduce new classes with the same logic), I will open a PR to fix it soon.;;;","30/Jan/23 16:49;zhuzh;Fix 21e844b6c00b1796fdfc00136ca26d90e889b149 is merged.
Will leave this ticket open for a while to check the benchmark results.;;;","01/Feb/23 06:12;zhuzh;The performance regression is gone.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Performance regression in checkpointSingleInput.UNALIGNED on 04.01.2023,FLINK-30623,13517474,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,fanrui,martijnvisser,martijnvisser,10/Jan/23 17:02,31/Jan/23 14:09,13/Jul/23 08:29,31/Jan/23 14:09,,,,,,,1.17.0,,,,,,,,Benchmarks,Runtime / Checkpointing,,,0,pull-request-available,,,"Performance regression
checkpointSingleInput.UNALIGNED median=338.1445195 recent_median=67.6453005
checkpointSingleInput.UNALIGNED_1 median=213.230041 recent_median=39.830277
deployAllTasks.STREAMING median=168.533106 recent_median=159.8534395
stateBackends.MEMORY median=3229.0248875 recent_median=2985.782919
tupleKeyBy median=4155.684199 recent_median=3987.5812305

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED_1&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=8&ben=deployAllTasks.STREAMING&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=6&ben=stateBackends.MEMORY&extr=on&quarts=on&equid=off&env=2&revs=200

http://codespeed.dak8s.net:8000/timeline/#/?exe=6&ben=tupleKeyBy&extr=on&quarts=on&equid=off&env=2&revs=200",,fanrui,lindong,martijnvisser,pnowojski,qingyue,roman,surendralilhore,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30533,,FLINK-30709,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 31 14:09:26 UTC 2023,,,,,,,,,,"0|z1ep5k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 09:38;Yanfei Lei;For checkpointSingleInput.UNALIGNED and checkpointSingleInput.UNALIGNED_1, the regression occurred during 13ef498172b...fb272D2cdebf. Seems to be related to fb272D2cdebf(FLINK-30533). I submitted two benchmark requests to verify this:
 * [#177|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/177/] (before FLINK-30533)
 ** checkpointSingleInput.UNALIGNED ： 333.635178(+-8.169488)
 ** checkpointSingleInput.UNALIGNED_1：213.837107(+-7.282883)
 * [#178|http://codespeed.dak8s.net:8080/job/flink-benchmark-request/178/] (after FLINK-30533)
 ** checkpointSingleInput.UNALIGNED ： 61.536982（+-3.581509）
 ** checkpointSingleInput.UNALIGNED_1：38.207438（+-2.937051）

[~lindong] could you please help take a look?;;;","11/Jan/23 09:49;martijnvisser;[~Yanfei Lei]  Thanks for helping out on this;;;","12/Jan/23 13:04;pnowojski;I have [some doubts|https://issues.apache.org/jira/browse/FLINK-30533?focusedCommentId=17676585&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17676585] about FLINK-30533 design. Maybe it would be best to revert it and re-open the discussion? ;;;","16/Jan/23 10:37;fanrui;Thanks [~martijnvisser] reports this issue, and [~Yanfei Lei]  [~pnowojski] discuss here.

After analysis, I think the root cause is FLINK-30533 doesn't check `recordWriter.isAvailable()` in the loop of SourceOperator.

When the recordWriter is unavailable, task shouldn't send data to downstream task, the task thread may get stuck in requestMemoryBuffer due to backpressure. The checkpoint barrier cannot be handled when task thread is stucking in requestMemoryBuffer, that's why performance regression for unaligned checkpoint. 

cc [~lindong] 

 ;;;","16/Jan/23 14:46;lindong;Thanks [~martijnvisser] for creating the issue. And thanks [~fanrui] for creating the bugfix PR!

Sorry, I didn't notice the Jira notification due to my gmail filter misconfiguration. I just fixed the configuration and should be able to notice Jira comments in the future.

I will look into this issue and the PR tomorrow morning.;;;","26/Jan/23 19:54;pnowojski;It seems like not everything has been fixed?

[http://codespeed.dak8s.net:8000/timeline/?ben=checkpointSingleInput.UNALIGNED&env=2]

Does anyone has some idea why?;;;","27/Jan/23 06:22;fanrui;FLINK-26803 was merged at 23.01.2023, I'm not sure whether it will affect the benchmark. I can check it today.;;;","27/Jan/23 08:51;pnowojski;Thanks!;;;","27/Jan/23 10:52;lindong;Merged to master branch with the following commits:

2d1510a9d559a49806a60ececfd854dd53a6591d
877511b8ea2b4ef62fc520a0d0bd9087f2f25c56
560f314703858464f5089e24e065347d00704af5;;;","27/Jan/23 12:05;lindong;It appears that the records/sec of checkpointSingleInput dropped by about 9% from 337.5 to 309.4.

My hypothesis for this regression is that most workload of this benchmark is related to control flow (e.g. checkpoint-related events) rather than data flow. Thus it can not take advantage of the optimization we put in emitNext(...). Instead, it suffers the additional overhead of the operation (e.g. checking whether mailbox is empty and whether task is available) we put in the emitNext(), which leads to the 10% performance regression.

 ;;;","27/Jan/23 12:53;fanrui;> It can not take advantage of the optimization we put in emitNext(...). Instead, it suffers the additional overhead of the operation (e.g. checking whether mailbox is empty and whether task is available) we put in the emitNext(), which leads to the 10% performance regression.

Hi [~lindong] thanks for your feedback. Sorry, I don't think it can lead to the 10% performance regression. We can take a look the benchmark of mapSink[1]. The records/sec can reach 40k, indicating that the additional overhead is very small. Shouldn't reduce 330 records/sec to 300.

I think FLINK-26803 is the root cause, the execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=5 by default. It means 5 subtasks will share the same Unaligned checkpooint file. It will reduce the number of small files, but the UC time will become larger.

I run the UNALIGNED benchmark on my Mac based on master, and check the benchmark result after setting execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=1.
{code:java}
// execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=5
Benchmark                                            (mode)   Mode  Cnt    Score    Error  Units
CheckpointingTimeBenchmark.checkpointSingleInput  UNALIGNED  thrpt   30  317.063 ± 12.762  ops/s

// execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=1
Benchmark                                            (mode)   Mode  Cnt    Score   Error  Units
CheckpointingTimeBenchmark.checkpointSingleInput  UNALIGNED  thrpt   30  351.852 ± 6.857  ops/s {code}
 

From the benchmark, I think we can ensure FLINK-26803 is the root cause. 

[~pnowojski], do you think we should set execution.checkpointing.unaligned.max-subtasks-per-channel-state-file=1 in the flink-benchmark? Or should we add the parameters about it? We can compare the performance impact on UC under different execution.checkpointing.unaligned.max-subtasks-per-channel-state-file.

 

[1] [http://codespeed.dak8s.net:8000/timeline/?ben=mapSink.F27_UNBOUNDED&env=2]
[2] [http://codespeed.dak8s.net:8000/timeline/#/?exe=1&ben=checkpointSingleInput.UNALIGNED&extr=on&quarts=on&equid=off&env=2&revs=200];;;","30/Jan/23 14:37;pnowojski;Thanks for the analysis [~fanrui] !
{quote} It means 5 subtasks will share the same Unaligned checkpooint file. It will reduce the number of small files, but the UC time will become larger.
{quote}
What do you think is the actual reason behind the regression? That now we have to enqueue writes from a couple of subtasks one after another, so for example with 2 subtasks, the second has to wait until first completes it's writes? 

And what do you think is the impact of this setting in a production setups? If this is an issue related to number of shared IO threads, it might be that we only increased the checkpoint time by a small constant ({{{}0.3ms / checkpoint{}}} based on the numbers that [~fanrui]  has quoted), that simply remains a constant with more realistic setups. Increase checkpoint time by 0.3ms when checkpoints are taking a couple of seconds, doesn't matter.;;;","31/Jan/23 09:47;fanrui;{quote} What do you think is the actual reason behind the regression? That now we have to enqueue writes from a couple of subtasks one after another, so for example with 2 subtasks, the second has to wait until first completes it's writes? 
{quote} 

When two subtasks share the same file, only one subtask can write the file at a time, that is, subtask 2 must wait while subtask1 is writing to the file, so the UC time will become larger.

{quote} And what do you think is the impact of this setting in a production setups?
{quote} 

I think the impact on production is minimal. If hdfs writes quickly, the checkpoint time will not increase significantly. Of course, if hdfs writes slowly, it may have an impact, a reasonable solution at this point is: flink or hdfs sre should improve the stability and performance of hdfs.
;;;","31/Jan/23 14:09;pnowojski;Ok, let's keep it as it is. Thanks [~fanrui] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
flink-connector-pulsar not retrievable from Apache's Snapshot Maven repository,FLINK-30618,13517434,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,martijnvisser,mapohl,mapohl,10/Jan/23 10:43,26/Jan/23 08:27,13/Jul/23 08:29,26/Jan/23 08:27,1.17.0,,,,,,1.17.0,,,,,,,,Connectors / Pulsar,Test Infrastructure,,,0,pull-request-available,test-stability,,"The build failure was caused by {{flink-connector-pulsar}} not being retrievable from the Apache Snapshot Maven repository:
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=de826397-1924-5900-0034-51895f69d4b7&t=f311e913-93a2-5a37-acab-4a63e1328f94&l=10132

{code}
Jan 10 02:03:24 [WARNING] The requested profile ""skip-webui-build"" could not be activated because it does not exist.
Jan 10 02:03:24 [ERROR] Failed to execute goal on project flink-python: Could not resolve dependencies for project org.apache.flink:flink-python:jar:1.17-SNAPSHOT: Could not find artifact org.apache.flink:flink-sql-connector-pulsar:jar:4.0-SNAPSHOT in apache.snapshots (https://repository.apache.org/snapshots) -> [Help 1]
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
Jan 10 02:03:24 [ERROR] Re-run Maven using the -X switch to enable full debug logging.
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] For more information about the errors and possible solutions, please read the following articles:
Jan 10 02:03:24 [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/DependencyResolutionException
Jan 10 02:03:24 [ERROR] 
Jan 10 02:03:24 [ERROR] After correcting the problems, you can resume the build with the command
Jan 10 02:03:24 [ERROR]   mvn <goals> -rf :flink-python
{code}",,mapohl,martijnvisser,syhily,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 26 08:27:50 UTC 2023,,,,,,,,,,"0|z1eoww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 10:49;mapohl;[~martijnvisser] may you look into this issue? I saw this before (I guess) but didn't consider it as an issue because it happened right after the pulsar connector was released (at least that was my reasoning). But this time, we should have all the artifacts already in place, shouldn't we?;;;","10/Jan/23 19:00;syhily;Since we have deploy the {{4.0-SNAPSHOT}}, the root cause may be something else?

{code}
Jan 10 02:03:10 [WARNING] Could not transfer metadata org.apache.flink:flink-sql-connector-pulsar:4.0-SNAPSHOT/maven-metadata.xml from/to apache.snapshots (https://repository.apache.org/snapshots): transfer failed for https://repository.apache.org/snapshots/org/apache/flink/flink-sql-connector-pulsar/4.0-SNAPSHOT/maven-metadata.xml, status: 502 Proxy Error
Jan 10 02:03:10 [WARNING] Failure to transfer org.apache.flink:flink-sql-connector-pulsar:4.0-SNAPSHOT/maven-metadata.xml from https://repository.apache.org/snapshots was cached in the local repository, resolution will not be reattempted until the update interval of apache.snapshots has elapsed or updates are forced. Original error: Could not transfer metadata org.apache.flink:flink-sql-connector-pulsar:4.0-SNAPSHOT/maven-metadata.xml from/to apache.snapshots (https://repository.apache.org/snapshots): transfer failed for https://repository.apache.org/snapshots/org/apache/flink/flink-sql-connector-pulsar/4.0-SNAPSHOT/maven-metadata.xml, status: 502 Proxy Error
{code}
;;;","11/Jan/23 09:54;mapohl;good catch, @yufan! We had issues with disk space in this build that manifested itself in the cache not being properly written:
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=eca6b3a6-1600-56cc-916a-c549b3cde3ff&t=ef90e7e5-0acf-4005-9e23-49f8aa9cc762&l=212]
 * [https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44633&view=logs&j=585d8b77-fa33-51bc-8163-03e54ba9ce5b&t=603e646d-7bc1-4fbf-9ad3-20cc53626a82&l=212];;;","11/Jan/23 16:21;mapohl;Ok, after looking into the AzureCI caching functionality and the Maven warning a bit, I'd be inclined to say that they are unrelated. It's rather a problem with the connection. I'll keep this issue open in case it pops up again.;;;","11/Jan/23 20:35;syhily;[~martijnvisser] I found that the Pulsar's {{4.0-SNAPSHOT}} build didn't get updated with the nightly build. Do we have some policy on when to update the externalized connector's snapshot build.;;;","12/Jan/23 14:23;chesnay;We haven't set up snapshot deployments for any externalized connector.;;;","14/Jan/23 00:51;syhily;[~chesnay] I found the document of the externalized connector can be updated to the nightly built of the flink website. So I want to know when we will finalize the snapshot deployment of the externalized connector?;;;","16/Jan/23 09:54;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44870&view=logs&j=52b61abe-a3cc-5bde-cc35-1bbe89bb7df5&t=54421a62-0c80-5aad-3319-094ff69180bb&l=10461;;;","16/Jan/23 09:56;mapohl;I'm wondering why we only observed this issue with the pulsar connector so far. As far as I remember, we also use other connectors in {{{}flink-python{}}}'s test suite.;;;","24/Jan/23 07:50;mapohl;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=45135&view=logs&j=fc5181b0-e452-5c8f-68de-1097947f6483&t=995c650b-6573-581c-9ce6-7ad4cc038461&l=7757;;;","26/Jan/23 08:27;martijnvisser;Fixed in master: 173ddfcb9fd2b15cf3697f5b79406d7d5ed1ad20;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't support batchMessageId when restore from checkpoint,FLINK-30616,13517411,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,songv,songv,10/Jan/23 07:53,10/Jan/23 08:57,13/Jul/23 08:29,10/Jan/23 08:45,1.16.0,pulsar-3.0.0,,,,,,,,,,,,,Connectors / Pulsar,,,,0,,,,"I have a non-partition topic: 
 * the producer for the topic sends batch messages to the topic(to improve the speed of producers)
 * the flink job consumes this topic by Exclusive subscription type

When the flink task manager is restarted for some reason, an exception is thrown when restored from the checkpoint:
{code:java}
java.lang.RuntimeException: One or more fetchers have encountered the exception
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager.checkErrors(SplitFetcherManager.java:225) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.SourceReaderBase.getNextFetch(SourceReaderBase.java:169) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.SourceReaderBase.pollNext(SourceReaderBase.java:130) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.pulsar.source.reader.source.PulsarOrderedSourceReader.pollNext(PulsarOrderedSourceReader.java:106) ~[?:?]
at org.apache.flink.streaming.api.operators.SourceOperator.emitNextNotReading(SourceOperator.java:403) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.api.operators.SourceOperator.emitNext(SourceOperator.java:387) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.io.StreamTaskSourceInput.emitNext(StreamTaskSourceInput.java:68) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.io.StreamOneInputProcessor.processInput(StreamOneInputProcessor.java:65) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.StreamTask.processInput(StreamTask.java:542) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.mailbox.MailboxProcessor.runMailboxLoop(MailboxProcessor.java:231) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.StreamTask.runMailboxLoop(StreamTask.java:831) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:780) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:914) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550) ~[flink-dist-1.16.0.jar:1.16.0]
at java.lang.Thread.run(Unknown Source) ~[?:?]
Caused by: java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:150) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-files-1.16.0.jar:1.16.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
... 1 more
Caused by: java.lang.IllegalArgumentException: We only support normal message id currently. This batch size is %d [83]
at org.apache.flink.util.Preconditions.checkArgument(Preconditions.java:160) ~[flink-dist-1.16.0.jar:1.16.0]
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.unwrapMessageId(MessageIdUtils.java:65) ~[?:?]
at org.apache.flink.connector.pulsar.source.enumerator.cursor.MessageIdUtils.nextMessageId(MessageIdUtils.java:44) ~[?:?]
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.beforeCreatingConsumer(PulsarOrderedPartitionSplitReader.java:92) ~[?:?]
at org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase.handleSplitsChanges(PulsarPartitionSplitReaderBase.java:171) ~[?:?]
at org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader.handleSplitsChanges(PulsarOrderedPartitionSplitReader.java:51) ~[?:?]
at org.apache.flink.connector.base.source.reader.fetcher.AddSplitsTask.run(AddSplitsTask.java:51) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.runOnce(SplitFetcher.java:142) ~[flink-connector-files-1.16.0.jar:1.16.0]
at org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher.run(SplitFetcher.java:105) ~[flink-connector-files-1.16.0.jar:1.16.0]
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[?:?]
at java.util.concurrent.FutureTask.run(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[?:?]
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[?:?]
... 1 more{code}
some important logs in the task manager:
{code:java}
2023-01-09 14:51:01,645 DEBUG org.apache.flink.streaming.api.operators.BackendRestorerProcedure [] - Creating operator state backend for SourceOperator_cbc357ccb763df2852fee8c4fc7d55f2_(1/1) and restoring with state from alternative (1/1).2023-01-09 14:51:01,664 INFO  org.apache.flink.connector.base.source.reader.SourceReaderBase [] - Adding split(s) to reader: [PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]2023-01-09 14:51:01,740 INFO  org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Starting split fetcher 02023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Prepare to run AddSplitsTask: [[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Enqueued task AddSplitsTask: [[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Cleaned wakeup flag.2023-01-09 14:51:01,741 DEBUG org.apache.flink.connector.base.source.reader.fetcher.SplitFetcher [] - Prepare to run AddSplitsTask: [[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,742 DEBUG org.apache.flink.connector.pulsar.source.reader.split.PulsarPartitionSplitReaderBase [] - Handle split changes SplitAddition:[[PulsarPartitionSplit{partition=persistent://ethereum-prod/raw/transactions|[0-65535]}]]2023-01-09 14:51:01,742 INFO  org.apache.flink.connector.pulsar.source.reader.split.PulsarOrderedPartitionSplitReader [] - Reset subscription position by the checkpoint 25551:17912:-1:82023-01-09 14:51:01,743 ERROR org.apache.flink.connector.base.source.reader.fetcher.SplitFetcherManager [] - Received uncaught exception.java.lang.RuntimeException: SplitFetcher thread 0 received unexpected exception while polling the records {code}
I don't know if it is a feature or a bug, but this means that we can't restore from a batch message id checkpoint. I would like to know what to do better. [~Tison] 

 ","flink version: 1.16.0

flink-connector-pulsar version: 1.16.0",songv,syhily,tison,,,,,,,,,,,,,,,,,,,FLINK-30552,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 08:57:17 UTC 2023,,,,,,,,,,"0|z1eors:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 08:35;syhily;Thanks for submitting this issue. This is a known issue and has been resolved in https://github.com/apache/flink-connector-pulsar/pull/11. We just need to backport to {{pulsar-3.0}} branch. Can you close this issue for duplicated?;;;","10/Jan/23 08:39;tison;[~songv] According to [~syhily]'s comment above, you may try out the master branch code on https://github.com/apache/flink-connector-pulsar to see if it solves your case. Also, I'd like to know whether the next feature release is also acceptable for you.;;;","10/Jan/23 08:44;songv;[~tison] [~syhily] Thanks, I need it to restore tasks in the prod env, I will try to backport the [https://github.com/apache/flink-connector-pulsar/pull/11] to pulsar-3.0 and build a package.;;;","10/Jan/23 08:53;songv;I found there are many conflicts while cherry-pick [https://github.com/apache/flink-connector-pulsar/pull/11]
When could we release a fix version for pulsar-3.0-1.16? [~syhily] ;;;","10/Jan/23 08:56;syhily;[~songv] The [backport PR|https://github.com/apache/flink-connector-pulsar/pull/16] has been submitted. You can check it for local building.;;;","10/Jan/23 08:57;songv;nice job!!!! txs [~syhily] ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expire snapshot should be reentrant,FLINK-30611,13517369,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,TsReaper,lzljs3620320,lzljs3620320,10/Jan/23 03:22,12/Jan/23 12:49,13/Jul/23 08:29,10/Jan/23 10:40,,,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,Table Store,,,,0,pull-request-available,,,"At present, if the file is incomplete, expire will throw an exception.
However, the snapshot in expire may be incomplete. It can be interrupted and killed suddenly.
Therefore, we should ensure the safety of expire, make it reentrant, and avoid throwing exceptions.",,lzljs3620320,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 10:40:18 UTC 2023,,,,,,,,,,"0|z1eokg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 10:40;TsReaper;master: 929a411f29f4fc76dfead6e716daae56c165724c
release-0.3: 1f0f28c3cf9c680ba9294362f3a4ba128236f8ab;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table.to_pandas doesn't support Map type,FLINK-30607,13517248,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,dianfu,xuannan,xuannan,09/Jan/23 14:18,29/Mar/23 04:15,13/Jul/23 08:29,11/Jan/23 02:47,1.15.3,,,,,,1.17.0,,,,,,,,API / Python,,,,0,pull-request-available,,,"It seems that the Table#to_pandas method in PyFlink doesn't support Map type. It throws the following exception.
{code:java}
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame.
: java.lang.UnsupportedOperationException: Python vectorized UDF doesn't support logical type MAP<INT, INT> currently.
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:743)
    at org.apache.flink.table.runtime.arrow.ArrowUtils$LogicalTypeToArrowTypeConverter.defaultMethod(ArrowUtils.java:617)
    at org.apache.flink.table.types.logical.utils.LogicalTypeDefaultVisitor.visit(LogicalTypeDefaultVisitor.java:167)
    at org.apache.flink.table.types.logical.MapType.accept(MapType.java:115)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowField(ArrowUtils.java:189)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.lambda$toArrowSchema$0(ArrowUtils.java:180)
    at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)
    at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1384)
    at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)
    at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)
    at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)
    at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)
    at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.toArrowSchema(ArrowUtils.java:181)
    at org.apache.flink.table.runtime.arrow.ArrowUtils.collectAsPandasDataFrame(ArrowUtils.java:483)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
    at org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
    at org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)
    at org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
    at org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)
    at org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)
    at java.lang.Thread.run(Thread.java:748) {code}
This can be reproduced with the following code.
{code:java}
env = StreamExecutionEnvironment.get_execution_environment()
t_env = StreamTableEnvironment.create(env)
table = t_env.from_descriptor(
    TableDescriptor.for_connector(""datagen"")
    .schema(
        Schema.new_builder()
        .column(""val"", DataTypes.MAP(DataTypes.INT(), DataTypes.INT()))
        .build()
    )
    .option(""number-of-rows"", ""10"")
    .build()
)
df = table.to_pandas()
print(df) {code}",,dianfu,xuannan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 04:15:58 UTC 2023,,,,,,,,,,"0|z1enu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"11/Jan/23 02:47;dianfu;Merged to master via b781a13dd615e8d131defe37ca9e550416c10595 ~ afc8fb08ab0879537814d3c77372268eb6d6a4de;;;","29/Mar/23 04:15;xuannan;[~dianfu] Thanks for the patch! This is very useful in our use case. May I ask if we have a plan to backport this to version 1.16?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'Streaming File Sink end-to-end test'  fails with UnsupportedOperationException,FLINK-30605,13517237,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Blocker,Fixed,,mapohl,mapohl,09/Jan/23 12:16,09/Jan/23 13:01,13/Jul/23 08:29,09/Jan/23 12:36,1.17.0,,,,,,,,,,,,,,API / DataStream,,,,0,test-stability,,,"We have a test failure in {{Streaming File Sink end-to-end test}} being caused by an {{UnsupportedOperationException}} ({{{}StreamingFileSink{}}} is not supported as a type):
{code:java}
org.apache.flink.client.program.ProgramInvocationException: The main method caused an error: Unsupported sink type: StreamingFileSink
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:372)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:222)
	at org.apache.flink.client.ClientUtils.executeProgram(ClientUtils.java:98)
	at org.apache.flink.client.cli.CliFrontend.executeProgram(CliFrontend.java:851)
	at org.apache.flink.client.cli.CliFrontend.run(CliFrontend.java:245)
	at org.apache.flink.client.cli.CliFrontend.parseAndRun(CliFrontend.java:1095)
	at org.apache.flink.client.cli.CliFrontend.lambda$mainInternal$9(CliFrontend.java:1189)
	at org.apache.flink.runtime.security.contexts.NoOpSecurityContext.runSecured(NoOpSecurityContext.java:28)
	at org.apache.flink.client.cli.CliFrontend.mainInternal(CliFrontend.java:1189)
	at org.apache.flink.client.cli.CliFrontend.main(CliFrontend.java:1157)
Caused by: java.lang.UnsupportedOperationException: Unsupported sink type: StreamingFileSink
	at org.apache.flink.connector.file.sink.FileSinkProgram.main(FileSinkProgram.java:88)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:355)
	... 9 more {code}
https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=44597&view=logs&j=af184cdd-c6d8-5084-0b69-7e9c67b35f7a&t=160c9ae5-96fd-516e-1c91-deb81f59292a&l=2154",,mapohl,martijnvisser,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-30166,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 09 13:01:54 UTC 2023,,,,,,,,,,"0|z1enrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 12:36;mapohl;The changes of FLINK-30166 were reverted on {{{}master{}}}.

master: 974f884021ea0587ffa029b997a76487e9911a36;;;","09/Jan/23 13:01;martijnvisser;Figured it out: the S3 tests don't run on CI (only after merging) and I didn't change them correctly;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CompactActionITCase in table store is unstable,FLINK-30603,13517189,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,zjureel,zjureel,09/Jan/23 06:24,29/Mar/23 01:59,13/Jul/23 08:29,29/Mar/23 01:59,table-store-0.4.0,,,,,,table-store-0.3.1,table-store-0.4.0,,,,,,,Table Store,,,,0,pull-request-available,,,"https://github.com/apache/flink-table-store/actions/runs/3927960511/jobs/6715071149

Error:  Failures: 
Error:    CompactActionITCase.testStreamingCompact:193 expected:<[+I 1|100|15|20221208, +I 1|100|15|20221209]> but was:<[+I 1|100|15|20221208]>
[INFO] 
Error:  Tests run: 221, Failures: 1, Errors: 0, Skipped: 4",,lzljs3620320,TsReaper,zjureel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 16 07:44:50 UTC 2023,,,,,,,,,,"0|z1engw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"09/Jan/23 07:32;lzljs3620320;[~TsReaper] CC;;;","10/Jan/23 10:39;TsReaper;master: b7188bcc46989c66e44f1fb04cd45972e1a6fe50
release-0.3: ad164c32fca33c365192b2025eaea61980d961eb;;;","16/Jan/23 07:44;zjureel;This test case is still unstable cc [~lzljs3620320]   [~TsReaper];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong code generated for WatermarkGenerator because of inconsistent source type info when deserialized from exec plan,FLINK-30598,13517158,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lincoln.86xy,lincoln.86xy,lincoln.86xy,08/Jan/23 14:21,06/Jul/23 09:45,13/Jul/23 08:29,06/Jul/23 09:45,1.16.0,,,,,,1.18.0,,,,,,,,Table SQL / Planner,,,,0,pull-request-available,,,"When compile from an exist exec plan which contains watermark declaration and it referred the metadata column, the generated code for WatermarkGenerator maybe wrong

because currently `DynamicTableSourceSpec`.getTableSource passes the user defined schema to `SourceAbilitySpec` to perform optimization like projection/watermark pushdown, while optimization path from sql use a fixed reorder form: ""PHYSICAL COLUMNS + METADATA COLUMNS"", this may cause the problem.

a repro-case:

{code}
@Test
    public void testWatermarkPushDownWithMetadata() throws Exception {
        // to verify FLINK-: the case declares metadata field first, without fix it will get a
        // wrong code generated by WatermarkGeneratorCodeGenerator which reference the incorrect
        // varchar column as the watermark field.
        createTestValuesSourceTable(
                ""MyTable"",
                JavaScalaConversionUtil.toJava(TestData.data3WithTimestamp()),
                new String[] {
                    ""ts timestamp(3) metadata"",
                    ""a int"",
                    ""b bigint"",
                    ""c varchar"",
                    ""watermark for ts as ts - interval '5' second""
                },
                new HashMap<String, String>() {
                    {
                        put(""enable-watermark-push-down"", ""true"");
                        put(""readable-metadata"", ""ts:timestamp(3)"");
                    }
                });

        File sinkPath =
                createTestCsvSinkTable(
                        ""MySink"", ""a int"", ""b bigint"", ""c varchar"", ""ts timestamp(3)"");

        compileSqlAndExecutePlan(""insert into MySink select a, b, c, ts from MyTable where b = 3"")
                .await();

        assertResult(
                Arrays.asList(
                        ""4,3,Hello world, how are you?,"" + toLocalDateTime(4000L),
                        ""5,3,I am fine.,"" + toLocalDateTime(5000L),
                        ""6,3,Luke Skywalker,"" + toLocalDateTime(6000L)),
                sinkPath);
    }
{code}

the wrong code snippet(`row.getString(3)` should be a TimestampData):
{code}
public Long currentWatermark(org.apache.flink.table.data.RowData row) throws Exception {
          
          org.apache.flink.table.data.binary.BinaryStringData field$19;
          boolean isNull$19;
          org.apache.flink.table.data.binary.BinaryStringData field$21;
          boolean isNull$22;
          org.apache.flink.table.data.TimestampData result$23;
          boolean isNull$24;
          org.apache.flink.table.data.TimestampData result$25;
          
          isNull$19 = row.isNullAt(3);
          field$19 = org.apache.flink.table.data.binary.BinaryStringData.EMPTY_UTF8;
          if (!isNull$19) {
            field$19 = ((org.apache.flink.table.data.binary.BinaryStringData) row.getString(3));
          }
{code}

 ",,lincoln.86xy,qingyue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 06 09:45:40 UTC 2023,,,,,,,,,,"0|z1ena0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"10/Jan/23 01:38;lincoln.86xy;This issue has several dependencies to be resolved, pending for a while.;;;","06/Jul/23 09:45;lincoln.86xy;fixed in master: d946eaac160748f39567818a221f25499fad84df;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Multiple POST /jars/:jarid/run requests with the same jobId, runs duplicate jobs",FLINK-30596,13517089,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,morezaei00,morezaei00,morezaei00,06/Jan/23 21:57,03/Jul/23 09:31,13/Jul/23 08:29,03/Jul/23 09:31,1.16.1,1.17.0,1.18.0,,,,1.16.3,1.17.2,1.18.0,,,,,,Runtime / REST,,,,0,pull-request-available,,,"Analysis from [~trohrmann]:

{quote}
The problem is the following: When submitting a job, then the {{Dispatcher}} will wait for the termination of a previous {{JobMaster}}. This is done to enable the proper cleanup of the job resources. In the initial submission case, there is no previous {{JobMaster}} with the same {{jobId}}. The problem is now that Flink schedules the [{{persistAndRunJob}}|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L571] action, which runs the newly submitted job, as [an asynchronous task|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L1312-L1318]. This is done to ensure that the action is run on the {{Dispatcher}}'s main thread since the termination future can be run on a different thread. Due to this behaviour, there can be other tasks enqueued in the {{Dispatcher}}'s work queue which are executed before. Such a task could be another job submission which wouldn't see that there is already a job submitted with the same {{jobId}} since [we only do this in {{runJob}}|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L602] which is called by {{persistAndRunJob}}. This is the reason why you don't see a duplicate job submission exception for the second job submission. Even worse, this will eventually [lead to an invalid state|https://github.com/apache/flink/blob/5f924bc84227a3a6c67b44e82c45fe444393f577/flink-runtime/src/main/java/org/apache/flink/runtime/dispatcher/Dispatcher.java#L611-L615] and fail the whole cluster entrypoint.
{quote}

The following fix to the {{Dispatcher}} seems to fix the issue, but before submitting a PR, I wanted to post this for possible follow up discussions:

{code:language=java}
private CompletableFuture<Void> waitForTerminatingJob(
            JobID jobId, JobGraph jobGraph, ThrowingConsumer<JobGraph, ?> action) {
        ...
        return FutureUtils.thenAcceptAsyncIfNotDone(
                jobManagerTerminationFuture,
                getMainThreadExecutor(),
                FunctionUtils.uncheckedConsumer(
                    (ignored) -> {
                        jobManagerRunnerTerminationFutures.remove(jobId);
                        action.accept(jobGraph);
                    }));
    }
{code}",,huwh,morezaei00,renqs,Weijie Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jun 29 08:39:59 UTC 2023,,,,,,,,,,"0|z1emuw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"02/Feb/23 09:11;Weijie Guo;[~morezaei00] Are you still doing this work? If not, I would like to continue.;;;","02/Feb/23 18:53;morezaei00;Hey [~Weijie Guo], I'd opened a discussion on the dev listing for feedback on the change, but it appears that there are no objections/feedback.

I'll go ahead and create a PR for this today.;;;","08/Feb/23 17:39;morezaei00;I was hoping to shine some light on this issue given that it affects any application using the REST endpoints to run/submit jobs, causing confusion on the actual state of the submitted jobs in a cluster.

I've created a PR against the {{master}} branch, and I'd like to be able to port it back to {{1.16.2}} and {{1.15.4}} to close the loop on the issue for the last three major releases:
https://github.com/apache/flink/pull/21849

Could I get someone from the Runtime team to take a look at the PR?;;;","16/May/23 08:12;renqs;[~morezaei00] Any updates on this issue? Thanks;;;","28/Jun/23 15:45;chesnay;master: b528d9b81c03345c0415490fc41e27968313e5f0
1.17: 91e1314fedafc7a23fc239dcbb7907ec2b32d1bb
1.16: 9b073c97e5e758dcfda40bc064fadd5b52dd4327;;;","29/Jun/23 04:12;morezaei00;I've backported the fix for 1.17 and 1.16. Please see GitHub links.
;;;","29/Jun/23 08:39;chesnay;I've already started the backports and was just waiting for CI, closing the PRs.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Java version because of JDK bug in the operator,FLINK-30594,13517046,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Critical,Fixed,gaborgsomogyi,gaborgsomogyi,gaborgsomogyi,06/Jan/23 15:07,06/Feb/23 12:51,13/Jul/23 08:29,06/Feb/23 12:47,,,,,,,kubernetes-operator-1.4.0,,,,,,,,Kubernetes Operator,,,,1,pull-request-available,,,"The following JDK bug is found during operator usage: https://bugs.openjdk.org/browse/JDK-8221218

This has been resolved in 11.0.18 which should be used in the operator base image.",,gaborgsomogyi,gyfora,xxxinli1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 06 12:47:26 UTC 2023,,,,,,,,,,"0|z1emlc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 15:20;gaborgsomogyi;I've double checked and the latest maven:3.8.6-openjdk-11 image contains still the problematic version.
{code:java}
openjdk version ""11.0.16"" 2022-07-19
OpenJDK Runtime Environment 18.9 (build 11.0.16+8)
OpenJDK 64-Bit Server VM 18.9 (build 11.0.16+8, mixed mode, sharing)
{code}
;;;","06/Jan/23 18:02;gaborgsomogyi;In the meantime I've realized that the following determines the java version:
{code:java}
...
FROM openjdk:11-jre
...
{code}
On the other hand this is the latest available which is the mentioned not yet fixed version.
;;;","19/Jan/23 10:51;gaborgsomogyi;I've just double checked and seems like there is no new java version:

{code:java}
$ docker run -it openjdk:11-jre java -version
openjdk version ""11.0.16"" 2022-07-19
OpenJDK Runtime Environment 18.9 (build 11.0.16+8)
OpenJDK 64-Bit Server VM 18.9 (build 11.0.16+8, mixed mode, sharing)
{code}
;;;","06/Feb/23 04:18;xxxinli1;[~gaborgsomogyi]  The OpenJDK images have announced deprecations.  How about turning the base image into Eclipse or other distributions?

 

[openjdk - Official Image | Docker Hub|https://hub.docker.com/_/openjdk] 

[eclipse-temurin Tags | Docker Hub|https://hub.docker.com/_/eclipse-temurin/tags?page=1&name=11];;;","06/Feb/23 11:49;gaborgsomogyi;[~xxxinli1] it makes sense, opened a PR.;;;","06/Feb/23 12:47;gyfora;merged to main f9024a7fe09492957d3708079bf28227af54f5c6;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix calcCodeGen failed if calc with like condition contains double quotation mark,FLINK-30586,13517006,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,337361684@qq.com,337361684@qq.com,337361684@qq.com,06/Jan/23 08:23,11/Jan/23 08:57,13/Jul/23 08:29,10/Jan/23 06:24,1.16.0,,,,,,1.17.0,,,,,,,,Table SQL / Runtime,,,,0,pull-request-available,,,"If I write a sql like ""SELECT * FROM MyTable WHERE b LIKE '%""%'"" in Flink-1.16 as

'like' condition contains double quotation mark, it will cause code gen failed because wrong code generated by codeGen. 

!code-gen-1.png!

 

!code-gen-2.png!

 ",,337361684@qq.com,godfrey,luoyuxia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jan/23 08:23;337361684@qq.com;code-gen-1.png;https://issues.apache.org/jira/secure/attachment/13054398/code-gen-1.png","06/Jan/23 08:23;337361684@qq.com;code-gen-2.png;https://issues.apache.org/jira/secure/attachment/13054397/code-gen-2.png",,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 10 06:24:30 UTC 2023,,,,,,,,,,"0|z1emcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 08:30;luoyuxia;Similar to FLINK-29651;;;","10/Jan/23 06:24;godfrey;Fixed in 1.17.0: 68b37fb867374df5a201f0b170e35c21266e5d7b

1.16.1: 54518e9e27c8cc17f27b5d9a4de48e71cd817e42;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OpenShift FlinkSessionJob artifact write error on non-default namespaces,FLINK-30577,13516721,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,tagarr,jbusche,jbusche,05/Jan/23 20:45,30/Jan/23 09:06,13/Jul/23 08:29,30/Jan/23 09:06,kubernetes-operator-1.3.0,,,,,,kubernetes-operator-1.4.0,,,,,,,,Kubernetes Operator,,,,0,pull-request-available,,,"[~tagarr] has pointed out an issue with using the /opt/flink/artifacts filesystem on OpenShift in non-default namespaces.  The OpenShift permissions don't allow write to /opt.  
```
org.apache.flink.util.FlinkRuntimeException: Failed to create the dir: /opt/flink/artifacts/jim/basic-session-deployment-only-example/basic-session-job-only-example
```
A few ways to solve the problem are:
1. Remove the comment on line 34 here in [flink-conf.yaml|https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/conf/flink-conf.yaml#L34] and change it to: /tmp/flink/artifacts

2. Append this after line 143 here in [values.yaml|https://github.com/apache/flink-kubernetes-operator/blob/main/helm/flink-kubernetes-operator/values.yaml#L142]:
kubernetes.operator.user.artifacts.base.dir: /tmp/flink/artifacts

3.  Changing it in line 142 of [KubernetesOperatorConfigOptions.java|https://github.com/apache/flink-kubernetes-operator/blob/main/flink-kubernetes-operator/src/main/java/org/apache/flink/kubernetes/operator/config/KubernetesOperatorConfigOptions.java#L142] like this:
.defaultValue(""/tmp/flink/artifacts"") 
and then rebuilding the operator image.


",,gyfora,jbusche,pvary,tagarr,tedhtchang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 30 09:06:59 UTC 2023,,,,,,,,,,"0|z1ekl4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 20:57;gyfora;Or users on open shift could simply configure the users.artifact.base.dir to something other than the default:) ;;;","06/Jan/23 05:30;jbusche;So document that they should edit the configmap and restart the operator? I just tried that and it worked:

 

oc edit cm flink-operator-config -n openshift-operators
{quote}change:

    # kubernetes.operator.user.artifacts.base.dir: /opt/flink/artifacts

to

    kubernetes.operator.user.artifacts.base.dir: /tmp/flink/artifacts
{quote}
 

Restart the operator pod:

oc get pods -n openshift-operators
{quote}NAME                                         READY   STATUS    RESTARTS   AGE

flink-kubernetes-operator-5f5bb584db-t75ts   2/2     Running   0          4m45s
{quote}
oc delete pod flink-kubernetes-operator-5f5bb584db-t75ts -n openshift-operators

 

Then in a non-default namespace the session jobs work:

oc get flinksessionjobs -n jim
{quote}NAME                             JOB STATUS   RECONCILIATION STATUS

basic-session-job-example        RUNNING      DEPLOYED

basic-session-job-example2       RUNNING      DEPLOYED

basic-session-job-only-example   RUNNING      DEPLOYED
{quote};;;","06/Jan/23 08:06;gyfora;I just wanted to point out this possiblity as well, I think if this causes problems in some envs we should simply change the default config value for that.
But we should consider how this will impact existing operator deployments.;;;","16/Jan/23 20:05;gyfora;[~jbusche] are you planning to work on this before the 1.4.0 release?;;;","17/Jan/23 06:03;jbusche;I hadn't heard back from [~tagarr] to see if documentation was enough, or if he wanted to pursue the change.  What's the timeline for 1.4.0?  I'll message him again now... thanks!;;;","18/Jan/23 09:45;gyfora;Target 1.4.0 feature freeze is early February ;;;","19/Jan/23 13:26;tagarr;So the issue is not just with olm, but also with the helm operator. One simple fix would be to add an emptyDir volume mount for /opt/flink/artifacts to the helm charts in flink-operator.yaml. Another workaround on openshift would be to add the anyuid scc onto the operator install namespace i.e. run 

oc adm policy add-scc-to-group anyuid system:serviceaccounts:<NAMESPACE>

 

 ;;;","19/Jan/23 13:26;tagarr;[~gyfora] you can assign the issue to me;;;","24/Jan/23 23:40;jbusche;I've switched to your branch [~tagarr] and then done the following:
 # Created a new docker image using the Dockerfile
 # Installed helm both in default and later in the flink namespaces using your code and the new docker image.
 # Deployed a sessionjob and flinkdep, they looked good.

Next, I cleaned up and created an OLM bundle, deployed it to openshift-operators.
 # Deployed sessionjob and flinkdeps to two different namespaces, they looked good.

Last, I deployed the operator to a specific namespace: jim2
 # Deployed sessionjob and flinkdeps to two different namespaces, they looked good.

In all cases, the operator log looked clean, and had entries like this:
{quote}oc logs -f flink-kubernetes-operator-86b7655f65-pj9x7 |grep artifacts

2023-01-24 23:38:25,401 o.a.f.k.o.a.ArtifactManager    [INFO ][jim2/basic-session-job-only-example] Created dir: /opt/flink/*artifacts*/jim2/basic-session-deployment-only-example/basic-session-job-only-example
{quote};;;","30/Jan/23 09:06;gyfora;merged to main 9441b1145ba05845e15b26b517d9b9f0732c9c1e;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table Store dedicated compact job may skip some records when checkpoint interval is long,FLINK-30573,13516624,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,TsReaper,TsReaper,TsReaper,05/Jan/23 09:53,06/Jan/23 08:27,13/Jul/23 08:29,06/Jan/23 08:27,table-store-0.3.0,table-store-0.4.0,,,,,table-store-0.3.0,table-store-0.4.0,,,,,,,Table Store,,,,0,pull-request-available,,,"Currently the sink for Table Store dedicated compact job only receives records about what buckets are changed, instead of what files are changed. If the writer is kept open, new files of this bucket may be skipped.",,TsReaper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 08:27:52 UTC 2023,,,,,,,,,,"0|z1ejzk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 08:27;TsReaper;master: 8fdbbd4e8c22a82d79509cab0add4a6aa7331672
release-0.3: 1ba8f372a05d64e7370f7dcbb65b77e23079f8ef;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
File Format can not change with data file exists,FLINK-30569,13516587,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,05/Jan/23 06:56,06/Jan/23 01:44,13/Jul/23 08:29,06/Jan/23 01:44,,,,,,,table-store-0.4.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"# Set file format to orc
# Write records.
# Set file format to parquet.
# Write records
# Read -> throw exception...

We should support change file format.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jan 06 01:44:17 UTC 2023,,,,,,,,,,"0|z1ejrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"06/Jan/23 01:44;lzljs3620320;master: 63a27cd7af945839f67afaf6e946bcf617ad18a2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select from a new table with Kafka LogStore crashes with UnknownTopicOrPartitionException,FLINK-30564,13516540,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,,Gerrrr,Gerrrr,04/Jan/23 23:34,29/Mar/23 01:44,13/Jul/23 08:29,29/Mar/23 01:44,,,,,,,,,,,,,,,Table Store,,,,0,,,,"Selecting from newly created table that uses Kafka as a Log Store creates a job that crash-loops with {{UnknownTopicOrPartitionException: This server does not host this topic-partition}} exception. This happens because neither {{CREATE TABLE}} nor {{SELECT FROM}} create the underlying topic. 

Steps to reproduce:
{noformat}
CREATE TABLE word_count (
    word STRING PRIMARY KEY NOT ENFORCED,
    cnt BIGINT
) WITH (
    'connector' = 'table-store',
    'path' = 's3://my-bucket/table-store',
    'log.system' = 'kafka',
    'kafka.bootstrap.servers' = 'broker:9092',
    'kafka.topic' = 'word_count_log',
    'auto-create' = 'true',
    'log.changelog-mode' = 'all',
    'log.consistency' = 'transactional'
);

SELECT * FROM word_count; {noformat}
 

JM logs:
{noformat}
flink          | 2023-01-04 23:27:24,292 ERROR org.apache.flink.runtime.source.coordinator.SourceCoordinatorContext [] - Exception while handling result from async call in SourceCoordinator-Source: word_count[1]. Triggering job failover.
flink          | org.apache.flink.util.FlinkRuntimeException: Failed to list subscribed topic partitions due to
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.KafkaSourceEnumerator.checkPartitionChanges(KafkaSourceEnumerator.java:234) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$null$1(ExecutorNotifier.java:83) ~[flink-dist-1.16.0.jar:1.16.0]
flink          |     at org.apache.flink.util.ThrowableCatchingRunnable.run(ThrowableCatchingRunnable.java:40) [flink-dist-1.16.0.jar:1.16.0]
flink          |     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_352]
flink          |     at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_352]
flink          |     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180) [?:1.8.0_352]
flink          |     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293) [?:1.8.0_352]
flink          |     at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_352]
flink          |     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_352]
flink          |     at java.lang.Thread.run(Thread.java:750) [?:1.8.0_352]
flink          | Caused by: java.lang.RuntimeException: Failed to get metadata for topics [word_count_log].
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.subscriber.KafkaSubscriberUtils.getTopicMetadata(KafkaSubscriberUtils.java:47) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.subscriber.TopicListSubscriber.getSubscribedTopicPartitions(TopicListSubscriber.java:52) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.table.store.shaded.connector.kafka.source.enumerator.KafkaSourceEnumerator.getSubscribedTopicPartitions(KafkaSourceEnumerator.java:219) ~[flink-table-store-dist-0.4-SNAPSHOT.jar:0.4-SNAPSHOT]
flink          |     at org.apache.flink.runtime.source.coordinator.ExecutorNotifier.lambda$notifyReadyAsync$2(ExecutorNotifier.java:80) ~[flink-dist-1.16.0.jar:1.16.0]
flink          |     ... 7 more
flink          | Caused by: java.util.concurrent.ExecutionException: org.apache.flink.table.store.shaded.org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. {noformat}",,Gerrrr,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Mar 29 01:43:59 UTC 2023,,,,,,,,,,"0|z1ejgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"29/Mar/23 01:43;lzljs3620320;https://github.com/apache/incubator-paimon/issues/732;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ChangelogStreamHandleReaderWithCache cause FileNotFoundException,FLINK-30561,13516470,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,Feifan Wang,Feifan Wang,Feifan Wang,04/Jan/23 12:49,10/Feb/23 12:21,13/Jul/23 08:29,10/Feb/23 12:21,1.16.0,1.17.0,,,,,1.16.2,1.17.0,,,,,,,Runtime / State Backends,,,,0,pull-request-available,,,"When a job with state changelog enabled continues to restart, the following exceptions may occur :
{code:java}
java.lang.RuntimeException: java.io.FileNotFoundException: /data1/hadoop/yarn/nm-local-dir/usercache/hadoop-rt/appcache/application_1671689962742_1333392/dstl-cache-file/dstl6215344559415829831.tmp (No such file or directory)
    at org.apache.flink.util.ExceptionUtils.rethrow(ExceptionUtils.java:321)
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:87)
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.hasNext(StateChangelogHandleStreamHandleReader.java:69)
    at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.readBackendHandle(ChangelogBackendRestoreOperation.java:107)
    at org.apache.flink.state.changelog.restore.ChangelogBackendRestoreOperation.restore(ChangelogBackendRestoreOperation.java:78)
    at org.apache.flink.state.changelog.ChangelogStateBackend.restore(ChangelogStateBackend.java:94)
    at org.apache.flink.state.changelog.AbstractChangelogStateBackend.createKeyedStateBackend(AbstractChangelogStateBackend.java:136)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.lambda$keyedStatedBackend$1(StreamTaskStateInitializerImpl.java:336)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.attemptCreateAndRestore(BackendRestorerProcedure.java:168)
    at org.apache.flink.streaming.api.operators.BackendRestorerProcedure.createAndRestore(BackendRestorerProcedure.java:135)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.keyedStatedBackend(StreamTaskStateInitializerImpl.java:353)
    at org.apache.flink.streaming.api.operators.StreamTaskStateInitializerImpl.streamOperatorStateContext(StreamTaskStateInitializerImpl.java:165)
    at org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:265)
    at org.apache.flink.streaming.runtime.tasks.RegularOperatorChain.initializeStateAndOpenOperators(RegularOperatorChain.java:106)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreGates(StreamTask.java:726)
    at org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$1.call(StreamTaskActionExecutor.java:55)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restoreInternal(StreamTask.java:702)
    at org.apache.flink.streaming.runtime.tasks.StreamTask.restore(StreamTask.java:669)
    at org.apache.flink.runtime.taskmanager.Task.runWithSystemExitMonitoring(Task.java:935)
    at org.apache.flink.runtime.taskmanager.Task.restoreAndInvoke(Task.java:904)
    at org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:728)
    at org.apache.flink.runtime.taskmanager.Task.run(Task.java:550)
    at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.FileNotFoundException: /data1/hadoop/yarn/nm-local-dir/usercache/hadoop-rt/appcache/application_1671689962742_1333392/dstl-cache-file/dstl6215344559415829831.tmp (No such file or directory)
    at java.io.FileInputStream.open0(Native Method)
    at java.io.FileInputStream.open(FileInputStream.java:195)
    at java.io.FileInputStream.&lt;init&gt;(FileInputStream.java:138)
    at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:158)
    at org.apache.flink.changelog.fs.ChangelogStreamHandleReaderWithCache.openAndSeek(ChangelogStreamHandleReaderWithCache.java:95)
    at org.apache.flink.changelog.fs.StateChangeIteratorImpl.read(StateChangeIteratorImpl.java:42)
    at org.apache.flink.runtime.state.changelog.StateChangelogHandleStreamHandleReader$1.advance(StateChangelogHandleStreamHandleReader.java:85)
    ... 21 more {code}
*Problem causes：*
 # *_ChangelogStreamHandleReaderWithCache_* use RefCountedFile manager local cache file. The reference count is incremented when the input stream is opened from the cache file, and decremented by one when the input stream is closed. So the input stream must be closed and only once.
 # _*StateChangelogHandleStreamHandleReader#getChanges()*_ may cause the input stream to be closed twice. This happens when changeIterator.read(tuple2.f0, tuple2.f1) throws an exception (for example, when the task is canceled for other reasons during the restore process) the current state change iterator will be closed twice.

{code:java}
private void advance() {
    while (!current.hasNext() && handleIterator.hasNext()) {
        try {
            current.close();
            Tuple2<StreamStateHandle, Long> tuple2 = handleIterator.next();
            LOG.debug(""read at {} from {}"", tuple2.f1, tuple2.f0);
            current = changeIterator.read(tuple2.f0, tuple2.f1);
        } catch (Exception e) {
            ExceptionUtils.rethrow(e);
        }
    }
}

@Override
public void close() throws Exception {
    current.close();
}{code}
So we should make sure current state change iterator only be closed once. I suggest to make the following changes to _*StateChangelogHandleStreamHandleReader*_ :
{code:java}
private boolean currentClosed = false;

private void advance() {
    while (!current.hasNext() && handleIterator.hasNext()) {
        try {
            current.close();
            currentClosed = true;

            Tuple2<StreamStateHandle, Long> tuple2 = handleIterator.next();
            LOG.debug(""read at {} from {}"", tuple2.f1, tuple2.f0);
            current = changeIterator.read(tuple2.f0, tuple2.f1);
            currentClosed = false;
        } catch (Exception e) {
            ExceptionUtils.rethrow(e);
        }
    }
}

@Override
public void close() throws Exception {
    if (!currentClosed) {
        current.close();
    }
}{code}
 

cc [~yuanmei] , [~roman] .",,Feifan Wang,masteryhx,ram_krish,roman,Weijie Guo,Yanfei Lei,,,,,,,,,,,,,,,,,,,,,,,,,,,,FLINK-28440,,FLINK-28898,FLINK-30107,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Feb 10 12:21:16 UTC 2023,,,,,,,,,,"0|z1ej1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 05:39;masteryhx;It seems FLINK-30107 and FLINK-28898 are caused by this ?;;;","05/Jan/23 07:54;Feifan Wang;Hi [~masteryhx] ,  FLINK-30107 and FLINK-28898 are caused by origin changelog file not found , the problem described by this ticket is local cached file not found.;;;","06/Jan/23 04:23;Yanfei Lei;Hi [~Feifan Wang], could you please share what circumstances the error occurs? 

> This happens when changeIterator.read(tuple2.f0, tuple2.f1) throws an exception (for example, when the task is canceled for other reasons during the restore process) 

IIUC, when the task is canceled for other reason, the whole job is canceled, for the next restart, the `FileNotFoundException` will not affect the next run, and the refCount would be reset in the next run. Will the previous refCount still be used after the job is canceled?;;;","06/Jan/23 11:43;Feifan Wang;Hi [~Yanfei Lei] , ChangelogStreamHandleReaderWithCache live across job attempts for providing higher cache hit radio, the previous refCount still used after job restarted.;;;","09/Jan/23 04:41;Yanfei Lei;[~Feifan Wang] Thanks for the clarification. I think this makes sense in case the TM process doesn't restart when the job failover.

 ;;;","16/Jan/23 08:26;Feifan Wang;Hi [~ym]  , I submitted a pr to fix this problem, can you help me review it ?;;;","26/Jan/23 01:19;Feifan Wang;Hi [~roman]  , I submitted a pr to fix this problem, can you help me review it ?
 ;;;","10/Feb/23 12:21;roman;Thanks for the fix [~Feifan Wang] !

Merged into master as ba2b55df207fb79ad776eaf64ec8a6c1ab27bac9,

into 1.17 as 0d14c6188252fadc1408034d73f232312c2f683f,

into 1.16 as 526d6f949356d65d685bdd223cc5dd71a3997135.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
May get wrong result for `if` expression if it's string data type,FLINK-30559,13516450,Bug,Resolved,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,luoyuxia,luoyuxia,luoyuxia,04/Jan/23 10:31,06/Jun/23 02:55,13/Jul/23 08:29,06/Jun/23 02:55,,,,,,,,,,,,,,,Table SQL / API,,,,0,pull-request-available,,,"Can be reproduced by the folowing code in `org.apache.flink.table.planner.runtime.batch.sql.CalcITCase`

 
{code:java}
checkResult(""SELECT if(b > 10, 'ua', c) from Table3"", data3) {code}
The actual result is [co, He, He, ...].

Seems it will only get the first two characters.

 ",,aitozi,lincoln.86xy,luoyuxia,martijnvisser,miamiaoxyz,qingyue,yunta,,,,,,,,,,,,,,,FLINK-30966,,,FLINK-31653,FLINK-31007,FLINK-31003,FLINK-30018,FLINK-30966,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon May 22 01:49:20 UTC 2023,,,,,,,,,,"0|z1eiww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 10:37;luoyuxia;I found the following generated code in Cal Node
{code:java}
// --- Start code generated by org.apache.flink.table.planner.codegen.calls.IfCallGen
          isNull$10 = false;
          if (!isNull$10) {
          if (((org.apache.flink.table.data.binary.BinaryStringData) str$6).numChars() > 2) {
          result$11 = ((org.apache.flink.table.data.binary.BinaryStringData) str$6).substring(0, 2);
          } else {
          if (((org.apache.flink.table.data.binary.BinaryStringData) str$6).numChars() < 2) {
          int padLength$12;
          padLength$12 = 2 - ((org.apache.flink.table.data.binary.BinaryStringData) str$6).numChars();
          org.apache.flink.table.data.binary.BinaryStringData padString$13;
          padString$13 = org.apache.flink.table.data.binary.BinaryStringData.blankString(padLength$12);
          result$11 = org.apache.flink.table.data.binary.BinaryStringDataUtil.concat(((org.apache.flink.table.data.binary.BinaryStringData) str$6), padString$13);
          } else {
          result$11 = ((org.apache.flink.table.data.binary.BinaryStringData) str$6);
          }
{code}
I found the code contains the code of substring, it's a bit of weird Seems there's some issue in here.;;;","10/Jan/23 08:47;miamiaoxyz;Hi, I would like to fix it, can i take this issue？
[|https://issues.apache.org/jira/secure/AddComment!default.jspa?id=13486520];;;","11/Jan/23 08:51;luoyuxia;Thanks [~miamiaoxyz]  Please feel free to take it.;;;","19/May/23 08:07;yunta;[~luoyuxia] This bug has been reported by many guys, can we make it resolved?;;;","22/May/23 01:49;luoyuxia;[~yunta] Thanks for reminder. Haven't been awared of that.  I'll push the pr forward. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The metric 'numRestarts' reported in SchedulerBase will be overridden by metric 'fullRestarts',FLINK-30558,13516448,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,xiasun,xiasun,xiasun,04/Jan/23 10:14,05/Jan/23 06:59,13/Jul/23 08:29,05/Jan/23 06:59,1.16.0,1.17.0,,,,,1.16.1,1.17.0,,,,,,,Runtime / Metrics,,,,0,pull-request-available,,,"The method SchedulerBase#registerJobMetrics register metrics 'numRestarts' and 'fullRestarts' with the same metric object, as discussed in FLINK-30246, that will result in the loss of the metric 'numRestarts'.
{code:java}
metrics.gauge(MetricNames.NUM_RESTARTS, numberOfRestarts); 
metrics.gauge(MetricNames.FULL_RESTARTS, numberOfRestarts);{code}
I have verified this problem via rest api /jobs/:jobid/metrics, and the response shows below, we can find that the metric 'numRestarts' is missing.
{noformat}
[{""id"":""numberOfFailedCheckpoints""},{""id"":""cancellingTime""},{""id"":""lastCheckpointSize""},{""id"":""totalNumberOfCheckpoints""},{""id"":""lastCheckpointExternalPath""},{""id"":""lastCheckpointRestoreTimestamp""},{""id"":""failingTime""},{""id"":""runningTime""},{""id"":""uptime""},{""id"":""restartingTime""},{""id"":""initializingTime""},{""id"":""numberOfInProgressCheckpoints""},{""id"":""downtime""},{""id"":""lastCheckpointProcessedData""},{""id"":""numberOfCompletedCheckpoints""},{""id"":""deployingTime""},{""id"":""lastCheckpointFullSize""},{""id"":""fullRestarts""},{""id"":""createdTime""},{""id"":""lastCheckpointDuration""},{""id"":""lastCheckpointPersistedData""}]{noformat}",,xiasun,zhuzh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,Java,Thu Jan 05 06:59:42 UTC 2023,,,,,,,,,,"0|z1eiwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"04/Jan/23 10:15;xiasun;I'd like to work on this issue, Could I be assigned this ticket?;;;","04/Jan/23 10:18;zhuzh;Yes this is a problem, especially considering that {{fullRestarts}} is deprecated and is supposed to be replaced with {{numRestarts}}.
Thanks for reporting this issue and volunteering to fix it. [~xiasun]
I have assigned you the ticket.;;;","05/Jan/23 06:59;zhuzh;master:
b50f42673fcd44bdd6de73f77d646df5c2ad1eed

1.16:
fe5e0c1772fc5392288a606e81b6f1dffe73cb90;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hive cluster can not read oss/s3 tables,FLINK-30555,13516434,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,lzljs3620320,lzljs3620320,lzljs3620320,04/Jan/23 07:36,05/Jan/23 12:08,13/Jul/23 08:29,05/Jan/23 12:08,,,,,,,table-store-0.3.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"FLINK-29964 add oss support for Hive, but only valid in the case of standalone Hive, the distributed Hive compute engine cannot access.
We should add more FileSystems.initialize to Hive connector",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jan 05 12:08:39 UTC 2023,,,,,,,,,,"0|z1eitc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 12:08;lzljs3620320;master: fc00d929b9e98693fbf8439fa50f51021b69a03a
release-0.3: 1574b224df0e88f82f5e374507938f8e38905791;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pulsar connector shouldn't assert the BatchMessageId size.,FLINK-30552,13516319,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,syhily,syhily,syhily,03/Jan/23 12:13,11/Jan/23 07:57,13/Jul/23 08:29,05/Jan/23 02:52,pulsar-4.0.0,,,,,,pulsar-3.0.1,pulsar-4.0.0,,,,,,,Connectors / Pulsar,,,,0,pull-request-available,,,"Pulsar will try to assert the batch message id size in {{MessageIdUtils}}, but the batch size is  determined by the producer in batch mode. So we can promise the size could be 1.

And the next message id calculation should be calculated by using BatchMessageId.",,syhily,tison,,,,,,,,,,,,,,,,,,,,,FLINK-30616,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jan 11 07:57:43 UTC 2023,,,,,,,,,,"0|z1ei3s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"05/Jan/23 02:52;tison;master via https://github.com/apache/flink-connector-pulsar/pull/11;;;","11/Jan/23 07:57;tison;3.0.1 via https://github.com/apache/flink-connector-pulsar/pull/16;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The SchemaManager doesn't check 'NOT NULL' specification when committing AddColumn change,FLINK-30545,13516269,Bug,Closed,FLINK,Flink,software,sewen,"<h3><a name=""Flink""></a>Welcome to the Apache Flink project</h3>
<p>Apache Flink is an open source platform for scalable batch and stream data processing.</p>",https://flink.apache.org,Major,Fixed,yzl,yzl,yzl,03/Jan/23 07:12,04/Jan/23 01:45,13/Jul/23 08:29,03/Jan/23 08:24,,,,,,,table-store-0.3.0,,,,,,,,Table Store,,,,0,pull-request-available,,,"Currently, table store doesn't support adding column with 'NOT NULL' specification, but it doesn't check this condition.",,lzljs3620320,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jan 03 08:24:30 UTC 2023,,,,,,,,,,"0|z1ehso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,"03/Jan/23 08:24;lzljs3620320;master:
a47dc2b483692df2a61d2a567f1c58323e6a4fdb
3b9ce35f6d65eda8c7ee8ffbe924a903f4a91249

release-0.3:
61484f9ec335c97ab92ba624926897bbdc58a8d7
32de0c4d2b3db5ff916000dd0ba55e0570710b5a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
