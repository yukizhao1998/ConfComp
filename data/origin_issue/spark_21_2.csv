Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Affects Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Outward issue link (Blocker),Inward issue link (Cloners),Outward issue link (Completes),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Parent Feature),Inward issue link (Problem/Incident),Inward issue link (Problem/Incident),Outward issue link (Problem/Incident),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Inward issue link (Required),Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Flags),Custom field (Flags),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Language),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Review Date),Custom field (Reviewer),Custom field (Severity),Custom field (Severity),Custom field (Shepherd),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target Version/s),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Make ColumnarToRowExec plan canonicalizable after (de)serialization,SPARK-37779,13419851,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,29/Dec/21 10:15,12/Dec/22 18:10,13/Jul/23 08:50,30/Dec/21 03:38,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"SPARK-23731 fixed the plans to be serializable by leveraging lazy but SPARK-28213 introduced new code path that calls the lazy val which triggers null point exception in https://github.com/apache/spark/blob/77b164aac9764049a4820064421ef82ec0bc14fb/sql/core/src/main/scala/org/apache/spark/sql/execution/Columnar.scala#L68

This can fail during canonicalization during, for example, eliminating sub common expressions:

{code}
java.lang.NullPointerException
    at org.apache.spark.sql.execution.FileSourceScanExec.supportsColumnar$lzycompute(DataSourceScanExec.scala:280)
    at org.apache.spark.sql.execution.FileSourceScanExec.supportsColumnar(DataSourceScanExec.scala:279)
    at org.apache.spark.sql.execution.InputAdapter.supportsColumnar(WholeStageCodegenExec.scala:509)
    at org.apache.spark.sql.execution.ColumnarToRowExec.<init>(Columnar.scala:67)
    ...
    at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized$lzycompute(QueryPlan.scala:581)
    at org.apache.spark.sql.catalyst.plans.QueryPlan.canonicalized(QueryPlan.scala:580)
    at org.apache.spark.sql.execution.ScalarSubquery.canonicalized$lzycompute(subquery.scala:110)
    ...
    at org.apache.spark.sql.catalyst.expressions.ExpressionEquals.hashCode(EquivalentExpressions.scala:275)
    ...
    at scala.collection.mutable.HashTable.findEntry$(HashTable.scala:135)
    at scala.collection.mutable.HashMap.findEntry(HashMap.scala:44)
    at scala.collection.mutable.HashMap.get(HashMap.scala:74)
    at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExpr(EquivalentExpressions.scala:46)
    at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTreeHelper$1(EquivalentExpressions.scala:147)
    at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:170)
    at org.apache.spark.sql.catalyst.expressions.SubExprEvaluationRuntime.$anonfun$proxyExpressions$1(SubExprEvaluationRuntime.scala:89)
    at org.apache.spark.sql.catalyst.expressions.SubExprEvaluationRuntime.$anonfun$proxyExpressions$1$adapted(SubExprEvaluationRuntime.scala:89)
    at scala.collection.immutable.List.foreach(List.scala:392)
{code}",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 30 03:38:57 UTC 2021,,,,,,,,,,"0|z0y3rc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/21 10:25;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/35058;;;","30/Dec/21 03:38;gurwls223;Issue resolved by pull request 35058
[https://github.com/apache/spark/pull/35058];;;",,,,,,,,,,,,,,
Upgrade SBT to 1.6.1,SPARK-37778,13419838,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,29/Dec/21 09:01,29/Dec/21 09:30,13/Jul/23 08:50,29/Dec/21 09:30,3.3.0,,,,,,,,3.3.0,,,,Build,,,,,0,,,,,"SBT 1.6.1 was released, which log4j 2 to 2.17.1 for CVE-2021-44832.
https://github.com/sbt/sbt/releases/tag/v1.6.1",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 29 09:30:10 UTC 2021,,,,,,,,,,"0|z0y3og:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Dec/21 09:07;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/35057;;;","29/Dec/21 09:30;dongjoon;Issue resolved by pull request 35057
[https://github.com/apache/spark/pull/35057];;;",,,,,,,,,,,,,,
Upgrade jackson to 2.13.1,SPARK-37763,13419644,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,28/Dec/21 05:30,28/Dec/21 07:29,13/Jul/23 08:50,28/Dec/21 07:29,3.3.0,,,,,,,,3.3.0,,,,Build,,,,,0,,,,,,,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 28 07:29:04 UTC 2021,,,,,,,,,,"0|z0y2hc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Dec/21 05:32;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35037;;;","28/Dec/21 07:29;dongjoon;Issue resolved by pull request 35037
[https://github.com/apache/spark/pull/35037];;;",,,,,,,,,,,,,,
Fix black version in dev/reformat-python,SPARK-37754,13419586,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,chia7712,chia7712,chia7712,27/Dec/21 19:41,28/Dec/21 09:55,13/Jul/23 08:50,28/Dec/21 09:55,3.3.0,,,,,,,,3.3.0,,,,Build,,,,,0,,,,,"SPARK-37737 updated the black from 21.5b2 to 21.12b0 and applies dev/reformat-python. However, reformat-python still recommend to install black:21.5b2",,apachespark,chia7712,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 28 09:55:52 UTC 2021,,,,,,,,,,"0|z0y24g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Dec/21 19:47;apachespark;User 'chia7712' has created a pull request for this issue:
https://github.com/apache/spark/pull/35033;;;","27/Dec/21 19:47;apachespark;User 'chia7712' has created a pull request for this issue:
https://github.com/apache/spark/pull/35033;;;","28/Dec/21 09:55;zero323;Issue resolved by pull request 35033
[https://github.com/apache/spark/pull/35033];;;",,,,,,,,,,,,,
plot.hist throws AttributeError on pandas=1.3.5,SPARK-37730,13419209,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mslapek,mslapek,mslapek,23/Dec/21 12:13,12/Dec/22 18:10,13/Jul/23 08:50,28/Dec/21 01:40,3.2.0,3.3.0,,,,,,,3.2.2,3.3.0,,,PySpark,,,,,0,,,,,"plot.hist from PySpark throws AttributeError exception when pyspark.pandas is used with pandas=1.3.5.

Pandas in commit [https://github.com/pandas-dev/pandas/commit/029907c9d69a0260401b78a016a6c4515d8f1c40]
replaced MPLPlot._add_legend_handle with MPLPlot._append_legend_handles_labels.

I've attached PR on github which replaces use of MPLPlot._add_legend_handle in PySpark with MPLPlot._append_legend_handles_labels.

 

Code:

 
{code:java}
import pyspark.pandas as ps
from matplotlib import pyplot as plt

ps.set_option(""plotting.backend"", ""matplotlib"")

df = ps.DataFrame({'data': [4, 5, 5, 6, 8, 9]})
df['data'].plot.hist()

plt.show()
 {code}
 

 

Truncated traceback:
{code:java}
Traceback (most recent call last):                                              
  File ""/home/develop/Documents/sparkbug/code.py"", line 6, in <module>
    df['data'].plot.hist()
  ...
  File ""/mnt/transient/develop/miniconda3/envs/testenv/lib/python3.9/site-packages/pyspark/pandas/plot/matplotlib.py"", line 403, in _make_plot
    self._add_legend_handle(artists[0], label, index=i)
AttributeError: 'PandasOnSparkHistPlot' object has no attribute '_add_legend_handle' {code}","Conda environment.yml (also tested with 3.3.0-SNAPSHOT):


{{name: testenv}}
{{channels:}}
{{  - conda-forge}}
{{dependencies:}}
{{  - python=3.9.9}}
{{  }}
{{  - numpy=1.21.5}}
{{  - pandas=1.3.5}}
{{  - matplotlib=3.5.1}}
{{  }}
{{  - pyspark=3.2.0}}

 ",apachespark,dongjoon,mslapek,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,Sun Jul 10 22:57:05 UTC 2022,,,,,,,,,,"0|z0xzsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/21 12:26;apachespark;User 'mslapek' has created a pull request for this issue:
https://github.com/apache/spark/pull/35000;;;","23/Dec/21 12:27;apachespark;User 'mslapek' has created a pull request for this issue:
https://github.com/apache/spark/pull/35000;;;","28/Dec/21 01:40;gurwls223;Issue resolved by pull request 35000
[https://github.com/apache/spark/pull/35000];;;","10/Jul/22 22:26;dongjoon;This is backported to branch-3.2 for Apache Spark 3.2.2 via https://github.com/apache/spark/commit/bc54a3f0c2e08893702c3929bfe7a9d543a08cdb;;;","10/Jul/22 22:56;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37146;;;","10/Jul/22 22:57;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/37146;;;",,,,,,,,,,
reading nested columns with ORC vectorized reader can cause ArrayIndexOutOfBoundsException,SPARK-37728,13419202,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yangyimin,yangyimin,yangyimin,23/Dec/21 11:58,04/Jan/22 17:27,13/Jul/23 08:50,28/Dec/21 06:11,3.2.0,,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"When spark.sql.orc.enableNestedColumnVectorizedReader is set to true, reading nested columns of ORC files can cause ArrayIndexOutOfBoundsException. Here is a simple reproduction:

1) create an ORC file which contains records of type Array<Array<String>>:
{code:java}
./bin/spark-shell {code}
{code:java}
case class Item(record: Array[Array[String]])

val data = new Array[Array[Array[String]]](100)
    for (i <- 0 to 99) {
      val temp = new Array[Array[String]](50)
      for (j <- 0 to 49) {
        temp(j) = new Array[String](1000)
        for (k <- 0 to 999) {
          temp(j)(k) = k.toString
        }
      }
      data(i) = temp
    }
val rdd = spark.sparkContext.parallelize(data, 1)
val df = rdd.map(x => Item(x)).toDF
df.write.orc(""file:///home/user_name/data"") {code}
 

2) read the orc with spark.sql.orc.enableNestedColumnVectorizedReader=true
{code:java}
./bin/spark-shell --conf spark.sql.orc.enableVectorizedReader=true --conf spark.sql.codegen.wholeStage=true --conf spark.sql.orc.enableNestedColumnVectorizedReader=true --conf spark.sql.orc.columnarReaderBatchSize=4096 {code}
{code:java}
val df = spark.read.orc(""file:///home/user_name/data"")
df.show(100) {code}
 

Then Spark threw ArrayIndexOutOfBoundsException:

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2455)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2404)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2403)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2403)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1162)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1162)
  at scala.Option.foreach(Option.scala:407)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1162)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2643)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2585)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2574)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:940)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2227)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2248)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:490)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:443)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3833)
  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2832)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3824)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3822)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2832)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:3053)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:807)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:766)
  ... 47 elided
Caused by: java.lang.ArrayIndexOutOfBoundsException: 4096
  at org.apache.spark.sql.execution.datasources.orc.OrcArrayColumnVector.getArray(OrcArrayColumnVector.java:53)
  at org.apache.spark.sql.vectorized.ColumnarArray.getArray(ColumnarArray.java:170)
  at org.apache.spark.sql.vectorized.ColumnarArray.getArray(ColumnarArray.java:31)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:363)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:136)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:507)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1468)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:510)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
  at java.lang.Thread.run(Thread.java:745)

 ",,apachespark,dongjoon,yangyimin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 28 08:06:28 UTC 2021,,,,,,,,,,"0|z0xzrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/21 13:09;apachespark;User 'yym1995' has created a pull request for this issue:
https://github.com/apache/spark/pull/35002;;;","23/Dec/21 13:10;apachespark;User 'yym1995' has created a pull request for this issue:
https://github.com/apache/spark/pull/35002;;;","28/Dec/21 06:11;dongjoon;This is resolved via https://github.com/apache/spark/pull/35002;;;","28/Dec/21 08:05;apachespark;User 'yym1995' has created a pull request for this issue:
https://github.com/apache/spark/pull/35038;;;","28/Dec/21 08:06;apachespark;User 'yym1995' has created a pull request for this issue:
https://github.com/apache/spark/pull/35038;;;",,,,,,,,,,,
Failed to execute pyspark test in Win WSL,SPARK-37721,13419136,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,23/Dec/21 04:10,12/Dec/22 18:11,13/Jul/23 08:50,23/Dec/21 10:14,3.3.0,,,,,,,,3.3.0,,,,Tests,,,,,0,,,,," 
{code:java}
Launching unittests with arguments python -m unittest test_rdd.RDDTests.test_range in /home/yikun/spark/python/pyspark/testsTraceback (most recent call last):
  File ""/mnt/d/Program Files/JetBrains/PyCharm 2021.1.3/plugins/python/helpers/pycharm/_jb_unittest_runner.py"", line 35, in <module>
    sys.exit(main(argv=args, module=None, testRunner=unittestpy.TeamcityTestRunner, buffer=not JB_DISABLE_BUFFERING))
  File ""/usr/lib/python3.8/unittest/main.py"", line 100, in __init__
    self.parseArgs(argv)
  File ""/usr/lib/python3.8/unittest/main.py"", line 147, in parseArgs
    self.createTests()
  File ""/usr/lib/python3.8/unittest/main.py"", line 158, in createTests
    self.test = self.testLoader.loadTestsFromNames(self.testNames,
  File ""/usr/lib/python3.8/unittest/loader.py"", line 220, in loadTestsFromNames
    suites = [self.loadTestsFromName(name, module) for name in names]
  File ""/usr/lib/python3.8/unittest/loader.py"", line 220, in <listcomp>
    suites = [self.loadTestsFromName(name, module) for name in names]
  File ""/usr/lib/python3.8/unittest/loader.py"", line 154, in loadTestsFromName
    module = __import__(module_name)
  File ""/home/yikun/spark/python/pyspark/tests/test_rdd.py"", line 37, in <module>
    from pyspark.testing.utils import ReusedPySparkTestCase, SPARK_HOME, QuietTest
  File ""/home/yikun/spark/python/pyspark/testing/utils.py"", line 47, in <module>
    SPARK_HOME = os.environ[""SPARK_HOME""]#_find_spark_home()
  File ""/usr/lib/python3.8/os.py"", line 675, in __getitem__
    raise KeyError(key) from None
KeyError: 'SPARK_HOME' {code}
Looks like we  should change ""SPARK_HOME = os.environ[""SPARK_HOME""]"" to ""SPARK_HOME = _find_spark_home()""

 ",,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 23 10:14:53 UTC 2021,,,,,,,,,,"0|z0xzcw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Dec/21 04:20;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34993;;;","23/Dec/21 10:14;gurwls223;Issue resolved by pull request 34993
[https://github.com/apache/spark/pull/34993];;;",,,,,,,,,,,,,,
No namespace assigned in Executor Pod ConfigMap,SPARK-37713,13418975,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dcoliversun,dcoliversun,dcoliversun,22/Dec/21 09:05,29/Jan/22 01:55,13/Jul/23 08:50,31/Dec/21 02:30,3.1.1,3.1.2,3.2.0,,,,,,3.3.0,,,,Kubernetes,,,,,0,,,,,"Since Spark 3.X, Executor pod needs separate executor configmap. But, no namespace is assigned in configmap when building it. K8s views configmap without namespace as global resource. Once pod access is restricted (global resources cannot be read), and executor cannot obtain configmap itself.",,apachespark,dcoliversun,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 29 01:55:01 UTC 2022,,,,,,,,,,"0|z0xydc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/21 09:19;apachespark;User 'dcoliversun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34983;;;","31/Dec/21 02:30;dongjoon;This is resolved via https://github.com/apache/spark/pull/34983;;;","24/Jan/22 08:41;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35299;;;","24/Jan/22 08:42;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35299;;;","29/Jan/22 01:55;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35359;;;",,,,,,,,,,,
Upgrade SBT to 1.5.8,SPARK-37703,13418790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,Jiangliuer,Jiangliuer,21/Dec/21 08:45,21/Dec/21 18:24,13/Jul/23 08:50,21/Dec/21 18:24,3.2.1,3.3.0,,,,,,,3.3.0,,,,Build,,,,,0,,,,,"SBT 1.5.8 was released a few hours ago, which includes a fix for CVE-2021-45105{color:#172b4d}.{color}

https://github.com/sbt/sbt/releases/tag/v1.5.8",,apachespark,dongjoon,Jiangliuer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 21 18:24:38 UTC 2021,,,,,,,,,,"0|z0xx88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Dec/21 10:25;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34971;;;","21/Dec/21 10:26;apachespark;User 'dchvn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34971;;;","21/Dec/21 18:24;dongjoon;Issue resolved by pull request 34971
[https://github.com/apache/spark/pull/34971];;;",,,,,,,,,,,,,
Disallow delete resources in spark sql cli,SPARK-37694,13418549,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,20/Dec/21 09:58,12/Dec/22 18:11,13/Jul/23 08:50,21/Dec/21 00:14,3.0.3,3.1.2,3.2.0,,,,,,3.3.0,,,,SQL,,,,,0,,,,,,,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 21 00:14:48 UTC 2021,,,,,,,,,,"0|z0xvqw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/21 10:07;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34960;;;","20/Dec/21 10:08;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34960;;;","21/Dec/21 00:14;gurwls223;Issue resolved by pull request 34960
[https://github.com/apache/spark/pull/34960];;;",,,,,,,,,,,,,
Fix ChildProcAppHandleSuite failed in spark-master-test-maven-hadoop-3.2,SPARK-37693,13418534,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,LuciferYang,LuciferYang,LuciferYang,20/Dec/21 09:00,20/Dec/21 15:23,13/Jul/23 08:50,20/Dec/21 15:23,3.3.0,,,,,,,,3.3.0,,,,Tests,,,,,0,,,,,https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-3.2/3522/#showFailuresLink,,apachespark,LuciferYang,viirya,,,,,,,,,,,,,,,,,,,,,SPARK-6035,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 20 15:23:39 UTC 2021,,,,,,,,,,"0|z0xvnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/21 09:08;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34958;;;","20/Dec/21 15:23;viirya;Issue resolved by pull request 34958
[https://github.com/apache/spark/pull/34958];;;",,,,,,,,,,,,,,
Incorrect annotations in SeriesGroupBy._cleanup_and_return ,SPARK-37678,13418294,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,17/Dec/21 18:33,20/Dec/21 22:18,13/Jul/23 08:50,20/Dec/21 22:18,3.3.0,,,,,,,,3.2.1,3.3.0,,,PySpark,,,,,0,,,,,"[{{SeriesGroupBy._cleanup_and_return}}|https://github.com/apache/spark/blob/02ee1ae10b938eaa1621c3e878d07c39b9887c2e/python/pyspark/pandas/groupby.py#L2997-L2998] annotations

{code:python}
    def _cleanup_and_return(self, pdf: pd.DataFrame) -> Series:
        return first_series(pdf).rename().rename(self._psser.name)
{code}

are inconsistent:

- If {{pdf}} is {{pd.DataFrame}} then output should be {{pd.Series}}.
- If output is {{ps.Series}} then {{pdf}} should be {{ps.DataFrame}}.

Doesn't seem like the method is used (it is possible that my search skills and PyCharm inspection failed), so I am not sure which of these options was intended.",,apachespark,ueshin,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 20 22:18:13 UTC 2021,,,,,,,,,,"0|z0xu68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/21 18:34;zero323;cc [~ueshin] Do you have any thoughts on this one by any chance?;;;","17/Dec/21 21:25;ueshin;Good catch!
It must be {{{}_cleanup_and_return(self, psdf: DataFrame) -> Series{}}}. (not {{pd.}})

??Doesn't seem like the method is used??

It's an actual implementation of an abstract method {{GroupBy._cleanup_and_return}} for {{{}SeriesGroupBy{}}}.
{{GroupBy._cleanup_and_return}} is called at many places in {{{}GroupBy{}}}.;;;","17/Dec/21 21:34;zero323;??It's an actual implementation of an abstract method {{GroupBy._cleanup_and_return}} for {{{}SeriesGroupBy{}}}.??????

Yeah, my brain must be scrambled at this point.

 

So that's basically {{ps.DataFrame -> ps.Series}}, right?;;;","17/Dec/21 21:39;ueshin;Yes!;;;","18/Dec/21 23:10;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/34950;;;","18/Dec/21 23:11;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/34950;;;","20/Dec/21 22:18;ueshin;Issue resolved by pull request 34950
https://github.com/apache/spark/pull/34950;;;",,,,,,,,,
"spark on k8s, when the user want to push python3.6.6.zip to the pod , but no permission to execute",SPARK-37677,13418226,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zhongjingxiong,zhongjingxiong,zhongjingxiong,17/Dec/21 12:17,28/Mar/23 05:27,13/Jul/23 08:50,28/Mar/23 05:27,3.2.0,,,,,,,,3.5.0,,,,Spark Core,,,,,0,,,,,"In cluster mode, I hava another question that when I unzip python3.6.6.zip in pod , but no permission to execute, my execute operation as follows：


{code:sh}
spark-submit \
--archives ./python3.6.6.zip#python3.6.6 \
--conf ""spark.pyspark.python=python3.6.6/python3.6.6/bin/python3"" \
--conf ""spark.pyspark.driver.python=python3.6.6/python3.6.6/bin/python3"" \
--conf spark.kubernetes.container.image.pullPolicy=Always \
./examples/src/main/python/pi.py 100
{code}

",,apachespark,gurwls223,snoot,ulysses,valux,xleesf,zhongjingxiong,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 28 05:27:28 UTC 2023,,,,,,,,,,"0|z0xtr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/21 12:24;gurwls223;zip compression algorithm by design doesn't keep the permissions of files IIRC. can we fix it?;;;","18/Dec/21 06:33;zhongjingxiong;can we fix it throuth modify file permissions when the file is decompressed?;;;","18/Dec/21 06:35;gurwls223;I know that some implementation of zip's do that via leveraging metadata IIRC. If we can figure out a way to keep the permissions, that would be great.;;;","18/Dec/21 06:41;zhongjingxiong;Let me have a try.;;;","31/Dec/21 12:30;apachespark;User 'zhongjingxiong' has created a pull request for this issue:
https://github.com/apache/spark/pull/35082;;;","31/Dec/21 12:31;apachespark;User 'zhongjingxiong' has created a pull request for this issue:
https://github.com/apache/spark/pull/35082;;;","31/Dec/21 13:51;zhongjingxiong;I found that unzip is implemented by decompressing files through Java's io. When we create an output file object, permission is given by default, so the permission of unzip's output file is consistent with the permission to create the file, which cannot be modified. Therefore, I try to give permission after the output file is created, Or you can directly save the permission information when reading the output file, but it need to modify the zip code here.;;;","22/Jan/22 08:51;apachespark;User 'zhongjingxiong' has created a pull request for this issue:
https://github.com/apache/spark/pull/35278;;;","22/Jan/22 08:53;zhongjingxiong;[~hyukjin.kwon] Hey sir, I brought up another one for this issue. This is the use of shell command is to decompress.;;;","14/Jun/22 06:50;valux;Any conclusions about this issue ? Keeping unsupport for .zip?;;;","03/Jan/23 02:35;zhongjingxiong;At present, I have repaired Hadoop version 3.3.5, but it has not been released yet. In the future, Spark needs to update the Hadoop version to solve this problem.[~valux] ;;;","28/Mar/23 03:25;snoot;User 'smallzhongfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/40572;;;","28/Mar/23 05:27;gurwls223;Issue resolved by pull request 40572
[https://github.com/apache/spark/pull/40572];;;",,,
'Index' object has no attribute 'levels' in  pyspark.pandas.frame.DataFrame.insert,SPARK-37668,13418093,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,itholic,zero323,zero323,16/Dec/21 20:49,12/Dec/22 18:11,13/Jul/23 08:50,23/Dec/21 13:15,3.3.0,,,,,,,,3.3.0,,,,PySpark,,,,,0,,,,," [This piece of code|https://github.com/apache/spark/blob/6e45b04db48008fa033b09df983d3bd1c4f790ea/python/pyspark/pandas/frame.py#L3991-L3993] in {{pyspark.pandas.frame}} is going to fail on runtime, when {{is_name_like_tuple}} evaluates to {{True}}

{code:python}
if is_name_like_tuple(column):
    if len(column) != len(self.columns.levels):
{code}

with 

{code}
'Index' object has no attribute 'levels'
{code}

To be honest, I am not sure what is intended behavior (initially, I suspected that we should have 

{code:python}
 if len(column) != self.columns.nlevels
{code}

but {{nlevels}} is hard-coded to one, and wouldn't be consistent with Pandas at all.",,apachespark,itholic,ueshin,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 23 13:15:27 UTC 2021,,,,,,,,,,"0|z0xsxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/21 20:50;zero323;cc [~hyukjin.kwon], [~itholic], [~ueshin], [~XinrongM].;;;","17/Dec/21 01:14;itholic;Thanks for the report! Let me take a look;;;","20/Dec/21 07:38;itholic;This should be only applied to MultiIndex columns, let me address it;;;","20/Dec/21 08:25;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/34957;;;","20/Dec/21 08:26;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/34957;;;","23/Dec/21 13:15;gurwls223;Issue resolved by pull request 34957
[https://github.com/apache/spark/pull/34957];;;",,,,,,,,,,
"Spark throws TreeNodeException (""Couldn't find gen_alias"") during wildcard column expansion",SPARK-37667,13418092,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kellan.burket,kellan.burket,16/Dec/21 20:48,12/Dec/22 18:11,13/Jul/23 08:50,20/Dec/21 15:42,3.1.2,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"I'm seeing a TreeNodeException (""Couldn't find {_}gen_alias{_}"") when running certain operations in Spark 3.1.2.

A few conditions need to be met to trigger the bug:
 - a DF with a nested struct joins to a second DF
 - a filter that compares a column in the right DF to a column in the left DF
 - wildcard column expansion of the nested struct
 - a group by statement on a struct column

*Data*
git@github.com:kellanburket/spark3bug.git

 
{code:java}
val rightDf = spark.read.parquet(""right.parquet"")
val leftDf = spark.read.parquet(""left.parquet""){code}
 

*Schemas*
{code:java}
leftDf.printSchema()
root
 |-- row: struct (nullable = true)
 |    |-- mid: string (nullable = true)
 |    |-- start: struct (nullable = true)
 |    |    |-- latitude: double (nullable = true)
 |    |    |-- longitude: double (nullable = true)
 |-- s2_cell_id: long (nullable = true){code}
{code:java}
rightDf.printSchema()
root
 |-- id: string (nullable = true)
 |-- s2_cell_id: long (nullable = true){code}
 

*Breaking Code*
{code:java}
leftDf.join(rightDf, ""s2_cell_id"").filter(
    ""id != row.start.latitude""
).select(
   col(""row.*""), col(""id"")
).groupBy(
    ""start""
).agg(
   min(""id"")
).show(){code}
 

*Working Examples*

The following examples don't seem to be effected by the bug

Works without group by:
{code:java}
leftDf.join(rightDf, ""s2_cell_id"").filter(
    ""id != row.start.latitude""
).select(
   col(""row.*""), col(""id"")
).show(){code}
Works without filter
{code:java}
leftDf.join(rightDf, ""s2_cell_id"").select(
   col(""row.*""), col(""id"")
).groupBy(
    ""start""
).agg(
   min(""id"")
).show(){code}
Works without wildcard expansion
{code:java}
leftDf.join(rightDf, ""s2_cell_id"").filter(
    ""id != row.start.latitude""
).select(
   col(""row.start""), col(""id"")
).groupBy(
    ""start""
).agg(
   min(""id"")
).show(){code}
Works with caching
{code:java}
leftDf.join(rightDf, ""s2_cell_id"").filter(
    ""id != row.start.latitude""
).cache().select(
   col(""row.*""),
   col(""id"")
).groupBy(
    ""start""
).agg(
   min(""id"")
).show(){code}
*Error message*

 

 
{code:java}
org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange hashpartitioning(start#2116, 1024), ENSURE_REQUIREMENTS, [id=#3849]
+- SortAggregate(key=[knownfloatingpointnormalized(if (isnull(start#2116)) null else named_struct(latitude, knownfloatingpointnormalized(normalizenanandzero(start#2116.latitude)), longitude, knownfloatingpointnormalized(normalizenanandzero(start#2116.longitude)))) AS start#2116], functions=[partial_min(id#2103)], output=[start#2116, min#2138])
   +- *(2) Sort [knownfloatingpointnormalized(if (isnull(start#2116)) null else named_struct(latitude, knownfloatingpointnormalized(normalizenanandzero(start#2116.latitude)), longitude, knownfloatingpointnormalized(normalizenanandzero(start#2116.longitude)))) AS start#2116 ASC NULLS FIRST], false, 0
      +- *(2) Project [_gen_alias_2133#2133 AS start#2116, id#2103]
         +- *(2) !BroadcastHashJoin [s2_cell_id#2108L], [s2_cell_id#2104L], Inner, BuildLeft, NOT (cast(id#2103 as double) = _gen_alias_2134#2134), false
            :- BroadcastQueryStage 0
            :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, false]),false), [id=#3768]
            :     +- *(1) Project [row#2107.start AS _gen_alias_2133#2133, s2_cell_id#2108L]
            :        +- *(1) Filter isnotnull(s2_cell_id#2108L)
            :           +- FileScan parquet [row#2107,s2_cell_id#2108L] Batched: false, DataFilters: [isnotnull(s2_cell_id#2108L)], Format: Parquet, Location: InMemoryFileIndex[s3://co.mira.public/spark3_bug/left], PartitionFilters: [], PushedFilters: [IsNotNull(s2_cell_id)], ReadSchema: struct<row:struct<start:struct<latitude:double,longitude:double>>,s2_cell_id:bigint>
            +- *(2) Filter (isnotnull(id#2103) AND isnotnull(s2_cell_id#2104L))
               +- *(2) ColumnarToRow
                  +- FileScan parquet [id#2103,s2_cell_id#2104L] Batched: true, DataFilters: [isnotnull(id#2103), isnotnull(s2_cell_id#2104L)], Format: Parquet, Location: InMemoryFileIndex[s3://co.mira.public/spark3_bug/right], PartitionFilters: [], PushedFilters: [IsNotNull(id), IsNotNull(s2_cell_id)], ReadSchema: struct<id:string,s2_cell_id:bigint>
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$1(ShuffleExchangeExec.scala:101)
  at org.apache.spark.sql.util.LazyValue.getOrInit(LazyValue.scala:41)
  at org.apache.spark.sql.execution.exchange.Exchange.getOrInitMaterializeFuture(Exchange.scala:71)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materializeFuture(ShuffleExchangeExec.scala:97)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize(ShuffleExchangeExec.scala:85)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.materialize$(ShuffleExchangeExec.scala:84)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.materialize(ShuffleExchangeExec.scala:129)
  at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:161)
  at org.apache.spark.sql.execution.adaptive.QueryStageExec.$anonfun$materialize$1(QueryStageExec.scala:74)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
  at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:74)
  at org.apache.spark.sql.execution.adaptive.MaterializeExecutable.tryStart(AdaptiveExecutable.scala:396)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.startChild(AdaptiveExecutor.scala:225)
  at org.apache.spark.sql.execution.adaptive.ExecutionHelper.start(ExecutionHelper.scala:47)
  at org.apache.spark.sql.execution.adaptive.QueryStageExecutable$$anon$2.$anonfun$new$1(AdaptiveExecutable.scala:251)
  at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2(ExecutionHelper.scala:55)
  at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$2$adapted(ExecutionHelper.scala:54)
  at scala.Option.foreach(Option.scala:407)
  at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1(ExecutionHelper.scala:54)
  at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.$anonfun$onChildSuccess$1$adapted(ExecutionHelper.scala:53)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.sql.execution.adaptive.ExecutionHelper$Listener.onChildSuccess(ExecutionHelper.scala:53)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2(AdaptiveExecutor.scala:314)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.$anonfun$onActiveChildSuccess$2$adapted(AdaptiveExecutor.scala:314)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onActiveChildSuccess(AdaptiveExecutor.scala:314)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutorRuntime.onChildSuccess(AdaptiveExecutor.scala:284)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1(AdaptiveExecutor.scala:92)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.$anonfun$doRun$1$adapted(AdaptiveExecutor.scala:91)
  at scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)
  at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
  at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
  at scala.collection.mutable.HashMap.foreach(HashMap.scala:149)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:91)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)
  at org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:184)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:183)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:405)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3760)
  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2763)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3751)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  at org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:110)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:135)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)
  at org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:232)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:135)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:253)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:134)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3749)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2763)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2970)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:303)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:340)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:866)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:825)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:834)
  ... 74 elided
Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
SortAggregate(key=[knownfloatingpointnormalized(if (isnull(start#2116)) null else named_struct(latitude, knownfloatingpointnormalized(normalizenanandzero(start#2116.latitude)), longitude, knownfloatingpointnormalized(normalizenanandzero(start#2116.longitude)))) AS start#2116], functions=[partial_min(id#2103)], output=[start#2116, min#2138])
+- *(2) Sort [knownfloatingpointnormalized(if (isnull(start#2116)) null else named_struct(latitude, knownfloatingpointnormalized(normalizenanandzero(start#2116.latitude)), longitude, knownfloatingpointnormalized(normalizenanandzero(start#2116.longitude)))) AS start#2116 ASC NULLS FIRST], false, 0
   +- *(2) Project [_gen_alias_2133#2133 AS start#2116, id#2103]
      +- *(2) !BroadcastHashJoin [s2_cell_id#2108L], [s2_cell_id#2104L], Inner, BuildLeft, NOT (cast(id#2103 as double) = _gen_alias_2134#2134), false
         :- BroadcastQueryStage 0
         :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, bigint, false]),false), [id=#3768]
         :     +- *(1) Project [row#2107.start AS _gen_alias_2133#2133, s2_cell_id#2108L]
         :        +- *(1) Filter isnotnull(s2_cell_id#2108L)
         :           +- FileScan parquet [row#2107,s2_cell_id#2108L] Batched: false, DataFilters: [isnotnull(s2_cell_id#2108L)], Format: Parquet, Location: InMemoryFileIndex[s3://co.mira.public/spark3_bug/left], PartitionFilters: [], PushedFilters: [IsNotNull(s2_cell_id)], ReadSchema: struct<row:struct<start:struct<latitude:double,longitude:double>>,s2_cell_id:bigint>
         +- *(2) Filter (isnotnull(id#2103) AND isnotnull(s2_cell_id#2104L))
            +- *(2) ColumnarToRow
               +- FileScan parquet [id#2103,s2_cell_id#2104L] Batched: true, DataFilters: [isnotnull(id#2103), isnotnull(s2_cell_id#2104L)], Format: Parquet, Location: InMemoryFileIndex[s3://co.mira.public/spark3_bug/right], PartitionFilters: [], PushedFilters: [IsNotNull(id), IsNotNull(s2_cell_id)], ReadSchema: struct<id:string,s2_cell_id:bigint>
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
  at org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:54)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:171)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:171)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:175)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:174)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$materializeFuture$2(ShuffleExchangeExec.scala:101)
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
  ... 143 more
Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: _gen_alias_2134#2134
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:329)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:75)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:329)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:334)
  at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:388)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:424)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:256)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:422)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:370)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:334)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:334)
  at org.apache.spark.sql.catalyst.trees.TreeNode.applyFunctionIfChanged$1(TreeNode.scala:388)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:424)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:256)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:422)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:370)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:334)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:318)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)
  at org.apache.spark.sql.execution.joins.JoinCodegenSupport.getJoinCondition(JoinCodegenSupport.scala:52)
  at org.apache.spark.sql.execution.joins.JoinCodegenSupport.getJoinCondition$(JoinCodegenSupport.scala:38)
  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.getJoinCondition(BroadcastHashJoinExec.scala:40)
  at org.apache.spark.sql.execution.joins.HashJoin.codegenInner(HashJoin.scala:392)
  at org.apache.spark.sql.execution.joins.HashJoin.codegenInner$(HashJoin.scala:389)
  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:40)
  at org.apache.spark.sql.execution.joins.HashJoin.doConsume(HashJoin.scala:356)
  at org.apache.spark.sql.execution.joins.HashJoin.doConsume$(HashJoin.scala:354)
  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:40)
  at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:282)
  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:253)
  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:210)
  at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:161)
  at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:283)
  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:255)
  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:210)
  at org.apache.spark.sql.execution.ColumnarToRowExec.consume(Columnar.scala:66)
  at org.apache.spark.sql.execution.ColumnarToRowExec.doProduce(Columnar.scala:191)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.ColumnarToRowExec.produce(Columnar.scala:66)
  at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:198)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:161)
  at org.apache.spark.sql.execution.joins.HashJoin.doProduce(HashJoin.scala:351)
  at org.apache.spark.sql.execution.joins.HashJoin.doProduce$(HashJoin.scala:350)
  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:40)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)
  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)
  at org.apache.spark.sql.execution.SortExec.doProduce(SortExec.scala:173)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:97)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:92)
  at org.apache.spark.sql.execution.SortExec.produce(SortExec.scala:41)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:726)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:795)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:194)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:232)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:229)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:190)
  at org.apache.spark.sql.execution.aggregate.SortAggregateExec.$anonfun$doExecute$1(SortAggregateExec.scala:56)
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
  ... 155 more
Caused by: java.lang.RuntimeException: Couldn't find _gen_alias_2134#2134 in [id#2103,s2_cell_id#2104L,_gen_alias_2133#2133,s2_cell_id#2108L]
  at scala.sys.package$.error(package.scala:30)
  at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)
  at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
  ... 244 more{code}
 ",,kellan.burket,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 20 15:41:28 UTC 2021,,,,,,,,,,"0|z0xsxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/21 08:25;gurwls223;quick question, does it fail in Spark 3.2 too?;;;","20/Dec/21 15:41;kellan.burket;Seems like it's working in Spark 3.2. So I guess nothing to do but wait for the EMR upgrade. Thanks!;;;",,,,,,,,,,,,,,
Mitigate ConcurrentModificationException thrown from tests in SparkContextSuite,SPARK-37663,13417932,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,16/Dec/21 09:26,17/Dec/21 20:22,13/Jul/23 08:50,17/Dec/21 20:22,3.3.0,,,,,,,,3.3.0,,,,Spark Core,Tests,,,,0,,,,,"ConcurrentModificationException can be thrown from tests in SparkContextSuite with Scala 2.13.
The cause seems to be same as SPARK-37315.",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 17 20:22:07 UTC 2021,,,,,,,,,,"0|z0xrxs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/21 10:48;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34922;;;","17/Dec/21 20:22;dongjoon;Issue resolved by pull request 34922
[https://github.com/apache/spark/pull/34922];;;",,,,,,,,,,,,,,
Fix FsHistoryProvider race condition between list and delet log info,SPARK-37659,13417873,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,16/Dec/21 06:13,02/May/22 08:12,13/Jul/23 08:50,23/Dec/21 07:16,3.1.2,3.2.1,3.3.0,,,,,,3.2.1,3.3.0,,,Web UI,,,,,0,,,,,"After SPARK-29043, FsHistoryProvider will list the log info without waitting all `mergeApplicationListing` task finished.

However the `LevelDBIterator` of list log info is not thread safe if some other threads delete the related log info at same time.

There is the error msg:
{code:java}
21/12/15 14:12:02 ERROR FsHistoryProvider: Exception in checking for event log updates
java.util.NoSuchElementException: 1^@__main__^@+hdfs://xxx/application_xxx.inprogress
        at org.apache.spark.util.kvstore.LevelDB.get(LevelDB.java:132)
        at org.apache.spark.util.kvstore.LevelDBIterator.next(LevelDBIterator.java:137)
        at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:44)
        at scala.collection.Iterator.foreach(Iterator.scala:941)
        at scala.collection.Iterator.foreach$(Iterator.scala:941)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1429)
        at scala.collection.IterableLike.foreach(IterableLike.scala:74)
        at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
        at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
        at scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)
        at scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)
        at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:184)
        at scala.collection.mutable.ListBuffer.$plus$plus$eq(ListBuffer.scala:47)
        at scala.collection.TraversableLike.to(TraversableLike.scala:678)
        at scala.collection.TraversableLike.to$(TraversableLike.scala:675)
        at scala.collection.AbstractTraversable.to(Traversable.scala:108)
        at scala.collection.TraversableOnce.toList(TraversableOnce.scala:299)
        at scala.collection.TraversableOnce.toList$(TraversableOnce.scala:299)
        at scala.collection.AbstractTraversable.toList(Traversable.scala:108)
        at org.apache.spark.deploy.history.FsHistoryProvider.checkForLogs(FsHistoryProvider.scala:588)
        at org.apache.spark.deploy.history.FsHistoryProvider.$anonfun$startPolling$3(FsHistoryProvider.scala:299)
{code}
",,apachespark,Gengliang.Wang,ulysses,yghu,,,,,,,,,,,,,,,SPARK-39082,,,,,,,,SPARK-39083,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 23 14:09:09 UTC 2021,,,,,,,,,,"0|z0xrko:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/21 07:23;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34919;;;","23/Dec/21 07:17;Gengliang.Wang;Issue resolved by [https://github.com/apache/spark/pull/34919];;;","23/Dec/21 14:09;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/35003;;;",,,,,,,,,,,,,
Skip PIP packaging test if Python version is lower than 3.7,SPARK-37658,13417859,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,16/Dec/21 03:57,12/Dec/22 17:51,13/Jul/23 08:50,16/Dec/21 06:23,3.3.0,,,,,,,,3.3.0,,,,Project Infra,PySpark,,,,0,,,,,"{code}
Writing pyspark-3.3.0.dev0/setup.cfg
Creating tar archive
removing 'pyspark-3.3.0.dev0' (and everything under it)
Installing dist into virtual env
Obtaining file:///home/jenkins/workspace/SparkPullRequestBuilder%402/python
pyspark requires Python '>=3.7' but the running Python is 3.6.8
Cleaning up temporary directory - /tmp/tmp.CCragmNU1X
[error] running /home/jenkins/workspace/SparkPullRequestBuilder@2/dev/run-pip-tests ; received return code 1
Attempting to post to GitHub...
{code}

After we drop Python 3.6 at SPARK-37632, PIP packaging test fails intermittently https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/146255/console. Apparently, different Python versions are installed. We should skip the test in these cases.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 16 06:23:19 UTC 2021,,,,,,,,,,"0|z0xrhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/21 04:03;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/34917;;;","16/Dec/21 06:23;dongjoon;Issue resolved by pull request 34917
[https://github.com/apache/spark/pull/34917];;;",,,,,,,,,,,,,,
Upgrade SBT to 1.5.7,SPARK-37656,13417843,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,16/Dec/21 01:53,12/Dec/22 18:10,13/Jul/23 08:50,16/Dec/21 05:26,3.2.1,3.3.0,,,,,,,3.2.1,3.3.0,,,Build,,,,,0,,,,,"SBT 1.5.7 was released a few hours ago, which includes a fix for CVE-2021-45046.
https://github.com/sbt/sbt/releases/tag/v1.5.7",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 16 05:26:18 UTC 2021,,,,,,,,,,"0|z0xre0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/21 01:58;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34915;;;","16/Dec/21 01:58;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34915;;;","16/Dec/21 05:26;gurwls223;Issue resolved by pull request 34915
[https://github.com/apache/spark/pull/34915];;;",,,,,,,,,,,,,
Regression - NullPointerException in Row.getSeq when field null,SPARK-37654,13417788,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,brandon.dahler.amazon,brandon.dahler.amazon,15/Dec/21 17:58,12/Dec/22 18:11,13/Jul/23 08:50,17/Dec/21 04:44,3.1.1,3.1.2,3.2.0,,,,,,3.1.3,3.2.1,3.3.0,,SQL,,,,,0,,,,,"h2. Description

A NullPointerException occurs in _org.apache.spark.sql.Row.getSeq(int)_ if the row contains a _null_ value at the requested index.
{code:java}
java.lang.NullPointerException
	at org.apache.spark.sql.Row.getSeq(Row.scala:319)
	at org.apache.spark.sql.Row.getSeq$(Row.scala:319)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)
	at org.apache.spark.sql.Row.getList(Row.scala:327)
	at org.apache.spark.sql.Row.getList$(Row.scala:326)
	at org.apache.spark.sql.catalyst.expressions.GenericRow.getList(rows.scala:166)
        ...
{code}
 

Prior to 3.1.1, the code would not throw an exception and instead would return a null _Seq_ instance.
h2. Reproduction
 # Start a new spark-shell instance
 # Execute the following script:
{code:scala}
import org.apache.spark.sql.Row

Row(Seq(""value"")).getSeq(0)
Row(Seq()).getSeq(0)
Row(null).getSeq(0) {code}

h3. Expected Output

res2 outputs a _null_ value.
{code:java}
scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala>

scala> Row(Seq(""value"")).getSeq(0)
res0: Seq[Nothing] = List(value)

scala> Row(Seq()).getSeq(0)
res1: Seq[Nothing] = List()

scala> Row(null).getSeq(0)
res2: Seq[Nothing] = null
{code}
h3. Actual Output

res2 throws a NullPointerException.
{code:java}
scala> import org.apache.spark.sql.Row
import org.apache.spark.sql.Row

scala>

scala> Row(Seq(""value"")).getSeq(0)
res0: Seq[Nothing] = List(value)

scala> Row(Seq()).getSeq(0)
res1: Seq[Nothing] = List()

scala> Row(null).getSeq(0)
java.lang.NullPointerException
  at org.apache.spark.sql.Row.getSeq(Row.scala:319)
  at org.apache.spark.sql.Row.getSeq$(Row.scala:319)
  at org.apache.spark.sql.catalyst.expressions.GenericRow.getSeq(rows.scala:166)
  ... 47 elided
{code}

h3. Environments Tested
Tested against the following releases using the provided reproduction steps:
 # spark-3.0.3-bin-hadoop2.7 - Succeeded
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.3
      /_/Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_312) {code}
 # spark-3.1.2-bin-hadoop3.2 - Failed
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.2
      /_/Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_312) {code}
 # spark-3.2.0-bin-hadoop3.2 - Failed
{code:java}
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.2.0
      /_/Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_312) {code}


h2. Regression Source
The regression appears to have been introduced in [25c7d0fe6ae20a4c1c42e0cd0b448c08ab03f3fb|https://github.com/apache/spark/commit/25c7d0fe6ae20a4c1c42e0cd0b448c08ab03f3fb#diff-722324a11a0e4635a59a9debc962da2c1678d86702a9a106fd0d51188f83853bR317], which addressed [SPARK-32526|https://issues.apache.org/jira/browse/SPARK-32526]

h2. Work Around
This regression can be worked around by using _Row.isNullAt(int)_ and handling the null scenario in user code, prior to calling _Row.getSeq(int)_ or _Row.getList(int)_.",,apachespark,brandon.dahler.amazon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 17 04:44:27 UTC 2021,,,,,,,,,,"0|z0xr1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/21 01:50;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34928;;;","17/Dec/21 04:44;gurwls223;Issue resolved by pull request 34928
[https://github.com/apache/spark/pull/34928];;;",,,,,,,,,,,,,,
"when charVarcharAsString is true, char datatype partition table query incorrect",SPARK-37643,13417345,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yghu,yghu,yghu,14/Dec/21 09:30,08/Jul/22 01:49,13/Jul/23 08:50,18/Apr/22 15:20,3.1.2,3.2.0,,,,,,,3.1.3,3.2.2,3.3.0,,SQL,,,,,0,correctness,,,,"This ticket aim at fixing the bug that does not apply right-padding for char types column when charVarcharAsString is true and partition data length is less than defined length.
For example, a query below returns nothing in master, but a correct result is `abc`.
{code:java}
scala> sql(""set spark.sql.legacy.charVarcharAsString=true"")
scala> sql(""CREATE TABLE tb01(i string, c char(5)) USING parquet partitioned by (c)"")
scala> sql(""INSERT INTO tb01 values(1, 'abc')"")
scala> sql(""select c from tb01 where c = 'abc'"").show
+---+
|  c|
+---+
+---+{code}
This is because `ApplyCharTypePadding` rpad the expr to charLength. We should handle this consider conf spark.sql.legacy.charVarcharAsString value.",spark 3.2.0,apachespark,cloud_fan,yghu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Apr 18 15:20:23 UTC 2022,,,,,,,,,,"0|z0xobk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/21 16:02;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/34900;;;","13/Apr/22 03:27;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/36171;;;","14/Apr/22 01:06;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/36187;;;","14/Apr/22 01:07;apachespark;User 'fhygh' has created a pull request for this issue:
https://github.com/apache/spark/pull/36187;;;","18/Apr/22 15:20;cloud_fan;Issue resolved by pull request 36187
[https://github.com/apache/spark/pull/36187];;;",,,,,,,,,,,
SHOW TBLPROPERTIES should print the fully qualified table name,SPARK-37635,13417262,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,14/Dec/21 04:26,14/Dec/21 10:16,13/Jul/23 08:50,14/Dec/21 10:16,3.3.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 14 10:16:03 UTC 2021,,,,,,,,,,"0|z0xnt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/21 04:31;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34890;;;","14/Dec/21 04:32;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34890;;;","14/Dec/21 10:16;sarutak;Issue resolved in https://github.com/apache/spark/pull/34890;;;",,,,,,,,,,,,,
Unwrap cast should skip if downcast failed with ansi enabled,SPARK-37633,13417240,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,mauzhang,mauzhang,mauzhang,14/Dec/21 00:41,15/Dec/21 19:33,13/Jul/23 08:50,15/Dec/21 19:32,3.1.2,3.2.0,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"Currently, unwrap cast throws ArithmeticException if down cast failed with ansi enabled. Since UnwrapCastInBinaryComparison is an optimizer rule, we should always skip on failure regardless of ansi config.",,apachespark,csun,mauzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 15 19:32:43 UTC 2021,,,,,,,,,,"0|z0xno8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Dec/21 02:20;apachespark;User 'manuzhang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34888;;;","15/Dec/21 19:32;csun;Issue resolved by pull request 34888
[https://github.com/apache/spark/pull/34888];;;",,,,,,,,,,,,,,
Upgrade SBT to 1.5.6,SPARK-37615,13416863,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,william,william,william,12/Dec/21 04:00,12/Dec/21 05:34,13/Jul/23 08:50,12/Dec/21 05:25,3.2.1,3.3.0,,,,,,,3.2.1,3.3.0,,,Build,,,,,0,,,,,"This issue aims to upgrade SBT to 1.5.6.

- https://github.com/sbt/sbt/releases/tag/v1.5.6

SBT 1.5.6 updates log4j 2 to 2.15.0, which fixes remote code execution vulnerability (CVE-2021-44228)

This only affects build servers.",,apachespark,dongjoon,william,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Dec 12 05:25:56 UTC 2021,,,,,,,,,,"0|z0xlcg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/21 04:02;apachespark;User 'williamhyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34869;;;","12/Dec/21 05:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34870;;;","12/Dec/21 05:23;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34870;;;","12/Dec/21 05:25;dongjoon;This is resolved via https://github.com/apache/spark/pull/34869;;;",,,,,,,,,,,,
Pyspark's newAPIHadoopRDD() method fails with ShortWritables,SPARK-37598,13416352,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,keith.massey,keith.massey,keith.massey,09/Dec/21 17:56,12/Dec/22 18:10,13/Jul/23 08:50,13/Dec/21 00:07,2.4.8,3.0.3,3.1.2,3.2.0,,,,,3.3.0,,,,PySpark,,,,,0,,,,,"If sc. newAPIHadoopRDD() is called from Pyspark using an InputFormat that has a ShortWritable as a field, then the call to newAPIHadoopRDD() fails. The reason is that shortWritable is not explicitly handled by PythonHadoopUtil the way that other numeric writables are (like LongWritable). The result is that the ShortWritable is not converted to an object that can be serialized by spark, and a serialization error occurs. Below is an example stack trace from within the pyspark shell:

{code:java}
>>> rdd = sc.newAPIHadoopRDD(inputFormatClass=""[org.elasticsearch.hadoop.mr|http://org.elasticsearch.hadoop.mr/].EsInputFormat"",
...             keyClass=""[org.apache.hadoop.io|http://org.apache.hadoop.io/].NullWritable"",
...             valueClass=""[org.elasticsearch.hadoop.mr|http://org.elasticsearch.hadoop.mr/].LinkedMapWritable"",
...             conf=conf)
2021-12-08 14:38:40,439 ERROR scheduler.TaskSetManager: task 0.0 in stage 15.0 (TID 31) had a not serializable result: org.apache.hadoop.io.ShortWritable
Serialization stack:
- object not serializable (class: [org.apache.hadoop.io|http://org.apache.hadoop.io/].ShortWritable, value: 1)
- writeObject data (class: java.util.HashMap)
- object (class java.util.HashMap, \{price=1})
- field (class: scala.Tuple2, name: _2, type: class java.lang.Object)
- object (class scala.Tuple2, (1,\{price=1}))
- element of array (index: 0)
- array (class [Lscala.Tuple2;, size 1); not retrying
Traceback (most recent call last):
 File ""<stdin>"", line 4, in <module>
 File ""/home/hduser/spark-3.1.2-bin-hadoop3.2/python/pyspark/context.py"", line 853, in newAPIHadoopRDD
  jconf, batchSize)
 File ""/home/hduser/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py"", line 1305, in __call__
 File ""/home/hduser/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py"", line 111, in deco
  return f(*a, **kw)
 File ""/home/hduser/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py"", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD.
: org.apache.spark.SparkException: Job aborted due to stage failure: task 0.0 in stage 15.0 (TID 31) had a not serializable result: org.apache.hadoop.io.ShortWritable
Serialization stack:
- object not serializable (class: [org.apache.hadoop.io|http://org.apache.hadoop.io/].ShortWritable, value: 1)
- writeObject data (class: java.util.HashMap)
- object (class java.util.HashMap, \{price=1})
- field (class: scala.Tuple2, name: _2, type: class java.lang.Object)
- object (class scala.Tuple2, (1,\{price=1}))
- element of array (index: 0)
- array (class [Lscala.Tuple2;, size 1)
at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)
at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)
at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)
at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)
at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)
at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)
at scala.Option.foreach(Option.scala:407)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)
at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)
at org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1449)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:414)
at org.apache.spark.rdd.RDD.take(RDD.scala:1422)
at org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:173)
at org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:385)
at org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
at py4j.Gateway.invoke(Gateway.java:282)
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
at py4j.commands.CallCommand.execute(CallCommand.java:79)
at py4j.GatewayConnection.run(GatewayConnection.java:238)
at java.lang.Thread.run(Thread.java:748)
{code}

",,apachespark,keith.massey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 13 00:07:49 UTC 2021,,,,,,,,,,"0|z0xio8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/21 18:02;apachespark;User 'masseyke' has created a pull request for this issue:
https://github.com/apache/spark/pull/34838;;;","09/Dec/21 18:03;apachespark;User 'masseyke' has created a pull request for this issue:
https://github.com/apache/spark/pull/34838;;;","13/Dec/21 00:07;gurwls223;Issue resolved by pull request 34838
[https://github.com/apache/spark/pull/34838];;;",,,,,,,,,,,,,
DSV2 InputMetrics are not getting update in corner case,SPARK-37585,13416060,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sandeep.katta2007,sandeep.katta2007,sandeep.katta2007,08/Dec/21 14:07,09/Feb/22 14:13,13/Jul/23 08:50,09/Feb/22 14:13,3.0.3,3.1.2,,,,,,,3.3.0,,,,Spark Core,,,,,0,,,,,"In some corner cases, DSV2 is not updating the input metrics.

 

This is very special case where the number of records read are less than 1000 and *hasNext* is not called for last element(cz input.hasNext returns false so MetricsIterator.hasNext is not called)

 

hasNext implementation of MetricsIterator

 
{code:java}
override def hasNext: Boolean = {
  if (iter.hasNext) {
    true
  } else {
    metricsHandler.updateMetrics(0, force = true)
    false
  } {code}
 

You reproduce this issue easily in spark-shell by running below code
{code:java}
import scala.collection.mutable
import org.apache.spark.scheduler.{SparkListener, SparkListenerTaskEnd}spark.conf.set(""spark.sql.sources.useV1SourceList"", """")
val dir = ""Users/tmp1""
spark.range(0, 100).write.format(""parquet"").mode(""overwrite"").save(dir)
val df = spark.read.format(""parquet"").load(dir)
val bytesReads = new mutable.ArrayBuffer[Long]()
val recordsRead = new mutable.ArrayBuffer[Long]()val bytesReadListener = new SparkListener() {
  override def onTaskEnd(taskEnd: SparkListenerTaskEnd): Unit = {
    bytesReads += taskEnd.taskMetrics.inputMetrics.bytesRead
    recordsRead += taskEnd.taskMetrics.inputMetrics.recordsRead
  }
}
spark.sparkContext.addSparkListener(bytesReadListener)
try {
df.limit(10).collect()
assert(recordsRead.sum > 0)
assert(bytesReads.sum > 0)
} finally {
spark.sparkContext.removeSparkListener(bytesReadListener)
} {code}
This code generally fails at *assert(bytesReads.sum > 0)* which confirms that updateMetrics API is not called

 ",,apachespark,cloud_fan,Saikrishna_Pujari,sandeep.katta2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 09 14:13:26 UTC 2022,,,,,,,,,,"0|z0xgvk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/22 05:30;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/35432;;;","08/Feb/22 05:31;apachespark;User 'bozhang2820' has created a pull request for this issue:
https://github.com/apache/spark/pull/35432;;;","09/Feb/22 14:13;cloud_fan;Issue resolved by pull request 35432
[https://github.com/apache/spark/pull/35432];;;",,,,,,,,,,,,,
ClassCastException: ArrayType cannot be cast to StructType,SPARK-37577,13415973,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,ravwojdyla,ravwojdyla,08/Dec/21 08:01,12/Dec/22 18:10,13/Jul/23 08:50,13/Dec/21 07:18,3.2.0,,,,,,,,3.2.1,3.3.0,,,SQL,,,,,1,,,,,"Reproduction:

{code:python}
import pyspark.sql.functions as F
from pyspark.sql.types import StructType, StructField, ArrayType, StringType

t = StructType([StructField('o', ArrayType(StructType([StructField('s',
               StringType(), False), StructField('b',
               ArrayType(StructType([StructField('e', StringType(),
               False)]), True), False)]), True), False)])

(
    spark.createDataFrame([], schema=t)
    .select(F.explode(""o"").alias(""eo""))
    .select(""eo.*"")
    .select(F.explode(""b""))
    .count()
)
{code}

Code above works fine in 3.1.2, fails in 3.2.0. See stacktrace below. Note that if you remove, field {{s}}, the code works fine, which is a bit unexpected and likely a clue.

{noformat}
Py4JJavaError: An error occurred while calling o156.count.
: java.lang.ClassCastException: class org.apache.spark.sql.types.ArrayType cannot be cast to class org.apache.spark.sql.types.StructType (org.apache.spark.sql.types.ArrayType and org.apache.spark.sql.types.StructType are in unnamed module of loader 'app')
	at org.apache.spark.sql.catalyst.expressions.GetStructField.childSchema$lzycompute(complexTypeExtractors.scala:107)
	at org.apache.spark.sql.catalyst.expressions.GetStructField.childSchema(complexTypeExtractors.scala:107)
	at org.apache.spark.sql.catalyst.expressions.GetStructField.$anonfun$extractFieldName$1(complexTypeExtractors.scala:117)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.catalyst.expressions.GetStructField.extractFieldName(complexTypeExtractors.scala:117)
	at org.apache.spark.sql.catalyst.optimizer.GeneratorNestedColumnAliasing$$anonfun$1$$anonfun$2.applyOrElse(NestedColumnAliasing.scala:372)
	at org.apache.spark.sql.catalyst.optimizer.GeneratorNestedColumnAliasing$$anonfun$1$$anonfun$2.applyOrElse(NestedColumnAliasing.scala:368)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUpWithPruning$4(TreeNode.scala:539)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUpWithPruning(TreeNode.scala:539)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:508)
	at org.apache.spark.sql.catalyst.optimizer.GeneratorNestedColumnAliasing$$anonfun$1.applyOrElse(NestedColumnAliasing.scala:368)
	at org.apache.spark.sql.catalyst.optimizer.GeneratorNestedColumnAliasing$$anonfun$1.applyOrElse(NestedColumnAliasing.scala:366)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsDownWithPruning$1(QueryPlan.scala:152)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:193)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:204)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:214)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:214)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsDownWithPruning(QueryPlan.scala:152)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsWithPruning(QueryPlan.scala:123)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressions(QueryPlan.scala:101)
	at org.apache.spark.sql.catalyst.optimizer.GeneratorNestedColumnAliasing$.unapply(NestedColumnAliasing.scala:366)
	at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$$anonfun$apply$14.applyOrElse(Optimizer.scala:826)
	at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$$anonfun$apply$14.applyOrElse(Optimizer.scala:783)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:486)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1128)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1127)
	at org.apache.spark.sql.catalyst.plans.logical.Generate.mapChildren(basicLogicalOperators.scala:121)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:486)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1128)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1127)
	at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:206)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:486)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1128)
	at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1127)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate.mapChildren(basicLogicalOperators.scala:940)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:486)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformWithPruning(TreeNode.scala:447)
	at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$.apply(Optimizer.scala:783)
	at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$.apply(Optimizer.scala:780)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:138)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:134)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:130)
	at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:148)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:166)
	at org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:163)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:163)
	at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:214)
	at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:259)
	at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:228)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3704)
	at org.apache.spark.sql.Dataset.count(Dataset.scala:3011)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:566)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.base/java.lang.Thread.run(Thread.java:834)
{noformat}",,apachespark,dongjoon,ravwojdyla,viirya,,,,,,,,,,,,,,,,,,,,,,,SPARK-38285,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 13 23:10:38 UTC 2021,,,,,,,,,,"0|z0xgc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/21 02:08;gurwls223;It works if you disable:

{code}
spark.conf.set(""spark.sql.optimizer.expression.nestedPruning.enabled"", False)
spark.conf.set(""spark.sql.optimizer.nestedSchemaPruning.enabled"", False)
{code}

cc [~viirya] FYI.

log:

{code}
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ColumnPruning ===
 Aggregate [count(1) AS count#22L]                     Aggregate [count(1) AS count#22L]
!+- Project [col#9]                                    +- Project
!   +- Generate explode(b#6), false, [col#9]              +- Project
!      +- Project [eo#3.s AS s#5, eo#3.b AS b#6]             +- Generate explode(b#6), [0], false, [col#9]
!         +- Project [eo#3]                                     +- Project [b#6]
!            +- Generate explode(o#0), false, [eo#3]               +- Project [eo#3.b AS b#6]
!               +- LogicalRDD [o#0], false                            +- Project [eo#3]
!                                                                        +- Generate explode(o#0), [0], false, [eo#3]
!                                                                           +- LogicalRDD [o#0], false

21/12/09 11:14:50 TRACE PlanChangeLogger:
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CollapseProject ===
 Aggregate [count(1) AS count#22L]                                Aggregate [count(1) AS count#22L]
 +- Project                                                       +- Project
!   +- Project                                                       +- Generate explode(b#6), [0], false, [col#9]
!      +- Generate explode(b#6), [0], false, [col#9]                    +- Project [eo#3.b AS b#6]
!         +- Project [b#6]                                                 +- Generate explode(o#0), [0], false, [eo#3]
!            +- Project [eo#3.b AS b#6]                                       +- LogicalRDD [o#0], false
!               +- Project [eo#3]
!                  +- Generate explode(o#0), [0], false, [eo#3]
!                     +- LogicalRDD [o#0], false
{code};;;","09/Dec/21 02:45;viirya;Thanks [~hyukjin.kwon]. I will take a look.;;;","09/Dec/21 08:54;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/34845;;;","13/Dec/21 07:18;dongjoon;This is resolved via https://github.com/apache/spark/pull/34845;;;","13/Dec/21 23:09;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/34885;;;","13/Dec/21 23:10;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/34885;;;",,,,,,,,,,
"null values should be saved as nothing rather than quoted empty Strings """" with default settings",SPARK-37575,13415955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Wayne Guo,Wayne Guo,Wayne Guo,08/Dec/21 05:20,12/Dec/22 18:10,13/Jul/23 08:50,14/Dec/21 08:32,2.4.0,3.2.0,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"As mentioned in sql migration guide([https://spark.apache.org/docs/latest/sql-migration-guide.html#upgrading-from-spark-sql-23-to-24]),
{noformat}
Since Spark 2.4, empty strings are saved as quoted empty strings """". In version 2.3 and earlier, empty strings are equal to null values and do not reflect to any characters in saved CSV files. For example, the row of ""a"", null, """", 1 was written as a,,,1. Since Spark 2.4, the same row is saved as a,,"""",1. To restore the previous behavior, set the CSV option emptyValue to empty (not quoted) string.{noformat}
But actually, both empty strings and null values are saved as quoted empty Strings """" rather than """" (for empty strings) and nothing(for null values)。

code:
{code:java}
val data = List(""spark"", null, """").toDF(""name"")
data.coalesce(1).write.csv(""spark_csv_test"")
{code}
 actual result:
{noformat}
line1: spark
line2: """"
line3: """"{noformat}
expected result:
{noformat}
line1: spark
line2: 
line3: """"
{noformat}",,apachespark,maxgekk,rajesh.balamohan,Wayne Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Apr 19 21:48:37 UTC 2022,,,,,,,,,,"0|z0xg88:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/21 05:45;Wayne Guo;related issues:

https://issues.apache.org/jira/browse/SPARK-17916

https://issues.apache.org/jira/browse/SPARK-25241

 ;;;","08/Dec/21 05:51;gurwls223;can you set nullValue and emptyValue options?;;;","08/Dec/21 06:01;Wayne Guo;As default writerSettings in CSVOptions,  nullValue is """",   emptyValueInWrite is ""\""\""""
{code:java}
val nullValue = parameters.getOrElse(""nullValue"", """")
val emptyValue = parameters.get(""emptyValue"")
val emptyValueInWrite = emptyValue.getOrElse(""\""\"""")
writerSettings.setNullValue(nullValue)
writerSettings.setEmptyValue(emptyValueInWrite) {code}
but the final result is not expected?;;;","08/Dec/21 06:06;gurwls223;Spark 2.4.X is EOL so it won't likely be fixed. Does it work with Spark 3+?;;;","08/Dec/21 06:38;Wayne Guo;Spark 3.2.0 has the same behavior.;;;","08/Dec/21 08:39;Wayne Guo;I think I maybe have found the root cause through debug Spark source code.

In {color:#0747a6}UnivocityGenerator{color}, when the value of column is null values, column's value has been changed  to {color:#00875a}options.nullValue{color}, default value  is """"

 
{code:java}
private def convertRow(row: InternalRow): Seq[String] = {
  var i = 0
  val values = new Array[String](row.numFields)
  while (i < row.numFields) {
    if (!row.isNullAt(i)) {
      values(i) = valueConverters(i).apply(row, i)
    } else {
      values(i) = options.nullValue
    }
    i += 1
  }
  values
} {code}
 

So，in {color:#0747a6}univocity-parsers lib{color}(depended by Spark) {color:#0747a6}AbstractWriter{color} class, element( is original null values) has been changed to  """" in UnivocityGenerator，not satisfied condition(element == null)，finally equals emptyValue, default value  is ""\""\""""

 
{code:java}
protected String getStringValue(Object element) {
   usingNullOrEmptyValue = false;
   if (element == null) {
      usingNullOrEmptyValue = true;
      return nullValue;
   }
   String string = String.valueOf(element);
   if (string.isEmpty()) {
      usingNullOrEmptyValue = true;
      return emptyValue;
   }
   return string;
} {code}
[~hyukjin.kwon]  Should we fix the change(isNullAt) in {color:#0747a6}UnivocityGenerator?{color}

 ;;;","08/Dec/21 08:51;Wayne Guo;I think I can make a fast repair to verify my conclusion and add some test cases.;;;","08/Dec/21 08:54;gurwls223;Yeah, it would be great to fix it, and upgrade the version used in Apache Spark.;;;","09/Dec/21 06:52;Wayne Guo;I  have made some bug fix in my local repo and add some test cases that passed, can you assign this issue to me?[~hyukjin.kwon] ;;;","09/Dec/21 06:55;gurwls223;We don't usually assign issues to someone. Once you open a PR, that will automatically assign appropriate ones. You can just add a comment that saying you're working on this ticket.;;;","09/Dec/21 18:32;apachespark;User 'wayneguow' has created a pull request for this issue:
https://github.com/apache/spark/pull/34853;;;","14/Dec/21 08:32;maxgekk;Issue resolved by pull request 34853
[https://github.com/apache/spark/pull/34853];;;","15/Dec/21 04:43;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/34905;;;","07/Apr/22 23:31;apachespark;User 'anchovYu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36110;;;","19/Apr/22 21:48;apachespark;User 'anchovYu' has created a pull request for this issue:
https://github.com/apache/spark/pull/36268;;;",
"IsolatedClient  fallbackVersion should be build in version, not always 2.7.4",SPARK-37573,13415949,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,08/Dec/21 03:45,10/Dec/21 03:56,13/Jul/23 08:50,10/Dec/21 03:56,3.2.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"Hadoop 3 fallback to 2.7.4 cause error
{code}
[info] org.apache.spark.sql.hive.client.VersionsSuite *** ABORTED *** (31 seconds, 320 milliseconds)
[info]   java.lang.ClassFormatError: Truncated class file
[info]   at java.lang.ClassLoader.defineClass1(Native Method)
[info]   at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
[info]   at java.lang.ClassLoader.defineClass(ClassLoader.java:635)
[info]   at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:266)
[info]   at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:258)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:405)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
[info]   at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:313)
[info]   at org.apache.spark.sql.hive.client.HiveClientBuilder$.buildClient(HiveClientBuilder.scala:50)
[info]   at org.apache.spark.sql.hive.client.VersionsSuite.$anonfun$new$2(VersionsSuite.scala:82)
[info]   at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:190)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:62)
[info]   at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234)
[info]   at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227)
[info]   at org.apache.spark.SparkFunSuite.runTest(SparkFunSuite.scala:62)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413)
[info]   at scala.collection.immutable.List.foreach(List.scala:431)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269)
[info]   at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268)
[info]   at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563)
[info]   at org.scalatest.Suite.run(Suite.scala:1112)
[info]   at org.scalatest.Suite.run$(Suite.scala:1094)
[
{code}",,angerszhuuu,apachespark,csun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 10 03:56:51 UTC 2021,,,,,,,,,,"0|z0xg6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/21 03:51;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34830;;;","08/Dec/21 03:52;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34830;;;","10/Dec/21 03:56;csun;Issue resolved by pull request 34830
[https://github.com/apache/spark/pull/34830];;;",,,,,,,,,,,,,
View Analysis incorrectly marks nested fields as nullable,SPARK-37569,13415924,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,shardulm,shardulm,shardulm,08/Dec/21 00:16,13/Dec/21 04:52,13/Jul/23 08:50,13/Dec/21 04:52,3.2.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"Consider a view as follows with all fields non-nullable (required)
{code:java}
spark.sql(""""""
    CREATE OR REPLACE VIEW v AS 
    SELECT id, named_struct('a', id) AS nested
    FROM RANGE(10)
"""""")
{code}
we can see that the view schema has been correctly stored as non-nullable
{code:java}
scala> System.out.println(spark.sessionState.catalog.externalCatalog.getTable(""default"", ""v2""))
CatalogTable(
Database: default
Table: v2
Owner: smahadik
Created Time: Tue Dec 07 09:00:42 PST 2021
Last Access: UNKNOWN
Created By: Spark 3.3.0-SNAPSHOT
Type: VIEW
View Text: SELECT id, named_struct('a', id) AS nested
    FROM RANGE(10)
View Original Text: SELECT id, named_struct('a', id) AS nested
    FROM RANGE(10)
View Catalog and Namespace: spark_catalog.default
View Query Output Columns: [id, nested]
Table Properties: [transient_lastDdlTime=1638896442]
Serde Library: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
InputFormat: org.apache.hadoop.mapred.SequenceFileInputFormat
OutputFormat: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
Storage Properties: [serialization.format=1]
Schema: root
 |-- id: long (nullable = false)
 |-- nested: struct (nullable = false)
 |    |-- a: long (nullable = false)
)
{code}
However, when trying to read this view, it incorrectly marks nested column {{a}} as nullable
{code:java}
scala> spark.table(""v2"").printSchema
root
 |-- id: long (nullable = false)
 |-- nested: struct (nullable = false)
 |    |-- a: long (nullable = true)
{code}
This is caused by [this line|https://github.com/apache/spark/blob/fb40c0e19f84f2de9a3d69d809e9e4031f76ef90/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3546] in Analyzer.scala. Going through the history of changes for this block of code, it seems like {{asNullable}} is a remnant of a time before we added [checks|https://github.com/apache/spark/blob/fb40c0e19f84f2de9a3d69d809e9e4031f76ef90/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L3543] to ensure that the from and to types of the cast were compatible. As nullability is already checked, it should be safe to add a cast without converting the target datatype to nullable.",,apachespark,cloud_fan,shardulm,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 13 04:52:37 UTC 2021,,,,,,,,,,"0|z0xg1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Dec/21 19:32;apachespark;User 'shardulm94' has created a pull request for this issue:
https://github.com/apache/spark/pull/34839;;;","13/Dec/21 04:52;cloud_fan;Issue resolved by pull request 34839
[https://github.com/apache/spark/pull/34839];;;",,,,,,,,,,,,,,
Deser void class fail with Java serialization,SPARK-37556,13415500,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,daijy,daijy,daijy,06/Dec/21 06:40,07/Dec/21 15:31,13/Jul/23 08:50,07/Dec/21 15:31,3.2.0,,,,,,,,3.0.4,3.1.3,3.2.1,3.3.0,Spark Core,,,,,0,,,,,"Spark code contains Java void type cannot be serialized with JavaSerializer. For example:
{code:java}
class Foo extends Serializable {
  Class k = Void.TYPE;
}
{code}
Spark will throw error:
{code:java}
java.lang.Void; local class name incompatible with stream class name ""void""
	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:703)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1940)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1806)
	at java.io.ObjectInputStream.readClass(ObjectInputStream.java:1771)
        ......
{code}
All other primitive type works. The reason is when SPARK-8730 try to fix the primitive type deserialization, it introduces the [following code|https://github.com/apache/spark/blob/v3.2.0/core/src/main/scala/org/apache/spark/serializer/JavaSerializer.scala#L80]:
{code:java}
private object JavaDeserializationStream {
  val primitiveMappings = Map[String, Class[_]](
    ""boolean"" -> classOf[Boolean],
    ""byte"" -> classOf[Byte],
    ""char"" -> classOf[Char],
    ""short"" -> classOf[Short],
    ""int"" -> classOf[Int],
    ""long"" -> classOf[Long],
    ""float"" -> classOf[Float],
    ""double"" -> classOf[Double],
    ""void"" -> classOf[Void]
  )
}
{code}
However, classOf[Void] is not the equivalence of other types. It's point to Void.class not Void.Type. The equivalence for void should be classOf[Unit]:
{code:java}
scala> classOf[Long]
val res0: Class[Long] = long

scala> classOf[Double]
val res1: Class[Double] = double

scala> classOf[Byte]
val res2: Class[Byte] = byte

scala> classOf[Void]
val res3: Class[Void] = class java.lang.Void  <--- this is wrong

scala> classOf[Unit]
val res4: Class[Unit] = void <---- this is right
{code}",,apachespark,daijy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 07 15:31:54 UTC 2021,,,,,,,,,,"0|z0xdfc:",9223372036854775807,,,,,,,,,,,,,3.3.0,,,,,,,,,,"06/Dec/21 06:44;apachespark;User 'daijyc' has created a pull request for this issue:
https://github.com/apache/spark/pull/34816;;;","06/Dec/21 06:44;apachespark;User 'daijyc' has created a pull request for this issue:
https://github.com/apache/spark/pull/34816;;;","07/Dec/21 15:31;srowen;Issue resolved by pull request 34816
[https://github.com/apache/spark/pull/34816];;;",,,,,,,,,,,,,
"Add PyArrow, pandas and plotly to release Docker image dependencies",SPARK-37554,13415486,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,gurwls223,gurwls223,,06/Dec/21 02:50,12/Dec/22 18:10,13/Jul/23 08:50,06/Dec/21 03:01,3.2.0,,,,,,,,3.2.2,3.3.0,,,Documentation,PySpark,,,,0,,,,,"In order to build the PySpark documentation, it requires to run several jobs for pandas API on Spark to generate plots. See also https://github.com/apache/spark/blob/master/.github/workflows/build_and_test.yml#L501-L502",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 06 03:01:10 UTC 2021,,,,,,,,,,"0|z0xdc8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/21 02:55;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/34813;;;","06/Dec/21 02:56;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/34813;;;","06/Dec/21 03:01;gurwls223;Issue resolved by pull request 34813
[https://github.com/apache/spark/pull/34813];;;",,,,,,,,,,,,,
V2 ReplaceTableAsSelect command should qualify location,SPARK-37546,13415432,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,huaxingao,huaxingao,04/Dec/21 23:10,06/Dec/21 10:31,13/Jul/23 08:50,06/Dec/21 10:31,3.3.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"V2 ReplaceTableAsSelect command should qualify location. Currently, 


{code:java}
spark.sql(""REPLACE TABLE testcat.t USING foo LOCATION '/tmp/foo' AS SELECT id FROM source"")
spark.sql(""DESCRIBE EXTENDED testcat.t"").show(false)
{code}

displays the location as `/tmp/foo` whereas V1 command displays/stores it as qualified (`file:/tmp/foo`).
",,apachespark,cloud_fan,huaxingao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 06 10:31:54 UTC 2021,,,,,,,,,,"0|z0xd0g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/21 23:28;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34807;;;","04/Dec/21 23:29;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34807;;;","06/Dec/21 10:31;cloud_fan;Issue resolved by pull request 34807
[https://github.com/apache/spark/pull/34807];;;",,,,,,,,,,,,,
V2 CreateTableAsSelect command should qualify location,SPARK-37545,13415378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,04/Dec/21 06:51,05/Dec/21 04:50,13/Jul/23 08:50,05/Dec/21 04:50,3.3.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"V2 CreateTableAsSelect command should qualify location. Currently, 

 
{code:java}
spark.sql(""CREATE TABLE testcat.t USING foo LOCATION '/tmp/foo' AS SELECT id FROM source"")
spark.sql(""DESCRIBE EXTENDED testcat.t"").show(false)
{code}
displays the location as `/tmp/foo` whereas V1 command displays/stores it as qualified (`[file:/tmp/foo|file:///tmp/foo]`).

 ",,apachespark,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Dec 04 07:14:24 UTC 2021,,,,,,,,,,"0|z0xcog:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/21 07:13;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/34806;;;","04/Dec/21 07:14;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/34806;;;",,,,,,,,,,,,,,
sequence over dates with month interval is producing incorrect results,SPARK-37544,13415375,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bersprockets,seva_ostapenko,seva_ostapenko,04/Dec/21 06:09,12/Dec/22 18:10,13/Jul/23 08:50,15/May/22 00:28,3.1.1,3.2.0,,,,,,,3.1.4,3.2.2,3.3.0,,SQL,,,,,0,correctness,,,,"Sequence function with dates and step interval in months producing unexpected results.

Here is a sample using Spark 3.2 (though the behavior is the same in 3.1.1 and presumably earlier):

{{scala> spark.sql(""select sequence(date '2021-01-01', date '2022-01-01', interval '3' month) x, date '2021-01-01' + interval '3' month y"").collect()}}
{{res1: Array[org.apache.spark.sql.Row] = Array([WrappedArray(2021-01-01, {color:#FF0000}*2021-03-31, 2021-06-30, 2021-09-30,* {color}{color:#172b4d}2022-01-01{color}),2021-04-01])}}

Expected result of adding 3 months to the 2021-01-01 is 2021-04-01, while sequence returns 2021-03-31.

At the same time sequence over timestamps works as expected:

{{scala> spark.sql(""select sequence(timestamp '2021-01-01 00:00', timestamp '2022-01-01 00:00', interval '3' month) x"").collect()}}
{{res2: Array[org.apache.spark.sql.Row] = Array([WrappedArray(2021-01-01 00:00:00.0, *2021-04-01* 00:00:00.0, *2021-07-01* 00:00:00.0, *2021-10-01* 00:00:00.0, 2022-01-01 00:00:00.0)])}}

 

A similar issue was reported in the past - [SPARK-31654] sequence producing inconsistent intervals for month step - ASF JIRA (apache.org)
It's marked resolved, but the problem is either resurfaced or was never actually fixed.","Ubuntu 20, OSX 11.6
OpenJDK 11, Spark 3.2",apachespark,bersprockets,maxgekk,seva_ostapenko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 17 23:04:51 UTC 2022,,,,,,,,,,"0|z0xcns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Dec/21 06:25;gurwls223;cc [~maxgekk] FYI;;;","07/Dec/21 06:37;maxgekk;We switched Spark's master on ANSI intervals recently. I ran the same example on it:
{code:java}
spark-sql> select sequence(date '2021-01-01', date '2022-01-01', interval '3' month) x, date '2021-01-01' + interval '3' month y;
[2021-01-01,2021-04-01,2021-07-01,2021-10-01,2022-01-01]    2021-04-01 {code};;;","13/May/22 20:18;bersprockets;Reproduction of the bug depends on your time zone. It all works as expected if you are running in the UTC time zone. The master branch will still produce incorrect results if your local time zone is, say, {{{}America/Los_Angeles{}}}. For example:
{noformat}
spark-sql> select sequence(date '2021-01-01', date '2022-01-01', interval '3' month) x;
[2021-01-01,2021-03-31,2021-06-30,2021-09-30,2022-01-01]
Time taken: 0.664 seconds, Fetched 1 row(s)
spark-sql> 
{noformat}
InternalSequenceBase converts the date to micros by multiplying days by micros per day. This converts the date into a time zone agnostic timestamp. However, InternalSequenceBase uses a function to perform the arithmetic (DateTimeUtils#timestampAddInterval) that assumes a _time zone aware_ timestamp.

If your time zone is America/Los_Angeles, and you specify a start date of '2021-01-01', InternalSequenceBase converts that to '2021-01-01 00:00:00 UTC'. However, what we should pass to DateTimeUtils#timestampAddInterval is '2021-01-01 00:00:00 _PST_' . As a result, the sequence ends up adding 3 months to '2020-12-31 16:00 PST', which is '2021-03-31 16:00 PDT'.

This is why the second value in the sequence is 2021-03-31 and not 2021-04-01.

One fix is to change InternalSequenceBase to use timestampNTZAddInterval for dates. However, that would put sequences out-of-sync with Spark's date arithmetic, which _is_ time zone aware. Take for example the following Spark date arithmetic:
{noformat}
select cast(date'2022-03-09' + interval '4' days '23' hour as date) as x;
{noformat}
In the {{America/Los_Angeles}} time zone, it returns {{{}2022-03-14{}}}.

However, in the {{UTC}} time zone, it instead returns {{{}2022-03-13{}}}.

So to be consistent with Spark date arithmetic, InternalSequenceBase should use daysToMicros and microsToDays for dates rather than simply multiplying days by a scale value. I will put up a PR that does exactly that in the next day or so.;;;","13/May/22 22:03;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36546;;;","13/May/22 22:03;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/36546;;;","15/May/22 00:28;gurwls223;Fixed in https://github.com/apache/spark/pull/36546;;;","17/Aug/22 23:04;apachespark;User 'bersprockets' has created a pull request for this issue:
https://github.com/apache/spark/pull/37559;;;",,,,,,,,,
Bump dev.ludovic.netlib to 2.2.1,SPARK-37534,13415236,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,luhenry,luhenry,luhenry,03/Dec/21 09:19,03/Dec/21 16:30,13/Jul/23 08:50,03/Dec/21 09:31,3.2.0,,,,,,,,3.2.1,3.3.0,,,Build,Graph,ML,MLlib,,0,,,,,"Bump the version of dev.ludovic.netlib from 2.2.0 to 2.2.1. This fixes a computation bug in sgemm. See [https://github.com/luhenry/netlib/issues/7|the issue[]|http://example.com]the] and [https://github.com/luhenry/netlib/compare/v2.2.0...v2.2.1|the diff]",,apachespark,dongjoon,luhenry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 03 09:50:02 UTC 2021,,,,,,,,,,"0|z0xbt4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/21 09:22;apachespark;User 'luhenry' has created a pull request for this issue:
https://github.com/apache/spark/pull/34783;;;","03/Dec/21 09:23;apachespark;User 'luhenry' has created a pull request for this issue:
https://github.com/apache/spark/pull/34783;;;","03/Dec/21 09:31;dongjoon;Issue resolved by pull request 34783
[https://github.com/apache/spark/pull/34783];;;","03/Dec/21 09:50;apachespark;User 'luhenry' has created a pull request for this issue:
https://github.com/apache/spark/pull/34797;;;",,,,,,,,,,,,
We should drop all tables after testing dynamic partition pruning,SPARK-37524,13415159,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,weixiuli,weixiuli,weixiuli,03/Dec/21 02:02,12/Dec/22 18:10,13/Jul/23 08:50,03/Dec/21 03:35,3.0.0,3.0.1,3.0.2,3.0.3,3.1.0,3.1.1,3.1.2,3.2.0,3.0.4,3.1.3,3.2.1,3.3.0,SQL,,,,,0,,,,,We should drop all tables after testing dynamic partition pruning.,,apachespark,weixiuli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 03 03:35:51 UTC 2021,,,,,,,,,,"0|z0xbc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/21 02:11;apachespark;User 'weixiuli' has created a pull request for this issue:
https://github.com/apache/spark/pull/34768;;;","03/Dec/21 03:35;gurwls223;Issue resolved by pull request 34768
[https://github.com/apache/spark/pull/34768];;;",,,,,,,,,,,,,,
 test_reuse_worker_of_parallelize_range is flaky,SPARK-37498,13414458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,30/Nov/21 06:22,12/Dec/22 18:11,13/Jul/23 08:50,17/Jan/22 23:47,3.3.0,,,,,,,,3.2.2,3.3.0,,,PySpark,Tests,,,,0,,,,," 
{code:java}
ERROR [2.132s]: test_reuse_worker_of_parallelize_range (pyspark.tests.test_worker.WorkerReuseTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/__w/spark/spark/python/pyspark/tests/test_worker.py"", line 195, in test_reuse_worker_of_parallelize_range
    self.assertTrue(pid in previous_pids)
AssertionError: False is not true
----------------------------------------------------------------------
Ran 12 tests in 22.589s
{code}
 

 

[1] https://github.com/apache/spark/runs/1182154542?check_suite_focus=true
[2] https://github.com/apache/spark/pull/33657#issuecomment-893969310
[3] https://github.com/Yikun/spark/runs/4362783540?check_suite_focus=true",,apachespark,LuciferYang,yikunkero,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 17 23:47:35 UTC 2022,,,,,,,,,,"0|z0x70o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/21 03:00;gurwls223;maybe we should add eventually https://github.com/apache/spark/blob/2fe9af8b2b91d0a46782dd6fff57eca8609be105/python/pyspark/testing/utils.py#L57 for the time being. I suspect that the worker sometimes die due to intensive CPU usage, or lack of file descriptors available .... ;;;","14/Jan/22 19:57;zero323;??maybe we should add eventually??

This might be a good idea, as this happens a lot.;;;","17/Jan/22 10:28;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35228;;;","17/Jan/22 10:29;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/35228;;;","17/Jan/22 23:47;gurwls223;Issue resolved by pull request 35228
[https://github.com/apache/spark/pull/35228];;;",,,,,,,,,,,
Fix Series.asof when values of the series is not sorted,SPARK-37491,13414415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,pralabhkumar,dchvn,dchvn,30/Nov/21 01:16,14/Mar/22 17:33,13/Jul/23 08:50,14/Mar/22 17:33,3.3.0,,,,,,,,,,,,PySpark,,,,,0,,,,,https://github.com/apache/spark/pull/34737#discussion_r758223279,,apachespark,dchvn,itholic,pralabhkumar,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-37482,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Mar 14 17:33:54 UTC 2022,,,,,,,,,,"0|z0x6r4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jan/22 18:27;pralabhkumar;I would like to work on this . Basically the problem is in series.py . We should not find max here.  

cond = [
F.max(F.when(index_scol <= SF.lit(index).cast(index_type), self.spark.column))
for index in where
]

 

 

cc [~hyukjin.kwon] [~itholic] ;;;","05/Jan/22 23:18;itholic;[~pralabhkumar] Sure, please go ahead!;;;","09/Jan/22 09:25;pralabhkumar;I am working on it .;;;","10/Jan/22 14:23;pralabhkumar;Lets take example of 

pser = pd.Series([2, 1, np.nan, 4], index=[10, 20, 30, 40], name=""Koalas"")

pser.asof([5,20])  will give output [Nan , 1] 

While 

ps.from_pandas(pser).asof[5,20] will give output [Nan, 2]

*Explanation*

Data frame created after applying condition.

F.when(index_scol <= SF.lit(index).cast(index_type)  Without applying max aggregation  

+-------------+--------------+-----------------+

|col_5        |col_25        |__index_level_0__|

+-------------+--------------+-----------------+

|null|2.0|10               |

|null|1.0|20               |

|null|null|30               |

|null|null|40               |

+-------------+--------------+-----------------+

Since we are taking max , output is coming 2. Ideally what we need is the last non null value or each col with increasing order of __index_level_0__.

Now to implement the logic . What I planning to do is create a below DF from the above DF , using explode , partition and row_number

__index_level_0__.        Identifier          value    row_number

40                                      col_5               null.      1

30                                    col_5                null       2

20                                    col_5                null       3

10                                    col_5               null         4

40                                     col_20         2              1

30                                     col_20        1              2

20                                    col_20         null         3

10                                  col_20            null         4  

 

Then filter on row_number=1 . There are other things to take care , but majority of the logic is this .

Please let me know if its in correct direction ( This is actually passing all the asof test cases ,including the  case which is described in jira. ) . 

 

[~itholic]  ;;;","13/Jan/22 09:53;apachespark;User 'pralabhkumar' has created a pull request for this issue:
https://github.com/apache/spark/pull/35191;;;","14/Mar/22 17:33;ueshin;Issue resolved by pull request 35191
https://github.com/apache/spark/pull/35191;;;",,,,,,,,,,
Disappearance of skipped stages mislead the bug hunting ,SPARK-37481,13414228,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,29/Nov/21 07:41,15/Mar/22 03:44,13/Jul/23 08:50,13/Dec/21 02:46,3.1.2,3.2.0,3.3.0,,,,,,3.2.1,3.3.0,,,Spark Core,,,,,0,,,,,"# 
 ## With FetchFailedException and Map Stage Retries

When rerunning spark-sql shell with the original SQL in [https://gist.github.com/yaooqinn/6acb7b74b343a6a6dffe8401f6b7b45c#gistcomment-3977315]

!https://user-images.githubusercontent.com/8326978/143821530-ff498caa-abce-483d-a24b-315aacf7e0a0.png!

1. stage 3 threw FetchFailedException and caused itself and its parent stage(stage 2) to retry
2. stage 2 was skipped before but its attemptId was still 0, so when its retry happened it got removed from `Skipped Stages` 

The DAG of Job 2 doesn't show that stage 2 is skipped anymore.

!https://user-images.githubusercontent.com/8326978/143824666-6390b64a-a45b-4bc8-b05d-c5abbb28cdef.png!

Besides, a retried stage usually has a subset of tasks from the original stage. If we mark it as an original one, the metrics might lead us into pitfalls.",,apachespark,Ngone51,Qin Yao,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Mar 15 03:44:53 UTC 2022,,,,,,,,,,"0|z0x5lk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/21 07:57;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34735;;;","13/Dec/21 02:46;Ngone51;Issue resolved by https://github.com/apache/spark/pull/34735;;;","13/Dec/21 02:47;Ngone51;Backport fix to 3.1/3.0 is still in progress.;;;","15/Mar/22 03:44;Ngone51;Backport fix to 3.1/3.0 also done.
 ;;;",,,,,,,,,,,,
Configurations in docs/running-on-kubernetes.md are not uptodate,SPARK-37480,13414225,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,yikunkero,yikunkero,yikunkero,29/Nov/21 07:26,01/Dec/21 17:02,13/Jul/23 08:50,01/Dec/21 06:49,3.2.0,,,,,,,,3.2.1,3.3.0,,,Kubernetes,,,,,0,,,,,,,apachespark,dongjoon,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 01 15:04:24 UTC 2021,,,,,,,,,,"0|z0x5kw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/21 07:30;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34734;;;","01/Dec/21 06:49;dongjoon;Issue resolved by pull request 34734
[https://github.com/apache/spark/pull/34734];;;","01/Dec/21 15:04;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34770;;;",,,,,,,,,,,,,
PySpark tests failing on Pandas 0.23,SPARK-37465,13413902,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,rshkv,rshkv,25/Nov/21 17:21,12/Dec/22 18:10,13/Jul/23 08:50,30/Nov/21 05:14,3.2.0,,,,,,,,3.3.0,,,,PySpark,,,,,0,,,,,"I was running Spark tests with Pandas {{0.23.4}} and got the error below. The minimum Pandas version is currently {{0.23.2}} [(Github)|https://github.com/apache/spark/blob/v3.2.0/python/setup.py#L114]. Upgrading to {{0.24.0}} fixes the error. I think Spark needs [this fix (Github)|https://github.com/pandas-dev/pandas/pull/21160/files#diff-1b7183f5b3970e2a1d39a82d71686e39c765d18a34231b54c857b0c4c9bb8222] in Pandas.
{code:java}
$ python/run-tests --testnames 'pyspark.pandas.tests.data_type_ops.test_boolean_ops BooleanOpsTest.test_floordiv'

...

======================================================================
ERROR [5.785s]: test_floordiv (pyspark.pandas.tests.data_type_ops.test_boolean_ops.BooleanOpsTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/circleci/project/python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py"", line 128, in test_floordiv
    self.assert_eq(b_pser // b_pser.astype(int), b_psser // b_psser.astype(int))
  File ""/home/circleci/miniconda/envs/python3/lib/python3.6/site-packages/pandas/core/ops.py"", line 1069, in wrapper
    result = safe_na_op(lvalues, rvalues)
  File ""/home/circleci/miniconda/envs/python3/lib/python3.6/site-packages/pandas/core/ops.py"", line 1033, in safe_na_op
    return na_op(lvalues, rvalues)
  File ""/home/circleci/miniconda/envs/python3/lib/python3.6/site-packages/pandas/core/ops.py"", line 1027, in na_op
    result = missing.fill_zeros(result, x, y, op_name, fill_zeros)
  File ""/home/circleci/miniconda/envs/python3/lib/python3.6/site-packages/pandas/core/missing.py"", line 641, in fill_zeros
    signs = np.sign(y if name.startswith(('r', '__r')) else x)
TypeError: ufunc 'sign' did not contain a loop with signature matching types dtype('bool') dtype('bool')
{code}
These are my relevant package versions:
{code:java}
$ conda list | grep -e numpy -e pyarrow -e pandas -e python
# packages in environment at /home/circleci/miniconda/envs/python3:
numpy                     1.16.6           py36h0a8e133_3  
numpy-base                1.16.6           py36h41b4c56_3  
pandas                    0.23.4           py36h04863e7_0  
pyarrow                   1.0.1           py36h6200943_36_cpu    conda-forge
python                    3.6.12               hcff3b4d_2    anaconda
python-dateutil           2.8.1                      py_0    anaconda
python_abi                3.6                     1_cp36m    conda-forg
{code}",,apachespark,rshkv,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-37514,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 30 05:14:39 UTC 2021,,,,,,,,,,"0|z0x3l4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/21 01:59;gurwls223;Maybe we should rather bump up the minimum pandas version to 1.0.0. Would you be interested in submitting a PR? cc [~XinrongM] [~ueshin] FYI;;;","26/Nov/21 02:00;gurwls223;cc [~yikunkero] and [~itholic] FYI;;;","26/Nov/21 14:13;rshkv;I also noticed that {{CategoricalOpsTest}} fails on pandas 0.25.3 (latest 0.x) and works on 1.x:
{code:java}
$ conda list | grep pandas
pandas                    0.25.3           py36he6710b0_0
$ python/run-tests --testnames 'pyspark.pandas.tests.data_type_ops.test_categorical_ops CategoricalOpsTest'
...
Running tests...
----------------------------------------------------------------------
/home/circleci/project/python/pyspark/context.py:238: FutureWarning: Python 3.6 support is deprecated in Spark 3.2.
  FutureWarning
  test_abs (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (2.353s)
  test_add (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (1.382s)
  test_and (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.265s)
ok (6.569s)                                                                     alOpsTest) ... 
  test_eq (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... FAIL (1.514s)
  test_floordiv (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.910s)
  test_from_to_pandas (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.143s)
  test_ge (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... FAIL (0.795s)
  test_gt (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... FAIL (0.891s)
  test_invert (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.044s)
  test_isnull (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.097s)
  test_le (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... FAIL (0.863s)
  test_lt (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... FAIL (0.844s)
  test_mod (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.897s)
  test_mul (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.860s)
  test_ne (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... FAIL (1.405s)
  test_neg (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.044s)
  test_or (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.160s)
  test_pow (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.821s)
  test_radd (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.081s)
  test_rand (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.100s)
  test_rfloordiv (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.083s)
  test_rmod (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.050s)
  test_rmul (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.079s)
  test_ror (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.095s)
  test_rpow (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.078s)
  test_rsub (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.078s)
  test_rtruediv (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.079s)
  test_sub (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.818s)
  test_truediv (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest) ... ok (0.832s)

======================================================================
FAIL [1.611s]: test_eq (pyspark.pandas.tests.data_type_ops.test_categorical_ops.CategoricalOpsTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/circleci/project/python/pyspark/testing/pandasutils.py"", line 122, in assertPandasEqual
    **kwargs
  File ""/home/circleci/.pyenv/versions/our-miniconda/envs/python3/lib/python3.6/site-packages/pandas/util/testing.py"", line 1248, in assert_series_equal
    assert_attr_equal('name', left, right, obj=obj)
  File ""/home/circleci/.pyenv/versions/our-miniconda/envs/python3/lib/python3.6/site-packages/pandas/util/testing.py"", line 941, in assert_attr_equal
    raise_assert_detail(obj, msg, left_attr, right_attr)
AssertionError: Series are different

Attribute ""name"" are different
[left]:  that_numeric_cat
[right]: None

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/home/circleci/project/python/pyspark/pandas/tests/data_type_ops/test_categorical_ops.py"", line 268, in test_eq
    psdf[""this_numeric_cat""] == psdf[""that_numeric_cat""],
  File ""/home/circleci/project/python/pyspark/testing/pandasutils.py"", line 223, in assert_eq
    self.assertPandasEqual(lobj, robj, check_exact=check_exact)
  File ""/home/circleci/project/python/pyspark/testing/pandasutils.py"", line 130, in assertPandasEqual
    raise AssertionError(msg) from e
AssertionError: Series are different

Attribute ""name"" are different
[left]:  that_numeric_cat
[right]: None

Left:
Name: that_numeric_cat, dtype: bool
bool

Right:
dtype: bool
bool

...
{code}
Upgrading pandas to 1.x fixes it:
{code:java}
$ conda list | grep pandas
pandas                    1.0.0            py36h0573a6f_0  
$ python/run-tests --testnames 'pyspark.pandas.tests.data_type_ops.test_categorical_ops CategoricalOpsTest'
Running PySpark tests. Output is in /home/circleci/project/python/unit-tests.log
Will test against the following Python executables: ['python3.6']
Will test the following Python tests: ['pyspark.pandas.tests.data_type_ops.test_categorical_ops CategoricalOpsTest']
python3.6 python_implementation is CPython
python3.6 version is: Python 3.6.12 :: Anaconda, Inc.
Starting test(python3.6): pyspark.pandas.tests.data_type_ops.test_categorical_ops CategoricalOpsTest
Finished test(python3.6): pyspark.pandas.tests.data_type_ops.test_categorical_ops CategoricalOpsTest (34s)
Tests passed in 34 seconds
{code};;;","26/Nov/21 15:35;rshkv;I'll give the pandas bump a shot.;;;","26/Nov/21 17:19;apachespark;User 'rshkv' has created a pull request for this issue:
https://github.com/apache/spark/pull/34724;;;","27/Nov/21 01:04;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34717;;;","27/Nov/21 01:05;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34717;;;","30/Nov/21 05:14;gurwls223;Issue resolved by pull request 34717
[https://github.com/apache/spark/pull/34717];;;",,,,,,,,
Upgrade commons-cli to 1.5.0,SPARK-37459,13413754,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,25/Nov/21 04:33,12/Dec/22 18:10,13/Jul/23 08:50,25/Nov/21 07:11,3.3.0,,,,,,,,3.3.0,,,,Build,,,,,0,,,,,"Currently used commons-cli is too old and contains an issue which affects the behavior of bin/spark-sql

{code}
bin/spark-sql -e 'SELECT ""Spark""'
...
Error in query: 
no viable alternative at input 'SELECT ""'(line 1, pos 7)

== SQL ==
SELECT ""Spark
-------^^^
{code}

The root cause of this issue seems to be resolved in CLI-185.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 25 07:11:07 UTC 2021,,,,,,,,,,"0|z0x2o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/21 04:41;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34707;;;","25/Nov/21 04:42;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34707;;;","25/Nov/21 07:11;gurwls223;Issue resolved by pull request 34707
[https://github.com/apache/spark/pull/34707];;;",,,,,,,,,,,,,
Char and Varchar breaks backward compatibility between v3 and v2,SPARK-37452,13413570,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,24/Nov/21 10:20,30/Nov/21 04:27,13/Jul/23 08:50,29/Nov/21 09:48,3.1.2,3.2.0,,,,,,,3.1.3,3.2.1,3.3.0,,SQL,,,,,0,,,,,"We will store table schema in table properties for the read-side to restore. In Spark 3.1, we add char/varchar support natively. In some commands like `create table`, `alter table` with these types,  the char(n) or varchar(n) will be stored directly to those properties. If a user uses Spark 2 to read such a table it will fail to parse the schema.

A table can be a newly created one by Spark 3.1 and later or an existing one modified by Spark 3.1 and on.  ",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 29 09:58:41 UTC 2021,,,,,,,,,,"0|z0x1j4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Nov/21 10:37;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34697;;;","24/Nov/21 10:38;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34697;;;","29/Nov/21 09:48;Qin Yao;Issue resolved by https://github.com/apache/spark/pull/34697;;;","29/Nov/21 09:58;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/34736;;;",,,,,,,,,,,,
Performance improvement regressed String to Decimal cast,SPARK-37451,13413515,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,yumwang,razajafri,razajafri,24/Nov/21 04:27,12/Dec/22 18:11,13/Jul/23 08:50,09/Dec/21 17:18,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,3.2.1,,,3.1.3,3.2.1,,,Spark Core,,,,,0,correctness,,,,"A performance improvement to how Spark casts Strings to Decimal in this [PR title|https://issues.apache.org/jira/browse/SPARK-32706], has introduced a regression


{noformat}

scala> :paste 
// Entering paste mode (ctrl-D to finish)

import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
spark.conf.set(""spark.sql.legacy.allowNegativeScaleOfDecimal"", true)
spark.conf.set(""spark.rapids.sql.castStringToDecimal.enabled"", true)
spark.conf.set(""spark.rapids.sql.castDecimalToString.enabled"", true)
val data = Seq(Row(""7.836725755512218E38""))
val schema=StructType(Array(StructField(""a"", StringType, false)))
val df =spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
df.select(col(""a"").cast(DecimalType(37,-17))).show


// Exiting paste mode, now interpreting.

+--------------------+
|                   a|
+--------------------+
|7.836725755512218...|
+--------------------+

scala> spark.version
res2: String = 3.0.1


scala> :paste
// Entering paste mode (ctrl-D to finish)

import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
spark.conf.set(""spark.sql.legacy.allowNegativeScaleOfDecimal"", true)
spark.conf.set(""spark.rapids.sql.castStringToDecimal.enabled"", true)
spark.conf.set(""spark.rapids.sql.castDecimalToString.enabled"", true)
val data = Seq(Row(""7.836725755512218E38""))
val schema=StructType(Array(StructField(""a"", StringType, false)))
val df =spark.createDataFrame(spark.sparkContext.parallelize(data), schema)
df.select(col(""a"").cast(DecimalType(37,-17))).show

// Exiting paste mode, now interpreting.

+----+
|   a|
+----+
|null|
+----+

import org.apache.spark.sql.types._
import org.apache.spark.sql.Row
data: Seq[org.apache.spark.sql.Row] = List([7.836725755512218E38])
schema: org.apache.spark.sql.types.StructType = StructType(StructField(a,StringType,false))
df: org.apache.spark.sql.DataFrame = [a: string]

scala> spark.version
res1: String = 3.1.1
{noformat}

",,apachespark,dongjoon,rajesh.balamohan,razajafri,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Dec 09 13:06:33 UTC 2021,,,,,,,,,,"0|z0x16w:",9223372036854775807,,,,,revans2,,,,,,,,3.1.3,3.2.1,,,,,,,,,"24/Nov/21 06:11;gurwls223;cc [~yumwang] FYI;;;","05/Dec/21 14:19;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34811;;;","05/Dec/21 14:19;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34811;;;","08/Dec/21 16:14;dongjoon;According to the JIRA description, I added a correctness label to this PR and set Target Versions: 3.1.3 and 3.2.1.
cc [~huaxingao];;;","09/Dec/21 13:06;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34851;;;",,,,,,,,,,,
"Catalyst optimizer very time-consuming and memory-intensive with some ""explode(array)"" ",SPARK-37392,13412849,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,martinf,martinf,19/Nov/21 19:58,31/Aug/22 22:52,13/Jul/23 08:50,08/Dec/21 05:07,3.1.2,3.2.0,,,,,,,3.1.3,3.2.1,3.3.0,,Optimizer,,,,,0,,,,,"The problem occurs with the simple code below:
{code:java}
import session.implicits._

Seq(
  (1, ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"", ""x"")
).toDF()
  .checkpoint() // or save and reload to truncate lineage
  .createOrReplaceTempView(""sub"")

session.sql(""""""
  SELECT
    *
  FROM
  (
    SELECT
      EXPLODE( ARRAY( * ) ) result
    FROM
    (
      SELECT
        _1 a, _2 b, _3 c, _4 d, _5 e, _6 f, _7 g, _8 h, _9 i, _10 j, _11 k, _12 l, _13 m, _14 n, _15 o, _16 p, _17 q, _18 r, _19 s, _20 t, _21 u
      FROM
        sub
    )
  )
  WHERE
    result != ''
  """""").show() {code}
It takes several minutes and a very high Java heap usage, when it should be immediate.

It does not occur when replacing the unique integer value (1) with a string value ({_}""x""{_}).

All the time is spent in the _PruneFilters_ optimization rule.

Not reproduced in Spark 2.4.1.",,apachespark,cloud_fan,joshrosen,martinf,shivamverma300,viirya,xkrogen,,,,,,,,,,,,,,,,,,,,,,,SPARK-40262,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jan 29 04:50:08 UTC 2022,,,,,,,,,,"0|z0wx34:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/21 23:35;joshrosen;When I ran this in {{spark-shell}} it triggered an OOM in {{{}LogicalPlan.constraints(){}}}. It In the heap dump I spotted an {{ExpressionSet}} with over 100,000 expressions.

Based on the constraints that I saw I think that they were introduced by the {{InferFiltersFromGenerate}} rule and that some sort of unexpected rule interaction is resulting in a huge blowup of derived constraints in {{{}PruneFilters{}}}. The {{InferFiltersFromGenerate}} rule was introduced in SPARK-32295 / Spark 3.1.0, which could explain why this issue isn't reproducible in Spark 2.4.1.

Looking at the constraints in the huge {{{}ExpressionSet{}}}, it looks like the vast majority (> 99%) of the constraints are {{GreaterThan}} or {{LessThan}} constraints of the form:
 * {{GreaterThan(Size(CreateArray(...)), Literal(0))}}
 * {{LessThan(Literal(0), Size(CreateArray(...)))}}

I think the {{GreaterThan}} comes from {{InferFiltersFromGenerate}} and suspect that the {{LessThan}} equivalents are introduced via expression canonicalization.

We'll need to dig a bit deeper to figure out what's leading to this buildup of duplicate constraints. Perhaps it's some sort of interaction between this particular shape of constraint, the constraint propagation system, and canonicalization? I'm not sure yet.;;;","03/Dec/21 14:14;cloud_fan;Great investigation, [~joshrosen] !

I think for this particular case, we should apply constant folding during constraints inferences, to avoid generating a lot of predicates that can be optimized out later. e.g. `Size(CreateArray(...))` can be optimized into a literal.

 ;;;","06/Dec/21 17:49;cloud_fan;I've figured out the root cause. The problem part of the query plan is:
{code:java}
   +- Project [_1#21 AS a#106, _2#22 AS b#107, _3#23 AS c#108, _4#24 AS d#109, _5#25 AS e#110, _6#26 AS f#111, _7#27 AS g#112, _8#28 AS h#113, _9#29 AS i#114, _10#30 AS j#115, _11#31 AS k#116, _12#32 AS l#117, _13#33 AS m#118, _14#34 AS n#119, _15#35 AS o#120, _16#36 AS p#121, _17#37 AS q#122, _18#38 AS r#123, _19#39 AS s#124, _20#40 AS t#125, _21#41 AS u#126]
      +- Filter (size(array(cast(_1#21 as string), _2#22, _3#23, _4#24, _5#25, _6#26, _7#27, _8#28, _9#29, _10#30, _11#31, _12#32, _13#33, _14#34, _15#35, _16#36, _17#37, _18#38, _19#39, _20#40, _21#41), true) > 0)
         +- LogicalRDD [_1#21, _2#22, _3#23, _4#24, _5#25, _6#26, _7#27, _8#28, _9#29, _10#30, _11#31, _12#32, _13#33, _14#34, _15#35, _16#36, _17#37, _18#38, _19#39, _20#40, _21#41] {code}
When calculating the constraints of the Project, the code is
{code:java}
var allConstraints = child.constraints
projectList.foreach {
  case a @ Alias(l: Literal, _) =>
    allConstraints += EqualNullSafe(a.toAttribute, l)
  case a @ Alias(e, _) =>
    // For every alias in `projectList`, replace the reference in constraints by its attribute.
    allConstraints ++= allConstraints.map(_ transform {
      case expr: Expression if expr.semanticEquals(e) =>
        a.toAttribute
    })
    allConstraints += EqualNullSafe(e, a.toAttribute)
  case _ => // Don't change.
} {code}
The `allConstraints` starts with a single `size(...)` predicate, and then we keep doubling it for each alias in the project list, which leads to around 2^20 predicates.

 

> It does not occur when replacing the unique integer value (1) with a string value ({_}""x""{_}).

This is because we don't have the `cast(_1#21 as string)`, and can optimize `size(array(...))` into a constant. The optimizer rule is too conservative, and skips optimizing `size(array(cast(_1#21 as string), ...))` because cast may fail and has side effects.;;;","06/Dec/21 18:53;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34823;;;","06/Dec/21 18:54;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34823;;;","08/Dec/21 05:07;cloud_fan;Issue resolved by pull request 34823
[https://github.com/apache/spark/pull/34823];;;","29/Jan/22 04:50;shivamverma300;Is there a plan to backport the changes to 3.1.2 and 3.2.0?;;;",,,,,,,,,
SIGNIFICANT bottleneck introduced by fix for SPARK-32001,SPARK-37391,13412845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,danny-seismic,danny-seismic,danny-seismic,19/Nov/21 19:27,12/Dec/22 18:11,13/Jul/23 08:50,24/Dec/21 01:11,3.1.0,3.1.1,3.1.2,3.2.0,,,,,3.1.3,3.2.1,3.3.0,,SQL,,,,,0,,,,,"The fix for https://issues.apache.org/jira/browse/SPARK-32001 ( [https://github.com/apache/spark/pull/29024/files#diff-345beef18081272d77d91eeca2d9b5534ff6e642245352f40f4e9c9b8922b085R58] ) does not seem to have consider the reality that some apps may rely on being able to establish many JDBC connections simultaneously for performance reasons.

The fix forces concurrency to 1 when establishing database connections and that strikes me as a *significant* user impacting change and a *significant* bottleneck.

Can anyone propose a workaround for this? I have an app that makes connections to thousands of databases and I can't upgrade to any version >3.1.x because of this significant bottleneck.

 

Thanks in advance for your help!",N/A,apachespark,danny-seismic,gaborgsomogyi,sarutak,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/21 16:18;danny-seismic;so-much-blocking.jpg;https://issues.apache.org/jira/secure/attachment/13036454/so-much-blocking.jpg","22/Nov/21 16:17;danny-seismic;spark-regression-dashes.jpg;https://issues.apache.org/jira/secure/attachment/13036453/spark-regression-dashes.jpg",,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 24 01:11:12 UTC 2021,,,,,,,,,,"0|z0wx28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/21 00:09;gurwls223;[~danny-seismic], it would be great to assess this issue futher with better problem description and, e.g.) preferably self-contained reproducer;;;","22/Nov/21 16:19;danny-seismic;[~hyukjin.kwon] , sorry, I seem to have gotten confused when identifying the source of the regression. I have updated the title and description to reflect the true source of the issue. I'm inclined to blame this change: https://github.com/apache/spark/pull/29024/files#diff-345beef18081272d77d91eeca2d9b5534ff6e642245352f40f4e9c9b8922b085R58

 

I'm sorry, but I don't have the capacity to provide a self-contained reproduction of the issue. Hopefully the problem is obvious enough that you will be able to see what is going on from the anecdotal evidence I can provide.

The introduction of SecurityConfigurationLock.synchronized prevents a given JDBC Driver from establishing more than one connection at a time (or at least severely limits the concurrency). This is a significant bottleneck for applications that use a single JDBC driver to establish many database connections.

The anecdotal evidence I can offer to support this claim:

1. I've attached a screenshot of some dashboards we use to monitor the QA deployment of the application in question. These graphs come from a 4.5 hour window where I had spark 3.1.2 deployed to QA. On the left side of the graph we were running Spark 2.4.5; in the middle we were running spark 3.1.2; and on the right side of the graph we are running spark 3.0.1.
 # The ""Success Rate"", ""CountActiveTasks"", ""CountActiveJobs"", ""CountTableTenantJobStart"", ""CountTableTenantJobEnd"" graphs all aim to demonstrate that with the deployment of spark 3.1.2 the throughput of the application was significantly reduced across the board.
 # The ""Overall Active Thread Count"", ""Count Active Executors"", and ""CountDeadExecutors"" graphs all aim to evidence that there was no change in the number of resources allocated to do work.
 # The ""Max MinsSinceLastAttempt"" graph should normally be a flat line unless the application is falling behind on the work that it is scheduled to do. It can be seen during the period of the spark 3.1.2 deployment the application is falling behind at a linear rate and begins to recover once spark 3.0.1 is deployed.

!spark-regression-dashes.jpg!

 

2. I've attached a screenshot of the thread dump from the spark driver process. It can be seen that many, many threads are blocked waiting for SecurityConfigurationLock. The screenshot only shows a handful of threads but there are 98 threads in total blocked wiating for the SecurityConfigurationLock.

!so-much-blocking.jpg!

 

It's worth noting that our QA deployment does significantly less work than our production deployment; if the QA deployment can't keep up then the production deployment has no chance. On the bright side, I had success updating the production deployment to spark 3.0.1 and that seems to be stable. Unfortunately, we use Databricks for our spark vendor and the LTS release they have that supports spark 3.0.1 is only scheduled to be maintained until September 2022, so we can't avoid this regression forever.

 

If I can answer any questions or provide any more info, please let me know. Thanks in advance!

 ;;;","22/Nov/21 16:48;danny-seismic;Here's an example stacktrace for one of the blocked threads:

{{org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:92)}}
{{org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:63)}}
{{org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$$Lambda$6294/1994845663.apply(Unknown Source)}}
{{org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)}}
{{org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)}}
{{org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)}}
{{org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:390)}}
{{org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:444)}}
{{org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:400)}}
{{org.apache.spark.sql.DataFrameReader$$Lambda$6224/1118373872.apply(Unknown Source)}}
{{scala.Option.getOrElse(Option.scala:189)}}
{{org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:400)}}
{{org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:273)}}
{{<application specific stack frames removed>}}
{{scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)}}
{{scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)}}
{{scala.concurrent.Future$$$Lambda$442/341778327.apply(Unknown Source)}}
{{scala.util.Success.$anonfun$map$1(Try.scala:255)}}
{{scala.util.Success.map(Try.scala:213)}}
{{scala.concurrent.Future.$anonfun$map$1(Future.scala:292)}}
{{scala.concurrent.Future$$Lambda$443/424848797.apply(Unknown Source)}}
{{scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)}}
{{scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)}}
{{scala.concurrent.impl.Promise$$Lambda$444/1710905079.apply(Unknown Source)}}
{{scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)}}
{{java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}
{{java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}
{{java.lang.Thread.run(Thread.java:748)}}

 

 

The stacktrace from the thread that is holding the lock looks like so:

{{java.net.SocketInputStream.socketRead0(Native Method)}}
{{java.net.SocketInputStream.socketRead(SocketInputStream.java:116)}}
{{java.net.SocketInputStream.read(SocketInputStream.java:171)}}
{{java.net.SocketInputStream.read(SocketInputStream.java:141)}}
{{com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.readInternal(IOBuffer.java:1019)}}
{{com.microsoft.sqlserver.jdbc.TDSChannel$ProxyInputStream.read(IOBuffer.java:1009)}}
{{sun.security.ssl.SSLSocketInputRecord.read(SSLSocketInputRecord.java:476)}}
{{sun.security.ssl.SSLSocketInputRecord.readHeader(SSLSocketInputRecord.java:470)}}
{{sun.security.ssl.SSLSocketInputRecord.bytesInCompletePacket(SSLSocketInputRecord.java:70)}}
{{sun.security.ssl.SSLSocketImpl.readApplicationRecord(SSLSocketImpl.java:1364)}}
{{sun.security.ssl.SSLSocketImpl.access$300(SSLSocketImpl.java:73)}}
{{sun.security.ssl.SSLSocketImpl$AppInputStream.read(SSLSocketImpl.java:973)}}
{{com.microsoft.sqlserver.jdbc.TDSChannel.read(IOBuffer.java:2058)}}
{{com.microsoft.sqlserver.jdbc.TDSReader.readPacket(IOBuffer.java:6617) => holding Monitor(com.microsoft.sqlserver.jdbc.TDSReader@1035497922})}}
{{com.microsoft.sqlserver.jdbc.TDSCommand.startResponse(IOBuffer.java:7805)}}
{{com.microsoft.sqlserver.jdbc.TDSCommand.startResponse(IOBuffer.java:7768)}}
{{com.microsoft.sqlserver.jdbc.SQLServerConnection.sendLogon(SQLServerConnection.java:5332)}}
{{com.microsoft.sqlserver.jdbc.SQLServerConnection.logon(SQLServerConnection.java:4066)}}
{{com.microsoft.sqlserver.jdbc.SQLServerConnection.access$000(SQLServerConnection.java:85)}}
{{com.microsoft.sqlserver.jdbc.SQLServerConnection$LogonCommand.doExecute(SQLServerConnection.java:4004)}}
{{com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:7418)}}
{{com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLServerConnection.java:3272) => holding Monitor(java.lang.Object@564746804})}}
{{com.microsoft.sqlserver.jdbc.SQLServerConnection.connectHelper(SQLServerConnection.java:2768)}}
{{com.microsoft.sqlserver.jdbc.SQLServerConnection.login(SQLServerConnection.java:2418)}}
{{com.microsoft.sqlserver.jdbc.SQLServerConnection.connectInternal(SQLServerConnection.java:2265)}}
{{com.microsoft.sqlserver.jdbc.SQLServerConnection.connect(SQLServerConnection.java:1291)}}
{{com.microsoft.sqlserver.jdbc.SQLServerDriver.connect(SQLServerDriver.java:881)}}
{{org.apache.spark.sql.execution.datasources.jdbc.connection.BasicConnectionProvider.getConnection(BasicConnectionProvider.scala:49)}}
{{org.apache.spark.sql.execution.datasources.jdbc.connection.ConnectionProviderBase.create(ConnectionProvider.scala:94) => holding Monitor(org.apache.spark.security.SecurityConfigurationLock$@404826344})}}
{{org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$createConnectionFactory$1(JdbcUtils.scala:63)}}
{{org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$$Lambda$6294/1994845663.apply(Unknown Source)}}
{{org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:56)}}
{{org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation$.getSchema(JDBCRelation.scala:226)}}
{{org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:35)}}
{{org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:390)}}
{{org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:444)}}
{{org.apache.spark.sql.DataFrameReader.$anonfun$load$3(DataFrameReader.scala:400)}}
{{org.apache.spark.sql.DataFrameReader$$Lambda$6224/1118373872.apply(Unknown Source)}}
{{scala.Option.getOrElse(Option.scala:189)}}
{{org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:400)}}
{{org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:273)}}
{{<application specific stack frames removed>}}
{{scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)}}
{{scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)}}
{{scala.concurrent.Future$$$Lambda$442/341778327.apply(Unknown Source)}}
{{scala.util.Success.$anonfun$map$1(Try.scala:255)}}
{{scala.util.Success.map(Try.scala:213)}}
{{scala.concurrent.Future.$anonfun$map$1(Future.scala:292)}}
{{scala.concurrent.Future$$Lambda$443/424848797.apply(Unknown Source)}}
{{scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)}}
{{scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)}}
{{scala.concurrent.impl.Promise$$Lambda$444/1710905079.apply(Unknown Source)}}
{{scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)}}
{{java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}
{{java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}
{{java.lang.Thread.run(Thread.java:748)}};;;","23/Nov/21 00:01;gurwls223;cc [~gaborgsomogyi] FYI;;;","23/Nov/21 08:25;gaborgsomogyi;[~hyukjin.kwon] thanks for pinging me. I've added my comment here: https://github.com/apache/spark/pull/29024/files#r754476290
The problem and the surrounding constraints are provided. If somebody has a meaningful solution then please share.

To sum it up here: A single JVM has only one security context and JDBC clients are able to read authentication credentials only from there which is a bottleneck.
;;;","29/Nov/21 16:03;apachespark;User 'tdg5' has created a pull request for this issue:
https://github.com/apache/spark/pull/34745;;;","29/Nov/21 16:04;apachespark;User 'tdg5' has created a pull request for this issue:
https://github.com/apache/spark/pull/34745;;;","22/Dec/21 19:58;apachespark;User 'tdg5' has created a pull request for this issue:
https://github.com/apache/spark/pull/34988;;;","22/Dec/21 20:31;apachespark;User 'tdg5' has created a pull request for this issue:
https://github.com/apache/spark/pull/34989;;;","22/Dec/21 20:32;apachespark;User 'tdg5' has created a pull request for this issue:
https://github.com/apache/spark/pull/34989;;;","22/Dec/21 20:35;danny-seismic;I've created three PRs to facilitate backporting this change to 3.1.3:


master PR: [https://github.com/apache/spark/pull/34745]

branch-3.1 PR: [https://github.com/apache/spark/pull/34988]

branch-3.2 PR: [https://github.com/apache/spark/pull/34989] ;;;","24/Dec/21 01:11;sarutak;Issue resolved in https://github.com/apache/spark/pull/34745 for Spark 3.3.0.;;;",,,,
Buggy method retrival in pyspark.docs.conf.setup,SPARK-37390,13412778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,zero323,zero323,zero323,19/Nov/21 14:06,20/Nov/21 10:46,13/Jul/23 08:50,20/Nov/21 10:46,3.1.0,3.2.0,3.3.0,,,,,,3.1.3,3.2.1,3.3.0,,Documentation,PySpark,,,,0,,,,,"[Currently we have this code|https://github.com/apache/spark/blob/04af08e9b6d692ed4b5df5581e39de138067211c/python/docs/source/conf.py#L374-L376]

{code:python}
def setup(app):
    # The app.add_javascript() is deprecated.
    getattr(app, ""add_js_file"", getattr(app, ""add_javascript""))('copybutton.js')
{code}

where nested {{getattr}} should handle compatibility issues between different Sphinx versions.

However, {{getattr(app, ""add_javascript""))}} is missing {{default}} and will fail in latest Sphinx version, without ever falling back to {{getattr(app, ""add_js_file"")}}

 ",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 20 10:46:33 UTC 2021,,,,,,,,,,"0|z0wwnc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/21 14:22;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/34669;;;","20/Nov/21 10:46;zero323;Issue resolved by pull request 34669
[https://github.com/apache/spark/pull/34669];;;",,,,,,,,,,,,,,
WidthBucket throws NullPointerException in WholeStageCodegenExec,SPARK-37388,13412764,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tomvanbussel,tomvanbussel,tomvanbussel,19/Nov/21 11:59,22/Nov/21 08:26,13/Jul/23 08:50,22/Nov/21 08:26,3.2.0,,,,,,,,3.1.3,3.2.1,3.3.0,,SQL,,,,,0,,,,,"Repro: Disable ConstantFolding and run
{code:java}
SELECT width_bucket(3.5, 3.0, 3.0, 888) {code}",,apachespark,tomvanbussel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 19 14:52:41 UTC 2021,,,,,,,,,,"0|z0wwk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/21 14:52;apachespark;User 'tomvanbussel' has created a pull request for this issue:
https://github.com/apache/spark/pull/34670;;;","19/Nov/21 14:52;apachespark;User 'tomvanbussel' has created a pull request for this issue:
https://github.com/apache/spark/pull/34670;;;",,,,,,,,,,,,,,
StatCounter should use mergeStats when merging with self.,SPARK-37374,13412646,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,18/Nov/21 22:00,12/Dec/22 18:11,13/Jul/23 08:50,19/Nov/21 02:16,3.3.0,,,,,,,,3.3.0,,,,PySpark,Spark Core,,,,0,,,,,"{{StatCounter}} should use {{mergeStats}} instead of {{merge}} when merging with {{self}}.

This is a long standing bug but usually this bug won't be hit unless users explicitly use {{mergeStats}} with {{self}}.",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 19 02:16:30 UTC 2021,,,,,,,,,,"0|z0wvu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/21 22:20;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/34653;;;","19/Nov/21 02:16;gurwls223;Issue resolved by pull request 34653
[https://github.com/apache/spark/pull/34653];;;",,,,,,,,,,,,,,
Add fine grained locking to BlockInfoManager,SPARK-37356,13412269,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hvanhovell,hvanhovell,hvanhovell,17/Nov/21 12:53,18/Nov/21 21:23,13/Jul/23 08:50,18/Nov/21 21:23,3.2.0,,,,,,,,3.3.0,,,,Spark Core,,,,,0,,,,,,,apachespark,hvanhovell,rajesh.balamohan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 17 13:20:02 UTC 2021,,,,,,,,,,"0|z0wti8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/21 13:20;apachespark;User 'hvanhovell' has created a pull request for this issue:
https://github.com/apache/spark/pull/34632;;;",,,,,,,,,,,,,,,
Pin `docutils` in branch-3.1,SPARK-37323,13411684,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Nov/21 21:50,12/Dec/22 18:10,13/Jul/23 08:50,14/Nov/21 23:49,3.1.3,,,,,,,,3.1.3,,,,Project Infra,,,,,0,,,,,"`docutils` 0.18 is released October 26 and causes Python linter failure in branch-3.1.
- https://pypi.org/project/docutils/#history
- https://github.com/apache/spark/commits/branch-3.1

{code}
Exception occurred:
  File ""/Users/dongjoon/.pyenv/versions/3.10.0/lib/python3.10/site-packages/docutils/writers/html5_polyglot/__init__.py"", line 445, in section_title_tags
    if (ids and self.settings.section_self_link
AttributeError: 'Values' object has no attribute 'section_self_link'
The full traceback has been saved in /var/folders/mq/c32xpgtj4tj19vt8b10wp8rc0000gn/T/sphinx-err-2h05ytx2.log, if you want to report the issue to the developers.
Please also report this if it was a user error, so that a better error message can be provided next time.
A bug report can be filed in the tracker at <https://github.com/sphinx-doc/sphinx/issues>. Thanks!
make: *** [html] Error 2
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,SPARK-37299,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 14 23:49:33 UTC 2021,,,,,,,,,,"0|z0wpww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/21 22:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34591;;;","14/Nov/21 23:49;gurwls223;Issue resolved by pull request 34591
[https://github.com/apache/spark/pull/34591];;;",,,,,,,,,,,,,,
Delete py_container_checks.zip after the test in DepsTestsSuite finishes,SPARK-37320,13411656,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,14/Nov/21 08:52,15/Nov/21 18:26,13/Jul/23 08:50,14/Nov/21 16:45,3.2.0,,,,,,,,3.1.3,3.2.1,3.3.0,,Kubernetes,Tests,,,,0,,,,,"When K8s integration tests run, py_container_checks.zip  still remains in resource-managers/kubernetes/integration-tests/tests/.
It's is created in the test ""Launcher python client dependencies using a zip file"" in DepsTestsSuite.",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Nov 14 16:45:54 UTC 2021,,,,,,,,,,"0|z0wpqo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/21 09:07;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34588;;;","14/Nov/21 16:45;dongjoon;Issue resolved by pull request 34588
[https://github.com/apache/spark/pull/34588];;;",,,,,,,,,,,,,,
Make FallbackStorageSuite robust in terms of DNS,SPARK-37318,13411649,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,14/Nov/21 03:16,24/Feb/22 17:37,13/Jul/23 08:50,14/Nov/21 09:02,3.2.0,3.3.0,,,,,,,3.2.1,3.3.0,,,Spark Core,Tests,,,,0,,,,,"Usually, the test case expects the hostname doesn't exist.
{code}
$ ping remote
ping: cannot resolve remote: Unknown host
{code}

In some DNS environments, it returns always.
{code}
$ ping remote
PING remote (23.217.138.110): 56 data bytes
64 bytes from 23.217.138.110: icmp_seq=0 ttl=57 time=8.660 ms
{code}",,apachespark,dongjoon,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Feb 24 17:37:33 UTC 2022,,,,,,,,,,"0|z0wpp4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/21 03:21;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34585;;;","14/Nov/21 03:22;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34585;;;","14/Nov/21 09:02;dongjoon;Issue resolved by pull request 34585
[https://github.com/apache/spark/pull/34585];;;","23/Feb/22 16:39;xkrogen;Note that the changes in this PR were reverted in SPARK-38062, in favor of a solution which fixes the production code rather than disabling the test case in certain environments.;;;","24/Feb/22 17:21;dongjoon;It's wrong in branch-3.2, [~xkrogen]. Please be careful about the Affected Versions.
bq. Note that the changes in this PR were reverted in SPARK-38062, in favor of a solution which fixes the production code rather than disabling the test case in certain environments.;;;","24/Feb/22 17:24;dongjoon;For a record,
- Apache Spark 3.2.1 ~ 3.2.x has this test case fix.
- For Apache Spark 3.3, SPARK-38062 improvement patch removes the restriction and reverted this test code change of SPARK-37318 logically.;;;","24/Feb/22 17:37;xkrogen;Great point [~dongjoon], thanks for pointing it out!;;;",,,,,,,,,
Mitigate ConcurrentModificationException thrown from a test in MLEventSuite,SPARK-37315,13411627,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,13/Nov/21 15:36,13/Nov/21 18:45,13/Jul/23 08:50,13/Nov/21 18:45,3.3.0,,,,,,,,3.3.0,,,,ML,Tests,,,,0,,,,,"Recently, I notice ConcurrentModificationException is sometimes thrown from the following part of the test ""pipeline read/write events"" in MLEventSuite when Scala 2.13 is used.
{code}
events.map(JsonProtocol.sparkEventToJson).foreach { event =>
  assert(JsonProtocol.sparkEventFromJson(event).isInstanceOf[MLEvent])
}
{code}

I think the root cause is the ArrayBuffer (events) is updated asynchronously by the following part.
{code}
private val listener: SparkListener = new SparkListener {
  override def onOtherEvent(event: SparkListenerEvent): Unit = event match {
    case e: MLEvent => events.append(e)
    case _ =>
  }
}
{code}",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 13 18:45:12 UTC 2021,,,,,,,,,,"0|z0wpk8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/21 16:05;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34583;;;","13/Nov/21 18:45;dongjoon;Issue resolved by pull request 34583
[https://github.com/apache/spark/pull/34583];;;",,,,,,,,,,,,,,
Upgrade kubernetes-client to 5.10.1,SPARK-37314,13411580,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,sarutak,sarutak,13/Nov/21 04:13,13/Nov/21 08:49,13/Jul/23 08:50,13/Nov/21 08:49,3.3.0,,,,,,,,3.3.0,,,,Build,Kubernetes,,,,0,,,,,"kubernetes-client 5.10.0 and 5.10.1 were released, which include some bug fixes.

https://github.com/fabric8io/kubernetes-client/releases/tag/v5.10.0
https://github.com/fabric8io/kubernetes-client/releases/tag/v5.10.1

Especially, the connection leak issue would affect Spark.
https://github.com/fabric8io/kubernetes-client/issues/3561",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Nov 13 08:49:55 UTC 2021,,,,,,,,,,"0|z0wp9s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/21 04:19;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34579;;;","13/Nov/21 08:49;dongjoon;Issue resolved by pull request 34579
[https://github.com/apache/spark/pull/34579];;;",,,,,,,,,,,,,,
Flaky Test: DDLParserSuite.create view -- basic,SPARK-37308,13411539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,dongjoon,dongjoon,12/Nov/21 18:34,12/Dec/22 18:11,13/Jul/23 08:50,18/Nov/21 04:17,3.3.0,,,,,,,,3.3.0,,,,SQL,Tests,,,,0,,,,,"- https://github.com/apache/spark/runs/4188475706
- https://github.com/apache/spark/runs/4185560718
- https://github.com/apache/spark/runs/4174326895
- https://github.com/apache/spark/runs/4197748386

 

!Screen Shot 2021-11-12 at 10.34.32 AM.png!",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-37367,,,,,"12/Nov/21 18:34;dongjoon;Screen Shot 2021-11-12 at 10.34.32 AM.png;https://issues.apache.org/jira/secure/attachment/13036048/Screen+Shot+2021-11-12+at+10.34.32+AM.png",,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 18 04:17:14 UTC 2021,,,,,,,,,,"0|z0wp0o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/21 18:58;dongjoon;This is a long standing issue I've observed before.
cc [~hyukjin.kwon];;;","13/Nov/21 06:34;gurwls223;Thanks for cc'ing me [~dongjoon]!;;;","18/Nov/21 02:20;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/34639;;;","18/Nov/21 04:17;gurwls223;Fixed in https://github.com/apache/spark/pull/34639;;;",,,,,,,,,,,,
Explicitly download the dependencies of guava and jetty-io in test-dependencies.sh,SPARK-37302,13411454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,12/Nov/21 10:19,24/Dec/21 02:16,13/Jul/23 08:50,12/Nov/21 19:14,3.2.0,,,,,,,,3.2.1,3.3.0,,,Build,,,,,0,,,,,"dev/run-tests.py fails if Scala 2.13 is used and guava or jetty-io is not in the both of Maven and Coursier local repository.
{code:java}
$ rm -rf ~/.m2/repository/*
$ # For Linux
$ rm -rf ~/.cache/coursier/v1/*
$ # For macOS
$ rm -rf ~/Library/Caches/Coursier/v1/*
$ dev/change-scala-version.sh 2.13
$ dev/test-dependencies.sh
$ build/sbt -Pscala-2.13 clean compile
...
[error] /home/kou/work/oss/spark-scala-2.13/common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java:24:1:  error: package com.google.common.primitives does not exist
[error] import com.google.common.primitives.Ints;
[error]                                    ^
[error] /home/kou/work/oss/spark-scala-2.13/common/network-common/src/main/java/org/apache/spark/network/client/TransportClientFactory.java:30:1:  error: package com.google.common.annotations does not exist
[error] import com.google.common.annotations.VisibleForTesting;
[error]                                     ^
[error] /home/kou/work/oss/spark-scala-2.13/common/network-common/src/main/java/org/apache/spark/network/client/TransportClientFactory.java:31:1:  error: package com.google.common.base does not exist
[error] import com.google.common.base.Preconditions;
...
{code}
{code:java}
[error] /home/kou/work/oss/spark-scala-2.13/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala:87:25: Class org.eclipse.jetty.io.ByteBufferPool not found - continuing with a stub.
[error]     val connector = new ServerConnector(
[error]                         ^
[error] /home/kou/work/oss/spark-scala-2.13/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionServer.scala:87:21: multiple constructors for ServerConnector with alternatives:
[error]   (x$1: org.eclipse.jetty.server.Server,x$2: java.util.concurrent.Executor,x$3: org.eclipse.jetty.util.thread.Scheduler,x$4: org.eclipse.jetty.io.ByteBufferPool,x$5: Int,x$6: Int,x$7: org.eclipse.jetty.server.ConnectionFactory*)org.eclipse.jetty.server.ServerConnector <and>
[error]   (x$1: org.eclipse.jetty.server.Server,x$2: org.eclipse.jetty.util.ssl.SslContextFactory,x$3: org.eclipse.jetty.server.ConnectionFactory*)org.eclipse.jetty.server.ServerConnector <and>
[error]   (x$1: org.eclipse.jetty.server.Server,x$2: org.eclipse.jetty.server.ConnectionFactory*)org.eclipse.jetty.server.ServerConnector <and>
[error]   (x$1: org.eclipse.jetty.server.Server,x$2: Int,x$3: Int,x$4: org.eclipse.jetty.server.ConnectionFactory*)org.eclipse.jetty.server.ServerConnector
[error]  cannot be invoked with (org.eclipse.jetty.server.Server, Null, org.eclipse.jetty.util.thread.ScheduledExecutorScheduler, Null, Int, Int, org.eclipse.jetty.server.HttpConnectionFactory)
[error]     val connector = new ServerConnector(
[error]                     ^
[error] /home/kou/work/oss/spark-scala-2.13/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala:207:13: Class org.eclipse.jetty.io.ClientConnectionFactory not found - continuing with a stub.
[error]         new HttpClient(new HttpClientTransportOverHTTP(numSelectors), null)
[error]             ^
[error] /home/kou/work/oss/spark-scala-2.13/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala:287:25: multiple constructors for ServerConnector with alternatives:
[error]   (x$1: org.eclipse.jetty.server.Server,x$2: java.util.concurrent.Executor,x$3: org.eclipse.jetty.util.thread.Scheduler,x$4: org.eclipse.jetty.io.ByteBufferPool,x$5: Int,x$6: Int,x$7: org.eclipse.jetty.server.ConnectionFactory*)org.eclipse.jetty.server.ServerConnector <and>
[error]   (x$1: org.eclipse.jetty.server.Server,x$2: org.eclipse.jetty.util.ssl.SslContextFactory,x$3: org.eclipse.jetty.server.ConnectionFactory*)org.eclipse.jetty.server.ServerConnector <and>
[error]   (x$1: org.eclipse.jetty.server.Server,x$2: org.eclipse.jetty.server.ConnectionFactory*)org.eclipse.jetty.server.ServerConnector <and>
[error]   (x$1: org.eclipse.jetty.server.Server,x$2: Int,x$3: Int,x$4: org.eclipse.jetty.server.ConnectionFactory*)org.eclipse.jetty.server.ServerConnector
[error]  cannot be invoked with (org.eclipse.jetty.server.Server, Null, org.eclipse.jetty.util.thread.ScheduledExecutorScheduler, Null, Int, Int, org.eclipse.jetty.server.ConnectionFactory)
[error]         val connector = new ServerConnector(
{code}
The reason is that exec-maven-plugin used in test-dependencies.sh downloads pom of guava and jetty-io but doesn't downloads the corresponding jars, and skip dependency testing if Scala 2.13 is used (if dependency testing runs, Maven downloads those jars).
{code}
if [[ ""$SCALA_BINARY_VERSION"" != ""2.12"" ]]; then
  # TODO(SPARK-36168) Support Scala 2.13 in dev/test-dependencies.sh
  echo ""Skip dependency testing on $SCALA_BINARY_VERSION""
  exit 0
fi
{code}
{code:java}
$ find ~/.m2 -name ""guava*""
...
/home/kou/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.pom
/home/kou/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.pom.sha1
...
/home/kou/.m2/repository/com/google/guava/guava-parent/14.0.1/guava-parent-14.0.1.pom
/home/kou/.m2/repository/com/google/guava/guava-parent/14.0.1/guava-parent-14.0.1.pom.sha1
...

$ find ~/.m2 -name ""jetty*""
...
/home/kou/.m2/repository/org/eclipse/jetty/jetty-io/9.4.43.v20210629/jetty-io-9.4.43.v20210629.pom
/home/kou/.m2/repository/org/eclipse/jetty/jetty-io/9.4.43.v20210629/jetty-io-9.4.43.v20210629.pom.sha1
...
{code}


Under the circumstances, building Spark using SBT fails.
run-tests.py builds Spark using SBT after the dependency testing so run-tests.py fails with Scala 2.13.

Further, I noticed that this issue can even happen with Sala 2.12 if the script exits at the following part.
{code}
if [ $? != 0 ]; then
  echo -e ""Error while getting version string from Maven:\n$OLD_VERSION""
  exit 1
fi 
{code}

This phenomenon is similar to SPARK-34762.",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Dec 24 02:16:54 UTC 2021,,,,,,,,,,"0|z0wohs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/21 11:09;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34570;;;","12/Nov/21 19:14;dongjoon;Issue resolved by pull request 34570
[https://github.com/apache/spark/pull/34570];;;","24/Dec/21 02:16;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/35006;;;",,,,,,,,,,,,,
Exponential planning time in case of non-deterministic function,SPARK-37290,13411254,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kupferk,kupferk,kupferk,11/Nov/21 11:28,22/Feb/22 05:52,13/Jul/23 08:50,22/Feb/22 05:52,3.1.2,,,,,,,,3.1.3,3.2.2,3.3.0,,SQL,,,,,0,,,,,"We are experiencing an exponential growth of processing time in case of some DataFrame queries including non-deterministic functions. I could create a small example program, which can be pasted into the Spark shell for reproducing the issue:
{code:scala}
val adselect_raw = spark.createDataFrame(Seq((""imp-1"",1),(""imp-2"",2)))
    .cache()
val adselect = adselect_raw.select(
        expr(""uuid()"").alias(""userUuid""),
        expr(""_1"").alias(""impressionUuid""),
        expr(""_1"").alias(""accessDateTime""),
        expr(""_1"").alias(""publisher""),
        expr(""_1"").alias(""site""),
        expr(""_1"").alias(""placement""),
        expr(""_1"").alias(""advertiser""),
        expr(""_1"").alias(""campaign""),
        expr(""_1"").alias(""lineItem""),
        expr(""_1"").alias(""creative""),
        expr(""_1"").alias(""browserLanguage""),
        expr(""_1"").alias(""geoLocode""),
        expr(""_1"").alias(""osFamily""),
        expr(""_1"").alias(""osName""),
        expr(""_1"").alias(""browserName""),
        expr(""_1"").alias(""referrerDomain""),
        expr(""_1"").alias(""placementIabCategory""),
        expr(""_1"").alias(""placementDeviceGroup""),
        expr(""_1"").alias(""placementDevice""),
        expr(""_1"").alias(""placementVideoType""),
        expr(""_1"").alias(""placementSection""),
        expr(""_1"").alias(""placementPlayer""),
        expr(""_1"").alias(""demandType""),
        expr(""_1"").alias(""techCosts""),
        expr(""_1"").alias(""mediaCosts""),
        expr(""_1"").alias(""directSPrice""),
        expr(""_1"").alias(""network""),
        expr(""_1"").alias(""deviceSetting""),
        expr(""_1"").alias(""placementGroup""),
        expr(""_1"").alias(""postalCode""),
        expr(""_1"").alias(""householdId"")
    )

val adcount_raw = spark.createDataFrame(Seq((""imp-1"", 1), (""imp-2"", 2)))
val adcount = adcount_raw.select(
        expr(""_1"").alias(""impressionUuid""),
        expr(""_2"").alias(""accessDateTime"")
    )

val result =  adselect.join(adcount, Seq(""impressionUuid""))
result.explain()
{code}
Further reducing the program (for example by removing the join or the cache) did not show the problem any more.

The problem occurs during planning time and debugging lead me to the function {{UnaryNode.getAllValidConstraints}} where the local variable {{allConstraints}} grew with an apparently exponential number of entries for the non-deterministic function ""{{{}uuid(){}}}"" in the code example above. Every time a new column from the large select is processed in the {{foreach}} loop in the function {{{}UnaryNode.getAllValidConstraints{}}}, the number of entries for the {{uuid()}} column in the ExpressionSet seems to be doubled:
{code:scala}
trait UnaryNode extends LogicalPlan with UnaryLike[LogicalPlan] {
  override def getAllValidConstraints(projectList: Seq[NamedExpression]): ExpressionSet = {
    var allConstraints = child.constraints
    projectList.foreach {
      case a @ Alias(l: Literal, _) =>
        allConstraints += EqualNullSafe(a.toAttribute, l)
      case a @ Alias(e, _) =>
        // KK: Since the ExpressionSet handles each non-deterministic function as a separate entry, each ""uuid()"" entry in allConstraints is re-added over an over again in every iteration, 
        // thereby doubling the list every time    
        allConstraints ++= allConstraints.map(_ transform {
          case expr: Expression if expr.semanticEquals(e) =>
            a.toAttribute
        })
        allConstraints += EqualNullSafe(e, a.toAttribute)
      case _ => // Don't change.
    }

    allConstraints
  }
}
{code}
As a workaround, we moved the {{uuid()}} column in our code to the end of the list in the select statement, which solved the issue (since all other columns were already processed in the {{foreach}} loop).",,apachespark,cloud_fan,igor.berman,kupferk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Feb 22 05:52:35 UTC 2022,,,,,,,,,,"0|z0wn9c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/22 19:27;apachespark;User 'Stelyus' has created a pull request for this issue:
https://github.com/apache/spark/pull/35231;;;","17/Jan/22 22:00;apachespark;User 'Stelyus' has created a pull request for this issue:
https://github.com/apache/spark/pull/35233;;;","22/Feb/22 05:52;cloud_fan;Issue resolved by pull request 35233
[https://github.com/apache/spark/pull/35233];;;",,,,,,,,,,,,,
Backport update pyspark.since annotation to 3.1 and 3.2,SPARK-37288,13411239,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,zero323,zero323,zero323,11/Nov/21 10:08,24/Jan/22 23:13,13/Jul/23 08:50,24/Jan/22 23:13,3.1.0,3.2.0,,,,,,,3.1.3,3.2.1,,,PySpark,,,,,0,,,,,"Annotation for {{pyspark.since}} has been changed in SPARK-36906, from

{code:python}
def since(version: str) -> Callable[[T], T]: ...
{code}

to

{code:python}
def since(version: Union[str, float]) -> Callable[[T], T]: ...
{code}

We should backport this change to 3.1 and 3.2 (see for example https://github.com/delta-io/delta/pull/305)

This is technically a bug in the downstream projects that runs mypy checks against pyspark.since. When they use it, for example, with {{pyspark.since(3.2)}}, mypy checks fails; however, this case is legitimate. After this change, the mypy change can pass.

",,apachespark,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jan 24 23:13:16 UTC 2022,,,,,,,,,,"0|z0wn60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/21 10:24;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/34555;;;","11/Nov/21 10:25;apachespark;User 'zero323' has created a pull request for this issue:
https://github.com/apache/spark/pull/34555;;;","24/Jan/22 23:13;zero323;Issue resolved by pull request 34555

https://github.com/apache/spark/pull/34555;;;",,,,,,,,,,,,,
Spark OOM issue,SPARK-37271,13411103,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,,mzshadab,mzshadab,10/Nov/21 15:53,10/Nov/21 15:56,13/Jul/23 08:50,10/Nov/21 15:56,3.1.0,,,,,,,,,,,,Spark Submit,,,,,0,,,,,,,mzshadab,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 10 15:56:42 UTC 2021,,,,,,,,,,"0|z0wmbs:",9223372036854775807,,,,,mzshadab,,,,,,,,,,,,,,,,,,"10/Nov/21 15:56;mzshadab;Memory increased for the container;;;","10/Nov/21 15:56;mzshadab;done;;;",,,,,,,,,,,,,,
Incorect result of filter using isNull condition,SPARK-37270,13411051,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yumwang,tkus,tkus,10/Nov/21 12:02,12/Dec/22 18:10,13/Jul/23 08:50,19/Nov/21 01:45,3.2.0,,,,,,,,3.2.1,,,,Spark Core,SQL,,,,0,correctness,,,,"Simple code that allows to reproduce this issue:
{code:java}
 val frame = Seq((false, 1)).toDF(""bool"", ""number"")
frame
  .checkpoint()
  .withColumn(""conditions"", when(col(""bool""), ""I am not null""))
  .filter(col(""conditions"").isNull)
  .show(false){code}
Although ""conditions"" column is null
{code:java}
 +-----+------+----------+
|bool |number|conditions|
+-----+------+----------+
|false|1     |null      |
+-----+------+----------+{code}
empty result is shown.

Execution plans:
{code:java}
== Parsed Logical Plan ==
'Filter isnull('conditions)
+- Project [bool#124, number#125, CASE WHEN bool#124 THEN I am not null END AS conditions#252]
   +- LogicalRDD [bool#124, number#125], false

== Analyzed Logical Plan ==
bool: boolean, number: int, conditions: string
Filter isnull(conditions#252)
+- Project [bool#124, number#125, CASE WHEN bool#124 THEN I am not null END AS conditions#252]
   +- LogicalRDD [bool#124, number#125], false

== Optimized Logical Plan ==
LocalRelation <empty>, [bool#124, number#125, conditions#252]

== Physical Plan ==
LocalTableScan <empty>, [bool#124, number#125, conditions#252]
 {code}
After removing checkpoint proper result is returned  and execution plans are as follow:
{code:java}
== Parsed Logical Plan ==
'Filter isnull('conditions)
+- Project [bool#124, number#125, CASE WHEN bool#124 THEN I am not null END AS conditions#256]
   +- Project [_1#119 AS bool#124, _2#120 AS number#125]
      +- LocalRelation [_1#119, _2#120]

== Analyzed Logical Plan ==
bool: boolean, number: int, conditions: string
Filter isnull(conditions#256)
+- Project [bool#124, number#125, CASE WHEN bool#124 THEN I am not null END AS conditions#256]
   +- Project [_1#119 AS bool#124, _2#120 AS number#125]
      +- LocalRelation [_1#119, _2#120]

== Optimized Logical Plan ==
LocalRelation [bool#124, number#125, conditions#256]

== Physical Plan ==
LocalTableScan [bool#124, number#125, conditions#256]
 {code}
It seems that the most important difference is LogicalRDD ->  LocalRelation

There are following ways (workarounds) to retrieve correct result:

1) remove checkpoint

2) add explicit .otherwise(null) to when

3) add checkpoint() or cache() just before filter

4) downgrade to Spark 3.1.2",,apachespark,bersprockets,csun,petertoth,tkus,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 19 01:45:14 UTC 2021,,,,,,,,,,"0|z0wm08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/21 04:02;gurwls223;Hm, I can't reproduce this locally. Are you able to reproduce this with running locally too? e.g.)

{code}
spark.sparkContext.setCheckpointDir(""/tmp/checkpoints"")
val frame = Seq((false, 1)).toDF(""bool"", ""number"")
frame
  .checkpoint()
  .withColumn(""conditions"", when(col(""bool""), ""I am not null""))
  .filter(col(""conditions"").isNull)
  .show(false)
{code};;;","11/Nov/21 19:52;bersprockets;I can reproduce locally. In 3.1, the above snippet returns a single row, but on 3.2 and master, it returns nothing.

You don't need checkpoint or cache to reproduce: simply reading from a table will do it.
{noformat}
drop table if exists base1;

create table base1 stored as parquet as
select * from values (false) tbl(a);

select count(*) from (
  select case when a then 'I am not null' end condition
  from base1
) s
where s.condition is null;
{noformat}
In 3.1, the above query returns 1. In 3.2 and master, it returns 0.

The issue seems to be [here|https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/expressions.scala#L680]. Because {{elseValue}} is None, the code doesn't push the {{isnull}} into the else branch, which results in:
{noformat}
Filter CASE WHEN a#12 THEN isnull(I am not null) END
Relation default.base1[a#12] parquet
{noformat}
This gets simplified to:
{noformat}
Filter (a#12 AND isnull(I am not null))
Relation default.base1[a#12] parquet
{noformat}
This gets simplified to:
{noformat}
Filter false
Relation default.base1[a#12] parquet
{noformat}
And finally, this gets simplified to:
{noformat}
LocalRelation <empty>, [a#12]
{noformat};;;","12/Nov/21 17:00;bersprockets;[~yumwang] Seems related to SPARK-33848.;;;","13/Nov/21 03:14;yumwang;Thank you [~bersprockets] I will fix it soon.;;;","13/Nov/21 08:32;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34580;;;","17/Nov/21 06:23;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/34627;;;","19/Nov/21 01:45;yumwang;Issue resolved by pull request 34627
[https://github.com/apache/spark/pull/34627];;;",,,,,,,,,
PYSPARK Arrow 3.2.0 docs link invalid,SPARK-37260,13410898,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,tgraves,tgraves,09/Nov/21 17:34,12/Dec/22 18:10,13/Jul/23 08:50,11/Nov/21 02:25,3.2.0,,,,,,,,3.2.1,,,,Documentation,,,,,0,,,,,"[http://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html]

links to:

[https://spark.apache.org/docs/latest/api/python/user_guide/arrow_pandas.html]

which links to:

[https://spark.apache.org/docs/latest/api/python/sql/arrow_pandas.rst]

But that is an invalid link.

I assume its supposed to point to:

https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html",,dchvn,tgraves,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 11 02:25:36 UTC 2021,,,,,,,,,,"0|z0wl28:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/21 04:03;dchvn;ping [~hyukjin.kwon] , is this issue resolved by [#34475|https://github.com/apache/spark/pull/34475]?;;;","11/Nov/21 02:25;gurwls223;oh yeah. that's fixed via #34475. There are some more ongoing issues on the docs. I will fix them up and probably we could initiate spark 3.2.1.;;;",,,,,,,,,,,,,,
JDBC read is always going to wrap the query in a select statement,SPARK-37259,13410874,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,KevinAppelBofa,KevinAppelBofa,09/Nov/21 15:21,06/May/22 21:30,13/Jul/23 08:50,06/May/22 21:30,3.1.2,,,,,,,,3.4.0,,,,SQL,,,,,0,,,,,"The read jdbc is wrapping the query it sends to the database server inside a select statement and there is no way to override this currently.

Initially I ran into this issue when trying to run a CTE query against SQL server and it fails, the details of the failure is in these cases:

[https://github.com/microsoft/mssql-jdbc/issues/1340]

[https://github.com/microsoft/mssql-jdbc/issues/1657]

[https://github.com/microsoft/sql-spark-connector/issues/147]

https://issues.apache.org/jira/browse/SPARK-32825

https://issues.apache.org/jira/browse/SPARK-34928

I started to patch the code to get the query to run and ran into a few different items, if there is a way to add these features to allow this code path to run, this would be extremely helpful to running these type of edge case queries.  These are basic examples here the actual queries are much more complex and would require significant time to rewrite.

Inside JDBCOptions.scala the query is being set to either, using the dbtable this allows the query to be passed without modification

 
{code:java}
name.trim
or
s""(${subquery}) SPARK_GEN_SUBQ_${curId.getAndIncrement()}""
{code}
 

Inside JDBCRelation.scala this is going to try to get the schema for this query, and this ends up running dialect.getSchemaQuery which is doing:
{code:java}
s""SELECT * FROM $table WHERE 1=0""{code}
Overriding the dialect here and initially just passing back the $table gets passed here and to the next issue which is in the compute function in JDBCRDD.scala

 
{code:java}
val sqlText = s""SELECT $columnList FROM ${options.tableOrQuery} $myTableSampleClause"" + s"" $myWhereClause $getGroupByClause $myLimitClause""
 
{code}
 

For these two queries, about a CTE query and using temp tables, finding out the schema is difficult without actually running the query and for the temp table if you run it in the schema check that will have the table now exist and fail when it runs the actual query.

 

The way I patched these is by doing these two items:

JDBCRDD.scala (compute)

 
{code:java}
    val runQueryAsIs = options.parameters.getOrElse(""runQueryAsIs"", ""false"").toBoolean
    val sqlText = if (runQueryAsIs) {
      s""${options.tableOrQuery}""
    } else {
      s""SELECT $columnList FROM ${options.tableOrQuery} $myWhereClause""
    }

{code}
JDBCRelation.scala (getSchema)
{code:java}
val useCustomSchema = jdbcOptions.parameters.getOrElse(""useCustomSchema"", ""false"").toBoolean
    if (useCustomSchema) {
      val myCustomSchema = jdbcOptions.parameters.getOrElse(""customSchema"", """").toString
      val newSchema = CatalystSqlParser.parseTableSchema(myCustomSchema)
      logInfo(s""Going to return the new $newSchema because useCustomSchema is $useCustomSchema and passed in $myCustomSchema"")
      newSchema
    } else {
      val tableSchema = JDBCRDD.resolveTable(jdbcOptions)
      jdbcOptions.customSchema match {
      case Some(customSchema) => JdbcUtils.getCustomSchema(
        tableSchema, customSchema, resolver)
      case None => tableSchema
      }
    }{code}
 

This is allowing the query to run as is, by using the dbtable option and then provide a custom schema that will bypass the dialect schema check

 

Test queries

 
{code:java}
query1 = """""" 
SELECT 1 as DummyCOL
""""""
query2 = """""" 
WITH DummyCTE AS
(
SELECT 1 as DummyCOL
)
SELECT *
FROM DummyCTE
""""""
query3 = """"""
(SELECT *
INTO #Temp1a
FROM
(SELECT @@VERSION as version) data
)
(SELECT *
FROM
#Temp1a)
""""""
{code}
 

Test schema

 
{code:java}
schema1 = """"""
DummyXCOL INT
""""""
schema2 = """"""
DummyXCOL STRING
""""""
{code}
 

Test code

 
{code:java}
jdbcDFWorking = (
    spark.read.format(""jdbc"")
    .option(""url"", f""jdbc:sqlserver://{server}:{port};databaseName={database};"")
    .option(""user"", user)
    .option(""password"", password)
    .option(""driver"", ""com.microsoft.sqlserver.jdbc.SQLServerDriver"")
    .option(""dbtable"", queryx)
    .option(""customSchema"", schemax)
    .option(""useCustomSchema"", ""true"")
    .option(""runQueryAsIs"", ""true"")
    .load()
)
 
{code}
 

Currently we ran into this on these two special SQL server queries however we aren't sure if there is other DB's we are using that we haven't hit this type of issue yet, without going through this I didn't realize the query is always wrapped in the SELECT no matter what you do.

This is on the Spark 3.1.2 and using the PySpark with the Python 3.7.11

Thank you for your consideration and assistance to a way to fix this

Kevin

 

 

 ",,akhalymon,apachespark,KevinAppelBofa,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,Patch,,,,,,,,9223372036854775807,,,,Tue May 03 13:00:08 UTC 2022,,,,,,,,,,"0|z0wkww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/21 15:56;petertoth;[~KevinAppelBofa], how about adding a new `withClause` to the JDBC options? Do you think you could split your CTE query to ""with clause"" and ""regular query"" parts manually and specify something like: .option(""withClause"", withClause).option(""query"", query)?
Because, that way we probably only need a small change to `sqlText` in `compute()` ([https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/jdbc/JDBCRDD.scala#L370-L371):]
{noformat}
    val sqlText = s""$withClause SELECT $columnList FROM ${options.tableOrQuery} $myTableSampleClause"" +
      s"" $myWhereClause $getGroupByClause $myLimitClause""
{noformat}
and also we could keep its other functionality.

Sidenote: technically we could extract the WITH clause in MsSqlServerDialect and assemble a dialect specific `sqlText` there, but it is not that simple to do it...;;;","22/Nov/21 19:17;KevinAppelBofa;It would be difficult to be able to actually split up the query into the parts, and to align one of the selects to match the one hard coded in the query; then the other issues issue about needing to patch into the dialect and handle how it passes that query today to get the schema and having a way to get that, without running the query twice.

The other query that uses temp tables, in the sql server it is either #temptable or ##temptable is also still an issue because of how it getting wrapped in the select and the similar item if that runs the query to get the schema, then it actually creates the tables and the query fails when it runs since the table exists

The other item is the query is going to do something to the query you pass in, so it would need to be based on dbtable being used that is only doing a trim; the query is wrapping:
s""(${subquery}) SPARK_GEN_SUBQ_${curId.getAndIncrement()}""
 

 ;;;","23/Nov/21 12:10;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/34693;;;","23/Nov/21 12:25;petertoth;I've opened a PR: [https://github.com/apache/spark/pull/34693] to support queries with CTE.

{quote}to get the schema and having a way to get that, without running the query twice.{quote}
I don't think that running the query twice to get the schema would be an issue as Spark adds a `WHERE 1=0` clause to the query. MSSQL engine should optimize the query and quickly return the schema with empty results.

{quote}The other item is the query is going to do something to the query you pass in, so it would need to be based on dbtable being used that is only doing a trim; the query is wrapping:
s""(${subquery}) SPARK_GEN_SUBQ_${curId.getAndIncrement()}""{quote}
I don't think this is an issue, please see new unit tests in the PR.

{quote}The other query that uses temp tables, in the sql server it is either #temptable or ##temptable is also still an issue because of how it getting wrapped in the select and the similar item if that runs the query to get the schema, then it actually creates the tables and the query fails when it runs since the table exists{quote}
I'm not sure that temp tables fit into Spark's JDBC world. Let me check if we can workaround them with the new `withClause`...
;;;","25/Nov/21 09:35;akhalymon;I've created another PR following [~KevinAppelBofa]  idea to unwrap the query and pass schema manually when the user chooses to do so. If let's say an option 'useRawQuery' passed, the query will run as-is. The downside of this user has to provide schema manually. However, there are also advantages, as we are not running the query twice, and the user doesn't have to modify the query and split the 'with' clause.

PR link is https://github.com/apache/spark/pull/34709

[~KevinAppelBofa] [~petertoth] what are your thoughts on this?;;;","25/Nov/21 09:40;apachespark;User 'akhalymon-cv' has created a pull request for this issue:
https://github.com/apache/spark/pull/34709;;;","29/Nov/21 15:21;KevinAppelBofa;[~petertoth] [~akhalymon] Thank you both for working on these patches, it took me a little bit to figure out how to test them but i got the Spark 3.3.0-SNAPSHOT compiled and then added both of your changes to different working copies and then recompile the spark-sql and then was able to test both of your changes.  I added comments into the github pull request links with how the testing went so far;;;","21/Apr/22 15:26;KevinAppelBofa;Both of these PR's were closed due to inactivity, I had tested them both and they both appeared to be working.  Is it possible these could get re activated and one of them pushed to the end?

Or is there someone new that could help to take this to the end and get this available in the main spark code base?

I am working on Spark 3.2.1 now and I had recompiled the code to do the above items, we also have been using this code for a few months on Spark 3.1.2 and it is working for us to be able to handle the CTE queries against MSSQL;;;","03/May/22 13:00;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/36440;;;",,,,,,,
try_simplify_traceback should not fail when tb_frame.f_lineno is None,SPARK-37253,13410780,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,dongjoon,dongjoon,09/Nov/21 08:53,12/Dec/22 18:10,13/Jul/23 08:50,09/Nov/21 11:04,3.1.2,3.2.0,3.3.0,,,,,,3.1.3,3.2.1,3.3.0,,PySpark,,,,,0,,,,,"{code}
Traceback (most recent call last):
  File ""/Users/dongjoon/APACHE/spark-merge/python/lib/pyspark.zip/pyspark/worker.py"", line 630, in main
    tb = try_simplify_traceback(sys.exc_info()[-1])
  File ""/Users/dongjoon/APACHE/spark-merge/python/lib/pyspark.zip/pyspark/util.py"", line 217, in try_simplify_traceback
    new_tb = types.TracebackType(
TypeError: 'NoneType' object cannot be interpreted as an integer
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-37244,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 09 11:04:21 UTC 2021,,,,,,,,,,"0|z0wkc0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/21 08:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34530;;;","09/Nov/21 09:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34530;;;","09/Nov/21 11:04;gurwls223;Issue resolved by pull request 34530
[https://github.com/apache/spark/pull/34530];;;",,,,,,,,,,,,,
Ignore test_memory_limit on non-Linux environment,SPARK-37252,13410731,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dongjoon,dongjoon,dongjoon,09/Nov/21 01:31,09/Nov/21 02:40,13/Jul/23 08:50,09/Nov/21 02:40,3.2.0,3.3.0,,,,,,,3.1.3,3.2.1,3.3.0,,PySpark,Tests,,,,0,,,,,"{code}
$ build/sbt -Phadoop-cloud -Phadoop-3.2 test:package
$ python/run-tests
...
======================================================================
FAIL: test_memory_limit (pyspark.tests.test_worker.WorkerMemoryTest)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Users/dongjoon/APACHE/spark-merge/python/pyspark/tests/test_worker.py"", line 212, in test_memory_limit
    self.assertEqual(soft_limit, 2 * 1024 * 1024 * 1024)
AssertionError: 9223372036854775807 != 2147483648

----------------------------------------------------------------------
{code}",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Nov 09 02:40:43 UTC 2021,,,,,,,,,,"0|z0wk1c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/21 01:49;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34527;;;","09/Nov/21 02:40;dongjoon;This is resolved via https://github.com/apache/spark/pull/34527;;;",,,,,,,,,,,,,,
Upgrade ORC to 1.6.12,SPARK-37238,13410518,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,08/Nov/21 04:55,08/Nov/21 07:07,13/Jul/23 08:50,08/Nov/21 07:07,3.2.1,,,,,,,,3.2.1,,,,Build,,,,,0,,,,,,,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 08 07:07:07 UTC 2021,,,,,,,,,,"0|z0wiq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/21 05:01;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34512;;;","08/Nov/21 07:07;sarutak;Issue resolved in https://github.com/apache/spark/pull/34512;;;",,,,,,,,,,,,,,
The number of dynamic partitions should early check when writing to external tables,SPARK-37217,13410229,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,dzcxzl,dzcxzl,dzcxzl,05/Nov/21 10:58,14/Dec/21 18:19,13/Jul/23 08:50,14/Dec/21 03:29,3.2.0,,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"[SPARK-29295|https://issues.apache.org/jira/browse/SPARK-29295] introduces a mechanism that writes to external tables is a dynamic partition method, and the data in the target partition will be deleted first.

Assuming that 1001 partitions are written, the data of 10001 partitions will be deleted first, but because hive.exec.max.dynamic.partitions is 1000 by default, loadDynamicPartitions will fail at this time, but the data of 1001 partitions has been deleted.",,apachespark,csun,dzcxzl,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Dec 14 04:18:58 UTC 2021,,,,,,,,,,"0|z0wh2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/21 11:35;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/34493;;;","14/Dec/21 03:29;csun;Issue resolved by pull request 34493
[https://github.com/apache/spark/pull/34493];;;","14/Dec/21 04:18;apachespark;User 'cxzl25' has created a pull request for this issue:
https://github.com/apache/spark/pull/34889;;;",,,,,,,,,,,,,
YarnShuffleIntegrationSuite  and other two similar cases in `resource-managers` test failed,SPARK-37209,13410090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,04/Nov/21 16:01,22/Nov/21 01:12,13/Jul/23 08:50,22/Nov/21 01:12,3.3.0,,,,,,,,3.0.4,3.1.3,3.2.1,3.3.0,Tests,YARN,,,,0,,,,,"Execute :
 # build/mvn clean package -DskipTests -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive
 # build/mvn test -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl resource-managers/yarn

The test will successful.

 

Execute :
 # build/mvn clean -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive
 # build/mvn clean test -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl resource-managers/yarn 

The test will failed.

 

Execute :
 # build/mvn clean package -DskipTests -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive
 # Delete assembly/target/scala-2.12/jars manually
 # build/mvn test -Phadoop-3.2 -Phive-2.3 -Phadoop-cloud -Pmesos -Pyarn -Pkinesis-asl -Phive-thriftserver -Pspark-ganglia-lgpl -Pkubernetes -Phive -Pscala-2.13 -pl resource-managers/yarn 

The test will failed.

 

The error stack is :
{code:java}
21/11/04 19:48:52.159 main ERROR Client: Application diagnostics message: User class threw exception: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times,
 most recent failure: Lost task 0.3 in stage 0.0 (TID 6) (localhost executor 1): java.lang.NoClassDefFoundError: breeze/linalg/Matrix
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:348)
        at org.apache.spark.util.Utils$.classForName(Utils.scala:216)
        at org.apache.spark.serializer.KryoSerializer$.$anonfun$loadableSparkClasses$1(KryoSerializer.scala:537)
        at scala.collection.immutable.List.flatMap(List.scala:293)
        at scala.collection.immutable.List.flatMap(List.scala:79)
        at org.apache.spark.serializer.KryoSerializer$.loadableSparkClasses$lzycompute(KryoSerializer.scala:535)
        at org.apache.spark.serializer.KryoSerializer$.org$apache$spark$serializer$KryoSerializer$$loadableSparkClasses(KryoSerializer.scala:502)
        at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:226)
        at org.apache.spark.serializer.KryoSerializer$$anon$1.create(KryoSerializer.scala:102)
        at com.esotericsoftware.kryo.pool.KryoPoolQueueImpl.borrow(KryoPoolQueueImpl.java:48)
        at org.apache.spark.serializer.KryoSerializer$PoolWrapper.borrow(KryoSerializer.scala:109)
        at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:346)
        at org.apache.spark.serializer.KryoSerializationStream.<init>(KryoSerializer.scala:266)
        at org.apache.spark.serializer.KryoSerializerInstance.serializeStream(KryoSerializer.scala:432)
        at org.apache.spark.shuffle.ShufflePartitionPairsWriter.open(ShufflePartitionPairsWriter.scala:76)
        at org.apache.spark.shuffle.ShufflePartitionPairsWriter.write(ShufflePartitionPairsWriter.scala:59)
        at org.apache.spark.util.collection.WritablePartitionedIterator.writeNext(WritablePartitionedPairCollection.scala:83)
        at org.apache.spark.util.collection.ExternalSorter.$anonfun$writePartitionedMapOutput$1(ExternalSorter.scala:772)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1468)
        at org.apache.spark.util.collection.ExternalSorter.writePartitionedMapOutput(ExternalSorter.scala:775)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:70)
        at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
        at org.apache.spark.scheduler.Task.run(Task.scala:136)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:507)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1468)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:510)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.ClassNotFoundException: breeze.linalg.Matrix
        at java.net.URLClassLoader.findClass(URLClassLoader.java:387)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:355)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
{code}
 

 

 ",,apachespark,LuciferYang,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Nov/21 16:08;LuciferYang;failed-unit-tests.log;https://issues.apache.org/jira/secure/attachment/13035707/failed-unit-tests.log","04/Nov/21 16:08;LuciferYang;success-unit-tests.log;https://issues.apache.org/jira/secure/attachment/13035706/success-unit-tests.log",,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 22 01:12:49 UTC 2021,,,,,,,,,,"0|z0wg7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Nov/21 12:10;LuciferYang;After some investigation, I found that this issue maybe related to `hadoop-3.x`, when use `hadoop-2.7` profile, the above test can be successful:

 
{code:java}
mvn clean install -DskipTests -pl resource-managers/yarn -Pyarn -Phadoop-2.7 -am
 mvn test -pl resource-managers/yarn -Pyarn -Phadoop-2.7 -Dtest=none -DwildcardSuites=org.apache.spark.deploy.yarn.YarnShuffleIntegrationSuite

Discovery starting.
Discovery completed in 259 milliseconds.
Run starting. Expected test count is: 1
YarnShuffleIntegrationSuite:
- external shuffle service
Run completed in 30 seconds, 765 milliseconds.
Total number of tests run: 1
Suites: completed 2, aborted 0
Tests: succeeded 1, failed 0, canceled 0, ignored 0, pending 0
All tests passed.{code}
It seems that when testing with hadoop-2.7, the result of executing `Utils.isTesting` is true, which helps test case to ignore the `NoClassDefFoundError` in the test, but when testing with hadoop-3.2, the result of executing `Utils.isTesting` is false.

 

But I haven't investigated the root cause with hadoop-3.2

 

cc [~hyukjin.kwon] [~dongjoon] [~srowen] 

 

 ;;;","16/Nov/21 15:15;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34620;;;","22/Nov/21 01:12;srowen;Issue resolved by pull request 34620
[https://github.com/apache/spark/pull/34620];;;",,,,,,,,,,,,,
Fix NotSerializableException when observe with TypedImperativeAggregate,SPARK-37203,13409790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,03/Nov/21 07:34,23/Sep/22 05:49,13/Jul/23 08:50,05/Nov/21 08:30,3.3.0,,,,,,,,3.1.3,3.2.1,3.3.0,,SQL,,,,,0,,,,,"
{code:java}
val namedObservation = Observation(""named"")

val df = spark.range(100)
val observed_df = df.observe(
   namedObservation, percentile_approx($""id"", lit(0.5), lit(100)).as(""percentile_approx_val""))

observed_df.collect()
namedObservation.get
{code}

throws exception as follows:

{code:java}
15:16:27.994 ERROR org.apache.spark.util.Utils: Exception encountered
java.io.NotSerializableException: org.apache.spark.sql.catalyst.expressions.aggregate.ApproximatePercentile$PercentileDigest
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1184)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$2(TaskResult.scala:55)
	at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$2$adapted(TaskResult.scala:55)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at org.apache.spark.scheduler.DirectTaskResult.$anonfun$writeExternal$1(TaskResult.scala:55)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1434)
	at org.apache.spark.scheduler.DirectTaskResult.writeExternal(TaskResult.scala:51)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:616)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

",,apachespark,beliefer,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 23 05:49:56 UTC 2022,,,,,,,,,,"0|z0wed4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/21 07:34;beliefer;I'm working on.;;;","03/Nov/21 08:05;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/34474;;;","03/Nov/21 08:07;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/34474;;;","05/Nov/21 08:30;cloud_fan;Issue resolved by pull request 34474
[https://github.com/apache/spark/pull/34474];;;","23/Sep/22 05:49;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/37977;;;",,,,,,,,,,,
Temp view didn't collect temp function that registered with catalog API,SPARK-37202,13409774,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,linhongliu-db,linhongliu-db,03/Nov/21 04:00,12/Nov/21 15:03,13/Jul/23 08:50,12/Nov/21 14:32,3.2.0,,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,linhongliu-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Nov 12 15:03:50 UTC 2021,,,,,,,,,,"0|z0we9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Nov/21 04:57;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/34473;;;","09/Nov/21 05:49;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/34528;;;","12/Nov/21 14:31;cloud_fan;resolved by https://github.com/apache/spark/pull/34546;;;","12/Nov/21 15:03;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/34546;;;",,,,,,,,,,,,
NPE in org.apache.spark.sql.hive.HiveShim$.toCatalystDecimal(HiveShim.scala:106),SPARK-37196,13409641,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,Yavorski_S,Yavorski_S,02/Nov/21 09:56,08/Nov/21 20:33,13/Jul/23 08:50,08/Nov/21 20:32,3.0.1,,,,,,,,3.0.4,3.1.3,3.2.1,3.3.0,Input/Output,,,,,0,,,,,"Still reproducible in Spark 3.0.1.

+*How to reproduce:*+


*SHELL*

> export SPARK_MAJOR_VERSION=3

*SPARK PART 1*

> spark-shell --master local
SPARK_MAJOR_VERSION is set to 3, using Spark3
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/10/25 11:43:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/10/25 11:43:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
Spark context Web UI available at <server>:4042
Spark context available as 'sc' (master = local, app id = local-1635151438558).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_302)
Type in expressions to have them evaluated.
Type :help for more information.

scala> val df = spark.sql(""select 'dummy' as name, 1000000000000000000010.7000000000000010 as value"")
df: org.apache.spark.sql.DataFrame = [name: string, value: decimal(38,16)]

scala> df.write.mode(""Overwrite"").parquet(""test.parquet"")

*HIVE*

hive> create external table test_precision(name string, value Decimal(18,6)) STORED AS PARQUET LOCATION 'test';
OK
Time taken: 0.067 seconds

*SPARK PART 2*
                   
scala> spark.conf.set(""spark.sql.hive.convertMetastoreParquet"",""false"");

scala> val df_hive = spark.sql(""select * from test_precision"");
df_hive: org.apache.spark.sql.DataFrame = [name: string, value: decimal(18,6)]

scala> df_hive.show;
21/10/25 12:04:51 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)/ 1]
*{color:red}java.lang.NullPointerException
        at org.apache.spark.sql.hive.HiveShim$.toCatalystDecimal(HiveShim.scala:106){color}*
        at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$13(TableReader.scala:465)
        at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$13$adapted(TableReader.scala:464)
        at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$18(TableReader.scala:493)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
21/10/25 12:04:51 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2, tklis-kappd0005.dev.df.sbrf.ru, executor driver): java.lang.NullPointerException
        at org.apache.spark.sql.hive.HiveShim$.toCatalystDecimal(HiveShim.scala:106)
        at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$13(TableReader.scala:465)
        at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$13$adapted(TableReader.scala:464)
        at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$18(TableReader.scala:493)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

21/10/25 12:04:51 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, tklis-kappd0005.dev.df.sbrf.ru, executor driver): java.lang.NullPointerException
        at org.apache.spark.sql.hive.HiveShim$.toCatalystDecimal(HiveShim.scala:106)
        at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$13(TableReader.scala:465)
        at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$13$adapted(TableReader.scala:464)
       at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$18(TableReader.scala:493)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
        at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
        at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
        at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
        at org.apache.spark.scheduler.Task.run(Task.scala:127)
        at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)
  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)
  at scala.Option.foreach(Option.scala:407)
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2139)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:467)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:420)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3627)
  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2697)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3618)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:764)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3616)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2697)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2904)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:300)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:337)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:824)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:783)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:792)
  ... 47 elided
Caused by: java.lang.NullPointerException
  at org.apache.spark.sql.hive.HiveShim$.toCatalystDecimal(HiveShim.scala:106)
  at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$13(TableReader.scala:465)
  at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$13$adapted(TableReader.scala:464)
  at org.apache.spark.sql.hive.HadoopTableReader$.$anonfun$fillObject$18(TableReader.scala:493)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
  at scala.collection.Iterator$$anon$10.next(Iterator.scala:459)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:729)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:340)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:872)
  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:872)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:349)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:313)
  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
  at org.apache.spark.scheduler.Task.run(Task.scala:127)
  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)
  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
  at java.lang.Thread.run(Thread.java:748)
",,angerszhuuu,apachespark,dongjoon,LuciferYang,Yavorski_S,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-36651,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 08 20:32:38 UTC 2021,,,,,,,,,,"0|z0wdg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/21 08:13;LuciferYang;Does Spark 3.2.0 also have this problem?

 ;;;","08/Nov/21 10:18;angerszhuuu;[~LuciferYang]  Still have problem. Working. on this

 ;;;","08/Nov/21 10:46;angerszhuuu;It's caused by you create a df with 

 {code}
root
 |-- name: string (nullable = false)
 |-- value: decimal(38,16) (nullable = false)
{code}

but create table schema as decimal(18, 6),

Then failed to convert and return null

{code}

  public HiveDecimal getPrimitiveJavaObject(Object o) {
    return o == null ? null : this.enforcePrecisionScale(((HiveDecimalWritable)o).getHiveDecimal());
  }

{code};;;","08/Nov/21 11:01;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34519;;;","08/Nov/21 11:01;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34519;;;","08/Nov/21 20:32;dongjoon;Issue resolved by pull request 34519
[https://github.com/apache/spark/pull/34519];;;",,,,,,,,,,
Allow merging DecimalTypes with different precision values ,SPARK-37191,13409559,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,01/Nov/21 23:01,03/Nov/21 03:21,13/Jul/23 08:50,03/Nov/21 03:21,3.0.3,3.1.0,3.1.1,3.2.0,,,,,3.3.0,,,,SQL,,,,,0,,,,,"When merging DecimalTypes with different precision but the same scale, one would get the following error:
{code:java}
Failed to merge fields 'col' and 'col'. Failed to merge decimal types with incompatible precision 17 and 12	at org.apache.spark.sql.types.StructType$.$anonfun$merge$2(StructType.scala:652)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.types.StructType$.$anonfun$merge$1(StructType.scala:644)
	at org.apache.spark.sql.types.StructType$.$anonfun$merge$1$adapted(StructType.scala:641)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.sql.types.StructType$.merge(StructType.scala:641)
	at org.apache.spark.sql.types.StructType.merge(StructType.scala:550) {code}
 

We could allow merging DecimalType values with different precision if the scale is the same for both types since there should not be any data correctness issues as one of the types will be extended, for example, DECIMAL(12, 2) -> DECIMAL(17, 2); however, this is not the case for upcasting when the scale is different - this would depend on the actual values.

 

Repro code:
{code:java}
import org.apache.spark.sql.types._

val schema1 = StructType(StructField(""col"", DecimalType(17, 2)) :: Nil)
val schema2 = StructType(StructField(""col"", DecimalType(12, 2)) :: Nil)
schema1.merge(schema2) {code}
 

This also affects Parquet schema merge which is where this issue was discovered originally:
{code:java}
import java.math.BigDecimal
import org.apache.spark.sql.Row
import org.apache.spark.sql.types._

val data1 = sc.parallelize(Row(new BigDecimal(""1234567890000.11"")) :: Nil, 1)
val schema1 = StructType(StructField(""col"", DecimalType(17, 2)) :: Nil)

val data2 = sc.parallelize(Row(new BigDecimal(""123456789.11"")) :: Nil, 1)
val schema2 = StructType(StructField(""col"", DecimalType(12, 2)) :: Nil)

spark.createDataFrame(data2, schema2).write.parquet(""/tmp/decimal-test.parquet"")
spark.createDataFrame(data1, schema1).write.mode(""append"").parquet(""/tmp/decimal-test.parquet"")

// Reading the DataFrame fails
spark.read.option(""mergeSchema"", ""true"").parquet(""/tmp/decimal-test.parquet"").show()

>>>
Failed merging schema:
root
 |-- col: decimal(17,2) (nullable = true)

Caused by: Failed to merge fields 'col' and 'col'. Failed to merge decimal types with incompatible precision 12 and 17



{code}
 ",,apachespark,cloud_fan,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Nov 03 03:21:06 UTC 2021,,,,,,,,,,"0|z0wcy0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/21 23:16;ivan.sadikov;This is somewhat related to https://issues.apache.org/jira/browse/SPARK-32317 although the issue is a bit different - they are trying to merge decimals of different scales.;;;","01/Nov/21 23:20;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/34462;;;","03/Nov/21 03:21;cloud_fan;Issue resolved by pull request 34462
[https://github.com/apache/spark/pull/34462];;;",,,,,,,,,,,,,
Pin PySpark version installed in the Binder environment for tagged commit,SPARK-37170,13409289,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,30/Oct/21 15:26,12/Dec/22 18:10,13/Jul/23 08:50,01/Nov/21 01:07,3.2.0,,,,,,,,3.2.1,3.3.0,,,Documentation,PySpark,,,,0,,,,,"I noticed that the PySpark 3.1.2 is installed in the live notebook environment even though the notebook is for PySpark 3.2.0.
http://spark.apache.org/docs/3.2.0/api/python/getting_started/index.html

I guess someone accessed to Binder and built the container image with v3.2.0 before we published the pyspark package to PyPi.
https://mybinder.org/

I think it's difficult to rebuild the image manually.
To avoid such accident, I'll propose to pin the version of PySpark in binder/postBuild

 

 ",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Nov 01 01:07:25 UTC 2021,,,,,,,,,,"0|z0wba0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/21 15:43;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34449;;;","01/Nov/21 01:07;gurwls223;Fixed in https://github.com/apache/spark/pull/34449;;;",,,,,,,,,,,,,,
MetricsReporter producing NullPointerException when element 'triggerExecution' not present in Map[],SPARK-37147,13408947,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,bornonborneo,bornonborneo,bornonborneo,28/Oct/21 16:27,30/Oct/21 22:48,13/Jul/23 08:50,30/Oct/21 22:48,3.2.0,,,,,,,,3.2.1,3.3.0,,,Structured Streaming,,,,,0,,,,,"The exception occurs in MetricsReporter when it tries to register gauges using lastProgress of each stream. This problem was partially fixed in https://issues.apache.org/jira/browse/SPARK-22975 but in introduced a NPE when 'triggerExecution' element is not present in durationMS map:

 
{code:java}
registerGauge(""latency"", _.durationMs.get(""triggerExecution"").longValue(), 0L)
{code}
 

I find it difficult to reproduce this every time but it happens every few restarts when structured streaming uses a slow event source (very little or no events). In my case it breaks metric reporting via Codehale/Dropwizard and generates multiple stacktraces such as:
{code:java}
21/09/16 09:51:36 ERROR ScheduledReporter: Exception thrown from GangliaReporter#report. Exception was suppressed.
java.lang.NullPointerException
	at org.apache.spark.sql.execution.streaming.MetricsReporter.$anonfun$new$3(MetricsReporter.scala:43)
	at org.apache.spark.sql.execution.streaming.MetricsReporter.$anonfun$new$3$adapted(MetricsReporter.scala:43)
	at scala.Option.map(Option.scala:230)
	at org.apache.spark.sql.execution.streaming.MetricsReporter$$anon$1.getValue(MetricsReporter.scala:68)
	at com.codahale.metrics.ganglia.GangliaReporter.reportGauge(GangliaReporter.java:353)
	at com.codahale.metrics.ganglia.GangliaReporter.report(GangliaReporter.java:240)
	at com.codahale.metrics.ScheduledReporter.report(ScheduledReporter.java:237)
	at com.codahale.metrics.ScheduledReporter.lambda$start$0(ScheduledReporter.java:177)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}
I'm happy to implement a fix.",,apachespark,bornonborneo,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,Sat Oct 30 22:48:29 UTC 2021,,,,,,,,,,"0|z0w960:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/21 17:06;apachespark;User 'gitplaneta' has created a pull request for this issue:
https://github.com/apache/spark/pull/34426;;;","30/Oct/21 22:48;kabhwan;Issue resolved via https://github.com/apache/spark/pull/34426;;;",,,,,,,,,,,,,,
Supplement the missing Java 11 benchmark result files,SPARK-37143,13408848,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,28/Oct/21 08:44,28/Oct/21 18:09,13/Jul/23 08:50,28/Oct/21 18:09,3.3.0,,,,,,,,3.3.0,,,,Tests,,,,,0,,,,,"CharVarcharBenchmark-results.txt and UpdateFieldsBenchmark-results.txt exist in the project, but CharVarcharBenchmark-jdk11-results.txt and UpdateFieldsBenchmark-jdk11-results.txt are missing

 ",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 18:09:04 UTC 2021,,,,,,,,,,"0|z0w8k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/21 09:05;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34423;;;","28/Oct/21 18:09;dongjoon;Issue resolved by pull request 34423
[https://github.com/apache/spark/pull/34423];;;",,,,,,,,,,,,,,
WorkerSuite cannot run on Mac OS,SPARK-37141,13408823,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,toujours33,LuciferYang,LuciferYang,28/Oct/21 05:47,28/Oct/21 18:04,13/Jul/23 08:50,28/Oct/21 18:04,3.3.0,,,,,,,,3.3.0,,,,Tests,,,,,0,,,,,"After SPARK-35907 run `org.apache.spark.deploy.worker.WorkerSuite` on Mac os(both M1 and Intel) failed
{code:java}
mvn clean install -DskipTests -pl core -am
mvn test -pl core -Dtest=none -DwildcardSuites=org.apache.spark.deploy.worker.WorkerSuite
{code}
{code:java}
WorkerSuite:
- test isUseLocalNodeSSLConfig
- test maybeUpdateSSLSettings
- test clearing of finishedExecutors (small number of executors)
- test clearing of finishedExecutors (more executors)
- test clearing of finishedDrivers (small number of drivers)
- test clearing of finishedDrivers (more drivers)
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  47.973 s
[INFO] Finished at: 2021-10-28T13:46:56+08:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.scalatest:scalatest-maven-plugin:2.0.2:test (test) on project spark-core_2.12: There are test failures -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException
{code}
{code:java}
21/10/28 13:46:56.133 dispatcher-event-loop-1 ERROR Utils: Failed to create directory /tmp
java.nio.file.FileAlreadyExistsException: /tmp
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384)
        at java.nio.file.Files.createDirectory(Files.java:674)
        at java.nio.file.Files.createAndCheckIsDirectory(Files.java:781)
        at java.nio.file.Files.createDirectories(Files.java:727)
        at org.apache.spark.util.Utils$.createDirectory(Utils.scala:292)
        at org.apache.spark.deploy.worker.Worker.createWorkDir(Worker.scala:221)
        at org.apache.spark.deploy.worker.Worker.onStart(Worker.scala:232)
        at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:120)
        at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)
        at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)
        at org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)
        at org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}
 ",,apachespark,dongjoon,LuciferYang,toujours33,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 18:04:08 UTC 2021,,,,,,,,,,"0|z0w8eg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/21 06:59;toujours33;I'm working on it;;;","28/Oct/21 08:05;apachespark;User 'toujours33' has created a pull request for this issue:
https://github.com/apache/spark/pull/34420;;;","28/Oct/21 18:04;dongjoon;Issue resolved by pull request 34420
[https://github.com/apache/spark/pull/34420];;;",,,,,,,,,,,,,
Fix some mirco-benchmarks run failed ,SPARK-37135,13408804,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,apachespark,LuciferYang,LuciferYang,28/Oct/21 02:41,12/Dec/22 18:10,13/Jul/23 08:50,28/Oct/21 07:15,3.3.0,,,,,,,,3.3.0,,,,Spark Core,SQL,,,,0,,,,,"2 mirco-benchmarks run failed:

 

org.apache.spark.serializer.KryoSerializerBenchmark
{code:java}
Running org.apache.spark.serializer.KryoSerializerBenchmark:Running org.apache.spark.serializer.KryoSerializerBenchmark:Running benchmark: Benchmark KryoPool vs old""pool of 1"" implementation  Running case: KryoPool:true21/10/27 16:09:26 ERROR SparkContext: Error initializing SparkContext.java.lang.AssertionError: assertion failed: spark.test.home is not set! at scala.Predef$.assert(Predef.scala:223) at org.apache.spark.deploy.worker.Worker.<init>(Worker.scala:148) at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:954) at org.apache.spark.deploy.LocalSparkCluster.$anonfun$start$2(LocalSparkCluster.scala:71) at org.apache.spark.deploy.LocalSparkCluster.$anonfun$start$2$adapted(LocalSparkCluster.scala:65) at scala.collection.immutable.Range.foreach(Range.scala:158) at org.apache.spark.deploy.LocalSparkCluster.start(LocalSparkCluster.scala:65) at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2971) at org.apache.spark.SparkContext.<init>(SparkContext.scala:562) at org.apache.spark.SparkContext.<init>(SparkContext.scala:138) at org.apache.spark.serializer.KryoSerializerBenchmark$.createSparkContext(KryoSerializerBenchmark.scala:86) at org.apache.spark.serializer.KryoSerializerBenchmark$.sc$lzycompute$1(KryoSerializerBenchmark.scala:58) at org.apache.spark.serializer.KryoSerializerBenchmark$.sc$1(KryoSerializerBenchmark.scala:58) at org.apache.spark.serializer.KryoSerializerBenchmark$.$anonfun$run$3(KryoSerializerBenchmark.scala:63) at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23) at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659) at scala.util.Success.$anonfun$map$1(Try.scala:255) at scala.util.Success.map(Try.scala:213) at scala.concurrent.Future.$anonfun$map$1(Future.scala:292) at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33) at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64) at java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1426) at java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290) at java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020) at java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656) at java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594) at java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183){code}
org.apache.spark.sql.execution.benchmark.DateTimeBenchmark
{code:java}
Exception in thread ""main"" org.apache.spark.sql.catalyst.parser.ParseException: Exception in thread ""main"" org.apache.spark.sql.catalyst.parser.ParseException: Cannot mix year-month and day-time fields: interval 1 month 2 day(line 1, pos 38)
== SQL ==cast(timestamp_seconds(id) as date) + interval 1 month 2 day--------------------------------------^^^
 at org.apache.spark.sql.errors.QueryParsingErrors$.mixedIntervalUnitsError(QueryParsingErrors.scala:214) at org.apache.spark.sql.catalyst.parser.AstBuilder.constructMultiUnitsIntervalLiteral(AstBuilder.scala:2435) at org.apache.spark.sql.catalyst.parser.AstBuilder.$anonfun$visitInterval$1(AstBuilder.scala:2479) at org.apache.spark.sql.catalyst.parser.ParserUtils$.withOrigin(ParserUtils.scala:133) at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:2454) at org.apache.spark.sql.catalyst.parser.AstBuilder.visitInterval(AstBuilder.scala:57) at org.apache.spark.sql.catalyst.parser.SqlBaseParser$IntervalContext.accept(SqlBaseParser.java:17681)
{code}
 ",,apachespark,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 07:15:15 UTC 2021,,,,,,,,,,"0|z0w8a8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/21 03:04;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34409;;;","28/Oct/21 07:15;gurwls223;Issue resolved by pull request 34409
[https://github.com/apache/spark/pull/34409];;;",,,,,,,,,,,,,,
TestUtils.isPythonVersionAtLeast38 returns incorrect results,SPARK-37121,13408525,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xkrogen,xkrogen,xkrogen,26/Oct/21 17:55,12/Dec/22 18:10,13/Jul/23 08:50,28/Oct/21 00:46,3.2.0,,,,,,,,3.2.1,3.3.0,,,Tests,,,,,0,,,,,"I was working on {{HiveExternalCatalogVersionsSuite}} recently and noticed that it was never running against the Spark 2.x release lines, only the 3.x ones. The problem was coming from here, specifically the Python 3.8+ version check:
{code}
    versions
      .filter(v => v.startsWith(""3"") || !TestUtils.isPythonVersionAtLeast38())
      .filter(v => v.startsWith(""3"") || !SystemUtils.isJavaVersionAtLeast(JavaVersion.JAVA_9))
{code}

I found that {{TestUtils.isPythonVersionAtLeast38()}} was always returning true, even when my system installation of Python3 was 3.7. Thinking it was an environment issue, I pulled up a debugger to check which version of Python the test JVM was seeing, and it was in fact Python 3.7.

Turns out the issue is with the {{isPythonVersionAtLeast38}} method:
{code}
  def isPythonVersionAtLeast38(): Boolean = {
    val attempt = if (Utils.isWindows) {
      Try(Process(Seq(""cmd.exe"", ""/C"", ""python3 --version""))
        .run(ProcessLogger(s => s.startsWith(""Python 3.8"") || s.startsWith(""Python 3.9"")))
        .exitValue())
    } else {
      Try(Process(Seq(""sh"", ""-c"", ""python3 --version""))
        .run(ProcessLogger(s => s.startsWith(""Python 3.8"") || s.startsWith(""Python 3.9"")))
        .exitValue())
    }
    attempt.isSuccess && attempt.get == 0
  }
{code}
It's trying to evaluate the version of Python using a {{ProcessLogger}}, but the logger accepts a {{String => Unit}} function, i.e., it does not make use of the return value in any way (since it's meant for logging). So the result of the {{startsWith}} checks are thrown away, and {{attempt.isSuccess && attempt.get == 0}} will always be true as long as your system has a {{python3}} binary of any version.",,apachespark,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 00:46:31 UTC 2021,,,,,,,,,,"0|z0w6k8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/21 18:07;apachespark;User 'xkrogen' has created a pull request for this issue:
https://github.com/apache/spark/pull/34395;;;","28/Oct/21 00:46;gurwls223;Issue resolved by pull request 34395
[https://github.com/apache/spark/pull/34395];;;",,,,,,,,,,,,,,
Can't read files in one of Parquet encryption modes (external keymaterial) ,SPARK-37117,13408381,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gershinsky,gershinsky,gershinsky,26/Oct/21 09:05,29/Oct/21 21:55,13/Jul/23 08:50,29/Oct/21 21:55,3.2.0,,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"Parquet encryption has a number of modes. One of them is ""external keymaterial"", which keeps encrypted data keys in a separate file (as opposed to inside Parquet file). Upon reading, the Spark Parquet connector does not pass the file path, which causes an NPE. ",,apachespark,csun,gershinsky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 06:20:03 UTC 2021,,,,,,,,,,"0|z0w5o8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/21 06:19;apachespark;User 'ggershinsky' has created a pull request for this issue:
https://github.com/apache/spark/pull/34415;;;","28/Oct/21 06:20;apachespark;User 'ggershinsky' has created a pull request for this issue:
https://github.com/apache/spark/pull/34415;;;",,,,,,,,,,,,,,
Fix MiMa failure with Scala 2.13,SPARK-37112,13408272,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,25/Oct/21 16:22,12/Dec/22 18:10,13/Jul/23 08:50,26/Oct/21 00:44,3.3.0,,,,,,,,3.3.0,,,,Build,,,,,0,,,,,"SPARK-36151 re-enabled MiMa for Scala 2.13 but it always fails in the scheduled build.
https://github.com/apache/spark/runs/3992588994?check_suite_focus=true#step:9:2303",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 26 00:44:11 UTC 2021,,,,,,,,,,"0|z0w500:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/21 16:55;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34382;;;","26/Oct/21 00:44;gurwls223;Issue resolved by pull request 34382
[https://github.com/apache/spark/pull/34382];;;",,,,,,,,,,,,,,
Switch from Maven to SBT to build Spark on AppVeyor,SPARK-37103,13408083,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,23/Oct/21 23:20,12/Dec/22 18:10,13/Jul/23 08:50,25/Oct/21 00:54,3.3.0,,,,,,,,3.3.0,,,,Project Infra,,,,,0,,,,,"Recently, building Spark on AppVeyor almost always fails due to StackOverflowError at compile time.
We can't identify the reason so far but one workaround would be building with SBT.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 25 00:54:53 UTC 2021,,,,,,,,,,"0|z0w3u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/21 23:49;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34373;;;","23/Oct/21 23:50;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34373;;;","24/Oct/21 02:41;gurwls223;Issue resolved by pull request 34373
[https://github.com/apache/spark/pull/34373];;;","25/Oct/21 00:54;gurwls223;Issue resolved by pull request 34373
[https://github.com/apache/spark/pull/34373];;;",,,,,,,,,,,,
Missing dependencies for hadoop-azure,SPARK-37102,13408067,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,vmalakhin,vmalakhin,vmalakhin,23/Oct/21 16:18,30/Oct/21 16:24,13/Jul/23 08:50,30/Oct/21 16:24,3.2.0,,,,,,,,3.3.0,,,,Build,,,,,0,,,,,"If we build spark distribution (from 3.2.0 release tag) via
{code:java}
./dev/make-distribution.sh --name custom-spark --tgz  -Pkubernetes -Phadoop-cloud{code}
and try to access Azure Data Lake storage like:
{code:java}
sqlContext.read.parquet(""my_data""){code}
{{then read operation fails with error:}}
{code:java}
java.lang.NoClassDefFoundError: org/codehaus/jackson/map/ObjectMapper{code}
{{And indeed I can see exclusion being applied in multiple places for org.codehaus.jackson:jackson-mapper-asl.}}

{{Surprisingly 3.1.2 is fine, but I believe it is because avro version was changed for 3.2.0 and update stopped bringing in such dependency.}}

{{Shall the build allow to bring extra dependencies in?}}

 ",,apachespark,csun,viirya,vmalakhin,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 30 16:24:20 UTC 2021,,,,,,,,,,"0|z0w3qg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Oct/21 17:05;vmalakhin;If I get following exclusions removed from hadoop-cloud/pom.xml under hadoop-azure dependecy

 
{code:java}
<exclusion>
 <groupId>org.codehaus.jackson</groupId>
 <artifactId>jackson-mapper-asl</artifactId>
</exclusion>
<exclusion>
 <groupId>com.fasterxml.jackson.core</groupId>
 <artifactId>jackson-core</artifactId>
</exclusion>
<exclusion>
 <groupId>com.google.guava</groupId>
 <artifactId>guava</artifactId>
</exclusion>{code}
 

then jackson-mapper-asl-1.9.13.jar  gets added and included into the build output and abfs connector starts to work.

 ;;;","25/Oct/21 18:15;apachespark;User 'vmalakhin' has created a pull request for this issue:
https://github.com/apache/spark/pull/34383;;;","30/Oct/21 16:24;srowen;Issue resolved by pull request 34383
[https://github.com/apache/spark/pull/34383];;;",,,,,,,,,,,,,
Alter table properties should invalidate cache,SPARK-37098,13407921,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,22/Oct/21 09:08,26/Oct/21 20:30,13/Jul/23 08:50,25/Oct/21 08:07,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.0.4,3.1.3,3.2.1,3.3.0,SQL,,,,,0,,,,,"The table properties can change the behavior of wriing. e.g. the parquet table with `parquet.compression`.

If you execute the following SQL, we will get the file with snappy compression rather than zstd.
{code:java}
CREATE TABLE t (c int) STORED AS PARQUET;
// cache table metadata
SELECT * FROM t;
ALTER TABLE t SET TBLPROPERTIES('parquet.compression'='zstd');
INSERT INTO TABLE t values(1);
{code}
So we should invalidate the table cache after alter table properties.",,apachespark,Qin Yao,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 26 04:58:55 UTC 2021,,,,,,,,,,"0|z0w2u0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/21 09:44;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34365;;;","25/Oct/21 08:07;Qin Yao;Issuse resolved by [https://github.com/apache/spark/pull/34365];;;","25/Oct/21 08:53;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34379;;;","26/Oct/21 04:57;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34390;;;","26/Oct/21 04:58;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34390;;;",,,,,,,,,,,
"ParquetFileFormat registers task completion listeners lazily, causing Python writer thread to segfault when off-heap vectorized reader is enabled",SPARK-37089,13407820,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,21/Oct/21 18:43,12/Dec/22 18:10,13/Jul/23 08:50,25/Oct/21 01:01,3.0.3,3.1.2,3.2.0,,,,,,3.2.1,3.3.0,,,PySpark,SQL,,,,0,,,,,"The task completion listener that closes the vectorized reader is registered lazily in ParquetFileFormat#buildReaderWithPartitionValues(). Since task completion listeners are executed in reverse order of registration, it always runs before the Python writer thread can be interrupted.

This contradicts the assumption in https://issues.apache.org/jira/browse/SPARK-37088 / https://github.com/apache/spark/pull/34245 that task completion listeners are registered bottom-up, preventing that fix from working properly.",,ankurd,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 25 01:01:52 UTC 2021,,,,,,,,,,"0|z0w27k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/21 14:43;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/34369;;;","25/Oct/21 01:01;gurwls223;Issue resolved by pull request 34369
[https://github.com/apache/spark/pull/34369];;;",,,,,,,,,,,,,,
Python UDF after off-heap vectorized reader can cause crash due to use-after-free in writer thread,SPARK-37088,13407787,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ankurd,ankurd,ankurd,21/Oct/21 14:57,21/Oct/21 15:43,13/Jul/23 08:50,21/Oct/21 15:35,3.0.3,3.1.2,3.2.0,,,,,,3.2.1,3.3.0,,,PySpark,Spark Core,SQL,,,0,,,,,"Python UDFs in Spark SQL are run in a separate Python process. The Python process is fed input by a dedicated thread (`BasePythonRunner.WriterThread`). This writer thread drives the child plan by pulling rows from its output iterator and serializing them across a socket.

When the child exec node is the off-heap vectorized Parquet reader, these rows are backed by off-heap memory. The child node uses a task completion listener to free the off-heap memory at the end of the task, which invalidates the output iterator and any rows it has produced. Since task completion listeners are registered bottom-up and executed in reverse order of registration, this is safe as long as an exec node never accesses its input after its task completion listener has executed.

The BasePythonRunner task completion listener violates this assumption. It interrupts the writer thread, but does not wait for it to exit. This causes a race condition that can lead to an executor crash:
1. The Python writer thread is processing a row backed by off-heap memory.
2. The task finishes, for example because it has reached a row limit.
3. The BasePythonRunner task completion listener sets the interrupt status of the writer thread, but the writer thread does not check it immediately.
4. The child plan's task completion listener frees its off-heap memory, invalidating the row that the Python writer thread is processing.
5. The Python writer thread attempts to access the invalidated row. The use-after-free triggers a segfault that crashes the executor.

https://issues.apache.org/jira/browse/SPARK-33277 describes the same issue, but the fix was incomplete. It did not address the situation where the Python writer thread accesses a freed row.",,ankurd,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 21 15:43:26 UTC 2021,,,,,,,,,,"0|z0w208:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/21 15:07;ankurd;https://github.com/apache/spark/pull/34245;;;","21/Oct/21 15:43;apachespark;User 'ankurdave' has created a pull request for this issue:
https://github.com/apache/spark/pull/34245;;;",,,,,,,,,,,,,,
Fix the R test of FPGrowthModel for Scala 2.13,SPARK-37086,13407782,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,21/Oct/21 14:12,12/Dec/22 18:10,13/Jul/23 08:50,22/Oct/21 04:35,3.3.0,,,,,,,,3.3.0,,,,ML,R,Tests,,,0,,,,,"Similar to the issue filed in SPARK-37059, the R test of FPGrowthModel assumes that the result records returned by FPGrowthModel.freqItemsets are sorted by a certain kind of order but it's wrong.
As a result, the test fails with Scala 2.13.

{code}
 ══ Failed ══════════════════════════════════════════════════════════════════════
── 1. Failure (test_mllib_fpm.R:42:3): spark.fpGrowth ──────────────────────────
`expected_itemsets` not equivalent to `itemsets`.
Component “items”: Component 1: Component 1: 1 string mismatch
Component “items”: Component 2: Length mismatch: comparison on first 1 components
Component “items”: Component 2: Component 1: 1 string mismatch
Component “items”: Component 3: Length mismatch: comparison on first 1 components
Component “items”: Component 4: Length mismatch: comparison on first 1 components
Component “items”: Component 4: Component 1: 1 string mismatch
Component “items”: Component 5: Length mismatch: comparison on first 1 components
Component “items”: Component 5: Component 1: 1 string mismatch
Component “freq”: Mean relative difference: 0.5454545
{code}",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 22 04:35:56 UTC 2021,,,,,,,,,,"0|z0w1z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/21 14:41;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34357;;;","21/Oct/21 14:42;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34357;;;","22/Oct/21 04:35;gurwls223;Issue resolved by pull request 34357
[https://github.com/apache/spark/pull/34357];;;",,,,,,,,,,,,,
Fix DataFrameWriterV2.partitionedBy to send the arguments to JVM properly,SPARK-37079,13407639,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ueshin,ueshin,ueshin,20/Oct/21 23:28,12/Dec/22 18:11,13/Jul/23 08:50,21/Oct/21 01:28,3.1.2,3.2.0,3.3.0,,,,,,3.1.3,3.2.1,3.3.0,,PySpark,SQL,,,,0,,,,,"In PySpark, {{DataFrameWriterV2.partitionedBy}} doesn't send the arguments to JVM properly.",,aajisaka,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 21 01:28:36 UTC 2021,,,,,,,,,,"0|z0w13c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/21 23:38;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/34347;;;","21/Oct/21 01:28;gurwls223;Issue resolved by pull request 34347
[https://github.com/apache/spark/pull/34347];;;",,,,,,,,,,,,,,
Support old 3-parameter Sink constructors,SPARK-37078,13407638,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,dongjoon,dongjoon,20/Oct/21 23:09,21/Oct/21 02:06,13/Jul/23 08:50,21/Oct/21 02:06,3.2.0,,,,,,,,3.2.1,3.3.0,,,Spark Core,,,,,0,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,SPARK-34520,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 21 02:06:15 UTC 2021,,,,,,,,,,"0|z0w134:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/21 00:00;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34348;;;","21/Oct/21 02:06;dongjoon;Issue resolved by pull request 34348
[https://github.com/apache/spark/pull/34348];;;",,,,,,,,,,,,,,
Implement StructType.toString explicitly for Scala 2.13,SPARK-37076,13407594,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,20/Oct/21 17:53,12/Dec/22 18:11,13/Jul/23 08:50,21/Oct/21 04:49,3.3.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"The string returned by StructType.toString is different  between Scala 2.12 and 2.13.

* Scala 2.12
{code}
val st = StructType(StructField(""a"", IntegerType) :: Nil)
st.toString
res0: String = StructType(StructField(a,IntegerType,true)
{code}

* Scala 2.13
{code}
val st = StructType(StructField(""a"", IntegerType) :: Nil)
st.toString
val res0: String = Seq(StructField(a,IntegerType,true))
{code}

It's because the logic to make the prefix of the string was changed from Scala 2.13.

Scala 2.12: https://github.com/scala/scala/blob/v2.12.15/src/library/scala/collection/TraversableLike.scala#L804
Scala 2:13:https://github.com/scala/scala/blob/v2.13.5/src/library/scala/collection/Seq.scala#L46",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 21 04:49:14 UTC 2021,,,,,,,,,,"0|z0w0tc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/21 18:39;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34341;;;","21/Oct/21 04:49;gurwls223;Issue resolved by pull request 34341
[https://github.com/apache/spark/pull/34341];;;",,,,,,,,,,,,,,
HiveClientImpl throws NoSuchMethodError: org.apache.hadoop.hive.ql.metadata.Hive.getWithoutRegisterFns,SPARK-37069,13407455,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,zhouyifan279,zhouyifan279,20/Oct/21 08:11,12/Dec/22 18:10,13/Jul/23 08:50,22/Oct/21 04:31,3.2.0,,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"If we run Spark SQL with external Hive 2.3.x (before 2.3.9) jars, following error will be thrown:
{code:java}
Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.hadoop.hive.ql.metadata.Hive.getWithoutRegisterFns(Lorg/apache/hadoop/hive/conf/HiveConf;)Lorg/apache/hadoop/hive/ql/metadata/Hive;Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.hadoop.hive.ql.metadata.Hive.getWithoutRegisterFns(Lorg/apache/hadoop/hive/conf/HiveConf;)Lorg/apache/hadoop/hive/ql/metadata/Hive; at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getHive$1(HiveClientImpl.scala:205) at scala.Option.map(Option.scala:230) at org.apache.spark.sql.hive.client.HiveClientImpl.getHive(HiveClientImpl.scala:204) at org.apache.spark.sql.hive.client.HiveClientImpl.client(HiveClientImpl.scala:267) at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:292) at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:234) at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:233) at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:283) at org.apache.spark.sql.hive.client.HiveClientImpl.databaseExists(HiveClientImpl.scala:394) at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$databaseExists$1(HiveExternalCatalog.scala:224) at scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23) at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:102) at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:224) at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:150) at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:140) at org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:170) at org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:168) at org.apache.spark.sql.hive.HiveSessionStateBuilder.$anonfun$catalog$2(HiveSessionStateBuilder.scala:61) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:119) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:119) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:1004) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:990) at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listTables(SessionCatalog.scala:982) at org.apache.spark.sql.execution.command.ShowTablesCommand.$anonfun$run$42(tables.scala:828) at scala.Option.getOrElse(Option.scala:189) at org.apache.spark.sql.execution.command.ShowTablesCommand.run(tables.scala:828) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75) at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73) at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84) at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110) at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267) at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457) at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106) at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93) at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91) at org.apache.spark.sql.Dataset.<init>(Dataset.scala:219) at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96) at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775) at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613) at Main$.main(Main.scala:13) at Main.main(Main.scala)
{code}
Here is a demo to reproduce the error.

pom.xml
{code:java}
<properties>
    <maven.compiler.source>8</maven.compiler.source>
    <maven.compiler.target>8</maven.compiler.target>
</properties>

<dependencies>
    <dependency>
        <groupId>org.apache.spark</groupId>
        <artifactId>spark-hive_2.12</artifactId>
        <version>3.2.0</version>
    </dependency>
</dependencies>
{code}
 

Main.scala

 
{code:java}
import org.apache.spark.sql.SparkSession

object Main {

  def main(args: Array[String]): Unit = {
    val spark = SparkSession.builder()
      .master(""local[*]"")
      .enableHiveSupport()
      .config(""spark.sql.hive.metastore.version"", ""2.3.8"")
      .config(""spark.sql.hive.metastore.jars"", ""/bigdata/apache-hive-2.3.8-bin/lib/*"")
      .getOrCreate()

    spark.sql(""show tables"")
  }

}
{code}
 

 ",,apachespark,csun,zhouyifan279,,,,,,,,,,,,,,,,,,,,,SPARK-35321,SPARK-34512,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Oct 22 04:31:49 UTC 2021,,,,,,,,,,"0|z0vzyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/21 08:45;zhouyifan279;IsolatedClientLoader#hiveVersion resolves version string '2.3.x' to HiveVersion hive.v2_3.

In SPARK-34512,  hive.v2_3.fullVersion is upgraded from '2.3.8' to '2.3.9'.

In SPARK-35321, the following code is used to create Hive object
{code:java}
private def getHive(conf: HiveConf): Hive = {
  VersionUtils.majorMinorPatchVersion(version.fullVersion).map {
    case (2, 3, v) if v >= 9 => Hive.getWithoutRegisterFns(conf)
    case _ => Hive.get(conf)
  }.getOrElse {
    throw QueryExecutionErrors.unsupportedHiveMetastoreVersionError(
      version.fullVersion, HiveUtils.HIVE_METASTORE_VERSION.key)
  }
}
{code}
As *version.fullVersion* is always '2.3.9', *case (2, 3, v) if v >= 9* always matches.

 ;;;","21/Oct/21 05:52;zhouyifan279;Gently ping [~csun];;;","21/Oct/21 17:28;csun;Thanks for the ping [~zhouyifan279]! yes this is a bug, and let me see how to fix it.;;;","21/Oct/21 19:42;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34360;;;","22/Oct/21 04:31;gurwls223;Issue resolved by pull request 34360
[https://github.com/apache/spark/pull/34360];;;",,,,,,,,,,,
Fix outer join return the wrong max rows if other side is empty,SPARK-37064,13407408,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,ulysses,ulysses,20/Oct/21 02:11,20/Oct/21 08:05,13/Jul/23 08:50,20/Oct/21 08:05,3.2.0,3.3.0,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"Outer join should return at least num rows of it's outer side, i.e left outer join with its left side, right outer join with its right side, full outer join with its both side.",,apachespark,cloud_fan,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 20 08:05:10 UTC 2021,,,,,,,,,,"0|z0vzo0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/21 02:36;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34336;;;","20/Oct/21 08:05;cloud_fan;Issue resolved by pull request 34336
[https://github.com/apache/spark/pull/34336];;;",,,,,,,,,,,,,,
Custom V2 Metrics uses wrong classname for lookup,SPARK-37061,13407373,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,rspitzer,rspitzer,rspitzer,19/Oct/21 19:39,21/Oct/21 00:59,13/Jul/23 08:50,21/Oct/21 00:59,3.2.0,,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"Currently CustomMetrics uses `getCanonicalName` to get the metric type name

https://github.com/apache/spark/blob/38493401d18d42a6cb176bf515536af97ba1338b/sql/core/src/main/scala/org/apache/spark/sql/execution/metric/CustomMetrics.scala#L31-L33

But when using reflection we need to use the original type name.

Here is an example when working with an inner class

{code:java title=""CanonicalName vs Name""}
Class.getName = org.apache.iceberg.spark.source.SparkBatchScan$FilesScannedMetric
Class.getCanonicalName =
org.apache.iceberg.spark.source.SparkBatchScan.FilesScannedMetric
{code}


The ""$"" name is required to look up this class while the ""."" version will fail with CNF.",,apachespark,dongjoon,rspitzer,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 21 00:59:55 UTC 2021,,,,,,,,,,"0|z0vzg8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Oct/21 22:38;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/34344;;;","20/Oct/21 22:47;apachespark;User 'RussellSpitzer' has created a pull request for this issue:
https://github.com/apache/spark/pull/34345;;;","21/Oct/21 00:59;dongjoon;Issue resolved by pull request 34345
[https://github.com/apache/spark/pull/34345];;;",,,,,,,,,,,,,
Report driver status does not handle response from backup masters,SPARK-37060,13407308,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,m.rostami,m.rostami,m.rostami,19/Oct/21 14:29,16/Dec/21 07:25,13/Jul/23 08:50,15/Dec/21 13:01,3.1.0,3.1.1,3.1.2,3.2.0,,,,,3.1.3,3.2.1,3.3.0,,Spark Core,,,,,0,,,,,"After an improvement in SPARK-31486, contributor uses 'asyncSendToMasterAndForwardReply' method instead of 'activeMasterEndpoint.askSync' to get the status of driver. Since the driver's status is only available in active master and the 'asyncSendToMasterAndForwardReply' method iterate over all of the masters, we have to handle the response from the backup masters in the client, which the developer did not consider in the SPARK-31486 change. So drivers running in cluster mode and on a cluster with multi-master affected by this bug. I created the patch for this bug and will soon be sent the pull request.",,apachespark,m.rostami,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Dec 15 13:49:55 UTC 2021,,,,,,,,,,"0|z0vz1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/21 15:33;apachespark;User 'mohamadrezarostami' has created a pull request for this issue:
https://github.com/apache/spark/pull/34331;;;","15/Dec/21 13:01;Ngone51;Issue resolved by https://github.com/apache/spark/pull/34331;;;","15/Dec/21 13:46;apachespark;User 'mohamadrezarostami' has created a pull request for this issue:
https://github.com/apache/spark/pull/34911;;;","15/Dec/21 13:49;apachespark;User 'mohamadrezarostami' has created a pull request for this issue:
https://github.com/apache/spark/pull/34912;;;","15/Dec/21 13:49;apachespark;User 'mohamadrezarostami' has created a pull request for this issue:
https://github.com/apache/spark/pull/34912;;;",,,,,,,,,,,
Ensure the sort order of the output in the PySpark doctests,SPARK-37059,13407288,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,19/Oct/21 12:46,20/Oct/21 00:52,13/Jul/23 08:50,20/Oct/21 00:52,3.3.0,,,,,,,,3.3.0,,,,PySpark,Tests,,,,0,,,,,"The collect_set builtin function doesn't ensure the sort order of its result for each row. FPGrouthModel.freqItemsets also doesn't ensure the sort order of the result rows.
Nevertheless, their PySpark doctests assume a certain kind of sort order, causing that such doctests fail with Scala 2.13.
",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 20 00:52:43 UTC 2021,,,,,,,,,,"0|z0vyxc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/21 13:34;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34330;;;","20/Oct/21 00:52;sarutak;Issue resolved in https://github.com/apache/spark/pull/34330;;;",,,,,,,,,,,,,,
Fix wrong DocSearch facet filter in release-tag.sh,SPARK-37057,13407215,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,19/Oct/21 06:52,19/Oct/21 08:53,13/Jul/23 08:50,19/Oct/21 08:53,3.2.0,3.3.0,,,,,,,3.2.1,3.3.0,,,Project Infra,,,,,0,,,,,"In release-tag.sh, the DocSearch facet filter should be updated as the release version before creating  git tag. 
If we missed the step, the facet filter is wrong in the new release doc:  https://github.com/apache/spark/blame/v3.2.0/docs/_config.yml#L42",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 19 08:53:11 UTC 2021,,,,,,,,,,"0|z0vyh4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/21 07:32;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34328;;;","19/Oct/21 08:53;Gengliang.Wang;Issue resolved by pull request 34328
[https://github.com/apache/spark/pull/34328];;;",,,,,,,,,,,,,,
Fix spark-3.2 can use --verbose with spark-shell,SPARK-37052,13407196,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,19/Oct/21 03:37,09/Nov/21 06:14,13/Jul/23 08:50,19/Oct/21 06:41,3.2.0,,,,,,,,3.2.1,3.3.0,,,Spark Shell,,,,,0,,,,,Should not pass --verbose to spark-shell since it's not a valid argument for spark-shell,,angerszhuuu,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,SPARK-37242,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 19 06:41:26 UTC 2021,,,,,,,,,,"0|z0vycw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Oct/21 03:43;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34322;;;","19/Oct/21 03:43;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34322;;;","19/Oct/21 06:41;cloud_fan;Issue resolved by pull request 34322
[https://github.com/apache/spark/pull/34322];;;",,,,,,,,,,,,,
executorIdleTimeout is not working for pending pods on K8s,SPARK-37049,13407173,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,wwei,wwei,wwei,18/Oct/21 23:08,26/Oct/21 08:06,13/Jul/23 08:50,20/Oct/21 05:52,3.1.0,,,,,,,,3.1.3,3.2.1,3.3.0,,Kubernetes,Spark Core,,,,0,,,,,"SPARK-33099 added the support to respect ""spark.dynamicAllocation.executorIdleTimeout"" in ExecutorPodsAllocator. However, when it checks if a pending executor pod is timed out, it checks against the pod's ""startTime"". A pending pod ""startTime"" is empty, and this causes the function ""isExecutorIdleTimedOut()"" always return true for pending pods.

This caused the issue, pending pods are deleted immediately when a stage is finished and several new pods got recreated again in the next stage. ",,apachespark,dongjoon,wwei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 26 08:06:12 UTC 2021,,,,,,,,,,"0|z0vy7s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/21 23:32;apachespark;User 'yangwwei' has created a pull request for this issue:
https://github.com/apache/spark/pull/34319;;;","18/Oct/21 23:32;apachespark;User 'yangwwei' has created a pull request for this issue:
https://github.com/apache/spark/pull/34319;;;","20/Oct/21 05:52;dongjoon;Issue resolved by pull request 34319
[https://github.com/apache/spark/pull/34319];;;","20/Oct/21 06:29;wwei;Thank you [~dongjoon].
Seems like this issue is assigned to the wrong ID, could u pls grant the contributor role to my ID (username: wwei, full name: Weiwei Yang)? Thanks!;;;","26/Oct/21 08:04;dongjoon;Oh, sure. Sorry, [~wwei].;;;","26/Oct/21 08:06;dongjoon;It's fixed now.;;;",,,,,,,,,,
Alter view does not preserve column case,SPARK-37046,13407119,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,asomani,asomani,asomani,18/Oct/21 17:22,20/Oct/21 08:44,13/Jul/23 08:50,20/Oct/21 08:43,3.2.0,,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"On running an `alter view` command, the column case is not preserved.

Repro:

 
{code:java}
scala> sql(""create view v as select 1 as A, 1 as B"")
res2: org.apache.spark.sql.DataFrame = []

scala> sql(""describe v"").collect.foreach(println)
[A,int,null]
[B,int,null]

scala> sql(""alter view v as select 1 as C, 1 as D"")
res4: org.apache.spark.sql.DataFrame = []

scala> sql(""describe v"").collect.foreach(println)
[c,int,null]
[d,int,null]
 
{code}
 

 ",,apachespark,asomani,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 20 08:43:50 UTC 2021,,,,,,,,,,"0|z0vxvs:",9223372036854775807,,,,,cloud_fan,,,,,,,,,,,,,,,,,,"18/Oct/21 17:22;asomani;I'll raise a PR soon;;;","18/Oct/21 17:40;apachespark;User 'somani' has created a pull request for this issue:
https://github.com/apache/spark/pull/34317;;;","20/Oct/21 08:43;cloud_fan;Issue resolved by pull request 34317
[https://github.com/apache/spark/pull/34317];;;",,,,,,,,,,,,,
Ensure the element type of ResolvedRFormula.terms is scala.Seq for Scala 2.13,SPARK-37026,13406920,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,17/Oct/21 02:59,12/Dec/22 18:11,13/Jul/23 08:50,18/Oct/21 01:03,3.3.0,,,,,,,,3.3.0,,,,Build,ML,,,,0,,,,,ResolvedRFormula.toString throws ClassCastException with Scala 2.13 because the type of ResolvedRFormula.terms is scala.Seq[scala.Seq[String]] but scala.Seq[scala.collection.mutable.ArraySeq$ofRef] will be passed.,,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 18 01:03:25 UTC 2021,,,,,,,,,,"0|z0vwnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/21 03:26;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34301;;;","18/Oct/21 01:03;gurwls223;Issue resolved by pull request 34301
[https://github.com/apache/spark/pull/34301];;;",,,,,,,,,,,,,,
Reduce the scope of synchronized to prevent deadlock.,SPARK-37017,13406719,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Zhixiong Chen,Zhixiong Chen,Zhixiong Chen,15/Oct/21 07:29,19/Oct/21 06:50,13/Jul/23 08:50,19/Oct/21 06:48,3.1.1,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"There is a synchronized in CatalogManager.currentNamespace function.

Sometimes a deadlock occurs.

The scope of synchronized can be reduced to prevent deadlock.",,apachespark,cloud_fan,xkrogen,Zhixiong Chen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 19 06:48:59 UTC 2021,,,,,,,,,,"0|z0vvew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/21 07:59;Zhixiong Chen;I have created a pull request for this issue: https://github.com/apache/spark/pull/34292;;;","15/Oct/21 08:21;apachespark;User 'chenzhx' has created a pull request for this issue:
https://github.com/apache/spark/pull/34292;;;","19/Oct/21 06:48;cloud_fan;Issue resolved by pull request 34292
[https://github.com/apache/spark/pull/34292];;;",,,,,,,,,,,,,
Job cancellation causes py4j errors on Jupyter due to pinned thread mode,SPARK-37004,13406471,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,gurwls223,mengxr,mengxr,14/Oct/21 05:15,12/Dec/22 18:11,13/Jul/23 08:50,06/Dec/21 08:36,3.2.0,,,,,,,,3.2.1,3.3.0,,,PySpark,,,,,0,,,,,"Spark 3.2.0 turned on py4j pinned thread mode by default (SPARK-35303). However, in a jupyter notebook, after I cancel (interrupt) a long-running Spark job, the next Spark command will fail with some py4j errors. See attached notebook for repro.

Cannot reproduce the issue after I turn off pinned thread mode .",,apachespark,joshrosen,mengxr,ueshin,viirya,,,,,,,,,,,,,,,,,,,SPARK-35303,,,,,,,,"14/Oct/21 05:16;mengxr;pinned.ipynb;https://issues.apache.org/jira/secure/attachment/13034941/pinned.ipynb",,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Dec 06 08:36:34 UTC 2021,,,,,,,,,,"0|z0vtw0:",9223372036854775807,,,,,,,,,,,,,3.2.1,,,,,,,,,,"14/Oct/21 07:40;gurwls223;It was found that it's a Py4J issue. I made a fix (https://github.com/bartdag/py4j/pull/440).;;;","16/Oct/21 05:18;gurwls223;Just for other people who face this issue from Spark 3.2.0: workaround for this issue is to set {{PYSPARK_PIN_THREAD}} environment variable to {{false}}.;;;","06/Dec/21 05:46;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/34814;;;","06/Dec/21 05:47;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/34814;;;","06/Dec/21 08:36;gurwls223;Fixed in https://github.com/apache/spark/pull/34814;;;",,,,,,,,,,,
Fix json_tuple throw NPE if fields exist no foldable null value,SPARK-36993,13406262,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,13/Oct/21 04:44,14/Oct/21 07:00,13/Jul/23 08:50,13/Oct/21 16:38,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.0.4,3.1.3,3.2.1,3.3.0,SQL,,,,,0,,,,,"If json_tuple exists no foldable null field, Spark would throw NPE during eval field.toString.

e.g. the query will fail with:

{code:java}
SELECT json_tuple('{""a"":""1""}', if(c1 < 1, null, 'a')) FROM ( SELECT rand() AS c1 );
{code}

{code:java}
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.JsonTuple.$anonfun$parseRow$2(jsonExpressions.scala:435)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple.parseRow(jsonExpressions.scala:435)
	at org.apache.spark.sql.catalyst.expressions.JsonTuple.$anonfun$eval$6(jsonExpressions.scala:413)

{code}
",,apachespark,maxgekk,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 14 01:34:54 UTC 2021,,,,,,,,,,"0|z0vsmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Oct/21 04:52;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34268;;;","13/Oct/21 04:53;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34268;;;","13/Oct/21 16:38;maxgekk;Issue resolved by pull request 34268
[https://github.com/apache/spark/pull/34268];;;","14/Oct/21 01:34;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34279;;;",,,,,,,,,,,,
Future typing errors in pyspark.pandas,SPARK-36985,13406117,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ueshin,zero323,zero323,12/Oct/21 10:13,12/Dec/22 18:11,13/Jul/23 08:50,13/Oct/21 01:25,3.3.0,,,,,,,,3.3.0,,,,PySpark,,,,,0,,,,,"The following problems are detected on master with mypy 0.920
{code:java}
$ git rev-parse HEAD             
36b3bbc0aa9f9c39677960cd93f32988c7d7aaca
$ mypy --version                 
mypy 0.920+dev.332b712df848cd242987864b38bd237364654532
$ mypy --config-file mypy.ini pyspark
pyspark/pandas/indexes/base.py:184: error: Incompatible types in assignment (expression has type ""CategoricalIndex"", variable has type ""MultiIndex"")  [assignment]
pyspark/pandas/indexes/base.py:188: error: Incompatible types in assignment (expression has type ""Int64Index"", variable has type ""MultiIndex"")  [assignment]
pyspark/pandas/indexes/base.py:192: error: Incompatible types in assignment (expression has type ""Float64Index"", variable has type ""MultiIndex"")  [assignment]
pyspark/pandas/indexes/base.py:197: error: Incompatible types in assignment (expression has type ""DatetimeIndex"", variable has type ""MultiIndex"")  [assignment]
pyspark/pandas/indexes/base.py:199: error: Incompatible types in assignment (expression has type ""Index"", variable has type ""MultiIndex"")  [assignment]
pyspark/pandas/indexes/base.py:201: error: ""MultiIndex"" has no attribute ""_anchor""  [attr-defined]
{code}",,apachespark,ueshin,zero323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 13 01:25:49 UTC 2021,,,,,,,,,,"0|z0vrq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/21 10:17;zero323;cc [~hyukjin.kwon] [~XinrongM] [~ueshin];;;","13/Oct/21 00:23;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/34266;;;","13/Oct/21 00:23;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/34266;;;","13/Oct/21 01:25;gurwls223;Fixed in https://github.com/apache/spark/pull/34266;;;",,,,,,,,,,,,
Add RewriteLateralSubquery rule into nonExcludableRules,SPARK-36979,13406048,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ulysses,ulysses,ulysses,12/Oct/21 02:42,19/Jun/22 03:47,13/Jul/23 08:50,12/Oct/21 23:13,3.3.0,,,,,,,,3.2.1,,,,SQL,,,,,0,,,,,"Lateral Join has no meaning without rule `RewriteLateralSubquery`. So now if we set `spark.sql.optimizer.excludedRules=org.apache.spark.sql.catalyst.optimizer.RewriteLateralSubquery`, the lateral join query will fail with:
{code:java}
java.lang.AssertionError: assertion failed: No plan for LateralJoin lateral-subquery#218
{code}
",,apachespark,dongjoon,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jun 19 03:47:59 UTC 2022,,,,,,,,,,"0|z0vrao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Oct/21 02:48;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34249;;;","12/Oct/21 11:25;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/34260;;;","12/Oct/21 23:13;dongjoon;Issue resolved by pull request 34260
[https://github.com/apache/spark/pull/34260];;;","19/Jun/22 03:47;apachespark;User 'wangyum' has created a pull request for this issue:
https://github.com/apache/spark/pull/36910;;;",,,,,,,,,,,,
Discrepancy in Q22 of TPCH for Spark 3.2,SPARK-36926,13404826,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,cloud_fan,apatnam,apatnam,04/Oct/21 17:24,12/Dec/22 18:10,13/Jul/23 08:50,06/Oct/21 05:31,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"When running TPCH scale 100 against 3.2, Query 22 has a discrepancy in the number of rows returned by the query. This was tested with both AQE on and off. All the other queries were matching in results. Below is the results that we got when testing Q22 on 3.2: 

 
{code:java}
  ""results"": [
    {
      ""name"": ""Q22"",
      ""mode"": ""collect"",
      ""parameters"": {},
      ""joinTypes"": [
        ""SortMergeJoin""
      ],
      ""tables"": [
        ""customer""
      ],
      ""parsingTime"": 0.016522,
      ""analysisTime"": 0.004132,
      ""optimizationTime"": 39.173868,
      ""planningTime"": 23.10939,
      ""executionTime"": 13762.183844,
      ""result"": 0,
      ""breakDown"": [],
      ""queryExecution"": ""== Parsed Logical Plan ==\n'Sort ['cntrycode ASC NULLS FIRST], true\n+- 'Aggregate ['cntrycode], ['cntrycode, 'count(1) AS numcust#150, 'sum('c_acctbal) AS totacctbal#151]\n   +- 'SubqueryAlias custsale\n      +- 'Project ['substring('c_phone, 1, 2) AS cntrycode#147, 'c_acctbal]\n         +- 'Filter (('substring('c_phone, 1, 2) IN (13,31,23,29,30,18,17) AND ('c_acctbal > scalar-subquery#148 [])) AND NOT exists#149 [])\n            :  :- 'Project [unresolvedalias('avg('c_acctbal), None)]\n            :  :  +- 'Filter (('c_acctbal > 0.00) AND 'substring('c_phone, 1, 2) IN (13,31,23,29,30,18,17))\n            :  :     +- 'UnresolvedRelation [customer], [], false\n            :  +- 'Project [*]\n            :     +- 'Filter ('o_custkey = 'c_custkey)\n            :        +- 'UnresolvedRelation [orders], [], false\n            +- 'UnresolvedRelation [customer], [], false\n\n== Analyzed Logical Plan ==\ncntrycode: string, numcust: bigint, totacctbal: decimal(22,2)\nSort [cntrycode#147 ASC NULLS FIRST], true\n+- Aggregate [cntrycode#147], [cntrycode#147, count(1) AS numcust#150L, sum(c_acctbal#11) AS totacctbal#151]\n   +- SubqueryAlias custsale\n      +- Project [substring(c_phone#10, 1, 2) AS cntrycode#147, c_acctbal#11]\n         +- Filter ((substring(c_phone#10, 1, 2) IN (13,31,23,29,30,18,17) AND (cast(c_acctbal#11 as decimal(16,6)) > cast(scalar-subquery#148 [] as decimal(16,6)))) AND NOT exists#149 [c_custkey#6L])\n            :  :- Aggregate [avg(c_acctbal#160) AS avg(c_acctbal)#154]\n            :  :  +- Filter ((cast(c_acctbal#160 as decimal(12,2)) > cast(0.00 as decimal(12,2))) AND substring(c_phone#159, 1, 2) IN (13,31,23,29,30,18,17))\n            :  :     +- SubqueryAlias spark_catalog.tpch_data_orc_100.customer\n            :  :        +- Relation tpch_data_orc_100.customer[c_custkey#155L,c_name#156,c_address#157,c_nationkey#158L,c_phone#159,c_acctbal#160,c_comment#161,c_mktsegment#162] orc\n            :  +- Project [o_orderkey#16L, o_custkey#17L, o_orderstatus#18, o_totalprice#19, o_orderpriority#20, o_clerk#21, o_shippriority#22, o_comment#23, o_orderdate#24]\n            :     +- Filter (o_custkey#17L = outer(c_custkey#6L))\n            :        +- SubqueryAlias spark_catalog.tpch_data_orc_100.orders\n            :           +- Relation tpch_data_orc_100.orders[o_orderkey#16L,o_custkey#17L,o_orderstatus#18,o_totalprice#19,o_orderpriority#20,o_clerk#21,o_shippriority#22,o_comment#23,o_orderdate#24] orc\n            +- SubqueryAlias spark_catalog.tpch_data_orc_100.customer\n               +- Relation tpch_data_orc_100.customer[c_custkey#6L,c_name#7,c_address#8,c_nationkey#9L,c_phone#10,c_acctbal#11,c_comment#12,c_mktsegment#13] orc\n\n== Optimized Logical Plan ==\nSort [cntrycode#147 ASC NULLS FIRST], true\n+- Aggregate [cntrycode#147], [cntrycode#147, count(1) AS numcust#150L, sum(c_acctbal#11) AS totacctbal#151]\n   +- Project [substring(c_phone#10, 1, 2) AS cntrycode#147, c_acctbal#11]\n      +- Join LeftAnti, (o_custkey#17L = c_custkey#6L)\n         :- Project [c_custkey#6L, c_phone#10, c_acctbal#11]\n         :  +- Filter ((isnotnull(c_acctbal#11) AND substring(c_phone#10, 1, 2) IN (13,31,23,29,30,18,17)) AND (cast(c_acctbal#11 as decimal(16,6)) > scalar-subquery#148 []))\n         :     :  +- Aggregate [avg(c_acctbal#160) AS avg(c_acctbal)#154]\n         :     :     +- Project [c_acctbal#160]\n         :     :        +- Filter (isnotnull(c_acctbal#160) AND ((c_acctbal#160 > 0.00) AND substring(c_phone#159, 1, 2) IN (13,31,23,29,30,18,17)))\n         :     :           +- Relation tpch_data_orc_100.customer[c_custkey#155L,c_name#156,c_address#157,c_nationkey#158L,c_phone#159,c_acctbal#160,c_comment#161,c_mktsegment#162] orc\n         :     +- Relation tpch_data_orc_100.customer[c_custkey#6L,c_name#7,c_address#8,c_nationkey#9L,c_phone#10,c_acctbal#11,c_comment#12,c_mktsegment#13] orc\n         +- Project [o_custkey#17L]\n            +- Relation tpch_data_orc_100.orders[o_orderkey#16L,o_custkey#17L,o_orderstatus#18,o_totalprice#19,o_orderpriority#20,o_clerk#21,o_shippriority#22,o_comment#23,o_orderdate#24] orc\n\n== Physical Plan ==\n*(7) Sort [cntrycode#147 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(cntrycode#147 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [id=#314]\n   +- *(6) HashAggregate(keys=[cntrycode#147], functions=[count(1), sum(c_acctbal#11)], output=[cntrycode#147, numcust#150L, totacctbal#151])\n      +- Exchange hashpartitioning(cntrycode#147, 200), ENSURE_REQUIREMENTS, [id=#310]\n         +- *(5) HashAggregate(keys=[cntrycode#147], functions=[partial_count(1), partial_sum(c_acctbal#11)], output=[cntrycode#147, count#170L, sum#171, isEmpty#172])\n            +- *(5) Project [substring(c_phone#10, 1, 2) AS cntrycode#147, c_acctbal#11]\n               +- *(5) SortMergeJoin [c_custkey#6L], [o_custkey#17L], LeftAnti\n                  :- *(2) Sort [c_custkey#6L ASC NULLS FIRST], false, 0\n                  :  +- Exchange hashpartitioning(c_custkey#6L, 200), ENSURE_REQUIREMENTS, [id=#292]\n                  :     +- *(1) Project [c_custkey#6L, c_phone#10, c_acctbal#11]\n                  :        +- *(1) Filter ((isnotnull(c_acctbal#11) AND substring(c_phone#10, 1, 2) IN (13,31,23,29,30,18,17)) AND (cast(c_acctbal#11 as decimal(16,6)) > Subquery scalar-subquery#148, [id=#249]))\n                  :           :  +- Subquery scalar-subquery#148, [id=#249]\n                  :           :     +- *(2) HashAggregate(keys=[], functions=[avg(c_acctbal#160)], output=[avg(c_acctbal)#154])\n                  :           :        +- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#245]\n                  :           :           +- *(1) HashAggregate(keys=[], functions=[partial_avg(c_acctbal#160)], output=[sum#175, count#176L])\n                  :           :              +- *(1) Project [c_acctbal#160]\n                  :           :                 +- *(1) Filter ((isnotnull(c_acctbal#160) AND (c_acctbal#160 > 0.00)) AND substring(c_phone#159, 1, 2) IN (13,31,23,29,30,18,17))\n                  :           :                    +- *(1) ColumnarToRow\n                  :           :                       +- FileScan orc tpch_data_orc_100.customer[c_phone#159,c_acctbal#160,c_mktsegment#162] Batched: true, DataFilters: [isnotnull(c_acctbal#160), (c_acctbal#160 > 0.00), substring(c_phone#159, 1, 2) IN (13,31,23,29,3..., Format: ORC, Location: CatalogFileIndex(0 paths)[], PartitionFilters: [], PushedFilters: [IsNotNull(c_acctbal), GreaterThan(c_acctbal,0.00)], ReadSchema: struct<c_phone:string,c_acctbal:decimal(12,2)>\n                  :           +- *(1) ColumnarToRow\n                  :              +- FileScan orc tpch_data_orc_100.customer[c_custkey#6L,c_phone#10,c_acctbal#11,c_mktsegment#13] Batched: true, DataFilters: [isnotnull(c_acctbal#11), substring(c_phone#10, 1, 2) IN (13,31,23,29,30,18,17)], Format: ORC, Location: CatalogFileIndex(0 paths)[], PartitionFilters: [], PushedFilters: [IsNotNull(c_acctbal)], ReadSchema: struct<c_custkey:bigint,c_phone:string,c_acctbal:decimal(12,2)>\n                  +- *(4) Sort [o_custkey#17L ASC NULLS FIRST], false, 0\n                     +- Exchange hashpartitioning(o_custkey#17L, 200), ENSURE_REQUIREMENTS, [id=#301]\n                        +- *(3) Project [o_custkey#17L]\n                           +- *(3) ColumnarToRow\n                              +- FileScan orc tpch_data_orc_100.orders[o_custkey#17L,o_orderdate#24] Batched: true, DataFilters: [], Format: ORC, Location: CatalogFileIndex(0 paths)[], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<o_custkey:bigint>\n""
    }
  ]
}
{code}
The correct result should be 7 rows returned, not 0 as shown above. This can be confirmed by testing the same exact query against the hive table using Presto/Hive, which both return 7. Additionally, this link also shows that it should be 7 [https://github.com/apache/impala/blob/master/testdata/workloads/tpch/queries/tpch-q22.test.] ",,apachespark,apatnam,Gengliang.Wang,mridulm80,rajesh.balamohan,vsowrirajan,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 06 05:31:48 UTC 2021,,,,,,,,,,"0|z0vjrk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/21 17:32;apatnam;cc: [~vsowrirajan] [~mridulm80];;;","04/Oct/21 18:04;vsowrirajan;The issue seems to be with this subquery part of q22.
{code:java}
sql(""""""select avg(c_acctbal) from customer where c_acctbal > 0.00 and substr(c_phone, 1, 2) in ('13', '31', '23', '29', '30', '18', '17')"""""").collect
Array([null])
{code}
 
{code:java}
sql(""""""select sum(c_acctbal)/count(c_acctbal) from customer where c_acctbal > 0.00 and substr(c_phone, 1, 2) in ('13', '31', '23', '29', '30', '18', '17')"""""").collect
Array([4998.769056963132322922]){code}
It seems like there is some behavior change wrt 3.2.;;;","04/Oct/21 18:26;mridulm80;+CC [~gengliang], [~cloud_fan];;;","04/Oct/21 18:54;vsowrirajan;I tried to get a simpler repro. 

{code:java}
 
import spark.implicits._
val buf = new scala.collection.mutable.ArrayBuffer[Double]
for (i <- 0.toDouble to 9999.toDouble by 0.01) for (j <- 0 to 5) { buf += i }

val df = buf.toDF
df.selectExpr(""cast(value as decimal(12, 2))"").agg(avg(""value"")).show

Array[null]

df.selectExpr(""cast(value as decimal(16, 2))"").agg(avg(""value"")).show

+-----------+
| avg(value)|
+-----------+
|4999.500000|
+-----------+
{code};;;","05/Oct/21 01:55;gurwls223;Just to clarify, do you know if this is a regression or not [~apatnam]?;;;","05/Oct/21 02:23;vsowrirajan;This looks like a regression given the behavior of other engines like hive, presto, impala even spark 3.1 all return 7 records.;;;","05/Oct/21 05:26;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34180;;;","05/Oct/21 05:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34180;;;","06/Oct/21 03:27;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34193;;;","06/Oct/21 03:28;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34193;;;","06/Oct/21 05:31;Gengliang.Wang;Issue resolved by pull request 34193
[https://github.com/apache/spark/pull/34193];;;",,,,,
pandas API on Spark: DataFrameGroupBy.apply raises an exception when it returns Series.,SPARK-36907,13404393,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,ueshin,ueshin,30/Sep/21 23:43,12/Dec/22 18:10,13/Jul/23 08:50,03/Oct/21 03:25,3.2.0,3.3.0,,,,,,,3.2.0,,,,PySpark,,,,,0,,,,,"{{DataFrameGroupBy.apply}} without shortcut could raise an exception when it returns {{Series}}.

{code:python}
>>> ps.options.compute.shortcut_limit = 3
>>> psdf = ps.DataFrame(
...     {""a"": [1, 2, 3, 4, 5, 6], ""b"": [1, 1, 2, 3, 5, 8], ""c"": [1, 4, 9, 16, 25, 36]},
...     columns=[""a"", ""b"", ""c""],
... )
>>> psdf.groupby(""b"").apply(lambda x: x[""a""])
org.apache.spark.api.python.PythonException: Traceback (most recent call last):
...
ValueError: Length mismatch: Expected axis has 2 elements, new values have 3 elements
{code}
",,apachespark,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Oct 03 03:25:48 UTC 2021,,,,,,,,,,"0|z0vh3c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/21 23:54;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/34160;;;","30/Sep/21 23:54;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/34160;;;","03/Oct/21 03:25;gurwls223;Issue resolved by pull request 34160
[https://github.com/apache/spark/pull/34160];;;",,,,,,,,,,,,,
Reading Hive view without explicit column names fails in Spark ,SPARK-36905,13404343,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,linhongliu-db,shardulm,shardulm,30/Sep/21 17:15,14/Oct/21 14:24,13/Jul/23 08:50,14/Oct/21 14:24,3.2.0,,,,,,,,3.2.1,3.3.0,,,SQL,,,,,0,,,,,"Consider a Hive view in which some columns are not explicitly named
{code:sql}
CREATE VIEW test_view AS
SELECT 1
FROM some_table
{code}
Reading this view in Spark leads to an {{AnalysisException}}
{code:java}
org.apache.spark.sql.AnalysisException: cannot resolve '`_c0`' given input columns: [1]
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:188)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$$nestedInanonfun$checkAnalysis$1$2.applyOrElse(CheckAnalysis.scala:185)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$2(TreeNode.scala:340)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:340)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:337)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:406)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:242)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:404)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:357)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:337)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformUp$1(TreeNode.scala:337)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:406)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:242)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:404)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:357)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:337)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformExpressionsUp$1(QueryPlan.scala:104)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$1(QueryPlan.scala:116)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:116)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:127)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$3(QueryPlan.scala:132)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.recursiveTransform$1(QueryPlan.scala:132)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$mapExpressions$4(QueryPlan.scala:137)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:242)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:137)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:104)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:185)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1$adapted(CheckAnalysis.scala:94)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:182)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:94)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:91)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:155)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveViews(Analyzer.scala:1147)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveViews(Analyzer.scala:1151)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.$anonfun$applyOrElse$82(Analyzer.scala:1207)
  at scala.Option.map(Option.scala:230)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1207)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$10.applyOrElse(Analyzer.scala:1155)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$3(AnalysisHelper.scala:90)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUp$1(AnalysisHelper.scala:90)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:221)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:86)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:84)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1155)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1116)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  at scala.collection.immutable.List.foldLeft(List.scala:89)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:212)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:204)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:204)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:74)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:144)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:771)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:144)
  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:74)
  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:72)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:64)
  at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:771)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
  at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:918)
  at org.apache.spark.sql.SparkSession.table(SparkSession.scala:592)
  ... 47 elided
{code}
TRACE level log showing the query plan
{code:java}
21/09/30 10:17:17 TRACE PlanChangeLogger:
=== Result of Batch Cleanup ===
 'Project [upcast('_c0, IntegerType) AS _c0#3]          'Project [upcast('_c0, IntegerType) AS _c0#3]
 +- Project [1 AS 1#4]                                  +- Project [1 AS 1#4]
    +- SubqueryAlias spark_catalog.default.some_table      +- SubqueryAlias spark_catalog.default.some_table
       +- Relation default.some_table[id#1L] orc              +- Relation default.some_table[id#1L] orc
{code}",,apachespark,Gengliang.Wang,sbernauer,shardulm,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 12 05:30:44 UTC 2021,,,,,,,,,,"0|z0vgs8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/21 18:17;shardulm;This worked fine prior to [https://github.com/apache/spark/pull/31368] cc:[~cloud_fan]

Previous output
{code:java}
scala> spark.table(""test_view"").explain(true)
== Parsed Logical Plan ==
'UnresolvedRelation [test_view], [], false

== Analyzed Logical Plan ==
_c0: int
SubqueryAlias spark_catalog.default.test_view
+- View (`default`.`test_view`, [_c0#4])
   +- Project [1 AS 1#5]
      +- SubqueryAlias spark_catalog.default.some_table
         +- Relation[id#1L] orc

== Optimized Logical Plan ==
Project [1 AS _c0#4]
+- Relation[id#1L] orc

== Physical Plan ==
*(1) Project [1 AS _c0#4]
+- *(1) ColumnarToRow
   +- FileScan orc default. some_table[] Batched: true, DataFilters: [], Format: ORC, Location: InMemoryFileIndex[file:/...], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<>
 {code};;;","30/Sep/21 18:23;xkrogen;[~shardulm] is important here that the view is from Hive? Can this be replicated in Spark w/o Hive?;;;","30/Sep/21 18:28;shardulm;This cannot be reproduced with a view created from Spark. This only happens on views created from Hive and then read in Spark.;;;","04/Oct/21 21:07;xkrogen;cc also [~maropu] [~viirya] [~csun];;;","06/Oct/21 16:24;Gengliang.Wang;[~shardulm] Thanks for reporting the issue. 
I don't think this is a release blocker. I will mention this one as a known issue in the release note if it is not resolved by then.;;;","12/Oct/21 05:29;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/34254;;;","12/Oct/21 05:30;apachespark;User 'linhongliu-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/34254;;;",,,,,,,,,
Return boolean for `dropTempView` and `dropGlobalTempView`,SPARK-36896,13404157,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XinrongM,XinrongM,XinrongM,29/Sep/21 23:00,12/Dec/22 18:10,13/Jul/23 08:50,30/Sep/21 05:27,3.3.0,,,,,,,,3.3.0,,,,PySpark,,,,,0,,,,,,"Currently`dropTempView` and `dropGlobalTempView` don't have return value, which conflicts with their docstring:
`Returns true if this view is dropped successfully, false otherwise.`.

 

We should fix that.",apachespark,XinrongM,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 30 05:27:40 UTC 2021,,,,,,,,,,"0|z0vfmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/21 23:05;apachespark;User 'xinrong-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/34147;;;","29/Sep/21 23:06;apachespark;User 'xinrong-databricks' has created a pull request for this issue:
https://github.com/apache/spark/pull/34147;;;","30/Sep/21 05:27;gurwls223;Issue resolved by pull request 34147
[https://github.com/apache/spark/pull/34147];;;",,,,,,,,,,,,,
Disable batch fetch for a shuffle when push based shuffle is enabled,SPARK-36892,13404133,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zhouyejoe,mridulm80,mridulm80,29/Sep/21 20:22,06/Oct/21 14:37,13/Jul/23 08:50,06/Oct/21 07:43,3.2.0,,,,,,,,3.2.0,,,,Shuffle,,,,,0,,,,,"When push based shuffle is enabled, efficient fetch of merged mapper shuffle output happens.
Unfortunately, this currently interacts badly with spark.sql.adaptive.fetchShuffleBlocksInBatch, potentially causing shuffle fetch to hang and/or duplicate data to be fetched, causing correctness issues.

Given batch fetch does not benefit spark stages reading merged blocks when push based shuffle is enabled, ShuffleBlockFetcherIterator.doBatchFetch can be disabled when push based shuffle is enabled.


Thx to [~Ngone51] for surfacing this issue.
+CC [~Gengliang.Wang]",,apachespark,Gengliang.Wang,joshrosen,mridulm80,mshen,xkrogen,zhouyejoe,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 06 14:37:01 UTC 2021,,,,,,,,,,"0|z0vfhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/21 22:22;zhouyejoe;I am working on this issue. We have a job which can reproduce this hanging issue and after disabling the batch fetch, the job can go through. Will post PR soon.;;;","30/Sep/21 05:26;Gengliang.Wang;[~zhouyejoe] Thank you!;;;","30/Sep/21 07:06;zhouyejoe;Raised PR [https://github.com/apache/spark/pull/34156.] UT to be added.;;;","30/Sep/21 07:06;apachespark;User 'zhouyejoe' has created a pull request for this issue:
https://github.com/apache/spark/pull/34156;;;","30/Sep/21 08:00;Gengliang.Wang;[~mridulm80] [~zhouyejoe] [~mshen] The push-based shuffle feature fails the 3.2.0 RC multiple times. Could you run some real workloads(e.g TPCDS) with the latest branch 3.2 after the fix is merged? ;;;","30/Sep/21 13:55;mshen;[~Gengliang.Wang]

This issue and the ones fixed earlier are surfaced as we started having a variety of LinkedIn's internal workloads testing against a version of Spark based on 3.2.0 RC.

Notice that when we previously productionized push-based shuffle internally at LinkedIn, it was developed based on top of Spark 2.3/2.4.

We are fairly certain about the major functionalities of push-based shuffle since they have been in production at LinkedIn for a year now.

However, some of the code for push-based shuffle in Spark 3.2.0 RC are new and hadn't been tested with our internal workloads until recently.

So hopefully this explains the context around the few recent issues, and in terms of testing with real workloads we are already doing it and will continue so to help with 3.2.0 release.;;;","30/Sep/21 18:27;Gengliang.Wang;[~mshen] Thanks for the tests. I understand it is not easy work. 
I talked to [~mridulm80] on Slack as well. I will hold the next RC until your tests are complete. 
;;;","04/Oct/21 17:25;mridulm80;[~Gengliang.Wang] Thanks for holding the release !
The team worked on testing the RC and fixed two issues:
* SPARK-36705 (https://github.com/apache/spark/pull/34158) and
* SPARK-36892 (https://github.com/apache/spark/pull/34156).

With these two fixes on top of RC6, in addition to internal tests, we had all queries for TPCDS (scale 100) completing successfully and correctly.
There was an issue identified with mismatch for TPCH query 22 - [~apatnam] has filed a jira (SPARK-36926) for it.
;;;","06/Oct/21 07:43;Gengliang.Wang;Issue resolved by pull request 34156
[https://github.com/apache/spark/pull/34156];;;","06/Oct/21 07:45;Gengliang.Wang;[~mridulm80] [~mshen] [~zhouyejoe] [~apatnam] Again, thanks for testing Spark 3.2 with real workloads. Now that all the blockers are resolved. I will have the new RC soon.;;;","06/Oct/21 14:37;mridulm80;Sounds good [~Gengliang.Wang], I am not aware of any other issues.
Thanks for driving the process !;;;",,,,,
Respect `spark.sql.parquet.filterPushdown` by explain() for DSv2,SPARK-36889,13404007,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,29/Sep/21 06:57,12/Dec/22 18:10,13/Jul/23 08:50,29/Sep/21 10:47,3.3.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"When filters pushdown for parquet is disabled via the SQL config spark.sql.parquet.filterPushdown, explain() still outputs pushed down filters:

{code}
== Parsed Logical Plan ==
'Filter ('c0 = 1)
+- RelationV2[c0#7] parquet file:/private/var/folders/p3/dfs6mf655d7fnjrsjvldh0tc0000gn/T/spark-ff7e9a24-fd4e-4981-9c75-e1bcde78e91a

== Analyzed Logical Plan ==
c0: int
Filter (c0#7 = 1)
+- RelationV2[c0#7] parquet file:/private/var/folders/p3/dfs6mf655d7fnjrsjvldh0tc0000gn/T/spark-ff7e9a24-fd4e-4981-9c75-e1bcde78e91a

== Optimized Logical Plan ==
Filter (isnotnull(c0#7) AND (c0#7 = 1))
+- RelationV2[c0#7] parquet file:/private/var/folders/p3/dfs6mf655d7fnjrsjvldh0tc0000gn/T/spark-ff7e9a24-fd4e-4981-9c75-e1bcde78e91a

== Physical Plan ==
*(1) Filter (isnotnull(c0#7) AND (c0#7 = 1))
+- *(1) ColumnarToRow
   +- BatchScan[c0#7] ParquetScan DataFilters: [isnotnull(c0#7), (c0#7 = 1)], Format: parquet, Location: InMemoryFileIndex(1 paths)[file:/private/var/folders/p3/dfs6mf655d7fnjrsjvldh0tc0000gn/T/spark-ff..., PartitionFilters: [], PushedFilters: [IsNotNull(c0), EqualTo(c0,1)], ReadSchema: struct<c0:int>, PushedFilters: [IsNotNull(c0), EqualTo(c0,1)] RuntimeFilters: []
{code}
See PushedFilters: [IsNotNull(c0), EqualTo(c0,1)]
",,apachespark,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 29 10:47:54 UTC 2021,,,,,,,,,,"0|z0vepk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/21 07:35;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34140;;;","29/Sep/21 10:47;gurwls223;Fixed in https://github.com/apache/spark/pull/34140;;;",,,,,,,,,,,,,,
Ambiguous Self-Join detected only on right dataframe,SPARK-36874,13403754,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,vincentdoba,vincentdoba,28/Sep/21 08:20,12/Dec/22 18:10,13/Jul/23 08:50,05/Oct/21 03:17,3.1.2,,,,,,,,3.1.3,3.2.0,,,SQL,,,,,0,correctness,,,,"When joining two dataframes, if they share the same lineage and one dataframe is a transformation of the other, Ambiguous Self Join detection only works when transformed dataframe is the right dataframe. 

For instance {{df1}} and {{df2}} where {{df2}} is a filtered {{df1}}, Ambiguous Self Join detection only works when {{df2}} is the right dataframe:

- {{df1.join(df2, ...)}} correctly fails with Ambiguous Self Join error
- {{df2.join(df1, ...)}} returns a valid dataframe

h1. Minimum Reproducible example
h2. Code
{code:scala}
import sparkSession.implicit._

val df1 = Seq((1, 2, ""A1""),(2, 1, ""A2"")).toDF(""key1"", ""key2"", ""value"")

val df2 = df1.filter($""value"" === ""A2"")

df2.join(df1, df1(""key1"") === df2(""key2"")).show()
{code}
h2. Expected Result

Throw the following exception:

{code}
Exception in thread ""main"" org.apache.spark.sql.AnalysisException: Column key2#11 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(""a"").join(df.as(""b""), $""a.id"" > $""b.id"")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.
	at org.apache.spark.sql.execution.analysis.DetectAmbiguousSelfJoin$.apply(DetectAmbiguousSelfJoin.scala:157)
	at org.apache.spark.sql.execution.analysis.DetectAmbiguousSelfJoin$.apply(DetectAmbiguousSelfJoin.scala:43)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:216)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:213)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:205)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:205)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:196)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:190)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:183)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:174)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:228)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:173)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:73)
	at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
	at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:71)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:63)
	at org.apache.spark.sql.Dataset$.$anonfun$ofRows$1(Dataset.scala:90)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:88)
	at org.apache.spark.sql.Dataset.withPlan(Dataset.scala:3715)
	at org.apache.spark.sql.Dataset.join(Dataset.scala:1079)
	at org.apache.spark.sql.Dataset.join(Dataset.scala:1041)
 ...
{code}
h2. Actual result

Empty dataframe:
{code:java}
+----+----+-----+----+----+-----+
|key1|key2|value|key1|key2|value|
+----+----+-----+----+----+-----+
+----+----+-----+----+----+-----+
{code}",,apachespark,cloud_fan,vincentdoba,,,,,,,,,,,,,,,,,,,,SPARK-28344,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 07 03:42:22 UTC 2021,,,,,,,,,,"0|z0vd5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/21 05:27;gurwls223;cc [~cloud_fan] FYI;;;","04/Oct/21 18:55;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34172;;;","05/Oct/21 03:17;cloud_fan;Issue resolved by pull request 34172
[https://github.com/apache/spark/pull/34172];;;","07/Oct/21 03:41;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34205;;;","07/Oct/21 03:42;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34205;;;",,,,,,,,,,,
Add provided Guava dependency for network-yarn module,SPARK-36873,13403712,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,28/Sep/21 06:19,28/Sep/21 15:04,13/Jul/23 08:50,28/Sep/21 10:24,3.2.0,,,,,,,,3.2.0,,,,Build,,,,,0,,,,,"In Spark 3.1 and earlier the network-yarn module implicitly relies on guava from hadoop-client dependency, which was changed by SPARK-33212 where we moved to shaded Hadoop client which no longer expose the transitive guava dependency. This was fine for a while since we were not using {{createDependencyReducedPom}} so the module picks up the transitive dependency from {{spark-network-common}}. However, this got changed by SPARK-36835 when we restored {{createDependencyReducedPom}} and now it is no longer able to find guava classes:
{code}
mvn test -pl common/network-yarn -Phadoop-3.2 -Phive-thriftserver -Pkinesis-asl -Pkubernetes -Pmesos -Pnetlib-lgpl -Pscala-2.12 -Pspark-ganglia-lgpl -Pyarn
...
[INFO] Compiling 1 Java source to /Users/sunchao/git/spark/common/network-yarn/target/scala-2.12/classes ...
[WARNING] [Warn] : bootstrap class path not set in conjunction with -source 8
[ERROR] [Error] /Users/sunchao/git/spark/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java:32: package com.google.common.annotations does not exist
[ERROR] [Error] /Users/sunchao/git/spark/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java:33: package com.google.common.base does not exist
[ERROR] [Error] /Users/sunchao/git/spark/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java:34: package com.google.common.collect does not exist
[ERROR] [Error] /Users/sunchao/git/spark/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java:118: cannot find symbol
  symbol:   class VisibleForTesting
  location: class org.apache.spark.network.yarn.YarnShuffleService
{code}",,apachespark,csun,Gengliang.Wang,xkrogen,,,,,,,,,,,,,,,,,,,,SPARK-33212,SPARK-36835,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 28 10:24:00 UTC 2021,,,,,,,,,,"0|z0vcw0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/21 06:30;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34125;;;","28/Sep/21 06:30;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34125;;;","28/Sep/21 10:24;Gengliang.Wang;Issue resolved by pull request 34125
[https://github.com/apache/spark/pull/34125];;;",,,,,,,,,,,,,
Misleading Error Message with Invalid Column and Group By,SPARK-36867,13403658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,jackowaya,jackowaya,27/Sep/21 19:52,12/Oct/21 14:48,13/Jul/23 08:50,12/Oct/21 14:47,3.1.2,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"When you run a query with an invalid column that also does a group by on a constructed column, the error message you get back references a missing column for the group by rather than the invalid column.

You can reproduce this in pyspark in 3.1.2 with the following code:
{code:python}
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName(""Group By Issue"").getOrCreate()
data = spark.createDataFrame(
    [(""2021-09-15"", 1), (""2021-09-16"", 2), (""2021-09-17"", 10), (""2021-09-18"", 25), (""2021-09-19"", 500), (""2021-09-20"", 50), (""2021-09-21"", 100)],
    schema=[""d"", ""v""]
    )
data.createOrReplaceTempView(""data"")
# This is valid
spark.sql(""select sum(v) as value, date(date_trunc('week', d)) as week from data group by week"").show()
# This is invalid because val is the wrong variable
spark.sql(""select sum(val) as value, date(date_trunc('week', d)) as week from data group by week"").show()
{code}

The error message for the second spark.sql line is
{quote}
pyspark.sql.utils.AnalysisException: cannot resolve '`week`' given input columns: [data.d, data.v]; line 1 pos 81;
'Aggregate ['week], ['sum('val) AS value#21, cast(date_trunc(week, cast(d#0 as timestamp), Some(America/New_York)) as date) AS week#22]
+- SubqueryAlias data
   +- LogicalRDD [d#0, v#1L], false
{quote}
but the actual problem is that I used the wrong variable name in a different part of the query. Nothing is wrong with {{week}} in this case.",,apachespark,cloud_fan,jackowaya,senthh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Oct 12 14:47:52 UTC 2021,,,,,,,,,,"0|z0vck0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Oct/21 15:44;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34244;;;","12/Oct/21 14:47;cloud_fan;Issue resolved by pull request 34244
[https://github.com/apache/spark/pull/34244];;;",,,,,,,,,,,,,,
Add PySpark API document of session_window,SPARK-36865,13403632,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,27/Sep/21 17:50,15/Nov/21 18:26,13/Jul/23 08:50,30/Sep/21 07:51,3.2.0,,,,,,,,3.2.1,,,,Documentation,PySpark,,,,0,,,,,"There is no PySpark API document of session_window.
The docstring of the function also doesn't comply with the numpydoc format.",,apachespark,kabhwan,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 30 07:52:56 UTC 2021,,,,,,,,,,"0|z0vce8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/21 18:28;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34118;;;","30/Sep/21 07:51;kabhwan;Issue resolved by pull request 34118
[https://github.com/apache/spark/pull/34118];;;","30/Sep/21 07:52;kabhwan;Let me set the fixed version to 3.2.1 for now. I'll update it to 3.2.0 if we have to trigger another RC.;;;",,,,,,,,,,,,,
Partition columns are overly eagerly parsed as dates,SPARK-36861,13403442,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,maxgekk,tanelk,tanelk,27/Sep/21 07:42,25/Nov/21 07:56,13/Jul/23 08:50,25/Nov/21 07:55,3.3.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"I have an input directory with subdirs:
* hour=2021-01-01T00
* hour=2021-01-01T01
* hour=2021-01-01T02
* ...

in spark 3.1 the 'hour' column is parsed as a string type, but in 3.2 RC it is parsed as date type and the hour part is lost.
",,apachespark,cloud_fan,Gengliang.Wang,sarutak,senthh,tanelk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Nov 25 07:55:24 UTC 2021,,,,,,,,,,"0|z0vb80:",9223372036854775807,,,,,,,,,,,,,3.3.0,,,,,,,,,,"27/Sep/21 07:44;tanelk;[~Gengliang.Wang] I think, that this should be considered as a blocker for the 3.2 release
 ;;;","27/Sep/21 07:47;tanelk;If this is expected behaviour, then I would expect there to be a simple way to turn this off. Currently only one I can think of is manually specifing the schema.;;;","27/Sep/21 07:54;Gengliang.Wang;[~tanelk] This is a new behavior introduced from https://github.com/apache/spark/pull/33709
However, turning into date and losing the hour part seems wrong. cc [~maxgekk] [~cloud_fan];;;","27/Sep/21 08:03;Gengliang.Wang;Hmm, the PR https://github.com/apache/spark/pull/33709 is only on master. I can't reproduce your case on 3.2.0 RC4 with:

{code:scala}
> val df = Seq((""2021-01-01T00"", 0), (""2021-01-01T01"", 1), (""2021-01-01T02"", 2)).toDF(""hour"", ""i"")
> df.write.partitionBy(""hour"").parquet(""/tmp/t1"")
> spark.read.parquet(""/tmp/t1"").schema
res2: org.apache.spark.sql.types.StructType = StructType(StructField(i,IntegerType,true), StructField(hour,StringType,true))
{code}

The issue can be reproduced on Spark master though.
;;;","27/Sep/21 08:07;tanelk;Sorry, indeed I ran the test on master. Nevermind it then, does not impact the 3.2 release.;;;","28/Sep/21 11:19;senthh;[~tanelk] This is issue not reproducable even in 3.1.2

 

root
 |-- i: integer (nullable = true)
 |-- hour: string (nullable = true);;;","28/Sep/21 11:34;tanelk;Yes, in 3.1 it is parsed as string. In 3.3 (master) it is parsed as date.;;;","30/Sep/21 07:03;sarutak;Hmm, if a ""T"" follows the date part but it's not a valid ISO 8601 format, casting a string to date should fail ?
In PostgreSQL, parsing will fail in such case.;;;","01/Oct/21 19:24;senthh;Yes in Spark 3.3, hour column is created as ""DateType"" but I could see hour part in subdirs created

===============

Spark session available as 'spark'.
 Welcome to
 ____ __
 / __/__ ___ _____/ /__
 _\ \/ _ \/ _ `/ __/ '_/
 /___/ .__/_,_/_/ /_/_\ version 3.3.0-SNAPSHOT
 /_/

Using Scala version 2.12.15 (OpenJDK 64-Bit Server VM, Java 1.8.0_292)
 Type in expressions to have them evaluated.
 Type :help for more information.

scala> val df = Seq((""2021-01-01T00"", 0), (""2021-01-01T01"", 1), (""2021-01-01T02"", 2)).toDF(""hour"", ""i"")
 df: org.apache.spark.sql.DataFrame = [hour: string, i: int]

scala> df.write.partitionBy(""hour"").parquet(""/tmp/t1"")

scala> spark.read.parquet(""/tmp/t1"").schema
 res1: org.apache.spark.sql.types.StructType = StructType(StructField(i,IntegerType,true), StructField(hour,DateType,true))

scala>

===============

 

and subdirs created are

===============

ls -l
 total 0
 -rw-r--r-- 1 senthilkumar wheel 0 Oct 2 00:44 _SUCCESS
 drwxr-xr-x 4 senthilkumar wheel 128 Oct 2 00:44 hour=2021-01-01T00
 drwxr-xr-x 4 senthilkumar wheel 128 Oct 2 00:44 hour=2021-01-01T01
 drwxr-xr-x 4 senthilkumar wheel 128 Oct 2 00:44 hour=2021-01-01T02

===============

 

It will be helpful if you share the list of sub-dirs created in your case.;;;","11/Oct/21 11:55;cloud_fan;I think partition value parsing needs to be stricter. cc [~maxgekk];;;","24/Nov/21 20:45;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34700;;;","24/Nov/21 20:46;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/34700;;;","25/Nov/21 07:55;cloud_fan;Issue resolved by pull request 34700
[https://github.com/apache/spark/pull/34700];;;",,,
"Building by ""./build/mvn"" may be stuck on MacOS",SPARK-36856,13403348,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,HectorZhang,HectorZhang,HectorZhang,26/Sep/21 09:19,28/Sep/21 09:29,13/Jul/23 08:50,28/Sep/21 09:28,3.2.0,3.3.0,,,,,,,3.2.0,,,,Build,,,,,0,,,,,"Command ""./build/mvn"" will be stuck on my MacOS 11.4. Because it is using error java home. On my mac, ""/usr/bin/java"" is a real file instead of a symbolic link, so the java home is set to path ""/usr"", and lead the launched maven process stuck with this error java home.",MacOS 11.4,apachespark,Gengliang.Wang,HectorZhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 28 09:28:38 UTC 2021,,,,,,,,,,"0|z0vanc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Sep/21 05:26;apachespark;User 'copperybean' has created a pull request for this issue:
https://github.com/apache/spark/pull/34111;;;","27/Sep/21 05:27;apachespark;User 'copperybean' has created a pull request for this issue:
https://github.com/apache/spark/pull/34111;;;","28/Sep/21 09:28;Gengliang.Wang;Issue resolved by pull request 34111
[https://github.com/apache/spark/pull/34111];;;",,,,,,,,,,,,,
"""sha2"" expression with bit_length of 224 returns incorrect results",SPARK-36836,13403027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,richardc-db,richardc-db,richardc-db,23/Sep/21 23:31,28/Sep/21 10:40,13/Jul/23 08:50,28/Sep/21 10:39,2.4.0,3.0.0,3.1.0,3.2.0,,,,,3.2.0,,,,SQL,,,,,0,,,,,"{{sha2(input, bit_length)}} returns incorrect results when {{bit_length == 224}}.

 

This bug seems to have been present since the {{sha2}} expression was introduced in 1.5.0.

 

Repro in spark shell:

{{spark.sql(""SELECT sha2('abc', 224)"").show()}}

 

Spark currently returns a garbled string, consisting of invalid UTF:

 {{#\t}""4�""�B�w��U�*��你���l��}}

The expected return value is: 

{{23097d223405d8228642a477bda255b32aadbce4bda0b3f7e36c9da7}}

 

This appears to happen because the  {{MessageDigest.digest()}} function appears to return bytes intended to be interpreted as a {{BigInt}} rather than a string. Thus, the output of {{MessageDigest.digest()}} must first be interpreted as a {{BigInt}} and then transformed into a hex string. ",,apachespark,Gengliang.Wang,richardc-db,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 28 10:39:23 UTC 2021,,,,,,,,,,"0|z0v8o0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/21 00:02;apachespark;User 'richardc-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/34086;;;","28/Sep/21 10:39;Gengliang.Wang;Issue resolved by pull request 34086
[https://github.com/apache/spark/pull/34086];;;",,,,,,,,,,,,,,
"Spark 3.2.0 POMs are no longer ""dependency reduced""",SPARK-36835,13403026,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,csun,joshrosen,joshrosen,23/Sep/21 23:19,28/Sep/21 06:31,13/Jul/23 08:50,24/Sep/21 02:17,3.2.0,,,,,,,,3.2.0,,,,Build,,,,,0,,,,,"It looks like Spark 3.2.0's POMs are no longer ""dependency reduced"". As a result, applications may pull in additional unnecessary dependencies when depending on Spark.

Spark uses the Maven Shade plugin to create effective POMs and to bundle shaded versions of certain libraries with Spark (namely, Jetty, Guava, and JPPML). [By default|https://maven.apache.org/plugins/maven-shade-plugin/shade-mojo.html#createDependencyReducedPom], the Maven Shade plugin generates simplified POMs which remove dependencies on artifacts that have been shaded.

SPARK-33212 / [b6f46ca29742029efea2790af7fdefbc2fcf52de|https://github.com/apache/spark/commit/b6f46ca29742029efea2790af7fdefbc2fcf52de] changed the configuration of the Maven Shade plugin, setting {{createDependencyReducedPom}} to {{false}}.

As a result, the generated POMs now include compile-scope dependencies on the shaded libraries. For example, compare the {{org.eclipse.jetty}} dependencies in:
 * Spark 3.1.2: [https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.12/3.1.2/spark-core_2.12-3.1.2.pom]
 * Spark 3.2.0 RC2: [https://repository.apache.org/content/repositories/orgapachespark-1390/org/apache/spark/spark-core_2.12/3.2.0/spark-core_2.12-3.2.0.pom]

I think we should revert back to generating ""dependency reduced"" POMs to ensure that Spark declares a proper set of dependencies and to avoid ""unknown unknown"" consequences of changing our generated POM format.

/cc [~csun]",,apachespark,csun,Gengliang.Wang,joshrosen,viirya,,,,,,,,,,,,,,,,,,,SPARK-33212,,SPARK-36873,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 27 04:24:40 UTC 2021,,,,,,,,,,"0|z0v8ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/21 23:36;csun;Sorry for the regression [~joshrosen]. I forgot exactly why I added that but let me see if we can safely revert it.;;;","24/Sep/21 00:02;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34085;;;","24/Sep/21 02:17;Gengliang.Wang;Issue resolved by pull request 34085
[https://github.com/apache/spark/pull/34085];;;","24/Sep/21 17:13;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34100;;;","27/Sep/21 04:24;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34110;;;",,,,,,,,,,,
Task/Stage/Job data remain in memory leads memory leak,SPARK-36827,13402743,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,taroplus,taroplus,22/Sep/21 17:27,24/Sep/21 09:25,13/Jul/23 08:50,24/Sep/21 09:25,3.1.2,,,,,,,,3.2.0,,,,Spark Core,,,,,0,,,,,"Noticing memory-leak like behavior, steady increase of heap after GC and eventually it leads to a service failure. 

The GC histogram shows very high number of Task/Data/Job data
{code}
 num     #instances         #bytes  class name 
---------------------------------------------- 
   6:       7835346     2444627952  org.apache.spark.status.TaskDataWrapper 
  25:       3765152      180727296  org.apache.spark.status.StageDataWrapper 
  88:        232255        9290200  org.apache.spark.status.JobDataWrapper 
{code}

Thread dumps show clearly the clean up thread is always doing cleanupStages
{code}
""element-tracking-store-worker"" #355 daemon prio=5 os_prio=0 tid=0x00007f31b0014800 nid=0x409 runnable [0x00007f2f25783000]
   java.lang.Thread.State: RUNNABLE
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.util.kvstore.KVTypeInfo$MethodAccessor.get(KVTypeInfo.java:162)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryView.compare(InMemoryStore.java:434)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryView.lambda$iterator$0(InMemoryStore.java:375)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryView$$Lambda$9000/574018760.compare(Unknown Source)
	at java.util.TimSort.gallopLeft(TimSort.java:542)
	at java.util.TimSort.mergeLo(TimSort.java:752)
	at java.util.TimSort.mergeAt(TimSort.java:514)
	at java.util.TimSort.mergeCollapse(TimSort.java:439)
	at java.util.TimSort.sort(TimSort.java:245)
	at java.util.Arrays.sort(Arrays.java:1512)
	at java.util.ArrayList.sort(ArrayList.java:1464)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryView.iterator(InMemoryStore.java:375)
	at org.apache.spark.util.kvstore.KVStoreView.closeableIterator(KVStoreView.java:117)
	at org.apache.spark.status.AppStatusListener.$anonfun$cleanupStages$2(AppStatusListener.scala:1269)
	at org.apache.spark.status.AppStatusListener$$Lambda$9126/608388595.apply(Unknown Source)
	at scala.collection.immutable.List.map(List.scala:297)
	at org.apache.spark.status.AppStatusListener.cleanupStages(AppStatusListener.scala:1260)
	at org.apache.spark.status.AppStatusListener.$anonfun$new$3(AppStatusListener.scala:98)
	at org.apache.spark.status.AppStatusListener$$Lambda$646/596139882.apply$mcVJ$sp(Unknown Source)
	at org.apache.spark.status.ElementTrackingStore.$anonfun$write$3(ElementTrackingStore.scala:135)
	at org.apache.spark.status.ElementTrackingStore.$anonfun$write$3$adapted(ElementTrackingStore.scala:133)
	at org.apache.spark.status.ElementTrackingStore$$Lambda$986/162337848.apply(Unknown Source)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at org.apache.spark.status.ElementTrackingStore.$anonfun$write$2(ElementTrackingStore.scala:133)
	at org.apache.spark.status.ElementTrackingStore.$anonfun$write$2$adapted(ElementTrackingStore.scala:131)
	at org.apache.spark.status.ElementTrackingStore$$Lambda$984/600376389.apply(Unknown Source)
	at org.apache.spark.status.ElementTrackingStore$LatchedTriggers.$anonfun$fireOnce$1(ElementTrackingStore.scala:58)
	at org.apache.spark.status.ElementTrackingStore$LatchedTriggers$$Lambda$985/1187323214.apply$mcV$sp(Unknown Source)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryLog(Utils.scala:2013)
	at org.apache.spark.status.ElementTrackingStore$$anon$1.run(ElementTrackingStore.scala:117)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
{code}

",,apachespark,Gengliang.Wang,taroplus,tenglei,ulysses,yumwang,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/21 17:29;taroplus;mem1.txt;https://issues.apache.org/jira/secure/attachment/13034043/mem1.txt","22/Sep/21 17:28;taroplus;worker.txt;https://issues.apache.org/jira/secure/attachment/13034042/worker.txt",,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 24 09:25:05 UTC 2021,,,,,,,,,,"0|z0v6ww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Sep/21 17:47;taroplus;one thread shows slightly different codepoint
{code}
""element-tracking-store-worker"" #355 daemon prio=5 os_prio=0 tid=0x00007f31b0014800 nid=0x409 runnable [0x00007f2f25783000]
   java.lang.Thread.State: RUNNABLE
	at java.util.concurrent.ConcurrentHashMap$Traverser.advance(ConcurrentHashMap.java:3318)
	at java.util.concurrent.ConcurrentHashMap$ValueIterator.next(ConcurrentHashMap.java:3439)
	at java.util.concurrent.ConcurrentHashMap$CollectionView.toArray(ConcurrentHashMap.java:4417)
	at java.util.ArrayList.<init>(ArrayList.java:178)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryView.copyElements(InMemoryStore.java:428)
	at org.apache.spark.util.kvstore.InMemoryStore$InMemoryView.iterator(InMemoryStore.java:374)
	at org.apache.spark.util.kvstore.KVStoreView.closeableIterator(KVStoreView.java:117)
{code}

this loop seems to be very expensive if there are millions of entries
{code}
stages.map .. (AppStatusListener - N)
        final List<T> sorted = copyElements();  (InMemoryStore)
        sorted.sort((e1, e2) -> modifier * compare(e1, e2, getter)); (InMemoryStore)
{code}

so it gets out of control quickly;;;","22/Sep/21 20:23;srowen;Those are stack dumps not heap dumps. Do you know anything about what is hanging on to the reference? I would see whether turning down retained jobs and tasks in the UI does anything to narrow it down. What you have here at best suggests an allocation hotspot. 

I recall this change which might possibly be related but not sure. Could be worth trying 3.2.0 when released. 

https://github.com/apache/spark/pull/33859;;;","22/Sep/21 21:00;taroplus;I do have multiple heap dumps, however it won't help the problem. Those are just stage/task/job data piling up (from completed jobs). Spark has a limit (e.g. spark.ui.retainedStages) and it's kicking off the clean up task. (which runs in element-tracking-store-worker thread).

However Spark is doing somewhat O(N*N) type of operation inside of the thread, the speed of deletion gets slower and slower as it gets more stages. As a result, the deletion speed is way slower than the creation of tasks. I have identified the part doing O(N*N) and testing a fix.;;;","22/Sep/21 21:03;taroplus;this line
 [https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala#L1276]

does _copy all stages to a new list and sort it_  for every single stage to delete (which is almost everything for my case);;;","22/Sep/21 21:42;srowen;Looks promising yeah. This loop seems like it could instead iterate once, remembering which stages it has seen, and then use that info to clean up, rather than iterating for each stage.;;;","22/Sep/21 21:56;taroplus;i still need to ramp up the process for getting a PR, but i believe handling remainingStages can be done later, here's what i'm testing ...

 
{code:java}

private def cleanupStages(count: Long): Unit = {
  val countToDelete = calculateNumberToRemove(count, conf.get(MAX_RETAINED_STAGES))
  if (countToDelete <= 0L) {
    return
  }

  // As the completion time of a skipped stage is always -1, we will remove skipped stages first.
  // This is safe since the job itself contains enough information to render skipped stages in the
  // UI.
  val view = kvstore.view(classOf[StageDataWrapper]).index(""completionTime"")
  val stages = KVUtils.viewToSeq(view, countToDelete.toInt) { s =>
    s.info.status != v1.StageStatus.ACTIVE && s.info.status != v1.StageStatus.PENDING
  }

  val stageIds = stages.map { s =>
    val key = Array(s.info.stageId, s.info.attemptId)
    kvstore.delete(s.getClass(), key)
    cleanupCachedQuantiles(key)
    key
  }

  // create remaining set of stages.
  val iter = view.closeableIterator()
  val remainingStages = try {
    iter.asScala.map(_.info.stageId).toSet
  } finally {
    iter.close()
  }
  stageIds.foreach(key => {
    // Check whether there are remaining attempts for the same stage. If there aren't, then
    // also delete the RDD graph data.
    if (!remainingStages.contains(key(0))) {
      kvstore.delete(classOf[RDDOperationGraphWrapper], key(0))
    }
  })

  // Delete summaries in one pass, as deleting them for each stage is slow
  kvstore.removeAllByIndexValues(classOf[ExecutorStageSummaryWrapper], ""stage"", stageIds)

  // Delete tasks for all stages in one pass, as deleting them for each stage individually is slow
  kvstore.removeAllByIndexValues(classOf[TaskDataWrapper], TaskIndexNames.STAGE, stageIds)
}

 {code};;;","24/Sep/21 03:32;apachespark;User 'taroplus' has created a pull request for this issue:
https://github.com/apache/spark/pull/34090;;;","24/Sep/21 04:57;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34092;;;","24/Sep/21 07:53;tenglei;I met the same problem in spark 2.3.x, and find a pull request from https://github.com/apache/spark/pull/24616 it seem to relate to this problem and it should work.How much time you spend to occur this problem?;;;","24/Sep/21 09:25;Gengliang.Wang;Issue resolved by pull request 34092
[https://github.com/apache/spark/pull/34092];;;",,,,,,
Found duplicate rewrite attributes,SPARK-36815,13402448,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gaoyajun02,gaoyajun02,gaoyajun02,21/Sep/21 11:32,22/Sep/21 16:34,13/Jul/23 08:50,22/Sep/21 16:33,3.0.2,,,,,,,,,,,,SQL,,,,,0,,,,,"We are using Spark version 3.0.2 in production and some ETLs contain multi-level CTEs and the following error occurs when we join them.
{code:java}
java.lang.AssertionError: assertion failed: Found duplicate rewrite attributes at scala.Predef$.assert(Predef.scala:223) at org.apache.spark.sql.catalyst.plans.QueryPlan.rewrite$1(QueryPlan.scala:207) at org.apache.spark.sql.catalyst.plans.QueryPlan.$anonfun$transformUpWithNewOutput$1(QueryPlan.scala:193) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:405) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:243) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:403)
{code}
I reproduced the problem with a simplified SQL as follows:
{code:java}
-- SQL
with
a as ( select name, get_json_object(json, '$.id') id, n from (
    select get_json_object(json, '$.name') name, json from values ('{""name"":""a"", ""id"": 1}' ) people(json)
    ) LATERAL VIEW explode(array(1, 1, 2)) num as n ),
b as ( select a1.name, a1.id, a1.n from a a1 left join (select name, count(1) c from a group by name) a2 on a1.name = a2.name)
select b1.name, b1.n, b1.id from b b1 join b b2 on b1.name = b2.name;{code}
In debugging I found that a reference to the root Project existed in both subqueries, and when `ResolveReferences` resolved the conflict, `rewrite` occurred in both subqueries, containing two new attrMapping, and they were both eventually passed to the root Project, leading to this error

plan:
{code:java}
Project [name#218, id#219, n#229]
+- Join LeftOuter, (name#218 = name#232)
   :- SubqueryAlias a1
   :  +- SubqueryAlias a
   :     +- Project [name#218, get_json_object(json#225, $.id) AS id#219, n#229]
   :        +- Generate explode(array(1, 1, 2)), false, num, [n#229]
   :           +- SubqueryAlias __auto_generated_subquery_name
   :              +- Project [get_json_object(json#225, $.name) AS name#218, json#225]
   :                 +- SubqueryAlias people
   :                    +- LocalRelation [json#225]
   +- SubqueryAlias a2
      +- Aggregate [name#232], [name#232, count(1) AS c#220L]
         +- SubqueryAlias a
            +- Project [name#232, get_json_object(json#226, $.id) AS id#219, n#230]
               +- Generate explode(array(1, 1, 2)), false, num, [n#230]
                  +- SubqueryAlias __auto_generated_subquery_name
                     +- Project [get_json_object(json#226, $.name) AS name#232, json#226]
                        +- SubqueryAlias people
                           +- LocalRelation [json#226]

{code}
 newPlan:
{code:java}
!Project [name#218, id#219, n#229]
+- Join LeftOuter, (name#218 = name#232)
   :- SubqueryAlias a1
   :  +- SubqueryAlias a
   :     +- Project [name#218, get_json_object(json#225, $.id) AS id#233, n#229]
   :        +- Generate explode(array(1, 1, 2)), false, num, [n#229]
   :           +- SubqueryAlias __auto_generated_subquery_name
   :              +- Project [get_json_object(json#225, $.name) AS name#218, json#225]
   :                 +- SubqueryAlias people
   :                    +- LocalRelation [json#225]
   +- SubqueryAlias a2
      +- Aggregate [name#232], [name#232, count(1) AS c#220L]
         +- SubqueryAlias a
            +- Project [name#232, get_json_object(json#226, $.id) AS id#234, n#230]
               +- Generate explode(array(1, 1, 2)), false, num, [n#230]
                  +- SubqueryAlias __auto_generated_subquery_name
                     +- Project [get_json_object(json#226, $.name) AS name#232, json#226]
                        +- SubqueryAlias people
                           +- LocalRelation [json#226]

{code}
attrMapping:
{code:java}
attrMapping = {ArrayBuffer@9099} ""ArrayBuffer"" size = 2
 0 = {Tuple2@17769} ""(id#219,id#233)""
 1 = {Tuple2@17770} ""(id#219,id#234)""
{code}
 

 

 ",,apachespark,gaoyajun02,viirya,,,,,,,,,,,,,,,,,,,,,,,,SPARK-33272,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 22 16:33:50 UTC 2021,,,,,,,,,,"0|z0v53k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Sep/21 11:53;gaoyajun02;https://issues.apache.org/jira/browse/SPARK-33272 fixes this issue, but the Spark 3.0.2 branch does not.

Hi [~cloud_fan], can we open backport PRs for 3.0?;;;","22/Sep/21 06:16;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/34068;;;","22/Sep/21 06:17;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/34068;;;","22/Sep/21 16:33;viirya;It was resolved by https://github.com/apache/spark/pull/34068.;;;",,,,,,,,,,,,
Use R 4.0.4 in K8s R image,SPARK-36806,13402100,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,20/Sep/21 05:57,21/Sep/21 02:57,13/Jul/23 08:50,20/Sep/21 17:53,3.1.3,3.2.0,3.3.0,,,,,,3.1.3,3.2.0,3.3.0,,Kubernetes,R,,,,0,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 20 17:53:05 UTC 2021,,,,,,,,,,"0|z0v2y8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/21 05:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34048;;;","20/Sep/21 17:53;dongjoon;Issue resolved by pull request 34048
[https://github.com/apache/spark/pull/34048];;;",,,,,,,,,,,,,,
Using the verbose parameter in yarn mode would cause application submission failure,SPARK-36804,13402090,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Li Jianmeng,Li Jianmeng,Li Jianmeng,20/Sep/21 04:12,09/Oct/21 13:55,13/Jul/23 08:50,09/Oct/21 13:55,3.1.2,,,,,,,,3.3.0,,,,YARN,,,,,0,,,,,"If we submit the spark application with the --verbose parameter in yarn mode, we will get the following exception:
{code:java}
Exception in thread ""main"" java.lang.IllegalArgumentException: Unknown/unsupported param List(--verbose)Exception in thread ""main"" 
{code}
I check the code, found SparkSubmit invoke YarnClusterApplication with --verbose arguments, however ClientArguments used by YarnClusterApplication don't support that argument by now, as a result, IllegalArgumentException is thrown in ClientArguments. I think we can support --verbose in YARN mode to keep consistency with other module
 ",,apachespark,dongjoon,Li Jianmeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Oct 09 13:55:29 UTC 2021,,,,,,,,,,"0|z0v2w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/21 04:17;Li Jianmeng;I am working on the fix;;;","20/Sep/21 04:35;dongjoon;Thank you for filing a JIRA, [~Li Jianmeng]. I removed `Fix Version` and `Target Version` because it should be filled by committers when the PRs are merged.
;;;","20/Sep/21 04:39;apachespark;User 'daugraph' has created a pull request for this issue:
https://github.com/apache/spark/pull/34046;;;","20/Sep/21 04:40;apachespark;User 'daugraph' has created a pull request for this issue:
https://github.com/apache/spark/pull/34046;;;","20/Sep/21 05:02;Li Jianmeng;[~dongjoon] Thanks for the correction, I will pay attention to it later.;;;","09/Oct/21 13:55;srowen;Issue resolved by pull request 34046
[https://github.com/apache/spark/pull/34046];;;",,,,,,,,,,
ClassCastException: optional int32 col-0 is not a group when reading legacy Parquet files ,SPARK-36803,13402082,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ivan.sadikov,ivan.sadikov,ivan.sadikov,20/Sep/21 00:20,22/Sep/21 09:43,13/Jul/23 08:50,22/Sep/21 09:42,3.1.2,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,"When reading Parquet files that have been written in legacy mode and schema evolution, we observed that 2-level LIST annotated types are traversed incorrectly. 

The root cause is the imprecise check on the underlying element type for Array types (and potentially Map types but I have not checked those yet) that happens here: [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/parquet/ParquetRowConverter.scala#L606]

The issue is only reproducible with schema evolution with parquet-mr reader and when there are two schemas like this:

File 1:
{code:java}
root
 |-- col-0: array (nullable = true)
 |    |-- element: struct (containsNull = false)
 |    |    |-- col-0: integer (nullable = true)
{code}
File 2:
{code:java}
root
 |-- col-0: array (nullable = true)
 |    |-- element: struct (containsNull = false)
 |    |    |-- col-0: integer (nullable = true)
 |    |    |-- col-1: integer (nullable = true){code}
 

When ParquetRowConverter tries to unwrap ArrayType, it checks if the underlying types between Parquet and Spark match. However, in the case above since the actual schema would include both fields, resulting in mismatch and failure to read File 1:
{noformat}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 11.0 failed 1 times, most recent failure: Lost task 1.0 in stage 11.0 (TID 18) (ip-1-2-3-4.us-west-2.compute.internal executor driver): java.lang.ClassCastException: optional int32 col-0 is not a group
at org.apache.parquet.schema.Type.asGroupType(Type.java:248)
at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter.org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(ParquetRowConverter.scala:424)
at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetArrayConverter$ElementConverter.<init>(ParquetRowConverter.scala:633)
at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter$ParquetArrayConverter.<init>(ParquetRowConverter.scala:616)
at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter.org$apache$spark$sql$execution$datasources$parquet$ParquetRowConverter$$newConverter(ParquetRowConverter.scala:390)
at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter.$anonfun$fieldConverters$1(ParquetRowConverter.scala:214)
at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
at scala.collection.TraversableLike.map(TraversableLike.scala:286)
at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
at scala.collection.AbstractTraversable.map(Traversable.scala:108)
at org.apache.spark.sql.execution.datasources.parquet.ParquetRowConverter.<init>(ParquetRowConverter.scala:210){noformat}
This happens due to L606 in ParquetRowConverter: 
{code:java}
DataType.equalsIgnoreCompatibleNullability(guessedElementType, elementType)
{code}
The code assumes that we are working with 3 level lists and would incorrectly remove the “dummy” level from the Parquet schema.

The actual error varies depending on column names - in this case struct type name matches primitive type name so we end up with ""optional int32 col-0 is not a group"". In other case, it could fail with IndexOutOfBoundException or NoSuchElementException when the column name is not found in the struct.

The reason it works with 3-level list, that DataType.equalsIgnoreCompatibleNullability(guessedElementType, elementType) always evaluates to false, we remove the “dummy” level and perform struct match which takes into account schema evolution.  

 

Repro:
{code:java}
import org.apache.spark.sql._
import org.apache.spark.sql.types._

val schema1 = StructType(
  StructField(""col-0"", ArrayType(
    StructType(
      StructField(""col-0"", IntegerType, true) :: Nil
    ), 
    containsNull = false
  )) :: Nil
)
val rdd1 = sc.parallelize(Row(Array(Row(1))) :: Nil, 1)
val df1 = spark.createDataFrame(rdd1, schema1)

df1.write.parquet(""/tmp/legacy-parquet"")

val schema2 = StructType(
  StructField(""col-0"", ArrayType(
    StructType(
      StructField(""col-0"", IntegerType, true) :: StructField(""col-1"", IntegerType, true) :: Nil
    ), 
    containsNull = false
  )) :: Nil
)
val rdd2 = sc.parallelize(Row(Array(Row(1, 2))) :: Nil, 1)
val df2 = spark.createDataFrame(rdd2, schema2)

df2.write.mode(""append"").parquet(""/tmp/legacy-parquet"")

// Fails with: Caused by: ClassCastException: optional int32 col-0 is not a group
display(spark.read.schema(schema2).parquet(""/tmp/legacy-parquet""))
{code}
 ",,apachespark,cloud_fan,ivan.sadikov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 22 09:42:36 UTC 2021,,,,,,,,,,"0|z0v2u8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/21 00:55;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/34044;;;","20/Sep/21 00:55;apachespark;User 'sadikovi' has created a pull request for this issue:
https://github.com/apache/spark/pull/34044;;;","22/Sep/21 09:42;cloud_fan;Issue resolved by pull request 34044
[https://github.com/apache/spark/pull/34044];;;",,,,,,,,,,,,,
"When SparkContext is stopped, metrics system should be flushed after listeners have finished processing",SPARK-36798,13401983,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,BOOTMGR,BOOTMGR,BOOTMGR,18/Sep/21 08:39,07/Oct/21 13:09,13/Jul/23 08:50,07/Oct/21 13:09,2.3.2,,,,,,,,3.3.0,,,,Structured Streaming,,,,,0,,,,,"In current implementation, when {{SparkContext.stop()}} is called, {{metricsSystem.report()}} is called before {{listenerBus.stop()}}. In this case, if some listener is producing some metrics, they would never reach sink.

Background:
We have some ingestion jobs in Spark Structured Streaming. To monitor them, collect some metrics like number of input rows, trigger time etc. from {{QueryProgressEvent}} received via {{StreamingQueryListener}}. These metrics are then pushed to DB by custom sinks registered in {{MetricsSystem}}. We noticed that these metrics are lost occasionally for last batch.",,apachespark,BOOTMGR,mridulm80,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 07 13:09:55 UTC 2021,,,,,,,,,,"0|z0v288:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/21 08:50;apachespark;User 'BOOTMGR' has created a pull request for this issue:
https://github.com/apache/spark/pull/34039;;;","18/Sep/21 08:51;apachespark;User 'BOOTMGR' has created a pull request for this issue:
https://github.com/apache/spark/pull/34039;;;","07/Oct/21 13:09;mridulm80;Issue resolved by pull request 34039
[https://github.com/apache/spark/pull/34039];;;",,,,,,,,,,,,,
Explain Formatted has Duplicated Node IDs with InMemoryRelation Present,SPARK-36795,13401923,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cwchentw,mikechen,mikechen,17/Sep/21 21:37,12/Dec/22 18:10,13/Jul/23 08:50,23/Sep/21 06:56,3.1.2,,,,,,,,3.2.1,,,,SQL,,,,,0,,,,,"When a query contains an InMemoryRelation, the output of Explain Formatted will contain duplicate node IDs.


{code:java}
== Physical Plan ==
AdaptiveSparkPlan (14)
+- == Final Plan ==
   * BroadcastHashJoin Inner BuildLeft (9)
   :- BroadcastQueryStage (5)
   :  +- BroadcastExchange (4)
   :     +- * Filter (3)
   :        +- * ColumnarToRow (2)
   :           +- InMemoryTableScan (1)
   :                 +- InMemoryRelation (2)
   :                       +- * ColumnarToRow (4)
   :                          +- Scan parquet default.t1 (3)
   +- * Filter (8)
      +- * ColumnarToRow (7)
         +- Scan parquet default.t2 (6)
+- == Initial Plan ==
   BroadcastHashJoin Inner BuildLeft (13)
   :- BroadcastExchange (11)
   :  +- Filter (10)
   :     +- InMemoryTableScan (1)
   :           +- InMemoryRelation (2)
   :                 +- * ColumnarToRow (4)
   :                    +- Scan parquet default.t1 (3)
   +- Filter (12)
      +- Scan parquet default.t2 (6)
{code}",,apachespark,mikechen,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 23 06:56:29 UTC 2021,,,,,,,,,,"0|z0v1uw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/21 21:50;apachespark;User 'ChenMichael' has created a pull request for this issue:
https://github.com/apache/spark/pull/34036;;;","23/Sep/21 06:56;gurwls223;Fixed in https://github.com/apache/spark/pull/34036;;;",,,,,,,,,,,,,,
use the correct constant type as the null value holder in array functions,SPARK-36789,13401785,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,17/Sep/21 06:16,17/Sep/21 08:08,13/Jul/23 08:50,17/Sep/21 08:08,3.0.0,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 17 06:20:01 UTC 2021,,,,,,,,,,"0|z0v108:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/21 06:20;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34029;;;",,,,,,,,,,,,,,,
ScanOperation should not push Filter through nondeterministic Project,SPARK-36783,13401703,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,16/Sep/21 16:45,17/Sep/21 02:54,13/Jul/23 08:50,17/Sep/21 02:54,3.1.0,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 17 02:54:54 UTC 2021,,,,,,,,,,"0|z0v0i0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/21 16:52;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/34023;;;","17/Sep/21 02:54;cloud_fan;Issue resolved by pull request 34023
[https://github.com/apache/spark/pull/34023];;;",,,,,,,,,,,,,,
Deadlock between map-output-dispatcher and dispatcher-BlockManagerMaster upon migrating shuffle blocks,SPARK-36782,13401673,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fthiele,fthiele,fthiele,16/Sep/21 14:03,10/Jan/22 20:30,13/Jul/23 08:50,23/Sep/21 04:58,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,3.2.1,3.3.0,,3.1.3,3.2.0,,,Block Manager,,,,,0,,,,,"I can observe a deadlock on the driver that can be triggered rather reliably in a job with a larger amount of tasks - upon using
{code:java}
spark.decommission.enabled: true
spark.storage.decommission.rddBlocks.enabled: true
spark.storage.decommission.shuffleBlocks.enabled: true
spark.storage.decommission.enabled: true{code}
 

It origins in the {{dispatcher-BlockManagerMaster}} making a call to {{updateBlockInfo}} when shuffles are migrated. This is not performed by a thread from the pool but instead by the {{dispatcher-BlockManagerMaster}} itself. I suppose this was done under the assumption that this would be very fast. However if the block that is updated is a shuffle index block it calls
{code:java}
mapOutputTracker.updateMapOutput(shuffleId, mapId, blockManagerId){code}
for which it waits to acquire a write lock as part of the {{MapOutputTracker}}.

If the timing is bad then one of the {{map-output-dispatchers}} are holding this lock as part of e.g. {{serializedMapStatus}}. In this function {{MapOutputTracker.serializeOutputStatuses}} is called and as part of that we do
{code:java}
if (arrSize >= minBroadcastSize) {
 // Use broadcast instead.
 // Important arr(0) is the tag == DIRECT, ignore that while deserializing !
 // arr is a nested Array so that it can handle over 2GB serialized data
 val arr = chunkedByteBuf.getChunks().map(_.array())
 val bcast = broadcastManager.newBroadcast(arr, isLocal){code}
which makes an RPC call to {{dispatcher-BlockManagerMaster}}. That one however is unable to answer as it is blocked while waiting on the aforementioned lock. Hence the deadlock. The ingredients of this deadlock are therefore: sufficient size of the array to go the broadcast-path, as well as timing of incoming {{updateBlockInfo}} call as happens regularly during decommissioning. Potentially earlier versions than 3.1.0 are affected but I could not sufficiently conclude that.

I have a stacktrace of all driver threads showing the deadlock: [^spark_stacktrace_deadlock.txt]

A coworker of mine wrote a patch that replicates the issue as a test case as well: [^0001-Add-test-showing-that-decommission-might-deadlock.patch]",,apachespark,eejbyfeldt,fthiele,Gengliang.Wang,rajesh.balamohan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/21 14:04;fthiele;0001-Add-test-showing-that-decommission-might-deadlock.patch;https://issues.apache.org/jira/secure/attachment/13033721/0001-Add-test-showing-that-decommission-might-deadlock.patch","16/Sep/21 14:03;fthiele;spark_stacktrace_deadlock.txt;https://issues.apache.org/jira/secure/attachment/13033720/spark_stacktrace_deadlock.txt",,2.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 23 06:00:48 UTC 2021,,,,,,,,,,"0|z0v0bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/21 21:27;apachespark;User 'f-thiele' has created a pull request for this issue:
https://github.com/apache/spark/pull/34043;;;","23/Sep/21 04:58;Gengliang.Wang;Issue resolved by pull request 34043
[https://github.com/apache/spark/pull/34043];;;","23/Sep/21 06:00;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/34076;;;",,,,,,,,,,,,,
Fix the uts to check the parquet compression,SPARK-36773,13401501,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jp.xiong,jp.xiong,jp.xiong,16/Sep/21 03:23,12/Dec/22 18:10,13/Jul/23 08:50,17/Sep/21 02:27,3.3.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"In my own test env. I import a wrong jar version about zstd-jni-1.4.4-3.jar with the parquet version 1.12.0, when I pass the full uts and release a spark version to my env , using the parquet table with compression zstd, it throw a ""NoClassDefFoundError"" Exception. And I check the parquet source code, find the right zstd version for the 1.12.0 is 1.4.9-1. Once I upgrade the version about zstd-jni, it work well. 

!https://clouddevops.huawei.com/vision-file-storage/api/file/download/upload-v2/2021/8/14/xwx915262/ed1735e06a684bf19cf1d1a68a130b6a/image.png!

But I think we need check for all plugin compression algorithm can work well when we upgrade the version about those compression jars and parquet version. ",,apachespark,jp.xiong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 17 02:48:01 UTC 2021,,,,,,,,,,"0|z0uz94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/21 03:52;apachespark;User 'BelodengKlaus' has created a pull request for this issue:
https://github.com/apache/spark/pull/34012;;;","16/Sep/21 03:53;apachespark;User 'BelodengKlaus' has created a pull request for this issue:
https://github.com/apache/spark/pull/34012;;;","17/Sep/21 02:48;gurwls223;Fixed in https://github.com/apache/spark/pull/34012;;;",,,,,,,,,,,,,
FinalizeShuffleMerge fails with an exception due to attempt id not matching,SPARK-36772,13401481,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,zhouyejoe,mridulm80,mridulm80,15/Sep/21 23:53,18/Sep/21 07:52,13/Jul/23 08:50,18/Sep/21 07:52,3.2.0,,,,,,,,3.2.0,,,,Shuffle,,,,,0,,,,,"As part of driver request to external shuffle services (ESS) to finalize the merge, it also passes its [application attempt id|https://github.com/apache/spark/blob/3f09093a21306b0fbcb132d4c9f285e56ac6b43c/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalBlockStoreClient.java#L180] so that ESS can validate the request is from the correct attempt.
This attempt id is fetched from the TransportConf passed in when creating the [ExternalBlockStoreClient|https://github.com/apache/spark/blob/67421d80b8935d91b86e8cd3becb211fa2abd54f/core/src/main/scala/org/apache/spark/SparkEnv.scala#L352] - and the transport conf leverages a [cloned copy|https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/core/src/main/scala/org/apache/spark/network/netty/SparkTransportConf.scala#L47] of the SparkConf passed to it.


Application attempt id is set as part of SparkContext [initialization|https://github.com/apache/spark/blob/67421d80b8935d91b86e8cd3becb211fa2abd54f/core/src/main/scala/org/apache/spark/SparkContext.scala#L586].
But this happens after driver SparkEnv has [already been created|https://github.com/apache/spark/blob/67421d80b8935d91b86e8cd3becb211fa2abd54f/core/src/main/scala/org/apache/spark/SparkContext.scala#L460].

Hence the attempt id that ExternalBlockStoreClient uses will always end up being -1 : which will not match the attempt id at ESS (which is based on spark.app.attempt.id) : resulting in merge finalization to always fail ("" java.lang.IllegalArgumentException: The attempt id -1 in this FinalizeShuffleMerge message does not match with the current attempt id 1 stored in shuffle service for application ..."")",,apachespark,Gengliang.Wang,mridulm80,xkrogen,zhouyejoe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 18 07:52:45 UTC 2021,,,,,,,,,,"0|z0uz4o:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"15/Sep/21 23:55;mridulm80;+CC [~Gengliang.Wang], [~Ngone51];;;","16/Sep/21 00:30;Gengliang.Wang;[~mridulm80]Thanks for the ping. I will cut RC3 after this one is resolved.;;;","16/Sep/21 05:36;zhouyejoe;I will work on this one and post PR ASAP.;;;","16/Sep/21 05:57;mridulm80;Thanks [~Gengliang.Wang] !
And thanks for working on the PR [~zhouyejoe] :);;;","16/Sep/21 07:44;apachespark;User 'zhouyejoe' has created a pull request for this issue:
https://github.com/apache/spark/pull/34018;;;","18/Sep/21 07:52;Gengliang.Wang;Issue resolved by pull request 34018
[https://github.com/apache/spark/pull/34018];;;",,,,,,,,,,
"Fix race-condition on ""ensure continuous stream is being used"" in KafkaContinuousTest",SPARK-36764,13401264,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,15/Sep/21 06:47,17/Sep/21 13:32,13/Jul/23 08:50,17/Sep/21 13:32,2.4.8,3.0.3,3.1.2,3.2.0,,,,,3.2.0,,,,Structured Streaming,,,,,0,,,,,"The test “ensure continuous stream is being used“ in KafkaContinuousTestquickly checks the actual type of the execution, and stop the query. Stopping the streaming query in continuous mode is done by interrupting query execution thread and join indefinitely.

In parallel, started streaming query is going to generate execution plan, including running optimizer. Some parts of SessionState can be built at that time, as they are defined as lazy. The problem is, some of them seem to be able to “swallow” the InterruptedException and let the thread run continuously.

That said, the query can’t indicate whether there is a request on stopping query, so the query won’t stop.",,apachespark,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 15 06:58:05 UTC 2021,,,,,,,,,,"0|z0uxso:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/21 06:48;kabhwan;Will submit a PR soon.;;;","15/Sep/21 06:57;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/34004;;;","15/Sep/21 06:58;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/34004;;;",,,,,,,,,,,,,
collection operators should handle duplicated NaN,SPARK-36740,13400779,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,13/Sep/21 12:28,27/Sep/21 04:44,13/Jul/23 08:50,27/Sep/21 04:44,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,correctness,,,,"collection operators should handle duplicated NaN, current OpenHashSet can't handle duplicated NaN",,angerszhuuu,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 15 15:43:07 UTC 2021,,,,,,,,,,"0|z0uusw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Sep/21 15:42;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34008;;;","15/Sep/21 15:43;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/34008;;;",,,,,,,,,,,,,,
Wrong description on Cot API,SPARK-36738,13400748,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,YActs,YActs,YActs,13/Sep/21 09:24,12/Dec/22 18:11,13/Jul/23 08:50,13/Sep/21 12:53,3.1.2,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,The description on cotangent API is not documented correctly.,,apachespark,YActs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 13 12:53:24 UTC 2021,,,,,,,,,,"0|z0uum0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/21 10:36;apachespark;User 'yutoacts' has created a pull request for this issue:
https://github.com/apache/spark/pull/33978;;;","13/Sep/21 12:53;gurwls223;Fixed in https://github.com/apache/spark/pull/33978;;;",,,,,,,,,,,,,,
Upgrade ORC to 1.5.13,SPARK-36734,13400701,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,13/Sep/21 03:56,16/Sep/21 07:53,13/Jul/23 08:50,16/Sep/21 07:51,3.1.2,,,,,,,,3.1.3,,,,Build,SQL,,,,0,,,,,,,apachespark,dongjoon,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 16 07:51:49 UTC 2021,,,,,,,,,,"0|z0uubk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/21 04:04;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33972;;;","16/Sep/21 07:51;Gengliang.Wang;Issue resolved by pull request 33972
[https://github.com/apache/spark/pull/33972];;;",,,,,,,,,,,,,,
Perf issue in SchemaPruning when a struct has many fields,SPARK-36733,13400694,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,taroplus,taroplus,13/Sep/21 03:11,12/Dec/22 18:11,13/Jul/23 08:50,15/Sep/21 01:34,3.1.2,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"Seeing a significant performance degradation in query processing when a table contains a significantly large number of fields (>10K).

Here's the stacktraces while processing a query
{code:java}
   java.lang.Thread.State: RUNNABLE   java.lang.Thread.State: RUNNABLE at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:285) at scala.collection.TraversableLike$$Lambda$296/874023329.apply(Unknown Source) at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198) at scala.collection.TraversableLike.map(TraversableLike.scala:285) at scala.collection.TraversableLike.map$(TraversableLike.scala:278) at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198) at org.apache.spark.sql.types.StructType.fieldNames(StructType.scala:108) at org.apache.spark.sql.catalyst.expressions.SchemaPruning$.$anonfun$sortLeftFieldsByRight$1(SchemaPruning.scala:70) at org.apache.spark.sql.catalyst.expressions.SchemaPruning$.$anonfun$sortLeftFieldsByRight$1$adapted(SchemaPruning.scala:70) at org.apache.spark.sql.catalyst.expressions.SchemaPruning$$$Lambda$3963/249742655.apply(Unknown Source) at scala.collection.TraversableLike.$anonfun$filterImpl$1(TraversableLike.scala:303) at scala.collection.TraversableLike$$Lambda$403/465534593.apply(Unknown Source) at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36) at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33) at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198) at scala.collection.TraversableLike.filterImpl(TraversableLike.scala:302) at scala.collection.TraversableLike.filterImpl$(TraversableLike.scala:296) at scala.collection.mutable.ArrayOps$ofRef.filterImpl(ArrayOps.scala:198) at scala.collection.TraversableLike.filter(TraversableLike.scala:394) at scala.collection.TraversableLike.filter$(TraversableLike.scala:394) at scala.collection.mutable.ArrayOps$ofRef.filter(ArrayOps.scala:198) at org.apache.spark.sql.catalyst.expressions.SchemaPruning$.sortLeftFieldsByRight(SchemaPruning.scala:70) at org.apache.spark.sql.catalyst.expressions.SchemaPruning$.$anonfun$sortLeftFieldsByRight$3(SchemaPruning.scala:75) at org.apache.spark.sql.catalyst.expressions.SchemaPruning$$$Lambda$3965/461314749.apply(Unknown Source) {code}",,apachespark,taroplus,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 15 01:34:33 UTC 2021,,,,,,,,,,"0|z0uua0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/21 03:16;taroplus;[https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/SchemaPruning.scala#L69]

often time (as long as I observed), left struct and the right struct are the same one. And every call to {{StructType.fieldNames}}  runs {{ fields.map(_.name). }}

this computation is quite expensive for 10K fields.

{{ val filteredRightFieldNames = rightStruct.fieldNames}}
{{    .filter(name => leftStruct.fieldNames.exists(resolver(_, name)))}}{{ }}

 ;;;","13/Sep/21 17:37;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33981;;;","13/Sep/21 17:38;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33981;;;","15/Sep/21 01:34;gurwls223;Issue resolved by pull request 33981
[https://github.com/apache/spark/pull/33981];;;",,,,,,,,,,,,
Upgrade ORC to 1.6.11,SPARK-36732,13400690,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,13/Sep/21 02:47,16/Sep/21 06:37,13/Jul/23 08:50,16/Sep/21 06:37,3.2.0,,,,,,,,3.2.0,,,,Build,SQL,,,,0,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 16 06:37:02 UTC 2021,,,,,,,,,,"0|z0uu94:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/21 02:56;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33971;;;","16/Sep/21 06:37;dongjoon;Issue resolved by pull request 33971
[https://github.com/apache/spark/pull/33971];;;",,,,,,,,,,,,,,
Upgrade Parquet to 1.12.1,SPARK-36726,13400614,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,csun,csun,csun,12/Sep/21 07:27,15/Sep/21 19:18,13/Jul/23 08:50,15/Sep/21 19:18,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,Upgrade Apache Parquet to 1.12.1,,apachespark,csun,dbtsai,,,,,,,,,,,,,,,SPARK-34276,,,,,,,,,SPARK-36696,PARQUET-2078,SPARK-34542,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 15 19:18:16 UTC 2021,,,,,,,,,,"0|z0uts8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/21 08:32;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33969;;;","15/Sep/21 19:18;dbtsai;Issue resolved by pull request 33969
[https://github.com/apache/spark/pull/33969];;;",,,,,,,,,,,,,,
Ensure HiveThriftServer2Suites to stop Thrift JDBC server on exit,SPARK-36725,13400605,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,11/Sep/21 19:17,11/Sep/21 22:56,13/Jul/23 08:50,11/Sep/21 22:56,3.0.3,3.1.2,3.2.0,,,,,,3.2.0,,,,Tests,,,,,0,,,,,"HiveThriftServer2Suites stops Thrift JDBC server via afterAll method.
But, if they are killed by signal (e.g. Ctrl-C), Thrift JDBC server will be remain.
{code}
$ jps
2792969 SparkSubmit
{code}",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 11 22:56:32 UTC 2021,,,,,,,,,,"0|z0utq8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/21 19:30;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33967;;;","11/Sep/21 22:56;dongjoon;This is resolved via https://github.com/apache/spark/pull/33967;;;",,,,,,,,,,,,,,
Problems with update function in koalas - pyspark pandas.,SPARK-36722,13400563,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dc-heros,bjornjorgensen,bjornjorgensen,11/Sep/21 10:39,15/Sep/21 18:09,13/Jul/23 08:50,15/Sep/21 18:09,3.2.0,3.3.0,,,,,,,3.2.0,,,,PySpark,,,,,0,,,,,"Hi I am using ""from pyspark import pandas as ps"" in a master build yesterday. 
I do have some columns that I need to join to one. 
In pandas I use update.


54 FD_OBJECT_SUPPLIES_SERVICES_OBJECT_SUPPLY_SERVICE_ADDITIONAL_INFORMATION 23 non-null object 
55 FD_OBJECT_SUPPLIES_SERVICES_OBJECT_SUPPLY_SERVICE_ADDITIONAL_INFORMATION.P 24348 non-null object
 
 
 pd1['FD_OBJECT_SUPPLIES_SERVICES_OBJECT_SUPPLY_SERVICE_ADDITIONAL_INFORMATION'].update(pd1['FD_OBJECT_SUPPLIES_SERVICES_OBJECT_SUPPLY_SERVICE_ADDITIONAL_INFORMATION.P'])
 
 ---------------------------------------------------------------------------
AssertionError Traceback (most recent call last)
/tmp/ipykernel_73/391781247.py in <module>
----> 1 pd1['FD_OBJECT_SUPPLIES_SERVICES_OBJECT_SUPPLY_SERVICE_ADDITIONAL_INFORMATION'].update(pd1['FD_OBJECT_SUPPLIES_SERVICES_OBJECT_SUPPLY_SERVICE_ADDITIONAL_INFORMATION.P'])

/opt/spark/python/pyspark/pandas/series.py in update(self, other)
 4549 raise TypeError(""'other' must be a Series"")
 4550 
-> 4551 combined = combine_frames(self._psdf, other._psdf, how=""leftouter"")
 4552 
 4553 this_scol = combined[""this""]._internal.spark_column_for(self._column_label)

/opt/spark/python/pyspark/pandas/utils.py in combine_frames(this, how, preserve_order_column, *args)
 139 elif len(args) == 1 and isinstance(args[0], DataFrame):
 140 assert isinstance(args[0], DataFrame)
--> 141 assert not same_anchor(
 142 this, args[0]
 143 ), ""We don't need to combine. `this` and `that` are same.""

AssertionError: We don't need to combine. `this` and `that` are same.


pd1.info()

54 FD_OBJECT_SUPPLIES_SERVICES_OBJECT_SUPPLY_SERVICE_ADDITIONAL_INFORMATION 23 non-null object 
55 FD_OBJECT_SUPPLIES_SERVICES_OBJECT_SUPPLY_SERVICE_ADDITIONAL_INFORMATION.P 24348 non-null object",,apachespark,bjornjorgensen,dc-heros,ueshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 15 18:09:42 UTC 2021,,,,,,,,,,"0|z0utgw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/21 00:22;dc-heros;I will make a PR to fix this one soon;;;","12/Sep/21 01:32;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33968;;;","12/Sep/21 01:33;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33968;;;","15/Sep/21 18:09;ueshin;Issue resolved by pull request 33968
https://github.com/apache/spark/pull/33968;;;",,,,,,,,,,,,
Wrong order of variable initialization may lead to incorrect behavior,SPARK-36717,13400448,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Li Jianmeng,Li Jianmeng,Li Jianmeng,10/Sep/21 12:05,10/Jan/22 23:52,13/Jul/23 08:50,08/Oct/21 12:13,3.1.2,,,,,,,,3.0.4,3.1.3,3.2.1,3.3.0,Spark Core,,,,,0,,,,,"Incorrect order of variable initialization may lead to incorrect behavior, Related code: [TorrentBroadcast.scala|https://github.com/apache/spark/blob/0494dc90af48ce7da0625485a4dc6917a244d580/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala#L94] , TorrentBroadCast will get wrong checksumEnabled value after initialization, this may not be what we need, we can move L94 front of setConf(SparkEnv.get.conf) to avoid this.

Supplement:

Snippet 1:
{code:java}
class Broadcast {
  def setConf(): Unit = {
    checksumEnabled = true
  }
  setConf()
  var checksumEnabled = false
}

println(new Broadcast().checksumEnabled){code}
output:
{code:java}
false{code}
Snippet 2:
{code:java}
class Broadcast {
  var checksumEnabled = false
  def setConf(): Unit = {
    checksumEnabled = true
  }
  setConf()
}

println(new Broadcast().checksumEnabled){code}
output: 
{code:java}
true{code}",,apachespark,humanoid,joshrosen,Li Jianmeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,scala,Wed Oct 13 07:52:36 UTC 2021,,,,,,,,,,"0|z0usrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/21 12:23;Li Jianmeng;I'm working on the fix, [https://github.com/apache/spark/pull/33957]
 ;;;","10/Sep/21 12:27;apachespark;User 'daugraph' has created a pull request for this issue:
https://github.com/apache/spark/pull/33957;;;","08/Oct/21 12:13;srowen;Resolved by https://github.com/apache/spark/pull/33957;;;","13/Oct/21 07:52;humanoid;please update ""fix versions""

commit was not included in 3.2.0 and will be included in 3.2.1;;;",,,,,,,,,,,,
explode(UDF) throw an exception,SPARK-36715,13400403,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,fchen,fchen,10/Sep/21 08:43,12/Dec/22 18:11,13/Jul/23 08:50,14/Sep/21 00:27,3.1.2,,,,,,,,3.1.3,3.2.0,,,SQL,,,,,0,,,,,"Code to reproduce:

 
{code:java}
spark.udf.register(""vec"", (i: Int) => (0 until i).toArray)
sql(""select explode(vec(8)) as c1"").show{code}
{code:java}
Once strategy's idempotence is broken for batch Infer FiltersOnce strategy's idempotence is broken for batch Infer Filters GlobalLimit 21                                                        GlobalLimit 21 +- LocalLimit 21                                                      +- LocalLimit 21    +- Project [cast(c1#3 as string) AS c1#12]                            +- Project [cast(c1#3 as string) AS c1#12]       +- Generate explode(vec(8)), false, [c1#3]                            +- Generate explode(vec(8)), false, [c1#3]          +- Filter ((size(vec(8), true) > 0) AND isnotnull(vec(8)))            +- Filter ((size(vec(8), true) > 0) AND isnotnull(vec(8)))!            +- OneRowRelation                                                     +- Filter ((size(vec(8), true) > 0) AND isnotnull(vec(8)))!                                                                                     +- OneRowRelation       java.lang.RuntimeException: Once strategy's idempotence is broken for batch Infer Filters GlobalLimit 21                                                        GlobalLimit 21 +- LocalLimit 21                                                      +- LocalLimit 21    +- Project [cast(c1#3 as string) AS c1#12]                            +- Project [cast(c1#3 as string) AS c1#12]       +- Generate explode(vec(8)), false, [c1#3]                            +- Generate explode(vec(8)), false, [c1#3]          +- Filter ((size(vec(8), true) > 0) AND isnotnull(vec(8)))            +- Filter ((size(vec(8), true) > 0) AND isnotnull(vec(8)))!            +- OneRowRelation                                                     +- Filter ((size(vec(8), true) > 0) AND isnotnull(vec(8)))!                                                                                     +- OneRowRelation        at org.apache.spark.sql.errors.QueryExecutionErrors$.onceStrategyIdempotenceIsBrokenForBatchError(QueryExecutionErrors.scala:1200) at org.apache.spark.sql.catalyst.rules.RuleExecutor.checkBatchIdempotence(RuleExecutor.scala:168) at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:254) at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200) at scala.collection.immutable.List.foreach(List.scala:431) at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200) at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179) at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88) at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179) at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:138) at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:196) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775) at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:196) at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:134) at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:130) at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:148) at org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:166) at org.apache.spark.sql.execution.QueryExecution.withCteMap(QueryExecution.scala:73) at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:163) at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:163) at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:214) at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:259) at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:228) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98) at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3731) at org.apache.spark.sql.Dataset.head(Dataset.scala:2755) at org.apache.spark.sql.Dataset.take(Dataset.scala:2962) at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288) at org.apache.spark.sql.Dataset.showString(Dataset.scala:327) at org.apache.spark.sql.Dataset.show(Dataset.scala:807) at org.apache.spark.sql.Dataset.show(Dataset.scala:766) at org.apache.spark.sql.Dataset.show(Dataset.scala:775) at org.apache.spark.sql.jdbc.SimpleSuite.$anonfun$new$268(JDBCSuite.scala:3125) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.scalatest.OutcomeOf.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf.outcomeOf$(OutcomeOf.scala:83) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) at org.scalatest.funsuite.AnyFunSuiteLike$$anon$1.apply(AnyFunSuiteLike.scala:226) at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:190) at org.scalatest.funsuite.AnyFunSuiteLike.invokeWithFixture$1(AnyFunSuiteLike.scala:224) at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTest$1(AnyFunSuiteLike.scala:236) at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) at org.scalatest.funsuite.AnyFunSuiteLike.runTest(AnyFunSuiteLike.scala:236) at org.scalatest.funsuite.AnyFunSuiteLike.runTest$(AnyFunSuiteLike.scala:218) at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(SparkFunSuite.scala:62) at org.scalatest.BeforeAndAfterEach.runTest(BeforeAndAfterEach.scala:234) at org.scalatest.BeforeAndAfterEach.runTest$(BeforeAndAfterEach.scala:227) at org.apache.spark.sql.jdbc.SimpleSuite.org$scalatest$BeforeAndAfter$$super$runTest(JDBCSuite.scala:2048) at org.scalatest.BeforeAndAfter.runTest(BeforeAndAfter.scala:213) at org.scalatest.BeforeAndAfter.runTest$(BeforeAndAfter.scala:203) at org.apache.spark.sql.jdbc.SimpleSuite.runTest(JDBCSuite.scala:2048) at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$runTests$1(AnyFunSuiteLike.scala:269) at org.scalatest.SuperEngine.$anonfun$runTestsInBranch$1(Engine.scala:413) at scala.collection.immutable.List.foreach(List.scala:431) at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) at org.scalatest.SuperEngine.runTestsInBranch(Engine.scala:396) at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:475) at org.scalatest.funsuite.AnyFunSuiteLike.runTests(AnyFunSuiteLike.scala:269) at org.scalatest.funsuite.AnyFunSuiteLike.runTests$(AnyFunSuiteLike.scala:268) at org.scalatest.funsuite.AnyFunSuite.runTests(AnyFunSuite.scala:1563) at org.scalatest.Suite.run(Suite.scala:1112) at org.scalatest.Suite.run$(Suite.scala:1094) at org.scalatest.funsuite.AnyFunSuite.org$scalatest$funsuite$AnyFunSuiteLike$$super$run(AnyFunSuite.scala:1563) at org.scalatest.funsuite.AnyFunSuiteLike.$anonfun$run$1(AnyFunSuiteLike.scala:273) at org.scalatest.SuperEngine.runImpl(Engine.scala:535) at org.scalatest.funsuite.AnyFunSuiteLike.run(AnyFunSuiteLike.scala:273) at org.scalatest.funsuite.AnyFunSuiteLike.run$(AnyFunSuiteLike.scala:272) at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:62) at org.scalatest.BeforeAndAfterAll.liftedTree1$1(BeforeAndAfterAll.scala:213) at org.scalatest.BeforeAndAfterAll.run(BeforeAndAfterAll.scala:210) at org.scalatest.BeforeAndAfterAll.run$(BeforeAndAfterAll.scala:208) at org.apache.spark.sql.jdbc.SimpleSuite.org$scalatest$BeforeAndAfter$$super$run(JDBCSuite.scala:2048) at org.scalatest.BeforeAndAfter.run(BeforeAndAfter.scala:273) at org.scalatest.BeforeAndAfter.run$(BeforeAndAfter.scala:271) at org.apache.spark.sql.jdbc.SimpleSuite.run(JDBCSuite.scala:2048) at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:45) at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13(Runner.scala:1322) at org.scalatest.tools.Runner$.$anonfun$doRunRunRunDaDoRunRun$13$adapted(Runner.scala:1316) at scala.collection.immutable.List.foreach(List.scala:431) at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:1316) at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24(Runner.scala:993) at org.scalatest.tools.Runner$.$anonfun$runOptionallyWithPassFailReporter$24$adapted(Runner.scala:971) at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:1482) at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:971) at org.scalatest.tools.Runner$.run(Runner.scala:798) at org.scalatest.tools.Runner.run(Runner.scala) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:133) at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:27){code}
 ",,apachespark,fchen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 14 00:27:42 UTC 2021,,,,,,,,,,"0|z0ushc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/21 08:43;fchen;working on this;;;","10/Sep/21 10:39;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33956;;;","10/Sep/21 10:39;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33956;;;","14/Sep/21 00:27;gurwls223;Issue resolved by pull request 33956
[https://github.com/apache/spark/pull/33956];;;",,,,,,,,,,,,
OverwriteByExpression conversion in DataSourceV2Strategy use wrong deleteExpr translation,SPARK-36706,13400359,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,huaxingao,suheng.cloud,suheng.cloud,10/Sep/21 03:51,12/Dec/22 18:11,13/Jul/23 08:50,20/Sep/21 20:28,3.1.2,,,,,,,,3.1.3,,,,SQL,,,,,0,,,,,"spark version release-3.1.2

we develop a hive datasource v2 plugin to support join among multiple hive clusters.
find that there maybe a bug in OverwriteByExpression conversion

code debug at https://github.com/apache/spark/blob/de351e30a90dd988b133b3d00fa6218bfcaba8b8/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/v2/DataSourceV2Strategy.scala#L216

where wrong param `deletExpr` used, which will result in duplicate filters",,apachespark,dongjoon,huaxingao,suheng.cloud,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 20 20:28:07 UTC 2021,,,,,,,,,,"0|z0us7k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Sep/21 04:11;gurwls223;cc [~huaxingao] FYI;;;","14/Sep/21 04:56;huaxingao;I will fix this. Thanks for pinging me [~hyukjin.kwon];;;","14/Sep/21 21:40;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33997;;;","20/Sep/21 20:28;dongjoon;Issue resolved by pull request 33997
[https://github.com/apache/spark/pull/33997];;;",,,,,,,,,,,,
InaccessibleObjectException in Java 9+ on startup,SPARK-36704,13400289,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,srowen,srowen,srowen,09/Sep/21 16:37,11/Sep/21 18:39,13/Jul/23 08:50,11/Sep/21 18:39,3.2.0,,,,,,,,3.0.4,3.1.3,3.2.0,,Spark Core,,,,,0,,,,,"A user reported this error on startup, which must be Java9+-related given the Java-9+ exception:

{code}
Caused by: java.lang.reflect.InaccessibleObjectException: Unable to make private java.nio.DirectByteBuffer(long,int) accessible: module java.base does not ""opens java.nio"" to unnamed module @71e9ddb4
        at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:357)
        at java.base/java.lang.reflect.AccessibleObject.checkCanSetAccessible(AccessibleObject.java:297)
        at java.base/java.lang.reflect.Constructor.checkCanSetAccessible(Constructor.java:188)
        at java.base/java.lang.reflect.Constructor.setAccessible(Constructor.java:181)
        at org.apache.spark.unsafe.Platform.<clinit>(Platform.java:56)
{code}

Code in this section tries to open up some access to DirectBuffer by reflection, which no longer works by default in Java 9+. We long ago tried to handle this by dealing with the exception and avoiding reflection where not possible. However this seems like a possible new way the same type of issue can manifest.

We can shore up the code checking this to try to handle this apparent error path similarly.",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 11 18:39:16 UTC 2021,,,,,,,,,,"0|z0urs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/21 17:30;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33947;;;","11/Sep/21 18:39;srowen;Issue resolved by pull request 33947
[https://github.com/apache/spark/pull/33947];;;",,,,,,,,,,,,,,
BlockManager re-registration is broken due to deferred removal of BlockManager ,SPARK-36700,13400120,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,Ngone51,Ngone51,Ngone51,09/Sep/21 02:28,13/Sep/21 02:34,13/Jul/23 08:50,13/Sep/21 02:34,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.0.4,3.1.3,3.2.0,3.3.0,Spark Core,,,,,0,,,,,"Due to the deferred removal of BlockManager (introduced in SPARK-35011), an expected BlockManager re-registration could be refused as the inactive BlockManager still exists in the map `blockManagerInfo`:

https://github.com/apache/spark/blob/9cefde8db373a3433b7e3ce328e4a2ce83b1aca2/core/src/main/scala/org/apache/spark/storage/BlockManagerMasterEndpoint.scala#L551",,mridulm80,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Sep 13 02:33:46 UTC 2021,,,,,,,,,,"0|z0uqqg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Sep/21 02:32;Ngone51;cc [~mridulm80] [~sumeet.gajjar] [~attilapiros]

cc [~gengliang] for the blocker fyi;;;","09/Sep/21 02:38;Ngone51;I'm working on the fix.;;;","09/Sep/21 17:28;mridulm80;Thanks for chasing this down [~Ngone51] !
Agree, this is an issue.;;;","13/Sep/21 02:33;Ngone51;Reverted by [https://github.com/apache/spark/pull/33942] and backported to 3.2, 3.1, 3.0.;;;",,,,,,,,,,,,
spark.read.parquet loads empty dataset,SPARK-36696,13400081,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,,ueshin,ueshin,08/Sep/21 18:50,15/Sep/21 19:51,13/Jul/23 08:50,15/Sep/21 19:51,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"Here's a parquet file Spark 3.2/master can't read properly.

The file was stored by pandas and must contain 3650 rows, but Spark 3.2/master returns an empty dataset.
{code:python}
>>> import pandas as pd
>>> len(pd.read_parquet('/path/to/example.parquet'))
3650

>>> spark.read.parquet('/path/to/example.parquet').count()
0
{code}
I guess it's caused by the parquet 1.12.0.

When I reverted two commits related to the parquet 1.12.0 from branch-3.2:
 - [https://github.com/apache/spark/commit/e40fce919ab77f5faeb0bbd34dc86c56c04adbaa]
 - [https://github.com/apache/spark/commit/cbffc12f90e45d33e651e38cf886d7ab4bcf96da]

it reads the data successfully.

We need to add some workaround, or revert the commits.",,csun,dbtsai,emkornfield@gmail.com,gershinsky,nemon,senthh,ueshin,,,,,,,,,,,,,,,,,,,,PARQUET-2078,SPARK-34276,,SPARK-36726,,"08/Sep/21 18:52;ueshin;example.parquet;https://issues.apache.org/jira/secure/attachment/13033228/example.parquet",,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 15 19:51:20 UTC 2021,,,,,,,,,,"0|z0uqhs:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"08/Sep/21 20:04;csun;This looks like the same issue as in PARQUET-2078. The file offset for the first row group is set to 31173 which causes {{filterFileMetaDataByMidpoint}} to filter out the only row group (range filter is [0, 37968], while startIndex is 31173 and total size is 35820).

Seems there is a bug in Apache Arrow which writes incorrect file offset. cc [~gershinsky] to see if you know any info there.;;;","08/Sep/21 20:11;csun;[This|https://github.com/apache/arrow/blob/master/cpp/src/parquet/metadata.cc#L1331] looks suspicious: why column chunk file offset = dictionary/data page offset + compressed size of the column chunk?;;;","09/Sep/21 13:56;gershinsky;The [fix|https://github.com/apache/parquet-mr/pull/925] for PARQUET-2078 solves this problem. But the Arrow folks need to fix the `RowGroup.offset` computation, since it might affect some of the encrypted files (if they are read by Spark).;;;","09/Sep/21 14:04;gershinsky;|why column chunk file offset = dictionary/data page offset + compressed size of the column chunk?|

A Java (parquet-mr) specific comment - this version uses mostly the offsets in the ColumnMetaData structure. Recently, it started to use the offset in RowGroup structure. But it doesn't use the offset in the ColumnChunk (AFAIK; at least my IJ couldn't find its usage :);;;","14/Sep/21 05:12;emkornfield@gmail.com;What [~gershinsky]  wrote seems to make sense from my reading of the code. I think the issue here PARQUET-2089.;;;","15/Sep/21 19:20;dbtsai;This issue is addressed by SPARK-34542 Can you verify and close this?;;;","15/Sep/21 19:51;ueshin;I confirmed the example file can be read after the upgrade.
I'd close this now. Thanks!;;;",,,,,,,,,
Fix SimplifyConditionalsInPredicate to be null-safe,SPARK-36686,13399840,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,hypercubestart,hypercubestart,hypercubestart,07/Sep/21 18:31,09/Sep/21 03:35,13/Jul/23 08:50,09/Sep/21 03:34,3.1.2,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,SimplifyConditionalsInPredicate rule is not null-safe and leads to incorrect results,,apachespark,cloud_fan,hypercubestart,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 09 03:34:18 UTC 2021,,,,,,,,,,"0|z0up08:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/21 18:34;apachespark;User 'hypercubestart' has created a pull request for this issue:
https://github.com/apache/spark/pull/33928;;;","07/Sep/21 18:35;apachespark;User 'hypercubestart' has created a pull request for this issue:
https://github.com/apache/spark/pull/33928;;;","09/Sep/21 03:34;cloud_fan;Issue resolved by pull request 33928
[https://github.com/apache/spark/pull/33928];;;",,,,,,,,,,,,,
Clean up `Cannot load filesystem` warning log when test the sql/core module with hadoop-2.7 profile,SPARK-36684,13399790,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,07/Sep/21 15:11,08/Sep/21 04:41,13/Jul/23 08:50,08/Sep/21 04:41,3.2.0,3.3.0,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"[https://amplab.cs.berkeley.edu/jenkins/job/spark-master-test-maven-hadoop-2.7/2493/consoleFull]

There are no failed test cases, but warning logs as follows when test sql/core module with hadoop-2.7 profile:
{code:java}
17:12:17.605 WARN org.apache.hadoop.fs.FileSystem: Cannot load filesystem
java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.WebHdfsFileSystem could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232)
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
	at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2631)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2650)
	at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>(FsUrlStreamHandlerFactory.java:62)
	at org.apache.spark.sql.internal.SharedState$.liftedTree2$1(SharedState.scala:193)
	at org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$setFsUrlStreamHandlerFactory(SharedState.scala:192)
	at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:54)
	at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:139)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:139)
	at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:138)
	at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:158)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:156)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:153)
	at org.apache.spark.sql.SparkSession.udf(SparkSession.scala:223)
	at test.org.apache.spark.sql.JavaUDFSuite.sourceTest(JavaUDFSuite.java:148)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:364)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:272)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:237)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Caused by: java.lang.NoClassDefFoundError: org/codehaus/jackson/map/ObjectMapper
	at org.apache.hadoop.hdfs.web.WebHdfsFileSystem.<clinit>(WebHdfsFileSystem.java:129)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at java.lang.Class.newInstance(Class.java:442)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
	... 48 more
Caused by: java.lang.ClassNotFoundException: org.codehaus.jackson.map.ObjectMapper
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:352)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	... 55 more
17:12:17.612 WARN org.apache.hadoop.fs.FileSystem: Cannot load filesystem
java.util.ServiceConfigurationError: org.apache.hadoop.fs.FileSystem: Provider org.apache.hadoop.hdfs.web.SWebHdfsFileSystem could not be instantiated
	at java.util.ServiceLoader.fail(ServiceLoader.java:232)
	at java.util.ServiceLoader.access$100(ServiceLoader.java:185)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:384)
	at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:404)
	at java.util.ServiceLoader$1.next(ServiceLoader.java:480)
	at org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2631)
	at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2650)
	at org.apache.hadoop.fs.FsUrlStreamHandlerFactory.<init>(FsUrlStreamHandlerFactory.java:62)
	at org.apache.spark.sql.internal.SharedState$.liftedTree2$1(SharedState.scala:193)
	at org.apache.spark.sql.internal.SharedState$.org$apache$spark$sql$internal$SharedState$$setFsUrlStreamHandlerFactory(SharedState.scala:192)
	at org.apache.spark.sql.internal.SharedState.<init>(SharedState.scala:54)
	at org.apache.spark.sql.SparkSession.$anonfun$sharedState$1(SparkSession.scala:139)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession.sharedState$lzycompute(SparkSession.scala:139)
	at org.apache.spark.sql.SparkSession.sharedState(SparkSession.scala:138)
	at org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:158)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:156)
	at org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:153)
	at org.apache.spark.sql.SparkSession.udf(SparkSession.scala:223)
	at test.org.apache.spark.sql.JavaUDFSuite.sourceTest(JavaUDFSuite.java:148)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:364)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeWithRerun(JUnit4Provider.java:272)
	at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:237)
	at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:158)
	at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:428)
	at org.apache.maven.surefire.booter.ForkedBooter.execute(ForkedBooter.java:162)
	at org.apache.maven.surefire.booter.ForkedBooter.run(ForkedBooter.java:562)
	at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:548)
Caused by: java.lang.NoClassDefFoundError: org.apache.hadoop.hdfs.web.WebHdfsFileSystem
	at java.lang.Class.getDeclaredConstructors0(Native Method)
	at java.lang.Class.privateGetDeclaredConstructors(Class.java:2671)
	at java.lang.Class.getConstructor0(Class.java:3075)
	at java.lang.Class.newInstance(Class.java:412)
	at java.util.ServiceLoader$LazyIterator.nextService(ServiceLoader.java:380)
	... 48 more
{code}
 ",,apachespark,dongjoon,LuciferYang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 08 04:41:13 UTC 2021,,,,,,,,,,"0|z0uop4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/21 15:30;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33926;;;","07/Sep/21 15:31;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33926;;;","08/Sep/21 04:41;dongjoon;Issue resolved by pull request 33926
[https://github.com/apache/spark/pull/33926];;;",,,,,,,,,,,,,
Fail to load Snappy codec,SPARK-36681,13399698,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,viirya,viirya,07/Sep/21 07:41,23/Jun/23 09:47,13/Jul/23 08:50,09/Mar/22 17:08,3.2.0,,,,,,,,3.3.0,,,,Spark Core,,,,,0,,,,,"snappy-java as a native library should not be relocated in Hadoop shaded client libraries. Currently we use Hadoop shaded client libraries in Spark. If trying to use SnappyCodec to write sequence file, we will encounter the following error:

{code}
[info]   Cause: java.lang.UnsatisfiedLinkError: org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Ljava/nio/ByteBuffer;IILjava/nio/ByteBuffer;I)I
[info]   at org.apache.hadoop.shaded.org.xerial.snappy.SnappyNative.rawCompress(Native Method)                                                                                                 
[info]   at org.apache.hadoop.shaded.org.xerial.snappy.Snappy.compress(Snappy.java:151)                                                                                                        
[info]   at org.apache.hadoop.io.compress.snappy.SnappyCompressor.compressDirectBuf(SnappyCompressor.java:282)
[info]   at org.apache.hadoop.io.compress.snappy.SnappyCompressor.compress(SnappyCompressor.java:210)
[info]   at org.apache.hadoop.io.compress.BlockCompressorStream.compress(BlockCompressorStream.java:149)
[info]   at org.apache.hadoop.io.compress.BlockCompressorStream.finish(BlockCompressorStream.java:142)
[info]   at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.writeBuffer(SequenceFile.java:1589) 
[info]   at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.sync(SequenceFile.java:1605)
[info]   at org.apache.hadoop.io.SequenceFile$BlockCompressWriter.close(SequenceFile.java:1629) 
{code}",,apachespark,csun,holden,icyjhl,joshrosen,koert,Qin Yao,smilegator,viirya,,,,,,,,,,,,,,,HADOOP-17891,,,,,,HADOOP-17125,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Oct 10 16:31:28 UTC 2022,,,,,,,,,,"0|z0uo4o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Sep/21 06:12;gurwls223;Is it 3.2 blocker?;;;","08/Sep/21 06:31;viirya;yea.;;;","09/Sep/21 02:09;viirya;After discussing with some PMC members, as this only affects sequence file (snappy codec), we will document it as known issue for 3.2 release. I will change it from blocker to normal bug.;;;","09/Sep/21 02:11;gurwls223;(y);;;","13/Sep/21 19:39;holden;Is there a known work around? Can we put the workaround in the release notes?;;;","13/Sep/21 20:39;viirya;The possible workaround is to use pure java implementation with snappy-java. So it doesn't try to load native library and we can avoid linked error.

Let me try it locally to verify and put it here as release notes.;;;","14/Sep/21 08:02;viirya;Unfortunately, I found some issues with the pure java implementation in snappy-java library. That's said even we switch to pure java mode, the Hadoop SnappyCodec still doesn't work for now.

I have the fix locally for the issues, but it needs to wait for snappy-java and Hadoop new releases. ;;;","24/Sep/21 04:42;smilegator;Can you link the related Jira in Apache Hadoop?;;;","19/Oct/21 04:11;koert;hadoop Jira issue:
https://issues.apache.org/jira/browse/HADOOP-17891

i have my doubt this only impacts sequence files. i am seeing this issue with snappy compressed csv files, snappy compress json files, etc.;;;","09/Mar/22 07:57;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/35784;;;","09/Mar/22 17:08;viirya;Issue resolved by pull request 35784
[https://github.com/apache/spark/pull/35784];;;","11/Apr/22 00:44;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36136;;;","11/Apr/22 00:44;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/36136;;;","26/May/22 11:53;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/36687;;;","10/Oct/22 12:38;icyjhl;Hi [~viirya], So this is only fixed in 3.3.0 and after?
Any workaround in 3.2?

Many Thanks!
;;;","10/Oct/22 16:31;viirya;This is fixed in 3.3.0 and later, yes, by upgrading to Hadoop 3.3.2.
As discussed above, there is no workaround in 3.2 for this issue.
If you are stick with 3.2, the only way is to upgrade to Hadoop 3.3.2 in Spark 3.2 source.
;;;"
NestedColumnAliasing pushes down aggregate functions into projections,SPARK-36677,13399616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,vicennial,vicennial,vicennial,06/Sep/21 16:45,08/Sep/21 01:19,13/Jul/23 08:50,08/Sep/21 01:19,3.2.0,3.3.0,,,,,,,3.2.0,,,,Optimizer,SQL,,,,0,,,,,"Aggregate functions are being pushed down into projections when nested columns are accessed causing the following error:
{code:java}
Caused by: UnsupportedOperationException: Cannot generate code for expression: ...{code}
Reproduction:

 
{code:java}
spark.sql(""drop table if exists test_aggregates"")
spark.sql(""create table if not exists test_aggregates(a STRUCT<c: STRUCT<e: string>, d: int>, b string)"")
val df = sql(""select max(a).c.e from (select a, b from test_aggregates) group by b"")
println(df.queryExecution.optimizedPlan)
{code}
 

The output of the above code:
{noformat}
'Aggregate [b#1], [_extract_e#5 AS max(a).c.e#3]
+- 'Project [max(a#0).c.e AS _extract_e#5, b#1]
   +- Relation default.test_aggregates[a#0,b#1] parquet
{noformat}
The error message when the dataframe is executed:
{noformat}
java.lang.UnsupportedOperationException: Cannot generate code for expression: max(input[0, struct<c:struct<e:string>,d:int>, true])
  at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotGenerateCodeForExpressionError(QueryExecutionErrors.scala:83)
  at org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode(Expression.scala:312)
  at org.apache.spark.sql.catalyst.expressions.Unevaluable.doGenCode$(Expression.scala:311)
  at org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression.doGenCode(interfaces.scala:99)
  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:151)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:146)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.nullSafeCodeGen(Expression.scala:525)
  at org.apache.spark.sql.catalyst.expressions.GetStructField.doGenCode(complexTypeExtractors.scala:126)
  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:151)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:146)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.nullSafeCodeGen(Expression.scala:525)
  at org.apache.spark.sql.catalyst.expressions.GetStructField.doGenCode(complexTypeExtractors.scala:126)
  at org.apache.spark.sql.catalyst.expressions.Expression.$anonfun$genCode$3(Expression.scala:151)
  at scala.Option.getOrElse(Option.scala:189)
  at org.apache.spark.sql.catalyst.expressions.Expression.genCode(Expression.scala:146)
  at org.apache.spark.sql.catalyst.expressions.Alias.genCode(namedExpressions.scala:171)
  at org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$2(basicPhysicalOperators.scala:73)
  at scala.collection.immutable.List.map(List.scala:293)
  at org.apache.spark.sql.execution.ProjectExec.$anonfun$doConsume$1(basicPhysicalOperators.scala:73)
  at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.withSubExprEliminationExprs(CodeGenerator.scala:1039)
  at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:73)
  at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:195)
  at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:150)
  at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:497)
  at org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:484)
  at org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:457)
  at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:497)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)
  at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:497)
  at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:54)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)
  at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:41)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:792)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:151)
  at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:96)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)
  at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:91)
  at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:91)
  at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:46)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:659)
  at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:722)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:184)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:180)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:135)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:135)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture$lzycompute(ShuffleExchangeExec.scala:140)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.mapOutputStatisticsFuture(ShuffleExchangeExec.scala:139)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.$anonfun$submitShuffleJob$1(ShuffleExchangeExec.scala:68)
  at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:222)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:219)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob(ShuffleExchangeExec.scala:68)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeLike.submitShuffleJob$(ShuffleExchangeExec.scala:67)
  at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.submitShuffleJob(ShuffleExchangeExec.scala:115)
  at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture$lzycompute(QueryStageExec.scala:170)
  at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.shuffleFuture(QueryStageExec.scala:170)
  at org.apache.spark.sql.execution.adaptive.ShuffleQueryStageExec.doMaterialize(QueryStageExec.scala:172)
  at org.apache.spark.sql.execution.adaptive.QueryStageExec.materialize(QueryStageExec.scala:82)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5(AdaptiveSparkPlanExec.scala:256)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$5$adapted(AdaptiveSparkPlanExec.scala:254)
  at scala.collection.Iterator.foreach(Iterator.scala:943)
  at scala.collection.Iterator.foreach$(Iterator.scala:943)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
  at scala.collection.IterableLike.foreach(IterableLike.scala:74)
  at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
  at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:254)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:226)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:365)
  at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:338)
  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3742)
  at org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:2998)
  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3733)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3731)
  at org.apache.spark.sql.Dataset.collect(Dataset.scala:2998)
  ... 47 elided
{noformat}
 ",,apachespark,vicennial,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 08 01:19:58 UTC 2021,,,,,,,,,,"0|z0unmg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/21 16:46;vicennial;I have a fix for this and have a PR on the way.;;;","06/Sep/21 18:16;apachespark;User 'vicennial' has created a pull request for this issue:
https://github.com/apache/spark/pull/33921;;;","06/Sep/21 18:16;apachespark;User 'vicennial' has created a pull request for this issue:
https://github.com/apache/spark/pull/33921;;;","08/Sep/21 01:19;viirya;Issue resolved by pull request 33921
[https://github.com/apache/spark/pull/33921];;;",,,,,,,,,,,,
Incorrect Unions of struct with mismatched field name case,SPARK-36673,13399539,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,viirya,shardulm,shardulm,06/Sep/21 08:47,17/Sep/21 16:08,13/Jul/23 08:50,17/Sep/21 13:38,3.1.1,3.2.0,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"If a nested field has different casing on two sides of the union, the resultant schema of the union will both fields in its schemaa
{code:java}
scala> val df1 = spark.range(2).withColumn(""nested"", struct(expr(""id * 5 AS INNER"")))
df1: org.apache.spark.sql.DataFrame = [id: bigint, nested: struct<INNER: bigint>]

val df2 = spark.range(2).withColumn(""nested"", struct(expr(""id * 5 AS inner"")))
df2: org.apache.spark.sql.DataFrame = [id: bigint, nested: struct<inner: bigint>]

scala> df1.union(df2).printSchema
root
 |-- id: long (nullable = false)
 |-- nested: struct (nullable = false)
 |    |-- INNER: long (nullable = false)
 |    |-- inner: long (nullable = false)
 {code}
This seems like a bug. I would expect that Spark SQL would either just union by index or if the user has requested {{unionByName}}, then it should matched fields case insensitively if {{spark.sql.caseSensitive}} is {{false}}.

However the output data only has one nested column
{code:java}
scala> df1.union(df2).show()
+---+------+
| id|nested|
+---+------+
|  0|   {0}|
|  1|   {5}|
|  0|   {0}|
|  1|   {5}|
+---+------+
{code}
Trying to project fields of {{nested}} throws an error:
{code:java}
scala> df1.union(df2).select(""nested.*"").show()
java.lang.ArrayIndexOutOfBoundsException: 1
  at org.apache.spark.sql.types.StructType.apply(StructType.scala:414)
  at org.apache.spark.sql.catalyst.expressions.GetStructField.dataType(complexTypeExtractors.scala:108)
  at org.apache.spark.sql.catalyst.expressions.Alias.toAttribute(namedExpressions.scala:192)
  at org.apache.spark.sql.catalyst.plans.logical.Project.$anonfun$output$1(basicLogicalOperators.scala:63)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.plans.logical.Project.output(basicLogicalOperators.scala:63)
  at org.apache.spark.sql.catalyst.plans.logical.Union.$anonfun$output$3(basicLogicalOperators.scala:260)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.catalyst.plans.logical.Union.output(basicLogicalOperators.scala:260)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.outputSet$lzycompute(QueryPlan.scala:49)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.outputSet(QueryPlan.scala:49)
  at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$$anonfun$apply$8.applyOrElse(Optimizer.scala:747)
  at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$$anonfun$apply$8.applyOrElse(Optimizer.scala:695)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:316)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:316)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:406)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:242)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:404)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:357)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:406)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:242)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:404)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:357)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:305)
  at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$.apply(Optimizer.scala:695)
  at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$.apply(Optimizer.scala:693)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  at scala.collection.immutable.List.foldLeft(List.scala:89)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:212)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:204)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:204)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:88)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:144)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:771)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:144)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:85)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:85)
  at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:96)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:114)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$simpleString$2(QueryExecution.scala:162)
  at org.apache.spark.sql.execution.ExplainUtils$.processPlan(ExplainUtils.scala:115)
  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:162)
  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:207)
  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:176)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:771)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3703)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2740)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2947)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:340)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:827)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:786)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:795)
  ... 47 elided
{code}

This behaviour was introduced in SPARK-26812.",,apachespark,cloud_fan,mgaido,shardulm,viirya,xkrogen,,,,,,,,,,,,,,,,,,,,,SPARK-26812,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 17 16:08:59 UTC 2021,,,,,,,,,,"0|z0un5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/21 08:53;shardulm;[~mgaido] [~cloud_fan] Since you guys were involved in the original PR for SPARK-26812, do you have thoughts on what the right behavior is here?;;;","06/Sep/21 09:14;mgaido;AFAIK, in SQL the names in the struct are case sensitive, while the name of the normal fields are not. I am not sure about the right behavior here, but maybe I would expect an error at analysis time. Definitely, the current behavior is not correct.;;;","09/Sep/21 20:55;xkrogen;From the Scaladoc for {{union}}:
{code}
   * Also as standard in SQL, this function resolves columns by position (not by name):
{code}
So it seems to me we should be ignoring the names altogether and just doing positional matching.

{quote}
AFAIK, in SQL the names in the struct are case sensitive, while the name of the normal fields are not.
{quote}
I was under the impression that nested fields are also case-insensitive. Do we have any documentation around this?

In any case, when we fix this, we need to be careful about {{unionByName}}, including the new-ish {{allowMissingColumns}} option...;;;","16/Sep/21 04:28;cloud_fan;This seems like a bug. [~viirya] what do you think?;;;","16/Sep/21 06:53;viirya;The schema after union looks incorrect. By the definition of `union`, there should be only one column in the nested struct.;;;","16/Sep/21 19:47;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/34025;;;","17/Sep/21 13:38;cloud_fan;Issue resolved by pull request 34025
[https://github.com/apache/spark/pull/34025];;;","17/Sep/21 16:08;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/34032;;;",,,,,,,,
Fail to load Lz4 codec,SPARK-36669,13399417,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,viirya,viirya,viirya,04/Sep/21 21:45,22/Sep/21 00:28,13/Jul/23 08:50,09/Sep/21 16:31,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"Currently we use Hadop 3.3.1's shaded client libraries. Lz4 is a provided dependency in Hadoop Common 3.3.1 for Lz4Codec. But it isn't excluded from relocation in these libraries. So to use lz4 as Parquet codec, we will hit the exception even we include lz4 as dependency.
{code:java}
[info]   Cause: java.lang.NoClassDefFoundError: org/apache/hadoop/shaded/net/jpountz/lz4/LZ4Factory                                                                                            
[info]   at org.apache.hadoop.io.compress.lz4.Lz4Compressor.<init>(Lz4Compressor.java:66)
[info]   at org.apache.hadoop.io.compress.Lz4Codec.createCompressor(Lz4Codec.java:119)                                                                                                         
[info]   at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:152)                                                                                                          
[info]   at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:168)                                                                                                          
 {code}
 

I already submitted a PR to Hadoop to fix it. Before it is released, at Spark side, we either downgrade to 3.3.0 or revert back to non-shaded hadoop client library.

 ",,apachespark,petertoth,viirya,,,,,,,,,,,,,,,,,,,,,,,,HADOOP-17891,,,SPARK-36679,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 09 16:31:35 UTC 2021,,,,,,,,,,"0|z0ume8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/21 22:11;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33913;;;","04/Sep/21 22:12;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33913;;;","06/Sep/21 08:41;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33912;;;","09/Sep/21 02:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33940;;;","09/Sep/21 02:42;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33940;;;","09/Sep/21 16:31;viirya;Issue resolved by pull request 33940
[https://github.com/apache/spark/pull/33940];;;",,,,,,,,,,
Close resources properly in StateStoreSuite/RocksDBStateStoreSuite,SPARK-36667,13399353,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kabhwan,kabhwan,kabhwan,04/Sep/21 03:14,07/Sep/21 00:40,13/Jul/23 08:50,07/Sep/21 00:40,3.2.0,,,,,,,,3.2.0,,,,Structured Streaming,,,,,0,,,,,"The StateStoreProvider instances created from ""newStoreProvider"" are NOT automatically closed.

While this is trivial for HDFSBackedStateStoreProvider, for RocksDBStateStoreProvider we leak RocksDB instance as well which should have closed. Most tests in the RocksDBStateStoreSuite initialize RocksDBStateStoreProvider, meaning that 60+ RocksDB instances are not closed in the suite.
",,apachespark,kabhwan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Sep 07 00:40:37 UTC 2021,,,,,,,,,,"0|z0um00:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Sep/21 03:15;kabhwan;Will submit a PR soon.;;;","06/Sep/21 02:13;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/33916;;;","07/Sep/21 00:40;viirya;Issue resolved by pull request 33916
[https://github.com/apache/spark/pull/33916];;;",,,,,,,,,,,,,
SQL sequence function with interval returns unexpected error in latest versions,SPARK-36639,13398658,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,amstel8,amstel8,01/Sep/21 12:17,06/Sep/21 20:23,13/Jul/23 08:50,03/Sep/21 14:47,3.0.3,3.1.2,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,"For example this returns {color:#FF0000}java.lang.ArrayIndexOutOfBoundsException: 1 {color}
{code:java}
select sequence(
 date_trunc('month', '2021-08-30'),
 date_trunc('month', '2021-08-15'),
 - interval 1 month){code}
Another cases like - all ok
{code:java}
select sequence(
 date_trunc('month', '2021-07-15'),
 date_trunc('month', '2021-08-30'),
 interval 1 month) as x

 , sequence(
 date_trunc('month', '2021-08-30'),
 date_trunc('month', '2021-07-15'),
 - interval 1 month) as y

 , sequence(
 date_trunc('month', '2021-08-15'),
 date_trunc('month', '2021-08-30'),
 interval 1 month) as z{code}

In version 3.0.0 this works",,amstel8,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Sep 03 17:19:37 UTC 2021,,,,,,,,,,"0|z0uhps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/21 16:25;sarutak;[~amstel8] Thank you for reporting. SPARK-31980 seems to bring this issue.
BTW, due to the change, the following code works with 3.1.2 but doesn't with 3.0.0.

{code}
select sequence(
 date_trunc('month', '2021-08-15'),
 date_trunc('month', '2021-08-30'),
 interval 1 month) as z
{code}

Let me investigate more.;;;","01/Sep/21 19:17;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33895;;;","03/Sep/21 14:48;sarutak;Issue resolved in https://github.com/apache/spark/pull/33895 for 3.1 and 3.2.;;;","03/Sep/21 17:18;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33909;;;","03/Sep/21 17:19;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33909;;;",,,,,,,,,,,
Tasks with Java proxy objects fail to deserialize,SPARK-36627,13398514,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,ssouza,ssouza,ssouza,31/Aug/21 17:05,28/Oct/21 23:15,13/Jul/23 08:50,28/Oct/21 23:15,3.0.3,,,,,,,,3.3.0,,,,Spark Core,,,,,0,,,,,"In JavaSerializer.JavaDeserializationStream we override resolveClass of ObjectInputStream to use the threads' contextClassLoader. However, we do not override resolveProxyClass, which is used when deserializing Java proxy objects, which makes spark use the wrong classloader when deserializing objects, which causes the job to fail with the following exception:

{code}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, <host>, executor 1): java.lang.ClassNotFoundException: <class>
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)
	at java.base/java.lang.Class.forName0(Native Method)
	at java.base/java.lang.Class.forName(Class.java:398)
	at java.base/java.io.ObjectInputStream.resolveProxyClass(ObjectInputStream.java:829)
	at java.base/java.io.ObjectInputStream.readProxyDesc(ObjectInputStream.java:1917)
	...
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)
{code}",,apachespark,ssouza,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 28 23:15:55 UTC 2021,,,,,,,,,,"0|z0ugts:",9223372036854775807,,,,,rshkv,,,,,,,,,,,,,,,,,,"31/Aug/21 17:14;apachespark;User 'fsamuel-bs' has created a pull request for this issue:
https://github.com/apache/spark/pull/33879;;;","31/Aug/21 17:14;apachespark;User 'fsamuel-bs' has created a pull request for this issue:
https://github.com/apache/spark/pull/33879;;;","28/Oct/21 23:15;srowen;Issue resolved by pull request 33879
[https://github.com/apache/spark/pull/33879];;;",,,,,,,,,,,,,
HDFSBackedStateStore and RocksDBStateStore have bugs on prefix scan,SPARK-36619,13398344,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,kabhwan,kabhwan,31/Aug/21 02:46,31/Aug/21 16:52,13/Jul/23 08:50,31/Aug/21 16:52,3.2.0,,,,,,,,3.2.0,,,,Structured Streaming,,,,,0,,,,,"In RocksDB state store provider implementation, we leverage iterators on prefix scan, which are being closed in rollback() method.

While this works now for session window since state store instance in read physical plan will always call abort, it could bring correctness issue for stateful operator which doesn't instantiate two different physical plans on read and write.

We should make sure these iterators get closed to let these iterators don't affect multiple micro-batches (plays as side-effect).

In HDFSBackedStateStore, we copy both data map and prefix key map on creating new map for writing, but we copied prefix key map as it is, which does shallow copy on value side (the type of value is Set), which is broken on reloading aborted version.",,apachespark,Gengliang.Wang,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 31 16:52:20 UTC 2021,,,,,,,,,,"0|z0ufs0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/21 03:41;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/33870;;;","31/Aug/21 16:52;Gengliang.Wang;Issue resolved by pull request 33870
[https://github.com/apache/spark/pull/33870];;;",,,,,,,,,,,,,,
Upgrade Jackson to 2.12.5,SPARK-36605,13397943,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,28/Aug/21 08:15,28/Aug/21 22:57,13/Jul/23 08:50,28/Aug/21 22:57,3.2.0,,,,,,,,3.3.0,,,,Build,,,,,0,,,,,"Recently, Jackson 2.12.5 was released and it seems to be expected as the last full patch release for 2.12.x.
This release includes a fix for a regression in jackson-databind introduced in 2.12.3 which Spark 3.2 currently depends on.
https://github.com/FasterXML/jackson/wiki/Jackson-Release-2.12.5",,apachespark,sarutak,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 28 22:57:44 UTC 2021,,,,,,,,,,"0|z0udaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/21 08:20;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33860;;;","28/Aug/21 08:21;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33860;;;","28/Aug/21 22:57;viirya;Issue resolved by pull request 33860
[https://github.com/apache/spark/pull/33860];;;",,,,,,,,,,,,,
Use WeakReference over SoftReference in LevelDB to avoid retaining iterator refs,SPARK-36603,13397925,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,srowen,srowen,srowen,28/Aug/21 01:09,05/Sep/21 22:11,13/Jul/23 08:50,05/Sep/21 22:11,3.1.2,,,,,,,,3.1.3,3.2.0,,,Spark Core,,,,,0,,,,,"See discussion at https://github.com/apache/spark/pull/28769#issuecomment-906722390

The SoftReference used to track iterators in LevelDB can prevent them from being GCed early enough, thus finalized and closed. A WeakReference would be more readily collected, and solves an apparent actual issue running a 3.1.x SHS.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,SPARK-31929,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Sep 05 22:11:34 UTC 2021,,,,,,,,,,"0|z0ud6w:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"28/Aug/21 01:15;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33859;;;","28/Aug/21 01:16;apachespark;User 'srowen' has created a pull request for this issue:
https://github.com/apache/spark/pull/33859;;;","05/Sep/21 22:11;sarutak;Issue resolved in https://github.com/apache/spark/pull/33859;;;",,,,,,,,,,,,,
ORC vectorized reader should properly check maximal number of fields,SPARK-36594,13397403,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengsu,chengsu,chengsu,26/Aug/21 03:55,26/Aug/21 06:55,13/Jul/23 08:50,26/Aug/21 06:55,3.2.0,3.3.0,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,Debugged internally and found a bug where we should disable vectorized reader now based on schema recursively. Currently we check `schema.length` to be no more than `wholeStageMaxNumFields` to enable vectorization. `schema.length` does not take nested columns sub-fields into condition (i.e. view nested column same as primitive column). This check will be wrong when enabling vectorization for nested columns. We should follow same check from `WholeStageCodegenExec` to check sub-fields recursively. This will not cause correctness issue but will cause performance issue where we may enable vectorization for nested columns by mistake when nested column has a lot of sub-fields.,,apachespark,chengsu,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 26 06:55:45 UTC 2021,,,,,,,,,,"0|z0u9z4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/21 04:00;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/33842;;;","26/Aug/21 04:00;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/33842;;;","26/Aug/21 04:48;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/33843;;;","26/Aug/21 04:49;apachespark;User 'c21' has created a pull request for this issue:
https://github.com/apache/spark/pull/33843;;;","26/Aug/21 06:55;cloud_fan;Issue resolved by pull request 33843
[https://github.com/apache/spark/pull/33843];;;",,,,,,,,,,,
Special timestamp_ntz value should be converted in the session tz,SPARK-36590,13397303,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,maxgekk,maxgekk,maxgekk,25/Aug/21 16:19,26/Aug/21 02:23,13/Jul/23 08:50,26/Aug/21 02:23,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"Currently, special timestamp_ntz strings like *today*, *tomorrow*, *yesterday* are converted to timestamp_ntz values using JVM timezone which is incorrect. The conversion should base on the session time zone. The examples below demonstrate the problem:

{code:sql}
$ export TZ=""Europe/Amsterdam""
$ ./bin/spark-sql -S
spark-sql> select timestamp_ntz'now';
2021-08-25 18:12:36.233

spark-sql> set spark.sql.session.timeZone=America/Los_Angeles;
spark.sql.session.timeZone	America/Los_Angeles
spark-sql> select timestamp_ntz'now';
2021-08-25 18:14:40.547
{code}
The special timestamp_ntz strings are converted in ""Europe/Amsterdam"" tz, and the converter doesn't respect Spark settings.",,apachespark,cloud_fan,maxgekk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 26 02:23:50 UTC 2021,,,,,,,,,,"0|z0u9d4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/21 16:51;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/33838;;;","25/Aug/21 16:52;apachespark;User 'MaxGekk' has created a pull request for this issue:
https://github.com/apache/spark/pull/33838;;;","26/Aug/21 02:23;cloud_fan;Issue resolved by pull request 33838
[https://github.com/apache/spark/pull/33838];;;",,,,,,,,,,,,,
"Support setting ""since"" version in FunctionRegistry",SPARK-36585,13397171,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Gengliang.Wang,Gengliang.Wang,Gengliang.Wang,25/Aug/21 12:24,15/Nov/21 18:26,13/Jul/23 08:50,25/Aug/21 14:35,3.2.0,,,,,,,,3.2.0,,,,Documentation,SQL,,,,0,,,,,"Spark 3.2.0 includes two new functions ""regexp"" and ""regexp_like"", which are identical to ""rlike"". However, in the generated documentation. the since versions of both functions are ""1.0.0"" since they are based on the expression ""RLike"".
https://dist.apache.org/repos/dist/dev/spark/v3.2.0-rc1-docs/_site/api/sql/index.html#regexp
https://dist.apache.org/repos/dist/dev/spark/v3.2.0-rc1-docs/_site/api/sql/index.html#regexp_like

FunctionRegistry should support setting ""since"" version to make sure the generated documentation is correct.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 25 14:35:22 UTC 2021,,,,,,,,,,"0|z0u8js:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"25/Aug/21 12:40;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33834;;;","25/Aug/21 12:41;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33834;;;","25/Aug/21 14:35;Gengliang.Wang;The issue is resolved in https://github.com/apache/spark/pull/33834;;;",,,,,,,,,,,,,
Missed broadcast join in V2 plan,SPARK-36568,13396784,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,petertoth,bersprockets,bersprockets,23/Aug/21 23:19,26/Aug/21 03:27,13/Jul/23 08:50,26/Aug/21 03:27,3.2.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"There are some joins that use broadcast hash join with DataSourceV1 but sort merge join with DataSourceV2, even though the two joins are loading the same files [1].

Example:

Create data:
{noformat}
import scala.util.Random

val rand = new Random(245665L)

val df = spark.range(1, 20000).map { x =>
  (x,
   rand.alphanumeric.take(20).mkString,
   rand.alphanumeric.take(20).mkString,
   rand.alphanumeric.take(20).mkString
  )
}.toDF(""key"", ""col1"", ""col2"", ""col3"")

df.write.mode(""overwrite"").format(""parquet"").save(""/tmp/tbl"")
df.write.mode(""overwrite"").format(""parquet"").save(""/tmp/lookup"")
{noformat}
Run this code:
{noformat}
bin/spark-shell --conf spark.sql.autoBroadcastJoinThreshold=400000

spark.read.format(""parquet"").load(""/tmp/tbl"").createOrReplaceTempView(""tbl"")
spark.read.format(""parquet"").load(""/tmp/lookup"").createOrReplaceTempView(""lookup"")

sql(""""""select t.key, t.col1, t.col2, t.col3
from tbl t
join lookup l
on t.key = l.key"""""").explain
{noformat}
For V2, do the same, except set {{spark.sql.sources.useV1SourceList=""""}}.

For V1, the result is:
{noformat}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [key#0L, col1#1, col2#2, col3#3]
   +- BroadcastHashJoin [key#0L], [key#8L], Inner, BuildRight, false
      :- Filter isnotnull(key#0L)
      :  +- FileScan parquet [key#0L,col1#1,col2#2,col3#3] Batched: true, DataFilters: [isnotnull(key#0L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/tbl], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,col1:string,col2:string,col3:string>
      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [id=#32]
         +- Filter isnotnull(key#8L)
            +- FileScan parquet [key#8L] Batched: true, DataFilters: [isnotnull(key#8L)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/lookup], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint>
{noformat}
For V2, the result is:
{noformat}
== Physical Plan ==
AdaptiveSparkPlan isFinalPlan=false
+- Project [key#0L, col1#1, col2#2, col3#3]
   +- SortMergeJoin [key#0L], [key#8L], Inner
      :- Sort [key#0L ASC NULLS FIRST], false, 0
      :  +- Exchange hashpartitioning(key#0L, 200), ENSURE_REQUIREMENTS, [id=#33]
      :     +- Filter isnotnull(key#0L)
      :        +- BatchScan[key#0L, col1#1, col2#2, col3#3] ParquetScan DataFilters: [isnotnull(key#0L)], Format: parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/tbl], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint,col1:string,col2:string,col3:string>, PushedFilters: [IsNotNull(key)] RuntimeFilters: []
      +- Sort [key#8L ASC NULLS FIRST], false, 0
         +- Exchange hashpartitioning(key#8L, 200), ENSURE_REQUIREMENTS, [id=#34]
            +- Filter isnotnull(key#8L)
               +- BatchScan[key#8L] ParquetScan DataFilters: [isnotnull(key#8L)], Format: parquet, Location: InMemoryFileIndex(1 paths)[file:/tmp/lookup], PartitionFilters: [], PushedFilters: [IsNotNull(key)], ReadSchema: struct<key:bigint>, PushedFilters: [IsNotNull(key)] RuntimeFilters: []
{noformat}
The initial plan with V1 uses broadcast hash join, but the initial plan with V2 uses sort merge join.

The V1 logical plan contains a projection over the relation for {{lookup}}, which restricts the output columns to just {{key}}. As a result, {{SizeInBytesOnlyStatsPlanVisitor#visitUnaryNode}}, when visiting the project node, reduces sizeInBytes based on the pruning:
{noformat}
Project [key#0L, col1#1, col2#2, col3#3]
+- Join Inner, (key#0L = key#8L)
   :- Filter isnotnull(key#0L)
   :  +- Relation [key#0L,col1#1,col2#2,col3#3] parquet
   +- Project [key#8L]
      +- Filter isnotnull(key#8L)
         +- Relation [key#8L,col1#9,col2#10,col3#11] parquet
{noformat}
The V2 logical plan does not contain this projection:
{noformat}
+- Join Inner, (key#0L = key#8L)
   :- Filter isnotnull(key#0L)
   :  +- RelationV2[key#0L, col1#1, col2#2, col3#3] parquet file:/tmp/tbl
   +- Filter isnotnull(key#8L)
      +- RelationV2[key#8L] parquet file:/tmp/lookup
{noformat}
[1] With my example, AQE converts the join to a broadcast hash join at run time for the V2 case. However, if AQE was disabled, it would obviously remain a sort merge join.",,apachespark,bersprockets,cloud_fan,KevinPis,petertoth,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 26 03:27:08 UTC 2021,,,,,,,,,,"0|z0u65s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/21 15:11;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/33825;;;","24/Aug/21 15:12;apachespark;User 'peter-toth' has created a pull request for this issue:
https://github.com/apache/spark/pull/33825;;;","26/Aug/21 03:27;cloud_fan;Issue resolved by pull request 33825
[https://github.com/apache/spark/pull/33825];;;",,,,,,,,,,,,,
LiveRDDDistribution.toApi throws NullPointerException,SPARK-36564,13396700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,23/Aug/21 13:10,24/Aug/21 20:34,13/Jul/23 08:50,24/Aug/21 20:34,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.0.4,3.1.3,3.2.0,,Spark Core,,,,,0,,,,,"{code:java}
21/08/23 12:26:29 ERROR AsyncEventQueue: Listener AppStatusListener threw an exception
java.lang.NullPointerException
	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:192)
	at com.google.common.collect.MapMakerInternalMap.putIfAbsent(MapMakerInternalMap.java:3507)
	at com.google.common.collect.Interners$WeakInterner.intern(Interners.java:85)
	at org.apache.spark.status.LiveEntityHelpers$.weakIntern(LiveEntity.scala:696)
	at org.apache.spark.status.LiveRDDDistribution.toApi(LiveEntity.scala:563)
	at org.apache.spark.status.LiveRDD.$anonfun$doUpdate$4(LiveEntity.scala:629)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.mutable.HashMap$$anon$2.$anonfun$foreach$3(HashMap.scala:158)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
	at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:158)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.spark.status.LiveRDD.doUpdate(LiveEntity.scala:629)
	at org.apache.spark.status.LiveEntity.write(LiveEntity.scala:51)
	at org.apache.spark.status.AppStatusListener.update(AppStatusListener.scala:1206)
	at org.apache.spark.status.AppStatusListener.maybeUpdate(AppStatusListener.scala:1212)
	at org.apache.spark.status.AppStatusListener.$anonfun$onExecutorMetricsUpdate$6(AppStatusListener.scala:956)
	at org.apache.spark.status.AppStatusListener.$anonfun$onExecutorMetricsUpdate$6$adapted(AppStatusListener.scala:956)
	at scala.collection.mutable.HashMap$$anon$2.$anonfun$foreach$3(HashMap.scala:158)
	at scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)
	at scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)
	at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:158)
	at org.apache.spark.status.AppStatusListener.flush(AppStatusListener.scala:1015)
	at org.apache.spark.status.AppStatusListener.onExecutorMetricsUpdate(AppStatusListener.scala:956)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:59)
	at org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)
	at org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:119)
	at org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:103)
	at org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1585)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)
{code}",,apachespark,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 24 20:34:45 UTC 2021,,,,,,,,,,"0|z0u5n4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/21 13:35;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33812;;;","23/Aug/21 13:35;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33812;;;","24/Aug/21 20:34;dongjoon;Issue resolved by pull request 33812
[https://github.com/apache/spark/pull/33812];;;",,,,,,,,,,,,,
Error message while trying to use spark sql functions directly on dataframe columns without using select expression,SPARK-36554,13396511,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,nicolasazrak,lekshmiii,lekshmiii,22/Aug/21 02:29,12/Dec/22 18:10,13/Jul/23 08:50,02/Nov/21 01:53,3.1.1,,,,,,,,3.3.0,,,,Documentation,Examples,PySpark,,,0,documentation,features,functions,spark-sql,"The below code generates a dataframe successfully . Here make_date function is used inside a select expression

 

from pyspark.sql.functions import  expr, make_date

df = spark.createDataFrame([(2020, 6, 26), (1000, 2, 29), (-44, 1, 1)],['Y', 'M', 'D'])

df.select(""*"",expr(""make_date(Y,M,D) as lk"")).show()

 

The below code fails with a message ""cannot import name 'make_date' from 'pyspark.sql.functions'"" . Here the make_date function is directly called on dataframe columns without select expression

 

from pyspark.sql.functions import make_date

df = spark.createDataFrame([(2020, 6, 26), (1000, 2, 29), (-44, 1, 1)],['Y', 'M', 'D'])

df.select(make_date(df.Y,df.M,df.D).alias(""datefield"")).show()

 
The error message generated is misleading when it says ""cannot  import make_date from pyspark.sql.functions""

 ",,apachespark,lekshmiii,nicolasazrak,sarutak,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,"22/Aug/21 02:31;lekshmiii;Screen Shot .png;https://issues.apache.org/jira/secure/attachment/13032268/Screen+Shot+.png",,,1.0,,,,,,,,,,,,,,,,,,,,,,false,SPARK-38783,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,Tue Nov 02 01:53:44 UTC 2021,,,,,,,,,,"0|z0u4h4:",9223372036854775807,,,,,lekshmiii,,,,,,,,,,,,,,,,,,"23/Aug/21 01:20;gurwls223;{{make_date}} is simply missing under {{functions.py}}. {{functions.py}} only contains commonly used functions.;;;","21/Oct/21 13:20;nicolasazrak;Hi [~lekshmiii] [~hyukjin.kwon] I've created [https://github.com/apache/spark/pull/34356] to fix this. It's my first PR, can I get a review?;;;","21/Oct/21 13:39;apachespark;User 'nicolasazrak' has created a pull request for this issue:
https://github.com/apache/spark/pull/34356;;;","21/Oct/21 13:48;lekshmiii;from pyspark.sql.functions import make_date

df = spark.createDataFrame([(2020, 6, 26), (1000, 2, 29), (-44, 1, 1)],['Y', 'M', 'D'])

df.select(make_date(df.Y,df.M,df.D).alias(""datefield"")).show()


[~nicolasazrak] so as part of the change will this code work without error message ?;;;","21/Oct/21 13:57;nicolasazrak;[~lekshmiii] yes, that will work without errors;;;","21/Oct/21 17:40;lekshmiii;[~nicolasazrak] what is the fix version ? Will it work on all versions from 3.1.1 onwards?;;;","21/Oct/21 17:55;nicolasazrak;Depends on when it gets merged. I don't know about the spark release strategy and which fixes are cherry-picked. That should be decided by an admin;;;","21/Oct/21 20:59;lekshmiii;@Nicolas Azrak  So how do I test if it is working ?;;;","22/Oct/21 20:12;nicolasazrak;[~lekshmiii] I've added a test to validate this is working. If you are using spark in a project and need this fix you would have to compile it using the patch I've submitted in the PR. ;;;","02/Nov/21 01:53;sarutak;Issue resolved in https://github.com/apache/spark/pull/34356;;;",,,,,,
KMeans fails with NegativeArraySizeException for K = 50000 after issue #27758 was introduced,SPARK-36553,13396348,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,podongfeng,anders.rydbirk,anders.rydbirk,20/Aug/21 11:49,02/Mar/22 19:55,13/Jul/23 08:50,02/Mar/22 19:55,3.1.1,,,,,,,,3.1.3,3.2.2,3.3.0,,ML,MLlib,PySpark,,,1,,,,,"We are running KMeans on approximately 350M rows of x, y, z coordinates using the following configuration:
{code:java}
KMeans(
  featuresCol='features',
  predictionCol='centroid_id',
  k=50000,
  initMode='k-means||',
  initSteps=2,
  tol=0.00005,
  maxIter=20,
  seed=SEED,
  distanceMeasure='euclidean'
)
{code}
When using Spark 3.0.0 this worked fine, but  when upgrading to 3.1.1 we are consistently getting errors unless we reduce K.

Stacktrace:

 
{code:java}
An error occurred while calling o167.fit.An error occurred while calling o167.fit.: java.lang.NegativeArraySizeException: -897458648 at scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:194) at scala.reflect.ManifestFactory$DoubleManifest.newArray(Manifest.scala:191) at scala.Array$.ofDim(Array.scala:221) at org.apache.spark.mllib.clustering.DistanceMeasure.computeStatistics(DistanceMeasure.scala:52) at org.apache.spark.mllib.clustering.KMeans.runAlgorithmWithWeight(KMeans.scala:280) at org.apache.spark.mllib.clustering.KMeans.runWithWeight(KMeans.scala:231) at org.apache.spark.ml.clustering.KMeans.$anonfun$fit$1(KMeans.scala:354) at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191) at scala.util.Try$.apply(Try.scala:213) at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191) at org.apache.spark.ml.clustering.KMeans.fit(KMeans.scala:329) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) at java.base/java.lang.reflect.Method.invoke(Unknown Source) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.base/java.lang.Thread.run(Unknown Source)
{code}
 

The issue is introduced by [#27758|#diff-725d4624ddf4db9cc51721c2ddaef50a1bc30e7b471e0439da28c5b5582efdfdR52]] which significantly reduces the maximum value of K. Snippit of line that throws error from [DistanceMeasure.scala:|#L52]]
{code:java}
val packedValues = Array.ofDim[Double](k * (k + 1) / 2)
{code}
 

*What we have tried:*
 * Reducing iterations
 * Reducing input volume
 * Reducing K

Only reducing K have yielded success.

 

*Possible workaround:*
 # Roll back to Spark 3.0.0 since a KMeansModel generated with 3.0.0 cannot be loaded in 3.1.1.
 # Reduce K. Currently trying with 45000.

 

*What we don't understand*:

Given the line of code above, we do not understand why we would get an integer overflow.

For K=50,000, packedValues should be allocated with the size of 1,250,025,000 < (2^31) and not result in a negative array size.

 

*Suggested resolution:*

I'm not strong in the inner workings on KMeans, but my immediate thought would be to add a fallback to previous logic for K larger than a set threshold if the optimisation is to stay in place, as it breaks compatibility from 3.0.0 to 3.1.1 for edge cases.

 

Please let me know if more information is needed, this is my first time raising a bug for a OS.",,anders.rydbirk,apachespark,podongfeng,,,,,,,,,,,,,,,,,,,,,,,,SPARK-31007,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Feb 09 06:39:37 UTC 2022,,,,,,,,,,"0|z0u3gw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/22 06:33;podongfeng;it is a overflow:

 
{code:java}
scala> val k = 50000
val k: Int = 50000

scala> k * (k + 1) / 2
val res1: Int = -897458648
 {code};;;","09/Feb/22 06:39;apachespark;User 'zhengruifeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/35457;;;",,,,,,,,,,,,,,
varchar datatype behave differently on  hive table  and datasource table,SPARK-36552,13396326,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,oceaneast,oceaneast,20/Aug/21 09:47,12/Dec/22 18:10,13/Jul/23 08:50,22/Aug/21 00:40,2.3.1,3.1.1,3.1.2,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"in spark 3.1.X, when set spark.sql.hive.convertMetastoreOrc=false,and 

spark.sql.legacy.charVarcharAsString=true.

Execute the following sql:

CREATE TABLE t (col varchar(2)) stored as orc;
INSERT INTO t SELECT 'aaa';
select * from t;

result is aa

 

But when set spark.sql.hive.convertMetastoreOrc=true,and 

spark.sql.legacy.charVarcharAsString=true

alse execute the sql, the result is ""aaa""

 

 

 

 ",,apachespark,KevinPis,oceaneast,Qin Yao,Resol1992,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Aug 22 00:40:38 UTC 2021,,,,,,,,,,"0|z0u3c0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/21 13:46;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33798;;;","22/Aug/21 00:40;gurwls223;Issue resolved by pull request 33798
[https://github.com/apache/spark/pull/33798];;;",,,,,,,,,,,,,,
Add sphinx-plotly-directive in Spark release Dockerfile,SPARK-36551,13396323,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,Gengliang.Wang,Gengliang.Wang,20/Aug/21 09:22,20/Aug/21 12:03,13/Jul/23 08:50,20/Aug/21 12:03,3.2.0,,,,,,,,3.2.0,,,,Build,,,,,0,,,,,"After https://github.com/apache/spark/pull/32726, Python doc build requires sphinx-plotly-directive.
We should install it from spark-rm/Dockerfile to make sure do-release-docker.sh can run successfully. 
Also, we should mention it in the README of docs.",,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 20 12:03:06 UTC 2021,,,,,,,,,,"0|z0u3bc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/21 09:26;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33797;;;","20/Aug/21 09:26;apachespark;User 'gengliangwang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33797;;;","20/Aug/21 12:03;Gengliang.Wang;Issue resolved by pull request 33797
[https://github.com/apache/spark/pull/33797];;;",,,,,,,,,,,,,
Deadlock in CoarseGrainedExecutorBackend.onDisconnected,SPARK-36532,13395616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,17/Aug/21 06:24,05/Dec/21 07:21,13/Jul/23 08:50,18/Aug/21 14:47,3.0.0,3.1.0,3.2.0,3.3.0,,,,,3.0.4,3.1.3,3.2.0,,Spark Core,,,,,0,,,,,The deadlock has the exactly same root cause as SPARK-14180 but just happens in a different code path.,,apachespark,cloud_fan,Ngone51,,,,,,,,,,,,,,,,,SPARK-33325,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 20 06:40:36 UTC 2021,,,,,,,,,,"0|z0tyy8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/21 07:17;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33759;;;","18/Aug/21 14:47;cloud_fan;Issue resolved by pull request 33759
[https://github.com/apache/spark/pull/33759];;;","20/Aug/21 06:40;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33795;;;",,,,,,,,,,,,,
Fix UISeleniumSuite in sql/hive-thriftserver,SPARK-36512,13395236,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,14/Aug/21 06:38,19/Aug/21 05:28,13/Jul/23 08:50,19/Aug/21 05:28,3.3.0,,,,,,,,3.3.0,,,,Tests,Web UI,,,,0,,,,,UISeleniumSuite in hive/thriftserver doesn't work even though the ignored test is enabled.,,apachespark,Gengliang.Wang,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 19 05:28:24 UTC 2021,,,,,,,,,,"0|z0twls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/21 07:08;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33741;;;","19/Aug/21 05:28;Gengliang.Wang;Issue resolved by pull request 33741
[https://github.com/apache/spark/pull/33741];;;",,,,,,,,,,,,,,
Executors don't get rescheduled in standalone mode when worker dies,SPARK-36509,13395114,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,peterk82,peterk82,13/Aug/21 12:36,28/Aug/21 09:07,13/Jul/23 08:50,28/Aug/21 09:07,3.0.1,3.1.1,3.1.2,,,,,,,,,,Spark Core,,,,,2,,,,,"This is reproducible with an application that uses less cores than what are available on the workers:

E.g. with 1 application with 1 executor, when the worker with the executor is killed, the application will not get another executor assigned even if there are enough resources in the cluster. This seems to be a regression, caused by [https://github.com/apache/spark/commit/51de86baed0776304c6184f2c04b6303ef48df90#diff-ca694acef669f50f9b45ca0d32ab6f5a516270bb26b33c4abb704e2dc00a1a03] .

That causes an assertion error on the master because it get's an executorStateChange from 'RUNNING' to 'RUNNING' instead of 'FAILED':
{noformat}
2021-08-13 14:04:12,554 [dispatcher-event-loop-2] INFO : I have been elected leader! New state: ALIVE
2021-08-13 14:04:12,554 [dispatcher-event-loop-2] INFO : I have been elected leader! New state: ALIVE
2021-08-13 14:04:56,489 [dispatcher-event-loop-10] INFO : Registering worker 172.27.64.1:58636 with 12 cores, 30.7 GiB RAM
2021-08-13 14:04:59,949 [dispatcher-event-loop-6] INFO : Registering worker 172.27.64.1:58694 with 12 cores, 30.7 GiB RAM
2021-08-13 14:05:20,212 [dispatcher-event-loop-2] INFO : Registering app query-frontend-null-172.27.64.1
2021-08-13 14:05:20,212 [dispatcher-event-loop-2] INFO : Registered app query-frontend-null-172.27.64.1 with ID app-20210813140520-0000
2021-08-13 14:05:20,228 [dispatcher-event-loop-2] INFO : Launching executor app-20210813140520-0000/0 on worker worker-20210813140459-172.27.64.1-58694
2021-08-13 14:05:37,991 [dispatcher-event-loop-9] ERROR: Ignoring errorjava.lang.AssertionError: assertion failed: executor 0 state transfer from RUNNING to RUNNING is illegal at scala.Predef$.assert(Predef.scala:223) at org.apache.spark.deploy.master.Master$$anonfun$receive$1.applyOrElse(Master.scala:323) at org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)
 {noformat}
 ",,apachespark,medb,peterk82,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Aug 28 09:07:33 UTC 2021,,,,,,,,,,"0|z0tvuo:",9223372036854775807,,,,,,,,,,,,,3.0.4,3.1.3,3.2.0,,,,,,,,"24/Aug/21 06:37;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33818;;;","28/Aug/21 09:07;sarutak;Issue resolved in https://github.com/apache/spark/pull/33818;;;",,,,,,,,,,,,,,
Remove/Replace missing links to AMP Camp materials from index.md,SPARK-36507,13395019,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,13/Aug/21 05:43,15/Nov/21 18:26,13/Jul/23 08:50,13/Aug/21 10:19,3.2.0,3.3.0,,,,,,,3.3.0,,,,Documentation,,,,,0,,,,,index.md includes the links to AMP Camp materials but the target locations are missing now.,,apachespark,maxgekk,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 13 10:19:52 UTC 2021,,,,,,,,,,"0|z0tv9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/21 06:07;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33734;;;","13/Aug/21 10:19;maxgekk;Issue resolved by pull request 33734
[https://github.com/apache/spark/pull/33734];;;",,,,,,,,,,,,,,
LSHModel.approxSimilarityJoin can generate invalid column names,SPARK-36501,13394960,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,tarmstrong,tarmstrong,tarmstrong,12/Aug/21 19:27,12/Dec/22 18:11,13/Jul/23 08:50,13/Aug/21 03:05,3.1.2,,,,,,,,3.2.0,,,,MLlib,,,,,0,flaky,,,,"""approxSimilarityJoin for self join"" in BucketedRandomProjectionLSHSuite can fail with:
        org.apache.spark.sql.AnalysisException: cannot resolve '`keys#á¼ç°©`.`ì™ã‹`' given input columns: [keys#á¼ç°©.ì™ã‹];
",,apachespark,tarmstrong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 13 03:05:42 UTC 2021,,,,,,,,,,"0|z0tuwg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/21 19:27;tarmstrong;I have a fix for this...;;;","12/Aug/21 20:03;apachespark;User 'timarmstrong' has created a pull request for this issue:
https://github.com/apache/spark/pull/33730;;;","13/Aug/21 03:05;gurwls223;Fixed in https://github.com/apache/spark/pull/33730;;;",,,,,,,,,,,,,
The temp_shuffle files are not cleaned up when a task is interrupted,SPARK-36500,13394951,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,jiangxb1987,jiangxb1987,jiangxb1987,12/Aug/21 18:22,12/Dec/22 18:11,13/Jul/23 08:50,13/Aug/21 10:26,2.4.8,3.0.3,3.1.2,,,,,,3.0.4,3.1.3,3.2.0,,Spark Core,,,,,0,,,,,"We discovered a potential file leak on the long running cluster. As the cluster running for a few hours, there are many ""temp_shuffle_*"" files left in the folder. It seems the files are not cleaned up as expected when the task finishes.",,apachespark,jiangxb1987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 13 10:26:12 UTC 2021,,,,,,,,,,"0|z0tuug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/21 21:03;apachespark;User 'jiangxb1987' has created a pull request for this issue:
https://github.com/apache/spark/pull/33731;;;","13/Aug/21 10:26;gurwls223;Issue resolved by pull request 33731
[https://github.com/apache/spark/pull/33731];;;",,,,,,,,,,,,,,
"Aggregate functions over no grouping keys, on tables with a single bucket, return multiple rows",SPARK-36489,13394830,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ibu250,ibu250,ibu250,12/Aug/21 07:08,12/Aug/21 07:24,13/Jul/23 08:50,12/Aug/21 07:23,3.1.0,3.1.1,3.1.2,3.1.3,3.2.0,,,,3.1.3,3.2.0,,,Optimizer,,,,,0,,,,,"When running any aggregate function, without any grouping keys, on a table with a single bucket, multiple rows are returned. 

This happens because the aggregate function satisfies the `AllTuples` distribution, no `Exchange` will be planned, and the bucketed scan will be disabled.

 

Reproduction:

 
{code:java}
sql(
   """"""
   |CREATE TABLE t1 (`id` BIGINT, `event_date` DATE)
   |USING PARQUET
   |CLUSTERED BY (id)
   |INTO 1 BUCKETS
   |"""""".stripMargin)

sql(
   """"""
   |INSERT INTO TABLE t1 VALUES(1.23, cast(""2021-07-07"" as date))
   |"""""".stripMargin)

sql(
   """"""
   |INSERT INTO TABLE t1 VALUES(2.28, cast(""2021-08-08"" as date))
   |"""""".stripMargin)

assert(sql(""select sum(id) from t1 where id is not null"").count == 1){code}
 ",,apachespark,cloud_fan,ibu250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 12 07:23:44 UTC 2021,,,,,,,,,,"0|z0tu3k:",9223372036854775807,,,,,chengsu,,,,,,,,,,,,,,,,,,"12/Aug/21 07:11;apachespark;User 'IonutBoicuAms' has created a pull request for this issue:
https://github.com/apache/spark/pull/33711;;;","12/Aug/21 07:12;apachespark;User 'IonutBoicuAms' has created a pull request for this issue:
https://github.com/apache/spark/pull/33711;;;","12/Aug/21 07:23;cloud_fan;Issue resolved by pull request 33711
[https://github.com/apache/spark/pull/33711];;;",,,,,,,,,,,,,
"""Invalid usage of '*' in expression"" error due to the feature of 'quotedRegexColumnNames' in some scenarios.",SPARK-36488,13394826,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,planga82,merrily01,merrily01,12/Aug/21 06:45,14/Feb/22 08:24,13/Jul/23 08:50,26/Aug/21 03:34,2.4.8,3.1.2,,,,,,,3.3.0,,,,Spark Core,SQL,,,,0,,,,," In some cases, the error happens when the following property is set.
{code:java}
spark.sql(""set spark.sql.parser.quotedRegexColumnNames=true"")
{code}
*case 1:* 
{code:java}
spark-sql> create table tb_test as select 1 as col_a, 2 as col_b;
spark-sql> select `tb_test`.`col_a`  from tb_test;
1

spark-sql> set spark.sql.parser.quotedRegexColumnNames=true;

spark-sql> select `tb_test`.`col_a`  from tb_test;
Error in query: Invalid usage of '*' in expression 'unresolvedextractvalue'


{code}
 

*case 2:*
{code:java}
         > select `col_a`/`col_b` as `col_c` from (select 3 as `col_a` ,  3.14 as `col_b`);
0.955414

spark-sql> set spark.sql.parser.quotedRegexColumnNames=true;

spark-sql> select `col_a`/`col_b` as `col_c` from (select 3 as `col_a` ,  3.14 as `col_b`);

Error in query: Invalid usage of '*' in expression 'divide'
{code}
 

This problem exists in 3.X, 2.4.X and master versions. 

 

Related issue ： 

https://issues.apache.org/jira/browse/SPARK-12139

（As can be seen in the latest comments, some people have encountered the same problem）

 

Similar problems：

https://issues.apache.org/jira/browse/SPARK-28897

 ",,apachespark,cloud_fan,fchen,merrily01,planga82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Feb 14 08:24:42 UTC 2022,,,,,,,,,,"0|z0tu2o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/21 23:14;planga82;Hi [~merrily01] ,

I think these are not bugs. As you can review in the documentation ([https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select.html]) when you set the parameter spark.sql.parser.quotedRegexColumnNames=true “quoted identifiers (using backticks) in SELECT statement are interpreted as regular expressions and SELECT statement can take regex-based column specification”.

In case 1, this means that in the expression `tb_test`.`col_a`  tb_test is treated as a regular expression that represents a column. And this syntaxis  “column.field” is used to access structType columns. And in this case it is not allowed to use regular expressions.

In case 2, a regex can retrieve more than one column, for example `col_*` is resolved to col_a, col_b, so it does not make sense that the operators of a division are a list of columns, this is not allowed.

I’m going to open a PR trying to improve the error message to avoid confusion.;;;","21/Aug/21 19:43;apachespark;User 'planga82' has created a pull request for this issue:
https://github.com/apache/spark/pull/33802;;;","23/Aug/21 09:00;merrily01;Hi [~planga82]  ,

  Thans for your attention and sorry for late reply.

  It's OK to improve the error message, but I don't quite agree with you that it is not a bug.

  Firstly, for a very common SQL, the query results of opening and closing parameters are different, which is not easy to accept. At least it can prove that this feature is not perfect.(I always feel that the regular expression of this feature should not deal with part 'database'. )

  Secondly, at the begining, this feature was compatible with and reference to hive feature. But now, the above cases can be executed normally when turn on the feature in Hive, Spak has a problem.

 

 ;;;","23/Aug/21 18:23;planga82;Yes, I'm with you that it's not very intuitive and it have room for improvement. I think it's interesting to take a closer look to support more expressions and to be more close to hive feature.

Thanks!;;;","26/Aug/21 03:34;cloud_fan;Issue resolved by pull request 33802
[https://github.com/apache/spark/pull/33802];;;","14/Feb/22 08:23;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35508;;;","14/Feb/22 08:24;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/35508;;;",,,,,,,,,
Fix intermittent test failure due to netty dependency version bump,SPARK-36483,13394768,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,mshen,mshen,mshen,11/Aug/21 20:53,13/Aug/21 01:17,13/Jul/23 08:50,13/Aug/21 01:17,3.2.0,,,,,,,,3.2.0,,,,Spark Core,,,,,0,,,,,"In SPARK-35132, Spark's netty dependency version was bumped from 4.1.51 to 4.1.63.

Since Netty version 4.1.52, a Netty specific io.netty.channel.StacklessClosedChannelException gets thrown when Netty's AbstractChannel encounters a closed channel.

This can sometimes break the test org.apache.spark.network.RPCIntegrationSuite as reported [here|[https://github.com/apache/spark/pull/33613#issuecomment-896697401].]

This is due to the hardcoded list of exception messages to check in RPCIntegrationSuite does not include this new StacklessClosedChannelException.",,apachespark,mridulm80,mshen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 13 01:17:01 UTC 2021,,,,,,,,,,"0|z0ttps:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/21 21:02;apachespark;User 'Victsm' has created a pull request for this issue:
https://github.com/apache/spark/pull/33713;;;","13/Aug/21 01:17;mridulm80;Issue resolved by pull request 33713
[https://github.com/apache/spark/pull/33713];;;",,,,,,,,,,,,,,
SessionWindowStateStoreSaveExec should not filter input rows against watermark,SPARK-36480,13394675,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,kabhwan,kabhwan,kabhwan,11/Aug/21 12:52,12/Aug/21 03:12,13/Jul/23 08:50,12/Aug/21 03:12,3.2.0,,,,,,,,3.2.0,,,,Structured Streaming,,,,,0,,,,,"SessionWindowStateStoreSaveExec receives all sessions including existing sessions into input rows and stores as they are. That said, we should not filter out input rows before storing into state store, but we do. 

Fortunately it hasn't showed any actual problem due to the nature how we deal with watermark against micro-batch and it seems hard to come up with the broken case, but it should be better to fix it before someone succeeds to touch the possible edge case.",,apachespark,kabhwan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 12 03:12:01 UTC 2021,,,,,,,,,,"0|z0tt54:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/21 13:01;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/33708;;;","11/Aug/21 13:02;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/33708;;;","12/Aug/21 03:12;viirya;The issue was resolved at https://github.com/apache/spark/pull/33708.;;;",,,,,,,,,,,,,
Table in unloaded catalog referenced by view should load correctly,SPARK-36466,13394359,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,chengpan,chengpan,,10/Aug/21 06:29,24/Nov/22 00:38,13/Jul/23 08:50,10/Aug/21 09:31,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,,,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 10 09:31:58 UTC 2021,,,,,,,,,,"0|z0tr6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/21 09:31;cloud_fan;Issue resolved by pull request 33692
[https://github.com/apache/spark/pull/33692];;;",,,,,,,,,,,,,,,
Fix Underlying Size Variable Initialization in ChunkedByteBufferOutputStream for Writing Over 2GB Data,SPARK-36464,13394352,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,kazuyukitanimura,kazuyukitanimura,kazuyukitanimura,10/Aug/21 05:20,10/Jan/22 23:35,13/Jul/23 08:50,10/Aug/21 17:34,2.0.2,2.1.3,2.2.3,2.3.4,2.4.8,3.0.3,3.1.2,3.3.0,3.0.4,3.1.3,3.2.0,3.2.1,Spark Core,,,,,0,,,,,"The `size` method of `ChunkedByteBufferOutputStream` returns a `Long` value; however, the underlying `_size` variable is initialized as `Int`.

That causes an overflow and returns a negative size when over 2GB data is written into `ChunkedByteBufferOutputStream`",,apachespark,dongjoon,kazuyukitanimura,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Oct 14 08:28:43 UTC 2021,,,,,,,,,,"0|z0tr5c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/21 05:57;apachespark;User 'kazuyukitanimura' has created a pull request for this issue:
https://github.com/apache/spark/pull/33690;;;","10/Aug/21 05:58;apachespark;User 'kazuyukitanimura' has created a pull request for this issue:
https://github.com/apache/spark/pull/33690;;;","10/Aug/21 17:34;dongjoon;Issue resolved by pull request 33690
[https://github.com/apache/spark/pull/33690];;;","11/Aug/21 02:59;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33697;;;","14/Oct/21 08:27;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34284;;;","14/Oct/21 08:27;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34284;;;","14/Oct/21 08:28;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/34284;;;",,,,,,,,,
Prohibit update mode in native support of session window,SPARK-36463,13394313,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,kabhwan,kabhwan,kabhwan,10/Aug/21 03:16,11/Aug/21 01:46,13/Jul/23 08:50,11/Aug/21 01:46,3.2.0,,,,,,,,3.2.0,,,,Structured Streaming,,,,,0,,,,,"The semantic of update mode for native support of session window seems to be broken.

Strictly saying, it doesn't break the semantic based on our explanation of update mode:

{quote}
Update Mode - Only the rows that were updated in the Result Table since the last trigger will be written to the external storage (available since Spark 2.1.1). Note that this is different from the Complete Mode in that this mode only outputs the rows that have changed since the last trigger. If the query doesn’t contain aggregations, it will be equivalent to Append mode.
{quote}

But given the grouping key is changing due to the nature of session window, there is no way to ""upsert"" the output into destination. If end users try to ""upsert"" the output based on the grouping key, it is high likely that a single session window output will be written into multiple rows across multiple updates.",,apachespark,Gengliang.Wang,kabhwan,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 11 01:46:25 UTC 2021,,,,,,,,,,"0|z0tqwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/21 03:17;kabhwan;Will submit a PR shortly.;;;","10/Aug/21 03:57;apachespark;User 'HeartSaVioR' has created a pull request for this issue:
https://github.com/apache/spark/pull/33689;;;","10/Aug/21 04:49;kabhwan;[~Gengliang.Wang]
FYI as I marked this as a blocker. I would like to remove some functionality which is unclear about the desired behavior before releasing the feature.
Also cc. to [~viirya] to see whether he could help reviewing the proposed change.;;;","10/Aug/21 05:02;Gengliang.Wang;[~kabhwan] Thanks for the info. I plan to cut RC1 next week. Please try to finish the PR this week. Thanks!;;;","10/Aug/21 05:10;viirya;Thanks [~kabhwan] ,[~Gengliang.Wang] . I will do the review asap.;;;","11/Aug/21 01:46;kabhwan;Issue resolved by pull request 33689
[https://github.com/apache/spark/pull/33689];;;",,,,,,,,,,
Not push down partition filter to ORCScan for DSv2,SPARK-36454,13394107,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,apachespark,huaxingao,huaxingao,09/Aug/21 01:13,09/Aug/21 17:47,13/Jul/23 08:50,09/Aug/21 17:47,3.3.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"Seems to me that partition filter is only used for partition pruning and shouldn't be pushed down to ORCScan. We don't push down partition filter to ORCScan in DSv1, and we don't push down partition filter for parquet in both DSv1 and DSv2.
",,apachespark,dongjoon,huaxingao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 09 17:47:42 UTC 2021,,,,,,,,,,"0|z0tpmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Aug/21 01:53;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33680;;;","09/Aug/21 01:53;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33680;;;","09/Aug/21 17:47;dongjoon;Issue resolved by pull request 33680
[https://github.com/apache/spark/pull/33680];;;",,,,,,,,,,,,,
ALTER TABLE REPLACE COLUMNS should check duplicates for the specified columns for v2 command,SPARK-36449,13394061,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,08/Aug/21 05:18,10/Aug/21 05:21,13/Jul/23 08:50,10/Aug/21 05:21,3.3.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,ALTER TABLE REPLACE COLUMNS currently doesn't check duplicates for the specified columns for v2 command.,,apachespark,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 10 05:21:10 UTC 2021,,,,,,,,,,"0|z0tpco:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/21 05:55;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/33676;;;","10/Aug/21 05:21;cloud_fan;Issue resolved by pull request 33676
[https://github.com/apache/spark/pull/33676];;;",,,,,,,,,,,,,,
Exceptions in NoSuchItemException.scala have to be case classes to preserve specific exceptions,SPARK-36448,13393965,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,manifoldQAQ,manifoldQAQ,manifoldQAQ,06/Aug/21 20:21,20/Aug/21 12:17,13/Jul/23 08:50,20/Aug/21 12:17,3.1.2,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"Exceptions in NoSuchItemException.scala are not case classes. This is causing issues because in Analyzer's [executeAndCheck|https://github.com/apache/spark/blob/888f8f03c89ea7ee8997171eadf64c87e17c4efe/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L196-L199] method always calls the `copy` method on the exception. However, since these exceptions are not case classes, the `copy` method was always delegated to `AnalysisException::copy`, which is not the specialized version",,apachespark,cloud_fan,manifoldQAQ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 20 12:17:13 UTC 2021,,,,,,,,,,"0|z0torc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/21 20:21;manifoldQAQ;I'd raise a PR shortly.;;;","06/Aug/21 20:53;apachespark;User 'yeshengm' has created a pull request for this issue:
https://github.com/apache/spark/pull/33673;;;","20/Aug/21 12:17;cloud_fan;Issue resolved by pull request 33673
[https://github.com/apache/spark/pull/33673];;;",,,,,,,,,,,,,
Avoid inlining non-deterministic With-CTEs,SPARK-36447,13393955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,maryannxue,maryannxue,maryannxue,06/Aug/21 17:44,24/Nov/21 09:55,13/Jul/23 08:50,13/Aug/21 03:50,3.1.2,3.2.0,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,maryannxue,,,,,,,,,,,,,,,,,,SPARK-28299,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 13 03:50:11 UTC 2021,,,,,,,,,,"0|z0top4:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"06/Aug/21 18:30;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/33671;;;","06/Aug/21 18:31;apachespark;User 'maryannxue' has created a pull request for this issue:
https://github.com/apache/spark/pull/33671;;;","13/Aug/21 03:50;cloud_fan;Issue resolved by pull request 33671
[https://github.com/apache/spark/pull/33671];;;",,,,,,,,,,,,,
Downloading lintr dependencies fail on GA,SPARK-36441,13393810,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,06/Aug/21 01:29,12/Dec/22 18:10,13/Jul/23 08:50,06/Aug/21 01:50,3.3.0,,,,,,,,3.0.4,3.1.3,3.2.0,,Project Infra,,,,,0,,,,,"Downloading lintr dependencies on GA fails.
I re-triggered the GA job but it still fail with the same error.

{code}
 * installing *source* package ‘devtools’ ...
** package ‘devtools’ successfully unpacked and MD5 sums checked
** using staged installation
** R
** inst
** byte-compile and prepare package for lazy loading
** help
*** installing help indices
*** copying figures
** building package indices
** installing vignettes
** testing if installed package can be loaded from temporary location
** testing if installed package can be loaded from final location
** testing if installed package keeps a record of temporary installation path
* DONE (devtools)

The downloaded source packages are in
	‘/tmp/Rtmpv53Ix4/downloaded_packages’
Using bundled GitHub PAT. Please add your own PAT to the env var `GITHUB_PAT`
Error: Failed to install 'unknown package' from GitHub:
  HTTP error 401.
  Bad credentials

  Rate limit remaining: 59/60
  Rate limit reset at: 2021-08-06 01:37:46 UTC

  
Execution halted
Error: Process completed with exit code 1.
{code}

https://github.com/apache/spark/runs/3257853825",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 06 01:50:30 UTC 2021,,,,,,,,,,"0|z0tnsw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/21 01:33;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33660;;;","06/Aug/21 01:50;gurwls223;Issue resolved by pull request 33660
[https://github.com/apache/spark/pull/33660];;;",,,,,,,,,,,,,,
Spark3 fails to read hive table with mixed format,SPARK-36440,13393803,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,,kings129,kings129,05/Aug/21 23:20,06/Aug/21 21:55,13/Jul/23 08:50,06/Aug/21 21:55,3.0.0,3.1.1,3.1.2,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"Spark3 fails to read hive table with mixed format with hive Serde, this is a regression compares to Spark 2.4. 

Replication steps :
 1. In spark 3 (3.0 or 3.1) spark shell:
{code:java}
scala> spark.sql(""create table tmp.test_table (id int, name string) partitioned by (pt int) stored as rcfile"")

scala> spark.sql(""insert into tmp.test_table (pt = 1) values (1, 'Alice'), (2, 'Bob')"")
{code}
2. Run hive command to change table file format (from RCFile to Parquet).
{code:java}
hive (default)> alter table set tmp.test_table fileformat Parquet;
{code}
3. Try to read partition (in RCFile format) with hive serde using Spark shell:
{code:java}
scala> spark.conf.set(""spark.sql.hive.convertMetastoreParquet"", ""false"")

scala> spark.sql(""select * from tmp.test_table where pt=1"").show{code}
Exception: (anonymized file path with <path>)
{code:java}
Caused by: java.lang.RuntimeException: s3a://<path>/data/part-00000-22112178-5dd7-4065-89d7-2ee550296909-c000 is not a Parquet file. expected magic number at tail [80, 65, 82, 49] but found [5, 96, 1, -33]
  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:524)
  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:505)
  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:499)
  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:448)
  at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:433)
  at org.apache.hadoop.hive.ql.io.parquet.ParquetRecordReaderBase.getSplit(ParquetRecordReaderBase.java:79)
  at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:75)
  at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.<init>(ParquetRecordReaderWrapper.java:60)
  at org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat.getRecordReader(MapredParquetInputFormat.java:75)
  at org.apache.spark.rdd.HadoopRDD$$anon$1.liftedTree1$1(HadoopRDD.scala:286)
  at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:285)
  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:243)
  at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:96)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
  at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
  <omitted more logs>{code}
 

 

 ",,csun,kings129,,,,,,,,,,,,,,,,,,,SPARK-36197,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 06 21:55:01 UTC 2021,,,,,,,,,,"0|z0tnrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/21 05:37;csun;Hmm really? Spark 2.x support this? I'm not sure why Spark is still expected to work in this case since the serde is changed to Parquet but the underlying data file is in ORC. It seems like an error that users should avoid.;;;","06/Aug/21 21:50;kings129;I found the root cause it's due to [https://github.com/apache/spark/pull/23559] removed partition input format support in Spark 2.4.
While I creating a fix PR for this issue, found it was recently fixed in https://issues.apache.org/jira/browse/SPARK-36197 and will be available in 3.2.0.;;;","06/Aug/21 21:55;kings129;Already fixed by https://issues.apache.org/jira/browse/SPARK-36197 for future versions.;;;",,,,,,,,,,,,,
Logs should show correct URL of where HistoryServer is started,SPARK-36433,13393791,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,thejdeep,thejdeep,thejdeep,05/Aug/21 21:34,16/Sep/21 14:42,13/Jul/23 08:50,16/Sep/21 09:27,3.3.0,,,,,,,,3.3.0,,,,Web UI,,,,,0,,,,,"Due to a recent refactoring in the WebUI bind() code, the log message to print the bound host and port information got moved and because of this the info printed is incorrect.

 

Example log - 21/08/05 10:47:38 INFO HistoryServer: Bound HistoryServer to 0.0.0.0, and started at <hostName>:-1

 

Notice above that the port is incorrect",,apachespark,Gengliang.Wang,holden,thejdeep,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Sep 16 14:42:49 UTC 2021,,,,,,,,,,"0|z0tnoo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/21 21:43;apachespark;User 'thejdeep' has created a pull request for this issue:
https://github.com/apache/spark/pull/33659;;;","13/Sep/21 19:48;holden;I think if this is a regression we should make this a blocker since finding the history server is an important part of people debugging their applications.;;;","16/Sep/21 09:27;Gengliang.Wang;Issue resolved by pull request 33659
[https://github.com/apache/spark/pull/33659];;;","16/Sep/21 09:29;Gengliang.Wang;[~thejdeep] [~holden] The regression change SPARK-36237 is merged on master only. So I change the affect version to 3.3.0 and the priority to ""minor"".;;;","16/Sep/21 14:42;thejdeep;Thanks [~Gengliang.Wang];;;",,,,,,,,,,,
Upgrade Jetty version to 9.4.43,SPARK-36432,13393778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,this,this,this,05/Aug/21 20:33,12/Dec/22 18:11,13/Jul/23 08:50,09/Aug/21 01:18,3.2.0,3.3.0,,,,,,,3.2.0,,,,Build,,,,,0,,,,,"Upgrade Jetty version to 9.4.43.v20210629 in current Spark master in order to fix vulnerability https://nvd.nist.gov/vuln/detail/CVE-2021-34429.
",,apachespark,this,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 09 01:18:45 UTC 2021,,,,,,,,,,"0|z0tnls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/21 20:48;apachespark;User 'this' has created a pull request for this issue:
https://github.com/apache/spark/pull/33656;;;","05/Aug/21 20:49;apachespark;User 'this' has created a pull request for this issue:
https://github.com/apache/spark/pull/33656;;;","09/Aug/21 01:18;gurwls223;Issue resolved by pull request 33656
[https://github.com/apache/spark/pull/33656];;;",,,,,,,,,,,,,
Adaptively calculate the target size when coalescing shuffle partitions in AQE,SPARK-36430,13393736,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,05/Aug/21 15:00,09/Aug/21 09:27,13/Jul/23 08:50,09/Aug/21 09:27,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,xkrogen,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 09 09:27:06 UTC 2021,,,,,,,,,,"0|z0tncg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/21 15:43;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33655;;;","05/Aug/21 15:43;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33655;;;","09/Aug/21 09:27;cloud_fan;Issue resolved by pull request 33655
[https://github.com/apache/spark/pull/33655];;;",,,,,,,,,,,,,
JacksonParser should throw exception when data type unsupported.,SPARK-36429,13393691,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,beliefer,beliefer,beliefer,05/Aug/21 11:16,23/Aug/21 04:04,13/Jul/23 08:50,06/Aug/21 07:06,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"Currently, when set spark.sql.timestampType=TIMESTAMP_NTZ, the behavior is different between from_json and from_csv.

{code:java}
-- !query
select from_json('{""t"":""26/October/2015""}', 't Timestamp', map('timestampFormat', 'dd/MMMMM/yyyy'))
-- !query schema
struct<from_json({""t"":""26/October/2015""}):struct<t:timestamp_ntz>>
-- !query output
{""t"":null}
{code}




{code:java}
-- !query
select from_csv('26/October/2015', 't Timestamp', map('timestampFormat', 'dd/MMMMM/yyyy'))
-- !query schema
struct<>
-- !query output
java.lang.Exception
Unsupported type: timestamp_ntz
{code}


We should make from_json throws exception too.",,apachespark,beliefer,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 23 04:04:38 UTC 2021,,,,,,,,,,"0|z0tn2g:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/21 11:44;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/33654;;;","06/Aug/21 06:45;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33663;;;","06/Aug/21 10:17;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/33668;;;","06/Aug/21 10:18;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/33668;;;","06/Aug/21 11:49;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33670;;;","06/Aug/21 11:50;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33670;;;","09/Aug/21 06:53;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/33684;;;","23/Aug/21 04:04;sarutak;Issue resolved in https://github.com/apache/spark/pull/33684.;;;",,,,,,,,
the 'seconds' parameter of 'make_timestamp' should accept integer type,SPARK-36428,13393688,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,apachespark,cloud_fan,cloud_fan,05/Aug/21 11:00,19/Aug/21 10:24,13/Jul/23 08:50,13/Aug/21 05:13,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"With ANSI mode, {{SELECT make_timestamp(1, 1, 1, 1, 1, 1)}} fails, because the 'seconds' parameter needs to be of type DECIMAL(8,6), and INT can't be implicitly casted to DECIMAL(8,6) under ANSI mode.

We should update the function {{make_timestamp}} to allow integer type 'seconds' parameter.",,apachespark,beliefer,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 19 10:24:17 UTC 2021,,,,,,,,,,"0|z0tn1s:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/21 07:58;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/33665;;;","13/Aug/21 05:13;cloud_fan;Issue resolved by pull request 33665
[https://github.com/apache/spark/pull/33665];;;","18/Aug/21 03:19;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/33775;;;","19/Aug/21 10:23;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/33787;;;","19/Aug/21 10:24;apachespark;User 'beliefer' has created a pull request for this issue:
https://github.com/apache/spark/pull/33787;;;",,,,,,,,,,,
Validate all SQL configs to prevent from wrong use for ConfigEntry,SPARK-36421,13393616,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,Qin Yao,Qin Yao,Qin Yao,05/Aug/21 04:40,12/Dec/22 18:10,13/Jul/23 08:50,06/Aug/21 02:03,3.1.2,3.2.0,,,,,,,3.2.0,3.3.0,,,SQL,,,,,0,,,,,"ConfigEntry(key=spark.sql.hive.metastore.version, defaultValue=2.3.7, doc=Version)

should not go to the doc and set -v command",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Aug 06 02:03:27 UTC 2021,,,,,,,,,,"0|z0tmls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Aug/21 04:49;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33647;;;","05/Aug/21 04:50;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33647;;;","06/Aug/21 02:03;gurwls223;Fixed in https://github.com/apache/spark/pull/33647;;;",,,,,,,,,,,,,
Redact sensitive information in Spark Thrift Server UI,SPARK-36400,13393349,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,dnskrv,dnskrv,03/Aug/21 23:04,01/Sep/21 07:25,13/Jul/23 08:50,18/Aug/21 04:34,3.1.2,,,,,,,,3.1.3,3.2.0,,,SQL,Web UI,,,,0,,,,,"Spark UI displays sensitive information on ""JDBC/ODBC Server"" tab

The reason of the issue is in [org.apache.spark.sql.hive.thriftserver.ui.SqlStatsPagedTable|https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/ui/ThriftServerPage.scala#L166] class [here|https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/ui/ThriftServerPage.scala#L266-L268]
{code:scala}
      <td>
        <span class=""description-input"">
          {info.statement}
        </span>
      </td>
{code}",,apachespark,dc-heros,dnskrv,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/21 23:04;dnskrv;SQL Statistics.png;https://issues.apache.org/jira/secure/attachment/13031423/SQL+Statistics.png",,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 07:25:45 UTC 2021,,,,,,,,,,"0|z0tkyg:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/21 09:09;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33743;;;","25/Aug/21 09:49;sarutak;Issue resolved in https://github.com/apache/spark/pull/33743.;;;","01/Sep/21 07:25;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33885;;;",,,,,,,,,,,,,
Redact sensitive information in Spark Thrift Server log,SPARK-36398,13393341,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,dnskrv,dnskrv,03/Aug/21 22:04,25/Aug/21 12:33,13/Jul/23 08:50,25/Aug/21 12:33,3.1.2,,,,,,,,3.1.3,3.2.0,,,Security,SQL,,,,0,,,,,"Spark Thrift Server logs query without sensitive information redaction in [org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.scala|https://github.com/apache/spark/blob/master/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala#L188]
{code:scala}
  override def runInternal(): Unit = {
    setState(OperationState.PENDING)
    logInfo(s""Submitting query '$statement' with $statementId"")
{code}
Logs
{code:sh}
21/08/03 20:49:46 INFO SparkExecuteStatementOperation: Submitting query 'CREATE OR REPLACE TEMPORARY VIEW test_view
    USING org.apache.spark.sql.jdbc
    OPTIONS (
        url=""jdbc:mysql://example.com:3306"",
        driver=""com.mysql.jdbc.Driver"",
        dbtable=""example.test"",
        user=""my_username"",
        password=""my_password""
    )' with 37e5d2cb-aa96-407e-b589-7cb212324100
21/08/03 20:49:46 INFO SparkExecuteStatementOperation: Running query with 37e5d2cb-aa96-407e-b589-7cb212324100
{code}",,apachespark,dnskrv,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 25 12:33:29 UTC 2021,,,,,,,,,,"0|z0tkwo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/21 12:34;dnskrv;Should be fixed by [PR#33743|https://github.com/apache/spark/pull/33743];;;","15/Aug/21 03:27;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33743;;;","25/Aug/21 10:02;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33832;;;","25/Aug/21 12:33;sarutak;Issue resolved in https://github.com/apache/spark/pull/33832.;;;",,,,,,,,,,,,
"When fetch chunk throw NPE, improve the error message",SPARK-36391,13393238,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,03/Aug/21 09:32,06/May/22 10:22,13/Jul/23 08:50,05/Aug/21 07:34,3.2.0,,,,,,,,3.2.0,3.3.0,,,Shuffle,,,,,0,,,,,"{code}
2021-07-31 22:00:24,810 ERROR server.ChunkFetchRequestHandler (ChunkFetchRequestHandler.java:lambda$respond$1(146)) - Error sending result ChunkFetchFailure[streamChunkId=StreamChunkId[streamId=1119950114515,chunkIndex=0],errorString=java.lang.NullPointerException
	at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:80)
	at org.apache.spark.network.server.ChunkFetchRequestHandler.processFetchRequest(ChunkFetchRequestHandler.java:101)
	at org.apache.spark.network.server.ChunkFetchRequestHandler.channelRead0(ChunkFetchRequestHandler.java:82)
	at org.apache.spark.network.server.ChunkFetchRequestHandler.channelRead0(ChunkFetchRequestHandler.java:51)
	at org.sparkproject.io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.access$600(AbstractChannelHandlerContext.java:61)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext$7.run(AbstractChannelHandlerContext.java:370)
	at org.sparkproject.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:500)
	at org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
] to /10.128.137.103:53818; closing connection
java.nio.channels.ClosedChannelException
	at org.sparkproject.io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)
	at org.sparkproject.io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)
	at org.sparkproject.io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)
	at org.sparkproject.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:112)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite(AbstractChannelHandlerContext.java:709)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:792)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:702)
	at org.sparkproject.io.netty.handler.timeout.IdleStateHandler.write(IdleStateHandler.java:302)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)
	at org.sparkproject.io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1104)
	at org.sparkproject.io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)
	at org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)
	at org.sparkproject.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)
	at org.sparkproject.io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
	at org.sparkproject.io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at org.sparkproject.io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.lang.Thread.run(Thread.java:748)
{code}",,angerszhuuu,apachespark,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 05 07:34:29 UTC 2021,,,,,,,,,,"0|z0tka0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/21 09:45;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33622;;;","03/Aug/21 09:46;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33622;;;","05/Aug/21 07:34;Ngone51;Issue resolved by [https://github.com/apache/spark/pull/33622]
[|https://issues.apache.org/jira/secure/AddComment!default.jspa?id=13393238];;;",,,,,,,,,,,,,
Revert the change that accepts negative mapId in ShuffleBlockId,SPARK-36389,13393182,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,csingh,csingh,csingh,03/Aug/21 03:00,03/Aug/21 07:18,13/Jul/23 08:50,03/Aug/21 06:35,3.2.0,,,,,,,,3.2.0,3.3.0,,,Shuffle,,,,,0,,,,,"With SPARK-32922, we added a change that {{ShuffleBlockId}} can have a negative mapId. This was to support push-based shuffle where {{-1}} as mapId indicated a push-merged block. However with SPARK-32923, a different type of {{BlockId}} was introduced - {{ShuffleMergedId}}, but reverting the change to {{ShuffleBlockId}} was missed. ",,apachespark,csingh,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30602,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 06:35:52 UTC 2021,,,,,,,,,,"0|z0tjxk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Aug/21 03:12;apachespark;User 'otterc' has created a pull request for this issue:
https://github.com/apache/spark/pull/33616;;;","03/Aug/21 03:12;apachespark;User 'otterc' has created a pull request for this issue:
https://github.com/apache/spark/pull/33616;;;","03/Aug/21 06:35;dongjoon;Issue resolved by pull request 33616
[https://github.com/apache/spark/pull/33616];;;",,,,,,,,,,,,,
NullPointerException throws during executor shutdown,SPARK-36383,13393097,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Ngone51,Ngone51,Ngone51,02/Aug/21 14:18,03/Aug/21 15:51,13/Jul/23 08:50,03/Aug/21 06:57,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.2.0,3.3.0,,,Spark Core,,,,,0,,,,,"{code:java}
21/07/23 16:04:10 WARN Executor: Unable to stop executor metrics poller
java.lang.NullPointerException
        at org.apache.spark.executor.Executor.stop(Executor.scala:318)
        at org.apache.spark.executor.Executor.$anonfun$stopHookReference$1(Executor.scala:76)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
        at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2025)
        at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at scala.util.Try$.apply(Try.scala:213)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
21/07/23 16:04:10 WARN Executor: Unable to stop heartbeater
java.lang.NullPointerException
        at org.apache.spark.executor.Executor.stop(Executor.scala:324)
        at org.apache.spark.executor.Executor.$anonfun$stopHookReference$1(Executor.scala:76)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
        at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2025)
        at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at scala.util.Try$.apply(Try.scala:213)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
21/07/23 16:04:10 ERROR Utils: Uncaught exception in thread shutdown-hook-0
java.lang.NullPointerException
        at org.apache.spark.executor.Executor.$anonfun$stop$3(Executor.scala:334)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.util.Utils$.withContextClassLoader(Utils.scala:231)
        at org.apache.spark.executor.Executor.stop(Executor.scala:334)
        at org.apache.spark.executor.Executor.$anonfun$stopHookReference$1(Executor.scala:76)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
        at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2025)
        at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
        at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
        at scala.util.Try$.apply(Try.scala:213)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
        at java.util.concurrent.FutureTask.run(FutureTask.java:266)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
        at java.lang.Thread.run(Thread.java:748)
{code}",,apachespark,dongjoon,Ngone51,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 08:45:54 UTC 2021,,,,,,,,,,"0|z0tjeo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/21 15:34;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33612;;;","03/Aug/21 06:57;dongjoon;Issue resolved by pull request 33612
[https://github.com/apache/spark/pull/33612];;;","03/Aug/21 08:45;apachespark;User 'Ngone51' has created a pull request for this issue:
https://github.com/apache/spark/pull/33620;;;",,,,,,,,,,,,,
Remove noisy footer from the summary table for metrics,SPARK-36382,13393076,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,02/Aug/21 13:05,02/Aug/21 15:40,13/Jul/23 08:50,02/Aug/21 15:40,3.3.0,,,,,,,,3.3.0,,,,Web UI,,,,,0,,,,,"In the WebUI, some tables are implemented using DataTables (https://datatables.net/).
By default, tables created using DataTables shows footer which says `Showing x to y of z entries`, which is helpful for some tables if table entries can grow
But the summary table for metrics in StagePage cannot grow so it's a little bit noisy.",,apachespark,Gengliang.Wang,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 02 15:40:58 UTC 2021,,,,,,,,,,"0|z0tja0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/21 13:14;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33611;;;","02/Aug/21 13:14;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33611;;;","02/Aug/21 15:40;Gengliang.Wang;Issue resolved by pull request 33611
[https://github.com/apache/spark/pull/33611];;;",,,,,,,,,,,,,
ALTER TABLE ADD/RENAME COLUMNS check exist does not use case sensitive for v2 command.,SPARK-36381,13393061,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xiaopenglei,xiaopenglei,xiaopenglei,02/Aug/21 12:16,12/Dec/22 18:10,13/Jul/23 08:50,04/Aug/21 01:04,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,ALTER TABLE ADD/RENAME COLUMNS check exist does not use case sensitive for v2 command.,,apachespark,xiaopenglei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 04 01:04:53 UTC 2021,,,,,,,,,,"0|z0tj6o:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/21 12:39;apachespark;User 'Peng-Lei' has created a pull request for this issue:
https://github.com/apache/spark/pull/33610;;;","03/Aug/21 04:13;apachespark;User 'Peng-Lei' has created a pull request for this issue:
https://github.com/apache/spark/pull/33618;;;","03/Aug/21 04:14;apachespark;User 'Peng-Lei' has created a pull request for this issue:
https://github.com/apache/spark/pull/33618;;;","04/Aug/21 01:04;gurwls223;Issue resolved by pull request 33618
[https://github.com/apache/spark/pull/33618];;;",,,,,,,,,,,,
Null at root level of a JSON array causes the parsing failure (w/ permissive mode),SPARK-36379,13393004,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,02/Aug/21 06:35,12/Dec/22 17:51,13/Jul/23 08:50,02/Aug/21 17:19,3.1.2,3.2.0,3.3.0,,,,,,3.2.0,3.3.0,,,SQL,,,,,0,,,,,"
{code}
scala> spark.read.json(Seq(""""""[{""a"": ""str""}, null, {""a"": ""str""}]"""""").toDS).collect()
{code}

{code}
...
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (172.30.3.20 executor driver): java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
{code}

Since the mode (by default) is permissive, we shouldn't just fail like above.",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,SPARK-36601,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 10:30:05 UTC 2021,,,,,,,,,,"0|z0tiu0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/21 07:16;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/33608;;;","02/Aug/21 17:19;dongjoon;This is resolved via https://github.com/apache/spark/pull/33608;;;","17/Aug/21 10:30;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/33762;;;",,,,,,,,,,,,,
ALTER TABLE ADD COLUMNS should check duplicates for the specified columns for v2 command,SPARK-36372,13392897,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,imback82,imback82,imback82,01/Aug/21 02:07,08/Aug/21 05:18,13/Jul/23 08:50,02/Aug/21 09:55,3.3.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,ALTER TABLE ADD COLUMNS currently doesn't check duplicates for the specified columns for v2 command.,,apachespark,cloud_fan,imback82,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 02 09:55:30 UTC 2021,,,,,,,,,,"0|z0ti68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/21 01:38;apachespark;User 'imback82' has created a pull request for this issue:
https://github.com/apache/spark/pull/33600;;;","02/Aug/21 09:55;cloud_fan;Issue resolved by pull request 33600
[https://github.com/apache/spark/pull/33600];;;",,,,,,,,,,,,,,
Upgrade Kubernetes Client Version to 5.6.0,SPARK-36358,13392746,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,30/Jul/21 11:47,30/Jul/21 15:25,13/Jul/23 08:50,30/Jul/21 15:25,3.3.0,,,,,,,,3.3.0,,,,Kubernetes,,,,,0,,,,,"This way [Retry HTTP operation in case IOException too (exponential backoff)|https://github.com/fabric8io/kubernetes-client/pull/3293] will be included",,apachespark,attilapiros,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 30 15:25:55 UTC 2021,,,,,,,,,,"0|z0th8w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/21 14:50;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/33593;;;","30/Jul/21 14:50;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/33593;;;","30/Jul/21 15:25;dongjoon;Issue resolved by pull request 33593
[https://github.com/apache/spark/pull/33593];;;",,,,,,,,,,,,,
EventLogFileReaders should not complain in case of no event log files,SPARK-36354,13392689,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,30/Jul/21 07:47,04/Aug/21 12:16,13/Jul/23 08:50,04/Aug/21 12:16,3.1.2,3.2.0,,,,,,,3.1.3,3.2.0,,,Spark Core,,,,,0,,,,,"{code}
21/07/30 07:38:26 WARN FsHistoryProvider: Error while reading new log s3a://.../eventlog_v2_spark-95b5c736c8e44037afcf152534d08771
java.lang.IllegalArgumentException: requirement failed: Log directory must contain at least one event log file!
        at scala.Predef$.require(Predef.scala:281)
        at org.apache.spark.deploy.history.RollingEventLogFilesFileReader.files$lzycompute(EventLogFileReaders.scala:216)
{code}

{code}
$ aws s3 ls s3://.../eventlog_v2_spark-95b5c736c8e44037afcf152534d08771/
2021-06-26 22:31:40          0 appstatus_spark-95b5c736c8e44037afcf152534d08771.inprogress
{code}",,apachespark,dongjoon,kabhwan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Aug 04 12:16:47 UTC 2021,,,,,,,,,,"0|z0tgw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/21 07:50;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33586;;;","30/Jul/21 07:50;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33586;;;","04/Aug/21 12:16;kabhwan;Issue resolved by pull request 33586
[https://github.com/apache/spark/pull/33586];;;",,,,,,,,,,,,,
Spark should check result plan's output schema name,SPARK-36352,13392667,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,30/Jul/21 06:14,26/Aug/21 14:09,13/Jul/23 08:50,09/Aug/21 08:49,3.2.0,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,,,angerszhuuu,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 17 15:19:18 UTC 2021,,,,,,,,,,"0|z0tgrc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/21 06:35;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33583;;;","30/Jul/21 06:36;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33583;;;","30/Jul/21 06:55;angerszhuuu;RemoveNoopOperators

CollapseProject;;;","09/Aug/21 08:49;cloud_fan;Issue resolved by pull request 33583
[https://github.com/apache/spark/pull/33583];;;","11/Aug/21 09:00;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33703;;;","11/Aug/21 09:01;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33703;;;","17/Aug/21 15:18;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33764;;;","17/Aug/21 15:19;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33764;;;",,,,,,,,
"unexpected Index loaded: pd.Index([10, 20, None], name=""x"")",SPARK-36348,13392530,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,yikunkero,yikunkero,yikunkero,29/Jul/21 12:06,12/Dec/22 18:11,13/Jul/23 08:50,20/Oct/21 05:12,3.2.0,,,,,,,,3.3.0,,,,PySpark,,,,,0,,,,,"{code:python}
pidx = pd.Index([10, 20, 15, 30, 45, None], name=""x"")
psidx = ps.Index(pidx)
self.assert_eq(psidx.astype(str), pidx.astype(str))
{code}

[left pandas on spark]:  Index(['10.0', '20.0', '15.0', '30.0', '45.0', 'nan'], dtype='object', name='x')
[right pandas]: Index(['10', '20', '15', '30', '45', 'None'], dtype='object', name='x')

The index is loaded as float64, so the follow step like astype would be diff with pandas

[1] https://github.com/apache/spark/blob/bcc595c112a23d8e3024ace50f0dbc7eab7144b2/python/pyspark/pandas/tests/indexes/test_base.py#L2249",,apachespark,yikunkero,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34849,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Oct 27 03:00:23 UTC 2021,,,,,,,,,,"0|z0tfww:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/21 00:20;gurwls223;cc [~itholic] [~XinrongM] [~ueshin] FYI;;;","19/Oct/21 08:18;yikunkero;revisit this, it had been already fixed in current master branch, will add a simple PR to optimized the testcase. 

{code:python}
pidx = pd.Index([10, 20, 15, 30, 45, None], name=""x"")
psidx = ps.Index(pidx)
self.assert_eq(psidx.astype(bool), pidx.astype(bool))
self.assert_eq(psidx.astype(str), pidx.astype(str))
{code}


;;;","20/Oct/21 02:35;apachespark;User 'Yikun' has created a pull request for this issue:
https://github.com/apache/spark/pull/34335;;;","20/Oct/21 05:12;gurwls223;Fixed in https://github.com/apache/spark/pull/34335;;;","27/Oct/21 03:00;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/34397;;;","27/Oct/21 03:00;apachespark;User 'itholic' has created a pull request for this issue:
https://github.com/apache/spark/pull/34397;;;",,,,,,,,,,
"In stage page, 'Aggregated Metrics by Executor' the underline displayed when the mouse is moved to the link is blocked",SPARK-36341,13392476,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,29/Jul/21 07:15,29/Jul/21 23:53,13/Jul/23 08:50,29/Jul/21 23:53,3.2.0,,,,,,,,3.3.0,,,,Web UI,,,,,0,,,,,!image-2021-07-29-15-15-50-593.png!,,angerszhuuu,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/21 07:15;angerszhuuu;image-2021-07-29-15-15-50-593.png;https://issues.apache.org/jira/secure/attachment/13031187/image-2021-07-29-15-15-50-593.png",,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 29 23:53:18 UTC 2021,,,,,,,,,,"0|z0tfkw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/21 07:16;angerszhuuu;Raise a pr soon;;;","29/Jul/21 07:27;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33571;;;","29/Jul/21 07:28;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33571;;;","29/Jul/21 23:53;srowen;Issue resolved by pull request 33571
[https://github.com/apache/spark/pull/33571];;;",,,,,,,,,,,,
aggsBuffer should collect AggregateExpression in the map range,SPARK-36339,13392454,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gaoyajun02,gaoyajun02,gaoyajun02,29/Jul/21 04:51,01/Sep/21 04:49,13/Jul/23 08:50,06/Aug/21 08:35,2.4.8,3.0.3,3.1.2,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,grouping,,,,"show demo for this ISSUE:
{code:java}
// SQL without error

SELECT name, count(name) c
FROM VALUES ('Alice'), ('Bob') people(name)
GROUP BY name GROUPING SETS(name);

// An error is reported after exchanging the order of the query columns:

SELECT count(name) c, name
FROM VALUES ('Alice'), ('Bob') people(name)
GROUP BY name GROUPING SETS(name);

{code}
The error message is：
{code:java}
Error in query: expression 'people.`name`' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() (or first_value) if you don't care which value you get.;;
Aggregate [name#5, spark_grouping_id#3], [count(name#1) AS c#0L, name#1]
+- Expand [List(name#1, name#4, 0)], [name#1, name#5, spark_grouping_id#3]
   +- Project [name#1, name#1 AS name#4]
      +- SubqueryAlias `people`
         +- LocalRelation [name#1]

{code}
So far, I have checked that there is no problem before version 2.3.

 

During debugging, I found that the behavior of constructAggregateExprs in ResolveGroupingAnalytics has changed.
{code:java}
    /*
     * Construct new aggregate expressions by replacing grouping functions.
     */
    private def constructAggregateExprs(
        groupByExprs: Seq[Expression],
        aggregations: Seq[NamedExpression],
        groupByAliases: Seq[Alias],
        groupingAttrs: Seq[Expression],
        gid: Attribute): Seq[NamedExpression] = aggregations.map {
      // collect all the found AggregateExpression, so we can check an expression is part of
      // any AggregateExpression or not.
      val aggsBuffer = ArrayBuffer[Expression]()
      // Returns whether the expression belongs to any expressions in `aggsBuffer` or not.
      def isPartOfAggregation(e: Expression): Boolean = {
        aggsBuffer.exists(a => a.find(_ eq e).isDefined)
      }
      replaceGroupingFunc(_, groupByExprs, gid).transformDown {
        // AggregateExpression should be computed on the unmodified value of its argument
        // expressions, so we should not replace any references to grouping expression
        // inside it.
        case e: AggregateExpression =>
          aggsBuffer += e
          e
        case e if isPartOfAggregation(e) => e
        case e =>
          // Replace expression by expand output attribute.
          val index = groupByAliases.indexWhere(_.child.semanticEquals(e))
          if (index == -1) {
            e
          } else {
            groupingAttrs(index)
          }
      }.asInstanceOf[NamedExpression]
    }

{code}
When performing aggregations.map, the aggsBuffer here seems to be outside the scope of the map. It can store the AggregateExpression of all the elements processed by the map function, but this is not before 2.3.",,apachespark,cloud_fan,fchen,gaoyajun02,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Sep 01 04:49:59 UTC 2021,,,,,,,,,,"0|z0tfg0:",9223372036854775807,,,,,,,,,,,,,3.1.2,,,,,,,,,,"29/Jul/21 08:23;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/33574;;;","06/Aug/21 08:35;cloud_fan;Issue resolved by pull request 33574
[https://github.com/apache/spark/pull/33574];;;","06/Aug/21 09:37;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/33667;;;","06/Aug/21 11:17;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/33669;;;","01/Sep/21 04:49;apachespark;User 'gaoyajun02' has created a pull request for this issue:
https://github.com/apache/spark/pull/33884;;;",,,,,,,,,,,
Spark sql creates staging dir inside database directory rather than creating inside table directory,SPARK-36327,13392292,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,senthh,senthh,senthh,28/Jul/21 08:50,27/Aug/21 19:59,13/Jul/23 08:50,27/Aug/21 19:59,3.1.2,,,,,,,,3.3.0,,,,Spark Core,SQL,,,,0,,,,,"Spark sql creates staging dir inside database directory rather than creating inside table directory.

 

This arises only when viewfs:// is configured. When the location is hdfs://, it doesn't occur.

 

Based on further investigation in file *SaveAsHiveFile.scala*, I could see that the directory hierarchy has been not properly handled for viewFS condition.
Parent path(db path) is passed rather than passing the actual directory(table location).

{{
// Mostly copied from Context.java#getExternalTmpPath of Hive 1.2
private def newVersionExternalTempPath(
path: Path,
hadoopConf: Configuration,
stagingDir: String): Path = {
val extURI: URI = path.toUri
if (extURI.getScheme == ""viewfs"")

{ getExtTmpPathRelTo(path.getParent, hadoopConf, stagingDir) }

else

{ new Path(getExternalScratchDir(extURI, hadoopConf, stagingDir), ""-ext-10000"") }

}
}}

Please refer below lines

===============================
if (extURI.getScheme == ""viewfs"") {
getExtTmpPathRelTo(path.getParent, hadoopConf, stagingDir)
===============================",,apachespark,dongjoon,petertoth,senthh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,Important,,,,,,,,,9223372036854775807,,,,Fri Aug 27 19:59:20 UTC 2021,,,,,,,,,,"0|z0teg0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"28/Jul/21 08:52;senthh;Shall I work on this Jira to fix this issue?;;;","29/Jul/21 11:04;senthh;Created PR https://github.com/apache/spark/pull/33577;;;","29/Jul/21 11:04;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33577;;;","29/Jul/21 11:04;apachespark;User 'senthh' has created a pull request for this issue:
https://github.com/apache/spark/pull/33577;;;","29/Jul/21 11:24;senthh;Hi [~dongjoon],  [~hyukjin.kwon]

 

Could you please review these minor changes? ;;;","30/Jul/21 07:21;dongjoon;I commented on the PR and looped other review, too. [~senthh].;;;","31/Jul/21 03:35;senthh;Hi [~sunchao]

Hive is creating .staging directories inside ""/db/table"" location but Spark-sql creates .staging directories inside /db/"" location when we use hadoop federation(viewFs). But works as expected (creating .staging inside /db/table/ location for other filesystems like hdfs).

HIVE:
{{
# beeline
> use dicedb;
> insert into table part_test partition (j=1) values (1);
...
INFO : Loading data to table dicedb.part_test partition (j=1) from **viewfs://cloudera/user/daisuke/dicedb/part_test/j=1/.hive-staging_hive_2021-07-19_13-04-44_989_6775328876605030677-1/-ext-10000**

}}

but spark's behaviour,

{{
spark-sql> use dicedb;
spark-sql> insert into table part_test partition (j=2) values (2);
21/07/19 13:07:37 INFO FileUtils: Creating directory if it doesn't exist: **viewfs://cloudera/user/daisuke/dicedb/.hive-staging_hive_2021-07-19_13-07-37_317_5083528872437596950-1**
... 
}}


The reason why we require this change is , if we allow spark-sql to create .staging directory inside /db/ location then we will end-up with security issues. We need to provide permission for ""viewfs:///db/"" location to all users who submit spark jobs.

After this change is applied spark-sql creates .staging inside /db/table/, similar to hive, as below,

{{
spark-sql> use dicedb;
21/07/28 00:22:47 INFO SparkSQLCLIDriver: Time taken: 0.929 seconds
spark-sql> insert into table part_test partition (j=8) values (8);
21/07/28 00:23:25 INFO HiveMetaStoreClient: Closed a connection to metastore, current connections: 1
21/07/28 00:23:26 INFO FileUtils: Creating directory if it doesn't exist: **viewfs://cloudera/user/daisuke/dicedb/part_test/.hive-staging_hive_2021-07-28_00-23-26_109_4548714524589026450-1** 
}}

The reason why we don't see this issue in Hive but only occurs in Spark-sql:

In hive, ""/db/table/tmp"" directory structure is passed for path and hence path.getParent returns ""db/table/"" . But in Spark we just pass ""/db/table"" so it is not required to use ""path.getParent"" for hadoop federation(viewfs)

 ;;;","27/Aug/21 19:59;dongjoon;Issue resolved by pull request 33577
[https://github.com/apache/spark/pull/33577];;;",,,,,,,,
Only skip AQEShuffleReadRule in the final stage if it breaks the distribution requirement,SPARK-36315,13392162,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,27/Jul/21 16:06,04/Sep/21 00:40,13/Jul/23 08:50,03/Aug/21 10:29,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-36666,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Aug 03 10:29:35 UTC 2021,,,,,,,,,,"0|z0tdn4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/21 16:36;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33541;;;","27/Jul/21 16:37;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33541;;;","03/Aug/21 10:29;cloud_fan;Issue resolved by pull request 33541
[https://github.com/apache/spark/pull/33541];;;",,,,,,,,,,,,,
Fix hasnan() window function in IndexOpsMixin,SPARK-36310,13391990,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,XinrongM,XinrongM,XinrongM,27/Jul/21 00:11,12/Dec/22 18:10,13/Jul/23 08:50,28/Jul/21 00:21,3.2.0,,,,,,,,3.2.0,,,,PySpark,,,,,0,,,,," 
{code:java}
File ""/__w/spark/spark/python/pyspark/pandas/groupby.py"", line 1497, in pyspark.pandas.groupby.GroupBy.rank
Failed example:
    df.groupby(""a"").rank().sort_index()
Exception raised:
...
pyspark.sql.utils.AnalysisException: It is not allowed to use a window function inside an aggregate function. Please use the inner window function in a sub-query.
{code}
As shown above, hasnans() used in ""rank"" causes ""It is not allowed to use a window function inside an aggregate function"" exception.

We shall adjust that.

 ",,apachespark,ueshin,XinrongM,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34849,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 28 00:21:48 UTC 2021,,,,,,,,,,"0|z0tckw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/21 21:45;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/33547;;;","28/Jul/21 00:21;gurwls223;Issue resolved by pull request 33547
[https://github.com/apache/spark/pull/33547];;;",,,,,,,,,,,,,,
ResolveAggregateFunctions should work with nested fields,SPARK-36275,13391605,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,allisonwang-db,allisonwang-db,allisonwang-db,23/Jul/21 17:51,28/Jul/21 05:35,13/Jul/23 08:50,28/Jul/21 05:35,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"A sort after Aggregate can fail to resolve if it contains nested fields. For example

{code:java}
SELECT c.x, SUM(c.y)
FROM VALUES NAMED_STRUCT('x', 'A', 'y', 1), NAMED_STRUCT('x', 'A', 'y', 2) AS t(c)
GROUP BY c.x
ORDER BY c.x
{code}

Error:
{code}
org.apache.spark.sql.AnalysisException: cannot resolve 'c.x' given input columns: [sum(c.y), x]; line 5 pos 9;
'Sort ['c.x ASC NULLS FIRST], true
+- Aggregate [c#0.x], [c#0.x AS x#2, sum(c#0.y) AS sum(c.y)#5L]
   +- SubqueryAlias t
      +- LocalRelation [c#0]
{code}

 ",,allisonwang-db,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 28 05:35:55 UTC 2021,,,,,,,,,,"0|z0ta7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/21 18:18;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/33498;;;","23/Jul/21 18:19;apachespark;User 'allisonwang-db' has created a pull request for this issue:
https://github.com/apache/spark/pull/33498;;;","28/Jul/21 05:35;cloud_fan;Issue resolved by pull request 33498
[https://github.com/apache/spark/pull/33498];;;",,,,,,,,,,,,,
Comparison of identical values,SPARK-36273,13391535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,almogtavor,almogtavor,almogtavor,23/Jul/21 10:24,03/Aug/21 00:48,13/Jul/23 08:50,23/Jul/21 17:48,3.1.3,,,,,,,,3.2.0,,,,Spark Core,,,,,0,,,,,"This PR fixes the use of the ""o.appAttemptId"" variable instead of the mistaken ""appAttemptId"" variable. The current situation is a comparison of identical values. Fix proposed on SPARK-36273 (https://github.com/apache/spark/pull/33493).",,almogtavor,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-30602,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 23 17:48:15 UTC 2021,,,,,,,,,,"0|z0t9rs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/21 11:06;apachespark;User 'almogtavor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33493;;;","23/Jul/21 17:48;srowen;Resolved by https://github.com/apache/spark/pull/33493;;;",,,,,,,,,,,,,,
Set the lowerbound of mypy version to 0.910,SPARK-36268,13391458,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,gurwls223,gurwls223,,23/Jul/21 02:03,12/Dec/22 18:10,13/Jul/23 08:50,23/Jul/21 03:28,3.2.0,,,,,,,,3.2.0,,,,PySpark,,,,,0,,,,,"https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/141519/console

{code}
python/pyspark/mllib/tree.pyi:29: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/tree.pyi:38: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/feature.pyi:34: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/feature.pyi:42: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/feature.pyi:48: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/feature.pyi:54: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/feature.pyi:76: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/feature.pyi:124: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/feature.pyi:165: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/clustering.pyi:45: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/clustering.pyi:72: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/classification.pyi:39: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
python/pyspark/mllib/classification.pyi:52: error: Overloaded function signatures 1 and 2 overlap with incompatible return types
Found 13 errors in 4 files (checked 314 source files)
1
{code}

Jenkins installed mypy at https://issues.apache.org/jira/browse/SPARK-32797 but seems the version installed is not same as GIthub Actions",,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 23 03:28:58 UTC 2021,,,,,,,,,,"0|z0t9ao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/21 02:08;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/33487;;;","23/Jul/21 02:09;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/33487;;;","23/Jul/21 03:28;gurwls223;Issue resolved by pull request 33487
[https://github.com/apache/spark/pull/33487];;;",,,,,,,,,,,,,
Upgrade ZSTD-JNI to 1.5.0-4,SPARK-36262,13391424,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,22/Jul/21 18:35,22/Jul/21 21:04,13/Jul/23 08:50,22/Jul/21 21:04,3.2.0,,,,,,,,3.2.0,,,,Build,,,,,0,,,,,"ZSTD-JNI 1.5.0-3 has a packaging issue.

https://github.com/luben/zstd-jni/issues/181#issuecomment-885138495",,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 22 21:04:40 UTC 2021,,,,,,,,,,"0|z0t934:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/21 19:04;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33483;;;","22/Jul/21 21:04;dongjoon;Issue resolved by pull request 33483
[https://github.com/apache/spark/pull/33483];;;",,,,,,,,,,,,,,
check string length for char/varchar and apply type coercion in UPDATE/MERGE command,SPARK-36247,13391184,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,21/Jul/21 18:09,27/Jul/21 05:57,13/Jul/23 08:50,27/Jul/21 05:57,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 27 05:57:49 UTC 2021,,,,,,,,,,"0|z0t7ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/21 18:15;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33468;;;","27/Jul/21 05:57;cloud_fan;Issue resolved by pull request 33468
[https://github.com/apache/spark/pull/33468];;;",,,,,,,,,,,,,,
WorkerDecommissionExtendedSuite flakes with GHA,SPARK-36246,13391176,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,holden,holden,holden,21/Jul/21 17:40,12/Dec/22 18:10,13/Jul/23 08:50,22/Jul/21 06:18,3.3.0,,,,,,,,3.1.3,3.2.0,,,Spark Core,Tests,,,,0,,,,,,,apachespark,holden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 22 06:18:10 UTC 2021,,,,,,,,,,"0|z0t7k0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/21 18:00;apachespark;User 'holdenk' has created a pull request for this issue:
https://github.com/apache/spark/pull/33467;;;","22/Jul/21 06:18;gurwls223;Fixed in https://github.com/apache/spark/pull/33467;;;",,,,,,,,,,,,,,
Upgrade zstd-jni to 1.5.0-3 to avoid a bug about buffer size calculation,SPARK-36244,13391144,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,21/Jul/21 14:19,22/Jul/21 02:37,13/Jul/23 08:50,22/Jul/21 02:37,3.2.0,3.3.0,,,,,,,3.2.0,,,,Build,,,,,0,,,,,"zstd-jni 1.5.0-3 was released few days ago.
This release resolves an issue about buffer size calculation, which can affect usage in Spark.
https://github.com/luben/zstd-jni/releases/tag/v1.5.0-3",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 22 02:37:36 UTC 2021,,,,,,,,,,"0|z0t7cw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/21 14:28;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33464;;;","22/Jul/21 02:37;dongjoon;Issue resolved by pull request 33464
[https://github.com/apache/spark/pull/33464];;;",,,,,,,,,,,,,,
Ensure spill file closed before set success to true in ExternalSorter.spillMemoryIteratorToDisk method,SPARK-36242,13391104,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,LuciferYang,LuciferYang,LuciferYang,21/Jul/21 09:38,27/Jul/21 17:20,13/Jul/23 08:50,23/Jul/21 15:20,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.0.4,3.1.3,3.2.0,3.3.0,Spark Core,,,,,0,,,,,"The processes of ExternalSorter.spillMemoryIteratorToDisk and ExternalAppendOnlyMap.spillMemoryIteratorToDisk are similar, but there are some differences in setting `success = true`

 

Code of ExternalSorter.spillMemoryIteratorToDisk as follows:

 
{code:java}
      if (objectsWritten > 0) {
        flush()
      } else {
        writer.revertPartialWritesAndClose()
      }
      success = true
    } finally {
      if (success) {
        writer.close()
      } else {
        ...
      }
    }{code}
Code of ExternalSorter.spillMemoryIteratorToDisk as follows:
{code:java}
  if (objectsWritten > 0) {
    flush()
    writer.close()
  } else {
    writer.revertPartialWritesAndClose()
  }
  success = true
} finally {
  if (!success) {
    ...
  }
}{code}
It seems that the processing of `ExternalSorter.spillMemoryIteratorToDisk` mehod is more reasonable, We should make sure setting `success = true` after the spill file is closed

 ",,apachespark,LuciferYang,Ngone51,zhuqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 26 03:21:36 UTC 2021,,,,,,,,,,"0|z0t740:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/21 10:00;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33460;;;","23/Jul/21 15:20;Ngone51;Issue resolved by https://github.com/apache/spark/pull/33460;;;","26/Jul/21 03:13;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33513;;;","26/Jul/21 03:14;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33512;;;","26/Jul/21 03:21;apachespark;User 'LuciferYang' has created a pull request for this issue:
https://github.com/apache/spark/pull/33514;;;",,,,,,,,,,,
SparkUI should bind handler after application started,SPARK-36237,13391068,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhuuu,angerszhuuu,angerszhuuu,21/Jul/21 06:21,02/Aug/21 11:36,13/Jul/23 08:50,02/Aug/21 11:36,3.2.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,"When we use prometheus to fetch metrics, always call before application started.

Then throw alot of exception not of NoSuchElementException
{code:java}

21/07/19 04:53:37 INFO Client: Preparing resources for our AM container
21/07/19 04:53:37 INFO Client: Uploading resource hdfs://tl3/packages/jars/spark-2.4-archive.tar.gz -> hdfs://R2/user/xiaoke.zhou/.sparkStaging/application_1624456325569_7143920/spark-2.4-archive.tar.gz
21/07/19 04:53:37 WARN JettyUtils: GET /jobs/ failed: java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.
java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:43)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:90)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:90)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(Htt
[2021-07-19 04:54:55,111] INFO - pChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
21/07/19 04:53:37 WARN ServletHandler: /jobs/
java.util.NoSuchElementException: Failed to get the application information. If you are starting up Spark, please wait a while until it's ready.
	at org.apache.spark.status.AppStatusStore.applicationInfo(AppStatusStore.scala:43)
	at org.apache.spark.ui.jobs.AllJobsPage.render(AllJobsPage.scala:275)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:90)
	at org.apache.spark.ui.WebUI$$anonfun$2.apply(WebUI.scala:90)
	at org.apache.spark.ui.JettyUtils$$anon$3.doGet(JettyUtils.scala:90)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:687)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:790)
	at org.spark_project.jetty.servlet.ServletHolder.handle(ServletHolder.java:848)
	at org.spark_project.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585)
	at org.spark_project.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1180)
	at org.spark_project.jetty.servlet.ServletHandler.doScope(ServletHandler.java:513)
	at org.spark_project.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1112)
	at org.spark_project.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141)
	at org.spark_project.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:493)
	at org.spark_project.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:213)
	at org.spark_project.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:134)
	at org.spark_project.jetty.server.Server.handle(Server.java:539)
	at org.spark_project.jetty.server.HttpChannel.handle(HttpChannel.java:333)
	at org.spark_project.jetty.server.HttpConnection.onFillable(HttpConnection.java:251)
	at org.spark_project.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:283)
	at org.spark_project.jetty.io.FillInterest.fillable(FillInterest.java:108)
	at org.spark_project.jetty.io.SelectChannelEndPoint$2.run(SelectChannelEndPoint.java:93)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.executeProduceConsume(ExecuteProduceConsume.java:303)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.produceConsume(ExecuteProduceConsume.java:148)
	at org.spark_project.jetty.util.thread.strategy.ExecuteProduceConsume.run(ExecuteProduceConsume.java:136)
	at org.spark_project.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:671)
	at org.spark_project.jetty.util.thread.QueuedThreadPool$2.run(QueuedThreadPool.java:589)
	at java.lang.Thread.run(Thread.java:748)
{code}",,angerszhuuu,apachespark,Gengliang.Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 02 11:36:49 UTC 2021,,,,,,,,,,"0|z0t6w0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/21 07:00;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33457;;;","02/Aug/21 11:36;Gengliang.Wang;Issue resolved by pull request 33457
[https://github.com/apache/spark/pull/33457];;;",,,,,,,,,,,,,,
conv() inconsistently handles invalid strings with > 64 invalid characters,SPARK-36229,13390987,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dc-heros,tarmstrong,tarmstrong,20/Jul/21 19:23,28/Jul/21 16:19,13/Jul/23 08:50,28/Jul/21 16:19,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"1/ SPARK-33428 fixed ArrayIndexOutofBoundsException but introduced a new inconsistency in behaviour where the returned value is different above the 64 char threshold.

 
{noformat}
scala> spark.sql(""select conv(repeat('?', 64), 10, 16)"").show
+---------------------------+
|conv(repeat(?, 64), 10, 16)|
+---------------------------+
|                          0|
+---------------------------+




scala> spark.sql(""select conv(repeat('?', 65), 10, 16)"").show
+---------------------------+
|conv(repeat(?, 65), 10, 16)|
+---------------------------+
|           FFFFFFFFFFFFFFFF|
+---------------------------+




scala> spark.sql(""select conv(repeat('?', 65), 10, -16)"").show
+----------------------------+
|conv(repeat(?, 65), 10, -16)|
+----------------------------+
|                          -1|
+----------------------------+




scala> spark.sql(""select conv(repeat('?', 64), 10, -16)"").show
+----------------------------+
|conv(repeat(?, 64), 10, -16)|
+----------------------------+
|                           0|
+----------------------------+{noformat}
 

2/ conv should return result equal to max unsigned long value in base toBase when there is overflow
{code:java}
scala> spark.sql(select conv('aaaaaaa0aaaaaaa0a', 16, 10)).show 
// which should be 18446744073709551615

+-------------------------------+
|conv(aaaaaaa0aaaaaaa0a, 16, 10)|
+-------------------------------+
|           12297828695278266890|
+-------------------------------+
{code}",,apachespark,cloud_fan,dc-heros,tarmstrong,,,,,,,,,,,,,,,,,,,,SPARK-33428,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 28 16:19:42 UTC 2021,,,,,,,,,,"0|z0t6e0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/21 19:24;tarmstrong;[~dgd_contributor] [~wenchen];;;","21/Jul/21 03:06;dc-heros;thanks, I will look into this

 ;;;","21/Jul/21 05:44;dc-heros;After look closely, I found out that the overflow check in encode is wrong and need to work on too.

For example:
{code:java}
scala> spark.sql(select conv('aaaaaaa0aaaaaaa0a', 16, 10)).show

+-------------------------------+
|conv(aaaaaaa0aaaaaaa0a, 16, 10)|
+-------------------------------+
|           12297828695278266890|
+-------------------------------+{code}
which should be 18446744073709551615

 

I will raise a pull request soon;;;","21/Jul/21 07:59;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33459;;;","28/Jul/21 16:19;cloud_fan;Issue resolved by pull request 33459
[https://github.com/apache/spark/pull/33459];;;",,,,,,,,,,,
Skip splitting a reducer partition when some mapStatus is null,SPARK-36228,13390966,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,20/Jul/21 16:35,21/Jul/21 14:19,13/Jul/23 08:50,21/Jul/21 14:19,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 21 14:19:04 UTC 2021,,,,,,,,,,"0|z0t69c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/21 16:57;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33445;;;","21/Jul/21 14:19;cloud_fan;Issue resolved by pull request 33445
[https://github.com/apache/spark/pull/33445];;;",,,,,,,,,,,,,,
python docstring referencing non existing Dataset class,SPARK-36225,13390889,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dominikgehl,dominikgehl,dominikgehl,20/Jul/21 10:15,12/Dec/22 18:10,13/Jul/23 08:50,24/Jul/21 07:58,3.1.2,,,,,,,,3.1.3,3.2.0,,,PySpark,,,,,0,,,,,Some python docstrings contain ```:class:`Dataset```` although there is no pyspark Dataset class,,apachespark,dominikgehl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 24 07:58:57 UTC 2021,,,,,,,,,,"0|z0t5s8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/21 10:57;apachespark;User 'dominikgehl' has created a pull request for this issue:
https://github.com/apache/spark/pull/33438;;;","20/Jul/21 10:58;apachespark;User 'dominikgehl' has created a pull request for this issue:
https://github.com/apache/spark/pull/33438;;;","24/Jul/21 07:58;gurwls223;Issue resolved by pull request 33438
[https://github.com/apache/spark/pull/33438];;;",,,,,,,,,,,,,
Normalize PartitionSpec for DescTable with PartitionSpec,SPARK-36213,13390751,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,19/Jul/21 17:03,23/Jul/21 22:08,13/Jul/23 08:50,21/Jul/21 16:55,2.4.8,3.0.3,3.1.2,3.2.0,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,!image-2021-07-20-16-26-09-456.png!,,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/21 08:26;Qin Yao;image-2021-07-20-16-26-09-456.png;https://issues.apache.org/jira/secure/attachment/13030866/image-2021-07-20-16-26-09-456.png",,,1.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 23 22:08:07 UTC 2021,,,,,,,,,,"0|z0t4xk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/21 17:56;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33424;;;","21/Jul/21 16:55;Qin Yao;Issue resolved by pull request 33424
[https://github.com/apache/spark/pull/33424];;;","23/Jul/21 22:08;apachespark;User 'viirya' has created a pull request for this issue:
https://github.com/apache/spark/pull/33504;;;",,,,,,,,,,,,,
type check fails for `F.udf(...).asNonDeterministic(),SPARK-36211,13390730,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,luran.he,luran.he,luran.he,19/Jul/21 14:48,27/Jul/21 07:15,13/Jul/23 08:50,27/Jul/21 07:15,3.1.2,,,,,,,,3.1.3,3.2.0,3.3.0,,PySpark,,,,,0,,,,,"The following code should type-check, but doesn't:
----

{{import uuid}}

{{pyspark.sql.functions as F}}

{{my_udf = F.udf(lambda: str(uuid.uuid4())).asNondeterministic()}}

----
In {{python/pyspark/sql/functions.pyi}} the {{udf}} signature is wrong",,apachespark,luran.he,xkrogen,zero323,,,,,,172800,172800,,0%,172800,172800,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,Python,Tue Jul 27 07:13:55 UTC 2021,,,,,,,,,,"0|z0t4sw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/21 14:54;apachespark;User 'luranhe' has created a pull request for this issue:
https://github.com/apache/spark/pull/33399;;;","27/Jul/21 07:13;zero323;Issue resolved by pull request 33399
https://github.com/apache/spark/pull/33399;;;",,,,,,,,,,,,,,
Preserve column insertion order in Dataset.withColumns,SPARK-36210,13390714,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,koert,koert,koert,19/Jul/21 14:17,20/Jul/21 20:34,13/Jul/23 08:50,20/Jul/21 16:11,3.0.3,3.1.2,3.2.0,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,"Dataset.withColumns uses a Map (columnMap) to store the mapping of column name to column. however this loses the order of the columns. also none of the operations used on the Map (find and filter) benefits from the map's lookup features. so it seems simpler to use a Seq instead, which also preserves the insertion order.",,apachespark,koert,viirya,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 20 16:11:57 UTC 2021,,,,,,,,,,"0|z0t4pc:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/21 15:55;apachespark;User 'koertkuipers' has created a pull request for this issue:
https://github.com/apache/spark/pull/33423;;;","19/Jul/21 15:56;apachespark;User 'koertkuipers' has created a pull request for this issue:
https://github.com/apache/spark/pull/33423;;;","20/Jul/21 16:11;viirya;Issue resolved by pull request 33423
[https://github.com/apache/spark/pull/33423];;;",,,,,,,,,,,,,
https://spark.apache.org/docs/latest/sql-programming-guide.html contains invalid link to Python doc,SPARK-36209,13390700,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dominikgehl,dominikgehl,dominikgehl,19/Jul/21 13:09,22/Jul/21 13:08,13/Jul/23 08:50,22/Jul/21 13:08,3.1.2,,,,,,,,3.1.3,3.2.0,,,Documentation,,,,,0,,,,,"On https://spark.apache.org/docs/latest/sql-programming-guide.html , the link to the python doc points to https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame which returns a ""Not found""
","On https://spark.apache.org/docs/latest/sql-programming-guide.html, the link to the python doc points to https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame which returns a ""Not found""",apachespark,dominikgehl,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-34606,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 22 13:08:43 UTC 2021,,,,,,,,,,"0|z0t4m8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/21 13:55;apachespark;User 'dominikgehl' has created a pull request for this issue:
https://github.com/apache/spark/pull/33420;;;","22/Jul/21 13:08;srowen;Resolved by https://github.com/apache/spark/pull/33420;;;",,,,,,,,,,,,,,
InputFormat of PartitionDesc is not respected,SPARK-36197,13390545,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,Qin Yao,Qin Yao,Qin Yao,18/Jul/21 14:29,16/Apr/22 20:25,13/Jul/23 08:50,19/Jul/21 08:00,3.1.2,3.2.0,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"A hive partition can have different PartitionDesc from TableDesc for describing Serde/InputFormatClass/OutputFormatClass, for a hive partitioned table, we shall respect this information in PartitionDesc first.",,apachespark,Qin Yao,,,,,,,,,,,,,,,,,,,SPARK-38536,SPARK-36440,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 19 08:00:44 UTC 2021,,,,,,,,,,"0|z0t3ns:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/21 14:36;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33406;;;","18/Jul/21 14:37;apachespark;User 'yaooqinn' has created a pull request for this issue:
https://github.com/apache/spark/pull/33406;;;","19/Jul/21 08:00;Qin Yao;Issue resolved by pull request 33406
[https://github.com/apache/spark/pull/33406];;;",,,,,,,,,,,,,
Set MaxMetaspaceSize JVM option to 2g,SPARK-36195,13390265,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,17/Jul/21 18:56,19/Jul/21 05:56,13/Jul/23 08:50,18/Jul/21 17:16,3.1.2,3.2.0,,,,,,,3.1.3,3.2.0,,,Build,,,,,0,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 18 17:16:35 UTC 2021,,,,,,,,,,"0|z0t1xs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/21 18:58;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33405;;;","18/Jul/21 17:16;dongjoon;This is resolved via https://github.com/apache/spark/pull/33405;;;",,,,,,,,,,,,,,
Recover SparkSubmit.runMain not to stop SparkContext in non-K8s env,SPARK-36193,13390211,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,17/Jul/21 09:38,19/Jul/21 05:27,13/Jul/23 08:50,19/Jul/21 05:27,3.1.2,3.2.0,,,,,,,3.1.3,3.2.0,,,Spark Core,,,,,0,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 19 05:27:03 UTC 2021,,,,,,,,,,"0|z0t1ls:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/21 09:41;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33403;;;","17/Jul/21 09:42;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33403;;;","19/Jul/21 05:27;dongjoon;Issue resolved by pull request 33403
[https://github.com/apache/spark/pull/33403];;;",,,,,,,,,,,,,
Make 'spark.sql.sources.disabledJdbcConnProviderList' as a static conf (as documneted),SPARK-36169,13389934,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,gurwls223,gurwls223,,16/Jul/21 01:01,12/Dec/22 18:11,13/Jul/23 08:50,16/Jul/21 02:43,3.3.0,,,,,,,,3.3.0,,,,SQL,,,,,0,,,,,{{spark.sql.sources.disabledJdbcConnProviderList}} is supposed to be a static config (it doesn't work runtime anyway) but it's currently placed as a runtime config,,apachespark,,,,,,,,,,,,,,,,,,,,,,,,,,,,,SPARK-32047,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 16 02:44:03 UTC 2021,,,,,,,,,,"0|z0szw8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/21 02:26;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/33381;;;","16/Jul/21 02:27;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/33381;;;","16/Jul/21 02:44;gurwls223;fixed in https://github.com/apache/spark/pull/33381;;;",,,,,,,,,,,,,
Replace 'python' to 'python3' in dev/test-dependencies.sh ,SPARK-36159,13389832,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Trivial,Fixed,gurwls223,gurwls223,,15/Jul/21 11:41,12/Dec/22 17:51,13/Jul/23 08:50,15/Jul/21 14:59,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.0.4,3.1.3,3.2.0,,Build,,,,,0,,,,,There's one last place to change python to python3 at dev/test-dependencies.sh. This is a followup of SPARK-29672,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 15 14:59:17 UTC 2021,,,,,,,,,,"0|z0sz9k:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/21 11:45;apachespark;User 'HyukjinKwon' has created a pull request for this issue:
https://github.com/apache/spark/pull/33368;;;","15/Jul/21 14:59;dongjoon;Issue resolved by pull request 33368
[https://github.com/apache/spark/pull/33368];;;",,,,,,,,,,,,,,
UnwrapCastInBinaryComparison fail when in.list contain CheckOverflow expression,SPARK-36130,13389513,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,fchen,fchen,fchen,14/Jul/21 03:17,12/Dec/22 18:11,13/Jul/23 08:50,14/Jul/21 07:58,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"UnwrapCastInBinaryComparison fail with the following case
 
{code:java}
CREATE TABLE tbl (d decimal(33, 27)) USING PARQUET 
SELECT d FROM tbl WHERE d NOT IN (d + 1){code}
 
Here {{in.list}} can be a {{CheckOverflow}} expression:
{code:java}
 cast(d#240 as decimal(34,27)) IN (CheckOverflow((promote_precision(cast(d#240 as decimal(34,27))) + 1.000000000000000000000000000), DecimalType(34,27), true)){code}
{{}}",,apachespark,cloud_fan,fchen,,,,,,,,,,,,,,,,,,,,,SPARK-35316,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 14 07:58:00 UTC 2021,,,,,,,,,,"0|z0sxbk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/21 03:45;gurwls223;cc [~sunchao] FYI;;;","14/Jul/21 03:54;fchen;Hi, [~hyukjin.kwon],

Copy from [https://github.com/apache/spark/pull/32488#issuecomment-879315179] And I'm working on this.;;;","14/Jul/21 05:42;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33335;;;","14/Jul/21 05:43;apachespark;User 'cfmcgrady' has created a pull request for this issue:
https://github.com/apache/spark/pull/33335;;;","14/Jul/21 07:58;cloud_fan;Issue resolved by pull request 33335
[https://github.com/apache/spark/pull/33335];;;",,,,,,,,,,,
Upgrade commons-compress to 1.21 to deal with CVEs,SPARK-36129,13389510,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,14/Jul/21 03:14,14/Jul/21 14:02,13/Jul/23 08:50,14/Jul/21 05:53,2.4.8,3.0.3,3.1.2,3.2.0,3.3.0,,,,3.0.4,3.1.3,3.2.0,,Build,,,,,0,,,,,"Some CVEs which affect commons-compress 1.20 are reported and fixed in 1.21.
https://commons.apache.org/proper/commons-compress/security-reports.html",,apachespark,dongjoon,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 14 06:13:57 UTC 2021,,,,,,,,,,"0|z0sxaw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/21 03:24;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33333;;;","14/Jul/21 03:25;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33333;;;","14/Jul/21 05:53;dongjoon;Issue resolved by pull request 33333
[https://github.com/apache/spark/pull/33333];;;","14/Jul/21 06:13;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33338;;;","14/Jul/21 06:13;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33337;;;",,,,,,,,,,,
CatalogFileIndex.filterPartitions should respect spark.sql.hive.metastorePartitionPruning,SPARK-36128,13389494,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,csun,csun,csun,14/Jul/21 00:41,12/Dec/22 18:11,13/Jul/23 08:50,16/Jul/21 20:33,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,Currently the config {{spark.sql.hive.metastorePartitionPruning}} is only used in {{PruneHiveTablePartitions}} but not {{PruneFileSourcePartitions}}. The latter calls {{CatalogFileIndex.filterPartitions}} which calls {{listPartitionsByFilter}} regardless of whether the above config is set or not. ,,apachespark,csun,viirya,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 16 20:33:15 UTC 2021,,,,,,,,,,"0|z0sx7c:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/21 03:51;gurwls223;hm, isn't {{spark.sql.hive.metastorePartitionPruning}} only for Hive table scan? ;;;","14/Jul/21 04:24;csun;[~hyukjin.kwon] you are right - I didn't know this config is designed to be only used by Hive table scan, but this poses a few issues:
1. by default, data source tables also manage their partitions through HMS, via config {{spark.sql.hive.manageFilesourcePartitions}}. This config also says ""When partition management is enabled, datasource tables store partition in the Hive metastore, and use the metastore to prune partitions during query planning"", so it sounds like they should have the same partition pruning mechanism as Hive tables, including the flag.
2. there is effectively only one implementation for {{ExternalCatalog}} which is HMS, so I'm not sure why we treat Hive table scans differently than data source table scans, even though both of them are reading partition metadata from HMS.;;;","14/Jul/21 04:40;gurwls223;That's okay. I was just thinking that we might have to have a separate config instead of reusing {{spark.sql.hive.metastorePartitionPruning}}.;;;","14/Jul/21 05:24;csun;Thanks, I'm slightly inclined to reuse the existing config but document the new behavior (e.g., it is used for data source tables too when {{spark.sql.hive.manageFilesourcePartitions}} is set). Let me raise a PR for this and we can discuss there.;;;","14/Jul/21 16:47;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33348;;;","14/Jul/21 16:48;apachespark;User 'sunchao' has created a pull request for this issue:
https://github.com/apache/spark/pull/33348;;;","16/Jul/21 20:33;viirya;Issue resolved by pull request 33348
[https://github.com/apache/spark/pull/33348];;;",,,,,,,,,
Spark does not passon needClientAuth to Jetty SSLContextFactory. Does not allow to configure mTLS authentication.,SPARK-36122,13389405,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,apachespark,skhandrikagmail,skhandrikagmail,13/Jul/21 14:27,17/Jul/21 14:00,13/Jul/23 08:50,17/Jul/21 14:00,2.4.7,,,,,,,,3.3.0,,,,Security,,,,,0,,,,,Spark does not pass on the needClientAuth flag to Jetty engine. This prevents the UI from honouring mutual TLS authentication using x509 certificates.,,apachespark,skhandrikagmail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Jul 17 14:00:05 UTC 2021,,,,,,,,,,"0|z0swnk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/21 11:17;apachespark;User 'skhandrikagmail' has created a pull request for this issue:
https://github.com/apache/spark/pull/33301;;;","17/Jul/21 14:00;srowen;Issue resolved by pull request 33301
[https://github.com/apache/spark/pull/33301];;;",,,,,,,,,,,,,,
Update the document about the behavior change of trimming characters for cast,SPARK-36081,13388845,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,sarutak,sarutak,sarutak,10/Jul/21 19:59,13/Jul/21 12:30,13/Jul/23 08:50,13/Jul/21 12:30,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,"sql-migration-guide.md mentions about the behavior of cast like as follows.
{code}
In Spark 3.0, when casting string value to integral types(tinyint, smallint, int and bigint), datetime types(date, timestamp and interval) and boolean type, the leading and trailing whitespaces (<= ASCII 32) will be trimmed before converted to these type values, for example, `cast(' 1\t' as int)` results `1`, `cast(' 1\t' as boolean)` results `true`, `cast('2019-10-10\t as date)` results the date value `2019-10-10`. In Spark version 2.4 and below, when casting string to integrals and booleans, it does not trim the whitespaces from both ends; the foregoing results is `null`, while to datetimes, only the trailing spaces (= ASCII 32) are removed.
{code}

In fact,  select cast('2019-10-10\b' as date); returns 2019-10-10 in Spark 3.0.0. 
But SPARK-32559 changed this behavior and since 3.0.1, the query returns NULL.",,apachespark,cloud_fan,sarutak,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 13 12:30:24 UTC 2021,,,,,,,,,,"0|z0st74:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"10/Jul/21 20:04;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33287;;;","10/Jul/21 20:04;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33287;;;","13/Jul/21 12:30;cloud_fan;Issue resolved by pull request 33287
[https://github.com/apache/spark/pull/33287];;;",,,,,,,,,,,,,
Null-based filter estimates should always be non-negative,SPARK-36079,13388751,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,karenfeng,karenfeng,karenfeng,09/Jul/21 22:57,20/Jul/21 13:51,13/Jul/23 08:50,20/Jul/21 13:51,3.2.0,,,,,,,,3.0.4,3.1.3,3.2.0,,SQL,,,,,0,,,,,"It's possible for a column's statistics to have a higher `nullCount` than the table's `rowCount`. In this case, the filter estimates come back outside of the reasonable range (between 0 and 1).",,apachespark,cloud_fan,karenfeng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 20 13:51:38 UTC 2021,,,,,,,,,,"0|z0ssm8:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/21 23:05;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/33286;;;","09/Jul/21 23:06;apachespark;User 'karenfeng' has created a pull request for this issue:
https://github.com/apache/spark/pull/33286;;;","20/Jul/21 13:51;cloud_fan;Issue resolved by pull request 33286
[https://github.com/apache/spark/pull/33286];;;",,,,,,,,,,,,,
[SQL] ArrayIndexOutOfBounds in CAST string to timestamp,SPARK-36076,13388718,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dc-heros,andygrove,andygrove,09/Jul/21 18:04,15/Jul/21 04:17,13/Jul/23 08:50,15/Jul/21 04:16,3.0.3,3.1.1,3.1.2,,,,,,3.0.4,3.1.3,,,SQL,,,,,0,,,,,"I discovered this bug during some fuzz testing.
{code:java}
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.1.1
      /_/
         
Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_282)
Type in expressions to have them evaluated.
Type :help for more information.scala> 

scala> import org.apache.spark.sql.types.DataTypes

scala> val df = Seq("":8:434421+ 98:38"").toDF(""c0"")
df: org.apache.spark.sql.DataFrame = [c0: string]

scala> val df2 = df.withColumn(""c1"", col(""c0"").cast(DataTypes.TimestampType))
df2: org.apache.spark.sql.DataFrame = [c0: string, c1: timestamp]

scala> df2.show
java.lang.ArrayIndexOutOfBoundsException: 9
  at org.apache.spark.sql.catalyst.util.DateTimeUtils$.stringToTimestamp(DateTimeUtils.scala:328)
  at org.apache.spark.sql.catalyst.expressions.CastBase.$anonfun$castToTimestamp$2(Cast.scala:455)
  at org.apache.spark.sql.catalyst.expressions.CastBase.buildCast(Cast.scala:295)
  at org.apache.spark.sql.catalyst.expressions.CastBase.$anonfun$castToTimestamp$1(Cast.scala:451)
  at org.apache.spark.sql.catalyst.expressions.CastBase.nullSafeEval(Cast.scala:840)
  at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:476)
 {code}",,andygrove,apachespark,dongjoon,xkrogen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 15 04:16:39 UTC 2021,,,,,,,,,,"0|z0ssew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"12/Jul/21 04:12;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33293;;;","13/Jul/21 15:06;apachespark;User 'dgd-contributor' has created a pull request for this issue:
https://github.com/apache/spark/pull/33325;;;","15/Jul/21 04:16;dongjoon;Issue resolved by pull request 33325
[https://github.com/apache/spark/pull/33325];;;",,,,,,,,,,,,,
No tests in hadoop-cloud run unless hadoop-3.2 profile is activated explicitly,SPARK-36068,13388555,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,09/Jul/21 07:28,12/Dec/22 18:10,13/Jul/23 08:50,05/Aug/21 00:40,3.2.0,3.3.0,,,,,,,3.2.0,,,,Build,Tests,,,,0,,,,,"No tests in hadoop-cloud are compiled and run unless hadoop-3.2 profile is activated explicitly.
This issue is similar to SPARK-36067.",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Aug 05 00:40:04 UTC 2021,,,,,,,,,,"0|z0srew:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/21 07:38;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33277;;;","09/Jul/21 07:39;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33277;;;","09/Jul/21 08:26;gurwls223;Issue resolved by pull request 33277
[https://github.com/apache/spark/pull/33277];;;","09/Jul/21 09:03;gurwls223;Reverted at https://github.com/apache/spark/commit/951e84f1b91fc2ac09b3afbe51bdd68af62d26fb;;;","05/Aug/21 00:40;gurwls223;Issue resolved by pull request 33277
[https://github.com/apache/spark/pull/33277];;;",,,,,,,,,,,
YarnClusterSuite fails due to NoClassDefFoundError unless hadoop-3.2 profile is activated explicitly,SPARK-36067,13388535,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,09/Jul/21 04:33,12/Dec/22 18:10,13/Jul/23 08:50,09/Jul/21 06:19,3.2.0,3.3.0,,,,,,,3.2.0,,,,Build,Tests,YARN,,,0,,,,,"YarnClusterSuite will fails due to NoClassDefFoundError unless hadoop-3.2 profile is activated explicitly regardless of building with SBT or Maven.
{code:java}
build/sbt -Pyarn ""yarn/testOnly org.apache.spark.deploy.yarn.YarnClusterSuite""
...
[info] YarnClusterSuite:
[info] org.apache.spark.deploy.yarn.YarnClusterSuite *** ABORTED *** (598 milliseconds)
[info]   java.lang.NoClassDefFoundError: org/bouncycastle/operator/OperatorCreationException
[info]   at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceInit(ResourceManager.java:888)
[info]   at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
[info]   at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndInitActiveServices(ResourceManager.java:1410)
[info]   at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:344)
[info]   at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)
[info]   at org.apache.hadoop.yarn.server.MiniYARNCluster.initResourceManager(MiniYARNCluster.java:359)
{code}",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 09 06:19:29 UTC 2021,,,,,,,,,,"0|z0srag:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/21 05:05;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33276;;;","09/Jul/21 06:19;gurwls223;Issue resolved by pull request 33276
[https://github.com/apache/spark/pull/33276];;;",,,,,,,,,,,,,,
Introduce pending pod limit for Spark on K8s,SPARK-36052,13388374,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,08/Jul/21 11:40,28/Jan/22 01:07,13/Jul/23 08:50,11/Aug/21 03:16,3.0.3,3.1.2,3.2.0,3.3.0,,,,,3.2.0,3.3.0,,,Kubernetes,,,,,0,releasenotes,,,,Introduce a new configuration to limit the number of pending PODs for Spark on K8S as the K8S scheduler could be overloaded with requests which slows down the resource allocations (especially in case of dynamic allocation).,,apachespark,attilapiros,dongjoon,zhuqi,,,,,,,,,,,,,,,,,,,,,,,SPARK-36060,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Aug 16 23:17:21 UTC 2021,,,,,,,,,,"0|z0sqao:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/21 09:06;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/33492;;;","11/Aug/21 03:16;dongjoon;Issue resolved by pull request 33492
[https://github.com/apache/spark/pull/33492];;;","16/Aug/21 23:16;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33755;;;","16/Aug/21 23:17;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33755;;;",,,,,,,,,,,,
Regression: Remote blocks stored on disk by BlockManager are not deleted,SPARK-36036,13388267,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,dtarima,dtarima,dtarima,08/Jul/21 02:11,11/Jul/21 16:54,13/Jul/23 08:50,11/Jul/21 16:54,3.3.0,,,,,,,,3.3.0,,,,Spark Core,,,,,0,,,,,"There was a regression since Spark started storing large remote files on disk (https://issues.apache.org/jira/browse/SPARK-22062). A refactoring introduced a hidden reference preventing the auto-deletion of the files (https://github.com/apache/spark/commit/a97001d21757ae214c86371141bd78a376200f66#diff-42a673b8fa5f2b999371dc97a5de7ebd2c2ec19447353d39efb7e8ebc012fe32L1677). Since then all underlying files of DownloadFile instances are kept on disk for the duration of the Spark application which sometimes results in ""no space left"" errors.",,apachespark,dtarima,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 11 16:54:59 UTC 2021,,,,,,,,,,"0|z0spmw:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/21 02:21;apachespark;User 'dtarima' has created a pull request for this issue:
https://github.com/apache/spark/pull/33251;;;","08/Jul/21 03:26;dtarima;It look like the bug exists in versions starting with `2.3.2`;;;","11/Jul/21 16:54;srowen;Issue resolved by pull request 33251
[https://github.com/apache/spark/pull/33251];;;",,,,,,,,,,,,,
Upgrade Kubernetes Client Version to 5.5.0,SPARK-36026,13387955,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,attilapiros,attilapiros,attilapiros,06/Jul/21 14:37,12/Dec/22 18:10,13/Jul/23 08:50,07/Jul/21 04:03,3.2.0,,,,,,,,3.3.0,,,,Kubernetes,,,,,0,,,,,"This way SPARK-35334 can be done as version 5.5.0 contains https://github.com/fabric8io/kubernetes-client/issues/3087.

",,apachespark,attilapiros,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Wed Jul 07 04:03:19 UTC 2021,,,,,,,,,,"0|z0snq0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/21 15:10;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/33233;;;","06/Jul/21 15:10;apachespark;User 'attilapiros' has created a pull request for this issue:
https://github.com/apache/spark/pull/33233;;;","07/Jul/21 04:03;gurwls223;Fixed in https://github.com/apache/spark/pull/33233;;;",,,,,,,,,,,,,
Check logical link in remove redundant projects,SPARK-36020,13387778,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,cloud_fan,cloud_fan,cloud_fan,05/Jul/21 19:18,20/Jul/21 16:01,13/Jul/23 08:50,06/Jul/21 13:19,3.2.0,,,,,,,,3.1.3,3.2.0,,,SQL,,,,,0,,,,,,,apachespark,cloud_fan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 20 16:01:05 UTC 2021,,,,,,,,,,"0|z0smmo:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/21 19:37;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33222;;;","07/Jul/21 02:14;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/33238;;;","20/Jul/21 16:01;apachespark;User 'cloud-fan' has created a pull request for this issue:
https://github.com/apache/spark/pull/33442;;;",,,,,,,,,,,,,
Use uuid as app id in kubernetes client mode,SPARK-36014,13387651,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,ulysses,ulysses,ulysses,05/Jul/21 07:38,18/Jul/21 22:44,13/Jul/23 08:50,18/Jul/21 22:44,3.2.0,,,,,,,,3.3.0,,,,Kubernetes,,,,,0,,,,,"Currently, spark on kubernetes with client mode would use `""spark-application-"" + System.currentTimeMillis` as app id by default. It would cause app id conflict if submit several spark applications to kubernetes cluster in a short time.

Unfortunately, the event log use app id as the file name. With the conflict event log file, the exception was thrown.
{code:java}
Caused by: java.io.FileNotFoundException: File does not exist: xxx/spark-application-1624766876324.lz4.inprogress (inode 5984170846) Holder does not have any open files.
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2697)
        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:521)
        at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:161)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2579)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:846)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:510)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:503)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:871)
        at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:817)
{code}",,apachespark,dongjoon,ulysses,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sun Jul 18 22:44:13 UTC 2021,,,,,,,,,,"0|z0slug:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/21 07:46;apachespark;User 'ulysses-you' has created a pull request for this issue:
https://github.com/apache/spark/pull/33211;;;","18/Jul/21 22:44;dongjoon;Issue resolved by pull request 33211
[https://github.com/apache/spark/pull/33211];;;",,,,,,,,,,,,,,
Upgrade Dropwizard Metrics to 4.2.2,SPARK-36013,13387636,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,sarutak,sarutak,sarutak,05/Jul/21 05:31,12/Dec/22 18:10,13/Jul/23 08:50,05/Jul/21 08:50,3.2.0,3.3.0,,,,,,,3.3.0,,,,Build,,,,,0,,,,,"Dropwizard 4.2.1 fixes a bug related to JMXReporter but 4.2.1 also contains a  bug. so upgrading to 4.2.2 seems better.
https://github.com/dropwizard/metrics/releases/tag/v4.2.1
https://github.com/dropwizard/metrics/releases/tag/v4.2.2",,apachespark,sarutak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 05 08:50:45 UTC 2021,,,,,,,,,,"0|z0slr4:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/21 05:38;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33209;;;","05/Jul/21 05:39;apachespark;User 'sarutak' has created a pull request for this issue:
https://github.com/apache/spark/pull/33209;;;","05/Jul/21 08:50;gurwls223;Issue resolved by pull request 33209
[https://github.com/apache/spark/pull/33209];;;",,,,,,,,,,,,,
Lost the null flag info when show create table,SPARK-36012,13387632,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,xiaopenglei,xiaopenglei,xiaopenglei,05/Jul/21 05:16,08/Jul/21 17:23,13/Jul/23 08:50,08/Jul/21 17:23,3.2.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"When execute `SHOW CREATE TALBE XXX` command, the ddl info lost the null flag.
{code:java}
// def toDDL: String = s""${quoteIdentifier(name)} ${dataType.sql}$getDDLComment""
{code}
 ",,apachespark,cloud_fan,xiaopenglei,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Thu Jul 08 17:23:13 UTC 2021,,,,,,,,,,"0|z0slq8:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"05/Jul/21 05:17;xiaopenglei;working on this;;;","05/Jul/21 11:18;apachespark;User 'Peng-Lei' has created a pull request for this issue:
https://github.com/apache/spark/pull/33219;;;","08/Jul/21 17:23;cloud_fan;Issue resolved by pull request 33219
[https://github.com/apache/spark/pull/33219];;;",,,,,,,,,,,,,
Disallow altering permanent views based on temporary views or UDFs,SPARK-36011,13387626,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,roryqi,roryqi,roryqi,05/Jul/21 04:45,06/Jul/21 15:40,13/Jul/23 08:50,06/Jul/21 06:56,3.1.2,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"PR [#15764|https://github.com/apache/spark/pull/15764] disabled creating permanent views based on temporary views or UDFs. But AlterViewCommand didn't block temporary objects.",,apachespark,cloud_fan,roryqi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 15:40:51 UTC 2021,,,,,,,,,,"0|z0slow:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/21 04:48;apachespark;User 'jerqi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33204;;;","05/Jul/21 04:49;apachespark;User 'jerqi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33204;;;","06/Jul/21 06:56;cloud_fan;Issue resolved by pull request 33204
[https://github.com/apache/spark/pull/33204];;;","06/Jul/21 12:38;apachespark;User 'jerqi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33230;;;","06/Jul/21 12:38;apachespark;User 'jerqi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33230;;;","06/Jul/21 15:40;apachespark;User 'jerqi' has created a pull request for this issue:
https://github.com/apache/spark/pull/33234;;;",,,,,,,,,,
Missing GraphX classes in registerKryoClasses util method,SPARK-36009,13387603,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,matthewrj,matthewrj,matthewrj,04/Jul/21 21:20,06/Jul/21 12:26,13/Jul/23 08:50,06/Jul/21 12:26,3.1.2,,,,,,,,3.3.0,,,,GraphX,,,,,0,,,,,"{{VertexRDDImpl}} uses an {{RDD[ShippableVertexPartition[VD]]}} however, {{GraphXUtils.registerKryoClasses}} does not register {{ShippableVertexPartition}}. This means when running with {{spark.kryo.registrationRequired}} set to {{true}}, we get a ""Class is not registered"" exception. This is an issue as it prevents other unregistered classes from being discovered using {{spark.kryo.registrationRequired}} as the first unregistered class found halts the whole job. It also potentially decreases the serialised size of the RDD when using Kryo.",,apachespark,matthewrj,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 12:26:14 UTC 2021,,,,,,,,,,"0|z0sljs:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/21 21:39;apachespark;User 'matthewrj' has created a pull request for this issue:
https://github.com/apache/spark/pull/32973;;;","04/Jul/21 21:40;apachespark;User 'matthewrj' has created a pull request for this issue:
https://github.com/apache/spark/pull/32973;;;","06/Jul/21 12:26;srowen;Issue resolved by pull request 32973
[https://github.com/apache/spark/pull/32973];;;",,,,,,,,,,,,,
Failed to run benchmark in GA,SPARK-36007,13387486,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Minor,Fixed,pingsutw,pingsutw,pingsutw,03/Jul/21 09:32,12/Dec/22 18:10,13/Jul/23 08:50,05/Jul/21 00:17,3.2.0,,,,,,,,3.2.0,3.3.0,,,Tests,,,,,0,,,,,"When I running the benchmark in GA, I met the below error.

[https://github.com/pingsutw/spark/runs/2867617238?check_suite_focus=true] 
{code:java}
java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1692)java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:175)21/06/20 07:40:02 ERROR SparkContext: Error initializing SparkContext.java.lang.AssertionError: assertion failed: spark.test.home is not set! at scala.Predef$.assert(Predef.scala:223) at org.apache.spark.deploy.worker.Worker.<init>(Worker.scala:148) at org.apache.spark.deploy.worker.Worker$.startRpcEnvAndEndpoint(Worker.scala:954) at org.apache.spark.deploy.LocalSparkCluster.$anonfun$start$2(LocalSparkCluster.scala:68) at org.apache.spark.deploy.LocalSparkCluster.$anonfun$start$2$adapted(LocalSparkCluster.scala:65) at scala.collection.immutable.Range.foreach(Range.scala:158) at org.apache.spark.deploy.LocalSparkCluster.start(LocalSparkCluster.scala:65) at org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2954) at org.apache.spark.SparkContext.<init>(SparkContext.scala:559) at org.apache.spark.SparkContext.<init>(SparkContext.scala:137) at org.apache.spark.serializer.KryoSerializerBenchmark$.createSparkContext(KryoSerializerBenchmark.scala:86) at org.apache.spark.serializer.KryoSerializerBenchmark$.sc$lzycompute$1(KryoSerializerBenchmark.scala:58) at org.apache.spark.serializer.KryoSerializerBenchmark$.sc$1(KryoSerializerBenchmark.scala:58) at org.apache.spark.serializer.KryoSerializerBenchmark$.$anonfun$run$3(KryoSerializerBenchmark.scala:63){code}",,apachespark,pingsutw,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Mon Jul 05 00:17:43 UTC 2021,,,,,,,,,,"0|z0sku0:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/21 09:40;apachespark;User 'pingsutw' has created a pull request for this issue:
https://github.com/apache/spark/pull/33203;;;","03/Jul/21 09:41;apachespark;User 'pingsutw' has created a pull request for this issue:
https://github.com/apache/spark/pull/33203;;;","05/Jul/21 00:17;gurwls223;Issue resolved by pull request 33203
[https://github.com/apache/spark/pull/33203];;;",,,,,,,,,,,,,
Update MiMa and audit Scala/Java API changes,SPARK-36004,13387415,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Blocker,Fixed,dongjoon,dongjoon,dongjoon,03/Jul/21 00:55,06/Jul/21 12:16,13/Jul/23 08:50,06/Jul/21 12:16,3.2.0,,,,,,,,3.3.0,,,,Project Infra,,,,,0,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 12:16:31 UTC 2021,,,,,,,,,,"0|z0skeg:",9223372036854775807,,,,,,,,,,,,,3.2.0,,,,,,,,,,"03/Jul/21 01:07;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33199;;;","06/Jul/21 12:16;srowen;Issue resolved by pull request 33199
[https://github.com/apache/spark/pull/33199];;;",,,,,,,,,,,,,,
Enable GitHub Action build_and_test on branch-3.2,SPARK-35995,13387378,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/Jul/21 17:27,02/Jul/21 20:09,13/Jul/23 08:50,02/Jul/21 17:57,3.2.0,,,,,,,,3.2.0,,,,Project Infra,,,,,0,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 02 20:09:54 UTC 2021,,,,,,,,,,"0|z0sk68:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/21 17:29;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33194;;;","02/Jul/21 17:30;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33194;;;","02/Jul/21 17:57;dongjoon;Issue resolved by pull request 33194
[https://github.com/apache/spark/pull/33194];;;","02/Jul/21 20:09;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/33197;;;","02/Jul/21 20:09;apachespark;User 'ueshin' has created a pull request for this issue:
https://github.com/apache/spark/pull/33197;;;",,,,,,,,,,,
Publish snapshot from branch-3.2,SPARK-35994,13387377,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,dongjoon,dongjoon,dongjoon,02/Jul/21 17:07,02/Jul/21 17:55,13/Jul/23 08:50,02/Jul/21 17:54,3.2.0,,,,,,,,3.2.0,,,,Project Infra,,,,,0,,,,,,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 02 17:54:52 UTC 2021,,,,,,,,,,"0|z0sk60:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/21 17:08;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33192;;;","02/Jul/21 17:54;dongjoon;Issue resolved by pull request 33192
[https://github.com/apache/spark/pull/33192];;;",,,,,,,,,,,,,,
Upgrade ORC to 1.6.9,SPARK-35992,13387264,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Critical,Fixed,dongjoon,dongjoon,dongjoon,02/Jul/21 11:11,02/Jul/21 16:50,13/Jul/23 08:50,02/Jul/21 16:50,3.2.0,,,,,,,,3.2.0,,,,Build,,,,,0,,,,,This issue aims to upgrade Apache ORC to 1.6.9 to bring ORC encryption masking fix.,,apachespark,dongjoon,,,,,,,,,,,,,,,,,,,,,,,,,,,,ORC-804,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Fri Jul 02 16:50:23 UTC 2021,,,,,,,,,,"0|z0sjhk:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/21 11:13;apachespark;User 'dongjoon-hyun' has created a pull request for this issue:
https://github.com/apache/spark/pull/33189;;;","02/Jul/21 16:50;dongjoon;Issue resolved by pull request 33189
[https://github.com/apache/spark/pull/33189];;;",,,,,,,,,,,,,,
File source V2 ignores partition filters when empty readDataSchema,SPARK-35985,13387216,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,steven.aerts,steven.aerts,steven.aerts,02/Jul/21 07:03,18/Sep/21 21:29,13/Jul/23 08:50,16/Jul/21 04:53,3.0.0,,,,,,,,3.2.0,,,,SQL,,,,,0,,,,,"A V2 datasource fails to rely on partition filters when it only wants to know how many entries there are, and is not interested of their context.
So when the {{readDataSchema}} of the {{FileScan}} is empty, partition filters are not pushed down and all data is scanned.

Some examples where this happens:
{code:java}
scala> spark.sql(""SELECT count(*) FROM parq WHERE day=20210702"").explain
== Physical Plan ==
*(2) HashAggregate(keys=[], functions=[count(1)])
+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#136]
 +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])
 +- *(1) Project
 +- *(1) Filter (isnotnull(day#68) AND (day#68 = 20210702))
 +- *(1) ColumnarToRow
 +- BatchScan[day#68] ParquetScan DataFilters: [], Format: parquet, Location: InMemoryFileIndex[file:/..., PartitionFilters: [], PushedFilers: [IsNotNull(day), EqualTo(day,20210702)], ReadSchema: struct<>, PushedFilters: [IsNotNull(day), EqualTo(day,20210702)]
scala> spark.sql(""SELECT input_file_name() FROM parq WHERE day=20210702"").explain
== Physical Plan ==
*(1) Project [input_file_name() AS input_file_name()#131]
+- *(1) Filter (isnotnull(day#68) AND (day#68 = 20210702))
 +- *(1) ColumnarToRow
 +- BatchScan[day#68] ParquetScan DataFilters: [], Format: parquet, Location: InMemoryFileIndex[file:/..., PartitionFilters: [], PushedFilers: [IsNotNull(day), EqualTo(day,20210702)], ReadSchema: struct<>, PushedFilters: [IsNotNull(day), EqualTo(day,20210702)]
{code}
 

Once the {{readDataSchema}} is not empty, it works correctly:
{code:java}
scala> spark.sql(""SELECT header.tenant FROM parq WHERE day=20210702"").explain
== Physical Plan ==
*(1) Project [header#51.tenant AS tenant#199]
+- BatchScan[header#51, day#68] ParquetScan DataFilters: [], Format: parquet, Location: InMemoryFileIndex[file:/..., PartitionFilters: [isnotnull(day#68), (day#68 = 20210702)], PushedFilers: [IsNotNull(day), EqualTo(day,20210702)], ReadSchema: struct<header:struct<tenant:string>>, PushedFilters: [IsNotNull(day), EqualTo(day,20210702)]{code}
 

In V1 this optimization is available:
{code:java}
scala> spark.sql(""SELECT count(*) FROM parq WHERE day=20210702"").explain
== Physical Plan ==
*(2) HashAggregate(keys=[], functions=[count(1)])
+- Exchange SinglePartition, ENSURE_REQUIREMENTS, [id=#27]
 +- *(1) HashAggregate(keys=[], functions=[partial_count(1)])
 +- *(1) Project
 +- *(1) ColumnarToRow
 +- FileScan parquet [year#15,month#16,day#17,hour#18] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex[file:/..., PartitionFilters: [isnotnull(day#17), (day#17 = 20210702)], PushedFilters: [], ReadSchema: struct<>{code}
The examples use {{ParquetScan}}, but the problem happens for all File based V2 datasources.

The fix for this issue feels very straight forward. In {{PruneFileSourcePartitions}} queries with an empty {{readDataSchema}} are explicitly excluded from being pushed down:
{code:java}
if filters.nonEmpty && scan.readDataSchema.nonEmpty =>{code}
Removing that condition seems to fix the issue however, this might be too naive.

I am making a PR with tests where this change can be discussed.",,apachespark,cloud_fan,praetp,steven.aerts,xkrogen,,,,,,,,,,,,,,,,SPARK-36776,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Sat Sep 18 21:29:30 UTC 2021,,,,,,,,,,"0|z0sj6w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/21 14:07;apachespark;User 'steven-aerts' has created a pull request for this issue:
https://github.com/apache/spark/pull/33191;;;","16/Jul/21 04:53;cloud_fan;Issue resolved by pull request 33191
[https://github.com/apache/spark/pull/33191];;;","18/Sep/21 21:28;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34037;;;","18/Sep/21 21:29;apachespark;User 'huaxingao' has created a pull request for this issue:
https://github.com/apache/spark/pull/34037;;;",,,,,,,,,,,,
When replace ExtractValue in NestedColumnAliasing we should use semanticEquals,SPARK-35972,13387027,Bug,Resolved,SPARK,Spark,software,matei,"<div>

<b><i>Apache Spark</i></b> is a fast and general cluster computing system.
It provides high-level APIs in 
Scala, Java, Python and R, and an optimized engine that supports general computation graphs.
It also supports a rich set of higher-level tools including 
<a href=""http://spark.apache.org/docs/latest/sql-programming-guide.html"">Spark SQL</a> for SQL and structured data processing, 
<a href=""http://spark.apache.org/docs/latest/mllib-guide.html"">MLLib</a> for machine learning, 
<a href=""http://spark.apache.org/docs/latest/graphx-programming-guide.html"">GraphX</a> for graph processing, and 
<a href=""http://spark.apache.org/docs/latest/streaming-programming-guide.html"">Spark Streaming</a>.


<table style=""border: 0px; width: 100%;"">
<tr>
<td>
For more information, see:
<br><a href=""http://spark.apache.org/"">The Spark Homepage</a>
<br>
<a href=""https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage"">The Spark Wiki</a> and <a href=""https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark"">How to Contribute to Spark</a>
<br>
<a href=""https://github.com/apache/spark"">Spark's Github Repository</a>
</td>
<td style=""text-align: center;"">
<a href=""http://spark.apache.org/""><img src=""http://spark.apache.org/images/spark-logo.png"" width=""180px""></a>
</td>

</tr>
</table>

</div>",http://spark.apache.org,Major,Fixed,angerszhu,angerszhu,angerszhu,01/Jul/21 11:02,16/Jul/21 07:42,13/Jul/23 08:50,06/Jul/21 07:11,3.1.2,3.2.0,,,,,,,3.1.3,3.2.0,,,SQL,,,,,0,,,,,"{code:java}
Job aborted due to stage failure: Task 47 in stage 1.0 failed 4 times, most recent failure: Lost task 47.3 in stage 1.0 (TID 328) (ip-idata-server.shopee.io executor 3): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Binding attribute, tree: _gen_alias_788#788
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:75)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:318)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(TreeNode.scala:377)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(TreeNode.scala:438)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:438)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(TreeNode.scala:377)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(TreeNode.scala:438)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:438)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(TreeNode.scala:386)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(TreeNode.scala:438)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:438)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(TreeNode.scala:377)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(TreeNode.scala:438)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:438)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChild$2(TreeNode.scala:386)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$4(TreeNode.scala:438)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:438)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:408)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:244)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:406)
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:359)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:307)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:96)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.collection.TraversableLike.map(TraversableLike.scala:238)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
	at scala.collection.immutable.List.map(List.scala:298)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:96)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:160)
	at org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1(basicPhysicalOperators.scala:93)
	at org.apache.spark.sql.execution.ProjectExec.$anonfun$doExecute$1$adapted(basicPhysicalOperators.scala:92)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2(RDD.scala:885)
	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndexInternal$2$adapted(RDD.scala:885)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:386)
	at org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1423)
	at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1350)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1414)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1237)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:384)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:335)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:337)
	at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.RuntimeException: Couldn't find _gen_alias_788#788 in [view_event_id#56,view_timestamp#57,page_type#82,page_section#58,target_type#88,platform#83,ab_test#65,event_timestamp#59,user_id#60L,device_id#61,app_version#67,session_id#86,pre_view_event_id#62,rand_pre_view_event_id#63,_gen_alias_726#726,_gen_alias_727#727,_gen_alias_728#728,_gen_alias_729#729,_gen_alias_730#730,_gen_alias_731#731,_gen_alias_732#732,_gen_alias_733#733,_gen_alias_734#734,_gen_alias_735#735,_gen_alias_736#736,_gen_alias_737#737,_gen_alias_738#738,_gen_alias_739#739,_gen_alias_740#740,_gen_alias_741#741,_gen_alias_742#742,_gen_alias_743#743,_gen_alias_744#744,_gen_alias_745#745,_gen_alias_746#746,_gen_alias_747#747,_gen_alias_748#748,_gen_alias_749#749,_gen_alias_750#750,_gen_alias_751#751,_gen_alias_752#752,_gen_alias_753#753,_gen_alias_754#754,_gen_alias_755#755,_gen_alias_756#756,_gen_alias_757#757,_gen_alias_758#758,_gen_alias_759#759,_gen_alias_760#760,_gen_alias_761#761,_gen_alias_762#762,_gen_alias_763#763,_gen_alias_764#764,_gen_alias_765#765,_gen_alias_766#766,_gen_alias_767#767,_gen_alias_768#768,_gen_alias_769#769,_gen_alias_770#770,_gen_alias_771#771,_gen_alias_772#772,_gen_alias_773#773,_gen_alias_774#774,_gen_alias_775#775,_gen_alias_776#776,_gen_alias_777#777,_gen_alias_778#778,_gen_alias_779#779,_gen_alias_780#780,_gen_alias_781#781,_gen_alias_782#782,_gen_alias_783#783,_gen_alias_784#784,_gen_alias_785#785,_gen_alias_786#786,_gen_alias_787#787]
	at scala.sys.package$.error(package.scala:30)
	at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.$anonfun$applyOrElse$1(BoundAttribute.scala:81)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
	... 157 more

Driver stacktrace:
{code}
",,angerszhu,apachespark,dongjoon,viirya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,,,9223372036854775807,,,,Tue Jul 06 07:29:32 UTC 2021,,,,,,,,,,"0|z0si0w:",9223372036854775807,,,,,,,,,,,,,,,,,,,,,,,"01/Jul/21 11:07;angerszhu;We meet a case that it can analyze/optimize and generate sparkplan well but when running, executor will throw exception like above in desc. looks like child's output loss data

@;;;","01/Jul/21 18:03;dongjoon;Hi, [~angerszhu]. Could you make this as a BUG?
Also, could you provide more detail?;;;","02/Jul/21 06:26;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33183;;;","02/Jul/21 06:27;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33183;;;","05/Jul/21 19:40;viirya;From the description, looks like a bug?;;;","06/Jul/21 07:12;viirya;Fixed at https://github.com/apache/spark/pull/33183.;;;","06/Jul/21 07:29;apachespark;User 'AngersZhuuuu' has created a pull request for this issue:
https://github.com/apache/spark/pull/33227;;;",,,,,,,,,
